{
    "Introduction": "Introduction During the last several years Deep Reinforcement Learning proved to be a fruitful approach to many arti\ufb01cial intelligence tasks of diverse domains. Breakthrough achievements include reaching human-level performance in such complex games as Go [22], multiplayer Dota [16] and real-time strategy StarCraft II [26]. The generality of DRL framework allows its application in both discrete and continuous domains to solve tasks in robotics and simulated environments [12]. Reinforcement Learning (RL) is usually viewed as general formalization of decision-making task and is deeply connected to dynamic programming, optimal control and game theory. [23] Yet its problem setting makes almost no assumptions about world model or its structure and usually supposes that environment is given to agent in a form of black-box. This allows to apply RL practically in all settings and forces designed algorithms to be adaptive to many kinds of challenges. Latest RL algorithms are usually reported to be transferable from one task to another with no task-speci\ufb01c changes and little to no hyperparameters tuning. As an object of desire is a strategy, i. e. a function mapping agent\u2019s observations to possible actions, reinforcement learning is considered to be a sub\ufb01led of machine learning. But instead of learning from data, as it is established in classical supervised and unsupervised learning problems, the agent learns from experience of interacting with environment. Being more \"natural\" model of learning, this setting causes new challenges, peculiar only to reinforcement learning, such as necessity of exploration integration and the problem of delayed and sparse rewards. The full setup and essential notation are introduced in section 2. Classical Reinforcement Learning research in the last third of previous century developed an extensive theoretical core for modern algorithms to ground on. Several algorithms are known ever since and are able to solve small-scale problems when either environment states can be enumerated (and stored in the memory) or optimal policy can be searched in the space of linear or quadratic functions of state representation features. Although these restrictions are extremely limiting, foundations of classical RL theory underlie modern approaches. These theoretical fundamentals are discussed in sections 3.1 and 5.1\u20135.2. Combining this framework with Deep Learning [5] was popularized by Deep Q-Learning algorithm, introduced in [14], which was able to play any of 57 Atari console games without tweaking network architecture or algorithm hyperparameters. This novel approach was extensively researched and signi\ufb01cantly improved in the following years. The principles of value-based direction in deep reinforcement learning are presented in section 3. One of the key ideas in the recent value-based DRL research is distributional approach, proposed in [1]. Further extending classical theoretical foundations and coming with practical DRL algorithms, it gave birth to distributional reinforcement learning paradigm, which potential is now being actively investigated. Its ideas are described in section 4. Second main direction of DRL research is policy gradient methods, which attempt to directly optimize the objective function, explicitly present in the problem setup. Their application to neural networks involve a series of particular obstacles, which requested specialized optimization techniques. Today they represent a competitive and scalable approach in deep reinforcement learning due to their enormous parallelization potential and continuous domain applicability. Policy gradient methods are discussed in section 5. Despite the wide range of successes, current state-of-art DRL methods still face a number of signi\ufb01cant drawbacks. As training of neural networks requires huge amounts of data, DRL demonstrates unsatisfying results in settings where data generation is expensive. Even in cases where interaction is nearly free (e. g. in simulated environments), DRL algorithms tend to require excessive amounts of iterations, which raise their computational and wall-clock time cost. Furthermore, DRL su\ufb00ers from random initialization and hyperparameters sensitivity, and its optimization process is known to be uncomfortably unstable [9]. Especially embarrassing consequence of these DRL features turned out to be low reproducibility of empirical observations from di\ufb00erent research groups [6]. In section 6, we attempt to launch state-of-art DRL algorithms on several standard testbed environments and discuss practical nuances of their application. 4 2. Reinforcement Learning problem setup 2.1. Assumptions of RL setting Informally, the process of sequential decision-making proceeds as follows. The agent is provided with some initial observation of environment and is required to choose some action from the given set of possibilities. The environment responds by transitioning to another state and generating a reward signal (scalar number), which is considered to be a ground-truth estimation of agent\u2019s performance. The process continues repeatedly with agent making choices of actions from observations and environment responding with next states and reward signals. The only goal of agent is to maximize the cumulative reward. This description of learning process model already introduces several key assumptions. Firstly, the time space is considered to be discrete, as agent interacts with environment sequentially. Secondly, it is assumed that provided environment incorporates some reward function as supervised indicator of success. This is an embodiment of the reward hypothesis, also referred to as Reinforcement Learning hypothesis: Proposition 1. (Reward Hypothesis) [23] \u00abAll of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u00bb Exploitation of this hypothesis draws a line between reinforcement learning and classical machine learning settings, supervised and unsupervised learning. Unlike unsupervised learning, RL assumes supervision, which, similar to labels in data for supervised learning, has a stochastic nature and represents a key source of knowledge. At the same time, no data or \u00abright answer\u00bb is provided to training procedure, which distinguishes RL from standard supervised learning. Moreover, RL is the only machine learning task providing explicit objective function (cumulative reward signal) to maximize, while in supervised and unsupervised setting optimized loss function is usually constructed by engineer and is not \u00abincluded\u00bb in data. The fact that reward signal is incorporated in the environment is considered to be one of the weakest points of RL paradigm, as for many real-life human goals introduction of this scalar reward signal is at the very least unobvious. For practical applications it is also natural to assume that agent\u2019s observations can be represented by some feature vectors, i. e. elements of Rd. The set of possible actions in most practical applications is usually uncomplicated and is either discrete (number of possible actions is \ufb01nite) or can be represented as subset of Rm (almost always [\u22121, 1]m or can be reduced to this case)1. RL algorithms are usually restricted to these two cases, but the mix of two (agent is required to choose both discrete and continuous quantities) can also be considered. The \ufb01nal assumption of RL paradigm is a Markovian property: Proposition 2. (Markovian property) Transitions depend solely on previous state and the last chosen action and are independent of all previous interaction history. Although this assumption may seem overly strong, it actually formalizes the fact that the world modeled by considered environment obeys some general laws. Giving that the agent knows the current state of the world and the laws, it is assumed that it is able to predict the consequences of his actions up to the internal stochasticity of these laws. In practice, both laws and complete state representation is unavailable to agent, which limits its forecasting capability. In the sequel we will work within the setting with one more assumption of full observability. This simpli\ufb01cation supposes that agent can observe complete world state, while in many real-life tasks only a part of observations is actually available. This restriction of RL theory can be removed by considering Partially observable Markov Decision Processes (PoMDP), which basically forces learning algorithms to have some kind of memory mechanism to store previously received observations. Further on we will stick to fully observable case. 1this set is considered to be permanent for all states of environment without any loss of generality as if agent chooses invalid action the world may remain in the same state with zero or negative reward signal or stochastically select some valid action for him. 5 2.2. Environment model Though the de\ufb01nition of Markov Decision Process (MDP) varies from source to source, its essential meaning remains the same. The de\ufb01nition below utilizes several simpli\ufb01cations without loss of generality.2 De\ufb01nition 1. Markov Decision Process (MDP) is a tuple (S, A, T, r, s0), where: \u2022 S \u2286 Rd \u2014 arbitrary set, called the state space. \u2022 A \u2014 a set, called the action space, either \u2013 discrete: |A| < +\u221e, or \u2013 continuous domain: A = [\u22121, 1]m. \u2022 T \u2014 transition probability p(s\u2032 | s, a), where s, s\u2032 \u2208 S, a \u2208 A. \u2022 r : S \u2192 R \u2014 reward function. \u2022 s0 \u2208 S \u2014 starting state. It is important to notice that in the most general case the only things available for RL algorithm beforehand are d (dimension of state space) and action space A. The only possible way of collecting more information for agent is to interact with provided environment and observe s0. It is obvious that the \ufb01rst choice of action a0 will be probably random. While the environment responds by sampling s1 \u223c p(s1 | s0, a0), this distribution, de\ufb01ned in T and considered to be a part of MDP, may be unavailable to agent\u2019s learning procedure. What agent does observe is s1 and reward signal r1 := r(s1) and it is the key information gathered by agent from interaction experience. De\ufb01nition 2. The tuple (st, at, rt+1, st+1) is called transition. Several sequential transitions are usually referred to as roll-out. Full track of observed quantities s0, a0, r1, s1, a1, r2, s2, a2, r3, s3, a3 . . . is called a trajectory. In general case, the trajectory is in\ufb01nite which means that the interaction process is neverending. However, in most practical cases the episodic property holds, which basically means that the interaction will eventually come to some sort of an end3. Formally, it can be simulated by the environment stucking in the last state with zero probability of transitioning to any other state and zero reward signal. Then it is convenient to reset the environment back to s0 to initiate new interaction. One such interaction cycle from s0 till reset, spawning one trajectory of some \ufb01nite length T , is called an episode. Without loss of generality, it can be considered that there exists a set of terminal states S+, which mark the ends of interactions. By convention, transitions (st, at, rt+1, st+1) are accompanied with binary \ufb02ag donet+1 \u2208 {0, 1}, whether st+1 belongs to S+. As timestep t at which the transition was gathered is usually of no importance, transitions are often denoted as (s, a, r\u2032, s\u2032, done) with primes marking the \u00abnext timestep\u00bb. Note that the length of episode T may vary between di\ufb00erent interactions, but the episodic property holds if interaction is guaranteed to end after some \ufb01nite time T max. If this is not the case, the task is called continuing. 2.3. Objective In reinforcement learning, the agent\u2019s goal is to maximize a cumulative reward. In episodic case, this reward can be expressed as a summation of all received reward signals during one episode and 2the reward function is often introduced as stochastic and dependent on action a, i. e. R(r | s, a): S \u00d7 A \u2192 P(R), while instead of \ufb01xed s0 a distribution over S is given. Both extensions can be taken into account in terms of presented de\ufb01nition by extending the state space and incorporating all the uncertainty into transition probability T. 3natural examples include the end of the game or agent\u2019s failure/success in completing some task. 6 is called the return: R := T \ufffd t=1 rt (1) Note that this quantity is formally a random variable, which depends on agent\u2019s choices and the outcomes of environment transitions. As this stochasticity is an inevitable part of interaction process, the underlying distribution from which rt is sampled must be properly introduced to set rigorously the task of return maximization. De\ufb01nition 3. Agent\u2019s algorithm for choosing a by given current state s, which in general can be viewed as distribution \u03c0(a | s) on domain A, is called a policy (strategy). Deterministic policy, when the policy is represented by deterministic function \u03c0 : S \u2192 A, can be viewed as a particular case of stochastic policy with degenerated policy \u03c0(a | s), when agent\u2019s output is still a distribution with zero probability to choose an action other than \u03c0(s). In both cases it is considered that agent sends to environment a sample a \u223c \u03c0(a | s). Note that given some policy \u03c0(a | s) and transition probabilities T, the complete interaction process becomes de\ufb01ned from probabilistic point of view: De\ufb01nition 4. For given MDP and policy \u03c0, the probability of observing s0, a0, s1, a1, s2, a2 . . . is called trajectory distribution and is denoted as T\u03c0: T\u03c0 := \ufffd t=0 p(st+1 | st, at)\u03c0(at | st) It is always substantial to keep track of what policy was used to collect certain transitions (roll-outs and episodes) during the learning procedure, as they are essentially samples from corresponding trajectory distribution. If the policy is modi\ufb01ed in any way, the trajectory distribution changes either. Now when a policy induces a trajectory distribution, it is possible to formulate a task of expected reward maximization: ET\u03c0 T \ufffd t=1 rt \u2192 max \u03c0 To ensure the \ufb01niteness of this expectation and avoid the case when agent is allowed to gather in\ufb01nite reward, limit on absolute value of rt can be assumed: |rt| \u2264 Rmax Together with the limit on episode length T max this restriction guarantees \ufb01niteness of optimal (maximal) expected reward. To extend this intuition to continuing tasks, the reward for each next interaction step is multiplied on some discount coe\ufb03cient \u03b3 \u2208 [0, 1), which is often introduced as part of MDP. This corresponds to the logic that with probability 1 \u2212 \u03b3 agent \u00abdies\u00bb and does not gain any additional reward, which models the paradigm \u00abbetter now than later\u00bb. In practice, this discount factor is set very close to 1. De\ufb01nition 5. For given MDP and policy \u03c0 the discounted expected reward is de\ufb01ned as J(\u03c0) := ET\u03c0 \ufffd t=0 \u03b3trt+1 Reinforcement learning task is to \ufb01nd an optimal policy \u03c0\u2217, which maximizes the discounted expected reward: J(\u03c0) \u2192 max \u03c0 (2) 7 2.4. Value functions Solving reinforcement learning task (2) usually leads to a policy, that maximizes the expected reward not only for starting state s0, but for any state s \u2208 S. This follows from the Markov property: the reward which is yet to be collected from some step t does not depend on previous history and for agent staying at state s the task of behaving optimal is equivalent to maximization of expected reward with current state s as a starting state. This is the particular reason why many reinforcement learning algorithms do not seek only optimal policy, but additional information about usefulness of each state. De\ufb01nition 6. For given MDP and policy \u03c0 the value function under policy \u03c0 is de\ufb01ned as V \u03c0(s) := ET\u03c0|s0=s \ufffd t=0 \u03b3trt+1 This value function estimates how good it is for agent utilizing strategy \u03c0 to visit state s and generalizes the notion of discounted expected reward J(\u03c0) that corresponds to V \u03c0(s0). As value function can be induced by any policy, value function V \u03c0\u2217(s) under optimal policy \u03c0\u2217 can also be considered. By convention4, it is denoted as V \u2217(s) and is called an optimal value function. Obtaining optimal value function V \u2217(s) doesn\u2019t provide enough information to reconstruct some optimal policy \u03c0\u2217 due to unknown world dynamics, i. e. transition probabilities. In other words, being blind to what state s may be the environment\u2019s response on certain action in a given state makes knowing optimal value function unhelpful. This intuition suggests to introduce a similar notion comprising more information: De\ufb01nition 7. For given MDP and policy \u03c0 the quality function (Q-function) under policy \u03c0 is de\ufb01ned as Q\u03c0(s, a) := ET\u03c0|s0=s,a0=a \ufffd t=0 \u03b3trt+1 It directly follows from the de\ufb01nitions that these two functions are deeply interconnected: Q\u03c0(s, a) = Es\u2032\u223cp(s\u2032|s,a) [r(s\u2032) + \u03b3V \u03c0(s\u2032)] (3) V \u03c0(s) = Ea\u223c\u03c0(a|s)Q\u03c0(s, a) (4) The notion of optimal Q-function Q\u2217(s, a) can be introduced analogically. But, unlike value function, obtaining Q\u2217(s, a) actually means solving a reinforcement learning task: indeed, Proposition 3. If Q\u2217(s, a) is a quality function under some optimal policy, then \u03c0\u2217(s) = argmax a Q\u2217(s, a) is an optimal policy. This result implies that instead of searching for optimal policy \u03c0\u2217, an agent can search for optimal Q-function and derive the policy from it. Proposition 4. For any MDP existence of optimal policy leads to existence of deterministic optimal policy. 4though optimal policy may not be unique, the value functions under any optimal policy that behaves optimally from any given state (not only s0) coincide. Yet, optimal policy may not know optimal behaviour for some states if it knows how to avoid them with probability 1. 8 2.5. Classes of algorithms Reinforcement learning algorithms are presented in a form of computational procedures specifying a strategy of collecting interaction experience and obtaining a policy with as higher J(\u03c0) as possible. They rarely include a stopping criterion like in classic optimization methods as the stochasticity of given setting prevents any reasonable veri\ufb01cation of optimality; usually the number of iterations to perform is determined by the amount of computational resources. All reinforcement learning algorithms can be roughly divided into four5 classes: \u2022 meta-heuristics: this class of algorithms treats the task as black-box optimization with zerothorder oracle. They usually generate a set of policies \u03c01 . . . \u03c0P and launch several episodes of interaction for each to determine best and worst policies according to average return. After that they try to construct more optimal policies using evolutionary or advanced random search techniques [17]. \u2022 policy gradient: these algorithms directly optimize (2), trying to obtain \u03c0\u2217 and no additional information about MDP, using approximate estimations of gradient with respect to policy parameters. They consider RL task as an optimization with stochastic \ufb01rst-order oracle and make use of interaction structure to lower the variance of gradient estimations. They will be discussed in sec. 5. \u2022 value-based algorithms construct optimal policy implicitly by gaining an approximation of optimal Q-function Q\u2217(s, a) using dynamic programming. In DRL, Q-function is represented with neural network and an approximate dynamic programming is performed using reduction to supervised learning. This framework will be discussed in sec. 3 and 4. \u2022 model-based algorithms exploit learned or given world dynamics, i. e. distributions p(s\u2032 | s, a) from T. The class of algorithms to work with when the model is explicitly provided is represented by such algorithms as Monte-Carlo Tree Search; if not, it is possible to imitate the world dynamics by learning the outputs of black box from interaction experience [10]. 2.6. Measurements of performance Achieved performance (score) from the point of average cumulative reward is not the only one measure of RL algorithm quality. When speaking of real-life robots, the required number of simulated episodes is always the biggest concern. It is usually measured in terms of interaction steps (where step is one transition performed by environment) and is referred to as sample e\ufb03ciency. When the simulation is more or less cheap, RL algorithms can be viewed as a special kind of optimization procedures. In this case, the \ufb01nal performance of the found policy is opposed to required computational resources, measured by wall-clock time. In most cases RL algorithms can be expected to \ufb01nd better policy after more iterations, but the amount of these iterations tend to be unjusti\ufb01ed. The ratio between amount of interactions and required wall-clock time for one update of policy varies signi\ufb01cantly for di\ufb00erent algorithms. It is well-known that model-based algorithms tend to have the greatest sample-e\ufb03ciency at the cost of expensive update iterations, while evolutionary algorithms require excessive amounts of interactions while providing massive resources for parallelization and reduction of wall-clock time. Value-based and policy gradient algorithms, which will be the focus of our further discussion, are known to lie somewhere in between. 5in many sources evolutionary algorithms are bypassed in discussion as they do not utilize the structure of RL task in any way. 9 3. Value-based algorithms 3.1. Temporal Di\ufb00erence learning In this section we consider temporal di\ufb00erence learning algorithm [23, Chapter 6], which is a classical Reinforcement Learning method in the base of modern value-based approach in DRL. The \ufb01rst idea behind this algorithm is to search for optimal Q-function Q\u2217(s, a) by solving a system of recursive equations which can be derived by recalling interconnection between Q-function and value function (3): Q\u03c0(s, a) = Es\u2032\u223cp(s\u2032|s,a) [r(s\u2032) + \u03b3V \u03c0(s\u2032)] = = {using (4)} = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3Ea\u2032\u223c\u03c0(a\u2032|s\u2032)Q\u03c0(s\u2032, a\u2032) \ufffd This equation, named Bellman equation, remains true for value functions under any policies including optimal policy \u03c0\u2217: Q\u2217(s, a) = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3Ea\u2032\u223c\u03c0(a\u2032|s\u2032)Q\u2217(s\u2032, a\u2032) \ufffd (5) Recalling proposition 3, optimal (deterministic) policy can be represented as \u03c0\u2217(s) = argmax a Q\u2217(s, a). Substituting this for \u03c0\u2217(s) in (5), we obtain fundamental Bellman optimality equation: Proposition 5. (Bellman optimality equation) Q\u2217(s, a) = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032) \ufffd (6) The straightforward utilization of this result is as follows. Consider the tabular case, when both state space S and action space A are \ufb01nite (and small enough to be listed in computer memory). Let us also assume for now that transition probabilities are available to training procedure. Then Q\u2217(s, a) : S \u00d7 A \u2192 R can be represented as a \ufb01nite table with |S||A| numbers. In this case (6) just gives a set of |S||A| equations for this table to satisfy. Addressing the values of the table as unknown variables, this system of equations can be solved using basic point iteration method: let Q\u2217 0(s, a) be initial arbitrary values of table (with the only exception that for terminal states s \u2208 S+, if any, Q\u2217 0(s, a) = 0 for all actions a). On each iteration t the table is updated by substituting current values of the table to the right side of equation until the process converges: Q\u2217 t+1(s, a) = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3 max a\u2032 Q\u2217 t (s\u2032, a\u2032) \ufffd (7) This straightforward approach of learning the optimal Q-function, named Q-learning, has been extensively studied in classical Reinforcement Learning. One of the central results is presented in the following convergence theorem: Proposition 6. Let by B denote an operator (S \u00d7 A \u2192 R) \u2192 (S \u00d7 A \u2192 R), updating Q\u2217 t as in (7): Q\u2217 t+1 = BQ\u2217 t for all state-action pairs s, a. Then B is a contraction mapping, i. .e. for any two tables Q1, Q2 \u2208 (S \u00d7 A \u2192 R) \u2225BQ1 \u2212 BQ2\u2225\u221e \u2264 \u03b3\u2225Q1 \u2212 Q2\u2225\u221e Therefore, there is a unique \ufb01xed point of the system of equations (7) and the point iteration method converges to it. The contraction mapping property is actually of high importance. It demonstrates that the point iteration algorithm converges with exponential speed and requires small amount of iterations. As the true Q\u2217 is a \ufb01xed point of (6), the algorithm is guaranteed to yield a correct answer. The trick is 10 that each iteration demands full pass across all state-action pairs and exact computation of expectations over transition probabilities. In general case, these expectations can\u2019t be explicitly computed. Instead, agent is restricted to samples from transition probabilities gained during some interaction experience. Temporal Di\ufb00erence (TD)6 algorithm proposes to collect this data using \u03c0t = argmax a Q\u2217 t (s, a) \u2248 \u03c0\u2217 and after each gathered transition (st, at, rt+1, st+1) update only one cell of the table: Q\u2217 t+1(s, a) = \uf8f1 \uf8f2 \uf8f3 (1 \u2212 \u03b1t)Q\u2217 t (s, a) + \u03b1t \ufffd rt+1 + \u03b3 max a\u2032 Q\u2217 t (st+1, a\u2032) \ufffd if s = st, a = at Q\u2217 t (s, a) else (8) where \u03b1t \u2208 (0, 1) plays the role of exponential smoothing parameter for estimating expectation Es\u2032\u223cp(s\u2032|st,at)(\u00b7) from samples. Two key ideas are introduced in the update formula (8): exponential smoothing instead of exact expectation computation and cell by cell updates instead of updating full table at once. Both are required to settle Q-learning algorithm for online application. As the set S+ of terminal states in online setting is usually unknown beforehand, a slight modi\ufb01cation of update (8) is used. If observed next state s\u2032 turns out to be terminal (recall the convention to denote this by \ufb02ag done), its value function is known to be equal to zero: V \u2217(s\u2032) = max a\u2032 Q\u2217(s\u2032, a\u2032) = 0 This knowledge is embedded in the update rule (8) by multiplying max a\u2032 Q\u2217 t (st+1, a\u2032) on (1 \u2212 donet+1). For the sake of shortness, this factor is often omitted but should be always present in implementations. Second important note about formula (8) is that it can be rewritten in the following equivalent way: Q\u2217 t+1(s, a) = \uf8f1 \uf8f2 \uf8f3 Q\u2217 t (s, a) + \u03b1t \ufffd rt+1 + \u03b3 max a\u2032 Q\u2217 t (st+1, a\u2032) \u2212 Q\u2217 t (s, a) \ufffd if s = st, a = at Q\u2217 t (s, a) else (9) The expression in the brackets, referred to as temporal di\ufb00erence, represents a di\ufb00erence between Q-value Q\u2217 t (s, a) and its one-step approximation rt+1 + \u03b3 max a\u2032 Q\u2217 t (st+1, a\u2032), which must be zero in expectation for true optimal Q-function. The idea of exponential smoothing allows us to formulate \ufb01rst practical algorithm which can work in the tabular case with unknown world dynamics: Algorithm 1: Temporal Di\ufb00erence algorithm Hyperparameters: \u03b1t \u2208 (0, 1) Initialize Q\u2217(s, a) arbitrary On each interaction step: 1. select a = argmax a Q\u2217(s, a) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. update table: Q\u2217(s, a) \u2190 Q\u2217(s, a) + \u03b1t \ufffd r\u2032 + (1 \u2212 done)\u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032) \u2212 Q\u2217(s, a) \ufffd It turns out that under several assumptions on state visitation during interaction process this procedure holds similar properties in terms of convergence guarantees, which are stated by the following theorem: 6also known as TD(0) due to theoretical generalizations 11 Proposition 7. [28] Let\u2019s de\ufb01ne et(s, a) = \ufffd \u03b1t (s, a) is updated on step t 0 otherwise Then if for every state-action pair (s, a) +\u221e \ufffd t et(s, a) = \u221e +\u221e \ufffd t et(s, a)2 < \u221e the algorithm 1 converges to optimal Q\u2217 with probability 1. This theorem states that basic policy iteration method can be actually applied online in the way proposed by TD algorithm, but demands \u00abenough exploration\u00bb from the strategy of interacting with MDP during training. Satisfying this demand remains a unique and common problem of reinforcement learning. The widespread kludge is \u03b5-greedy strategy which basically suggests to choose random action instead of a = argmax a Q\u2217(s, a) with probability \u03b5t. The probability \u03b5t is usually set close to 1 during \ufb01rst interaction iterations and scheduled to decrease to a constant close to 0. This heuristic makes agent visit all states with non-zero probabilities independent of what current approximation Q\u2217(s, a) suggests. The main practical issue with Temporal Di\ufb00erence algorithm is that it requires table Q\u2217(s, a) to be explicitly stored in memory, which is impossible for MDP with high state space complexity. This limitation substantially restricted its applicability until its combination with deep neural network was proposed. 3.2. Deep Q-learning (DQN) Utilization of neural nets to model either a policy or a Q-function frees from constructing taskspeci\ufb01c features and opens possibilities of applying RL algorithms to complex tasks, e. g. tasks with images as input. Video games are classical example of such tasks where raw pixels of screen are provided as state representation and, correspondingly, as input to either policy or Q-function. Main idea of Deep Q-learning [14] is to adapt Temporal Di\ufb00erence algorithm so that update formula (9) would be equivalent to gradient descent step for training a neural network to solve a certain regression task. Indeed, it can be noticed that the exponential smoothing parameter \u03b1t resembles learning rate of \ufb01rst-order gradient optimization procedures, while the exploration conditions from theorem 7 look identical to restrictions on learning rate of stochastic gradient descent. The key hint is that (9) is actually a gradient descent step in the parameter space of the table functions family: Q\u2217(s, a, \u03b8) = \u03b8s,a where all \u03b8s,a form a vector of parameters \u03b8 \u2208 R|S||A|. To unravel this fact, it is convenient to introduce some notation from regression tasks. First, let\u2019s denote by y the target of our regression task, i. e. the quantity that our model is trying to predict: y(s, a) := r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8) (10) where s\u2032 is a sample from p(s\u2032 | s, a) and s, a is input data. In this notation (9) is equivalent to: \u03b8t+1 = \u03b8t + \u03b1t [y(s, a) \u2212 Q\u2217(s, a, \u03b8t)] es,a where we multiplied scalar value \u03b1t [y(s, a) \u2212 Q\u2217(s, a, \u03b8t)] on the following vector es,a es,a i,j := \ufffd 1 (i, j) = (s, a) 0 (i, j) \u0338= (s, a) to formulate an update of only one component of \u03b8 in a vector form. By this we transitioned to update in parameter space using Q\u2217(s, a, \u03b8) = \u03b8s,a. Remark that for table functions family the 12 derivative of Q\u2217(s, a, \u03b8) by \u03b8 for given input s, a is its one-hot encoding, i. e. exactly es,a: \u2202Q\u2217(s, a, \u03b8) \u2202\u03b8 = es,a (11) The statement now is that this formula is a gradient descent update for regression with input s, a, target y(s, a) and MSE loss function: Loss(y(s, a), Q\u2217(s, a, \u03b8t)) = (Q\u2217(s, a, \u03b8t) \u2212 y(s, a))2 (12) Indeed: \u03b8t+1 = \u03b8t + \u03b1t [y(s, a) \u2212 Q\u2217(s, a, \u03b8t)] es,a = {(12)} = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202Q\u2217 es,a {(11)} = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202Q\u2217 \u2202Q\u2217(s, a, \u03b8t) \u2202\u03b8 = {chain rule} = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202\u03b8 The obtained result is evidently a gradient descent step formula to minimize MSE loss function with target (10): \u03b8t+1 = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202\u03b8 (13) It is important that dependence of y from \u03b8 is ignored during gradient computation (otherwise the chain rule application with y being dependent on \u03b8 is incorrect). On each step of temporal difference algorithm new target y is constructed using current Q-function approximation, and a new regression task with this target is set. For this \ufb01xed target one MSE optimization step is done according to (13), and on the next step a new regression task is de\ufb01ned. Though during each step the target is considered to represent some ground truth like it is in supervised learning, here it provides a direction of optimization and because of this reason is sometimes called a guess. Notice that representation (13) is equivalent to standard TD update (9) with all theoretical results remaining while the parametric family Q(s, a, \u03b8) is a table functions family. At the same time, (13) can be formally applied to any parametric function family including neural networks. It must be taken into account that this transition is not rigorous and all theoretical guarantees provided by theorem 7 are lost at this moment. Further on we assume that optimal Q-function is approximated with neural network Q\u2217 \u03b8(s, a) with parameters \u03b8. Note that for discrete action space case this network may take only s as input and output |A| numbers representing Q\u2217 \u03b8(s, a1) . . . Q\u2217 \u03b8(s, a|A|), which allows to \ufb01nd an optimal action in a given state s with a single forward pass through the net. Therefore target y for given transition (s, a, r\u2032, s\u2032, done) can be computed with one forward pass and optimization step can be performed in one more forward7 and one backward pass. Small issue with this straightforward approach is that, of course, it is impractical to train neural networks with batches of size 1. In [14] it is proposed to use experience replay to store all collected transitions (s, a, r\u2032, s\u2032, done) as data samples and on each iteration sample a batch of standard for neural networks training size. As usual, the loss function is assumed to be an average of losses for each transition from the batch. This utilization of previously experienced transitions is legit because TD algorithm is known to be an o\ufb00-policy algorithm, which means it can work with arbitrary transitions gathered by any agent\u2019s interaction experience. One more important bene\ufb01t from experience replay is sample decorrelation as consecutive transitions from interaction are often similar to each other since agent usually locates at the particular part of MDP. Though empirical results of described algorithm turned out to be promising, the behaviour of Q\u2217 \u03b8 values indicated the instability of learning process. Reconstruction of target after each optimization step led to so-called compound error when approximation error propagated from the closeto-terminal states to the starting in avalanche manner and could lead to guess being 106 and more times bigger than the true Q\u2217 value. To address this problem, [14] introduced a kludge known as target network, which basic idea is to solve \ufb01xed regression problem for K > 1 steps, i. .e. recompute target every K-th step instead of each. 7in implementations it is possible to combine s and s\u2032 in one batch and perform these two forward passes \u00abat once\u00bb. 13 To avoid target recomputation for the whole experience replay, the copy of neural network Q\u2217 \u03b8 is stored, called the target network. Its architecture is the same while weights \u03b8\u2212 are a copy of Q\u2217 \u03b8 from the moment of last target recomputation8 and its main purpose is to generate targets y for given current batch. Combining all things together and adding \u03b5-greedy strategy to facilitate exploration, we obtain classic DQN algorithm: Algorithm 2: Deep Q-learning (DQN) Hyperparameters: B \u2014 batch size, K \u2014 target network update frequency, \u03b5(t) \u2208 (0, 1] \u2014 greedy exploration parameter, Q\u2217 \u03b8 \u2014 neural network, SGD optimizer. Initialize weights of \u03b8 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 On each interaction step: 1. select a randomly with probability \u03b5(t), else a = argmax a Q\u2217 \u03b8(s, a) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. add observed transition to experience replay 4. sample batch of size B from experience replay 5. for each transition T from the batch compute target: y(T ) = r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212) 6. compute loss: Loss = 1 B \ufffd T (Q\u2217(s, a, \u03b8) \u2212 y(T ))2 7. make a step of gradient descent using \u2202 Loss \u2202\u03b8 8. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 3.3. Double DQN Although target network successfully prevented Q\u2217 \u03b8 from unbounded growth and empirically stabilized learning process, the values of Q\u2217 \u03b8 on many domains were evident to tend to overestimation. The problem is presumed to reside in max operation in target construction formula (10): y = r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212) During this estimation max shifts Q-value estimation towards either to those actions that led to high reward due to luck or to the actions with overestimating approximation error. The solution proposed in [25] is based on idea of separating action selection and action evaluation to carry out each of these operations using its own approximation of Q\u2217: max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212) = Q\u2217(s\u2032, argmax a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212), \u03b8\u2212) \u2248 \u2248 Q\u2217(s\u2032, argmax a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212 1 ), \u03b8\u2212 2 ) The simplest, but expensive, implementation of this idea is to run two independent DQN (\u00abTwin DQN\u00bb) algorithms and use the twin network to evaluate actions: y1 = r(s\u2032) + \u03b3Q\u2217 1(s\u2032, argmax a\u2032 Q\u2217 2(s\u2032, a\u2032, \u03b8\u2212 2 ), \u03b8\u2212 1 ) 8alternative, but more computationally expensive option, is to update target network weights on each step using exponential smoothing 14 y2 = r(s\u2032) + \u03b3Q\u2217 2(s\u2032, argmax a\u2032 Q\u2217 1(s\u2032, a\u2032, \u03b8\u2212 1 ), \u03b8\u2212 2 ) Intuitively, each Q-function here may prefer lucky or overestimated actions, but the other Q-function judges them according to its own luck and approximation error, which may be as underestimating as overestimating. Ideally these two DQNs should not share interaction experience to achieve that, which makes such algorithm twice as expensive both in terms of computational cost and sample e\ufb03ciency. Double DQN [25] is more compromised option which suggests to use current weights of network \u03b8 for action selection and target network weights \u03b8\u2212 for action evaluation, assuming that when the target network update frequency K is big enough these two networks are su\ufb03ciently di\ufb00erent: y = r(s\u2032) + \u03b3Q\u2217(s\u2032, argmax a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8), \u03b8\u2212) 3.4. Dueling DQN Another issue with DQN algorithm 2 emerges when a huge part of considered MDP consists of states of low optimal value V \u2217(s), which is an often case. The problem is that when the agent visits unpromising state instead of lowering its value V \u2217(s) it remembers only low pay-o\ufb00 for performing some action a in it by updating Q\u2217(s, a). This leads to regular returns to this state during future interactions until all actions prove to be unpromising and all Q\u2217(s, a) are updated. The problem gets worse when the cardinality of action space is high or there are many similar actions in action space. One bene\ufb01t of deep reinforcement learning is that we are able to facilitate generalization across actions by specifying the architecture of neural network. To do so, we need to encourage the learning of V \u2217(s) from updates of Q\u2217(s, a). The idea of dueling architecture [27] is to incorporate approximation of V \u2217(s) explicitly in computational graph. For that purpose we need the de\ufb01nition of advantage function: De\ufb01nition 8. For given MDP and policy \u03c0 the advantage function under policy \u03c0 is de\ufb01ned as A\u03c0(s, a) := Q\u03c0(s, a) \u2212 V \u03c0(s) (14) Advantage function is evidently interconnected with Q-function and value function and actually shows the relative advantage of selecting action a comparing to average performance of the policy. If for some state A\u03c0(s, a) > 0, then modifying \u03c0 to select a more often in this particular state will lead to better policy as its average return will become bigger than initial V \u03c0(s). This follows from the following property of arbitrary advantage function: Ea\u223c\u03c0(a|s)A\u03c0(s, a) = Ea\u223c\u03c0(a|s) [Q\u03c0(s, a) \u2212 V \u03c0(s)] = = Ea\u223c\u03c0(a|s)Q\u03c0(s, a) \u2212 V \u03c0(s) = {using (4)} = V \u03c0(s) \u2212 V \u03c0(s) = 0 (15) De\ufb01nition of optimal advantage function A\u2217(s, a) is analogous and allows us to reformulate Q\u2217(s, a) in terms of V \u2217(s) and A\u2217(s, a): Q\u2217(s, a) = V \u2217(s) + A\u2217(s, a) (16) Straightforward utilization of this decomposition is following: after several feature extracting layers the network is joined with two heads, one outputting single scalar V \u2217(s) and one outputting |A| numbers A\u2217(s, a) like it was done in DQN for Q-function. After that this scalar value estimation is added to all components of A\u2217(s, a) in order to obtain Q\u2217(s, a) according to (16). The problem with this naive approach is that due to (15) advantage function can not be arbitrary and must hold the property (15) for Q\u2217(s, a) to be identi\ufb01able. This restriction (15) on advantage function can be simpli\ufb01ed for the case when optimal policy is 15 induced by optimal Q-function: 0 = Ea\u223c\u03c0\u2217(a|s)Q\u2217(s, a) \u2212 V \u2217(s) = = Q\u2217(s, argmax a Q\u2217(s, a)) \u2212 V \u2217(s) = = max a Q\u2217(s, a) \u2212 V \u2217(s) = = max a [Q\u2217(s, a) \u2212 V \u2217(s)] = = max a A\u2217(s, a) This condition can be easily satis\ufb01ed in computational graph by subtracting max a A\u2217(s, a) from advantage head. This will be equivalent to the following formula of dueling DQN: Q\u2217(s, a) = V \u2217(s) + A\u2217(s, a) \u2212 max a A\u2217(s, a) (17) The interesting nuance of this improvement is that after evaluation on Atari-57 authors discovered that substituting max operation in (17) with averaging across actions led to better results (while usage of unidenti\ufb01able formula (16) led to poor performance). Although gradients can be backpropagated through both operation and formula (17) seems theoretically justi\ufb01ed, in practical implementations averaging instead of maximum is widespread. 3.5. Noisy DQN By default, DQN algorithm does not concern the exploration problem and is always augmented with \u03b5-greedy strategy to force agent to discover new states. This baseline exploration strategy su\ufb00ers from being extremely hyperparameter-sensitive as early decrease of \u03b5(t) to close to zero values may lead to stucking in local optima, when agent is unable to explore new options due to imperfect Q\u2217, while high values of \u03b5(t) force agent to behave randomly for excessive amount of episodes, which slows down learning. In other words, \u03b5-greedy strategy transfers responsibility to solve exploration-exploitation trade-o\ufb00 on engineer. The key reason why \u03b5-greedy exploration strategy is relatively primitive is that exploration priority does not depend on current state. Intuitively, the choice whether to exploit knowledge by selecting approximately optimal action or to explore MDP by selecting some other depends on how explored the current state s is. Discovering a new part of state space after any amount of interaction probably indicates that random actions are good to try there, while close-to-initial states will probably be su\ufb03ciently explored after several \ufb01rst episodes. In \u03b5-greedy strategy agent selects action using deterministic Q\u2217(s, a, \u03b8) and only afterwards injects state-independent noise in a form of \u03b5(t) probability of choosing random action. Noisy networks [4] were proposed as a simple extension of DQN to provide state-dependent and parameterfree exploration by injecting noise of trainable volume to all (or most9) nodes in computational graph. Let a linear layer with m inputs and n outputs in q-network perform the following computation: y(x) = W x + b where x \u2208 Rm is input, W \u2208 Rn\u00d7m \u2014 weights matrix, b \u2208 Rm \u2014 bias. In noisy layers it is proposed to substitute deterministic parameters with samples from N (\u00b5, \u03c3) where \u00b5, \u03c3 are trained with gradient descent10. On the forward pass through the noisy layer we sample \u03b5W \u223c N (0, Inm\u00d7nm), \u03b5b \u223c N (0, In\u00d7n) and then compute W = (\u00b5W + \u03c3W \u2299 \u03b5W ) b = (\u00b5b + \u03c3b \u2299 \u03b5b) y(x) = W x + b where \u2299 denotes element-wise multiplication, \u00b5W , \u03c3W \u2208 Rn\u00d7m, \u00b5b, \u03c3b \u2208 Rn \u2014 trainable parameters of the layer. Note that the number of parameters for such layers is doubled comparing to ordinary layers. 9usually it is not injected in very \ufb01rst layers responsible for feature extraction like convolutional layers in networks for images as input. 10using standard reparametrization trick 16 As the output of q-network now becomes a random variable, loss value becomes a random variable too. Like in similar models for supervised learning, on each step an expectation of loss function over noise is minimized: E\u03b5 Loss(\u03b8, \u03b5) \u2192 min \u03b8 The gradient in this setting can be estimated using Monte-Carlo: \u2207\u03b8E\u03b5 Loss(\u03b8, \u03b5) = E\u03b5\u2207\u03b8 Loss(\u03b8, \u03b5) \u2248 \u2207\u03b8 Loss(\u03b8, \u03b5) \u03b5 \u223c N (0, I) It can be seen that amount of noise actually in\ufb02icting output of network may vary for di\ufb00erent inputs, i. e. for di\ufb00erent states. There are no guarantees that this amount will reduce as the interaction proceeds; the behaviour of average magnitude of noise injected in the network with time is reported to be extremely sensitive to initialization of \u03c3W , \u03c3b and vary from MDP to MDP. One technical issue with noisy layers is that on each pass an excessive amount (by the number of network parameters) of noise samples is required. This may substantially reduce computational e\ufb03ciency of forward pass through the network. For optimization purposes it is proposed to obtain noise for weights matrices in the following way: sample just n + m noise samples \u03b51 W \u223c N (0, Im\u00d7m), \u03b52 W \u223c N (0, In\u00d7n) and acquire matrix noise in a factorized form: \u03b5W = f(\u03b51 W )f(\u03b52 W )T where f is a scaling function, e. g. f(x) = sign(x) \ufffd |x|. The bene\ufb01t of this procedure is that it requires m + n samples instead of mn, but sacri\ufb01ces the interlayer independence of noise. 3.6. Prioritized experience replay In DQN each batch of transitions is sampled from experience replay using uniform distribution, treating collected data as equally prioritized. In such scheme states for each update come from the same distribution as they come from interaction experience (except that they become decorellated), which agrees with TD algorithm as the basement of DQN. Intuitively observed transitions vary in their importance. At the beginning of training most guesses tend to be more or less random as they rely on arbitrarily initialized Q\u2217 \u03b8 and the only source of trusted information are transitions with non-zero received reward, especially near terminal states where V \u2217 \u03b8 (s\u2032) is known to be equal to 0. In the midway of training, most of experience replay is \ufb01lled with the memory of interaction within well-learned part of MDP while the most crucial information is contained in transitions where agent explored new promising areas and gained novel reward yet to be propagated through Bellman equation. All these signi\ufb01cant transitions are drowned in collected data and rarely appear in sampled batches. The central idea of prioritized experience replay [18] is that priority of some transition T = (s, a, r\u2032, s\u2032, done) is proportional to temporal di\ufb00erence: \u03c1(T ) := y(T ) \u2212 Q\u2217(s, a, \u03b8) = \ufffd Loss(y(T ), Q\u2217(s, a, \u03b8)) (18) Using these priorities as proxy of transition importances, sampling from experience replay proceeds using following probabilities: P(T ) \u221d \u03c1(T )\u03b1 where hyperparameter \u03b1 \u2208 R+ controls the degree to which the sampling weights are sparsi\ufb01ed: the case \u03b1 = 0 corresponds to uniform sampling distribution while \u03b1 = +\u221e is equivalent to greedy sampling of transitions with highest priority. The problem with (18) claim is that each transition\u2019s priority changes after each network update. As it is impractical to recalculate loss for the whole data after each step, some simpli\ufb01cations must be put up with. The straightforward option is to update priority only for sampled transitions in the current batch. New transitions can be added to experience replay with highest priority, i. e. max T \u03c1(T )11. Second debatable issue of prioritized replay is that it actually substitutes loss function of DQN updates, which assumed uniform sampling of visited states to ensure they come from state visitation distribution: ET \u223cUniform Loss(T ) \u2192 min \u03b8 11which can be computed online with O(1) complexity 17 While it is not clear what distribution is better to sample from to ensure exploration restrictions of theorem 7, prioritized experienced replay changes this distribution in uncontrollable way. Despite its fruitfulness at the beginning and midway of training process, this distribution shift may destabilize learning close to the end and make algorithm stuck with locally optimal policy. Since formally this issue is about estimating an expectation over one probability with preference to sample from another one, the standard technique called importance sampling can be used as countermeasure: ET \u223cUniform Loss(T ) = M \ufffd i=0 1 M Loss(Ti) = = M \ufffd i=0 P(Ti) 1 MP(Ti) Loss(Ti) = = ET \u223cP(T ) 1 MP(T ) Loss(T ) where M is a number of transitions stored in experience replay memory. Importance sampling implies that we can avoid distribution shift that introduces undesired bias by making smaller gradient updates for signi\ufb01cant transitions which now appear in the batches with higher frequency. The price for bias elimination is that importance sampling weights lower prioritization e\ufb00ect by slowing down learning of highlighted new information. This duality resembles trade-o\ufb00 between bias and variance, but important moment here is that distribution shift does not cause any seeming issues at the beginning of training when agent behaves close to random and do not produce valid state visitation distribution anyway. The idea proposed in [18] based on this intuition is to anneal the importance sampling weights so they correct bias properly only towards the end of training procedure. LossprioritizedER = ET \u223cP(T ) \ufffd 1 BP(T ) \ufffd\u03b2(t) Loss(T ) where \u03b2(t) \u2208 [0, 1] and approaches 112 as more interaction steps are executed. If \u03b2(t) is set to 0, no bias correction is held, while \u03b2(t) = 1 corresponds to unbiased loss function, i. e. equivalent to sampling from uniform distribution. The most signi\ufb01cant and obvious drawback of prioritized experience replay approach is that it introduces additional hyperparameters. Although \u03b1 represents one number, algorithm\u2019s behaviour may turn out to be sensitive to its choosing, and \u03b2(t) must be designed by engineer as some scheduled motion from something near 0 to 1, and its well-turned selection may require inaccessible knowledge about how many steps it will take for algorithm to \u00abwarm up\u00bb. 3.7. Multi-step DQN One more widespread modi\ufb01cation of Q-learning in RL community is substituting one-step approximation present in Bellman optimality equation (6) with N-step: Proposition 8. (N-step Bellman optimality equation) Q\u2217(s0, a0) = ET\u03c0\u2217|s0,a0 \ufffd N \ufffd t=1 \u03b3t\u22121r(st) + \u03b3N max aN Q\u2217(sN, aN) \ufffd (19) Indeed, de\ufb01nition of Q\u2217(s, a) consists of average return and can be viewed as making T max steps from state s0 after selecting action a0, while vanilla Bellman optimality equation represents Q\u2217(s, a) as reward from one next step in the environment and estimation of the rest of trajectory reward recursively. N-step Bellman equation (19) generalizes these two opposites. All the same reasoning as for DQN can be applied to N-step Bellman equation to obtain N-step DQN algorithm, which only modi\ufb01cation appears in target computation: y(s0, a0) = N \ufffd t=1 \u03b3t\u22121r(st) + \u03b3N max aN Q\u2217(sN, aN, \u03b8) (20) 12often it is initialized by a constant close to 0 and is linearly increased until it reaches 1 18 To perform this computation, we are required to obtain for given state s and a not only one next step, but N steps. To do so, instead of transitions N-step roll-outs are stored, which can be done by precomputing following tuples: T = \ufffd s, a, N \ufffd n=1 \u03b3n\u22121r(n), s(N), done \ufffd where r(n) is the reward received in n steps after visitation of considered state s, s(N) is state visited in N steps, and done is a \ufb02ag whether the episode ended during N-step roll-out13. All other aspects of algorithm remain the same in practical implementations, and the case N = 1 corresponds to standard DQN. The goal of using N > 1 is to accelerate propagation of reward from terminal states backwards through visited states to s0 as less update steps will be required to take into account freshly observed reward and optimize behaviour at the beginning of episodes. The price is that formula (20) includes an important trick: to calculate such target, for second (and following) step action a\u2032 must be sampled from \u03c0\u2217 for Bellman equation (19) to remain true. In other words, application of N-step Q-learning is theoretically improper when behaviour policy di\ufb00ers from \u03c0\u2217. Note that we do not face this problem in the case N = 1 in which we are required to sample only from transition probability p(s\u2032 | s, a) for given state-action pair s, a. Even considering \u03c0\u2217 \u2248 argmax a Q\u2217(s, a, \u03b8), where Q\u2217 is our current approximation of \u03c0\u2217, makes N-step DQN an on-policy algorithm when for every state-action pair s, a it is preferable to sample target using the closest approximation of \u03c0\u2217 available. This questions usage of experience replay or at the very least encourages to limit its capacity to store only M max newest transitions with M max being relatively not very big. To see the negative e\ufb00ect of N-step DQN, consider the following toy example. Suppose agent makes a mistake on the second step after s and ends episode with huge negative reward. Then in the case N > 2 each time the roll-out starting with this s is sampled in the batch, the value of Q\u2217(s, a, \u03b8) will be updated with this received negative reward even if Q\u2217(s\u2032, \u00b7, \u03b8) already learned not to repeat this mistake again. Yet empirical results in many domains demonstrate that raising N from 1 to 2-3 may result in substantial acceleration of training and positively a\ufb00ect the \ufb01nal performance. On the contrary, the theoretical groundlessness of this approach explains its negative e\ufb00ects when N is set too big. 13all N-step roll-outs must be considered including those terminated at k-th step for k < N. 19 4. Distributional approach for value-based methods 4.1. Theoretical foundations The setting of RL task inherently carries internal stochasticity of which agent has no substantial control. Sometimes intelligent behaviour implies taking risks with severe chance of low episode return. All this information resides in the distribution of return R (1) as random variable. While value-based methods aim at learning expectation of this random variable as it is the quantity we actually care about, in distributional approach [1] it is proposed to learn the whole distribution of returns. It further extends the information gathered by algorithm about MDP towards model-based case in which the whole MDP is imitated by learning both reward function r(s) and transitions T, but still restricts itself only to reward and doesn\u2019t intend to learn world model. In this section we discuss some theoretical extensions of temporal di\ufb00erence ideas in the case when expectations on both sides of Bellman equation (5) and Bellman optimality equation (6) are taken away. The central object of study in Q-learning was Q-function, which for given state and action returns the expectation of reward. To rewrite Bellman equation not in terms of expectations, but in terms of the whole distributions, we require a corresponding notation. De\ufb01nition 9. For given MDP and policy \u03c0 the value distribution of policy \u03c0 is a random variable de\ufb01ned as Z\u03c0(s, a) := \ufffd t=0 \u03b3trt+1 \ufffd\ufffd\ufffd s0 = s, a0 = a Note that Z\u03c0 just represents a random variable which is taken expectation of in de\ufb01nition of Q-function: Q\u03c0(s, a) = ET\u03c0Z\u03c0(s, a) Using this de\ufb01nition of value distribution, Bellman equation can be rewritten to extend the recursive connection between adjacent states from expectations of returns to the whole distributions of returns: Proposition 9. (Distributional Bellman Equation) [1] Z\u03c0(s, a) c.d.f. = r(s\u2032) + \u03b3Z\u03c0(s\u2032, a\u2032) \ufffd\ufffd s\u2032 \u223c p(s\u2032 | s, a), a\u2032 \u223c \u03c0(a\u2032 | s\u2032) (21) Here we used some auxiliary notation: by c.d.f. = we mean that cumulative distribution functions of two random variables to the right and left are equal almost everywhere. Such equations are called recursive distributional equations and are well-known in theoretical probability theory14. By using | we describe a sampling procedure for the random variable to the right side of equation: for given s, a next state s\u2032 is sampled from transition probability, then a\u2032 is sampled from given policy, then random variable Z\u03c0(s\u2032, a\u2032) is sampled to calculate a resulting sample r(s\u2032) + \u03b3Z\u03c0(s\u2032, a\u2032). While the space of Q-functions Q\u03c0(s, a) \u2208 S \u00d7 A \u2192 R is \ufb01nite, the space of value distributions is a space of mappings from state-action pair to continuous distributions: Z\u03c0(s, a) \u2208 S \u00d7 A \u2192 P(R) and it is important to notice that even in the table-case when state and action spaces are \ufb01nite, the space of value distributions is essentially in\ufb01nite. Crucial moment for us will be that convergence properties now depend on chosen metric15. The choice of metric in S \u00d7 A \u2192 P(R) represents the same issue as in the space of continuous random variables P(R): if we choose a metric in the latter, we can construct one in the former: 14to get familiar with this notion, consider this basic example: X1 c.d.f. = X2 \u221a 2 + X3 \u221a 2 where X1, X2, X3 are random variables coming from N (0, \u03c32). 15in \ufb01nite spaces it is true that convergence in one metric guarantees convergence to the same point for any other metric. 20 Proposition 10. If d(X, Y ) is a metric in the space P(R), then d(Z1, Z2) := sup s\u2208S,a\u2208A d(Z1(s, a), Z2(s, a)) is a metric in the space S \u00d7 A \u2192 P(R). The particularly interesting for us example of metric in P(R) will be Wasserstein metric, which concerns only random variables with bounded moments, so we will additionally assume that for all state-action pairs s, a EZ\u03c0(s, a)p \u2264 +\u221e are \ufb01nite for p \u2265 1. Proposition 11. For 1 \u2264 p \u2264 +\u221e for two random variables X, Y on continuous domain with pth bounded moments and cumulative distribution functions FX and FY correspondingly a Wasserstein distance Wp(X, Y ) := \uf8eb \uf8ed 1 \ufffd 0 \ufffd\ufffd\ufffdF \u22121 X (\u03c9) \u2212 F \u22121 Y (\u03c9) \ufffd\ufffd\ufffd p d\u03c9 \uf8f6 \uf8f8 1 p W\u221e(X, Y ) := sup \u03c9\u2208[0,1] \ufffd\ufffd\ufffdF \u22121 X (\u03c9) \u2212 F \u22121 Y (\u03c9) \ufffd\ufffd\ufffd is a metric in the space of random variables with p-th bounded moments. Thus we can conclude from proposition 10 that maximal form of Wasserstein metric W p(Z1, Z2) = sup s\u2208S,a\u2208A Wp(Z1(s, a), Z2(s, a)) (22) is a metric in the space of value distributions. We now concern convergence properties of point iteration method to solve (21) in order to obtain Z\u03c0 for given policy \u03c0, i. e. solve the task of policy evaluation. For that purpose we initialize Z\u03c0 0 (s, a) arbitrarily16 and perform the following updates for all state-action pairs s, a: Z\u03c0 t+1(s, a) c.d.f. := r(s\u2032) + \u03b3Z\u03c0 t (s\u2032, a\u2032) (23) Here we assume that we are able to compute the distribution of random variable on the right side knowing \u03c0, all transition probabilities T, the distribution of Z\u03c0 t and reward function. The question whether the sequence {Z\u03c0 t } converges to Z\u03c0 can be given a detailed answer: Proposition 12. [1] Denote by B the following operator (S \u00d7 A \u2192 P(R)) \u2192 (S \u00d7 A \u2192 P(R)), updating Z\u03c0 t as in (23): Z\u03c0 t+1 = BZ\u03c0 t for all state-action pairs s, a. Then B is a contraction mapping in W p (22) for 1 \u2264 p \u2264 +\u221e, i.e. for any two value distributions Z1, Z2 W p(BZ1, BZ2) \u2264 \u03b3W p(Z1, Z2) Hence there is a unique \ufb01xed point of system of equations (21) and the point iteration method converges to it. One more curious theoretical result is that B is in general not a contraction mapping for such distances as Kullback-Leibler divergence, Total Variation distance and Kolmogorov distance17. It shows 16here we consider value distributions from theoretical point of view, assuming that we are able to explicitly store a table of |S||A| continuous distributions without any approximations. 17one more metric for which the contraction property was shown is Cramer metric: l2(X, Y ) = \uf8eb \uf8ed \ufffd R (FX(\u03c9) \u2212 FY (\u03c9))2 d\u03c9 \uf8f6 \uf8f8 1 2 where FX, FY are c.d.f. of random variables X, Y correspondingly. 21 that metric selection indeed in\ufb02uences convergence rate. Similar to traditional value functions, we can de\ufb01ne optimal value distribution Z\u2217(s, a). Substituting18 \u03c0\u2217(s) = argmax a ET\u03c0\u2217 Z\u2217(s, a) into (21), we obtain distributional Bellman optimality equation: Proposition 13. (Distributional Bellman optimality equation) Z\u2217(s, a) c.d.f. = r(s\u2032) + \u03b3Z\u2217(s\u2032, argmax a\u2032 ET\u03c0\u2217 Z\u2217(s\u2032, a\u2032)) \ufffd\ufffd s\u2032 \u223c p(s\u2032 | s, a) (24) Now we concern the same question whether the point iteration method of solving (24) leads to solution Z\u2217 and whether it is a contraction mapping for some metric. The answer turns out to be negative. Proposition 14. [1] Point iteration for solving (24) may diverge. Level of impact of this result is not completely clear. Point iteration for (24) preserves means of distributions, i. e. it will eventually converge to Q\u2217(s, a) with all theoretical guarantees from classical Q-learning. The reason behind divergence theorems hides in the rest of distributions like other moments and situations when equivalent (in terms of average return) actions may lead to di\ufb00erent higher moments. 4.2. Categorical DQN There are obvious obstacles for practical application of distributional Q-learning following from complication of working with arbitrary continuous distributions. Usually we are restricted to approximations inside some family of parametric distributions, so we have to perform a projection step on each iteration. Second matter in combining distributional Q-learning with deep neural networks is to take into account that only samples from p(s\u2032 | s, a) are available for each update. To provide a distributional analog of temporal di\ufb00erence algorithm 9, some analog of exponential smoothing for distributional setting must be proposed. Categorical DQN [1] (also referred as c51) provides straightforward design of practical distributional algorithm. While DQN was a resemblance of temporal di\ufb00erence algorithm, Categorical DQN attempts to follow the logic of DQN. The concept is as following. The neural network with parameters \u03b8 in this setting takes as input s \u2208 S and for each action a outputs parameters \u03b6\u03b8(s, a) of distributions of random variable Z\u2217 \u03b8(s, a). As in DQN, experience replay can be used to collect observed transitions and sample a batch for each update step. For each transition T = (s, a, r\u2032, s\u2032, done) in the batch a guess is computed: y(T ) c.d.f. := r\u2032 + (1 \u2212 done)\u03b3Z\u2217 \u03b8 \ufffd s\u2032, argmax a\u2032 EZ\u2217 \u03b8(s\u2032, a\u2032) \ufffd (25) Note that expectation of Z\u2217 \u03b8(s\u2032, a\u2032) is computed explicitly using the form of chosen parametric family of distributions and outputted parameters \u03b6\u03b8(s\u2032, a\u2032), as is the distribution of random variable r\u2032 + (1 \u2212 done)\u03b3Z\u2217 \u03b8(s\u2032, a\u2032). In other words, in this setting guess y(T ) is also a continuous random variable, distribution of which can be constructed only approximately. As both target and model output are distributions, it is reasonable to design loss function in a form of some divergence D between y(T ) and Z\u2217 \u03b8(s, a): Loss(\u03b8) = ET D \ufffd y(T ) \u2225 Z\u2217 \u03b8(s, a) \ufffd (26) \u03b8t+1 = \u03b8t \u2212 \u03b1\u2202 Loss(\u03b8t) \u2202\u03b8 18to perform this step validly, a clari\ufb01cation concerning argmax operator de\ufb01nition must be given. The choice of action a returned by this operator in the cases when several actions lead to the same maximal average returns must not depend on Z, as this choice a\ufb00ects higher moments of resulted distribution. To overcome this issue, for example, in the case of \ufb01nite action space all actions can be enumerated and the optimal action with the lowest index is returned by operator. 22 The particular choice of this divergence must be made with concern that y(T ) is a \u00absample\u00bb from a full one-step approximation of Z\u2217 \u03b8 which includes transition probabilities: yfull(s, a) c.d.f. := \ufffd s\u2032\u2208S p(s\u2032 | s, a)y(s, a, r(s\u2032), s\u2032, done(s\u2032)) (27) This form is precisely the right side of distributional Bellman optimality equation as we just incorporated intermediate sampling of s\u2032 into the value of random variable. In other words, if transition probabilities T were known, the update could be made using distribution of yfull as a target. Lossfull(\u03b8) = Es,aD(yfull(s, a) \u2225 Z\u2217 \u03b8(s, a)) This motivates to choose KL(y(T ) \u2225 Z\u2217 \u03b8(s, a)) (speci\ufb01cally with this order of arguments) as D to exploit the following property (we denote by pX a p.d.f. pf random variable X): \u2207\u03b8ET KL(yfull(s, a) \u2225 Z\u2217 \u03b8(s, a)) = \u2207\u03b8 \ufffd ET \ufffd R \u2212pyfull(s,a)(\u03c9) log pZ\u2217 \u03b8 (s,a))(\u03c9)d\u03c9 + const(\u03b8) \ufffd = {using (27)} = \u2207\u03b8ET \ufffd R Es\u2032\u223cp(s\u2032|s,a) \u2212 py(T )(\u03c9) log pZ\u2217 \u03b8 (s,a))(\u03c9)d\u03c9 = {taking expectation out} = \u2207\u03b8ET Es\u2032\u223cp(s\u2032|s,a) \ufffd R \u2212py(T )(\u03c9) log pZ\u2217 \u03b8 (s,a))(\u03c9)d\u03c9 = = \u2207\u03b8ET Es\u2032\u223cp(s\u2032|s,a) KL \ufffd y(T ) \u2225 Z\u2217 \u03b8(s, a) \ufffd This property basically states that gradient of loss function (26) with KL as D is an unbiased (Monte-Carlo) estimation of gradient of KL-divergence for \u00abfull\u00bb distribution (27), which resembles the employment of exponential smoothing in temporal di\ufb00erence learning. For many other divergences, including Wasserstein metric, same statement is not true, so their utilization in described online setting will lead to biased gradients and all theory-grounded intuition that algorithm moves in the right direction becomes distinctively lost. Moreover, KL-divergence is known to be one of the easiest divergences to work with due to its nice smoothness properties and wide prevalence in many deep learning pipelines. Described above motivation to choose KL-divergence as an actual objective for minimization is contradictory. Theoretical analysis of distributional Q-learning, speci\ufb01cally theorem 12, though concerning policy evaluation other than optimal Z\u2217 search, explicitly hints that the process converges exponentially fast for Wasserstein metric, while even for precisely made updates in terms of KLdivergence we are not guaranteed to get any closer to true solution. More \u00abpractical\u00bb defect of KL-divergence is that it demands two comparable distributions to share the same domain. This means that by choosing KL-divergence we pledge to guarantee that y(T ) and Z\u2217 \u03b8(s, a) in (26) have coinciding support. This emerging restriction seems limiting even beforehand as for episodic MDP value distribution in terminal states is obviously degenerated (their support consists of one point r(s) which is given all probability mass) which means that our value distribution approximation is basically ensured to never be precise. In Categorical DQN, as follows from the name, the family of distributions is chosen to be categorical on the \ufb01xed support {z0, z1 . . . zA\u22121} where A is number of atoms. As no prior information about MDP is given, the basic choice of this support is uniform grid from some Vmin \u2208 R to V max \u2208 R: zi = Vmin + i A \u2212 1(Vmax \u2212 Vmin), i \u2208 0, 1, . . . A \u2212 1 These bounds, though, must be chosen carefully as they implicitly assume Vmin \u2264 Z\u2217(s, a) \u2264 Vmax and if these inequalities are not tight, the approximation will obviously become poor. Therefore the neural network outputs A numbers, summing into 1, to represent arbitrary distribution on this support: \u03b6i(s, a, \u03b8) := P(Z\u2217 \u03b8(s, a) = zi) Within this family of distributions, computation of expectation, greedy action selection and KLdivergence is trivial. One problem hides in target formula (25): while we can compute distribution y(T ), its support may in general di\ufb00er from {z0 . . . zA\u22121}. To avoid the issue of disjoint supports, 23 a projection step must be done to \ufb01nd the closest to target distribution within the chosen family19. Therefore the resulting target used in the loss function is y(T ) c.d.f. := \u03a0C \ufffd r\u2032 + (1 \u2212 done)\u03b3Z\u2217 \u03b8 \ufffd s\u2032, argmax a\u2032 EZ\u2217 \u03b8(s\u2032, a\u2032) \ufffd\ufffd where \u03a0C is projection operator. The resulting practical algorithm, named c51 after categorical distributions with A = 51 atoms, inherits ideas of experience replay, \u03b5-greedy exploration and target network from DQN. Empirically, though, usage of target network remains an open question as the chosen family of distributions restricts value approximation from unbounded growth by \u00abclipping\u00bb predictions at zA\u22121 and z0, yet it is still considered slightly improving performance. Algorithm 3: Categorical DQN (c51) Hyperparameters: B \u2014 batch size, Vmax, Vmin, A \u2014 parameters of support, K \u2014 target network update frequency, \u03b5(t) \u2208 (0, 1] \u2014 greedy exploration parameter, \u03b6\u2217 \u2014 neural network, SGD optimizer. Initialize weights \u03b8 of neural net \u03b6\u2217 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 Precompute support grid zi = Vmin + i A\u22121(Vmax \u2212 Vmin) On each interaction step: 1. select a randomly with probability \u03b5(t), else a = argmax a \ufffd i zi\u03b6\u2217 i (s, a, \u03b8) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. add observed transition to experience replay 4. sample batch of size B from experience replay 5. for each transition T from the batch compute target: P(y(T ) = r\u2032 + \u03b3zi) = \u03b6\u2217 i \ufffd s\u2032, argmax a\u2032 \ufffd i zi\u03b6\u2217 i (s\u2032, a\u2032, \u03b8\u2212), \u03b8\u2212 \ufffd 6. project y(T ) on support {z0, z1 . . . zA\u22121} 7. compute loss: Loss = 1 B \ufffd T KL(y(T ) \u2225 Z\u2217(s, a, \u03b8)) 8. make a step of gradient descent using \u2202 Loss \u2202\u03b8 9. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 4.3. Quantile Regression DQN (QR-DQN) Categorical DQN discovered a gap between theory and practice as KL-divergence, used in practical algorithm, is theoretically unjusti\ufb01ed. Theorem 12 hints that the true divergence we should care about is actually Wasserstein metric, but it remained unclear how it could be optimized using only samples from transition probabilities T. In [3] it was discovered that selecting another family of distributions to approximate Z\u2217 \u03b8(s, a) will reduce Wasserstein minimization task to the search for quantiles of speci\ufb01c distributions. The 19to project a categorical distribution with support {v0, v1 . . . vA\u22121} on categorical distributions with support {z0, z1 . . . zA\u22121} one can just \ufb01nd for each vi the closest two atoms zj \u2264 vi \u2264 zj+1 and split all probability mass for vi between zj and zj+1 proportional to closeness. If vi < z0, then all its probability mass is given to z0, same with upper bound. 24 latter can be done in online setting using quantile regression technique. This led to alternative distributional Q-learning algorithm named Quantile Regression DQN (QR-DQN). The basic idea is to \u00abswap\u00bb \ufb01xed support and learned probabilities of Categorical DQN. We will now consider the family with \ufb01xed probabilities for A-atomed categorical distribution with arbitrary support {\u03b6\u2217 0(s, a, \u03b8), \u03b6\u2217 1(s, a, \u03b8), . . . , \u03b6\u2217 A\u22121(s, a, \u03b8)}. Again, we will assume all probabilities to be equal given the absence of any prior knowledge; namely, our distribution family is now Z\u2217 \u03b8(s, a) \u223c Uniform \ufffd \u03b6\u2217 0(s, a, \u03b8), . . . , \u03b6\u2217 A\u22121(s, a, \u03b8) \ufffd In this setting neural network outputs A arbitrary real numbers that represent the support of uniform categorical distribution20, where A is the number of atoms and the only hyperparameter to select. For table-case setting, on each step of point iteration we desire to update the cell for given stateaction pair s, a with full distribution of random variable to the right side of (24). If we are limited to store only A atoms of the support, the true distribution must be projected on the space of Aatomed categorical distributions. Consider now this task of projecting some given random variable with c.d.f. F (\u03c9) in terms of Wasserstein distance. Speci\ufb01cally, we will be interested in minimizing W1-distance for p = 1 as the theorem 12 states the contraction property for all 1 \u2264 p \u2264 +\u221e and we are free to choose any: \ufffd 1 0 \ufffd\ufffd\ufffdF \u22121(\u03c9) \u2212 U \u22121 z0,z1...zA\u22121(\u03c9) \ufffd\ufffd\ufffd d\u03c9 \u2192 min z0,z1...zA\u22121 (28) where Uz0,z1...zA\u22121 is c.d.f. for uniform categorical distribution on given support. Its inverse, also known as quantile function, has a following simple form: U \u22121 z0,z1...zA\u22121(\u03c9) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 z0 0 \u2264 \u03c9 < 1 A z1 1 A \u2264 \u03c9 < 2 A ... zA\u22121 A\u22121 A \u2264 \u03c9 < 1 Substituting this into (28) A\u22121 \ufffd i=0 \ufffd i+1 A i A \ufffd\ufffdF \u22121(\u03c9) \u2212 zi \ufffd\ufffd d\u03c9 \u2192 min z0,z1...zA\u22121 splits the optimization of Wasserstein into A independent tasks that can be solved separately: \ufffd i+1 A i A \ufffd\ufffdF \u22121(\u03c9) \u2212 zi \ufffd\ufffd d\u03c9 \u2192 min zi (29) Proposition 15. [3] Let\u2019s denote \u03c4i := i A + i+1 A 2 Then every solution for (29) satis\ufb01es F (zi) = \u03c4i, i. e. it is \u03c4i-th quantile of c. d. f. F . The result 15 states that we require only A speci\ufb01c quantiles of random variable to the right side of Bellman equation21. Hence the last thing to do to design a practical algorithm is to develop a procedure of unbiased estimation of quantiles for the random variable on the right side of distribution Bellman optimality equation (24). 20Note that target distribution is now guaranteed to remain within this distribution family as multiplying on \u03b3 just shrinks the support and adding r\u2032 just shifts it. We assume that if some atoms of the support coincide, the distribution is still A-atomed categorical; for example, for degenerated distribution (like in the case of terminal states) \u03b6\u2217 0(s, a, \u03b8) = \u03b6\u2217 1(s, a, \u03b8) = \u00b7 \u00b7 \u00b7 = \u03b6\u2217 A\u22121(s, a, \u03b8). This shows that projection step heuristic is not needed for this particular choice of distribution family. 21It can be proved that for table-case policy evaluation algorithm which stores in each cell not expectations of reward (as in Q-learning) but A quantiles updated according to distributional Bellman equation (21) using theorem 15 converges to quantiles of Z\u2217(s, a) in Wasserstein metric for 1 \u2264 p \u2264 +\u221e and its update operator is a contraction mapping in W\u221e. 25 Quantile regression is the standard technique to estimate the quantiles of empirical distribution (i. .e. distribution that is represented by \ufb01nite amount of i. i. d. samples from it). Recall from machine learning that the constant solution optimizing l1-loss is median, i. .e. 1 2-th quantile. This fact can be generalized to arbitrary quantiles: Proposition 16. (Quantile Regression) [11] Let\u2019s de\ufb01ne loss as Loss(c, X) = \ufffd \u03c4(c \u2212 X) c \u2265 X (1 \u2212 \u03c4)(X \u2212 c) c < X Then solution for EX Loss(c, X) \u2192 min c\u2208R (30) is \u03c4-th quantile of distribution of X. As usual in the case of neural networks, it is impractical to optimize (30) until convergence on each iteration for each of A desired quantiles \u03c4i. Instead just one step of gradient optimization is made and the outputs of neural network \u03b6\u2217 i (s, a, \u03b8), which play the role of c in formula (30), are moved towards the quantile estimation via backpropagation. In other words, (30) sets a loss function for network outputs; the losses for di\ufb00erent quantiles are summed up. The resulting loss is LossQR(s, a, \u03b8) = A\u22121 \ufffd i=0 Es\u2032\u223cp(s\u2032|s,a)Ey\u223cy(T ) \ufffd \u03c4 \u2212 I[\u03b6\u2217 i (s, a, \u03b8) < y] \ufffd \ufffd \u03b6\u2217 i (s, a, \u03b8) \u2212 y \ufffd (31) where I denotes an indicator function. The expectation over y \u223c y(T ) for given transition can be computed in closed form: indeed, y(T ) is also an A-atomed categorical distribution with support {r\u2032 + \u03b3\u03b6\u2217 0(s\u2032, a\u2032), . . . , r\u2032 + \u03b3\u03b6\u2217 A\u22121(s\u2032, a\u2032)}, where a\u2032 = argmax a\u2032 EZ\u2217(s\u2032, a\u2032, \u03b8) = argmax a\u2032 1 A \ufffd i \u03b6\u2217 i (s\u2032, a\u2032, \u03b8) and expectation over transition probabilities, as always, is estimated using Monte-Carlo by sampling transitions from experience replay. Algorithm 4: Quantile Regression DQN (QR-DQN) Hyperparameters: B \u2014 batch size, A \u2014 number of atoms, K \u2014 target network update frequency, \u03b5(t) \u2208 (0, 1] \u2014 greedy exploration parameter, \u03b6\u2217 \u2014 neural network, SGD optimizer. Initialize weights \u03b8 of neural net \u03b6\u2217 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 Precompute mid-quantiles \u03c4i = i A + i+1 A 2 On each interaction step: 1. select a randomly with probability \u03b5(t), else a = argmax a 1 A \ufffd i \u03b6\u2217 i (s, a, \u03b8) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. add observed transition to experience replay 4. sample batch of size B from experience replay 5. for each transition T from the batch compute the support of target distribution: y(T )j = r\u2032 + \u03b3\u03b6\u2217 j \ufffd s\u2032, argmax a\u2032 1 A \ufffd i \u03b6\u2217 i (s\u2032, a\u2032, \u03b8\u2212), \u03b8\u2212 \ufffd 26 6. compute loss: Loss = 1 BA \ufffd T \ufffd i \ufffd j \ufffd \u03c4i \u2212 I[\u03b6\u2217 i (s, a, \u03b8) < y(T )j] \ufffd \ufffd \u03b6\u2217 i (s, a, \u03b8) \u2212 y(T )j \ufffd 7. make a step of gradient descent using \u2202 Loss \u2202\u03b8 8. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 4.4. Rainbow DQN Success of Deep Q-learning encouraged a full-scale research of value-based deep reinforcement learning by studying various drawbacks of DQN and developing auxiliary extensions. In many articles some extensions from previous research were already considered and embedded in compared algorithms during empirical studies. In Rainbow DQN [7], seven Q-learning-based ideas are united in one procedure with ablation studies held whether all these incorporated extensions are essentially necessary for resulted RL algorithm: \u2022 DQN (sec. 3.2) \u2022 Double DQN (sec. 3.3) \u2022 Dueling DQN (sec. 3.4) \u2022 Noisy DQN (sec. 3.5) \u2022 Prioritized Experience Replay (sec. 3.6) \u2022 Multi-step DQN (sec. 3.7) \u2022 Categorical22 DQN (sec. 4.2) There is little ambiguity on how these ideas can be combined; we will discuss several nonstraightforward circumstances and provide the full algorithm description after. To apply prioritized experience replay in distributional setting, the measure of transition importance must be provided. The main idea is inherited from ordinary DQN where priority is just loss for this transition: \u03c1(T ) := Loss(y(T ), Z\u2217(s, a, \u03b8)) = KL(y(T ) \u2225 Z\u2217(s, a, \u03b8)) To combine noisy networks with double DQN heuristic, it is proposed to resample noise on each forward pass through the network and through its copy for target computation. This decision implies that action selection, action evaluation and network utilization are independent and stochastic (for exploration cultivation) steps. The one snagging combination here is categorical DQN and dueling DQN. To merge these ideas, we need to model advantage A\u2217(s, a, \u03b8) in distributional setting. In Rainbow this is done straightforwardly: the network has two heads, value stream v(s, \u03b8) outputting A real values and advantage stream a(s, a, \u03b8) outputting A \u00d7 |A| real values. Then these streams are integrated using the same formula (17) with the only exception being softmax applied across atoms dimension to guarantee that output is categorical distribution: \u03b6\u2217 i (s, a, \u03b8) \u221d exp \ufffd v(s, \u03b8)i + a(s, a, \u03b8)i \u2212 1 |A| \ufffd a a(s, a, \u03b8)i \ufffd (32) Combining lack of intuition behind this integration formula with usage of mean instead of theoretically justi\ufb01ed max makes this element of Rainbow the most questionable. During the ablation studies it was discovered that dueling architecture is the only component that can be removed without noticeable loss of performance. All other ingredients are believed to be crucial for resulting algorithm as they address di\ufb00erent problems. 22Quantile Regression can be considered instead 27 Algorithm 5: Rainbow DQN Hyperparameters: B \u2014 batch size, Vmax, Vmin, A \u2014 parameters of support, K \u2014 target network update frequency, N \u2014 multi-step size, \u03b1 \u2014 degree of prioritized experience replay, \u03b2(t) \u2014 importance sampling bias correction for prioritized experience replay, \u03b6\u2217 \u2014 neural network, SGD optimizer. Initialize weights \u03b8 of neural net \u03b6\u2217 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 Precompute support grid zi = Vmin + i A\u22121(Vmax \u2212 Vmin) On each interaction step: 1. select a = argmax a \ufffd i zi\u03b6\u2217 i (s, a, \u03b8, \u03b5), \u03b5 \u223c N (0, I) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. construct N-step transition T = \ufffd s, a, \ufffdN n=0 \u03b3nr(n+1), s(N), done \ufffd and add it to experience replay with priority maxT \u03c1(T ) 4. sample batch of size B from experience replay using probabilities P(T ) \u221d \u03c1(T )\u03b1 5. compute weights for the batch (where M is the size of experience replay memory) w(T ) = \ufffd 1 MP(T ) \ufffd\u03b2(t) 6. for each transition T = (s, a, \u00afr, \u00afs, done) from the batch compute target (detached from computational graph to prevent backpropagation): \u03b51, \u03b52 \u223c N (0, I) P(y(T ) = \u00afr + \u03b3Nzi) = \u03b6\u2217 i \ufffd \u00afs, argmax \u00afa \ufffd i zi\u03b6\u2217 i (\u00afs, \u00afa, \u03b8, \u03b51), \u03b8\u2212, \u03b52 \ufffd 7. project y(T ) on support {z0, z1 . . . zA\u22121} 8. update transition priorities \u03c1(T ) \u2190 KL(y(T ) \u2225 Z\u2217(s, a, \u03b8, \u03b5)), \u03b5 \u223c N (0, I) 9. compute loss: Loss = 1 B \ufffd T w(T )\u03c1(T ) 10. make a step of gradient descent using \u2202 Loss \u2202\u03b8 11. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 28 5. Policy Gradient algorithms 5.1. Policy Gradient theorem Alternative approach to solving RL task is direct optimization of objective J(\u03b8) = ET \u223c\u03c0\u03b8 \ufffd t=1 \u03b3t\u22121rt \u2192 max \u03b8 (33) as a function of \u03b8. Policy gradient methods provide a framework how to construct an e\ufb03cient optimization procedure based on stochastic \ufb01rst-order optimization within RL setting. We will assume that \u03c0\u03b8(a | s) is a stochastic policy parameterized with \u03b8 \u2208 \u0398. It turns out, that if \u03c0 is di\ufb00erentiable by \u03b8, then so is our goal (33). We now proceed to discuss the technique of derivative calculation which is based on employment of log-derivative trick: Proposition 17. For arbitrary distribution \u03c0(a) parameterized by \u03b8: \u2207\u03b8\u03c0(a) = \u03c0(a)\u2207\u03b8 log \u03c0(a) (34) In most general form, this trick allows us to derive the gradient of expectation of an arbitrary function f(a, \u03b8) : A \u00d7 \u0398 \u2192 R, di\ufb00erentiable by \u03b8, with respect to some distribution \u03c0\u03b8(a), also parameterized by \u03b8: \u2207\u03b8Ea\u223c\u03c0\u03b8(a)f(a, \u03b8) = \u2207\u03b8 \ufffd A \u03c0\u03b8(a)f(a, \u03b8)da = = \ufffd A \u2207\u03b8 [\u03c0\u03b8(a)f(a, \u03b8)] da = {product rule} = \ufffd A [\u2207\u03b8\u03c0\u03b8(a)f(a, \u03b8) + \u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8)] da = = \ufffd A \u2207\u03b8\u03c0\u03b8(a)f(a, \u03b8)da + E\u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8) = {log-derivative trick (34)} = \ufffd A \u03c0\u03b8(a)\u2207\u03b8 log \u03c0\u03b8(a)f(a, \u03b8)da + E\u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8) = = E\u03c0\u03b8(a)\u2207\u03b8 log \u03c0\u03b8(a)f(a, \u03b8) + E\u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8) This technique can be applied sequentially (to expectations over \u03c0\u03b8(a0 | s0), \u03c0\u03b8(a1 | s1) and so on) to obtain the gradient \u2207\u03b8J(\u03b8). Proposition 18. (Policy Gradient Theorem) [24] For any MDP and di\ufb00erentiable policy \u03c0\u03b8 the gradient of objective (33) is \u2207\u03b8J(\u03b8) = ET \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st)Q\u03c0(st, at) (35) For future references, we require another form of formula (35), which provides another point of view. For this purpose, let us de\ufb01ne a discounted state visitation frequency: De\ufb01nition 10. For given MDP and given policy \u03c0 its discounted state visitation frequency is de\ufb01ned by d\u03c0(s) := (1 \u2212 \u03b3) \ufffd t=0 \u03b3tP(st = s) where st are taken from trajectories T sampled using given policy \u03c0. Discounted state visitation frequencies, if normalized, represent a marginalized probability for agent to land in a given state s23. It is rarely attempted to be learned, but it assists theoretical 23the \u03b3t weighting in this de\ufb01nition is often introduced to incorporate the same reduction of contribution of later states in the whole gradient according to (35). Similar notation is sometimes used for state visitation frequency without discount. 29 study by allowing us to rewrite expectations over trajectories with separated intrinsic and extrinsic randomness of the decision making process: \u2207\u03b8J(\u03b8) = Es\u223cd\u03c0(s)Ea\u223c\u03c0(a|s)\u2207\u03b8 log \u03c0\u03b8(a | s)Q\u03c0(s, a) (36) This form is equivalent to (35) as sampling a trajectory and going through all visited states with weights \u03b3t induces the same distribution as de\ufb01ned in d\u03c0(s). Now, although we acquired an explicit form of objective\u2019s gradient, we are able to compute it only approximately, using Monte-Carlo estimation for expectations via sampling one or several trajectories. Second form of gradient (36) reveals that it is possible to use roll-outs of trajectories without waiting for episode ending, as the states for the roll-outs come from the same distribution as they would for complete episode trajectories24. The essential thing is that exactly the policy \u03c0(\u03b8) must be used for sampling to obtain unbiased Monte-Carlo estimation (otherwise state visitation frequency d\u03c0(s) is di\ufb00erent). These features are commonly underlined by notation E\u03c0, which is a shorter form of Es\u223cd\u03c0(s)Ea\u223c\u03c0(a|s). When convenient, we will use it to reduce the gradient to a shorter form: \u2207\u03b8J(\u03b8) = E\u03c0(\u03b8)\u2207\u03b8 log \u03c0\u03b8(a | s)Q\u03c0(s, a) (37) Second important thing worth mentioning is that Q\u03c0(s, a) is essentially present in the gradient. Remark that it is never available to the algorithm and must also be somehow estimated. 5.2. REINFORCE REINFORCE [29] provides a straightforward approach to approximately calculate the gradient (35) in episodic case using Monte-Carlo estimation: N games are played and Q-function under policy \u03c0 is approximated with corresponding return: Q\u03c0(s, a) = ET \u223c\u03c0\u03b8|s,aR(T ) \u2248 R(T ), T \u223c \u03c0\u03b8 | s, a The resulting formula is therefore the following: \u2207\u03b8J(\u03b8) \u2248 1 N N \ufffd T \ufffd t=0 \ufffd \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st) \ufffd\ufffd t\u2032=t \u03b3t\u2032\u2212trt\u2032+1 \ufffd\ufffd (38) This estimation is unbiased as both approximation of Q\u03c0 and approximation of expectation over trajectories are done using Monte-Carlo. Given that estimation of gradient is unbiased, stochastic gradient ascent or more advanced stochastic optimization techniques are known to converge to local optimum. From theoretical point of view REINFORCE can be applied straightforwardly for any parametric family \u03c0\u03b8(a | s) including neural networks. Yet the enormous time required for convergence and the problem of stucking in local optimums make this naive approach completely impractical. The main source of problems is believed to be the high variance of gradient estimation (38), as the convergence rate of stochastic gradient descent directly depends on the variance of gradient estimation. The standard technique of variance reduction is an introduction of baseline. The idea is to add some term that will not a\ufb00ect the expectation, but may a\ufb00ect the variance. One such baseline can be derived using following reasoning: for any distribution it is true that \ufffd A \u03c0\u03b8(a | s)da = 1. Taking the gradient \u2207\u03b8 from both sides, we obtain: 0 = \ufffd A \u2207\u03b8\u03c0\u03b8(a | s)da = {log-derivative trick (34)} = \ufffd A \u03c0\u03b8(a | s)\u2207\u03b8 log \u03c0\u03b8(a | s)da = = E\u03c0\u03b8(a|s)\u2207\u03b8 log \u03c0\u03b8(a | s) 24in practice and in most policy gradients algorithms, sampling roll-outs never include \u03b3t weights, which formally corresponds to estimating gradient using incorrect equation (\u00abapproximation\u00bb): \u2207\u03b8J(\u03b8) \u2248 ET \u223c\u03c0\u03b8 \ufffd t=0 \u2207\u03b8 log \u03c0\u03b8(at | st)Q\u03c0(st, at) which di\ufb00ers from the correct version (35) in ignoring \u03b3t multiplier. On the one hand, it equalizes the contribution of different terms and agrees with intuition, but on the other hand such gradient estimation does not imply optimization of any reasonable objective and breaks the idea of straightforward gradient ascent [15]. 30 Multiplying this expression on some constant, we can scale this baseline: E\u03c0\u03b8(a|s) const(a)\u2207\u03b8 log \u03c0\u03b8(a | s) = 0 Notice that the constant here must be independent of a, but may depend on s. Application of this technique to our case provides the following result25: Proposition 19. For any arbitrary function b(s): S \u2192 R, called baseline: \u2207\u03b8J(\u03b8) = ET \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st) (Q\u03c0(st, at) \u2212 b(st)) Selection of the baseline is up to us as long as it does not depend on actions at. The intent is to choose it in a way that minimizes the variance. It is believed that high variance of (38) originates from multiplication of Q\u03c0(s, a), which may have arbitrary scale (e. .g. in a range [100, 200]) while \u2207\u03b8 log \u03c0\u03b8(at | st) naturally has varying signs26. To reduce the variance, the baseline must be chosen so that absolute values of expression inside the expectation are shifted towards zero. Wherein the optimal baseline is provided by the following theorem: Proposition 20. The solution for VT \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st) (Q\u03c0(st, at) \u2212 b(st)) \u2192 min b(s) is given by b(s) = Ea\u223c\u03c0\u03b8(a|s)\u03b3t\u2225\u2207\u03b8 log \u03c0\u03b8(a | s)\u22252 2Q\u03c0(s, a) Ea\u223c\u03c0\u03b8(a|s)\u2225\u2207\u03b8 log \u03c0\u03b8(a | s)\u22252 2 (39) As can be seen, optimal baseline calculation involves expectations which again can only be computed (in most cases) using Monte-Carlo (both for numerator and denominator). For that purpose, for every visited state s estimations of Q\u03c0(s, a) are needed for all (or some) actions a, as otherwise estimation of baseline will coincide with estimation of Q\u03c0(s, a) and collapse gradient to zero. Practical utilization of result (39) is to consider a constant baseline independent of s with similar optimal form: b = ET \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2225\u2207\u03b8 log \u03c0\u03b8(at | st)\u22252 2Q\u03c0(st, at) ET \u223c\u03c0\u03b8 \ufffd t=0 \u2225\u2207\u03b8 log \u03c0\u03b8(at | st)\u22252 2 Utilization of some kind of baseline, not necessarily optimal, is known to signi\ufb01cantly reduce the variance of gradient estimation and is an essential part of any policy gradient method. The \ufb01nal step to make this family of algorithms applicable when using deep neural networks is to reduce variance of Q\u03c0 estimation by employing RL task structure like it was done in value-based methods. 5.3. Advantage Actor-Critic (A2C) Suppose that in optimal baseline formula (39) it happens that \u2225\u2207\u03b8 log \u03c0\u03b8(a | s)\u22252 2 = const(a). Though in reality this is actually not true, under this circumstance the optimal baseline formula signi\ufb01cantly reduces and unravels a close-to-optimal but simple form of baseline: b(s) = \u03b3tEa\u223c\u03c0\u03b8(a|s)Q\u03c0(s, a) = \u03b3tV \u03c0(s) Substituting this baseline into gradient formula (37) and recalling the de\ufb01nition of advantage function (14), the gradient can now be rewritten as follows: \u2207\u03b8J(\u03b8) = E\u03c0(\u03b8)\u2207\u03b8 log \u03c0\u03b8(a | s)A\u03c0(s, a) (40) This representation of gradient is used as the basement for most policy gradient algorithms as it o\ufb00ers lower variance while selecting the baseline expressed in terms of value functions which can be 25this result can be generalized by introducing di\ufb00erent baselines for estimation of di\ufb00erent components of \u2207\u03b8J(\u03b8). 26this follows, for example, from baseline derivation. 31 e\ufb03ciently learned similar to how it was done in value-based methods. Such algorithms are usually named Actor-Critic as they consist of two neural networks: \u03c0\u03b8(a | s), representing a policy, called an actor, and V \u03c0 \u03c6 (s) with parameters \u03c6, approximately estimating actor\u2019s performance, called a critic. Note that the choice of value function to learn can be arbitrary; it is possible to learn Q\u03c0 or A\u03c0 instead, as all of them are deeply interconnected. Value function V \u03c0 is chosen as the simplest one since it depends only on state and thus is hoped to be easier to learn. Having a critic V \u03c0 \u03c6 (s), Q-function can be approximated in a following way: Q\u03c0(s, a) \u2248 r\u2032 + \u03b3V \u03c0(s\u2032) \u2248 r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) First approximation is done using Monte-Carlo, while second approximation inevitably introduces bias. Important thing to notice is that at this moment our gradient estimation stops being unbiased and all theoretical guarantees of converging are once again lost. Advantage function therefore can be obtained according to the de\ufb01nition: A\u03c0(s, a) = Q\u03c0(s, a) \u2212 V \u03c0(s) \u2248 r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2212 V \u03c0 \u03c6 (s) (41) Note that biased estimation of baseline doesn\u2019t make gradient estimation biased by itself, as baseline can be an arbitrary function of state. All bias introduction happens inside the approximation of Q\u03c0. It is possible to use critic only for baseline, which allows complete avoidance of bias, but then the only way to estimate Q\u03c0 is via playing several games and using corresponding returns, which su\ufb00ers from higher variance and low sample e\ufb03ciency. The logic behind training procedure for the critic is taken from value-based methods: for given policy \u03c0 its value function can be obtained using point iteration for solving V \u03c0(s) = Ea\u223c\u03c0(a|s)Es\u2032\u223cp(s\u2032|s,a) [r\u2032 + \u03b3V \u03c0(s\u2032)] Similar to DQN, on each update a target is computed using current approximation y = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) and then MSE is minimized to move values of V \u03c0 \u03c6 (s) towards the guess. Notice that to compute the target for critic we require samples from the policy \u03c0 which is being evaluated. Although actor evolves throughout optimization process, we assume that one update of policy \u03c0 does not lead to signi\ufb01cant change of true V \u03c0 and thus our critic, which approximates value function for older version of policy, is close enough to construct the target. But if samples from, for example, old policy are used to compute the guess, the step of critic update will correspond to learning the value function for old policy other than current. Essentially, this means that both actor and critic training procedures require samples from current policy \u03c0, making Actor-Critic algorithm onpolicy by design. Consequently, samples that were collected on previous update iterations become useless and can be forgotten. This is the key reason why policy gradient algorithms are usually less sample-e\ufb03cient than value-based. Now as we have an approximation of value function, advantage estimation can be done using one-step transitions (41). As the procedure of training an actor, i. .e. gradient estimation (40), also does not demand sampling the whole trajectory, each update now requires only a small roll-out to be sampled. The amount of transitions in the roll-out corresponds to the size of mini-batch. The problem with roll-outs is that the data is obviously not i. i. d., which is crucial for training networks. In value-based methods, this problem was solved with experience replay, but in policy gradient algorithms it is essential to collect samples from scratch after each update of the networks parameters. The practical solution for simulated environments is to launch several instances of environment (for example, on di\ufb00erent cores of multiprocessor) in parallel threads and have several parallel interactions. After several steps in each environment, the batch for update is collected by uniting transitions from all instances and one synchronous27 update of networks parameters \u03b8 and \u03c6 is performed. One more optimization that can be done is to partially share weights of networks \u03b8 and \u03c6. It is justi\ufb01ed as \ufb01rst layers of both networks correspond to basic features extraction and these features are likely to be the same for optimal policy and value function. While it reduces the number of training parameters almost twice, it might destabilize learning process as the scales of gradient (40) and 27there is also an asynchronous modi\ufb01cation of advantage actor critic algorithm (A3C) which accelerates the training process by storing a copy of network for each thread and performing weights synchronization from time to time. 32 gradient of critic\u2019s MSE loss may be signi\ufb01cantly di\ufb00erent, so they should be balanced with additional hyperparameter. Algorithm 6: Advantage Actor-Critic (A2C) Hyperparameters: B \u2014 batch size, V \u2217 \u03c6 \u2014 critic neural network, \u03c0\u03b8 \u2014 actor neural network, \u03b1 \u2014 critic loss scaling, SGD optimizer. Initialize weights \u03b8, \u03c6 arbitrary On each step: 1. obtain a roll-out of size B using policy \u03c0(\u03b8) 2. for each transition T from the roll-out compute advantage estimation: A\u03c0(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2212 V \u03c0 \u03c6 3. compute target (detached from computational graph to prevent backpropagation): y(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) 4. compute critic loss: Loss = 1 B \ufffd T \ufffd y(T ) \u2212 V \u03c0 \u03c6 \ufffd2 5. compute critic gradients: \u2207critic = \u2202 Loss \u2202\u03c6 6. compute actor gradient: \u2207actor = 1 B \ufffd T \u2207\u03b8 log \u03c0\u03b8(a | s)A\u03c0(T ) 7. make a step of gradient descent using \u2207actor + \u03b1\u2207critic 5.4. Generalized Advantage Estimation (GAE) There is a design dilemma in Advantage Actor Critic algorithm concerning the choice whether to use the critic to estimate Q\u03c0(s, a) and introduce bias into gradient estimation or to restrict critic employment only for baseline and cause higher variance with necessity of playing the whole episodes for each update step. Actually, the range of possibilities is wider. Since Actor-Critic is an on-policy algorithm by design, we are free to use N-step approximations instead of one-step: using Q\u03c0(s, a) \u2248 N\u22121 \ufffd n=0 \u03b3nr(n+1) + \u03b3NV \u03c0 \ufffd s(N)\ufffd we can de\ufb01ne N-step advantage estimator as A\u03c0 (N)(s, a) := N\u22121 \ufffd n=0 \u03b3nr(n+1) + \u03b3NV \u03c0 \u03c6 \ufffd s(N)\ufffd \u2212 V \u03c0 \u03c6 (s) For N = 1 this estimation corresponds to Actor-Critic one-step estimation with high bias and low variance. For N = \u221e it yields the estimator with critic used only for baseline with no bias and high variance. Intermediate values correspond to something in between. Note that to use N-step advantage estimation we have to perform N steps of interaction after given state-action pair. 33 Usually \ufb01nding a good value for N as hyperparameter is di\ufb03cult as its \u00aboptimal\u00bb value may \ufb02oat throughout the learning process. In Generalized Advantage Estimation (GAE) [20] it is proposed to construct an ensemble out of di\ufb00erent N-step advantage estimators using exponential smoothing with some hyperparameter \u03bb: A\u03c0 GAE(s, a) := (1 \u2212 \u03bb) \ufffd A\u03c0 (1)(s, a) + \u03bbA\u03c0 (2)(s, a) + \u03bb2A\u03c0 (3)(s, a) + . . . \ufffd (42) Here the parameter \u03bb \u2208 [0, 1] allows smooth control over bias-variance trade-o\ufb00: \u03bb = 0 corresponds to Actor-Critic with higher bias and lower variance while \u03bb \u2192 1 corresponds to REINFORCE with no bias and high variance. But unlike N as hyperparameter, it uses mix of di\ufb00erent estimators in intermediate case. GAE proved to be a convenient way how more information can be obtained from collected rollout in practice. Instead of waiting for episode termination to compute (42) we may use \u00abtruncated\u00bb GAE which ensembles only those N-step advantage estimators that are available: A\u03c0 trunc.GAE(s, a) := A\u03c0 (1)(s, a) + \u03bbA\u03c0 (2)(s, a) + \u03bb2A\u03c0 (3)(s, a) + \u00b7 \u00b7 \u00b7 + \u03bbN\u22121A\u03c0 (N)(s, a) 1 + \u03bb + \u03bb2 + \u00b7 \u00b7 \u00b7 + \u03bbN\u22121 Note that the amount N of available estimators may be di\ufb00erent for di\ufb00erent transitions from rollout: if we performed K steps of interaction in some instance of environment starting from some state-action pair s, a, we can use N = K step estimators; for next state-action pair s\u2032, a\u2032 we have only N = K\u22121 transitions and so on, while the last state-action pair sN\u22121, aN\u22121 can be estimated only using A\u03c0 (1) as only N = 1 following transition is available. Although di\ufb00erent transitions are estimated with di\ufb00erent precision (leading to di\ufb00erent bias and variance), this approach allows to use all available information for each transition and utilize multi-step approximations without dropping last transitions of roll-outs used only for target computation. 5.5. Natural Policy Gradient (NPG) In this section we discuss the motivation and basic principles behind the idea of natural gradient descent, which we will require for future references. The standard gradient descent optimization method is known to be extremely sensitive to the choice of parametrization. Suppose we attempt to solve the following optimization task: f(q) \u2192 min q where q is a distribution and F is arbitrary di\ufb00erentiable function. We often restrict q to some parametric family and optimize similar objective, but with respect to some vector of parameters \u03b8 as unknown variable: f(q\u03b8) \u2192 min \u03b8 Classic example of such problem is maximum likelihood task when we try to \ufb01t the parameters of our model to some observed data. The problem is that when using standard gradient descent both the convergence rate and overall performance of optimization method substantially depend on the choice of parametrization q\u03b8. The problem holds even if we \ufb01x speci\ufb01c distribution family as many distribution families allow di\ufb00erent parametrizations. To see why gradient descent is parametrization-sensitive, consider the model which is used at some current point \u03b8k to determine the direction of next optimization step: \ufffd f(q\u03b8k) + \u27e8\u2207\u03b8f(q\u03b8k), \u03b4\u03b8\u27e9 \u2192 min \u03b4\u03b8 \u2225\u03b4\u03b8\u22252 2 < \u03b1k where \u03b1k is learning rate at step k. Being \ufb01rst-order method, gradient descent constructs a \u00abmodel\u00bb which approximates F locally around \u03b8k using \ufb01rst-order Taylor expansion and employs standard Euclidean metric to determine a region of trust for this model. Then this surrogate task is solved analytically to obtain well-known update formula: \u03b4\u03b8 \u221d \u2212\u2207\u03b8f(q\u03b8k) 34 The issue arises from reliance on Eucliden metric in the space of parameters. In most parametrizations, small changes in parameters space do not guarantee small change in distribution space and vice versa: some small changes in distribution may demand big steps in parameters space28. Natural gradient proposes to use another metric, which achieves invariance to parametrization of distribution q using the properties of Fisher matrix: De\ufb01nition 11. For distribution q\u03b8 Fisher matrix Fq(\u03b8) is de\ufb01ned as Fq(\u03b8) := Ex\u223cq\u2207\u03b8 log q\u03b8(x)(\u2207\u03b8 log q\u03b8(x))T Note that Fisher matrix depends on parametrization. Yet for any parametrization it is guaranteed to be positive semi-de\ufb01nite by de\ufb01nition. Moreover, it induces a so-called Riemannian metric29 in the space of parameters: d(\u03b81, \u03b82)2 := (\u03b82 \u2212 \u03b81)T Fq(\u03b81)(\u03b82 \u2212 \u03b81) In natural gradient descent it is proposed to use this metric instead of Euclidean: \ufffd f(q\u03b8k) + \u27e8\u2207\u03b8f(q\u03b8k), \u03b4\u03b8\u27e9 \u2192 min \u03b4\u03b8 \u03b4\u03b8T Fq(\u03b8k)\u03b4\u03b8 < \u03b1k This surrogate task can be solved analytically to obtain the following optimization direction: \u03b4\u03b8 \u221d \u2212Fq(\u03b8k)\u22121\u2207\u03b8f(q\u03b8k) (43) The direction of gradient descent is corrected by Fisher matrix which concerns the scale across different axes. This direction, speci\ufb01ed by Fq(\u03b8k)\u22121\u2207\u03b8f(q\u03b8k), is called natural gradient. Let\u2019s discuss why this new metric really provides us invariance to distribution parametrization. We already obtained natural gradient for q being parameterized by \u03b8 (43). Assume that we have another parametrization q\u03bd. These new parameters \u03bd are somehow related to \u03b8; we suppose there is some functional dependency \u03b8(\u03bd), which we assume to be di\ufb00erentiable with jacobian J. In this notation: \u03b4\u03b8 = J\u03b4\u03bd, Jij := \u2202\u03b8i \u2202\u03bdj (44) The central property of Fisher matrix, which provides the desired invariance, is the following: Proposition 21. If \u03b8 = \u03b8(\u03bd) with jacobian J, then reparametrization formula for Fisher matrix is Fq(\u03bd) = JT Fq(\u03b8)J (45) Now it can be derived that natural gradient for parametrization with \u03bd is the same as for \u03b8. If we want to calculate natural gradient in terms of \u03bd, then our step is, according to (44): \u03b4\u03b8 = J\u03b4\u03bd = {natural gradient in terms of \u03bd} \u221d JFq(\u03bdk)\u22121\u2207\u03bdf(q\u03bdk) = {Fisher matrix reparametrization (45)} = J \ufffd JT Fq(\u03b8k)J \ufffd\u22121 \u2207\u03bdf(q\u03bdk) {chain rule} = J \ufffd JT Fq(\u03b8k)J \ufffd\u22121 \u2207\u03bd\u03b8(\u03bdk)T \u2207\u03b8f(q\u03b8k) = = JJ\u22121Fq(\u03b8k)\u22121J\u2212T JT \u2207\u03b8f(q\u03b8k) = = Fq(\u03b8k)\u22121\u2207\u03b8f(q\u03b8k) 28classic example is that N (0, 100) is similar to N (1, 100) while N (0, 0.1) is completely di\ufb00erent from N (1, 0.1), although Euclidean distance in parameter space is the same for both pairs. 29in Euclidean space the general form of scalar product is \u27e8x, y\u27e9 := xT Gy, where G is \ufb01xed positive semi-de\ufb01nite matrix. The metric induced by this scalar product is correspondingly d(x, y)2 := (y \u2212x)T G(y \u2212x). The di\ufb00erence in Riemannian space is that G, called metric tensor, depends on x, so the relative distance may vary for di\ufb00erent points. It is used to describe the distances between points on manifolds and holds important properties which Fisher matrix inherits as metric tensor for distribution space. 35 which can be seen to be the same as in (43). Application of natural gradient descent in DRL setting is complicated in practice. Theoretically, the only change that must be done is scaling of gradient using inverse Fisher matrix (43). Yet, Fisher matrix requires n2 memory and O(n3) computational costs for inversion where n is the number of parameters. For neural networks this causes the same complications as the application of secondorder optimization methods. K-FAC optimization method [13] provides a speci\ufb01c approximation form of Fisher matrix for neural networks with linear layers which can be e\ufb03ciently computed, stored and inverted. Usage of K-FAC approximation allows to compute natural gradient directly using (43). 5.6. Trust-Region Policy Optimization (TRPO) The main drawback of Actor-Critic algorithm is believed to be the abandonment of experience that was used for previous updates. As the number of updates required is usually huge, this is considered to be a substantial loss of information. Yet, it is not clear how this information can be e\ufb00ectively used for newer updates. Suppose we want to make an update of \u03c0(\u03b8), but using samples collected by some \u03c0old. The straightforward approach is importance sampling technique, which naive application to gradient formula (40) yields the following result: \u2207\u03b8J(\u03b8) = ET \u223c\u03c0old P(T | \u03c0(\u03b8)) P(T | \u03c0old) \ufffd t=0 \u2207\u03b8 log \u03c0\u03b8(at | st)A\u03c0(st, at) The emerged importance sampling weight is actually computable as transition probabilities cross out: P(T | \u03c0(\u03b8)) P(T | \u03c0old) = \ufffd t=1 \u03c0\u03b8(at | st) \ufffd t=1 \u03c0old(at | st) The problem with this coe\ufb03cient is that it tends either to be exponentially small or to explode. Even with some heuristic normalization of coe\ufb03cients the batch gradient would become dominated by one or several transitions and destabilize the training procedure by introducing even more variance. Notice that application of importance sampling to another representation of gradient (37) yields seemingly di\ufb00erent result: \u2207\u03b8J(\u03b8) = E\u03c0old d\u03c0(\u03b8)(s) d\u03c0old(s) \u03c0\u03b8(a | s) \u03c0old(a | s)\u2207\u03b8 log \u03c0\u03b8(a | s)A\u03c0(s, a) (46) Here we avoided common for the whole trajectories importance sampling weights by using the definition of state visitation frequencies. But this result is even less practical as these frequencies are unknown to us. The \ufb01rst key idea behind the theory concerning this problem is that may be these importance sampling coe\ufb03cients behave more stable if the policies \u03c0old and \u03c0(\u03b8) are in some terms \u00abclose\u00bb. Intuitively, in this case d\u03c0(\u03b8)(s) d\u03c0old(s) of formula (46) is close to 1 as state visitation frequencies are similar, and the remained importance sampling coe\ufb03cient becomes acceptable in practice. And if some two policies are similar, their values of our objective (2) are probably close too. For any two policies, \u03c0 and \u03c0old: J(\u03c0) \u2212 J(\u03c0old) = ET \u223c\u03c0 \ufffd t=0 \u03b3tr(st) \u2212 J(\u03c0old) = = ET \u223c\u03c0 \ufffd t=0 \u03b3tr(st) \u2212 V \u03c0old(s0) = = ET \u223c\u03c0 \ufffd\ufffd t=0 \u03b3tr(st) \u2212 V \u03c0old(s0) \ufffd = {trick \ufffd\u221e t=0 (at+1 \u2212 at) = \u2212a0 30} = ET \u223c\u03c0 \ufffd\ufffd t=0 \u03b3tr(st) + \ufffd t=0 \ufffd \u03b3t+1V \u03c0old(st+1) \u2212 \u03b3tV \u03c0old(st) \ufffd\ufffd = {regroup} = ET \u223c\u03c0 \ufffd t=0 \u03b3t \ufffd r(st) + \u03b3V \u03c0old(st+1) \u2212 V \u03c0old(st) \ufffd = 36 {by de\ufb01nition (3)} = ET \u223c\u03c0 \ufffd t=0 \u03b3t \ufffd Q\u03c0old(st, at) \u2212 V \u03c0old(st) \ufffd {by de\ufb01nition (14)} = ET \u223c\u03c0 \ufffd t=0 \u03b3tA\u03c0old(st, at) The result obtained above is often referred to as relative policy performance identity and is actually very interesting: it states that we can substitute reward with advantage function of arbitrary policy and that will shift the objective by the constant. Using the discounted state visitation frequencies de\ufb01nition 10, relative policy performance identity can be rewritten as J(\u03c0) \u2212 J(\u03c0old) = 1 1 \u2212 \u03b3 Es\u223cd\u03c0(s)Ea\u223c\u03c0(a|s)A\u03c0old(s, a) Now assume we want to optimize parameters \u03b8 of policy \u03c0 while using data collected by \u03c0old: applying importance sampling in the same manner: J(\u03c0\u03b8) \u2212 J(\u03c0old) = 1 1 \u2212 \u03b3 Es\u223cd\u03c0old(s) d\u03c0\u03b8(s) d\u03c0old(s)Ea\u223c\u03c0old(a|s) \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) As we have in mind the idea of \u03c0old being close to \u03c0\u03b8, the question is how well this identity can be approximated if we assume d\u03c0\u03b8(s) = d\u03c0old(s). Under this assumption: J(\u03c0\u03b8) \u2212 J(\u03c0old) \u2248 L\u03c0old(\u03b8) := 1 1 \u2212 \u03b3 Es\u223cd\u03c0old(s)Ea\u223c\u03c0old(a|s) \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) The point is that interaction using \u03c0old corresponds to sampling from the expectations presented in L\u03c0old(\u03b8): L\u03c0old(\u03b8) = E\u03c0old \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) The approximation quality of L\u03c0old(\u03b8) can be described by the following theorem: Proposition 22. [19] \ufffd\ufffdJ(\u03c0\u03b8) \u2212 J(\u03c0old) \u2212 L\u03c0old(\u03b8) \ufffd\ufffd \u2264 C max s KL(\u03c0old \u2225 \u03c0\u03b8)[s] where C is some constant and KL(\u03c0old \u2225 \u03c0\u03b8)[s] is a shorten notation for KL(\u03c0old(a | s) \u2225 \u03c0\u03b8(a | s)). There is an important corollary of proposition 22: J(\u03c0\u03b8) \u2212 J(\u03c0old) \u2265 L\u03c0old(\u03b8) \u2212 C max s KL(\u03c0old \u2225 \u03c0\u03b8)[s] which not only states that expression on the right side represents a lower bound, but also that the optimization procedure \u03b8k+1 = argmax \u03b8 \ufffd L\u03c0\u03b8k (\u03b8) \u2212 C max s KL(\u03c0\u03b8k \u2225 \u03c0\u03b8)[s] \ufffd (47) will yield a policy with guaranteed monotonic improvement31. In practice there are several obstacles which preserve us from obtaining such procedure. First of all, our advantage function estimation is never precise. Secondly, it is hard to estimate precise value of constant C. One last obstacle is that it is not clear how to calculate KL-divergence in its maximal form (with max taken across all states). 30and if MDP is episodic, for terminal states V \u03c0old(sT ) = 0 by de\ufb01nition. 31the maximum of lower bound is non-negative as its value for \u03b8 = \u03b8k equals zero, which causes J(\u03c0k+1)\u2212J(\u03c0k) \u2265 0. 37 In Trust-Region policy optimization [19] the idea of practical algorithm, approximating procedure (47), is analyzed. To address the last issue, the naive approximation is proposed to substitute max with averaging across states32: max s KL(\u03c0old \u2225 \u03c0\u03b8)[s] \u2248 Es\u223cd\u03c0old(s) KL(\u03c0old \u2225 \u03c0\u03b8)[s] The second step of TRPO is to rewrite the task of unconstrained minimization (47) in equivalent constrained (\u00abtrust-region\u00bb) form33 to incorporate the unknown constant C into learning rate: \ufffd L\u03c0old(\u03b8) \u2192 max \u03b8 Es\u223cd(s|\u03c0old) KL(\u03c0old \u2225 \u03c0\u03b8)[s] < C (48) Note that this rewrites an update iteration in terms of optimization methods: while L\u03c0old(\u03b8) is an approximation of true objective J(\u03c0\u03b8) \u2212 J(\u03c0old), the constraint sets the region of trust to the surrogate. Remark that constraint is actually a divergence in policy space, i. e. it is very similar to a metric in the space of distributions while the surrogate is a function of the policy and depends on parameters \u03b8 only through \u03c0\u03b8. To solve the constrained problem (48), the technique from convex optimization is used. Assume that \u03c0old is a current policy and we want to update its parameters \u03b8k. Then the objective of (48) is modeled using \ufb01rst-order Taylor expansion around \u03b8k while constraint is modeled using secondorder 34 Taylor approximation: \ufffd L\u03c0old(\u03b8k + \u03b4\u03b8) \u2248 \u27e8\u2207\u03b8 L\u03c0old(\u03b8)|\u03b8k , \u03b4\u03b8\u27e9 \u2192 max \u03b4\u03b8 Es\u223cd(s|\u03c0old) KL(\u03c0old \u2225 \u03c0\u03b8k+\u03b4\u03b8) \u2248 1 2Es\u223cd(s|\u03c0old)\u03b4\u03b8T \u22072 \u03b8 KL(\u03c0old \u2225 \u03c0\u03b8) \ufffd\ufffd \u03b8k \u03b4\u03b8 < C It turns out, that this model is equivalent to natural policy gradient, discussed in sec. 5.5: Proposition 23. \u22072 \u03b8 KL(\u03c0\u03b8 \u2225 \u03c0old)[s] \ufffd\ufffd \u03b8k = F\u03c0(a|s)(\u03b8) so KL-divergence constraint can be approximated with metric induced by Fisher matrix. Moreover, the gradient of surrogate function is \u2207\u03b8L\u03c0old(\u03b8)|\u03b8k = E\u03c0old \u2207\u03b8\u03c0\u03b8(a | s)|\u03b8k \u03c0old(a | s) A\u03c0old(s, a) = {\u03c0old = \u03c0\u03b8k} = E\u03c0old\u2207\u03b8 log \u03c0\u03b8k(a | s)A\u03c0old(s, a) which is exactly an Actor-Critic gradient. Therefore the formula of update step is given by \u03b4\u03b8 \u221d \u2212F\u03c0(\u03b8)\u22121\u2207\u03b8L\u03c0old(\u03b8) where \u2207\u03b8L\u03c0old(\u03b8) coincides with standard policy gradient, and F\u03c0(\u03b8) is hessian of KL-divergence: F\u03c0(\u03b8) := Es\u223cd\u03c0old(s) \u22072 \u03b8 KL(\u03c0old \u2225 \u03c0\u03b8) \ufffd\ufffd \u03b8k In practical implementations KL-divergence can be Monte-Carlo estimated using collected rollout. The size of roll-out must be signi\ufb01cantly bigger than in Actor-Critic to achieve su\ufb03cient precision of hessian estimation. Then to obtain a direction of optimization step the following system of linear equations F\u03c0(\u03b8)\u03b4\u03b8 = \u2212\u2207\u03b8L\u03c0old(\u03b8) is solved using a conjugate gradients method which is able to work with Hessian-vector multiplication procedure instead of requiring to calculate F\u03c0(\u03b8) explicitly. 32the distribution from which the states come is set to be d\u03c0old(s) for convenience as this is the distribution from which they come in L\u03c0old(\u03b8). 33the unconstrained objective is Lagrange function for constrained form. 34as \ufb01rst-order term is zero. 38 TRPO also accompanies the update step with a line-search procedure which dynamically adjusts step length using standard backtracking heuristic. As TRPO intuitively seeks for policy improvement on each step, the idea is to check whether the lower bound (47) is positive after the biggest step allowed according to KL-constraint and reduce the step size until it becomes positive. Unlike Actor-Critic, TRPO performs extremely expensive complicated update steps but requires relatively small number of iterations in return. Of course, due to many approximations done, the overall procedure is only a resemblance of theoretically-justi\ufb01ed iterations (47) providing improvement guarantees. 5.7. Proximal Policy Optimization (PPO) Proximal Policy Optimization [21] proposes alternative heuristic way of performing lower bound (47) optimization which demonstrated encouraging empirical results. PPO still substitutes max s KL on average, but leaves the surrogate in unconstrained form, suggesting to treat unknown constant C as a hyperparameter: E\u03c0old \ufffd \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) \u2212 C KL(\u03c0old \u2225 \u03c0\u03b8)[s] \ufffd \u2192 max \u03b8 (49) The naive idea would be to straightforwardly optimize (49) as it is equivalent to solving the constraint trust-region task (48). To avoid Hessian-involved computations, one possible option is just to perform one step of \ufb01rst-order gradient optimization of (49). Such algorithm was empirically discovered to perform poorly as importance sampling coe\ufb03cients \u03c0\u03b8(a|s) \u03c0old(a|s) tend to unbounded growth. In PPO it is proposed to cope with this problem in a simple old-fashioned way: by clipping. Let\u2019s denote by r(\u03b8) := \u03c0\u03b8(a | s) \u03c0old(a | s) an importance sampling weight and by rclip(\u03b8) := clip(r(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) its clipped version where \u03f5 \u2208 (0, 1) is a hyperparameter. Then the clipped version of lower bound is: E\u03c0old \ufffd min \ufffd r(\u03b8)A\u03c0old(s, a), rclip(\u03b8)A\u03c0old(s, a) \ufffd \u2212 C KL(\u03c0old \u2225 \u03c0\u03b8)[s] \ufffd \u2192 max \u03b8 (50) Here the minimum operation is introduced to guarantee that the surrogate objective remains a lower bound. Thus the clipping at 1 + \u03f5 may occur only in the case if advantage is positive while clipping at 1 \u2212 \u03f5 may occur if advantage is negative. In both cases, clipping represents a penalty for importance sampling weight r(\u03b8) being too far from 1. The overall procedure suggested by PPO to optimize the \u00abstabilized\u00bb version of lower bound (50) is the following. A roll-out is collected using current policy \u03c0old with some parameters \u03b8. Then the batches of typical size (as for Actor-Critic methods) are sampled from collected roll-out and several steps of SGD optimization of (50) proceed with respect to policy parameters \u03b8. During this process the policy \u03c0old is considered to be \ufb01xed and new interaction steps are not performed, while in implementations there is no need to store old weights \u03b8k since everything required from \u03c0old is to collect transitions and remember the probabilities \u03c0old(a | s). The idea is that during these several steps we may use transitions from the collected roll-out several times. Similar alternative is to perform several epochs of training by passing through roll-out several times, as it is often done in deep learning. Interesting fact discovered by the authors of PPO during ablation studies is that removing KLpenalty term doesn\u2019t a\ufb00ect the overall empirical performance. That is why in many implementations PPO does not include KL-term at all, making the \ufb01nal surrogate objective have a following form: E\u03c0old min \ufffd r(\u03b8)A\u03c0old(s, a), rclip(\u03b8)A\u03c0old(s, a) \ufffd \u2192 max \u03b8 (51) Note that in this form the surrogate is not generally a lower bound and \u00abimprovement guarantees\u00bb intuition is lost. 39 Algorithm 7: Proximal Policy Optimization (PPO) Hyperparameters: B \u2014 batch size, R \u2014 rollout size, n_epochs \u2014 number of epochs, \u03b5 \u2014 clipping parameter, V \u2217 \u03c6 \u2014 critic neural network, \u03c0\u03b8 \u2014 actor neural network, \u03b1 \u2014 critic loss scaling, SGD optimizer. Initialize weights \u03b8, \u03c6 arbitrary On each step: 1. obtain a roll-out of size R using policy \u03c0(\u03b8), storing action probabilities as \u03c0old(a | s). 2. for each transition T from the roll-out compute advantage estimation (detached from computational graph to prevent backpropagation): A\u03c0(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2212 V \u03c0 \u03c6 3. perform n_epochs passes through roll-out using batches of size B; for each batch: \u2022 compute critic target (detached from computational graph to prevent backpropagation): y(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2022 compute critic loss: Loss = 1 B \ufffd T \ufffd y(T ) \u2212 V \u03c0 \u03c6 \ufffd2 \u2022 compute critic gradients: \u2207critic = \u2202 Loss \u2202\u03c6 \u2022 compute importance sampling weights: r\u03b8(T ) = \u03c0\u03b8(a | s) \u03c0old(a | s) \u2022 compute clipped importance sampling weights: rclip \u03b8 (T ) = clip(r\u03b8(T ), 1 \u2212 \u03f5, 1 + \u03f5) \u2022 compute actor gradient: \u2207actor = 1 B \ufffd T \u2207\u03b8 min \ufffd r\u03b8(T )A\u03c0(T ), rclip \u03b8 (T )A\u03c0(T ) \ufffd \u2022 make a step of gradient descent using \u2207actor + \u03b1\u2207critic 40 ",
    "Experiments": "Experiments 6.1. Setup We performed our experiments using custom implementation of discussed algorithms attempting to incorporate best features from di\ufb00erent o\ufb03cial and uno\ufb03cial sources and unifying all algorithms in a single library interface. The full code is available at our github. While custom implementation might not be the most e\ufb03cient, it hinted us several ambiguities in algorithms which are resolved di\ufb00erently in di\ufb00erent sources. We describe these nuances and the choices made for our experiments in appendix A. For each environment we launch several algorithms to train the network with the same architecture with the only exception being the head which is speci\ufb01ed by the algorithm (see table 1). DQN Linear transformation to |A| arbitrary real values Dueling First head: linear transformation to |A| arbitrary real values Second head: linear transformations to an arbitrary scalar Aggregated using dueling architecture formula (17) Categorical |A| linear transformations with softmax to A values Dueling Categorical First head: linear transformation to |A| arbitrary real values Second head: |A| linear transformations to A arbitrary real values Aggregated using dueling architecture formula (32) Quantile |A| linear transformations to A arbitrary real values Dueling Quantile First head: linear transformation to |A| arbitrary real values Second head: |A| linear transformations to A arbitrary real values Aggregated using dueling architecture formula (32) without softmax A2C / PPO Actor head: linear transformation with softmax to |A| values Critic head: linear transformation to scalar value Table 1: Heads used for di\ufb00erent algorithms. Here |A| is the number of actions and A is the chosen number of atoms. For noisy networks all fully-connected layers in the feature extractor and in the head are substituted with noisy layers, doubling the number of their trained parameters. Both usage of noisy layers and the choice of the head in\ufb02uences the total number of parameters trained by the algorithm. As practical tuning of hyperparameters is computationally consuming activity, we set all hyperparameters to their recommended values while trying to share the values of common hyperparameters among algorithms without a\ufb00ecting overall performance. We choose to give each algorithm same amount of interaction steps to provide the fair comparison of their sample e\ufb03ciency. Thus the wall-clock time, number of episodes played and the number of network parameters updates varies for di\ufb00erent algorithms. 6.2. Cartpole Cartpole from OpenAI Gym [2] is considered to be one of the simplest environments for DRL algorithms testing. The state is described with 4 real numbers while action space is two-dimensional discrete. The environment rewards agent with +1 each tick until the episode ends. Poor action choices lead to early termination. The game is considered solved if agent holds for 200 ticks, therefore 200 is maximum reward in this environment. In our \ufb01rst experiment we launch algorithms for 10 000 interaction steps to train a neural network on the Cartpole environment. The network consists of two fully-connected hidden layers with 128 neurons and an algorithm-speci\ufb01c head. We used ReLU for activations. The results of a single launch are provided35 in table 2. 35we didn\u2019t tune hyperparameters for each of the algorithms, so the con\ufb01gurations used might not be optimal. 41 Reached 200 Average reward Average FPS Double DQN 23.0 126.17 95.78 Dueling Double DQN 27.0 121.78 62.65 DQN 33.0 116.27 101.53 Categorical DQN 28.0 110.87 74.95 Prioritized Double DQN 37.0 110.52 85.58 Categorical Prioritized Double DQN 46.0 104.86 66.00 Quantile Prioritized Double DQN 42.0 100.76 68.62 Categorical DQN with target network 44.0 96.08 73.92 Quantile Double DQN 54.0 93.14 75.40 Quantile DQN 70.0 88.12 77.93 Categorical Double DQN 42.0 81.25 70.90 Noisy Quantile Prioritized Dueling DQN 86.0 74.13 21.41 Twin DQN 57.0 71.14 52.51 Noisy Double DQN 67.0 71.06 31.81 Noisy Prioritized Double DQN 94.0 67.34 30.72 Quantile Regression Rainbow 106.0 67.11 21.54 Rainbow 91.0 64.01 20.35 Noisy Quantile Prioritized Double DQN 127.0 63.01 28.27 Noisy Categorical Prioritized Double DQN 63.0 62.04 27.81 PPO with GAE 144.0 53.06 390.53 Noisy Prioritized Dueling Double DQN 180.0 47.52 22.56 PPO 184.0 45.19 412.88 Noisy Categorical Prioritized Dueling Double DQN 428.0 22.09 20.63 A2C 12.30 1048.64 A2C with GAE 11.50 978.00 Table 2: Results on Cartpole for di\ufb00erent algorithms: number of episode when the highest score of 200 was reached, average reward across all played episodes and average number of frames processed in a second (FPS). 6.3. Pong We used Atari Pong environment from OpenAI Gym [2] as our main testbed to study the behaviour of the following algorithms: \u2022 DQN \u2014 Deep Q-learning (sec. 3.2) \u2022 c51 \u2014 Categorical DQN (sec. 4.2) \u2022 QR-DQN \u2014 Quantile Regression DQN (sec. 4.3) \u2022 Rainbow (sec. 4.4) \u2022 A2C \u2014 Advantage Actor Critic (sec. 5.3) extended with GAE (sec. 5.4) \u2022 PPO \u2014 Proximal Policy Optimization (sec. 5.7) extended with GAE (sec. 5.4) In Pong, each episode is split into rounds. Each round ends with player either winning or loosing. The episode ends when the player wins or looses 21 rounds. The reward is given after each round and is +1 for winning and -1 for loosing. Therefore the maximum total reward is 21 and the minimum is -21. Note that the \ufb02ag done indicating episode ending is not provided to the agent after each round but only at the end of full game (consisting of 21-41 rounds). The standard preprocessing for Atari games proposed in DQN [14] was applied to the environment (see table 3). Thus, state space is represented by (84, 84) grayscale pixels input (1 channel with domain [0, 255]). Action space is discrete with |A| = 6 actions. All algorithms were given 1 000 000 interaction steps to train the network with the same feature extractor presented on \ufb01g. 1. The number of trained parameters is presented in table 4. All used hyperparameters are listed in table 7 in appendix B. 42 NoopResetEnv Do nothing \ufb01rst 30 frames of games to imitate the pause between game start and real player reaction. MaxAndSkipEnv Each interaction steps takes 4 frames of the game to allow less frequent switch of action. Max is taken over 4 passed frames to obtain an observation. FireResetEnv Presses \u00abFire\u00bb button at \ufb01rst frame to launch the game, otherwise screen remains frozen. WarpFrame Turns observation to grayscale image of size 84x84. Table 3: Atari Pong preprocessing Algorithm Number of trained parameters DQN 1 681 062 c51 1 834 962 QR-DQN 1 834 962 Rainbow 3 650 410 A2C 1 681 575 PPO 1 681 575 Table 4: Number of trained parameters in Pong experiment. 6.4. Interaction-training trade-o\ufb00 in value-based algorithms There is a common belief that policy gradient algorithms are much faster in terms of computational costs while value-based algorithms are preferable when simulation is expensive because of their sample e\ufb03ciency. This follows from the nature of algorithms, as the fraction \u00abobservations per network updates\u00bb is extremely di\ufb00erent for these two families: indeed, in DQN it is often assumed to perform one network update after each new transitions, while A2C collects about 32-40 observations for only one update. That makes the number of network updates performed during 1M steps interaction process substantially di\ufb00erent and is the main reason of policy gradients speed rate. Also policy gradient algorithms use several threads for parallel simulations (8 in our experiments) while value-based algorithms are formally single-threaded. Yet they can also enjoy multi-threaded interaction, in the simplest form by playing 1 step in all instances of environment and then performing L steps of network optimization [8]. For consistency with single-threaded case it is reasonable to set the value of L to be equal to the number of threads to maintain the same fraction \u00abobservations per network updates\u00bb. However it has been reported that lowering value of L in two or four times can positively a\ufb00ect wall-clock time with some loss of sample e\ufb03ciency, while raising batch size may mitigate this downgrade. The overall impact of such acceleration of value-based algorithms on performance properties is not well studied and may alter their behaviour. In our experiments on Pong it became evident that value-based algorithms perform extensive amount of redundant network optimization steps, absorbing knowledge faster than novel information from new transitions comes in. This reasoning in particular follows from the success of PPO on Pong task which performs more than 10 times less network updates. Vanilla algorithm Accelerated version Threads 1 8 Batch size 32 128 L 1 2 Interactions per update 1 4 Table 5: Setup for value-based acceleration experiment We compared two versions of value-based algorithms: vanilla version, which is single-threaded with standard batch size (32) and L = 1 meaning that each observed transition is followed with one network optimization step, and accelerated version, where 1 interaction step is performed in 8 parallel instances of environment and L is set to be 2 instead of 8 which raises the fraction \u00abob43 ",
    "Results": "Results We compare the results of launch of six algorithms on Pong from two perspectives: sample e\ufb03ciency (\ufb01g. 4) and wall-clock time (\ufb01g. 5). We do not compare \ufb01nal performance of these algorithms as all six algorithms are capable to reach near-maximum \ufb01nal score on Pong given more iterations, while results after 1M iterations on a single launch signi\ufb01cantly depend on chance. All algorithms start with a warm-up session during which they try to explore the environment and 44 Interactions per update Average transitions per second Algorithm vanilla accelerated vanilla accelerated DQN 1 4 55.74 168.43 c51 1 4 44.08 148.76 QR-DQN 1 4 47.46 155.97 Rainbow 1 4 19.30 70.22 A2C 40 656.25 PPO 10.33 327.13 Table 6: Computational e\ufb03ciency of vanilla and accelerated versions. 0 200000 400000 600000 800000 1000000 interaction step 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes Acceleration's influence on sample efficiency DQN accelerated DQN vanilla c51 accelerated c51 vanilla QR-DQN accelerated QR-DQN vanilla Rainbow accelerated Rainbow vanilla Figure 2: Training curves of vanilla and accelerated version of value-based algorithms on 1M steps of Pong. Although accelerated versions perform network updates four times less frequent, the performance degradation is not observed. learn \ufb01rst dependencies how the result of random behaviour can be surpassed. Epsilon-greedy with tuned parameters provides su\ufb03cient amount of exploration for DQN, c51 and QR-DQN whithout slowing down further learning while hyperparameter-free noisy networks are the main reason why Rainbow has substantially longer warm-up. Policy gradient algorithms incorporate exploration strategy in stochasticity of learned policy but underutilization of observed samples leads to almost 1M-frames warm-up for A2C. It can be observed that PPO successfully mitigates this problem by reusing samples thrice. Nevertheless, both PPO and A2C solve Pong relatively quickly after the warm-up stage is over. Value-based algorithm proved to be more computationally costly. QR-DQN and categorical DQN introduce more complicated loss computation, yet their slowdown compared to standard DQN is moderate. On the contrary, Rainbow is substantially slower mainly because of noise generation involvement. Furthermore, combination of noisy networks and prioritized replay results in even less stable training process. We provide loss curves for all six algorithms and statistics for noise magnitude and prioritized replay for Rainbow in appendix C; some additional visualizations of trained algorithms playing episodes of Pong are presented in appendix D. 45 0 200 400 600 800 minutes 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes Acceleration effect on value-based algorithms DQN accelerated DQN vanilla c51 accelerated c51 vanilla QR-DQN accelerated QR-DQN vanilla Rainbow accelerated Rainbow vanilla Figure 3: Training curves of vanilla and accelerated version of value-based algorithms on 1M steps of Pong from wall-clock time. 0 200000 400000 600000 800000 1000000 interaction step 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes Comparing different algorithms on Pong DQN c51 QR-DQN Rainbow A2C PPO Figure 4: Training curves of all algorithms on 1M steps of Pong. 0 50 100 150 200 minutes 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes DQN (1h 38m) c51 (1h 52m) QR-DQN (1h 46m) Rainbow (3h 57m) A2C (0h 25m) PPO (0h 50m) Comparing wall-clock time of different algorithms on Pong DQN c51 QR-DQN Rainbow A2C PPO Figure 5: Training curves of all algorithms on 1M steps of Pong from wall-clock time. 46 ",
    "Discussion": "",
    "Abstract": "Abstract Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many arti\ufb01cial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a \ufb01eld of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justi\ufb01cation, practical limitations and observed empirical properties. 3 1. Introduction During the last several years Deep Reinforcement Learning proved to be a fruitful approach to many arti\ufb01cial intelligence tasks of diverse domains. Breakthrough achievements include reaching human-level performance in such complex games as Go [22], multiplayer Dota [16] and real-time strategy StarCraft II [26]. The generality of DRL framework allows its application in both discrete and continuous domains to solve tasks in robotics and simulated environments [12]. Reinforcement Learning (RL) is usually viewed as general formalization of decision-making task and is deeply connected to dynamic programming, optimal control and game theory. [23] Yet its problem setting makes almost no assumptions about world model or its structure and usually supposes that environment is given to agent in a form of black-box. This allows to apply RL practically in all settings and forces designed algorithms to be adaptive to many kinds of challenges. Latest RL algorithms are usually reported to be transferable from one task to another with no task-speci\ufb01c changes and little to no hyperparameters tuning. As an object of desire is a strategy, i. e. a function mapping agent\u2019s observations to possible actions, reinforcement learning is considered to be a sub\ufb01led of machine learning. But instead of learning from data, as it is established in classical supervised and unsupervised learning problems, the agent learns from experience of interacting with environment. Being more \"natural\" model of learning, this setting causes new challenges, peculiar only to reinforcement learning, such as necessity of exploration integration and the problem of delayed and sparse rewards. The full setup and essential notation are introduced in section 2. Classical Reinforcement Learning research in the last third of previous century developed an extensive theoretical core for modern algorithms to ground on. Several algorithms are known ever since and are able to solve small-scale problems when either environment states can be enumerated (and stored in the memory) or optimal policy can be searched in the space of linear or quadratic functions of state representation features. Although these restrictions are extremely limiting, foundations of classical RL theory underlie modern approaches. These theoretical fundamentals are discussed in sections 3.1 and 5.1\u20135.2. Combining this framework with Deep Learning [5] was popularized by Deep Q-Learning algorithm, introduced in [14], which was able to play any of 57 Atari console games without tweaking network architecture or algorithm hyperparameters. This novel approach was extensively researched and signi\ufb01cantly improved in the following years. The principles of value-based direction in deep reinforcement learning are presented in section 3. One of the key ideas in the recent value-based DRL research is distributional approach, proposed in [1]. Further extending classical theoretical foundations and coming with practical DRL algorithms, it gave birth to distributional reinforcement learning paradigm, which potential is now being actively investigated. Its ideas are described in section 4. Second main direction of DRL research is policy gradient methods, which attempt to directly optimize the objective function, explicitly present in the problem setup. Their application to neural networks involve a series of particular obstacles, which requested specialized optimization techniques. Today they represent a competitive and scalable approach in deep reinforcement learning due to their enormous parallelization potential and continuous domain applicability. Policy gradient methods are discussed in section 5. Despite the wide range of successes, current state-of-art DRL methods still face a number of signi\ufb01cant drawbacks. As training of neural networks requires huge amounts of data, DRL demonstrates unsatisfying results in settings where data generation is expensive. Even in cases where interaction is nearly free (e. g. in simulated environments), DRL algorithms tend to require excessive amounts of iterations, which raise their computational and wall-clock time cost. Furthermore, DRL su\ufb00ers from random initialization and hyperparameters sensitivity, and its optimization process is known to be uncomfortably unstable [9]. Especially embarrassing consequence of these DRL features turned out to be low reproducibility of empirical observations from di\ufb00erent research groups [6]. In section 6, we attempt to launch state-of-art DRL algorithms on several standard testbed environments and discuss practical nuances of their application. 4 2. Reinforcement Learning problem setup 2.1. Assumptions of RL setting Informally, the process of sequential decision-making proceeds as follows. The agent is provided with some initial observation of environment and is required to choose some action from the given set of possibilities. The environment responds by transitioning to another state and generating a reward signal (scalar number), which is considered to be a ground-truth estimation of agent\u2019s performance. The process continues repeatedly with agent making choices of actions from observations and environment responding with next states and reward signals. The only goal of agent is to maximize the cumulative reward. This description of learning process model already introduces several key assumptions. Firstly, the time space is considered to be discrete, as agent interacts with environment sequentially. Secondly, it is assumed that provided environment incorporates some reward function as supervised indicator of success. This is an embodiment of the reward hypothesis, also referred to as Reinforcement Learning hypothesis: Proposition 1. (Reward Hypothesis) [23] \u00abAll of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u00bb Exploitation of this hypothesis draws a line between reinforcement learning and classical machine learning settings, supervised and unsupervised learning. Unlike unsupervised learning, RL assumes supervision, which, similar to labels in data for supervised learning, has a stochastic nature and represents a key source of knowledge. At the same time, no data or \u00abright answer\u00bb is provided to training procedure, which distinguishes RL from standard supervised learning. Moreover, RL is the only machine learning task providing explicit objective function (cumulative reward signal) to maximize, while in supervised and unsupervised setting optimized loss function is usually constructed by engineer and is not \u00abincluded\u00bb in data. The fact that reward signal is incorporated in the environment is considered to be one of the weakest points of RL paradigm, as for many real-life human goals introduction of this scalar reward signal is at the very least unobvious. For practical applications it is also natural to assume that agent\u2019s observations can be represented by some feature vectors, i. e. elements of Rd. The set of possible actions in most practical applications is usually uncomplicated and is either discrete (number of possible actions is \ufb01nite) or can be represented as subset of Rm (almost always [\u22121, 1]m or can be reduced to this case)1. RL algorithms are usually restricted to these two cases, but the mix of two (agent is required to choose both discrete and continuous quantities) can also be considered. The \ufb01nal assumption of RL paradigm is a Markovian property: Proposition 2. (Markovian property) Transitions depend solely on previous state and the last chosen action and are independent of all previous interaction history. Although this assumption may seem overly strong, it actually formalizes the fact that the world modeled by considered environment obeys some general laws. Giving that the agent knows the current state of the world and the laws, it is assumed that it is able to predict the consequences of his actions up to the internal stochasticity of these laws. In practice, both laws and complete state representation is unavailable to agent, which limits its forecasting capability. In the sequel we will work within the setting with one more assumption of full observability. This simpli\ufb01cation supposes that agent can observe complete world state, while in many real-life tasks only a part of observations is actually available. This restriction of RL theory can be removed by considering Partially observable Markov Decision Processes (PoMDP), which basically forces learning algorithms to have some kind of memory mechanism to store previously received observations. Further on we will stick to fully observable case. 1this set is considered to be permanent for all states of environment without any loss of generality as if agent chooses invalid action the world may remain in the same state with zero or negative reward signal or stochastically select some valid action for him. 5 2.2. Environment model Though the de\ufb01nition of Markov Decision Process (MDP) varies from source to source, its essential meaning remains the same. The de\ufb01nition below utilizes several simpli\ufb01cations without loss of generality.2 De\ufb01nition 1. Markov Decision Process (MDP) is a tuple (S, A, T, r, s0), where: \u2022 S \u2286 Rd \u2014 arbitrary set, called the state space. \u2022 A \u2014 a set, called the action space, either \u2013 discrete: |A| < +\u221e, or \u2013 continuous domain: A = [\u22121, 1]m. \u2022 T \u2014 transition probability p(s\u2032 | s, a), where s, s\u2032 \u2208 S, a \u2208 A. \u2022 r : S \u2192 R \u2014 reward function. \u2022 s0 \u2208 S \u2014 starting state. It is important to notice that in the most general case the only things available for RL algorithm beforehand are d (dimension of state space) and action space A. The only possible way of collecting more information for agent is to interact with provided environment and observe s0. It is obvious that the \ufb01rst choice of action a0 will be probably random. While the environment responds by sampling s1 \u223c p(s1 | s0, a0), this distribution, de\ufb01ned in T and considered to be a part of MDP, may be unavailable to agent\u2019s learning procedure. What agent does observe is s1 and reward signal r1 := r(s1) and it is the key information gathered by agent from interaction experience. De\ufb01nition 2. The tuple (st, at, rt+1, st+1) is called transition. Several sequential transitions are usually referred to as roll-out. Full track of observed quantities s0, a0, r1, s1, a1, r2, s2, a2, r3, s3, a3 . . . is called a trajectory. In general case, the trajectory is in\ufb01nite which means that the interaction process is neverending. However, in most practical cases the episodic property holds, which basically means that the interaction will eventually come to some sort of an end3. Formally, it can be simulated by the environment stucking in the last state with zero probability of transitioning to any other state and zero reward signal. Then it is convenient to reset the environment back to s0 to initiate new interaction. One such interaction cycle from s0 till reset, spawning one trajectory of some \ufb01nite length T , is called an episode. Without loss of generality, it can be considered that there exists a set of terminal states S+, which mark the ends of interactions. By convention, transitions (st, at, rt+1, st+1) are accompanied with binary \ufb02ag donet+1 \u2208 {0, 1}, whether st+1 belongs to S+. As timestep t at which the transition was gathered is usually of no importance, transitions are often denoted as (s, a, r\u2032, s\u2032, done) with primes marking the \u00abnext timestep\u00bb. Note that the length of episode T may vary between di\ufb00erent interactions, but the episodic property holds if interaction is guaranteed to end after some \ufb01nite time T max. If this is not the case, the task is called continuing. 2.3. Objective In reinforcement learning, the agent\u2019s goal is to maximize a cumulative reward. In episodic case, this reward can be expressed as a summation of all received reward signals during one episode and 2the reward function is often introduced as stochastic and dependent on action a, i. e. R(r | s, a): S \u00d7 A \u2192 P(R), while instead of \ufb01xed s0 a distribution over S is given. Both extensions can be taken into account in terms of presented de\ufb01nition by extending the state space and incorporating all the uncertainty into transition probability T. 3natural examples include the end of the game or agent\u2019s failure/success in completing some task. 6 is called the return: R := T \ufffd t=1 rt (1) Note that this quantity is formally a random variable, which depends on agent\u2019s choices and the outcomes of environment transitions. As this stochasticity is an inevitable part of interaction process, the underlying distribution from which rt is sampled must be properly introduced to set rigorously the task of return maximization. De\ufb01nition 3. Agent\u2019s algorithm for choosing a by given current state s, which in general can be viewed as distribution \u03c0(a | s) on domain A, is called a policy (strategy). Deterministic policy, when the policy is represented by deterministic function \u03c0 : S \u2192 A, can be viewed as a particular case of stochastic policy with degenerated policy \u03c0(a | s), when agent\u2019s output is still a distribution with zero probability to choose an action other than \u03c0(s). In both cases it is considered that agent sends to environment a sample a \u223c \u03c0(a | s). Note that given some policy \u03c0(a | s) and transition probabilities T, the complete interaction process becomes de\ufb01ned from probabilistic point of view: De\ufb01nition 4. For given MDP and policy \u03c0, the probability of observing s0, a0, s1, a1, s2, a2 . . . is called trajectory distribution and is denoted as T\u03c0: T\u03c0 := \ufffd t=0 p(st+1 | st, at)\u03c0(at | st) It is always substantial to keep track of what policy was used to collect certain transitions (roll-outs and episodes) during the learning procedure, as they are essentially samples from corresponding trajectory distribution. If the policy is modi\ufb01ed in any way, the trajectory distribution changes either. Now when a policy induces a trajectory distribution, it is possible to formulate a task of expected reward maximization: ET\u03c0 T \ufffd t=1 rt \u2192 max \u03c0 To ensure the \ufb01niteness of this expectation and avoid the case when agent is allowed to gather in\ufb01nite reward, limit on absolute value of rt can be assumed: |rt| \u2264 Rmax Together with the limit on episode length T max this restriction guarantees \ufb01niteness of optimal (maximal) expected reward. To extend this intuition to continuing tasks, the reward for each next interaction step is multiplied on some discount coe\ufb03cient \u03b3 \u2208 [0, 1), which is often introduced as part of MDP. This corresponds to the logic that with probability 1 \u2212 \u03b3 agent \u00abdies\u00bb and does not gain any additional reward, which models the paradigm \u00abbetter now than later\u00bb. In practice, this discount factor is set very close to 1. De\ufb01nition 5. For given MDP and policy \u03c0 the discounted expected reward is de\ufb01ned as J(\u03c0) := ET\u03c0 \ufffd t=0 \u03b3trt+1 Reinforcement learning task is to \ufb01nd an optimal policy \u03c0\u2217, which maximizes the discounted expected reward: J(\u03c0) \u2192 max \u03c0 (2) 7 2.4. Value functions Solving reinforcement learning task (2) usually leads to a policy, that maximizes the expected reward not only for starting state s0, but for any state s \u2208 S. This follows from the Markov property: the reward which is yet to be collected from some step t does not depend on previous history and for agent staying at state s the task of behaving optimal is equivalent to maximization of expected reward with current state s as a starting state. This is the particular reason why many reinforcement learning algorithms do not seek only optimal policy, but additional information about usefulness of each state. De\ufb01nition 6. For given MDP and policy \u03c0 the value function under policy \u03c0 is de\ufb01ned as V \u03c0(s) := ET\u03c0|s0=s \ufffd t=0 \u03b3trt+1 This value function estimates how good it is for agent utilizing strategy \u03c0 to visit state s and generalizes the notion of discounted expected reward J(\u03c0) that corresponds to V \u03c0(s0). As value function can be induced by any policy, value function V \u03c0\u2217(s) under optimal policy \u03c0\u2217 can also be considered. By convention4, it is denoted as V \u2217(s) and is called an optimal value function. Obtaining optimal value function V \u2217(s) doesn\u2019t provide enough information to reconstruct some optimal policy \u03c0\u2217 due to unknown world dynamics, i. e. transition probabilities. In other words, being blind to what state s may be the environment\u2019s response on certain action in a given state makes knowing optimal value function unhelpful. This intuition suggests to introduce a similar notion comprising more information: De\ufb01nition 7. For given MDP and policy \u03c0 the quality function (Q-function) under policy \u03c0 is de\ufb01ned as Q\u03c0(s, a) := ET\u03c0|s0=s,a0=a \ufffd t=0 \u03b3trt+1 It directly follows from the de\ufb01nitions that these two functions are deeply interconnected: Q\u03c0(s, a) = Es\u2032\u223cp(s\u2032|s,a) [r(s\u2032) + \u03b3V \u03c0(s\u2032)] (3) V \u03c0(s) = Ea\u223c\u03c0(a|s)Q\u03c0(s, a) (4) The notion of optimal Q-function Q\u2217(s, a) can be introduced analogically. But, unlike value function, obtaining Q\u2217(s, a) actually means solving a reinforcement learning task: indeed, Proposition 3. If Q\u2217(s, a) is a quality function under some optimal policy, then \u03c0\u2217(s) = argmax a Q\u2217(s, a) is an optimal policy. This result implies that instead of searching for optimal policy \u03c0\u2217, an agent can search for optimal Q-function and derive the policy from it. Proposition 4. For any MDP existence of optimal policy leads to existence of deterministic optimal policy. 4though optimal policy may not be unique, the value functions under any optimal policy that behaves optimally from any given state (not only s0) coincide. Yet, optimal policy may not know optimal behaviour for some states if it knows how to avoid them with probability 1. 8 2.5. Classes of algorithms Reinforcement learning algorithms are presented in a form of computational procedures specifying a strategy of collecting interaction experience and obtaining a policy with as higher J(\u03c0) as possible. They rarely include a stopping criterion like in classic optimization methods as the stochasticity of given setting prevents any reasonable veri\ufb01cation of optimality; usually the number of iterations to perform is determined by the amount of computational resources. All reinforcement learning algorithms can be roughly divided into four5 classes: \u2022 meta-heuristics: this class of algorithms treats the task as black-box optimization with zerothorder oracle. They usually generate a set of policies \u03c01 . . . \u03c0P and launch several episodes of interaction for each to determine best and worst policies according to average return. After that they try to construct more optimal policies using evolutionary or advanced random search techniques [17]. \u2022 policy gradient: these algorithms directly optimize (2), trying to obtain \u03c0\u2217 and no additional information about MDP, using approximate estimations of gradient with respect to policy parameters. They consider RL task as an optimization with stochastic \ufb01rst-order oracle and make use of interaction structure to lower the variance of gradient estimations. They will be discussed in sec. 5. \u2022 value-based algorithms construct optimal policy implicitly by gaining an approximation of optimal Q-function Q\u2217(s, a) using dynamic programming. In DRL, Q-function is represented with neural network and an approximate dynamic programming is performed using reduction to supervised learning. This framework will be discussed in sec. 3 and 4. \u2022 model-based algorithms exploit learned or given world dynamics, i. e. distributions p(s\u2032 | s, a) from T. The class of algorithms to work with when the model is explicitly provided is represented by such algorithms as Monte-Carlo Tree Search; if not, it is possible to imitate the world dynamics by learning the outputs of black box from interaction experience [10]. 2.6. Measurements of performance Achieved performance (score) from the point of average cumulative reward is not the only one measure of RL algorithm quality. When speaking of real-life robots, the required number of simulated episodes is always the biggest concern. It is usually measured in terms of interaction steps (where step is one transition performed by environment) and is referred to as sample e\ufb03ciency. When the simulation is more or less cheap, RL algorithms can be viewed as a special kind of optimization procedures. In this case, the \ufb01nal performance of the found policy is opposed to required computational resources, measured by wall-clock time. In most cases RL algorithms can be expected to \ufb01nd better policy after more iterations, but the amount of these iterations tend to be unjusti\ufb01ed. The ratio between amount of interactions and required wall-clock time for one update of policy varies signi\ufb01cantly for di\ufb00erent algorithms. It is well-known that model-based algorithms tend to have the greatest sample-e\ufb03ciency at the cost of expensive update iterations, while evolutionary algorithms require excessive amounts of interactions while providing massive resources for parallelization and reduction of wall-clock time. Value-based and policy gradient algorithms, which will be the focus of our further discussion, are known to lie somewhere in between. 5in many sources evolutionary algorithms are bypassed in discussion as they do not utilize the structure of RL task in any way. 9 3. Value-based algorithms 3.1. Temporal Di\ufb00erence learning In this section we consider temporal di\ufb00erence learning algorithm [23, Chapter 6], which is a classical Reinforcement Learning method in the base of modern value-based approach in DRL. The \ufb01rst idea behind this algorithm is to search for optimal Q-function Q\u2217(s, a) by solving a system of recursive equations which can be derived by recalling interconnection between Q-function and value function (3): Q\u03c0(s, a) = Es\u2032\u223cp(s\u2032|s,a) [r(s\u2032) + \u03b3V \u03c0(s\u2032)] = = {using (4)} = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3Ea\u2032\u223c\u03c0(a\u2032|s\u2032)Q\u03c0(s\u2032, a\u2032) \ufffd This equation, named Bellman equation, remains true for value functions under any policies including optimal policy \u03c0\u2217: Q\u2217(s, a) = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3Ea\u2032\u223c\u03c0(a\u2032|s\u2032)Q\u2217(s\u2032, a\u2032) \ufffd (5) Recalling proposition 3, optimal (deterministic) policy can be represented as \u03c0\u2217(s) = argmax a Q\u2217(s, a). Substituting this for \u03c0\u2217(s) in (5), we obtain fundamental Bellman optimality equation: Proposition 5. (Bellman optimality equation) Q\u2217(s, a) = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032) \ufffd (6) The straightforward utilization of this result is as follows. Consider the tabular case, when both state space S and action space A are \ufb01nite (and small enough to be listed in computer memory). Let us also assume for now that transition probabilities are available to training procedure. Then Q\u2217(s, a) : S \u00d7 A \u2192 R can be represented as a \ufb01nite table with |S||A| numbers. In this case (6) just gives a set of |S||A| equations for this table to satisfy. Addressing the values of the table as unknown variables, this system of equations can be solved using basic point iteration method: let Q\u2217 0(s, a) be initial arbitrary values of table (with the only exception that for terminal states s \u2208 S+, if any, Q\u2217 0(s, a) = 0 for all actions a). On each iteration t the table is updated by substituting current values of the table to the right side of equation until the process converges: Q\u2217 t+1(s, a) = Es\u2032\u223cp(s\u2032|s,a) \ufffd r(s\u2032) + \u03b3 max a\u2032 Q\u2217 t (s\u2032, a\u2032) \ufffd (7) This straightforward approach of learning the optimal Q-function, named Q-learning, has been extensively studied in classical Reinforcement Learning. One of the central results is presented in the following convergence theorem: Proposition 6. Let by B denote an operator (S \u00d7 A \u2192 R) \u2192 (S \u00d7 A \u2192 R), updating Q\u2217 t as in (7): Q\u2217 t+1 = BQ\u2217 t for all state-action pairs s, a. Then B is a contraction mapping, i. .e. for any two tables Q1, Q2 \u2208 (S \u00d7 A \u2192 R) \u2225BQ1 \u2212 BQ2\u2225\u221e \u2264 \u03b3\u2225Q1 \u2212 Q2\u2225\u221e Therefore, there is a unique \ufb01xed point of the system of equations (7) and the point iteration method converges to it. The contraction mapping property is actually of high importance. It demonstrates that the point iteration algorithm converges with exponential speed and requires small amount of iterations. As the true Q\u2217 is a \ufb01xed point of (6), the algorithm is guaranteed to yield a correct answer. The trick is 10 that each iteration demands full pass across all state-action pairs and exact computation of expectations over transition probabilities. In general case, these expectations can\u2019t be explicitly computed. Instead, agent is restricted to samples from transition probabilities gained during some interaction experience. Temporal Di\ufb00erence (TD)6 algorithm proposes to collect this data using \u03c0t = argmax a Q\u2217 t (s, a) \u2248 \u03c0\u2217 and after each gathered transition (st, at, rt+1, st+1) update only one cell of the table: Q\u2217 t+1(s, a) = \uf8f1 \uf8f2 \uf8f3 (1 \u2212 \u03b1t)Q\u2217 t (s, a) + \u03b1t \ufffd rt+1 + \u03b3 max a\u2032 Q\u2217 t (st+1, a\u2032) \ufffd if s = st, a = at Q\u2217 t (s, a) else (8) where \u03b1t \u2208 (0, 1) plays the role of exponential smoothing parameter for estimating expectation Es\u2032\u223cp(s\u2032|st,at)(\u00b7) from samples. Two key ideas are introduced in the update formula (8): exponential smoothing instead of exact expectation computation and cell by cell updates instead of updating full table at once. Both are required to settle Q-learning algorithm for online application. As the set S+ of terminal states in online setting is usually unknown beforehand, a slight modi\ufb01cation of update (8) is used. If observed next state s\u2032 turns out to be terminal (recall the convention to denote this by \ufb02ag done), its value function is known to be equal to zero: V \u2217(s\u2032) = max a\u2032 Q\u2217(s\u2032, a\u2032) = 0 This knowledge is embedded in the update rule (8) by multiplying max a\u2032 Q\u2217 t (st+1, a\u2032) on (1 \u2212 donet+1). For the sake of shortness, this factor is often omitted but should be always present in implementations. Second important note about formula (8) is that it can be rewritten in the following equivalent way: Q\u2217 t+1(s, a) = \uf8f1 \uf8f2 \uf8f3 Q\u2217 t (s, a) + \u03b1t \ufffd rt+1 + \u03b3 max a\u2032 Q\u2217 t (st+1, a\u2032) \u2212 Q\u2217 t (s, a) \ufffd if s = st, a = at Q\u2217 t (s, a) else (9) The expression in the brackets, referred to as temporal di\ufb00erence, represents a di\ufb00erence between Q-value Q\u2217 t (s, a) and its one-step approximation rt+1 + \u03b3 max a\u2032 Q\u2217 t (st+1, a\u2032), which must be zero in expectation for true optimal Q-function. The idea of exponential smoothing allows us to formulate \ufb01rst practical algorithm which can work in the tabular case with unknown world dynamics: Algorithm 1: Temporal Di\ufb00erence algorithm Hyperparameters: \u03b1t \u2208 (0, 1) Initialize Q\u2217(s, a) arbitrary On each interaction step: 1. select a = argmax a Q\u2217(s, a) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. update table: Q\u2217(s, a) \u2190 Q\u2217(s, a) + \u03b1t \ufffd r\u2032 + (1 \u2212 done)\u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032) \u2212 Q\u2217(s, a) \ufffd It turns out that under several assumptions on state visitation during interaction process this procedure holds similar properties in terms of convergence guarantees, which are stated by the following theorem: 6also known as TD(0) due to theoretical generalizations 11 Proposition 7. [28] Let\u2019s de\ufb01ne et(s, a) = \ufffd \u03b1t (s, a) is updated on step t 0 otherwise Then if for every state-action pair (s, a) +\u221e \ufffd t et(s, a) = \u221e +\u221e \ufffd t et(s, a)2 < \u221e the algorithm 1 converges to optimal Q\u2217 with probability 1. This theorem states that basic policy iteration method can be actually applied online in the way proposed by TD algorithm, but demands \u00abenough exploration\u00bb from the strategy of interacting with MDP during training. Satisfying this demand remains a unique and common problem of reinforcement learning. The widespread kludge is \u03b5-greedy strategy which basically suggests to choose random action instead of a = argmax a Q\u2217(s, a) with probability \u03b5t. The probability \u03b5t is usually set close to 1 during \ufb01rst interaction iterations and scheduled to decrease to a constant close to 0. This heuristic makes agent visit all states with non-zero probabilities independent of what current approximation Q\u2217(s, a) suggests. The main practical issue with Temporal Di\ufb00erence algorithm is that it requires table Q\u2217(s, a) to be explicitly stored in memory, which is impossible for MDP with high state space complexity. This limitation substantially restricted its applicability until its combination with deep neural network was proposed. 3.2. Deep Q-learning (DQN) Utilization of neural nets to model either a policy or a Q-function frees from constructing taskspeci\ufb01c features and opens possibilities of applying RL algorithms to complex tasks, e. g. tasks with images as input. Video games are classical example of such tasks where raw pixels of screen are provided as state representation and, correspondingly, as input to either policy or Q-function. Main idea of Deep Q-learning [14] is to adapt Temporal Di\ufb00erence algorithm so that update formula (9) would be equivalent to gradient descent step for training a neural network to solve a certain regression task. Indeed, it can be noticed that the exponential smoothing parameter \u03b1t resembles learning rate of \ufb01rst-order gradient optimization procedures, while the exploration conditions from theorem 7 look identical to restrictions on learning rate of stochastic gradient descent. The key hint is that (9) is actually a gradient descent step in the parameter space of the table functions family: Q\u2217(s, a, \u03b8) = \u03b8s,a where all \u03b8s,a form a vector of parameters \u03b8 \u2208 R|S||A|. To unravel this fact, it is convenient to introduce some notation from regression tasks. First, let\u2019s denote by y the target of our regression task, i. e. the quantity that our model is trying to predict: y(s, a) := r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8) (10) where s\u2032 is a sample from p(s\u2032 | s, a) and s, a is input data. In this notation (9) is equivalent to: \u03b8t+1 = \u03b8t + \u03b1t [y(s, a) \u2212 Q\u2217(s, a, \u03b8t)] es,a where we multiplied scalar value \u03b1t [y(s, a) \u2212 Q\u2217(s, a, \u03b8t)] on the following vector es,a es,a i,j := \ufffd 1 (i, j) = (s, a) 0 (i, j) \u0338= (s, a) to formulate an update of only one component of \u03b8 in a vector form. By this we transitioned to update in parameter space using Q\u2217(s, a, \u03b8) = \u03b8s,a. Remark that for table functions family the 12 derivative of Q\u2217(s, a, \u03b8) by \u03b8 for given input s, a is its one-hot encoding, i. e. exactly es,a: \u2202Q\u2217(s, a, \u03b8) \u2202\u03b8 = es,a (11) The statement now is that this formula is a gradient descent update for regression with input s, a, target y(s, a) and MSE loss function: Loss(y(s, a), Q\u2217(s, a, \u03b8t)) = (Q\u2217(s, a, \u03b8t) \u2212 y(s, a))2 (12) Indeed: \u03b8t+1 = \u03b8t + \u03b1t [y(s, a) \u2212 Q\u2217(s, a, \u03b8t)] es,a = {(12)} = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202Q\u2217 es,a {(11)} = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202Q\u2217 \u2202Q\u2217(s, a, \u03b8t) \u2202\u03b8 = {chain rule} = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202\u03b8 The obtained result is evidently a gradient descent step formula to minimize MSE loss function with target (10): \u03b8t+1 = \u03b8t \u2212 \u03b1t \u2202 Loss(y, Q\u2217(s, a, \u03b8t)) \u2202\u03b8 (13) It is important that dependence of y from \u03b8 is ignored during gradient computation (otherwise the chain rule application with y being dependent on \u03b8 is incorrect). On each step of temporal difference algorithm new target y is constructed using current Q-function approximation, and a new regression task with this target is set. For this \ufb01xed target one MSE optimization step is done according to (13), and on the next step a new regression task is de\ufb01ned. Though during each step the target is considered to represent some ground truth like it is in supervised learning, here it provides a direction of optimization and because of this reason is sometimes called a guess. Notice that representation (13) is equivalent to standard TD update (9) with all theoretical results remaining while the parametric family Q(s, a, \u03b8) is a table functions family. At the same time, (13) can be formally applied to any parametric function family including neural networks. It must be taken into account that this transition is not rigorous and all theoretical guarantees provided by theorem 7 are lost at this moment. Further on we assume that optimal Q-function is approximated with neural network Q\u2217 \u03b8(s, a) with parameters \u03b8. Note that for discrete action space case this network may take only s as input and output |A| numbers representing Q\u2217 \u03b8(s, a1) . . . Q\u2217 \u03b8(s, a|A|), which allows to \ufb01nd an optimal action in a given state s with a single forward pass through the net. Therefore target y for given transition (s, a, r\u2032, s\u2032, done) can be computed with one forward pass and optimization step can be performed in one more forward7 and one backward pass. Small issue with this straightforward approach is that, of course, it is impractical to train neural networks with batches of size 1. In [14] it is proposed to use experience replay to store all collected transitions (s, a, r\u2032, s\u2032, done) as data samples and on each iteration sample a batch of standard for neural networks training size. As usual, the loss function is assumed to be an average of losses for each transition from the batch. This utilization of previously experienced transitions is legit because TD algorithm is known to be an o\ufb00-policy algorithm, which means it can work with arbitrary transitions gathered by any agent\u2019s interaction experience. One more important bene\ufb01t from experience replay is sample decorrelation as consecutive transitions from interaction are often similar to each other since agent usually locates at the particular part of MDP. Though empirical results of described algorithm turned out to be promising, the behaviour of Q\u2217 \u03b8 values indicated the instability of learning process. Reconstruction of target after each optimization step led to so-called compound error when approximation error propagated from the closeto-terminal states to the starting in avalanche manner and could lead to guess being 106 and more times bigger than the true Q\u2217 value. To address this problem, [14] introduced a kludge known as target network, which basic idea is to solve \ufb01xed regression problem for K > 1 steps, i. .e. recompute target every K-th step instead of each. 7in implementations it is possible to combine s and s\u2032 in one batch and perform these two forward passes \u00abat once\u00bb. 13 To avoid target recomputation for the whole experience replay, the copy of neural network Q\u2217 \u03b8 is stored, called the target network. Its architecture is the same while weights \u03b8\u2212 are a copy of Q\u2217 \u03b8 from the moment of last target recomputation8 and its main purpose is to generate targets y for given current batch. Combining all things together and adding \u03b5-greedy strategy to facilitate exploration, we obtain classic DQN algorithm: Algorithm 2: Deep Q-learning (DQN) Hyperparameters: B \u2014 batch size, K \u2014 target network update frequency, \u03b5(t) \u2208 (0, 1] \u2014 greedy exploration parameter, Q\u2217 \u03b8 \u2014 neural network, SGD optimizer. Initialize weights of \u03b8 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 On each interaction step: 1. select a randomly with probability \u03b5(t), else a = argmax a Q\u2217 \u03b8(s, a) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. add observed transition to experience replay 4. sample batch of size B from experience replay 5. for each transition T from the batch compute target: y(T ) = r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212) 6. compute loss: Loss = 1 B \ufffd T (Q\u2217(s, a, \u03b8) \u2212 y(T ))2 7. make a step of gradient descent using \u2202 Loss \u2202\u03b8 8. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 3.3. Double DQN Although target network successfully prevented Q\u2217 \u03b8 from unbounded growth and empirically stabilized learning process, the values of Q\u2217 \u03b8 on many domains were evident to tend to overestimation. The problem is presumed to reside in max operation in target construction formula (10): y = r(s\u2032) + \u03b3 max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212) During this estimation max shifts Q-value estimation towards either to those actions that led to high reward due to luck or to the actions with overestimating approximation error. The solution proposed in [25] is based on idea of separating action selection and action evaluation to carry out each of these operations using its own approximation of Q\u2217: max a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212) = Q\u2217(s\u2032, argmax a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212), \u03b8\u2212) \u2248 \u2248 Q\u2217(s\u2032, argmax a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8\u2212 1 ), \u03b8\u2212 2 ) The simplest, but expensive, implementation of this idea is to run two independent DQN (\u00abTwin DQN\u00bb) algorithms and use the twin network to evaluate actions: y1 = r(s\u2032) + \u03b3Q\u2217 1(s\u2032, argmax a\u2032 Q\u2217 2(s\u2032, a\u2032, \u03b8\u2212 2 ), \u03b8\u2212 1 ) 8alternative, but more computationally expensive option, is to update target network weights on each step using exponential smoothing 14 y2 = r(s\u2032) + \u03b3Q\u2217 2(s\u2032, argmax a\u2032 Q\u2217 1(s\u2032, a\u2032, \u03b8\u2212 1 ), \u03b8\u2212 2 ) Intuitively, each Q-function here may prefer lucky or overestimated actions, but the other Q-function judges them according to its own luck and approximation error, which may be as underestimating as overestimating. Ideally these two DQNs should not share interaction experience to achieve that, which makes such algorithm twice as expensive both in terms of computational cost and sample e\ufb03ciency. Double DQN [25] is more compromised option which suggests to use current weights of network \u03b8 for action selection and target network weights \u03b8\u2212 for action evaluation, assuming that when the target network update frequency K is big enough these two networks are su\ufb03ciently di\ufb00erent: y = r(s\u2032) + \u03b3Q\u2217(s\u2032, argmax a\u2032 Q\u2217(s\u2032, a\u2032, \u03b8), \u03b8\u2212) 3.4. Dueling DQN Another issue with DQN algorithm 2 emerges when a huge part of considered MDP consists of states of low optimal value V \u2217(s), which is an often case. The problem is that when the agent visits unpromising state instead of lowering its value V \u2217(s) it remembers only low pay-o\ufb00 for performing some action a in it by updating Q\u2217(s, a). This leads to regular returns to this state during future interactions until all actions prove to be unpromising and all Q\u2217(s, a) are updated. The problem gets worse when the cardinality of action space is high or there are many similar actions in action space. One bene\ufb01t of deep reinforcement learning is that we are able to facilitate generalization across actions by specifying the architecture of neural network. To do so, we need to encourage the learning of V \u2217(s) from updates of Q\u2217(s, a). The idea of dueling architecture [27] is to incorporate approximation of V \u2217(s) explicitly in computational graph. For that purpose we need the de\ufb01nition of advantage function: De\ufb01nition 8. For given MDP and policy \u03c0 the advantage function under policy \u03c0 is de\ufb01ned as A\u03c0(s, a) := Q\u03c0(s, a) \u2212 V \u03c0(s) (14) Advantage function is evidently interconnected with Q-function and value function and actually shows the relative advantage of selecting action a comparing to average performance of the policy. If for some state A\u03c0(s, a) > 0, then modifying \u03c0 to select a more often in this particular state will lead to better policy as its average return will become bigger than initial V \u03c0(s). This follows from the following property of arbitrary advantage function: Ea\u223c\u03c0(a|s)A\u03c0(s, a) = Ea\u223c\u03c0(a|s) [Q\u03c0(s, a) \u2212 V \u03c0(s)] = = Ea\u223c\u03c0(a|s)Q\u03c0(s, a) \u2212 V \u03c0(s) = {using (4)} = V \u03c0(s) \u2212 V \u03c0(s) = 0 (15) De\ufb01nition of optimal advantage function A\u2217(s, a) is analogous and allows us to reformulate Q\u2217(s, a) in terms of V \u2217(s) and A\u2217(s, a): Q\u2217(s, a) = V \u2217(s) + A\u2217(s, a) (16) Straightforward utilization of this decomposition is following: after several feature extracting layers the network is joined with two heads, one outputting single scalar V \u2217(s) and one outputting |A| numbers A\u2217(s, a) like it was done in DQN for Q-function. After that this scalar value estimation is added to all components of A\u2217(s, a) in order to obtain Q\u2217(s, a) according to (16). The problem with this naive approach is that due to (15) advantage function can not be arbitrary and must hold the property (15) for Q\u2217(s, a) to be identi\ufb01able. This restriction (15) on advantage function can be simpli\ufb01ed for the case when optimal policy is 15 induced by optimal Q-function: 0 = Ea\u223c\u03c0\u2217(a|s)Q\u2217(s, a) \u2212 V \u2217(s) = = Q\u2217(s, argmax a Q\u2217(s, a)) \u2212 V \u2217(s) = = max a Q\u2217(s, a) \u2212 V \u2217(s) = = max a [Q\u2217(s, a) \u2212 V \u2217(s)] = = max a A\u2217(s, a) This condition can be easily satis\ufb01ed in computational graph by subtracting max a A\u2217(s, a) from advantage head. This will be equivalent to the following formula of dueling DQN: Q\u2217(s, a) = V \u2217(s) + A\u2217(s, a) \u2212 max a A\u2217(s, a) (17) The interesting nuance of this improvement is that after evaluation on Atari-57 authors discovered that substituting max operation in (17) with averaging across actions led to better results (while usage of unidenti\ufb01able formula (16) led to poor performance). Although gradients can be backpropagated through both operation and formula (17) seems theoretically justi\ufb01ed, in practical implementations averaging instead of maximum is widespread. 3.5. Noisy DQN By default, DQN algorithm does not concern the exploration problem and is always augmented with \u03b5-greedy strategy to force agent to discover new states. This baseline exploration strategy su\ufb00ers from being extremely hyperparameter-sensitive as early decrease of \u03b5(t) to close to zero values may lead to stucking in local optima, when agent is unable to explore new options due to imperfect Q\u2217, while high values of \u03b5(t) force agent to behave randomly for excessive amount of episodes, which slows down learning. In other words, \u03b5-greedy strategy transfers responsibility to solve exploration-exploitation trade-o\ufb00 on engineer. The key reason why \u03b5-greedy exploration strategy is relatively primitive is that exploration priority does not depend on current state. Intuitively, the choice whether to exploit knowledge by selecting approximately optimal action or to explore MDP by selecting some other depends on how explored the current state s is. Discovering a new part of state space after any amount of interaction probably indicates that random actions are good to try there, while close-to-initial states will probably be su\ufb03ciently explored after several \ufb01rst episodes. In \u03b5-greedy strategy agent selects action using deterministic Q\u2217(s, a, \u03b8) and only afterwards injects state-independent noise in a form of \u03b5(t) probability of choosing random action. Noisy networks [4] were proposed as a simple extension of DQN to provide state-dependent and parameterfree exploration by injecting noise of trainable volume to all (or most9) nodes in computational graph. Let a linear layer with m inputs and n outputs in q-network perform the following computation: y(x) = W x + b where x \u2208 Rm is input, W \u2208 Rn\u00d7m \u2014 weights matrix, b \u2208 Rm \u2014 bias. In noisy layers it is proposed to substitute deterministic parameters with samples from N (\u00b5, \u03c3) where \u00b5, \u03c3 are trained with gradient descent10. On the forward pass through the noisy layer we sample \u03b5W \u223c N (0, Inm\u00d7nm), \u03b5b \u223c N (0, In\u00d7n) and then compute W = (\u00b5W + \u03c3W \u2299 \u03b5W ) b = (\u00b5b + \u03c3b \u2299 \u03b5b) y(x) = W x + b where \u2299 denotes element-wise multiplication, \u00b5W , \u03c3W \u2208 Rn\u00d7m, \u00b5b, \u03c3b \u2208 Rn \u2014 trainable parameters of the layer. Note that the number of parameters for such layers is doubled comparing to ordinary layers. 9usually it is not injected in very \ufb01rst layers responsible for feature extraction like convolutional layers in networks for images as input. 10using standard reparametrization trick 16 As the output of q-network now becomes a random variable, loss value becomes a random variable too. Like in similar models for supervised learning, on each step an expectation of loss function over noise is minimized: E\u03b5 Loss(\u03b8, \u03b5) \u2192 min \u03b8 The gradient in this setting can be estimated using Monte-Carlo: \u2207\u03b8E\u03b5 Loss(\u03b8, \u03b5) = E\u03b5\u2207\u03b8 Loss(\u03b8, \u03b5) \u2248 \u2207\u03b8 Loss(\u03b8, \u03b5) \u03b5 \u223c N (0, I) It can be seen that amount of noise actually in\ufb02icting output of network may vary for di\ufb00erent inputs, i. e. for di\ufb00erent states. There are no guarantees that this amount will reduce as the interaction proceeds; the behaviour of average magnitude of noise injected in the network with time is reported to be extremely sensitive to initialization of \u03c3W , \u03c3b and vary from MDP to MDP. One technical issue with noisy layers is that on each pass an excessive amount (by the number of network parameters) of noise samples is required. This may substantially reduce computational e\ufb03ciency of forward pass through the network. For optimization purposes it is proposed to obtain noise for weights matrices in the following way: sample just n + m noise samples \u03b51 W \u223c N (0, Im\u00d7m), \u03b52 W \u223c N (0, In\u00d7n) and acquire matrix noise in a factorized form: \u03b5W = f(\u03b51 W )f(\u03b52 W )T where f is a scaling function, e. g. f(x) = sign(x) \ufffd |x|. The bene\ufb01t of this procedure is that it requires m + n samples instead of mn, but sacri\ufb01ces the interlayer independence of noise. 3.6. Prioritized experience replay In DQN each batch of transitions is sampled from experience replay using uniform distribution, treating collected data as equally prioritized. In such scheme states for each update come from the same distribution as they come from interaction experience (except that they become decorellated), which agrees with TD algorithm as the basement of DQN. Intuitively observed transitions vary in their importance. At the beginning of training most guesses tend to be more or less random as they rely on arbitrarily initialized Q\u2217 \u03b8 and the only source of trusted information are transitions with non-zero received reward, especially near terminal states where V \u2217 \u03b8 (s\u2032) is known to be equal to 0. In the midway of training, most of experience replay is \ufb01lled with the memory of interaction within well-learned part of MDP while the most crucial information is contained in transitions where agent explored new promising areas and gained novel reward yet to be propagated through Bellman equation. All these signi\ufb01cant transitions are drowned in collected data and rarely appear in sampled batches. The central idea of prioritized experience replay [18] is that priority of some transition T = (s, a, r\u2032, s\u2032, done) is proportional to temporal di\ufb00erence: \u03c1(T ) := y(T ) \u2212 Q\u2217(s, a, \u03b8) = \ufffd Loss(y(T ), Q\u2217(s, a, \u03b8)) (18) Using these priorities as proxy of transition importances, sampling from experience replay proceeds using following probabilities: P(T ) \u221d \u03c1(T )\u03b1 where hyperparameter \u03b1 \u2208 R+ controls the degree to which the sampling weights are sparsi\ufb01ed: the case \u03b1 = 0 corresponds to uniform sampling distribution while \u03b1 = +\u221e is equivalent to greedy sampling of transitions with highest priority. The problem with (18) claim is that each transition\u2019s priority changes after each network update. As it is impractical to recalculate loss for the whole data after each step, some simpli\ufb01cations must be put up with. The straightforward option is to update priority only for sampled transitions in the current batch. New transitions can be added to experience replay with highest priority, i. e. max T \u03c1(T )11. Second debatable issue of prioritized replay is that it actually substitutes loss function of DQN updates, which assumed uniform sampling of visited states to ensure they come from state visitation distribution: ET \u223cUniform Loss(T ) \u2192 min \u03b8 11which can be computed online with O(1) complexity 17 While it is not clear what distribution is better to sample from to ensure exploration restrictions of theorem 7, prioritized experienced replay changes this distribution in uncontrollable way. Despite its fruitfulness at the beginning and midway of training process, this distribution shift may destabilize learning close to the end and make algorithm stuck with locally optimal policy. Since formally this issue is about estimating an expectation over one probability with preference to sample from another one, the standard technique called importance sampling can be used as countermeasure: ET \u223cUniform Loss(T ) = M \ufffd i=0 1 M Loss(Ti) = = M \ufffd i=0 P(Ti) 1 MP(Ti) Loss(Ti) = = ET \u223cP(T ) 1 MP(T ) Loss(T ) where M is a number of transitions stored in experience replay memory. Importance sampling implies that we can avoid distribution shift that introduces undesired bias by making smaller gradient updates for signi\ufb01cant transitions which now appear in the batches with higher frequency. The price for bias elimination is that importance sampling weights lower prioritization e\ufb00ect by slowing down learning of highlighted new information. This duality resembles trade-o\ufb00 between bias and variance, but important moment here is that distribution shift does not cause any seeming issues at the beginning of training when agent behaves close to random and do not produce valid state visitation distribution anyway. The idea proposed in [18] based on this intuition is to anneal the importance sampling weights so they correct bias properly only towards the end of training procedure. LossprioritizedER = ET \u223cP(T ) \ufffd 1 BP(T ) \ufffd\u03b2(t) Loss(T ) where \u03b2(t) \u2208 [0, 1] and approaches 112 as more interaction steps are executed. If \u03b2(t) is set to 0, no bias correction is held, while \u03b2(t) = 1 corresponds to unbiased loss function, i. e. equivalent to sampling from uniform distribution. The most signi\ufb01cant and obvious drawback of prioritized experience replay approach is that it introduces additional hyperparameters. Although \u03b1 represents one number, algorithm\u2019s behaviour may turn out to be sensitive to its choosing, and \u03b2(t) must be designed by engineer as some scheduled motion from something near 0 to 1, and its well-turned selection may require inaccessible knowledge about how many steps it will take for algorithm to \u00abwarm up\u00bb. 3.7. Multi-step DQN One more widespread modi\ufb01cation of Q-learning in RL community is substituting one-step approximation present in Bellman optimality equation (6) with N-step: Proposition 8. (N-step Bellman optimality equation) Q\u2217(s0, a0) = ET\u03c0\u2217|s0,a0 \ufffd N \ufffd t=1 \u03b3t\u22121r(st) + \u03b3N max aN Q\u2217(sN, aN) \ufffd (19) Indeed, de\ufb01nition of Q\u2217(s, a) consists of average return and can be viewed as making T max steps from state s0 after selecting action a0, while vanilla Bellman optimality equation represents Q\u2217(s, a) as reward from one next step in the environment and estimation of the rest of trajectory reward recursively. N-step Bellman equation (19) generalizes these two opposites. All the same reasoning as for DQN can be applied to N-step Bellman equation to obtain N-step DQN algorithm, which only modi\ufb01cation appears in target computation: y(s0, a0) = N \ufffd t=1 \u03b3t\u22121r(st) + \u03b3N max aN Q\u2217(sN, aN, \u03b8) (20) 12often it is initialized by a constant close to 0 and is linearly increased until it reaches 1 18 To perform this computation, we are required to obtain for given state s and a not only one next step, but N steps. To do so, instead of transitions N-step roll-outs are stored, which can be done by precomputing following tuples: T = \ufffd s, a, N \ufffd n=1 \u03b3n\u22121r(n), s(N), done \ufffd where r(n) is the reward received in n steps after visitation of considered state s, s(N) is state visited in N steps, and done is a \ufb02ag whether the episode ended during N-step roll-out13. All other aspects of algorithm remain the same in practical implementations, and the case N = 1 corresponds to standard DQN. The goal of using N > 1 is to accelerate propagation of reward from terminal states backwards through visited states to s0 as less update steps will be required to take into account freshly observed reward and optimize behaviour at the beginning of episodes. The price is that formula (20) includes an important trick: to calculate such target, for second (and following) step action a\u2032 must be sampled from \u03c0\u2217 for Bellman equation (19) to remain true. In other words, application of N-step Q-learning is theoretically improper when behaviour policy di\ufb00ers from \u03c0\u2217. Note that we do not face this problem in the case N = 1 in which we are required to sample only from transition probability p(s\u2032 | s, a) for given state-action pair s, a. Even considering \u03c0\u2217 \u2248 argmax a Q\u2217(s, a, \u03b8), where Q\u2217 is our current approximation of \u03c0\u2217, makes N-step DQN an on-policy algorithm when for every state-action pair s, a it is preferable to sample target using the closest approximation of \u03c0\u2217 available. This questions usage of experience replay or at the very least encourages to limit its capacity to store only M max newest transitions with M max being relatively not very big. To see the negative e\ufb00ect of N-step DQN, consider the following toy example. Suppose agent makes a mistake on the second step after s and ends episode with huge negative reward. Then in the case N > 2 each time the roll-out starting with this s is sampled in the batch, the value of Q\u2217(s, a, \u03b8) will be updated with this received negative reward even if Q\u2217(s\u2032, \u00b7, \u03b8) already learned not to repeat this mistake again. Yet empirical results in many domains demonstrate that raising N from 1 to 2-3 may result in substantial acceleration of training and positively a\ufb00ect the \ufb01nal performance. On the contrary, the theoretical groundlessness of this approach explains its negative e\ufb00ects when N is set too big. 13all N-step roll-outs must be considered including those terminated at k-th step for k < N. 19 4. Distributional approach for value-based methods 4.1. Theoretical foundations The setting of RL task inherently carries internal stochasticity of which agent has no substantial control. Sometimes intelligent behaviour implies taking risks with severe chance of low episode return. All this information resides in the distribution of return R (1) as random variable. While value-based methods aim at learning expectation of this random variable as it is the quantity we actually care about, in distributional approach [1] it is proposed to learn the whole distribution of returns. It further extends the information gathered by algorithm about MDP towards model-based case in which the whole MDP is imitated by learning both reward function r(s) and transitions T, but still restricts itself only to reward and doesn\u2019t intend to learn world model. In this section we discuss some theoretical extensions of temporal di\ufb00erence ideas in the case when expectations on both sides of Bellman equation (5) and Bellman optimality equation (6) are taken away. The central object of study in Q-learning was Q-function, which for given state and action returns the expectation of reward. To rewrite Bellman equation not in terms of expectations, but in terms of the whole distributions, we require a corresponding notation. De\ufb01nition 9. For given MDP and policy \u03c0 the value distribution of policy \u03c0 is a random variable de\ufb01ned as Z\u03c0(s, a) := \ufffd t=0 \u03b3trt+1 \ufffd\ufffd\ufffd s0 = s, a0 = a Note that Z\u03c0 just represents a random variable which is taken expectation of in de\ufb01nition of Q-function: Q\u03c0(s, a) = ET\u03c0Z\u03c0(s, a) Using this de\ufb01nition of value distribution, Bellman equation can be rewritten to extend the recursive connection between adjacent states from expectations of returns to the whole distributions of returns: Proposition 9. (Distributional Bellman Equation) [1] Z\u03c0(s, a) c.d.f. = r(s\u2032) + \u03b3Z\u03c0(s\u2032, a\u2032) \ufffd\ufffd s\u2032 \u223c p(s\u2032 | s, a), a\u2032 \u223c \u03c0(a\u2032 | s\u2032) (21) Here we used some auxiliary notation: by c.d.f. = we mean that cumulative distribution functions of two random variables to the right and left are equal almost everywhere. Such equations are called recursive distributional equations and are well-known in theoretical probability theory14. By using | we describe a sampling procedure for the random variable to the right side of equation: for given s, a next state s\u2032 is sampled from transition probability, then a\u2032 is sampled from given policy, then random variable Z\u03c0(s\u2032, a\u2032) is sampled to calculate a resulting sample r(s\u2032) + \u03b3Z\u03c0(s\u2032, a\u2032). While the space of Q-functions Q\u03c0(s, a) \u2208 S \u00d7 A \u2192 R is \ufb01nite, the space of value distributions is a space of mappings from state-action pair to continuous distributions: Z\u03c0(s, a) \u2208 S \u00d7 A \u2192 P(R) and it is important to notice that even in the table-case when state and action spaces are \ufb01nite, the space of value distributions is essentially in\ufb01nite. Crucial moment for us will be that convergence properties now depend on chosen metric15. The choice of metric in S \u00d7 A \u2192 P(R) represents the same issue as in the space of continuous random variables P(R): if we choose a metric in the latter, we can construct one in the former: 14to get familiar with this notion, consider this basic example: X1 c.d.f. = X2 \u221a 2 + X3 \u221a 2 where X1, X2, X3 are random variables coming from N (0, \u03c32). 15in \ufb01nite spaces it is true that convergence in one metric guarantees convergence to the same point for any other metric. 20 Proposition 10. If d(X, Y ) is a metric in the space P(R), then d(Z1, Z2) := sup s\u2208S,a\u2208A d(Z1(s, a), Z2(s, a)) is a metric in the space S \u00d7 A \u2192 P(R). The particularly interesting for us example of metric in P(R) will be Wasserstein metric, which concerns only random variables with bounded moments, so we will additionally assume that for all state-action pairs s, a EZ\u03c0(s, a)p \u2264 +\u221e are \ufb01nite for p \u2265 1. Proposition 11. For 1 \u2264 p \u2264 +\u221e for two random variables X, Y on continuous domain with pth bounded moments and cumulative distribution functions FX and FY correspondingly a Wasserstein distance Wp(X, Y ) := \uf8eb \uf8ed 1 \ufffd 0 \ufffd\ufffd\ufffdF \u22121 X (\u03c9) \u2212 F \u22121 Y (\u03c9) \ufffd\ufffd\ufffd p d\u03c9 \uf8f6 \uf8f8 1 p W\u221e(X, Y ) := sup \u03c9\u2208[0,1] \ufffd\ufffd\ufffdF \u22121 X (\u03c9) \u2212 F \u22121 Y (\u03c9) \ufffd\ufffd\ufffd is a metric in the space of random variables with p-th bounded moments. Thus we can conclude from proposition 10 that maximal form of Wasserstein metric W p(Z1, Z2) = sup s\u2208S,a\u2208A Wp(Z1(s, a), Z2(s, a)) (22) is a metric in the space of value distributions. We now concern convergence properties of point iteration method to solve (21) in order to obtain Z\u03c0 for given policy \u03c0, i. e. solve the task of policy evaluation. For that purpose we initialize Z\u03c0 0 (s, a) arbitrarily16 and perform the following updates for all state-action pairs s, a: Z\u03c0 t+1(s, a) c.d.f. := r(s\u2032) + \u03b3Z\u03c0 t (s\u2032, a\u2032) (23) Here we assume that we are able to compute the distribution of random variable on the right side knowing \u03c0, all transition probabilities T, the distribution of Z\u03c0 t and reward function. The question whether the sequence {Z\u03c0 t } converges to Z\u03c0 can be given a detailed answer: Proposition 12. [1] Denote by B the following operator (S \u00d7 A \u2192 P(R)) \u2192 (S \u00d7 A \u2192 P(R)), updating Z\u03c0 t as in (23): Z\u03c0 t+1 = BZ\u03c0 t for all state-action pairs s, a. Then B is a contraction mapping in W p (22) for 1 \u2264 p \u2264 +\u221e, i.e. for any two value distributions Z1, Z2 W p(BZ1, BZ2) \u2264 \u03b3W p(Z1, Z2) Hence there is a unique \ufb01xed point of system of equations (21) and the point iteration method converges to it. One more curious theoretical result is that B is in general not a contraction mapping for such distances as Kullback-Leibler divergence, Total Variation distance and Kolmogorov distance17. It shows 16here we consider value distributions from theoretical point of view, assuming that we are able to explicitly store a table of |S||A| continuous distributions without any approximations. 17one more metric for which the contraction property was shown is Cramer metric: l2(X, Y ) = \uf8eb \uf8ed \ufffd R (FX(\u03c9) \u2212 FY (\u03c9))2 d\u03c9 \uf8f6 \uf8f8 1 2 where FX, FY are c.d.f. of random variables X, Y correspondingly. 21 that metric selection indeed in\ufb02uences convergence rate. Similar to traditional value functions, we can de\ufb01ne optimal value distribution Z\u2217(s, a). Substituting18 \u03c0\u2217(s) = argmax a ET\u03c0\u2217 Z\u2217(s, a) into (21), we obtain distributional Bellman optimality equation: Proposition 13. (Distributional Bellman optimality equation) Z\u2217(s, a) c.d.f. = r(s\u2032) + \u03b3Z\u2217(s\u2032, argmax a\u2032 ET\u03c0\u2217 Z\u2217(s\u2032, a\u2032)) \ufffd\ufffd s\u2032 \u223c p(s\u2032 | s, a) (24) Now we concern the same question whether the point iteration method of solving (24) leads to solution Z\u2217 and whether it is a contraction mapping for some metric. The answer turns out to be negative. Proposition 14. [1] Point iteration for solving (24) may diverge. Level of impact of this result is not completely clear. Point iteration for (24) preserves means of distributions, i. e. it will eventually converge to Q\u2217(s, a) with all theoretical guarantees from classical Q-learning. The reason behind divergence theorems hides in the rest of distributions like other moments and situations when equivalent (in terms of average return) actions may lead to di\ufb00erent higher moments. 4.2. Categorical DQN There are obvious obstacles for practical application of distributional Q-learning following from complication of working with arbitrary continuous distributions. Usually we are restricted to approximations inside some family of parametric distributions, so we have to perform a projection step on each iteration. Second matter in combining distributional Q-learning with deep neural networks is to take into account that only samples from p(s\u2032 | s, a) are available for each update. To provide a distributional analog of temporal di\ufb00erence algorithm 9, some analog of exponential smoothing for distributional setting must be proposed. Categorical DQN [1] (also referred as c51) provides straightforward design of practical distributional algorithm. While DQN was a resemblance of temporal di\ufb00erence algorithm, Categorical DQN attempts to follow the logic of DQN. The concept is as following. The neural network with parameters \u03b8 in this setting takes as input s \u2208 S and for each action a outputs parameters \u03b6\u03b8(s, a) of distributions of random variable Z\u2217 \u03b8(s, a). As in DQN, experience replay can be used to collect observed transitions and sample a batch for each update step. For each transition T = (s, a, r\u2032, s\u2032, done) in the batch a guess is computed: y(T ) c.d.f. := r\u2032 + (1 \u2212 done)\u03b3Z\u2217 \u03b8 \ufffd s\u2032, argmax a\u2032 EZ\u2217 \u03b8(s\u2032, a\u2032) \ufffd (25) Note that expectation of Z\u2217 \u03b8(s\u2032, a\u2032) is computed explicitly using the form of chosen parametric family of distributions and outputted parameters \u03b6\u03b8(s\u2032, a\u2032), as is the distribution of random variable r\u2032 + (1 \u2212 done)\u03b3Z\u2217 \u03b8(s\u2032, a\u2032). In other words, in this setting guess y(T ) is also a continuous random variable, distribution of which can be constructed only approximately. As both target and model output are distributions, it is reasonable to design loss function in a form of some divergence D between y(T ) and Z\u2217 \u03b8(s, a): Loss(\u03b8) = ET D \ufffd y(T ) \u2225 Z\u2217 \u03b8(s, a) \ufffd (26) \u03b8t+1 = \u03b8t \u2212 \u03b1\u2202 Loss(\u03b8t) \u2202\u03b8 18to perform this step validly, a clari\ufb01cation concerning argmax operator de\ufb01nition must be given. The choice of action a returned by this operator in the cases when several actions lead to the same maximal average returns must not depend on Z, as this choice a\ufb00ects higher moments of resulted distribution. To overcome this issue, for example, in the case of \ufb01nite action space all actions can be enumerated and the optimal action with the lowest index is returned by operator. 22 The particular choice of this divergence must be made with concern that y(T ) is a \u00absample\u00bb from a full one-step approximation of Z\u2217 \u03b8 which includes transition probabilities: yfull(s, a) c.d.f. := \ufffd s\u2032\u2208S p(s\u2032 | s, a)y(s, a, r(s\u2032), s\u2032, done(s\u2032)) (27) This form is precisely the right side of distributional Bellman optimality equation as we just incorporated intermediate sampling of s\u2032 into the value of random variable. In other words, if transition probabilities T were known, the update could be made using distribution of yfull as a target. Lossfull(\u03b8) = Es,aD(yfull(s, a) \u2225 Z\u2217 \u03b8(s, a)) This motivates to choose KL(y(T ) \u2225 Z\u2217 \u03b8(s, a)) (speci\ufb01cally with this order of arguments) as D to exploit the following property (we denote by pX a p.d.f. pf random variable X): \u2207\u03b8ET KL(yfull(s, a) \u2225 Z\u2217 \u03b8(s, a)) = \u2207\u03b8 \ufffd ET \ufffd R \u2212pyfull(s,a)(\u03c9) log pZ\u2217 \u03b8 (s,a))(\u03c9)d\u03c9 + const(\u03b8) \ufffd = {using (27)} = \u2207\u03b8ET \ufffd R Es\u2032\u223cp(s\u2032|s,a) \u2212 py(T )(\u03c9) log pZ\u2217 \u03b8 (s,a))(\u03c9)d\u03c9 = {taking expectation out} = \u2207\u03b8ET Es\u2032\u223cp(s\u2032|s,a) \ufffd R \u2212py(T )(\u03c9) log pZ\u2217 \u03b8 (s,a))(\u03c9)d\u03c9 = = \u2207\u03b8ET Es\u2032\u223cp(s\u2032|s,a) KL \ufffd y(T ) \u2225 Z\u2217 \u03b8(s, a) \ufffd This property basically states that gradient of loss function (26) with KL as D is an unbiased (Monte-Carlo) estimation of gradient of KL-divergence for \u00abfull\u00bb distribution (27), which resembles the employment of exponential smoothing in temporal di\ufb00erence learning. For many other divergences, including Wasserstein metric, same statement is not true, so their utilization in described online setting will lead to biased gradients and all theory-grounded intuition that algorithm moves in the right direction becomes distinctively lost. Moreover, KL-divergence is known to be one of the easiest divergences to work with due to its nice smoothness properties and wide prevalence in many deep learning pipelines. Described above motivation to choose KL-divergence as an actual objective for minimization is contradictory. Theoretical analysis of distributional Q-learning, speci\ufb01cally theorem 12, though concerning policy evaluation other than optimal Z\u2217 search, explicitly hints that the process converges exponentially fast for Wasserstein metric, while even for precisely made updates in terms of KLdivergence we are not guaranteed to get any closer to true solution. More \u00abpractical\u00bb defect of KL-divergence is that it demands two comparable distributions to share the same domain. This means that by choosing KL-divergence we pledge to guarantee that y(T ) and Z\u2217 \u03b8(s, a) in (26) have coinciding support. This emerging restriction seems limiting even beforehand as for episodic MDP value distribution in terminal states is obviously degenerated (their support consists of one point r(s) which is given all probability mass) which means that our value distribution approximation is basically ensured to never be precise. In Categorical DQN, as follows from the name, the family of distributions is chosen to be categorical on the \ufb01xed support {z0, z1 . . . zA\u22121} where A is number of atoms. As no prior information about MDP is given, the basic choice of this support is uniform grid from some Vmin \u2208 R to V max \u2208 R: zi = Vmin + i A \u2212 1(Vmax \u2212 Vmin), i \u2208 0, 1, . . . A \u2212 1 These bounds, though, must be chosen carefully as they implicitly assume Vmin \u2264 Z\u2217(s, a) \u2264 Vmax and if these inequalities are not tight, the approximation will obviously become poor. Therefore the neural network outputs A numbers, summing into 1, to represent arbitrary distribution on this support: \u03b6i(s, a, \u03b8) := P(Z\u2217 \u03b8(s, a) = zi) Within this family of distributions, computation of expectation, greedy action selection and KLdivergence is trivial. One problem hides in target formula (25): while we can compute distribution y(T ), its support may in general di\ufb00er from {z0 . . . zA\u22121}. To avoid the issue of disjoint supports, 23 a projection step must be done to \ufb01nd the closest to target distribution within the chosen family19. Therefore the resulting target used in the loss function is y(T ) c.d.f. := \u03a0C \ufffd r\u2032 + (1 \u2212 done)\u03b3Z\u2217 \u03b8 \ufffd s\u2032, argmax a\u2032 EZ\u2217 \u03b8(s\u2032, a\u2032) \ufffd\ufffd where \u03a0C is projection operator. The resulting practical algorithm, named c51 after categorical distributions with A = 51 atoms, inherits ideas of experience replay, \u03b5-greedy exploration and target network from DQN. Empirically, though, usage of target network remains an open question as the chosen family of distributions restricts value approximation from unbounded growth by \u00abclipping\u00bb predictions at zA\u22121 and z0, yet it is still considered slightly improving performance. Algorithm 3: Categorical DQN (c51) Hyperparameters: B \u2014 batch size, Vmax, Vmin, A \u2014 parameters of support, K \u2014 target network update frequency, \u03b5(t) \u2208 (0, 1] \u2014 greedy exploration parameter, \u03b6\u2217 \u2014 neural network, SGD optimizer. Initialize weights \u03b8 of neural net \u03b6\u2217 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 Precompute support grid zi = Vmin + i A\u22121(Vmax \u2212 Vmin) On each interaction step: 1. select a randomly with probability \u03b5(t), else a = argmax a \ufffd i zi\u03b6\u2217 i (s, a, \u03b8) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. add observed transition to experience replay 4. sample batch of size B from experience replay 5. for each transition T from the batch compute target: P(y(T ) = r\u2032 + \u03b3zi) = \u03b6\u2217 i \ufffd s\u2032, argmax a\u2032 \ufffd i zi\u03b6\u2217 i (s\u2032, a\u2032, \u03b8\u2212), \u03b8\u2212 \ufffd 6. project y(T ) on support {z0, z1 . . . zA\u22121} 7. compute loss: Loss = 1 B \ufffd T KL(y(T ) \u2225 Z\u2217(s, a, \u03b8)) 8. make a step of gradient descent using \u2202 Loss \u2202\u03b8 9. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 4.3. Quantile Regression DQN (QR-DQN) Categorical DQN discovered a gap between theory and practice as KL-divergence, used in practical algorithm, is theoretically unjusti\ufb01ed. Theorem 12 hints that the true divergence we should care about is actually Wasserstein metric, but it remained unclear how it could be optimized using only samples from transition probabilities T. In [3] it was discovered that selecting another family of distributions to approximate Z\u2217 \u03b8(s, a) will reduce Wasserstein minimization task to the search for quantiles of speci\ufb01c distributions. The 19to project a categorical distribution with support {v0, v1 . . . vA\u22121} on categorical distributions with support {z0, z1 . . . zA\u22121} one can just \ufb01nd for each vi the closest two atoms zj \u2264 vi \u2264 zj+1 and split all probability mass for vi between zj and zj+1 proportional to closeness. If vi < z0, then all its probability mass is given to z0, same with upper bound. 24 latter can be done in online setting using quantile regression technique. This led to alternative distributional Q-learning algorithm named Quantile Regression DQN (QR-DQN). The basic idea is to \u00abswap\u00bb \ufb01xed support and learned probabilities of Categorical DQN. We will now consider the family with \ufb01xed probabilities for A-atomed categorical distribution with arbitrary support {\u03b6\u2217 0(s, a, \u03b8), \u03b6\u2217 1(s, a, \u03b8), . . . , \u03b6\u2217 A\u22121(s, a, \u03b8)}. Again, we will assume all probabilities to be equal given the absence of any prior knowledge; namely, our distribution family is now Z\u2217 \u03b8(s, a) \u223c Uniform \ufffd \u03b6\u2217 0(s, a, \u03b8), . . . , \u03b6\u2217 A\u22121(s, a, \u03b8) \ufffd In this setting neural network outputs A arbitrary real numbers that represent the support of uniform categorical distribution20, where A is the number of atoms and the only hyperparameter to select. For table-case setting, on each step of point iteration we desire to update the cell for given stateaction pair s, a with full distribution of random variable to the right side of (24). If we are limited to store only A atoms of the support, the true distribution must be projected on the space of Aatomed categorical distributions. Consider now this task of projecting some given random variable with c.d.f. F (\u03c9) in terms of Wasserstein distance. Speci\ufb01cally, we will be interested in minimizing W1-distance for p = 1 as the theorem 12 states the contraction property for all 1 \u2264 p \u2264 +\u221e and we are free to choose any: \ufffd 1 0 \ufffd\ufffd\ufffdF \u22121(\u03c9) \u2212 U \u22121 z0,z1...zA\u22121(\u03c9) \ufffd\ufffd\ufffd d\u03c9 \u2192 min z0,z1...zA\u22121 (28) where Uz0,z1...zA\u22121 is c.d.f. for uniform categorical distribution on given support. Its inverse, also known as quantile function, has a following simple form: U \u22121 z0,z1...zA\u22121(\u03c9) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 z0 0 \u2264 \u03c9 < 1 A z1 1 A \u2264 \u03c9 < 2 A ... zA\u22121 A\u22121 A \u2264 \u03c9 < 1 Substituting this into (28) A\u22121 \ufffd i=0 \ufffd i+1 A i A \ufffd\ufffdF \u22121(\u03c9) \u2212 zi \ufffd\ufffd d\u03c9 \u2192 min z0,z1...zA\u22121 splits the optimization of Wasserstein into A independent tasks that can be solved separately: \ufffd i+1 A i A \ufffd\ufffdF \u22121(\u03c9) \u2212 zi \ufffd\ufffd d\u03c9 \u2192 min zi (29) Proposition 15. [3] Let\u2019s denote \u03c4i := i A + i+1 A 2 Then every solution for (29) satis\ufb01es F (zi) = \u03c4i, i. e. it is \u03c4i-th quantile of c. d. f. F . The result 15 states that we require only A speci\ufb01c quantiles of random variable to the right side of Bellman equation21. Hence the last thing to do to design a practical algorithm is to develop a procedure of unbiased estimation of quantiles for the random variable on the right side of distribution Bellman optimality equation (24). 20Note that target distribution is now guaranteed to remain within this distribution family as multiplying on \u03b3 just shrinks the support and adding r\u2032 just shifts it. We assume that if some atoms of the support coincide, the distribution is still A-atomed categorical; for example, for degenerated distribution (like in the case of terminal states) \u03b6\u2217 0(s, a, \u03b8) = \u03b6\u2217 1(s, a, \u03b8) = \u00b7 \u00b7 \u00b7 = \u03b6\u2217 A\u22121(s, a, \u03b8). This shows that projection step heuristic is not needed for this particular choice of distribution family. 21It can be proved that for table-case policy evaluation algorithm which stores in each cell not expectations of reward (as in Q-learning) but A quantiles updated according to distributional Bellman equation (21) using theorem 15 converges to quantiles of Z\u2217(s, a) in Wasserstein metric for 1 \u2264 p \u2264 +\u221e and its update operator is a contraction mapping in W\u221e. 25 Quantile regression is the standard technique to estimate the quantiles of empirical distribution (i. .e. distribution that is represented by \ufb01nite amount of i. i. d. samples from it). Recall from machine learning that the constant solution optimizing l1-loss is median, i. .e. 1 2-th quantile. This fact can be generalized to arbitrary quantiles: Proposition 16. (Quantile Regression) [11] Let\u2019s de\ufb01ne loss as Loss(c, X) = \ufffd \u03c4(c \u2212 X) c \u2265 X (1 \u2212 \u03c4)(X \u2212 c) c < X Then solution for EX Loss(c, X) \u2192 min c\u2208R (30) is \u03c4-th quantile of distribution of X. As usual in the case of neural networks, it is impractical to optimize (30) until convergence on each iteration for each of A desired quantiles \u03c4i. Instead just one step of gradient optimization is made and the outputs of neural network \u03b6\u2217 i (s, a, \u03b8), which play the role of c in formula (30), are moved towards the quantile estimation via backpropagation. In other words, (30) sets a loss function for network outputs; the losses for di\ufb00erent quantiles are summed up. The resulting loss is LossQR(s, a, \u03b8) = A\u22121 \ufffd i=0 Es\u2032\u223cp(s\u2032|s,a)Ey\u223cy(T ) \ufffd \u03c4 \u2212 I[\u03b6\u2217 i (s, a, \u03b8) < y] \ufffd \ufffd \u03b6\u2217 i (s, a, \u03b8) \u2212 y \ufffd (31) where I denotes an indicator function. The expectation over y \u223c y(T ) for given transition can be computed in closed form: indeed, y(T ) is also an A-atomed categorical distribution with support {r\u2032 + \u03b3\u03b6\u2217 0(s\u2032, a\u2032), . . . , r\u2032 + \u03b3\u03b6\u2217 A\u22121(s\u2032, a\u2032)}, where a\u2032 = argmax a\u2032 EZ\u2217(s\u2032, a\u2032, \u03b8) = argmax a\u2032 1 A \ufffd i \u03b6\u2217 i (s\u2032, a\u2032, \u03b8) and expectation over transition probabilities, as always, is estimated using Monte-Carlo by sampling transitions from experience replay. Algorithm 4: Quantile Regression DQN (QR-DQN) Hyperparameters: B \u2014 batch size, A \u2014 number of atoms, K \u2014 target network update frequency, \u03b5(t) \u2208 (0, 1] \u2014 greedy exploration parameter, \u03b6\u2217 \u2014 neural network, SGD optimizer. Initialize weights \u03b8 of neural net \u03b6\u2217 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 Precompute mid-quantiles \u03c4i = i A + i+1 A 2 On each interaction step: 1. select a randomly with probability \u03b5(t), else a = argmax a 1 A \ufffd i \u03b6\u2217 i (s, a, \u03b8) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. add observed transition to experience replay 4. sample batch of size B from experience replay 5. for each transition T from the batch compute the support of target distribution: y(T )j = r\u2032 + \u03b3\u03b6\u2217 j \ufffd s\u2032, argmax a\u2032 1 A \ufffd i \u03b6\u2217 i (s\u2032, a\u2032, \u03b8\u2212), \u03b8\u2212 \ufffd 26 6. compute loss: Loss = 1 BA \ufffd T \ufffd i \ufffd j \ufffd \u03c4i \u2212 I[\u03b6\u2217 i (s, a, \u03b8) < y(T )j] \ufffd \ufffd \u03b6\u2217 i (s, a, \u03b8) \u2212 y(T )j \ufffd 7. make a step of gradient descent using \u2202 Loss \u2202\u03b8 8. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 4.4. Rainbow DQN Success of Deep Q-learning encouraged a full-scale research of value-based deep reinforcement learning by studying various drawbacks of DQN and developing auxiliary extensions. In many articles some extensions from previous research were already considered and embedded in compared algorithms during empirical studies. In Rainbow DQN [7], seven Q-learning-based ideas are united in one procedure with ablation studies held whether all these incorporated extensions are essentially necessary for resulted RL algorithm: \u2022 DQN (sec. 3.2) \u2022 Double DQN (sec. 3.3) \u2022 Dueling DQN (sec. 3.4) \u2022 Noisy DQN (sec. 3.5) \u2022 Prioritized Experience Replay (sec. 3.6) \u2022 Multi-step DQN (sec. 3.7) \u2022 Categorical22 DQN (sec. 4.2) There is little ambiguity on how these ideas can be combined; we will discuss several nonstraightforward circumstances and provide the full algorithm description after. To apply prioritized experience replay in distributional setting, the measure of transition importance must be provided. The main idea is inherited from ordinary DQN where priority is just loss for this transition: \u03c1(T ) := Loss(y(T ), Z\u2217(s, a, \u03b8)) = KL(y(T ) \u2225 Z\u2217(s, a, \u03b8)) To combine noisy networks with double DQN heuristic, it is proposed to resample noise on each forward pass through the network and through its copy for target computation. This decision implies that action selection, action evaluation and network utilization are independent and stochastic (for exploration cultivation) steps. The one snagging combination here is categorical DQN and dueling DQN. To merge these ideas, we need to model advantage A\u2217(s, a, \u03b8) in distributional setting. In Rainbow this is done straightforwardly: the network has two heads, value stream v(s, \u03b8) outputting A real values and advantage stream a(s, a, \u03b8) outputting A \u00d7 |A| real values. Then these streams are integrated using the same formula (17) with the only exception being softmax applied across atoms dimension to guarantee that output is categorical distribution: \u03b6\u2217 i (s, a, \u03b8) \u221d exp \ufffd v(s, \u03b8)i + a(s, a, \u03b8)i \u2212 1 |A| \ufffd a a(s, a, \u03b8)i \ufffd (32) Combining lack of intuition behind this integration formula with usage of mean instead of theoretically justi\ufb01ed max makes this element of Rainbow the most questionable. During the ablation studies it was discovered that dueling architecture is the only component that can be removed without noticeable loss of performance. All other ingredients are believed to be crucial for resulting algorithm as they address di\ufb00erent problems. 22Quantile Regression can be considered instead 27 Algorithm 5: Rainbow DQN Hyperparameters: B \u2014 batch size, Vmax, Vmin, A \u2014 parameters of support, K \u2014 target network update frequency, N \u2014 multi-step size, \u03b1 \u2014 degree of prioritized experience replay, \u03b2(t) \u2014 importance sampling bias correction for prioritized experience replay, \u03b6\u2217 \u2014 neural network, SGD optimizer. Initialize weights \u03b8 of neural net \u03b6\u2217 arbitrary Initialize \u03b8\u2212 \u2190 \u03b8 Precompute support grid zi = Vmin + i A\u22121(Vmax \u2212 Vmin) On each interaction step: 1. select a = argmax a \ufffd i zi\u03b6\u2217 i (s, a, \u03b8, \u03b5), \u03b5 \u223c N (0, I) 2. observe transition (s, a, r\u2032, s\u2032, done) 3. construct N-step transition T = \ufffd s, a, \ufffdN n=0 \u03b3nr(n+1), s(N), done \ufffd and add it to experience replay with priority maxT \u03c1(T ) 4. sample batch of size B from experience replay using probabilities P(T ) \u221d \u03c1(T )\u03b1 5. compute weights for the batch (where M is the size of experience replay memory) w(T ) = \ufffd 1 MP(T ) \ufffd\u03b2(t) 6. for each transition T = (s, a, \u00afr, \u00afs, done) from the batch compute target (detached from computational graph to prevent backpropagation): \u03b51, \u03b52 \u223c N (0, I) P(y(T ) = \u00afr + \u03b3Nzi) = \u03b6\u2217 i \ufffd \u00afs, argmax \u00afa \ufffd i zi\u03b6\u2217 i (\u00afs, \u00afa, \u03b8, \u03b51), \u03b8\u2212, \u03b52 \ufffd 7. project y(T ) on support {z0, z1 . . . zA\u22121} 8. update transition priorities \u03c1(T ) \u2190 KL(y(T ) \u2225 Z\u2217(s, a, \u03b8, \u03b5)), \u03b5 \u223c N (0, I) 9. compute loss: Loss = 1 B \ufffd T w(T )\u03c1(T ) 10. make a step of gradient descent using \u2202 Loss \u2202\u03b8 11. if t mod K = 0: \u03b8\u2212 \u2190 \u03b8 28 5. Policy Gradient algorithms 5.1. Policy Gradient theorem Alternative approach to solving RL task is direct optimization of objective J(\u03b8) = ET \u223c\u03c0\u03b8 \ufffd t=1 \u03b3t\u22121rt \u2192 max \u03b8 (33) as a function of \u03b8. Policy gradient methods provide a framework how to construct an e\ufb03cient optimization procedure based on stochastic \ufb01rst-order optimization within RL setting. We will assume that \u03c0\u03b8(a | s) is a stochastic policy parameterized with \u03b8 \u2208 \u0398. It turns out, that if \u03c0 is di\ufb00erentiable by \u03b8, then so is our goal (33). We now proceed to discuss the technique of derivative calculation which is based on employment of log-derivative trick: Proposition 17. For arbitrary distribution \u03c0(a) parameterized by \u03b8: \u2207\u03b8\u03c0(a) = \u03c0(a)\u2207\u03b8 log \u03c0(a) (34) In most general form, this trick allows us to derive the gradient of expectation of an arbitrary function f(a, \u03b8) : A \u00d7 \u0398 \u2192 R, di\ufb00erentiable by \u03b8, with respect to some distribution \u03c0\u03b8(a), also parameterized by \u03b8: \u2207\u03b8Ea\u223c\u03c0\u03b8(a)f(a, \u03b8) = \u2207\u03b8 \ufffd A \u03c0\u03b8(a)f(a, \u03b8)da = = \ufffd A \u2207\u03b8 [\u03c0\u03b8(a)f(a, \u03b8)] da = {product rule} = \ufffd A [\u2207\u03b8\u03c0\u03b8(a)f(a, \u03b8) + \u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8)] da = = \ufffd A \u2207\u03b8\u03c0\u03b8(a)f(a, \u03b8)da + E\u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8) = {log-derivative trick (34)} = \ufffd A \u03c0\u03b8(a)\u2207\u03b8 log \u03c0\u03b8(a)f(a, \u03b8)da + E\u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8) = = E\u03c0\u03b8(a)\u2207\u03b8 log \u03c0\u03b8(a)f(a, \u03b8) + E\u03c0\u03b8(a)\u2207\u03b8f(a, \u03b8) This technique can be applied sequentially (to expectations over \u03c0\u03b8(a0 | s0), \u03c0\u03b8(a1 | s1) and so on) to obtain the gradient \u2207\u03b8J(\u03b8). Proposition 18. (Policy Gradient Theorem) [24] For any MDP and di\ufb00erentiable policy \u03c0\u03b8 the gradient of objective (33) is \u2207\u03b8J(\u03b8) = ET \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st)Q\u03c0(st, at) (35) For future references, we require another form of formula (35), which provides another point of view. For this purpose, let us de\ufb01ne a discounted state visitation frequency: De\ufb01nition 10. For given MDP and given policy \u03c0 its discounted state visitation frequency is de\ufb01ned by d\u03c0(s) := (1 \u2212 \u03b3) \ufffd t=0 \u03b3tP(st = s) where st are taken from trajectories T sampled using given policy \u03c0. Discounted state visitation frequencies, if normalized, represent a marginalized probability for agent to land in a given state s23. It is rarely attempted to be learned, but it assists theoretical 23the \u03b3t weighting in this de\ufb01nition is often introduced to incorporate the same reduction of contribution of later states in the whole gradient according to (35). Similar notation is sometimes used for state visitation frequency without discount. 29 study by allowing us to rewrite expectations over trajectories with separated intrinsic and extrinsic randomness of the decision making process: \u2207\u03b8J(\u03b8) = Es\u223cd\u03c0(s)Ea\u223c\u03c0(a|s)\u2207\u03b8 log \u03c0\u03b8(a | s)Q\u03c0(s, a) (36) This form is equivalent to (35) as sampling a trajectory and going through all visited states with weights \u03b3t induces the same distribution as de\ufb01ned in d\u03c0(s). Now, although we acquired an explicit form of objective\u2019s gradient, we are able to compute it only approximately, using Monte-Carlo estimation for expectations via sampling one or several trajectories. Second form of gradient (36) reveals that it is possible to use roll-outs of trajectories without waiting for episode ending, as the states for the roll-outs come from the same distribution as they would for complete episode trajectories24. The essential thing is that exactly the policy \u03c0(\u03b8) must be used for sampling to obtain unbiased Monte-Carlo estimation (otherwise state visitation frequency d\u03c0(s) is di\ufb00erent). These features are commonly underlined by notation E\u03c0, which is a shorter form of Es\u223cd\u03c0(s)Ea\u223c\u03c0(a|s). When convenient, we will use it to reduce the gradient to a shorter form: \u2207\u03b8J(\u03b8) = E\u03c0(\u03b8)\u2207\u03b8 log \u03c0\u03b8(a | s)Q\u03c0(s, a) (37) Second important thing worth mentioning is that Q\u03c0(s, a) is essentially present in the gradient. Remark that it is never available to the algorithm and must also be somehow estimated. 5.2. REINFORCE REINFORCE [29] provides a straightforward approach to approximately calculate the gradient (35) in episodic case using Monte-Carlo estimation: N games are played and Q-function under policy \u03c0 is approximated with corresponding return: Q\u03c0(s, a) = ET \u223c\u03c0\u03b8|s,aR(T ) \u2248 R(T ), T \u223c \u03c0\u03b8 | s, a The resulting formula is therefore the following: \u2207\u03b8J(\u03b8) \u2248 1 N N \ufffd T \ufffd t=0 \ufffd \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st) \ufffd\ufffd t\u2032=t \u03b3t\u2032\u2212trt\u2032+1 \ufffd\ufffd (38) This estimation is unbiased as both approximation of Q\u03c0 and approximation of expectation over trajectories are done using Monte-Carlo. Given that estimation of gradient is unbiased, stochastic gradient ascent or more advanced stochastic optimization techniques are known to converge to local optimum. From theoretical point of view REINFORCE can be applied straightforwardly for any parametric family \u03c0\u03b8(a | s) including neural networks. Yet the enormous time required for convergence and the problem of stucking in local optimums make this naive approach completely impractical. The main source of problems is believed to be the high variance of gradient estimation (38), as the convergence rate of stochastic gradient descent directly depends on the variance of gradient estimation. The standard technique of variance reduction is an introduction of baseline. The idea is to add some term that will not a\ufb00ect the expectation, but may a\ufb00ect the variance. One such baseline can be derived using following reasoning: for any distribution it is true that \ufffd A \u03c0\u03b8(a | s)da = 1. Taking the gradient \u2207\u03b8 from both sides, we obtain: 0 = \ufffd A \u2207\u03b8\u03c0\u03b8(a | s)da = {log-derivative trick (34)} = \ufffd A \u03c0\u03b8(a | s)\u2207\u03b8 log \u03c0\u03b8(a | s)da = = E\u03c0\u03b8(a|s)\u2207\u03b8 log \u03c0\u03b8(a | s) 24in practice and in most policy gradients algorithms, sampling roll-outs never include \u03b3t weights, which formally corresponds to estimating gradient using incorrect equation (\u00abapproximation\u00bb): \u2207\u03b8J(\u03b8) \u2248 ET \u223c\u03c0\u03b8 \ufffd t=0 \u2207\u03b8 log \u03c0\u03b8(at | st)Q\u03c0(st, at) which di\ufb00ers from the correct version (35) in ignoring \u03b3t multiplier. On the one hand, it equalizes the contribution of different terms and agrees with intuition, but on the other hand such gradient estimation does not imply optimization of any reasonable objective and breaks the idea of straightforward gradient ascent [15]. 30 Multiplying this expression on some constant, we can scale this baseline: E\u03c0\u03b8(a|s) const(a)\u2207\u03b8 log \u03c0\u03b8(a | s) = 0 Notice that the constant here must be independent of a, but may depend on s. Application of this technique to our case provides the following result25: Proposition 19. For any arbitrary function b(s): S \u2192 R, called baseline: \u2207\u03b8J(\u03b8) = ET \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st) (Q\u03c0(st, at) \u2212 b(st)) Selection of the baseline is up to us as long as it does not depend on actions at. The intent is to choose it in a way that minimizes the variance. It is believed that high variance of (38) originates from multiplication of Q\u03c0(s, a), which may have arbitrary scale (e. .g. in a range [100, 200]) while \u2207\u03b8 log \u03c0\u03b8(at | st) naturally has varying signs26. To reduce the variance, the baseline must be chosen so that absolute values of expression inside the expectation are shifted towards zero. Wherein the optimal baseline is provided by the following theorem: Proposition 20. The solution for VT \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2207\u03b8 log \u03c0\u03b8(at | st) (Q\u03c0(st, at) \u2212 b(st)) \u2192 min b(s) is given by b(s) = Ea\u223c\u03c0\u03b8(a|s)\u03b3t\u2225\u2207\u03b8 log \u03c0\u03b8(a | s)\u22252 2Q\u03c0(s, a) Ea\u223c\u03c0\u03b8(a|s)\u2225\u2207\u03b8 log \u03c0\u03b8(a | s)\u22252 2 (39) As can be seen, optimal baseline calculation involves expectations which again can only be computed (in most cases) using Monte-Carlo (both for numerator and denominator). For that purpose, for every visited state s estimations of Q\u03c0(s, a) are needed for all (or some) actions a, as otherwise estimation of baseline will coincide with estimation of Q\u03c0(s, a) and collapse gradient to zero. Practical utilization of result (39) is to consider a constant baseline independent of s with similar optimal form: b = ET \u223c\u03c0\u03b8 \ufffd t=0 \u03b3t\u2225\u2207\u03b8 log \u03c0\u03b8(at | st)\u22252 2Q\u03c0(st, at) ET \u223c\u03c0\u03b8 \ufffd t=0 \u2225\u2207\u03b8 log \u03c0\u03b8(at | st)\u22252 2 Utilization of some kind of baseline, not necessarily optimal, is known to signi\ufb01cantly reduce the variance of gradient estimation and is an essential part of any policy gradient method. The \ufb01nal step to make this family of algorithms applicable when using deep neural networks is to reduce variance of Q\u03c0 estimation by employing RL task structure like it was done in value-based methods. 5.3. Advantage Actor-Critic (A2C) Suppose that in optimal baseline formula (39) it happens that \u2225\u2207\u03b8 log \u03c0\u03b8(a | s)\u22252 2 = const(a). Though in reality this is actually not true, under this circumstance the optimal baseline formula signi\ufb01cantly reduces and unravels a close-to-optimal but simple form of baseline: b(s) = \u03b3tEa\u223c\u03c0\u03b8(a|s)Q\u03c0(s, a) = \u03b3tV \u03c0(s) Substituting this baseline into gradient formula (37) and recalling the de\ufb01nition of advantage function (14), the gradient can now be rewritten as follows: \u2207\u03b8J(\u03b8) = E\u03c0(\u03b8)\u2207\u03b8 log \u03c0\u03b8(a | s)A\u03c0(s, a) (40) This representation of gradient is used as the basement for most policy gradient algorithms as it o\ufb00ers lower variance while selecting the baseline expressed in terms of value functions which can be 25this result can be generalized by introducing di\ufb00erent baselines for estimation of di\ufb00erent components of \u2207\u03b8J(\u03b8). 26this follows, for example, from baseline derivation. 31 e\ufb03ciently learned similar to how it was done in value-based methods. Such algorithms are usually named Actor-Critic as they consist of two neural networks: \u03c0\u03b8(a | s), representing a policy, called an actor, and V \u03c0 \u03c6 (s) with parameters \u03c6, approximately estimating actor\u2019s performance, called a critic. Note that the choice of value function to learn can be arbitrary; it is possible to learn Q\u03c0 or A\u03c0 instead, as all of them are deeply interconnected. Value function V \u03c0 is chosen as the simplest one since it depends only on state and thus is hoped to be easier to learn. Having a critic V \u03c0 \u03c6 (s), Q-function can be approximated in a following way: Q\u03c0(s, a) \u2248 r\u2032 + \u03b3V \u03c0(s\u2032) \u2248 r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) First approximation is done using Monte-Carlo, while second approximation inevitably introduces bias. Important thing to notice is that at this moment our gradient estimation stops being unbiased and all theoretical guarantees of converging are once again lost. Advantage function therefore can be obtained according to the de\ufb01nition: A\u03c0(s, a) = Q\u03c0(s, a) \u2212 V \u03c0(s) \u2248 r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2212 V \u03c0 \u03c6 (s) (41) Note that biased estimation of baseline doesn\u2019t make gradient estimation biased by itself, as baseline can be an arbitrary function of state. All bias introduction happens inside the approximation of Q\u03c0. It is possible to use critic only for baseline, which allows complete avoidance of bias, but then the only way to estimate Q\u03c0 is via playing several games and using corresponding returns, which su\ufb00ers from higher variance and low sample e\ufb03ciency. The logic behind training procedure for the critic is taken from value-based methods: for given policy \u03c0 its value function can be obtained using point iteration for solving V \u03c0(s) = Ea\u223c\u03c0(a|s)Es\u2032\u223cp(s\u2032|s,a) [r\u2032 + \u03b3V \u03c0(s\u2032)] Similar to DQN, on each update a target is computed using current approximation y = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) and then MSE is minimized to move values of V \u03c0 \u03c6 (s) towards the guess. Notice that to compute the target for critic we require samples from the policy \u03c0 which is being evaluated. Although actor evolves throughout optimization process, we assume that one update of policy \u03c0 does not lead to signi\ufb01cant change of true V \u03c0 and thus our critic, which approximates value function for older version of policy, is close enough to construct the target. But if samples from, for example, old policy are used to compute the guess, the step of critic update will correspond to learning the value function for old policy other than current. Essentially, this means that both actor and critic training procedures require samples from current policy \u03c0, making Actor-Critic algorithm onpolicy by design. Consequently, samples that were collected on previous update iterations become useless and can be forgotten. This is the key reason why policy gradient algorithms are usually less sample-e\ufb03cient than value-based. Now as we have an approximation of value function, advantage estimation can be done using one-step transitions (41). As the procedure of training an actor, i. .e. gradient estimation (40), also does not demand sampling the whole trajectory, each update now requires only a small roll-out to be sampled. The amount of transitions in the roll-out corresponds to the size of mini-batch. The problem with roll-outs is that the data is obviously not i. i. d., which is crucial for training networks. In value-based methods, this problem was solved with experience replay, but in policy gradient algorithms it is essential to collect samples from scratch after each update of the networks parameters. The practical solution for simulated environments is to launch several instances of environment (for example, on di\ufb00erent cores of multiprocessor) in parallel threads and have several parallel interactions. After several steps in each environment, the batch for update is collected by uniting transitions from all instances and one synchronous27 update of networks parameters \u03b8 and \u03c6 is performed. One more optimization that can be done is to partially share weights of networks \u03b8 and \u03c6. It is justi\ufb01ed as \ufb01rst layers of both networks correspond to basic features extraction and these features are likely to be the same for optimal policy and value function. While it reduces the number of training parameters almost twice, it might destabilize learning process as the scales of gradient (40) and 27there is also an asynchronous modi\ufb01cation of advantage actor critic algorithm (A3C) which accelerates the training process by storing a copy of network for each thread and performing weights synchronization from time to time. 32 gradient of critic\u2019s MSE loss may be signi\ufb01cantly di\ufb00erent, so they should be balanced with additional hyperparameter. Algorithm 6: Advantage Actor-Critic (A2C) Hyperparameters: B \u2014 batch size, V \u2217 \u03c6 \u2014 critic neural network, \u03c0\u03b8 \u2014 actor neural network, \u03b1 \u2014 critic loss scaling, SGD optimizer. Initialize weights \u03b8, \u03c6 arbitrary On each step: 1. obtain a roll-out of size B using policy \u03c0(\u03b8) 2. for each transition T from the roll-out compute advantage estimation: A\u03c0(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2212 V \u03c0 \u03c6 3. compute target (detached from computational graph to prevent backpropagation): y(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) 4. compute critic loss: Loss = 1 B \ufffd T \ufffd y(T ) \u2212 V \u03c0 \u03c6 \ufffd2 5. compute critic gradients: \u2207critic = \u2202 Loss \u2202\u03c6 6. compute actor gradient: \u2207actor = 1 B \ufffd T \u2207\u03b8 log \u03c0\u03b8(a | s)A\u03c0(T ) 7. make a step of gradient descent using \u2207actor + \u03b1\u2207critic 5.4. Generalized Advantage Estimation (GAE) There is a design dilemma in Advantage Actor Critic algorithm concerning the choice whether to use the critic to estimate Q\u03c0(s, a) and introduce bias into gradient estimation or to restrict critic employment only for baseline and cause higher variance with necessity of playing the whole episodes for each update step. Actually, the range of possibilities is wider. Since Actor-Critic is an on-policy algorithm by design, we are free to use N-step approximations instead of one-step: using Q\u03c0(s, a) \u2248 N\u22121 \ufffd n=0 \u03b3nr(n+1) + \u03b3NV \u03c0 \ufffd s(N)\ufffd we can de\ufb01ne N-step advantage estimator as A\u03c0 (N)(s, a) := N\u22121 \ufffd n=0 \u03b3nr(n+1) + \u03b3NV \u03c0 \u03c6 \ufffd s(N)\ufffd \u2212 V \u03c0 \u03c6 (s) For N = 1 this estimation corresponds to Actor-Critic one-step estimation with high bias and low variance. For N = \u221e it yields the estimator with critic used only for baseline with no bias and high variance. Intermediate values correspond to something in between. Note that to use N-step advantage estimation we have to perform N steps of interaction after given state-action pair. 33 Usually \ufb01nding a good value for N as hyperparameter is di\ufb03cult as its \u00aboptimal\u00bb value may \ufb02oat throughout the learning process. In Generalized Advantage Estimation (GAE) [20] it is proposed to construct an ensemble out of di\ufb00erent N-step advantage estimators using exponential smoothing with some hyperparameter \u03bb: A\u03c0 GAE(s, a) := (1 \u2212 \u03bb) \ufffd A\u03c0 (1)(s, a) + \u03bbA\u03c0 (2)(s, a) + \u03bb2A\u03c0 (3)(s, a) + . . . \ufffd (42) Here the parameter \u03bb \u2208 [0, 1] allows smooth control over bias-variance trade-o\ufb00: \u03bb = 0 corresponds to Actor-Critic with higher bias and lower variance while \u03bb \u2192 1 corresponds to REINFORCE with no bias and high variance. But unlike N as hyperparameter, it uses mix of di\ufb00erent estimators in intermediate case. GAE proved to be a convenient way how more information can be obtained from collected rollout in practice. Instead of waiting for episode termination to compute (42) we may use \u00abtruncated\u00bb GAE which ensembles only those N-step advantage estimators that are available: A\u03c0 trunc.GAE(s, a) := A\u03c0 (1)(s, a) + \u03bbA\u03c0 (2)(s, a) + \u03bb2A\u03c0 (3)(s, a) + \u00b7 \u00b7 \u00b7 + \u03bbN\u22121A\u03c0 (N)(s, a) 1 + \u03bb + \u03bb2 + \u00b7 \u00b7 \u00b7 + \u03bbN\u22121 Note that the amount N of available estimators may be di\ufb00erent for di\ufb00erent transitions from rollout: if we performed K steps of interaction in some instance of environment starting from some state-action pair s, a, we can use N = K step estimators; for next state-action pair s\u2032, a\u2032 we have only N = K\u22121 transitions and so on, while the last state-action pair sN\u22121, aN\u22121 can be estimated only using A\u03c0 (1) as only N = 1 following transition is available. Although di\ufb00erent transitions are estimated with di\ufb00erent precision (leading to di\ufb00erent bias and variance), this approach allows to use all available information for each transition and utilize multi-step approximations without dropping last transitions of roll-outs used only for target computation. 5.5. Natural Policy Gradient (NPG) In this section we discuss the motivation and basic principles behind the idea of natural gradient descent, which we will require for future references. The standard gradient descent optimization method is known to be extremely sensitive to the choice of parametrization. Suppose we attempt to solve the following optimization task: f(q) \u2192 min q where q is a distribution and F is arbitrary di\ufb00erentiable function. We often restrict q to some parametric family and optimize similar objective, but with respect to some vector of parameters \u03b8 as unknown variable: f(q\u03b8) \u2192 min \u03b8 Classic example of such problem is maximum likelihood task when we try to \ufb01t the parameters of our model to some observed data. The problem is that when using standard gradient descent both the convergence rate and overall performance of optimization method substantially depend on the choice of parametrization q\u03b8. The problem holds even if we \ufb01x speci\ufb01c distribution family as many distribution families allow di\ufb00erent parametrizations. To see why gradient descent is parametrization-sensitive, consider the model which is used at some current point \u03b8k to determine the direction of next optimization step: \ufffd f(q\u03b8k) + \u27e8\u2207\u03b8f(q\u03b8k), \u03b4\u03b8\u27e9 \u2192 min \u03b4\u03b8 \u2225\u03b4\u03b8\u22252 2 < \u03b1k where \u03b1k is learning rate at step k. Being \ufb01rst-order method, gradient descent constructs a \u00abmodel\u00bb which approximates F locally around \u03b8k using \ufb01rst-order Taylor expansion and employs standard Euclidean metric to determine a region of trust for this model. Then this surrogate task is solved analytically to obtain well-known update formula: \u03b4\u03b8 \u221d \u2212\u2207\u03b8f(q\u03b8k) 34 The issue arises from reliance on Eucliden metric in the space of parameters. In most parametrizations, small changes in parameters space do not guarantee small change in distribution space and vice versa: some small changes in distribution may demand big steps in parameters space28. Natural gradient proposes to use another metric, which achieves invariance to parametrization of distribution q using the properties of Fisher matrix: De\ufb01nition 11. For distribution q\u03b8 Fisher matrix Fq(\u03b8) is de\ufb01ned as Fq(\u03b8) := Ex\u223cq\u2207\u03b8 log q\u03b8(x)(\u2207\u03b8 log q\u03b8(x))T Note that Fisher matrix depends on parametrization. Yet for any parametrization it is guaranteed to be positive semi-de\ufb01nite by de\ufb01nition. Moreover, it induces a so-called Riemannian metric29 in the space of parameters: d(\u03b81, \u03b82)2 := (\u03b82 \u2212 \u03b81)T Fq(\u03b81)(\u03b82 \u2212 \u03b81) In natural gradient descent it is proposed to use this metric instead of Euclidean: \ufffd f(q\u03b8k) + \u27e8\u2207\u03b8f(q\u03b8k), \u03b4\u03b8\u27e9 \u2192 min \u03b4\u03b8 \u03b4\u03b8T Fq(\u03b8k)\u03b4\u03b8 < \u03b1k This surrogate task can be solved analytically to obtain the following optimization direction: \u03b4\u03b8 \u221d \u2212Fq(\u03b8k)\u22121\u2207\u03b8f(q\u03b8k) (43) The direction of gradient descent is corrected by Fisher matrix which concerns the scale across different axes. This direction, speci\ufb01ed by Fq(\u03b8k)\u22121\u2207\u03b8f(q\u03b8k), is called natural gradient. Let\u2019s discuss why this new metric really provides us invariance to distribution parametrization. We already obtained natural gradient for q being parameterized by \u03b8 (43). Assume that we have another parametrization q\u03bd. These new parameters \u03bd are somehow related to \u03b8; we suppose there is some functional dependency \u03b8(\u03bd), which we assume to be di\ufb00erentiable with jacobian J. In this notation: \u03b4\u03b8 = J\u03b4\u03bd, Jij := \u2202\u03b8i \u2202\u03bdj (44) The central property of Fisher matrix, which provides the desired invariance, is the following: Proposition 21. If \u03b8 = \u03b8(\u03bd) with jacobian J, then reparametrization formula for Fisher matrix is Fq(\u03bd) = JT Fq(\u03b8)J (45) Now it can be derived that natural gradient for parametrization with \u03bd is the same as for \u03b8. If we want to calculate natural gradient in terms of \u03bd, then our step is, according to (44): \u03b4\u03b8 = J\u03b4\u03bd = {natural gradient in terms of \u03bd} \u221d JFq(\u03bdk)\u22121\u2207\u03bdf(q\u03bdk) = {Fisher matrix reparametrization (45)} = J \ufffd JT Fq(\u03b8k)J \ufffd\u22121 \u2207\u03bdf(q\u03bdk) {chain rule} = J \ufffd JT Fq(\u03b8k)J \ufffd\u22121 \u2207\u03bd\u03b8(\u03bdk)T \u2207\u03b8f(q\u03b8k) = = JJ\u22121Fq(\u03b8k)\u22121J\u2212T JT \u2207\u03b8f(q\u03b8k) = = Fq(\u03b8k)\u22121\u2207\u03b8f(q\u03b8k) 28classic example is that N (0, 100) is similar to N (1, 100) while N (0, 0.1) is completely di\ufb00erent from N (1, 0.1), although Euclidean distance in parameter space is the same for both pairs. 29in Euclidean space the general form of scalar product is \u27e8x, y\u27e9 := xT Gy, where G is \ufb01xed positive semi-de\ufb01nite matrix. The metric induced by this scalar product is correspondingly d(x, y)2 := (y \u2212x)T G(y \u2212x). The di\ufb00erence in Riemannian space is that G, called metric tensor, depends on x, so the relative distance may vary for di\ufb00erent points. It is used to describe the distances between points on manifolds and holds important properties which Fisher matrix inherits as metric tensor for distribution space. 35 which can be seen to be the same as in (43). Application of natural gradient descent in DRL setting is complicated in practice. Theoretically, the only change that must be done is scaling of gradient using inverse Fisher matrix (43). Yet, Fisher matrix requires n2 memory and O(n3) computational costs for inversion where n is the number of parameters. For neural networks this causes the same complications as the application of secondorder optimization methods. K-FAC optimization method [13] provides a speci\ufb01c approximation form of Fisher matrix for neural networks with linear layers which can be e\ufb03ciently computed, stored and inverted. Usage of K-FAC approximation allows to compute natural gradient directly using (43). 5.6. Trust-Region Policy Optimization (TRPO) The main drawback of Actor-Critic algorithm is believed to be the abandonment of experience that was used for previous updates. As the number of updates required is usually huge, this is considered to be a substantial loss of information. Yet, it is not clear how this information can be e\ufb00ectively used for newer updates. Suppose we want to make an update of \u03c0(\u03b8), but using samples collected by some \u03c0old. The straightforward approach is importance sampling technique, which naive application to gradient formula (40) yields the following result: \u2207\u03b8J(\u03b8) = ET \u223c\u03c0old P(T | \u03c0(\u03b8)) P(T | \u03c0old) \ufffd t=0 \u2207\u03b8 log \u03c0\u03b8(at | st)A\u03c0(st, at) The emerged importance sampling weight is actually computable as transition probabilities cross out: P(T | \u03c0(\u03b8)) P(T | \u03c0old) = \ufffd t=1 \u03c0\u03b8(at | st) \ufffd t=1 \u03c0old(at | st) The problem with this coe\ufb03cient is that it tends either to be exponentially small or to explode. Even with some heuristic normalization of coe\ufb03cients the batch gradient would become dominated by one or several transitions and destabilize the training procedure by introducing even more variance. Notice that application of importance sampling to another representation of gradient (37) yields seemingly di\ufb00erent result: \u2207\u03b8J(\u03b8) = E\u03c0old d\u03c0(\u03b8)(s) d\u03c0old(s) \u03c0\u03b8(a | s) \u03c0old(a | s)\u2207\u03b8 log \u03c0\u03b8(a | s)A\u03c0(s, a) (46) Here we avoided common for the whole trajectories importance sampling weights by using the definition of state visitation frequencies. But this result is even less practical as these frequencies are unknown to us. The \ufb01rst key idea behind the theory concerning this problem is that may be these importance sampling coe\ufb03cients behave more stable if the policies \u03c0old and \u03c0(\u03b8) are in some terms \u00abclose\u00bb. Intuitively, in this case d\u03c0(\u03b8)(s) d\u03c0old(s) of formula (46) is close to 1 as state visitation frequencies are similar, and the remained importance sampling coe\ufb03cient becomes acceptable in practice. And if some two policies are similar, their values of our objective (2) are probably close too. For any two policies, \u03c0 and \u03c0old: J(\u03c0) \u2212 J(\u03c0old) = ET \u223c\u03c0 \ufffd t=0 \u03b3tr(st) \u2212 J(\u03c0old) = = ET \u223c\u03c0 \ufffd t=0 \u03b3tr(st) \u2212 V \u03c0old(s0) = = ET \u223c\u03c0 \ufffd\ufffd t=0 \u03b3tr(st) \u2212 V \u03c0old(s0) \ufffd = {trick \ufffd\u221e t=0 (at+1 \u2212 at) = \u2212a0 30} = ET \u223c\u03c0 \ufffd\ufffd t=0 \u03b3tr(st) + \ufffd t=0 \ufffd \u03b3t+1V \u03c0old(st+1) \u2212 \u03b3tV \u03c0old(st) \ufffd\ufffd = {regroup} = ET \u223c\u03c0 \ufffd t=0 \u03b3t \ufffd r(st) + \u03b3V \u03c0old(st+1) \u2212 V \u03c0old(st) \ufffd = 36 {by de\ufb01nition (3)} = ET \u223c\u03c0 \ufffd t=0 \u03b3t \ufffd Q\u03c0old(st, at) \u2212 V \u03c0old(st) \ufffd {by de\ufb01nition (14)} = ET \u223c\u03c0 \ufffd t=0 \u03b3tA\u03c0old(st, at) The result obtained above is often referred to as relative policy performance identity and is actually very interesting: it states that we can substitute reward with advantage function of arbitrary policy and that will shift the objective by the constant. Using the discounted state visitation frequencies de\ufb01nition 10, relative policy performance identity can be rewritten as J(\u03c0) \u2212 J(\u03c0old) = 1 1 \u2212 \u03b3 Es\u223cd\u03c0(s)Ea\u223c\u03c0(a|s)A\u03c0old(s, a) Now assume we want to optimize parameters \u03b8 of policy \u03c0 while using data collected by \u03c0old: applying importance sampling in the same manner: J(\u03c0\u03b8) \u2212 J(\u03c0old) = 1 1 \u2212 \u03b3 Es\u223cd\u03c0old(s) d\u03c0\u03b8(s) d\u03c0old(s)Ea\u223c\u03c0old(a|s) \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) As we have in mind the idea of \u03c0old being close to \u03c0\u03b8, the question is how well this identity can be approximated if we assume d\u03c0\u03b8(s) = d\u03c0old(s). Under this assumption: J(\u03c0\u03b8) \u2212 J(\u03c0old) \u2248 L\u03c0old(\u03b8) := 1 1 \u2212 \u03b3 Es\u223cd\u03c0old(s)Ea\u223c\u03c0old(a|s) \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) The point is that interaction using \u03c0old corresponds to sampling from the expectations presented in L\u03c0old(\u03b8): L\u03c0old(\u03b8) = E\u03c0old \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) The approximation quality of L\u03c0old(\u03b8) can be described by the following theorem: Proposition 22. [19] \ufffd\ufffdJ(\u03c0\u03b8) \u2212 J(\u03c0old) \u2212 L\u03c0old(\u03b8) \ufffd\ufffd \u2264 C max s KL(\u03c0old \u2225 \u03c0\u03b8)[s] where C is some constant and KL(\u03c0old \u2225 \u03c0\u03b8)[s] is a shorten notation for KL(\u03c0old(a | s) \u2225 \u03c0\u03b8(a | s)). There is an important corollary of proposition 22: J(\u03c0\u03b8) \u2212 J(\u03c0old) \u2265 L\u03c0old(\u03b8) \u2212 C max s KL(\u03c0old \u2225 \u03c0\u03b8)[s] which not only states that expression on the right side represents a lower bound, but also that the optimization procedure \u03b8k+1 = argmax \u03b8 \ufffd L\u03c0\u03b8k (\u03b8) \u2212 C max s KL(\u03c0\u03b8k \u2225 \u03c0\u03b8)[s] \ufffd (47) will yield a policy with guaranteed monotonic improvement31. In practice there are several obstacles which preserve us from obtaining such procedure. First of all, our advantage function estimation is never precise. Secondly, it is hard to estimate precise value of constant C. One last obstacle is that it is not clear how to calculate KL-divergence in its maximal form (with max taken across all states). 30and if MDP is episodic, for terminal states V \u03c0old(sT ) = 0 by de\ufb01nition. 31the maximum of lower bound is non-negative as its value for \u03b8 = \u03b8k equals zero, which causes J(\u03c0k+1)\u2212J(\u03c0k) \u2265 0. 37 In Trust-Region policy optimization [19] the idea of practical algorithm, approximating procedure (47), is analyzed. To address the last issue, the naive approximation is proposed to substitute max with averaging across states32: max s KL(\u03c0old \u2225 \u03c0\u03b8)[s] \u2248 Es\u223cd\u03c0old(s) KL(\u03c0old \u2225 \u03c0\u03b8)[s] The second step of TRPO is to rewrite the task of unconstrained minimization (47) in equivalent constrained (\u00abtrust-region\u00bb) form33 to incorporate the unknown constant C into learning rate: \ufffd L\u03c0old(\u03b8) \u2192 max \u03b8 Es\u223cd(s|\u03c0old) KL(\u03c0old \u2225 \u03c0\u03b8)[s] < C (48) Note that this rewrites an update iteration in terms of optimization methods: while L\u03c0old(\u03b8) is an approximation of true objective J(\u03c0\u03b8) \u2212 J(\u03c0old), the constraint sets the region of trust to the surrogate. Remark that constraint is actually a divergence in policy space, i. e. it is very similar to a metric in the space of distributions while the surrogate is a function of the policy and depends on parameters \u03b8 only through \u03c0\u03b8. To solve the constrained problem (48), the technique from convex optimization is used. Assume that \u03c0old is a current policy and we want to update its parameters \u03b8k. Then the objective of (48) is modeled using \ufb01rst-order Taylor expansion around \u03b8k while constraint is modeled using secondorder 34 Taylor approximation: \ufffd L\u03c0old(\u03b8k + \u03b4\u03b8) \u2248 \u27e8\u2207\u03b8 L\u03c0old(\u03b8)|\u03b8k , \u03b4\u03b8\u27e9 \u2192 max \u03b4\u03b8 Es\u223cd(s|\u03c0old) KL(\u03c0old \u2225 \u03c0\u03b8k+\u03b4\u03b8) \u2248 1 2Es\u223cd(s|\u03c0old)\u03b4\u03b8T \u22072 \u03b8 KL(\u03c0old \u2225 \u03c0\u03b8) \ufffd\ufffd \u03b8k \u03b4\u03b8 < C It turns out, that this model is equivalent to natural policy gradient, discussed in sec. 5.5: Proposition 23. \u22072 \u03b8 KL(\u03c0\u03b8 \u2225 \u03c0old)[s] \ufffd\ufffd \u03b8k = F\u03c0(a|s)(\u03b8) so KL-divergence constraint can be approximated with metric induced by Fisher matrix. Moreover, the gradient of surrogate function is \u2207\u03b8L\u03c0old(\u03b8)|\u03b8k = E\u03c0old \u2207\u03b8\u03c0\u03b8(a | s)|\u03b8k \u03c0old(a | s) A\u03c0old(s, a) = {\u03c0old = \u03c0\u03b8k} = E\u03c0old\u2207\u03b8 log \u03c0\u03b8k(a | s)A\u03c0old(s, a) which is exactly an Actor-Critic gradient. Therefore the formula of update step is given by \u03b4\u03b8 \u221d \u2212F\u03c0(\u03b8)\u22121\u2207\u03b8L\u03c0old(\u03b8) where \u2207\u03b8L\u03c0old(\u03b8) coincides with standard policy gradient, and F\u03c0(\u03b8) is hessian of KL-divergence: F\u03c0(\u03b8) := Es\u223cd\u03c0old(s) \u22072 \u03b8 KL(\u03c0old \u2225 \u03c0\u03b8) \ufffd\ufffd \u03b8k In practical implementations KL-divergence can be Monte-Carlo estimated using collected rollout. The size of roll-out must be signi\ufb01cantly bigger than in Actor-Critic to achieve su\ufb03cient precision of hessian estimation. Then to obtain a direction of optimization step the following system of linear equations F\u03c0(\u03b8)\u03b4\u03b8 = \u2212\u2207\u03b8L\u03c0old(\u03b8) is solved using a conjugate gradients method which is able to work with Hessian-vector multiplication procedure instead of requiring to calculate F\u03c0(\u03b8) explicitly. 32the distribution from which the states come is set to be d\u03c0old(s) for convenience as this is the distribution from which they come in L\u03c0old(\u03b8). 33the unconstrained objective is Lagrange function for constrained form. 34as \ufb01rst-order term is zero. 38 TRPO also accompanies the update step with a line-search procedure which dynamically adjusts step length using standard backtracking heuristic. As TRPO intuitively seeks for policy improvement on each step, the idea is to check whether the lower bound (47) is positive after the biggest step allowed according to KL-constraint and reduce the step size until it becomes positive. Unlike Actor-Critic, TRPO performs extremely expensive complicated update steps but requires relatively small number of iterations in return. Of course, due to many approximations done, the overall procedure is only a resemblance of theoretically-justi\ufb01ed iterations (47) providing improvement guarantees. 5.7. Proximal Policy Optimization (PPO) Proximal Policy Optimization [21] proposes alternative heuristic way of performing lower bound (47) optimization which demonstrated encouraging empirical results. PPO still substitutes max s KL on average, but leaves the surrogate in unconstrained form, suggesting to treat unknown constant C as a hyperparameter: E\u03c0old \ufffd \u03c0\u03b8(a | s) \u03c0old(a | s)A\u03c0old(s, a) \u2212 C KL(\u03c0old \u2225 \u03c0\u03b8)[s] \ufffd \u2192 max \u03b8 (49) The naive idea would be to straightforwardly optimize (49) as it is equivalent to solving the constraint trust-region task (48). To avoid Hessian-involved computations, one possible option is just to perform one step of \ufb01rst-order gradient optimization of (49). Such algorithm was empirically discovered to perform poorly as importance sampling coe\ufb03cients \u03c0\u03b8(a|s) \u03c0old(a|s) tend to unbounded growth. In PPO it is proposed to cope with this problem in a simple old-fashioned way: by clipping. Let\u2019s denote by r(\u03b8) := \u03c0\u03b8(a | s) \u03c0old(a | s) an importance sampling weight and by rclip(\u03b8) := clip(r(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) its clipped version where \u03f5 \u2208 (0, 1) is a hyperparameter. Then the clipped version of lower bound is: E\u03c0old \ufffd min \ufffd r(\u03b8)A\u03c0old(s, a), rclip(\u03b8)A\u03c0old(s, a) \ufffd \u2212 C KL(\u03c0old \u2225 \u03c0\u03b8)[s] \ufffd \u2192 max \u03b8 (50) Here the minimum operation is introduced to guarantee that the surrogate objective remains a lower bound. Thus the clipping at 1 + \u03f5 may occur only in the case if advantage is positive while clipping at 1 \u2212 \u03f5 may occur if advantage is negative. In both cases, clipping represents a penalty for importance sampling weight r(\u03b8) being too far from 1. The overall procedure suggested by PPO to optimize the \u00abstabilized\u00bb version of lower bound (50) is the following. A roll-out is collected using current policy \u03c0old with some parameters \u03b8. Then the batches of typical size (as for Actor-Critic methods) are sampled from collected roll-out and several steps of SGD optimization of (50) proceed with respect to policy parameters \u03b8. During this process the policy \u03c0old is considered to be \ufb01xed and new interaction steps are not performed, while in implementations there is no need to store old weights \u03b8k since everything required from \u03c0old is to collect transitions and remember the probabilities \u03c0old(a | s). The idea is that during these several steps we may use transitions from the collected roll-out several times. Similar alternative is to perform several epochs of training by passing through roll-out several times, as it is often done in deep learning. Interesting fact discovered by the authors of PPO during ablation studies is that removing KLpenalty term doesn\u2019t a\ufb00ect the overall empirical performance. That is why in many implementations PPO does not include KL-term at all, making the \ufb01nal surrogate objective have a following form: E\u03c0old min \ufffd r(\u03b8)A\u03c0old(s, a), rclip(\u03b8)A\u03c0old(s, a) \ufffd \u2192 max \u03b8 (51) Note that in this form the surrogate is not generally a lower bound and \u00abimprovement guarantees\u00bb intuition is lost. 39 Algorithm 7: Proximal Policy Optimization (PPO) Hyperparameters: B \u2014 batch size, R \u2014 rollout size, n_epochs \u2014 number of epochs, \u03b5 \u2014 clipping parameter, V \u2217 \u03c6 \u2014 critic neural network, \u03c0\u03b8 \u2014 actor neural network, \u03b1 \u2014 critic loss scaling, SGD optimizer. Initialize weights \u03b8, \u03c6 arbitrary On each step: 1. obtain a roll-out of size R using policy \u03c0(\u03b8), storing action probabilities as \u03c0old(a | s). 2. for each transition T from the roll-out compute advantage estimation (detached from computational graph to prevent backpropagation): A\u03c0(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2212 V \u03c0 \u03c6 3. perform n_epochs passes through roll-out using batches of size B; for each batch: \u2022 compute critic target (detached from computational graph to prevent backpropagation): y(T ) = r\u2032 + \u03b3V \u03c0 \u03c6 (s\u2032) \u2022 compute critic loss: Loss = 1 B \ufffd T \ufffd y(T ) \u2212 V \u03c0 \u03c6 \ufffd2 \u2022 compute critic gradients: \u2207critic = \u2202 Loss \u2202\u03c6 \u2022 compute importance sampling weights: r\u03b8(T ) = \u03c0\u03b8(a | s) \u03c0old(a | s) \u2022 compute clipped importance sampling weights: rclip \u03b8 (T ) = clip(r\u03b8(T ), 1 \u2212 \u03f5, 1 + \u03f5) \u2022 compute actor gradient: \u2207actor = 1 B \ufffd T \u2207\u03b8 min \ufffd r\u03b8(T )A\u03c0(T ), rclip \u03b8 (T )A\u03c0(T ) \ufffd \u2022 make a step of gradient descent using \u2207actor + \u03b1\u2207critic 40 6. Experiments 6.1. Setup We performed our experiments using custom implementation of discussed algorithms attempting to incorporate best features from di\ufb00erent o\ufb03cial and uno\ufb03cial sources and unifying all algorithms in a single library interface. The full code is available at our github. While custom implementation might not be the most e\ufb03cient, it hinted us several ambiguities in algorithms which are resolved di\ufb00erently in di\ufb00erent sources. We describe these nuances and the choices made for our experiments in appendix A. For each environment we launch several algorithms to train the network with the same architecture with the only exception being the head which is speci\ufb01ed by the algorithm (see table 1). DQN Linear transformation to |A| arbitrary real values Dueling First head: linear transformation to |A| arbitrary real values Second head: linear transformations to an arbitrary scalar Aggregated using dueling architecture formula (17) Categorical |A| linear transformations with softmax to A values Dueling Categorical First head: linear transformation to |A| arbitrary real values Second head: |A| linear transformations to A arbitrary real values Aggregated using dueling architecture formula (32) Quantile |A| linear transformations to A arbitrary real values Dueling Quantile First head: linear transformation to |A| arbitrary real values Second head: |A| linear transformations to A arbitrary real values Aggregated using dueling architecture formula (32) without softmax A2C / PPO Actor head: linear transformation with softmax to |A| values Critic head: linear transformation to scalar value Table 1: Heads used for di\ufb00erent algorithms. Here |A| is the number of actions and A is the chosen number of atoms. For noisy networks all fully-connected layers in the feature extractor and in the head are substituted with noisy layers, doubling the number of their trained parameters. Both usage of noisy layers and the choice of the head in\ufb02uences the total number of parameters trained by the algorithm. As practical tuning of hyperparameters is computationally consuming activity, we set all hyperparameters to their recommended values while trying to share the values of common hyperparameters among algorithms without a\ufb00ecting overall performance. We choose to give each algorithm same amount of interaction steps to provide the fair comparison of their sample e\ufb03ciency. Thus the wall-clock time, number of episodes played and the number of network parameters updates varies for di\ufb00erent algorithms. 6.2. Cartpole Cartpole from OpenAI Gym [2] is considered to be one of the simplest environments for DRL algorithms testing. The state is described with 4 real numbers while action space is two-dimensional discrete. The environment rewards agent with +1 each tick until the episode ends. Poor action choices lead to early termination. The game is considered solved if agent holds for 200 ticks, therefore 200 is maximum reward in this environment. In our \ufb01rst experiment we launch algorithms for 10 000 interaction steps to train a neural network on the Cartpole environment. The network consists of two fully-connected hidden layers with 128 neurons and an algorithm-speci\ufb01c head. We used ReLU for activations. The results of a single launch are provided35 in table 2. 35we didn\u2019t tune hyperparameters for each of the algorithms, so the con\ufb01gurations used might not be optimal. 41 Reached 200 Average reward Average FPS Double DQN 23.0 126.17 95.78 Dueling Double DQN 27.0 121.78 62.65 DQN 33.0 116.27 101.53 Categorical DQN 28.0 110.87 74.95 Prioritized Double DQN 37.0 110.52 85.58 Categorical Prioritized Double DQN 46.0 104.86 66.00 Quantile Prioritized Double DQN 42.0 100.76 68.62 Categorical DQN with target network 44.0 96.08 73.92 Quantile Double DQN 54.0 93.14 75.40 Quantile DQN 70.0 88.12 77.93 Categorical Double DQN 42.0 81.25 70.90 Noisy Quantile Prioritized Dueling DQN 86.0 74.13 21.41 Twin DQN 57.0 71.14 52.51 Noisy Double DQN 67.0 71.06 31.81 Noisy Prioritized Double DQN 94.0 67.34 30.72 Quantile Regression Rainbow 106.0 67.11 21.54 Rainbow 91.0 64.01 20.35 Noisy Quantile Prioritized Double DQN 127.0 63.01 28.27 Noisy Categorical Prioritized Double DQN 63.0 62.04 27.81 PPO with GAE 144.0 53.06 390.53 Noisy Prioritized Dueling Double DQN 180.0 47.52 22.56 PPO 184.0 45.19 412.88 Noisy Categorical Prioritized Dueling Double DQN 428.0 22.09 20.63 A2C 12.30 1048.64 A2C with GAE 11.50 978.00 Table 2: Results on Cartpole for di\ufb00erent algorithms: number of episode when the highest score of 200 was reached, average reward across all played episodes and average number of frames processed in a second (FPS). 6.3. Pong We used Atari Pong environment from OpenAI Gym [2] as our main testbed to study the behaviour of the following algorithms: \u2022 DQN \u2014 Deep Q-learning (sec. 3.2) \u2022 c51 \u2014 Categorical DQN (sec. 4.2) \u2022 QR-DQN \u2014 Quantile Regression DQN (sec. 4.3) \u2022 Rainbow (sec. 4.4) \u2022 A2C \u2014 Advantage Actor Critic (sec. 5.3) extended with GAE (sec. 5.4) \u2022 PPO \u2014 Proximal Policy Optimization (sec. 5.7) extended with GAE (sec. 5.4) In Pong, each episode is split into rounds. Each round ends with player either winning or loosing. The episode ends when the player wins or looses 21 rounds. The reward is given after each round and is +1 for winning and -1 for loosing. Therefore the maximum total reward is 21 and the minimum is -21. Note that the \ufb02ag done indicating episode ending is not provided to the agent after each round but only at the end of full game (consisting of 21-41 rounds). The standard preprocessing for Atari games proposed in DQN [14] was applied to the environment (see table 3). Thus, state space is represented by (84, 84) grayscale pixels input (1 channel with domain [0, 255]). Action space is discrete with |A| = 6 actions. All algorithms were given 1 000 000 interaction steps to train the network with the same feature extractor presented on \ufb01g. 1. The number of trained parameters is presented in table 4. All used hyperparameters are listed in table 7 in appendix B. 42 NoopResetEnv Do nothing \ufb01rst 30 frames of games to imitate the pause between game start and real player reaction. MaxAndSkipEnv Each interaction steps takes 4 frames of the game to allow less frequent switch of action. Max is taken over 4 passed frames to obtain an observation. FireResetEnv Presses \u00abFire\u00bb button at \ufb01rst frame to launch the game, otherwise screen remains frozen. WarpFrame Turns observation to grayscale image of size 84x84. Table 3: Atari Pong preprocessing Algorithm Number of trained parameters DQN 1 681 062 c51 1 834 962 QR-DQN 1 834 962 Rainbow 3 650 410 A2C 1 681 575 PPO 1 681 575 Table 4: Number of trained parameters in Pong experiment. 6.4. Interaction-training trade-o\ufb00 in value-based algorithms There is a common belief that policy gradient algorithms are much faster in terms of computational costs while value-based algorithms are preferable when simulation is expensive because of their sample e\ufb03ciency. This follows from the nature of algorithms, as the fraction \u00abobservations per network updates\u00bb is extremely di\ufb00erent for these two families: indeed, in DQN it is often assumed to perform one network update after each new transitions, while A2C collects about 32-40 observations for only one update. That makes the number of network updates performed during 1M steps interaction process substantially di\ufb00erent and is the main reason of policy gradients speed rate. Also policy gradient algorithms use several threads for parallel simulations (8 in our experiments) while value-based algorithms are formally single-threaded. Yet they can also enjoy multi-threaded interaction, in the simplest form by playing 1 step in all instances of environment and then performing L steps of network optimization [8]. For consistency with single-threaded case it is reasonable to set the value of L to be equal to the number of threads to maintain the same fraction \u00abobservations per network updates\u00bb. However it has been reported that lowering value of L in two or four times can positively a\ufb00ect wall-clock time with some loss of sample e\ufb03ciency, while raising batch size may mitigate this downgrade. The overall impact of such acceleration of value-based algorithms on performance properties is not well studied and may alter their behaviour. In our experiments on Pong it became evident that value-based algorithms perform extensive amount of redundant network optimization steps, absorbing knowledge faster than novel information from new transitions comes in. This reasoning in particular follows from the success of PPO on Pong task which performs more than 10 times less network updates. Vanilla algorithm Accelerated version Threads 1 8 Batch size 32 128 L 1 2 Interactions per update 1 4 Table 5: Setup for value-based acceleration experiment We compared two versions of value-based algorithms: vanilla version, which is single-threaded with standard batch size (32) and L = 1 meaning that each observed transition is followed with one network optimization step, and accelerated version, where 1 interaction step is performed in 8 parallel instances of environment and L is set to be 2 instead of 8 which raises the fraction \u00abob43 Convolution\u00a08x8\u00a0 with\u00a0stride\u00a0=\u00a04 Convolution\u00a04x4\u00a0 with\u00a0stride\u00a0=\u00a02 Convolution\u00a03x3\u00a0 with\u00a0stride\u00a0=\u00a01 Fully\u00adconnected\u00a0layer  Algorithm\u00adspecific\u00a0head  (1, 84, 84) (32, 20, 20) (64, 9, 9) (64, 7, 7) 512\u00a0FEATURES Figure 1: Network used for Atari Pong. All activation functions are ReLU. For Rainbow the fully-connected layer and all dense layers in the algorithm-speci\ufb01c head are substituted with noisy layers. servations per training step\u00bb in four times. To compensate this change we raised batch size in four times. As expected, average speed of algorithms increases in approximately 3.5 times (see table 6). We provide training curves with respect to 1M performed interaction steps on \ufb01g. 2 and with respect to wall-clock time on \ufb01g. 3. The only vanilla algorithm that achieved better \ufb01nal score comparing to its accelerated rival is QR-DQN, while other three algorithms demonstrated both acceleration and performance improvement. The latter is probably caused by randomness as relaunch of algorithms within the same setting and hyperparameters can be strongly in\ufb02uenced by random seed. It can be assumed that fraction \u00abobservations per updates\u00bb is an important hyperparameter of value-based algorithms which can control the trade-o\ufb00 between wall-clock time and sample e\ufb03ciency. From our results it follows that low fraction leads to excessive network updates and may slow down learning in several times. Yet this hyperparameter can barely be tuned universally for all kinds of tasks opposed to many other hyperparameters that usually have their recommended default values. We stick further to the accelerated version and use its results in \ufb01nal comparisons. 6.5. Results We compare the results of launch of six algorithms on Pong from two perspectives: sample e\ufb03ciency (\ufb01g. 4) and wall-clock time (\ufb01g. 5). We do not compare \ufb01nal performance of these algorithms as all six algorithms are capable to reach near-maximum \ufb01nal score on Pong given more iterations, while results after 1M iterations on a single launch signi\ufb01cantly depend on chance. All algorithms start with a warm-up session during which they try to explore the environment and 44 Interactions per update Average transitions per second Algorithm vanilla accelerated vanilla accelerated DQN 1 4 55.74 168.43 c51 1 4 44.08 148.76 QR-DQN 1 4 47.46 155.97 Rainbow 1 4 19.30 70.22 A2C 40 656.25 PPO 10.33 327.13 Table 6: Computational e\ufb03ciency of vanilla and accelerated versions. 0 200000 400000 600000 800000 1000000 interaction step 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes Acceleration's influence on sample efficiency DQN accelerated DQN vanilla c51 accelerated c51 vanilla QR-DQN accelerated QR-DQN vanilla Rainbow accelerated Rainbow vanilla Figure 2: Training curves of vanilla and accelerated version of value-based algorithms on 1M steps of Pong. Although accelerated versions perform network updates four times less frequent, the performance degradation is not observed. learn \ufb01rst dependencies how the result of random behaviour can be surpassed. Epsilon-greedy with tuned parameters provides su\ufb03cient amount of exploration for DQN, c51 and QR-DQN whithout slowing down further learning while hyperparameter-free noisy networks are the main reason why Rainbow has substantially longer warm-up. Policy gradient algorithms incorporate exploration strategy in stochasticity of learned policy but underutilization of observed samples leads to almost 1M-frames warm-up for A2C. It can be observed that PPO successfully mitigates this problem by reusing samples thrice. Nevertheless, both PPO and A2C solve Pong relatively quickly after the warm-up stage is over. Value-based algorithm proved to be more computationally costly. QR-DQN and categorical DQN introduce more complicated loss computation, yet their slowdown compared to standard DQN is moderate. On the contrary, Rainbow is substantially slower mainly because of noise generation involvement. Furthermore, combination of noisy networks and prioritized replay results in even less stable training process. We provide loss curves for all six algorithms and statistics for noise magnitude and prioritized replay for Rainbow in appendix C; some additional visualizations of trained algorithms playing episodes of Pong are presented in appendix D. 45 0 200 400 600 800 minutes 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes Acceleration effect on value-based algorithms DQN accelerated DQN vanilla c51 accelerated c51 vanilla QR-DQN accelerated QR-DQN vanilla Rainbow accelerated Rainbow vanilla Figure 3: Training curves of vanilla and accelerated version of value-based algorithms on 1M steps of Pong from wall-clock time. 0 200000 400000 600000 800000 1000000 interaction step 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes Comparing different algorithms on Pong DQN c51 QR-DQN Rainbow A2C PPO Figure 4: Training curves of all algorithms on 1M steps of Pong. 0 50 100 150 200 minutes 20 15 10 5 0 5 10 15 20 average score for the last 20 episodes DQN (1h 38m) c51 (1h 52m) QR-DQN (1h 46m) Rainbow (3h 57m) A2C (0h 25m) PPO (0h 50m) Comparing wall-clock time of different algorithms on Pong DQN c51 QR-DQN Rainbow A2C PPO Figure 5: Training curves of all algorithms on 1M steps of Pong from wall-clock time. 46 7. Discussion We have concerned two main directions of universal model-free RL algorithm design and attempted to recreate several state-of-art pipelines. While the extensions of DQN are reasonable solutions of evident DQN problems, their e\ufb00ect is not clearly seen on simple tasks like Pong36. Current state-of-art in single-threaded value-based approach, Rainbow DQN, is full of \u00abglue and tape\u00bb decisions that might be not the most e\ufb00ective way of training process stabilization. Distributional value-based approach is one of the cheapest in terms of resources extensions of vanilla DQN algorithm. Although it is reported to provide substantial performance improvement in empirical experiments, the reason behind this result remains unclear as expectation of return is the key quantity for agent\u2019s decision making while the rest of learned distribution does not a\ufb00ect his choices. One hypothesis to explain this phenomenon is that attempting to capture wider range of dependencies inside given MDP may provide auxiliary helping tasks to the algorithm, leading to better learning of expectation. Intuitively it seems that more reasonable switch of DQN to distributional setting would be learning the Bayesian uncertainty of expectation of return given observed data, but scalable practical algorithms within this orthogonal paradigm are yet to be created. Policy gradient algorithms are aimed at direct optimization of objective and currently beat valuebased approach in terms of computational costs. They tend to have less hyperparameters but are extremely sensitive to the choice of optimizer parameters and especially learning rate. We have a\ufb03rmed the e\ufb00ectiveness of state-of-art algorithm PPO, which succeeded to solve Pong within an hour without hyperparameter tuning. Though on the one hand this algorithm was derived from TRPO theory, it essentially deviates from it and substitutes trust region updates with heuristic clipping. It can be observed in our results that PPO provides better gradients to the same network than DQN-based algorithms despite the absence of experience replay. While it is fair to assume that forgetting experienced transitions leads to information loss, it is also true that most observations stored in replay memory are already learned or contain no useful information. The latter makes most transitions in the sampled mini-batches insigni\ufb01cant, and, while prioritized replay attacks this issue, it might still be the case that current experience replay management techniques are imperfect. There are still a lot of deviations of empirical results from theoretical perspectives. It is yet unclear which techniques are of the highest potential and what explanation lies behind many heuristic elements composing current state-of-art results. Possibly essential elements of modeling human-like reinforcement learning are yet to be unraveled as active research in this area promises substantial acceleration, generalization and stabilization of DRL algorithms. 36although it takes several hours to train, Pong is considered to be the easiest of 57 Atari games and one of the most basic testbeds for RL algorithms. 47 ",
    "References": "References [1] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 449\u2013458. JMLR. org, 2017. [2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [3] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning with quantile regression. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [4] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017. [5] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning, volume 1. MIT press Cambridge, 2016. [6] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [7] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [8] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt, and D. Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018. [9] A. Irpan. Deep reinforcement learning doesn\u0432\u0490\u0404t work yet. Online (Feb. 14): https://www. alexirpan. com/2018/02/14/rl-hard. html, 2018. [10] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. [11] R. Koenker and G. Bassett Jr. Regression quantiles. Econometrica: journal of the Econometric Society, pages 33\u201350, 1978. [12] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [13] J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417, 2015. [14] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [15] C. Nota and P. S. Thomas. Is the policy gradient a gradient? arXiv preprint arXiv:1906.07073, 2019. [16] OpenAI. Openai \ufb01ve. https://blog.openai.com/openai-five/, 2018. [17] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. [18] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [19] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In Icml, volume 37, pages 1889\u20131897, 2015. [20] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 48 [22] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. [23] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. [24] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000. [25] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence, 2016. [26] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik, A. Huang, P. Georgiev, R. Powell, T. Ewalds, D. Horgan, M. Kroiss, I. Danihelka, J. Agapiou, J. Oh, V. Dalibard, D. Choi, L. Sifre, Y. Sulsky, S. Vezhnevets, J. Molloy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z. Wang, T. Pfa\ufb00, T. Pohlen, Y. Wu, D. Yogatama, J. Cohen, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, C. Apps, K. Kavukcuoglu, D. Hassabis, and D. Silver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/ alphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019. [27] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015. [28] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992. [29] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992. 49 Appendix A. Implementation details Here we describe several technical details of our implementation which may potentially in\ufb02uence the obtained results. In most papers on value-based algorithms hyperparameters recommended for Atari games assume raw input in the range [0, 255], while in various implementations of policy gradient algorithms normalized input in the range [0, 1] is considered. Stepping aside from these agreements may damage the convergence speed both for value-based and policy gradient algorithms as the change of input domain requires hyperparameters retuning. We use MSE loss emerged in theoretical intuition for DQN while in many sources it is recommended to use Huber loss37 instead to stabilize learning. In all value-based algorithms except c51 we update target network each K-th frame instead of exponential smoothing of its parameters as it is computationally cheaper. For c51 we remove target network heuristic as apriori limited domain prevents unbounded growth of predictions. We do not architecturally force quantiles outputted by the network in Quantile Regression DQN to satisfy \u03b60 \u2264 \u03b61 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b6A\u22121. As in the original paper, we assume that all A outputs of network are arbitrary real values and use a standard linear transformation as our last layer. In dueling architectures we subtract mean of A(s, a) across actions instead of theoretically assumed maximum as proposed by original paper authors. We implement sampling from prioritized replay using SumTree data structure and in informal experiments a\ufb03rmed the acceleration it provides. The importance sampling weight annealing \u03b2(t) is represented by initial value \u03b2(0) = \u03b2 which is then linearly annealed to 1 during \ufb01rst T\u03b2 frames; both \u03b2 and T\u03b2 are hyperparameters. We do not allow priorities P(T ) to be greater than 1 by clipping as suggested in the original paper. This may mitigate the e\ufb00ect of prioritization replay but stabilizes the process. As importance sampling weights w(T ) = 1 BP(T ) are potentially very close to zero, in original article it was proposed to normalize them on max w(T ). In some implementations the maximum is taken over the whole experience replay while in others maximum is taken over current batch, which is not theoretically justi\ufb01ed but computationally much faster. We stick to the latter option. For noisy layers we use factorized noise sampling: for layer with m inputs and n outputs we sample \u03b51 \u2208 Rn, \u03b52 \u2208 Rm from standard normal distributions and scale both using f(\u03b5) = sign(\u03b5)\u221a\u03b5. Thus we use f(\u03b51)f(\u03b52)T as our noise sample for weights matrix and f(\u03b52) as noise sample for bias. All noise is shared across mini-batch. Noise is resampled on each forward pass through the network and thus is independent between evaluation, selection and interaction. Despite all these simpli\ufb01cations, we found noisy layers to be the most computationally expensive modi\ufb01cation of DQN leading to substantial degradation of wall-clock time. For policy gradient algorithms we add additional policy entropy term to the loss to force exploration. We also de\ufb01ne actor loss as a scalar function that yields the same gradients as in the corresponding gradient estimation (40) for A2C to compute it using PyTorch mechanics. For PPO objective (51) provides analogous \u00abactor loss\u00bb; thus, in both policy gradient algorithms the full loss is de\ufb01ned as summation of actor, critic and entropy losses, with the two latter being scaled using scalar hyperparameters. We use shared network architecture for policy gradient algorithms with one feature extractor and two heads, one for policy and one for critic. KL-penalty is not used in our PPO implementation. Also we do not normalize advantage estimations across the roll-out to zero mean and unit standard deviation as additionally done in some implementations. We use PyTorch default initialization for linear and convolutional layers although orthogonal initialization of all layers is reported to be bene\ufb01cial for policy gradient algorithms. Initial values of sigmas for noisy layers is set to be constant and equal to \u03c3init m where \u03c3init is a hyperparameter and m is the number of inputs in accordance with original paper. We use Adam as our optimizer with default \u03b21 = 0.9, \u03b22 = 0.999, \u03b5 = 1e\u22128. No gradient clipping is performed. 37Huber loss is de\ufb01ned as Loss(y, \u02c6y) = \ufffd (y \u2212 \u02c6y)2 if |y \u2212 \u02c6y| < 1 |y \u2212 \u02c6y| else 50 Appendix B. Hyperparameters DQN QR-DQN c51 Rainbow A2C PPO Reward discount factor \u03b3 0.99 \u03b5(t)-greedy strategy 0.01 + 0.99e\u2212 t 30 000 Interactions per training step 4 Batch size B 128 32 Rollout capacity 40 1024 PPO number of epochs 3 Replay bu\ufb00er initialization size38 10 000 transitions Replay bu\ufb00er capacity M 1 000 000 transitions Target network updates K each 1000-th step Number of atoms A 51 Vmin, Vmax [\u221210, 10] Noisy layers std initialization 0.5 Multistep N 3 Prioritization degree \u03b1 0.5 Prioritization bias correction \u03b2 0.4 Unbiased prioritization after 100 000 steps GAE coe\ufb00. \u03bb 0.95 Critic loss weight 0.5 Entropy loss weight 0.01 PPO clip \u03f5 0.1 Optimizer Adam Learning rate 0.0001 Table 7: Selected hyperparameters for Atari Pong 38number of transitions to collect in replay memory before starting network optimization using mini-batch sampling. 51 Appendix C. Training statistics on Pong 0 50000 100000 150000 200000 250000 network update step 0.0 0.2 0.4 0.6 0.8 1.0 loss DQN loss behaviour 0 50000 100000 150000 200000 250000 network update step 0.00 0.02 0.04 0.06 0.08 0.10 loss DQN loss (averaged across 1000 steps) average loss std Figure 6: DQN loss behaviour during training on Pong. 0 50000 100000 150000 200000 network update step 0 1 2 3 4 loss c51 loss behaviour 0 50000 100000150000200000 network update step 7.5 10.0 12.5 15.0 17.5 20.0 22.5 QR-DQN loss behaviour 0 50000 100000150000200000 network update step 1 2 3 4 5 6 7 8 Rainbow loss behaviour Figure 7: Loss behaviours of c51, QR-DQN and Rainbow during training on Pong. 52 0 50000 100000 150000 200000 250000 network update step 0.965 0.970 0.975 0.980 0.985 0.990 0.995 1.000 median weight in mini-batch   (smoothed with window=1000) Importance sampling correction weights 0 50000 100000 150000 200000 250000 network update step 0.01000 0.01005 0.01010 0.01015 0.01020 0.01025 Average noise magnitude Figure 8: Rainbow statistics during training. Left: smoothed with window 1000 median of importance sampling weights from sampled mini-batches. Right: average noise magnitude logged at each 20-th step of training. 0 5000 10000 15000 20000 25000 network update step 1.5 1.0 0.5 0.0 0.5 1.0 loss Advantage Actor-Critic loss behaviour Actor loss Critic loss Entropy loss Figure 9: A2C loss behaviour during training. 0 20000 40000 60000 80000 network update step 2.0 1.5 1.0 0.5 0.0 0.5 1.0 loss Proximal Policy Optimization loss behaviour Actor loss Critic loss Entropy loss Figure 10: PPO loss behaviour during training. 53 Appendix D. Playing Pong behaviour 0 200 400 600 800 1000 1200 1400 1600 episode step 0.5 0.0 0.5 1.0 1.5 2.0 2.5 state value DQN playing Pong Predicted V(s) Reward-to-go losses wins Figure 11: DQN playing one episode of Pong. 0 200 400 600 800 1000 1200 1400 1600 episode step 0.0 0.5 1.0 1.5 2.0 2.5 state value c51 playing Pong Predicted V(s) Reward-to-go losses wins Figure 12: c51 playing one episode of Pong. 0 200 400 600 800 1000 1200 1400 1600 episode step -10.0 -8.0 -6.0 -4.0 -2.0 0.0 2.0 4.0 6.0 8.0 10.0 state value c51 value distribution during one played episode 0.0 0.1 0.2 0.3 0.4 0.5 Figure 13: c51 value distribution prediction during one episode of Pong. 54 0 500 1000 1500 2000 2500 episode step 1 0 1 2 3 state value Quantile Regression DQN playing Pong Predicted V(s) Reward-to-go losses wins Figure 14: Quantile Regression DQN playing one episode of Pong. 0 500 1000 1500 2000 2500 episode step 1 0 1 2 3 4 state value Quantile Regression DQN value distribution approximation during one played episode Figure 15: Quantile Regression DQN value distribution prediction during one episode of Pong. 0 250 500 750 1000 1250 1500 1750 2000 episode step 0.5 0.0 0.5 1.0 1.5 2.0 state value Rainbow playing Pong Predicted V(s) Reward-to-go losses wins Figure 16: Rainbow playing one episode of Pong (exploration turned o\ufb00, i.e. all noise samples are zero). 0 250 500 750 1000 1250 1500 1750 2000 episode step -10.0 -8.0 -6.0 -4.0 -2.0 0.0 2.0 4.0 6.0 8.0 10.0 state value Rainbow value distribution during one played episode 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Figure 17: Rainbow value distribution prediction during one episode of Pong (exploration turned o\ufb00, i.e. all noise samples are zero). 55 0 250 500 750 1000 1250 1500 1750 2000 episode step 2.0 1.5 1.0 0.5 0.0 0.5 state value A2C playing Pong Predicted V(s) Reward-to-go losses wins Figure 18: A2C playing one episode of Pong. 0 250 500 750 1000 1250 1500 1750 2000 episode step NOOP FIRE RIGHT LEFT RIGHTFIRE LEFTFIRE actions A2C policy during one played episode 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Figure 19: A2C policy distribution during one episode of Pong. 0 250 500 750 1000 1250 1500 1750 2000 episode step 1.5 1.0 0.5 0.0 0.5 1.0 1.5 state value PPO playing Pong Predicted V(s) Reward-to-go losses wins Figure 20: PPO playing one episode of Pong. 0 250 500 750 1000 1250 1500 1750 2000 episode step NOOP FIRE RIGHT LEFT RIGHTFIRE LEFTFIRE actions PPO policy during one played episode 0.2 0.4 0.6 0.8 Figure 21: PPO policy distribution during one episode of Pong. 56 ",
    "title": "Modern Deep Reinforcement Learning Algorithms",
    "paper_info": "Moscow State University\nFaculty of Computational Mathematics and Cybernetics\nDepartment of Mathematical Methods of Forecasting\nModern Deep Reinforcement Learning Algorithms\nWritten by:\nSergey Ivanov\nqbrick@mail.ru\nScienti\ufb01c advisor:\nAlexander D\u2019yakonov\ndjakonov@mail.ru\nMoscow, 2019\narXiv:1906.10025v2  [cs.LG]  6 Jul 2019\n",
    "GPTsummary": "                    - (1): The paper focuses on deep reinforcement learning (DRL), which combines classical theoretical results with deep learning paradigm to achieve breakthroughs in AI tasks. The goal is to review the latest DRL algorithms with a focus on theoretical justification, practical limitations, and observed empirical properties. \n\n                    - (2): The paper reviews past methods in the area of DRL, pointing out their problems and limitations. The approach in this paper is well motivated as it aims to build upon the latest developments to address these limitations. \n\n                    - (3): The research methodology proposed in the paper consists of reviewing the latest DRL algorithms, comparing their theoretical justifications, practical limitations, and empirical properties. The authors then perform experiments on two environments - Cartpole and Atari Pong - with the different algorithms and analyze the results. \n\n                    - (4): The methods achieve impressive performance on the environments tested, with some algorithms achieving better results than others. The authors conclude that while there is still much work to be done in the area of DRL, the latest algorithms have shown promising breakthroughs in AI tasks.\n\n\n\n\n\n\n\n8. Conclusion: \n\n- (1): This paper reviews the latest deep reinforcement learning (DRL) algorithms with a focus on theoretical justification, practical limitations, and observed empirical properties. It provides a well-motivated approach that aims to build upon the latest developments to address the limitations of past methods. The research methodology proposed in the paper consists of reviewing the latest DRL algorithms, comparing their theoretical justifications, practical limitations, and empirical properties. The authors then perform experiments on two environments - Cartpole and Atari Pong - with the different algorithms and analyze the results. The latest algorithms have shown promising breakthroughs in AI tasks, although there is still much work to be done in the area of DRL.\n\n- (2): Innovation point: The paper focuses on reviewing the latest DRL algorithms, combining classical theoretical results with the deep learning paradigm to achieve breakthroughs in AI tasks, and building upon the latest developments to address the limitations of past methods. \n\nPerformance: The methods achieve impressive performance on the environments tested, with some algorithms achieving better results than others. The authors conduct experiments on two environments (Cartpole and Atari Pong) and analyze the results. \n\nWorkload: The paper provides a comprehensive review of the latest DRL algorithms and conducts experiments on two environments to compare their empirical properties. It requires a substantial workload to conduct a thorough review and experiments to support the authors' findings. \n\nNote: Proper nouns need to be marked in English.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This paper reviews the latest deep reinforcement learning (DRL) algorithms with a focus on theoretical justification, practical limitations, and observed empirical properties. It provides a well-motivated approach that aims to build upon the latest developments to address the limitations of past methods. The research methodology proposed in the paper consists of reviewing the latest DRL algorithms, comparing their theoretical justifications, practical limitations, and empirical properties. The authors then perform experiments on two environments - Cartpole and Atari Pong - with the different algorithms and analyze the results. The latest algorithms have shown promising breakthroughs in AI tasks, although there is still much work to be done in the area of DRL.\n\n- (2): Innovation point: The paper focuses on reviewing the latest DRL algorithms, combining classical theoretical results with the deep learning paradigm to achieve breakthroughs in AI tasks, and building upon the latest developments to address the limitations of past methods. \n\nPerformance: The methods achieve impressive performance on the environments tested, with some algorithms achieving better results than others. The authors conduct experiments on two environments (Cartpole and Atari Pong) and analyze the results. \n\nWorkload: The paper provides a comprehensive review of the latest DRL algorithms and conducts experiments on two environments to compare their empirical properties. It requires a substantial workload to conduct a thorough review and experiments to support the authors' findings. \n\nNote: Proper nouns need to be marked in English.\n\n\n"
}