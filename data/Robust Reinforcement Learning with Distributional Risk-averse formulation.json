{
    "Abstract": "Abstract Robust Reinforcement Learning tries to make predictions more robust to changes in the dynamics or rewards of the system. This problem is particularly important when the dynamics and rewards of the environment are estimated from the data. In this paper, we approximate the Robust Reinforcement Learning constrained with a \u03a6-divergence using an approximate Risk-Averse formulation. We show that the classical Reinforcement Learning formulation can be robusti\ufb01ed using standard deviation penalization of the objective. Two algorithms based on Distributional Reinforcement Learning, one for discrete and one for continuous action spaces are proposed and tested in a classical Gym environment to demonstrate the robustness of the algorithms. 1. ",
    "Introduction": "Introduction The classical Reinforcement Learning (RL) problem using Markov Decision Processes (MDPs) modelization gives a practical framework for solving sequential decision problems under uncertainty of the environment. However, for real-world applications, the \ufb01nal chosen policy can be sometimes very sensitive to sampling errors, inaccuracy of the model parameters, and de\ufb01nition of the reward. For discrete state-action space, Robust MDPs is treated by Yang (2017); Petrik & Russel (2019); Grand-Cl\u00e9ment & Kroer (2020a;b) or (Behzadian et al., 2021) among others. Here we focus on more general continuous state space S with discrete or continuous action space A and with constraints de\ufb01ned using \u03a6-divergence. Robust RL (Morimoto & Doya, 2005) with continuous action space focuses on robustness in the dynamics of the system (changes of P) and has been studied in Abdullah et al. (2019); Singh et al. (2020); Urp\u00ed et al. (2021); Eysenbach & Levine (2021) among others. 1Centre de Math\u00e9matiques Appliqu\u00e9es, Ecole polytechnique, France 2INRIA HeKA, INSERM, Sorbonne Universit\u00e9, Universit\u00e9 Paris Cit\u00e9, France. Correspondence to: Pierre Clavier <pierre.clavier@polytechnique.edu>. ICML 2022 Workshop on Responsible Decision Making in Dynamic Environments, Baltimore, Maryland, USA, 2022. Copyright 2022 by the author(s). Eysenbach & Levine (2021) tackles the problem of both reward and transitions using Max Entropy RL whereas the problem of robustness in action noise perturbation is presented in Tessler et al. (2019). Here we tackle the problem of Robustness thought dynamics of the system. In this paper, we show that it is possible to approximate a Robust Distributional Reinforcement Learning heuristic with \u03a6-divergence constraints into a Risk-averse formulation, using a formulation based on mean-standard deviation optimization. Moreover, we focus on the idea that generalization, regularization, and robustness are strongly linked together as Husain et al. (2021); Derman & Mannor (2020); Derman et al. (2021); Ying et al. (2021); Brekelmans et al. (2022) noticed in the MDPs or RL framework. The contribution of the work is the following: we motivate the use of standard deviation penalization and derive two algorithms for discrete and continuous action space that are Robust to change in dynamics. These algorithms do not require lots of additional parameter tuning, only the Mean-Standard Deviation trade-off has to be chosen carefully. Moreover, we show that our formulation using Distributional Reinforcement Learning is robust to change dynamics on discrete and continuous action spaces from Mujoco suite. 2. Notations Considering a Markov Decision Process (MDP) (S, A, P, \u03b3), where S is the state space, A is the action space, P (s\u2032, r | s, a) is the reward and transition distribution from state s to s\u2032 taking action a and \u03b3 \u2208 (0, 1) is the discount factor. Stochastic policy are denoted \u03c0(a | s) : S \u2192 \u2206(A) and we consider the cases of action space either discrete our continuous. A rollout or trajectory using \u03c0 from state s using initial action a is de\ufb01ned as the the random sequence \u03c4 P,\u03c0|s,a = ((s0, a0, r0) , (s1, a1, r1) , . . .) with s0 = s, a0 = a, at \u223c \u03c0 (\u00b7 | st) and (rt, st+1) \u223c P (\u00b7, \u00b7 | st, at) ; we denote the distribution over rollouts by P(\u03c4) with P(\u03c4) = P0 (s0) \ufffdT t=0 P (st+1, rt | st, at) \u03c0 (at | st) d\u03c4 and usually write \u03c4 \u223c P = (P, \u03c0). Moreover, considering the distribution of discounted cumulative return ZP,\u03c0(s, a) = R(\u03c4 P,\u03c0|s,a) with R(\u03c4) = \ufffd\u221e t=0 \u03b3trt, the Q-function QP,\u03c0 : S \u00d7 A \u2192 R of \u03c0 is the expected discounted cumulative return of the distribution de\ufb01ned as follows : arXiv:2206.06841v1  [cs.LG]  14 Jun 2022 Submission and Formatting Instructions for ICML 2022 QP,\u03c0(s, a) := E[ZP,\u03c0(s, a)]. The classical initial goal of RL also called risk-neutral RL, is to \ufb01nd the optimal policy \u03c0\u2217 where QP,\u03c0\u2217(s, a) \u2265 QP,\u03c0(s, a) for all \u03c0 and s \u2208 S, a \u2208 A. Finally, the Bellman operator T \u03c0 and Bellman optimal operator T \u2217 are de\ufb01ned as follow: T \u03c0Q(s, a) := r(s, a) + \u03b3EP,\u03c0 [Q (s\u2032, a\u2032)] and T \u2217Q(s, a) := r(s, a) + \u03b3EP [maxa\u2032 Q (s\u2032, a\u2032)]. Applying either operator from an initial Q0 converges to a \ufb01xed point Q\u03c0 or Q\u2217 at a geometric rate as both operators are contractive. Simplifying the notation with regards to s, a, \u03c0 and P, we de\ufb01ne the set of greedy policies w.r.t. Q called G(Q) = arg max \u03c0\u2208\u03a0\u27e8Q, \u03c0\u27e9. A classical approach to estimating an optimal policy is known as Approximate Modi\ufb01ed Policy Iteration (AMPI) (Scherrer et al., 2015) \ufffd \u03c0k+1 \u2208 G (Qk) Qk+1 = (T \u03c0k+1)m Qk + \u03f5k+1 , which usually reduces to Approximate Value Iteration (AVI, m = 1 ) and Approximate Policy Iteration (API, m = \u221e) as special cases. The term \u03f5k+1 accounts for errors made when applying the Bellman Operator in RL algorithms with stochastic approximation. 3. Robust formulation in greedy step of API. In this section, we would like to \ufb01nd a policy that is robust to change of environment law P as small variations of P should not affect too much the new policy in the greedy step. In our case we are not looking at classical greedy step \u03c0\u2032 \u2208 G(Q) = arg max \u03c0\u2208\u03a0\u27e8Q, \u03c0\u27e9 rather at the following : \u03c0\u2032 \u2208 G(Q) = arg max \u03c0\u2208\u03a0 \u27e8min P Q(P,\u03c0), \u03c0\u27e9. This heuristic in the greedy step can also be justi\ufb01ed by trying to avoid an overestimation of the Q functions present in the Deep RL algorithms. Using this formulation, we need to constrain the set of admissible transitions from state-action to the next state P to get a solution to the problem. In general, without constraint, the problem is NP-Hard, so it requires constraining the problem to speci\ufb01c distributions that are not too far from the original one using distance between distributions such as the Wasserstein metric (Abdullah et al., 2019) or other speci\ufb01c distances where the problem can be simpli\ufb01ed (Eysenbach & Levine, 2021). If a explicit form of minP Q(P,\u03c0) could be computed exactly for a given divergence, it would lead to a simpli\ufb01cation of this max-min optimization problem into a simple maximisation one. In fact, simpli\ufb01cation of the problem is possible using speci\ufb01c \u03a6-divergence denoted H\u03a6 to constrain the problem with \u03a6 a closed convex function such that \u03a6 : R \u2192 R \u222a {+\u221e} and \u03a6(z) \u2265 \u03a6(1) = 0 for all z \u2208 R : H\u03a6 (Q | P) = \ufffd i:pi>0 pi\u03a6 \ufffd qi pi \ufffd with \ufffd i:pi>0 qi = 1 and qi \u2265 0. This constraint requires qi = 0 if pi = 0 which makes the measure Q absolutely continuous with respect to P.The \u03c72-divergence are a particular case of \u03a6-divergence with \u03a6(z) = (z \u2212 1)2. For trajectories sampled from distribution P0 and looking at distribution P closed to P0 with regards to the \u03c72-divergence, the minimisation problem reduces to : min P \u2208D\u03c72(P \u2225P0)\u2264\u03b1 Q(P,\u03c0) = Q(P0,\u03c0) \u2212 \u03b1VP0[Z] 1 2 . (1) The proof can be found in annex A for \u03b1 such that \u03b1 \u2264 VP0[R(\u03c4)]/ \ufffd\ufffd\ufffd \u02dcR \ufffd\ufffd\ufffd 2 \u221e \u2264 1 with \u02dcR(\u03c4) = R(\u03c4)\u2212E\u03c4\u223cP0[R(\u03c4)] the centered return and VP0[Z] the variance of returns. For \u03b1 > VP0[R(\u03c4)]/ \ufffd\ufffd\ufffd \u02dcR \ufffd\ufffd\ufffd 2 \u221e, the equality becomes an inequality but we still optimize a lower bound of our initial problem. De\ufb01ning a new greedy step which is penalized by the standard deviation : \u03c0\u2032 \u2208 G\u03b1(Q) = arg max \u03c0\u2208\u03a0 \u27e8Q(P0,\u03c0) \u2212 \u03b1VP0[Z] 1 2 , \u03c0\u27e9 we now look at the current AMPI to improve robustness : \ufffd \u03c0k+1 \u2208 G\u03b1 (Qk) Qk+1 = (T \u03c0k+1)m Qk + \u03f5k+1 , This idea is very close to Risk-averse formulation in RL (i.e minimizing risk measure and not only the mean of rewards) but here the idea is to approximate a robustness problem in RL. To do so, the standard deviation of the distribution of the returns must be estimated. Many ways are possible but we favour distributional RL (Bellemare et al., 2017; Dabney et al., 2017; 2018) which achieve very good performances in many RL applications. Estimating quantiles of the distribution of return, we can simply estimate standard deviation using classical estimator of the standard deviation given the quantiles over an uniform grid{qi(s, a)}1\u2264i\u2264n, : V[Z(s, a)] 1 2 = \u03c3(s, a) = \ufffd\ufffdn i=1 (qi(s, a) \u2212 \u00afq(s, a))2 where \u00afq is the classical estimator of the mean. A different interpretation of this formulation could be that by taking actions with less variance, we construct a con\ufb01dence interval with the standard deviation of the distribution : Z\u03c0(s, a) d= \u00afZ(s, a) \u2212 \u03b1\u03c3(s, a). This idea is present in classical UCB algorithms (Auer, 2002) or pessimism/optimism Deep RL. Here we construct a con\ufb01dence interval using the distribution of the return and not different estimates of the Q function such as in Moskovitz et al. (2021); Bai et al. (2022). In the next section, we derive two algorithms, one for discrete action space and one for continuous action space using this idea. A very interesting way of doing robust Learning is by doing Max entropy RL such as in the SAC algorithm. In Eysenbach & Levine (2021), a demonstration that SAC is a surrogate of Robust RL is demonstrated formally and numerically and we will compare our algorithm to this method. Submission and Formatting Instructions for ICML 2022 4. Distributional RL Distributional RL aims at approximating the return random variable Z\u03c0(s, a) := \ufffd\u221e t=0 \u03b3tR (st, at) with s0 = s, a0 = a , st+1 \u223c P (\u00b7 | st, at) , at \u223c \u03c0 (\u00b7 | st)and R that we explicitly treat as a random variable. The classical RL framework approximate the expectation of the return or the Q-function, Q\u03c0(s, a) := E [Z\u03c0(s, a)] . Many algorithms and distributional representation of the critic exits (Bellemare et al., 2017; Dabney et al., 2017; 2018) but here we focus on QR-DQN (Dabney et al., 2017) that approximates the distribution of returs Z\u03c0(s, a) with Z\u03c8(s, a) := 1 M \ufffdM m=1 \u03b4 \ufffd \u03b8m \u03c8 (s, a) \ufffd , a mixture of atoms-Dirac delta functions located at \u03b81 \u03c8(s, a), . . . , \u03b8M \u03c8 (s, a) given by a parametric model \u03b8\u03c8 : S \u00d7 A \u2192 RM. Parameters \u03c8 of a neural network are obtained by minimizing the average over the 1-Wasserstein distance between Z\u03c8 and the temporal difference target distribution T\u03c0Z \u00af \u03c8, where T\u03c0 is the distributional Bellman operator de\ufb01ned in Bellemare et al. (2017). The control version or optimal operator is denoted T Z \u00af \u03c8, with T \u03c0Z(s, a) = R(s, a) + \u03b3Z (s\u2032, a\u2032). According to Dabney et al. (2017), the minimization of the 1-Wasserstein loss can be done by learning quantile locations for fractions \u03c4m = 2m\u22121 2M , m \u2208 [1..M] via quantile regression loss, de\ufb01ned for a quantile fraction \u03c4 \u2208 [0, 1] as : L\u03c4 QR(\u03b8) : = E \u02dc Z\u223cZ \ufffd \u03c1\u03c4( \u02dcZ \u2212 \u03b8) \ufffd with \u03c1\u03c4(u) = u(\u03c4 \u2212 I(u < 0)), \u2200u \u2208 R . Finally, to obtain better gradients when u is small, Huber quantile loss (or asymmetric Huber loss) can be used: \u03c1H \u03c4 (u) = |\u03c4 \u2212 I(u < 0)|L1 H(u), where L1 H(u) is a classical Huber loss with parameter 1. The quantile representation has the advantage of not \ufb01xing the support of the learned distribution and is used to represent the distribution of return in our algorithm for both discrete and continuous action space. 5. Algorithm We use a distributional maximum entropy framework for continuous action space which is closed to the TQC algorithm (Kuznetsov et al., 2020). This method uses an actor-critic framework with a distributional truncated critic to ovoid overestimation in the estimation with the max operator. This algorithm is based on a soft-policy iteration where we penalize the target yi(s, a) using the entropy of the distribution. More formally, to compute the target, the principle is to train N approximate estimate Z\u03c81, . . . Z\u03c8C of the distribution of returns Z\u03c0 where Z\u03c8c maps each (s, a) to Z\u03c8c(s, a) := 1 M \ufffdM m=1 \u03b4 \ufffd \u03b8m \u03c8n(s, a) \ufffd , which is supported on atoms \u03b81 \u03c8c(s, a), . . . , \u03b8M \u03c8c(s, a). Then approximations Z\u03c81, . . . Z\u03c8N are trained on the temporal difference target distribution denoted Y (s, a) constructed as follow. First atoms of trained distributions Z\u03c81 (s\u2032, a\u2032) , . . . , Z\u03c8C (s\u2032, a\u2032) are pooled into Z (s\u2032, a\u2032) := \ufffd \u03b8m \u03c8c (s\u2032, a\u2032) | c \u2208 [1..C], m \u2208 [1..M] \ufffd . We denote elements of Z (s\u2032, a\u2032) sorted in ascending order by z(i) (s\u2032, a\u2032), with i \u2208 [1..MC]. Then we only keep the kC smallest elements of Z (s\u2032, a\u2032). We remove outliers of distribution to avoir overestimation of the value function. Finally the atoms of the target distribution Y (s, a) := 1 kC \ufffdkC i=1 \u03b4 (yi(s, a)) are computed according to a soft policy gradient method where we penalised with the log of the policy : yi(s, a) := r(s, a) + \u03b3 \ufffd z(i) (s\u2032, a\u2032) \u2212 \u03b7 log \u03c0\u03c6 (a\u2032 | s\u2032) \ufffd . (2) The 1-Wasserstein distance between each of Z\u03c8n(s, a), n \u2208 [1..N] and the temporal difference target distribution Y (s, a) is minimized learning the locations for quantile fractions \u03c4m = 2m\u22121 2M , m \u2208 [1..M]. Similarly, we minimize the loss : JZ (\u03c8c) = ED,\u03c0 \ufffd 1 MkC \ufffdM j=1 \ufffdkC i=1 \u03c1H \u03c4j \ufffd yi(s, a) \u2212 \u03b8j \u03c8c(s, a) \ufffd\ufffd over the parameters \u03c8n, for each critic. With this formulation, the learning of all quantiles \u03b8m \u03c8n(s, a) is dependent on all atoms of the truncated mixture of target distributions. To optimize the actor, the following loss based on KL-divergence denoted DKL is used for soft policy improvement, : J\u03c0,\u03b1(\u03c6) = ED \ufffd DKL \ufffd \u03c0\u03c6 (\u00b7 | s) \u2225 exp( 1 \u03b7 \u03be\u03b1(\u03b8\u03c8(s,\u00b7))) D \ufffd\ufffd where \u03b7 can be seen as a temperature and needs to be tuned and D is a constant of normalisation. This expression simplify into : J\u03c0,\u03b1(\u03c6) = ED,\u03c0 \ufffd \u03b7 log \u03c0\u03c6(a | s) \u2212 1 C C \ufffd c=1 \u03be\u03b1(\u03b8\u03c8c(s, a)) \ufffd where s \u223c D, a \u223c \u03c0\u03c6(\u00b7 | s). Nontruncated estimate of the Q-value are used for policy optimization to avoid a double truncation, in fact the Z-functions already approximate truncated future distribution. Finally, \u03b7 is the entropy temperature coef\ufb01cient and is dynamically adjusted by taking a gradient step with respect to the loss like in Haarnoja et al. (2018) : J(\u03b7) = ED,\u03c0\u03c6 [(\u2212 log \u03c0\u03c6 (at | st) \u2212 H\u03b7) \u03b7] at every time the \u03c0\u03c6 changes. Temperature \u03b7 decreases if the policy entropy, \u2212 log \u03c0\u03c6 (at | st), is higher than H\u03b7 and increases otherwise. The algorithm is summarized as follows : Our algorithm is based SAC framework but with many distributional critics to improve the estimation of Qfunctions while using mean-standard deviation objective in the policy loss to improve robustness. ",
    "Experiments": "Experiments We try different experiments on continuous and discrete action space (see Annex B.2. for discrete case) to demonstrate the interest of our algorithms for robustness using \u03be : Z \u2192 E[Z] \u2212 \u03b1V[Z] 1 2 instead of the mean. For continuous action space, we compare our algorithm with SAC which achives state of the art in robust control (Eysenbach & Levine, 2021) on the Mujoco environment such as Hopperv3, Walker-v3, or HalfCheetah-v3. We use a version where the entropy coef\ufb01cient is adjusted during learning for both SAC and our algorithm as it requires less parameter tuning. Moreover, we show the in\ufb02uence of a distributional critic without a mean-standard deviation greedy step using \u03b1 = 0 to demonstrate the advantage of using a distributional critic against the classical SAC algorithm. We also compare our results to TQC algorithm which is in fact very close to SAC algorithm except the distributional critic. Finally, \u03b1 the penalty is increased to show that for the tested environment, there exists a value of \u03b1 such as prediction are more robust to change of dynamics. In these simulations, variations of dynamics are carried out by moving the relative mass which is an in\ufb02uential physical parameter in all environments. All algorithms are trained with a relative mass of 1 and then tested on new environment where the mass varies from 0.5 to 2. Two phenomena can be observed for the 3 environments. In Fig 1, we see that we can \ufb01nd a value of \u03b1 where the robustness is clearly improved without deteriorating the average performance. Normalized performance using by the maximum of the performance for every curve to highlight robustness and not only mean-performance can be found in annex B.1. If a too strong penalty is applied, the average performance can be decreased as in the HalfCheetah-v3 environment (see annex B.1). For Hopper-v3, a \u03b1 calibrated at 5 gives very good robustness performances while for Walker2d-v3, the value is closer to 2. This phenomenon was expected and in agreement with our formulation. Moreover, our algorithm outperforms the SAC algorithm for Robustness tasks in all environments. The tuning of \u03b1 is discussed in annex B.1. The second surprising observation is that penalizing our objective also improves performance in terms of stability during training and in terms of average performance, especially for Hopper-v3 and Walker2d-v3 in Fig 1. Similar results are present in the work of (Moskovitz et al., 2021) which gives an interpretation in terms of optimism and pessimism for the environments. This phenomenon is not yet explained, but it is present in some environments that are particularly unstable and have a lot of variance. We observe similar results for discrete environments such as Cartpole-v1 and Acrobot-v1 (See annex B.2.). 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 500 1000 1500 2000 2500 3000 3500 4000 Mean Reward SAC =0 =1 =2 =3 =4 =5 (a) Hopper-v3 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 0 1000 2000 3000 4000 5000 6000 Mean Reward SAC =0 =1 =2 =3 =4 =5 (b) Walker2d-v3 Figure 1: Mean over 20 trajectories varying relative mass of environments. 7. Conclusions In this paper, we have tried to show that by using a meanstandard deviation formulation to choose our actions pessimistically, we can increase the robustness of our environment for continuous and discrete environments without adding complexity. A single \ufb01xed \u03b1 parameter must be tuned to obtain good performance without penalizing the average performance too much. Moreover, for some environments, it is relevant to penalize to increase the average performance as well when there is lots of variability in the environment. About the limitations of this work, the convergence of the algorithm to a \ufb01xed point is not shown for mean-standard deviation penalization and this formulation is still based on a heuristic even if the link between robustness and penalization is established. Theoretical link with a (Kumar et al., 2022) could be an interesting way of analyzing our algorithm as they use action value to penalize their objective and remains a way to explain this phenomenon. This is left for future work. ",
    "References": "References Abdullah, M. A., Ren, H., Ammar, H. B., Milenkovic, V., Luo, R., Zhang, M., and Wang, J. Wasserstein robust reinforcement learning. arXiv preprint arXiv:1907.13196, 2019. Auer, P. Using con\ufb01dence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002. Bai, C., Wang, L., Yang, Z., Deng, Z., Garg, A., Liu, P., and Wang, Z. Pessimistic bootstrapping for uncertaintydriven of\ufb02ine reinforcement learning. arXiv preprint arXiv:2202.11566, 2022. Behzadian, B., Petrik, M., and Ho, C. P. Fast algorithms for l\u221e-constrained s-rectangular robust mdps. Advances in Neural Information Processing Systems, 34, 2021. Bellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. 34th International Conference on Machine Learning, ICML 2017, 1:693\u2013711, 7 2017. URL https://arxiv. org/abs/1707.06887v1. Brekelmans, R., Genewein, T., Grau-Moya, J., Del\u00e9tang, G., Kunesch, M., Legg, S., and Ortega, P. Your policy regularizer is secretly an adversary. arXiv preprint arXiv:2203.12592, 2022. Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. Distributional reinforcement learning with quantile regression. 32nd AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018, pp. 2892\u20132901, 10 2017. URL https://arxiv.org/abs/1710.10044v1. Dabney, W., Ostrovski, G., Silver, D., and Munos, R. Implicit quantile networks for distributional reinforcement learning. 35th International Conference on Machine Learning, ICML 2018, 3:1774\u20131787, 6 2018. URL https://arxiv.org/abs/1806.06923v1. Derman, E. and Mannor, S. Distributional robustness and regularization in reinforcement learning. arXiv preprint arXiv:2003.02894, 2020. Derman, E., Geist, M., and Mannor, S. Twice regularized mdps and the equivalence between robustness and regularization. Advances in Neural Information Processing Systems, 34, 2021. Duchi, J. and Namkoong, H. Learning models with uniform performance via distributionally robust optimization. arXiv preprint arXiv:1810.08750, 2018. Duchi, J., Glynn, P., and Namkoong, H. Statistics of robust optimization: A generalized empirical likelihood approach. arXiv preprint arXiv:1610.03425, 2016. Eysenbach, B. and Levine, S. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257, 2021. Geist, M., Scherrer, B., and Pietquin, O. A theory of regularized markov decision processes. In International Conference on Machine Learning, pp. 2160\u20132169. PMLR, 2019. Gotoh, J.-y., Kim, M. J., and Lim, A. E. Robust empirical optimization is almost the same as mean\u2013variance optimization. Operations research letters, 46(4):448\u2013452, 2018. Grand-Cl\u00e9ment, J. and Kroer, C. First-order methods for wasserstein distributionally robust mdp. arXiv preprint arXiv:2009.06790, 2020a. Grand-Cl\u00e9ment, J. and Kroer, C. Scalable \ufb01rst-order methods for robust mdps. arXiv preprint arXiv:2005.05434, 2020b. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. 35th International Conference on Machine Learning, ICML 2018, 5:2976\u20132989, 1 2018. URL http://arxiv.org/ abs/1801.01290. Husain, H., Ciosek, K., and Tomioka, R. Regularized policies are reward robust. In International Conference on Arti\ufb01cial Intelligence and Statistics, pp. 64\u201372. PMLR, 2021. Jaimungal, S., Pesenti, S. M., Wang, Y. S., and Tatsat, H. Robust risk-aware reinforcement learning. SSRN Electronic Journal, 8 2021. doi: 10.2139/SSRN.3910498. URL https://papers.ssrn.com/abstract= 3910498. Jain, A., Patil, G., Jain, A., Khetarpal, K., and Precup, D. Variance penalized on-policy and off-policy actor-critic. 2021a. URL www.aaai.org. Jain, A., Patil, G., Jain, A., Khetarpal, K., and Precup, D. Variance penalized on-policy and off-policy actor-critic. arXiv preprint arXiv:2102.01985, 2021b. Kumar, N., Levy, K., Wang, K., and Mannor, S. Ef\ufb01cient policy iteration for robust markov decision processes via regularization. arXiv preprint arXiv:2205.14327, 2022. Kuznetsov, A., Shvechikov, P., Grishin, A., and Vetrov, D. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In International Conference on Machine Learning, pp. 5556\u20135566. PMLR, 2020. Submission and Formatting Instructions for ICML 2022 Ma, Y. J., Jayaraman, D., and Bastani, O. Conservative of\ufb02ine distributional reinforcement learning. 7 2021. URL https://arxiv.org/abs/2107.06106v2. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. 12 2013. URL https://arxiv.org/abs/1312.5602v1. Morimoto, J. and Doya, K. Robust reinforcement learning. Neural computation, 17(2):335\u2013359, 2005. Moskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M., and Jordan, M. Tactical optimism and pessimism for deep reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. Nam, D. W., Kim, Y., and Park, C. Y. Gmac: A distributional perspective on actor-critic framework. In International Conference on Machine Learning, pp. 7927\u20137936. PMLR, 2021. Petrik, M. and Russel, R. H. Beyond con\ufb01dence regions: Tight bayesian ambiguity sets for robust mdps. Advances in neural information processing systems, 32, 2019. Raf\ufb01n, A., Hill, A., Ernestus, M., Gleave, A., Kanervisto, A., and Dormann, N. Stable baselines3, 2019. Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M. Approximate modi\ufb01ed policy iteration and its application to the game of tetris. J. Mach. Learn. Res., 16(49):1629\u20131676, 2015. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. Trust region policy optimization. 32nd International Conference on Machine Learning, ICML 2015, 3:1889\u20131897, 2 2015. URL http://arxiv.org/ abs/1502.05477. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv, 7 2017a. ISSN 23318422. URL https:// arxiv.org/abs/1707.06347v2. PPO algorithm premier papier<br/>Important \u00e0 citer<br/><br/>. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. Singh, R., Zhang, Q., and Chen, Y. Improving robustness via risk averse distributional reinforcement learning. In Learning for Dynamics and Control, pp. 958\u2013968. PMLR, 2020. Smirnova, E., Dohmatob, E., and Mary, J. Distributionally robust reinforcement learning. 2 2019. URL https: //arxiv.org/abs/1902.08708v2. Tessler, C., Efroni, Y., and Mannor, S. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pp. 6215\u20136224. PMLR, 2019. Urp\u00ed, N. A., Curi, S., and Krause, A. Risk-averse of\ufb02ine reinforcement learning. arXiv preprint arXiv:2102.05371, 2021. Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M. Leverage the average: an analysis of kl regularization in reinforcement learning. Advances in Neural Information Processing Systems, 33:12163\u2013 12174, 2020. Wang, H. and Zhou, X. Y. Continuous-time mean\u2013variance portfolio selection: A reinforcement learning framework. Mathematical Finance, 30(4):1273\u20131308, 2020. Wiesemann, W., Kuhn, D., and Rustem, B. Robust markov decision processes. Mathematics of Operations Research, 38(1):153\u2013183, 2013. Yang, I. A convex optimization approach to distributionally robust markov decision processes with wasserstein distance. IEEE Control Systems Letters, 1:164\u2013169, 2017. ISSN 24751456. doi: 10.1109/LCSYS.2017.2711553. Ying, C., Zhou, X., Su, H., Yan, D., and Zhu, J. Towards safe reinforcement learning via constraining conditional value-at-risk. 2021. Zhang, J. and Weng, P. Safe distributional reinforcement learning. Zhang, J. and Weng, P. Safe distributional reinforcement learning. In International Conference on Distributed Arti\ufb01cial Intelligence, pp. 107\u2013128. Springer, 2021. Submission and Formatting Instructions for ICML 2022 A. Proof of (1) We consider the following equality : min P \u2208D\u03c72(P \u2225P0)\u2264\u03b1 Q(P,\u03c0) = Q(P0,\u03c0) \u2212 \u03b1VP0[Z] 1 2 . (3) Consider that trajectories \u03c4 is drawn from P but here we will write P the transition of the environment as the policy \u03c0 is \ufb01xed and it is the only part which differ. Writing \u02dcR(\u03c4) = R(\u03c4) \u2212 E\u03c4\u223cP0[R(\u03c4)] we get : \u2225E\u03c4\u223cP [R(\u03c4)] \u2212 E\u03c4\u223cP0[R(\u03c4)]\u2225 = \ufffd\ufffd\ufffd\ufffd \ufffd \u03c4 \u02dcR(\u03c4) \ufffd p(\u03c4) \u2212 p0(\u03c4) \ufffd d\u03c4 \ufffd\ufffd\ufffd\ufffd = \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd \u03c4 \u02dcR(\u03c4) \ufffd p0(\u03c4) \ufffd p(\u03c4) \u2212 p0(\u03c4) \ufffd \ufffd p0(\u03c4) d\u03c4 \ufffd\ufffd\ufffd\ufffd\ufffd \u2264 \ufffd\ufffd\ufffd\ufffd \ufffd \u03c4 \u02dcR(\u03c4)2p0(\u03c4)d\u03c4 \ufffd\ufffd\ufffd\ufffd 1 2 \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd \u03c4 \ufffd p(\u03c4) \u2212 p0(\u03c4) \ufffd2 p0(\u03c4) d\u03c4 \ufffd\ufffd\ufffd\ufffd\ufffd 1 2 = VP0[R(\u03c4)] 1 2 D\u03c72(P\u2225P0) 1 2 , because of the positivity of divergence and of the variance, norms are removed. This inequality comes from Cauchy-Schwarz inequality and becomes an equality if for \u03bb \u2208 R : \u02dcR(\u03c4)p0(\u03c4) = \u03bb(p(\u03c4) \u2212 p0(\u03c4)) \u21d0\u21d2 p(\u03c4) = p0(\u03c4)(1 + 1 \u03bb \u02dcR(\u03c4)). (4) However, p(\u03c4) needs to be non-negative and sum to one as it is a measure. Normalisation condition is respected by construction however to ensure that the measure is non-negative, this requires \ufffd\ufffd\ufffd \u02dcR(\u03c4)/\u03bb \ufffd\ufffd\ufffd \u2264 1 in the case where \u03bb \u2264 0 . In this case of equality, we obtain from 4 that D\u03c72(P\u2225P0) = VP0[R(\u03c4)] \u03bb2 . Replacing the divergence in the inequality, the following result holds : \u2225E\u03c4\u223cP [R(\u03c4)] \u2212 E\u03c4\u223cP0[R(\u03c4)]\u2225 \u2264 VP0(R(\u03c4)) \u03bb . For proving (3) we are interested in the case where D\u03c72(P\u2225P0) \u2264 \u03b1, from the initial inequality we obtain : min P \u2208D\u03c72(P \u2225P0)\u2264\u03b1 Q(P,\u03c0) \u2265 min P \u2208D\u03c72(P \u2225P0)\u2264\u03b1 Q(P0,\u03c0) \u2212 D\u03c72(P\u2225P0)VP0[Z] 1 2 = Q(P0,\u03c0) \u2212 \u03b1VP0[Z] 1 2 with the maximum value of \u03b1 equals to D\u03c72(P\u2225P0) = VP0[R(\u03c4)] \u03bb2 \u2264 VP0[R(\u03c4)] \u2225 \u02dc R\u2225 2 \u221e = \u2225 \u02dc R\u2225 2 2 \u2225 \u02dc R\u2225 2 \u221e \u2264 1, where the \ufb01rst inequality comes from the conditions \ufffd\ufffd\ufffd \u02dcR(\u03c4)/\u03bb \ufffd\ufffd\ufffd \u2264 1 and the last one comes from that the L2 norm is smaller than \u221e-norm. If our problem is contrained, assuming \u03b1 \u2264 VP0[R(\u03c4)] \u2225 \u02dc R\u2225 2 \u221e \u2264 1, we obtain the following results with the maximum attained for D\u03c72(P\u2225P0) = \u03b1 : min P\u2208D\u03c72(P \u2225P0)\u2264\u03b1 Q(P,\u03c0) = Q(P0,\u03c0) \u2212 \u03b1V[Z] 1 2 . (5) For \u03b1 > 1, we still optimize a lower bound of the quantity of interest. The formulation of our algorithm becomes: Submission and Formatting Instructions for ICML 2022 \ufffd \u03c0k+1 \u2208 G\u03b1 (Zk) = G(\u03be\u03b1(Zk) = arg max \u03c0\u2208\u03a0 \u27e8E[Zk] \u2212 \u03b1 \ufffd V[Zk], \u03c0\u27e9 Zk+1 = (T \u03c0k+1)m Zk . B. Further Experimental Details All experiements were run on a cluster containing an Intel Xeon CPU Gold 6230, 20 cores and every single experiments was performed on a single CPU between 3 and 6 hours for continuous control and less than 1 hour for discrete control environment. Pre-trained models will be available for all algorithms and environments on a GitHub link. The Mujoco OpenAI Gym tasks licensing information is given at https://github.com/openai/gym/blob/master/LICENSE.md. Baseline implementation of PPO, SAC, TQC and QRDQN can be found in Raf\ufb01n et al. (2019). Moreover, hyperparameters across all experiments used are displayed in Table 2, 1 and 3 . B.1. Results for continuous action spaces Tuning of \u03b1 must be chosen carefully, for example, \u03b1 is chosen in {0, 1, ..., 5} for Hopper-v3 and Walker2d-v3 whereas values of \u03b1 are chosen smaller in {0, 0.1, 0.5.1, 1.5, 2} and not in a bigger interval. As a rule of thumb for choosing \u03b1, we can look at the empirical mean and variance at the end of the trajectories to see if the environment has rewards that \ufb02uctuate a lot. The smaller the mean/variance ratio, the more likely we are to penalise our environment. For HalfCheetah-v3, the mean/variance ratio is about approximately 100, so we will favour smaller penalties than for Walker2d where the mean/variance ratio is about 50 or 10 for Hopper-v3. 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 500 1000 1500 2000 2500 3000 3500 4000 Mean Reward SAC =0 =1 =2 =3 =4 =5 (a) Hopper 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 0 1000 2000 3000 4000 5000 6000 Mean Reward SAC =0 =1 =2 =3 =4 =5 (b) Walker2d 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 0 2000 4000 6000 8000 10000 12000 14000 16000 Mean Reward SAC =0 =0.1 =0.5 =1 =1.5 =2 (c) HalfCheetah Figure 2: Mean results on 20 trajectories varying relative mass. 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 0.2 0.4 0.6 0.8 1.0 Mean Reward Mean over 20 trajectories SAC =0 =1 =2 =3 =4 =5 (a) Hopper 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 0.0 0.2 0.4 0.6 0.8 1.0 Mean Reward Mean over 20 trajectories SAC =0 =1 =2 =3 =4 =5 (b) Walker2d 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Relative mass 0.0 0.2 0.4 0.6 0.8 1.0 Mean Reward SAC =0 =0.1 =0.5 =1 =1.5 =2 (c) HalfCheetah Figure 3: Normalised mean results on 20 trajectories varying relative mass. Submission and Formatting Instructions for ICML 2022 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Number of steps 1e6 500 1000 1500 2000 2500 3000 3500 Mean Reward Mean Reward of 20 trajectories \u00b1 standard deviation SAC =0 =1 =2 =3 =4 =5 (a) Hopper 1 2 3 4 5 Number of steps 1e6 2000 4000 6000 8000 10000 12000 14000 16000 Mean Reward Mean Reward of 20 trajectories \u00b1 standard deviation SAC =0 =0.1 =0.5 =1 =1.5 =2 (b) Walker2d 1 2 3 4 5 Number of steps 1e6 0 1000 2000 3000 4000 5000 6000 Mean Reward Mean Reward of 20 trajectories \u00b1 standard deviation SAC =0 =1 =2 =3 =4 =5 (c) HalfCheetah Figure 4: Mean performances over 20 trajectories \u00b1 variance. B.2. Results on discrete action spaces We test our algorithm QRDQN with standard deviation penalization on discrete action space, varying the length of the pole in Cartpole-v1 and Acrobot-v1 environments. We observe similar results for the discrete environment in terms of robustness. Training is done for a length of the pole equal to the x-axis of the black star on the graph, then for testing, the length of the pole is increased or decreased. We show that robustness is increased when we penalised our distributional critic. We have compared our algorithm to PPO which has shown relatively good results in terms of robustness for discrete action space in (Abdullah et al., 2019) as SAC does not apply to discrete action space. The same phenomenon is observed in terms of robustness as for the continuous environments. However, the more surprising observations on Hopper and Walker2d with an improvement in performance on average is to be quali\ufb01ed. This is partly explained by the fact that the maximum reward is reached in Cartpole and Acrobot quickly. An ablation study can be found in annex C where we study the impact of penalization on our behavior policy during testing and on the policy used during learning. It is shown that both are needed in the algorithm. 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 length of the pole 100 200 300 400 500 Mean Reward PPO =0 =1 =3 =5 =7 0.5 1.0 1.5 2.0 2.5 3.0 3.5 length of the pole 350 300 250 200 150 100 50 Mean Reward PPO  =0 =1 =2 =3 Figure 5: Mean over 20 trajectories varying length of the pole trained on the x-axis of the black star for Cartpole-v1 and Acrobot-v1 environments B.3. Ablation study The purpose of this ablation study is to look at the in\ufb02uence of penalization in the discrete action space with QRDQN. In the \ufb01gures below, we look at the in\ufb02uence of penalizing only during training, which will have the effect of choosing less risky actions during training in order to increase robustness. This curve is denoted Train penalized. Then we look at the in\ufb02uence of penalizing only once the policy has been learned using classic QRDQN without penalization. Only mean-var actions are selected here during testing and not during training. This experience is denoted Train Penalization. Finally, we compare its variants with our algorithm called Full penalization. The results of the ablation are: to achieve optimal performance, both phases are necessary. When penalties are applied only during training. Good performance is obtained in general close to the length 1 where we train our algorithm. However, the performance is dif\ufb01cult to generalize when the pole length is increased as we do not Submission and Formatting Instructions for ICML 2022 penalize during testing. When we penalize only during testing: even if the performances deteriorate, we see that it tends to add robustness because the curves have less tendency to decrease when we increase the length of the pole. The performances are not very high as we play different acts than those taken during the learning. So both phases are therefore necessary for our algorithm. Penalizing during training allows for safer exploration and penalizing during testing allows for better generalization. The ablation study for the continuous case is more dif\ufb01cult to do. Indeed, the fact that the penalty occurs only in the gradient descent phase makes it dif\ufb01cult to penalize only in the test phase. 0 1 2 3 4 5 6 7 8 Relative mass 100 200 300 400 500 Mean Reward Full penalization Test penalization Train penalization (a) \u03b1 = 1 0 1 2 3 4 5 6 7 8 Relative mass 100 200 300 400 500 Mean Reward Full penalization Test penalization Train penalization (b) \u03b1 = 3 0 1 2 3 4 5 6 7 8 Relative mass 100 200 300 400 500 Mean Reward Full penalization Test penalization Train penalization (a) \u03b1 = 1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Relative mass 100 200 300 400 500 Mean Reward Full penalization Test penalization Train penalization (b) \u03b1 = 3 Figure 6: Ablation study for different values of \u03b1 Submission and Formatting Instructions for ICML 2022 C. Hyperparameters For HalfCheetah-v3 , penalisation is chosen in [0, 2] and not [0, 5] like in Walker-v3 and Hopper-v3. Table 1: Table of best hyperparameter for Cartpole-v1 Hyperparameter QRDQN with standard deviation penalisation PPO Learning Rate 2.3e-3 3e-4 Optimizer Adam Adam Replay Buffer Size 10e5 N/A Number of Quantiles 10 N/A Huber parameter \u03ba 1 N/A Penalisation \u03b1 {0,1,3,5,7 } N/A Network Hidden Layers for Policy N/A 256:256 Network Hidden Layers for Critic 256:256 256:256 Number of samples per Minibatch 64 256 Discount factor \u03b3 0.99 0.99 Target smoothing coef\ufb01cient \u03b2 .0.005 N/A Non-linearity ReLu ReLu Target update interval 10 N/A Gradient steps per iteration 1 1 Entropy coef\ufb01cient N/A 0 GAE \u03bb 0.95 0.8 Table 2: Table of best hyperparameter for Acrobot-v1 Hyperparameter QRDQN with standard deviation penalisation PPO Learning Rate 6.3e-4 3e-4 Optimizer Adam Adam Replay Buffer Size 50 000 N/A Number of Quantiles 25 N/A Huber parameter \u03ba 1 N/A Penalisation \u03b1 {0, 0.5, 1, 2, 3} N/A Network Hidden Layers for Critic 256:256 256:256 Network Hidden Layers for Policy N/A 256:256 Number of samples per Minibatch 128 64 Discount factor \u03b3 0.99 0.99 Target smoothing coef\ufb01cient \u03b2 .0.005 N/A Non-linearity ReLu ReLu Target update interval 250 N/A Gradient steps per iteration 4 1 Entropy coef\ufb01cient N/A 0 GAE \u03bb 0.95 0.95 Submission and Formatting Instructions for ICML 2022 Table 3: Table of best hyperparameter for all continuous environments Hyperparameter TQC with standard deviation penalisation SAC Learning Rate linear decay from 7.3e-4 linear decay from 7.3e-4 Optimizer Adam Adam Replay Buffer Size 106 106 Expected Entropy Target \u2212dimA \u2212dimA Number of Quantiles 25 N/A Huber parameter \u03ba 1 N/A Penalisation \u03b1 {0, 1, ...5} N/A Network Hidden Layers for Policy 256:256 256:256 Network Hidden Layers for Critic 512:512:512 256:256 Number of dropped atoms 2 N/A Number of samples per Minibatch 256 256 Discount factor \u03b3 0.99 0.99 Target smoothing coef\ufb01cient \u03b2 .0.005 0.005 Non-linearity ReLu ReLu Target update interval 1 1 Gradient steps per iteration 1 1 ",
    "title": "Robust Reinforcement Learning with Distributional Risk-averse formulation",
    "paper_info": "Robust Reinforcement Learning with Distributional Risk-averse formulation\nPierre Clavier 1 2 St\u00e9phanie Allassonni\u00e8re 2 Erwan Le Pennec 1\nAbstract\nRobust Reinforcement Learning tries to make pre-\ndictions more robust to changes in the dynamics\nor rewards of the system. This problem is particu-\nlarly important when the dynamics and rewards\nof the environment are estimated from the data. In\nthis paper, we approximate the Robust Reinforce-\nment Learning constrained with a \u03a6-divergence\nusing an approximate Risk-Averse formulation.\nWe show that the classical Reinforcement Learn-\ning formulation can be robusti\ufb01ed using standard\ndeviation penalization of the objective. Two al-\ngorithms based on Distributional Reinforcement\nLearning, one for discrete and one for continu-\nous action spaces are proposed and tested in a\nclassical Gym environment to demonstrate the\nrobustness of the algorithms.\n1. Introduction\nThe classical Reinforcement Learning (RL) problem using\nMarkov Decision Processes (MDPs) modelization gives a\npractical framework for solving sequential decision prob-\nlems under uncertainty of the environment. However, for\nreal-world applications, the \ufb01nal chosen policy can be some-\ntimes very sensitive to sampling errors, inaccuracy of the\nmodel parameters, and de\ufb01nition of the reward. For dis-\ncrete state-action space, Robust MDPs is treated by Yang\n(2017); Petrik & Russel (2019); Grand-Cl\u00e9ment & Kroer\n(2020a;b) or (Behzadian et al., 2021) among others. Here\nwe focus on more general continuous state space S with\ndiscrete or continuous action space A and with constraints\nde\ufb01ned using \u03a6-divergence. Robust RL (Morimoto & Doya,\n2005) with continuous action space focuses on robustness\nin the dynamics of the system (changes of P) and has been\nstudied in Abdullah et al. (2019); Singh et al. (2020); Urp\u00ed\net al. (2021); Eysenbach & Levine (2021) among others.\n1Centre de Math\u00e9matiques Appliqu\u00e9es, Ecole polytechnique,\nFrance 2INRIA HeKA, INSERM, Sorbonne Universit\u00e9, Uni-\nversit\u00e9 Paris Cit\u00e9, France. Correspondence to: Pierre Clavier\n<pierre.clavier@polytechnique.edu>.\nICML 2022 Workshop on Responsible Decision Making in Dy-\nnamic Environments, Baltimore, Maryland, USA, 2022. Copyright\n2022 by the author(s).\nEysenbach & Levine (2021) tackles the problem of both\nreward and transitions using Max Entropy RL whereas the\nproblem of robustness in action noise perturbation is pre-\nsented in Tessler et al. (2019). Here we tackle the problem\nof Robustness thought dynamics of the system.\nIn this paper, we show that it is possible to approximate\na Robust Distributional Reinforcement Learning heuristic\nwith \u03a6-divergence constraints into a Risk-averse formula-\ntion, using a formulation based on mean-standard deviation\noptimization. Moreover, we focus on the idea that general-\nization, regularization, and robustness are strongly linked\ntogether as Husain et al. (2021); Derman & Mannor (2020);\nDerman et al. (2021); Ying et al. (2021); Brekelmans et al.\n(2022) noticed in the MDPs or RL framework. The contri-\nbution of the work is the following: we motivate the use of\nstandard deviation penalization and derive two algorithms\nfor discrete and continuous action space that are Robust to\nchange in dynamics. These algorithms do not require lots\nof additional parameter tuning, only the Mean-Standard De-\nviation trade-off has to be chosen carefully. Moreover, we\nshow that our formulation using Distributional Reinforce-\nment Learning is robust to change dynamics on discrete and\ncontinuous action spaces from Mujoco suite.\n2. Notations\nConsidering\na\nMarkov\nDecision\nProcess\n(MDP)\n(S, A, P, \u03b3), where S is the state space, A is the ac-\ntion space, P (s\u2032, r | s, a) is the reward and transition\ndistribution from state s to s\u2032 taking action a and \u03b3 \u2208 (0, 1)\nis the discount factor. Stochastic policy are denoted \u03c0(a |\ns) : S \u2192 \u2206(A) and we consider the cases of action space\neither discrete our continuous.\nA rollout or trajectory using \u03c0 from state s using initial\naction a is de\ufb01ned as the the random sequence \u03c4 P,\u03c0|s,a =\n((s0, a0, r0) , (s1, a1, r1) , . . .) with s0 = s, a0 = a, at \u223c\n\u03c0 (\u00b7 | st) and (rt, st+1)\n\u223c\nP (\u00b7, \u00b7 | st, at) ; we denote\nthe distribution over rollouts by P(\u03c4) with P(\u03c4)\n=\nP0 (s0) \ufffdT\nt=0 P (st+1, rt | st, at) \u03c0 (at | st) d\u03c4 and usually\nwrite \u03c4 \u223c P = (P, \u03c0). Moreover, considering the dis-\ntribution of discounted cumulative return ZP,\u03c0(s, a) =\nR(\u03c4 P,\u03c0|s,a) with R(\u03c4) = \ufffd\u221e\nt=0 \u03b3trt, the Q-function\nQP,\u03c0 : S \u00d7 A \u2192 R of \u03c0 is the expected discounted\ncumulative return of the distribution de\ufb01ned as follows :\narXiv:2206.06841v1  [cs.LG]  14 Jun 2022\n",
    "GPTsummary": "\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work is that it proposes a Robust Reinforcement Learning approach that focuses on making predictions more robust to changes in the system's dynamics or rewards, which is critical for real-world applications. The proposed method achieves this by approximating the Robust Distributional Reinforcement Learning heuristic with \u03a6-divergence constraints into a Risk-averse formulation that focuses on mean-standard deviation optimization.\n\n- (2): In terms of innovation point, this article proposes a novel approach to Robust Reinforcement Learning by deriving two algorithms that focus solely on the Mean-Standard Deviation trade-off and approximating the Robust Distributional Reinforcement Learning heuristic. However, there is no novelty in the use of standard deviation penalization, which has been used in prior related works. In terms of performance, the proposed methods are tested in a classical Gym environment, and the results show that they are robust to changes in dynamics in both discrete and continuous action spaces from Mujoco suite. However, the experiments are limited to a single environment, and the performance is not significantly better than the state-of-the-art methods. In terms of workload, the paper covers a comprehensive literature review, and the proposed methods are well explained with mathematical derivations. Overall, this article provides a promising approach to Robust Reinforcement Learning, but there is room for improvement in terms of innovation and empirical evaluation.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is that it proposes a Robust Reinforcement Learning approach that focuses on making predictions more robust to changes in the system's dynamics or rewards, which is critical for real-world applications. The proposed method achieves this by approximating the Robust Distributional Reinforcement Learning heuristic with \u03a6-divergence constraints into a Risk-averse formulation that focuses on mean-standard deviation optimization.\n\n- (2): In terms of innovation point, this article proposes a novel approach to Robust Reinforcement Learning by deriving two algorithms that focus solely on the Mean-Standard Deviation trade-off and approximating the Robust Distributional Reinforcement Learning heuristic. However, there is no novelty in the use of standard deviation penalization, which has been used in prior related works. In terms of performance, the proposed methods are tested in a classical Gym environment, and the results show that they are robust to changes in dynamics in both discrete and continuous action spaces from Mujoco suite. However, the experiments are limited to a single environment, and the performance is not significantly better than the state-of-the-art methods. In terms of workload, the paper covers a comprehensive literature review, and the proposed methods are well explained with mathematical derivations. Overall, this article provides a promising approach to Robust Reinforcement Learning, but there is room for improvement in terms of innovation and empirical evaluation.\n\n\n"
}