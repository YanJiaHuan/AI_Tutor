{
    "Abstract": "Abstract\u2014This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been ef\ufb01ciently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to \ufb01nd the optimal policy in reasonable time. Therefore, deep reinforcement learning, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we \ufb01rst give a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data of\ufb02oading, network security, and connectivity preservation which are all important to next generation networks such as 5G and beyond. Furthermore, we present applications of deep reinforcement learning for traf\ufb01c routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying deep reinforcement learning. Keywords- Deep reinforcement learning, deep Q-learning, networking, communications, spectrum access, rate control, security, caching, data of\ufb02oading, data collection. I. ",
    "Introduction": "INTRODUCTION Reinforcement learning [1] is one of the most important research directions of machine learning which has signi\ufb01cant impacts to the development of Arti\ufb01cial Intelligence (AI) over the last 20 years. Reinforcement learning is a learning process in which an agent can periodically make decisions, observe the results, and then automatically adjust its strategy to achieve the optimal policy. However, this learning process, even though N. C. Luong and D. Niyato are with School of Computer Science and Engineering, Nanyang Technological University, Singapore. E-mails: clnguyen@ntu.edu.sg, dniyato@ntu.edu.sg. D. T. Hoang is with the Faculty of Engineering and Information Technology, University of Technology Sydney, Australia. E-mail: hoang.dinh@uts.edu.au. S. Gong is with the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China. E-mail: sm.gong@siat.ac.cn. P. Wang is with Department of Electrical Engineering & Computer Science, York University, Canada. E-mail: pingw@yorku.ca. Y.-C. Liang is with Center for Intelligent Networking and Communications (CINC), with University of Electronic Science and Technology of China, Chengdu, China. E-mail: liangyc@ieee.org. D. I. Kim is with School of Information and Communication Engineering, Sungkyunkwan University, Korea. Email: dikim@skku.ac.kr. proved to converge, takes a lot of time to reach the best policy as it has to explore and gain knowledge of an entire system, making it unsuitable and inapplicable to large-scale networks. Consequently, applications of reinforcement learning are very limited in practice. Recently, deep learning [2] has been introduced as a new breakthrough technique. It can overcome the limitations of reinforcement learning, and thus open a new era for the development of reinforcement learning, namely Deep Reinforcement Learning (DRL). The DRL embraces the advantage of Deep Neural Networks (DNNs) to train the learning process, thereby improving the learning speed and the performance of reinforcement learning algorithms. As a result, DRL has been adopted in a numerous applications of reinforcement learning in practice such as robotics, computer vision, speech recognition, and natural language processing [2]. One of the most famous applications of DRL is AlphaGo [3], the \ufb01rst computer program which can beat a human professional without handicaps on a full-sized 19\u00d719 board. In the areas of communications and networking, DRL has been recently used as an emerging tool to effectively address various problems and challenges. In particular, modern networks such as Internet of Things (IoT), Heterogeneous Networks (HetNets), and Unmanned Aerial Vehicle (UAV) network become more decentralized, ad-hoc, and autonomous in nature. Network entities such as IoT devices, mobile users, and UAVs need to make local and autonomous decisions, e.g., spectrum access, data rate selection, transmit power control, and base station association, to achieve the goals of different networks including, e.g., throughput maximization and energy consumption minimization. Under uncertain and stochastic environments, most of the decision-making problems can be modeled by a so-called Markov Decision Process (MDP) [4]. Dynamic programming [5], [6] and other algorithms such as value iteration, as well as reinforcement learning techniques can be adopted to solve the MDP. However, the modern networks are large-scale and complicated, and thus the computational complexity of the techniques rapidly becomes unmanageable. As a result, DRL has been developing to be an alternative solution to overcome the challenge. In general, the DRL approaches provide the following advantages: \u2022 DRL can obtain the solution of sophisticated network optimizations. Thus, it enables network controllers, e.g., base stations, in modern networks to solve non-convex and complex problems, e.g., joint user association, computation, and transmission schedule, to achieve the oparXiv:1810.07862v1  [cs.NI]  18 Oct 2018 2 timal solutions without complete and accurate network information. \u2022 DRL allows network entities to learn and build knowledge about the communication and networking environment. Thus, by using DRL, the network entities, e.g., a mobile user, can learn optimal policies, e.g., base station selection, channel selection, handover decision, caching and of\ufb02oading decisions, without knowing channel model and mobility pattern. \u2022 DRL provides autonomous decision-making. With the DRL approaches, network entities can make observation and obtain the best policy locally with minimum or without information exchange among each other. This not only reduces communication overheads but also improves security and robustness of the networks. \u2022 DRL improves signi\ufb01cantly the learning speed, especially in the problems with large state and action spaces. Thus, in large-scale networks, e.g., IoT systems with thousands of devices, DRL allows network controller or IoT gateways to control dynamically user association, spectrum access, and transmit power for a massive number of IoT devices and mobile users. \u2022 Several other problems in communications and networking such as cyber-physical attacks, interference management, and data of\ufb02oading can be modeled as games, e.g., the non-cooperative game. DRL has been recently used as an ef\ufb01cient tool to solve the games, e.g., \ufb01nding the Nash equilibrium, without the complete information. Although there are some surveys related to DRL, they do not focus on communications and networking. For example, the surveys of applications of DRL for computer vision and natural language processing can be found in [7] and [8]. Also, there are surveys related to the use of only \u201cdeep learning\u201d for networking. For example, the survey of machine learning for wireless networks is given in [9], but it does not focus on the DRL approaches. To the best of our knowledge, there is no survey speci\ufb01cally discussing the applications of DRL in communications and networking. This motivates us to deliver the survey with the tutorial of DRL and the comprehensive literature review on the applications of DRL to address issues in communications and networking. For convenience, the related works in this survey are classi\ufb01ed based on issues in communications and networking as shown in Fig. 2. The major issues include network access, data rate control, wireless caching, data of\ufb02oading, network security, connectivity preservation, traf\ufb01c routing, and data collection. Also, the percentages of DRL related works for different networks and different issues in the networks are shown in Figs. 1(a) and 1(b), respectively. From the \ufb01gures, we observe that the majority of the related works are for the cellular networks. Also, the related works to the wireless caching and of\ufb02oading have received more attention than the other issues. The rest of this paper is organized as follows. Section II presents the introduction of reinforcement learning and discusses DRL techniques as well as their extensions. Section III reviews the applications of DRL for dynamic network access and adaptive data rate control. Section IV discusses the apIoT 9% Cellular 31% Ad-hoc 19% Space  communications 13% Congitive  radio 9% Others 28% (a) Network access 13% Rate  control 8% Wireless  caching 19% Computational  offloading 13% Network security 12% Connectivity  prevservation  8% Traffic  routing 9% Resource  scheduling 9% Data collection 9% (b) Fig. 1: Percentages of related work for (a) different networks and (b) different issues in the networks. TABLE I: List of abbreviations Abbreviation Description ANN/APF Arti\ufb01cial Neural Network/Arti\ufb01cial Potential Field A3C Asynchronous Advantage Actor- Critic CRN Cognitive Radio Network CNN Convolutional Neural Network DRL/DQL Deep Reinforcement Learning/Deep Q-Learning DNN Deep Neural Network DQN/DDQN/DRQN Deep Q-Network/Double DQN/Deep Recurrent QLearning DASH Dynamic Adaptive Streaming over HTTP DoS Denial-of-Service ESN Echo State Network FNN/RNN Feedforward Neural Network/Recurrent Neural Network FSMC Finite-State Markov Channel HVFT High Volume Flexible Time ITS Intelligent Transportation System LSM/LSTM Liquid State Machine/Long Short-Term Memory MEC Mobile Edge Computing MDP/POMDP Markov Decision Process/Partially Observable MDP NFSP Neural Fictitious Self-Play NFV Network Function Virtualization RDPG Recurrent Deterministic Policy Gradient RCNN Recursive Convolutional Neural Network RRH/BBU Remote Radio Head/BaseBand Unit RSSI Received Signal Strength Indicators SPD Sequential Prisoner\u2019s Dilemma SBS/BS Small Base Station/Base Station SDN Software-De\ufb01ned Network SU/PU Secondary User/Primary User UDN/UAN Ultra-Density Network/Underwater Acoustic Network UAV Unmanned Aerial Vehicle VANET/V2V Vehicular Ad hoc Network/Vehicle-to-Vehicle plications of DRL for wireless caching and data of\ufb02oading. Section V presents DRL related works for network security and connectivity preservation. Section VI considers how to use DRL to deal with other issues in communications and networking. Important challenges, open issues, and future research 3 Applications of deep reinforcement learning  for communications and networking Network access and rate  control  Miscellaneous issues Security and  connectivity preservation Caching and offloading     Network  access Adaptive  rate control Proactive  caching Data   offloading Network  security Connectivity preservation Traffic  routing Resource  scheduling Data  collection Fig. 2: A taxonomy of the applications of deep reinforcement learning for communications and networking. directions are outlined in Section VII. Section VIII concludes the paper. The list of abbreviations commonly appeared in this paper is given in Table I. Note that DRL consists of two different algorithms which are Deep Q-Learning (DQL) and policy gradients [10]. In particular, DQL is mostly used for the DRL related works. Therefore, in the rest of the paper, we use \u201cDRL\u201d and \u201cDQL\u201d interchangeably to refer to the DRL algorithms. II. DEEP REINFORCEMENT LEARNING: AN OVERVIEW In this section, we \ufb01rst present fundamental knowledge of Markov decision processes, reinforcement learning, and deep learning techniques which are important branches of machine learning theory. We then discuss DRL technique that can capitalize on the capability of the deep learning to improve ef\ufb01ciency and performance in terms of the learning rate for reinforcement learning algorithms. Afterward, advanced DRL models and their extensions are reviewed. A. Markov Decision Processes MDP [4] is a discrete time stochastic control process. MDP provides a mathematical framework for modeling decisionmaking problems in which outcomes are partly random and under control of a decision maker or an agent. MDPs are useful for studying optimization problems which can be solved by dynamic programming and reinforcement learning techniques. Typically, an MDP is de\ufb01ned by a tuple (S, A, p, r) where S is a \ufb01nite set of states, A is a \ufb01nite set of actions, p is a transition probability from state s to state s\u2032 after action a is executed, and r is the immediate reward obtained after action a is performed. We denote \u03c0 as a \u201cpolicy\u201d which is a mapping from a state to an action. The goal of an MDP is to \ufb01nd an optimal policy to maximize the reward function. An MDP can be \ufb01nite or in\ufb01nite time horizon. For an in\ufb01nite time horizon MDP, we aim to \ufb01nd an optimal policy \u03c0\u2217 to maximize the expected total reward de\ufb01ned by \u221e \ufffd t=0 \u03b3rt(st, at), where at = \u03c0\u2217(st), and \u03b3 \u2208 [0, 1] is the discount factor. 1) Partially Observable Markov Decision Process: In MDPs, we assume that the system state is fully observable by the agent. However, in many cases, the agent only can observe a part of the system state, and thus Partially Observable Markov Decision Processes (POMDPs) [11] can be used to model the decision-making problems. A typical POMDP model is de\ufb01ned by a 6-tuple (S, A, p, r, \u2126, O), where S, A, p, r are de\ufb01ned the same as in the MDP model, \u2126 and O are de\ufb01ned as the set of observations and observation probabilities, respectively. At each time step, the system is at state s \u2208 S. Then, the agent takes an action a \u2208 A and the system transits to state s\u2032 \u2208 S. At the same time, the agent has a new observation o \u2208 \u2126 with probability O(o|s, a, s\u2032). Finally, the agent receives an immediate reward r that is equal to r(s, a) in the MDP. Similar to the MDP model, the agent in POMDP also aims to \ufb01nd the optimal policy \u03c0\u2217 in order to maximize its expected long-term discounted reward \u221e \ufffd t=0 \u03b3rt(st, \u03c0\u2217(st)). 2) Markov Games: In game theory, a Markov game, or a stochastic game [12], is a dynamic game with probabilistic transitions played by multiple players, i.e., agents. A typical Markov game model is de\ufb01ned by a tuple (I, S, {Ai}i\u2208I, p, {ri}i\u2208I), where \u2022 I \u225c {1, . . . , i, . . . , I} is a set of agents, \u2022 S \u225c {S1, . . . , Si, . . . , SI} is the global state space of the all agents with Si being the state space of agent i, \u2022 {Ai}i\u2208I are sets of action spaces of the agents with Ai being the action space of agent i, \u2022 p \u225c S\u00d7A1\u00d7\u00b7 \u00b7 \u00b7\u00d7AI \u2192 [0, 1] is the transition probability function of the system. \u2022 {ri}i\u2208I are payoff functions of the agents with ri \u225c S \u00d7 A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 AI \u2192 R, i.e., the payoff of agent i obtained after all actions of the agents are executed. In a Markov game, the agents start at some initial state s0 \u2208 S. After observing the current state, all the agents simultaneously select their actions a = {a1, . . . , aI} and they will receive their corresponding rewards together with their own new observations. At the same time, the system will transit to a new state s\u2032 \u2208 S with probability p(s\u2032|s, a). The 4 procedure is repeated at the new state and continues for a \ufb01nite or in\ufb01nite number of stages. In this game, all the agents try to \ufb01nd their optimal policies to maximize their own expected long-term average rewards, i.e., \u221e \ufffd t=0 \u03b3iri t(st, \u03c0\u2217 i (st)), \u2200i. The set of all optimal policies of this game, i.e., {\u03c0\u2217 1, . . . , \u03c0\u2217 I} is known to be the equilibrium of this game. If there is a \ufb01nite number of players and the sets of states and actions are \ufb01nite, then the Markov game always has a Nash equilibrium [13] under a \ufb01nite number of stages. The same is true for Markov games with in\ufb01nite stages, but the total payoff of agents is the discounted sum [13]. B. Reinforcement Learning Reinforcement learning, an important branch of machine learning, is an effective tool and widely used in the literature to address MDPs [1]. In a reinforcement learning process, an agent can learn its optimal policy through interaction with its environment. In particular, the agent \ufb01rst observes its current state, and then takes an action, and receives its immediate reward together with its new state as illustrated in Fig. 3(a). The observed information, i.e., the immediate reward and new state, is used to adjust the agent\u2019s policy, and this process will be repeated until the agent\u2019s policy approaches to the optimal policy. In reinforcement learning, Q-learning is the most effective method and widely used in the literature. In the following, we will discuss the Q-learning algorithm and its extensions for advanced MDP models. 1) Q-Learning Algorithm: In an MDP, we aim to \ufb01nd an optimal policy \u03c0\u2217 : S \u2192 A for the agent to minimize the overall cost for the system. Accordingly, we \ufb01rst de\ufb01ne value function V\u03c0 : S \u2192 R that represents the expected value obtained by following policy \u03c0 from each state s \u2208 S. The value function V for policy \u03c0 quanti\ufb01es the goodness of the policy through an in\ufb01nite horizon and discounted MDP that can be expressed as follows: V\u03c0(s) = E\u03c0 \ufffd \u221e \ufffd t=0 \u03b3rt(st, at)|s0 = s \ufffd = E\u03c0 \ufffd rt(st, at) + \u03b3V\u03c0(st+1)|s0 = s \ufffd . (1) Since we aim to \ufb01nd the optimal policy \u03c0\u2217, an optimal action at each state can be found through the optimal value function expressed by V\u2217(s) = max at \ufffd E\u03c0 \ufffd rt(st, at)+\u03b3V\u03c0(st+1) \ufffd\ufffd . If we denote Q\u2217(s, a) \u225c rt(st, at)+\u03b3E\u03c0 \ufffd V\u03c0(st+1) \ufffd as the optimal Q-function for all state-action pairs, then the optimal value function can be written by V\u2217(s) = max a \ufffd Q\u2217(s, a) \ufffd . Now, the problem is reduced to \ufb01nd optimal values of Qfunction, i.e., Q\u2217(s, a), for all state-action pairs, and this can be done through iterative processes. In particular, the Qfunction is updated according to the following rule: Qt+1(s, a) =Qt(s, a)+ \u03b1t \ufffd rt(s, a) + \u03b3 max a\u2032 Qt(s, a\u2032) \u2212 Qt(s, a) \ufffd . (2) The core idea behind this update is to \ufb01nd the Temporal Difference (TD) between the predicted Q-value, i.e., rt(s, a)+ \u03b3max a\u2032 Qt(s, a\u2032) and its current value, i.e., Qt(s, a). In (2), the learning rate \u03b1t is used to determine the impact of new information to the existing Q-value. The learning rate can be chosen to be a constant, or it can be adjusted dynamically during the learning process. However, it must satisfy Assumption 1 to guarantee the convergence for the Q-learning algorithm. Assumption 1. The step size \u03b1t is deterministic, nonnegative and satis\ufb01es the following conditions: \u03b1t \u2208 [0, 1], \u221e \ufffd t=0 \u03b1t = \u221e, and \u221e \ufffd t=0 (\u03b1t)2 < \u221e . The step size adaptation \u03b1t = 1 t is one of the most common examples used in reinforcement learning. More discussions for selecting an appropriate step size can be found in [14]. The details of the Q-learning algorithm are then provided in Algorithm 1. Algorithm 1 The Q-learning algorithm Input: For each state-action pair (s, a), initialize the table entry Q(s, a) arbitrarily, e.g., to zero. Observe the current state s, initialize a value for the learning rate \u03b1 and the discount factor \u03b3. for t := 1 to T do From the current state-action pair (s, a), execute action a and obtain the immediate reward r and a new state s\u2032. Select an action a\u2032 based on the state s\u2032 and then update the table entry for Q(s, a) as follows: Qt+1(s, a) \u2190 Qt(s, a) + \u03b1t \ufffd rt(s, a)+ \u03b3 max a\u2032 Qt(s\u2032, a\u2032) \u2212 Qt(s, a) \ufffd (3) Replace s \u2190 s\u2032. end for Output: \u03c0\u2217(s) = arg maxa Q\u2217(s, a). Once either all Q-values converge or a certain number of iterations is reached, the algorithm will terminate. The algorithm then yields the optimal policy indicating an action to be taken at each state such that Q\u2217(s, a) is maximized for all states in the state space, i.e., \u03c0\u2217(s) = arg max a Q\u2217(s, a). Under the assumption of the step size (i.e., Assumption 1), it is proved in [15] that the Q-learning algorithm converges to the optimum action-values with probability one. 2) SARSA: An Online Q-Learning Algorithm: Although the Q-learning algorithm can \ufb01nd the optimal policy for the agent without requiring knowledge about the environment, this algorithm works in an of\ufb02ine fashion. In particular, Algorithm 1 can obtain the optimal policy only after all Q-values converge. Therefore, this section presents an alternative online learning algorithm, i.e., the SARSA algorithm, which allows the agent to approach the optimal policy in an online fashion. Different from the Q-learning algorithm, the SARSA algorithm is an online algorithm which allows the agent to 5 Environment Agent Action a Immediate reward r Observed state s Controller  policy \u03c0  Input layer Hidden layer Output layer Outputs Inputs Weights \u03b8  Environment Agent Action a Immediate reward r Observed state s Controller policy \u03c0  features Weights \u03b8  (a) (b) (c) Fig. 3: (a) Reinforcement learning, (b) Arti\ufb01cial neural network, and (c) Deep Q-learning. choose optimal actions at each time step in a real-time fashion without waiting until the algorithm converges. In the Qlearning algorithm, the policy is updated according to the maximum reward of available actions regardless of which policy is applied, i.e., an off-policy method. In contrast, the SARSA algorithm interacts with the environment and updates the policy directly from the actions taken, i.e., an on-policy method. Note that the SARSA algorithm updates Q-values from the quintuple Q(s, a, r, s\u2032, a\u2032). 3) Q-Learning for Markov Games: To apply Q-learning algorithm to the Markov game context, we \ufb01rst de\ufb01ne the Q-function for agent i by Qi(s, ai, a\u2212i), where a\u2212i \u225c {a1, . . . , ai\u22121, ai+1, . . . , aI} denotes the set of actions of all agents except i. Then, the Nash Q-function of agent i is de\ufb01ned by: Q\u2217 i (s, ai, a\u2212i) = ri(s, ai, a\u2212i)+ \u03b2 \ufffd s\u2032\u2208S p(s\u2032|s, ai, a\u2212i)Vi(s\u2032, \u03c0\u2217 1, . . . , \u03c0\u2217 I), (4) where (\u03c0\u2217 1, . . . , \u03c0\u2217 I) is the joint Nash equilibrium strategy, ri(s, ai, a\u2212i) is agent i\u2019s immediate reward in state s under the joint action (ai, a\u2212i), and Vi(s\u2032, \u03c0\u2217 1, . . . , \u03c0\u2217 I) is the total discounted reward over an in\ufb01nite time horizon starting from state s\u2032 given that all the agents follow the equilibrium strategies. In [13], the authors propose a multi-agent Q-learning algorithm for general-sum Markov games which allows the agents to perform updates based on assuming Nash equilibrium behavior over the current Q-values. In particular, agent i will learn its Q-values by forming an arbitrary guess from starting time of the game. At each time step t, agent i observes the current state and takes an action ai. Then, it observes its immediate reward ri, actions taken by others a\u2212i, others\u2019 immediate rewards, and the new system state s\u2032. After that, agent i calculates a Nash equilibrium (\u03c01(s\u2032), . . . , \u03c0I(s\u2032)) for the state game (Qt 1(s\u2032), . . . , Qt I(s\u2032)), and updates its Q-values according to: Qt+1 i (s, ai, a\u2212i) = (1\u2212\u03b1t)Qt i(s, ai, a\u2212i)+\u03b1t[ri t +\u03b3N i t (s\u2032)], (5) where \u03b1t \u2208 (0, 1) is the learning rate and N i t (s\u2032) \u225c Qt i(s\u2032)\u00d7 \u03c01(s\u2032) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u03c0I(s\u2032). In order to calculate the Nash equilibrium, agent i needs to know (Qt 1(s\u2032), . . . , Qt I(s\u2032)). However, the information about other agents\u2019 Q-values is not given, and thus agent i must learn this information too. To do so, agent i will set estimations about others\u2019 Q-values at the beginning of the game, e.g., Qj 0(s, ai, a\u2212i) = 0, \u2200j, s. As the game proceeds, agent i observes other agents\u2019 immediate rewards and previous actions. That information can then be used to update agent i\u2019s conjectures on other agents\u2019 Q-functions. Agent i updates its beliefs about agent j\u2019s Q-function, according to the same updating rule in (5). Then, the authors prove that under some highly restrictive assumptions on the form of the state games during learning, the proposed multi-agent Q-learning algorithm is guaranteed to be converged. C. Deep Learning Deep learning [2] is composed of a set of algorithms and techniques that attempt to \ufb01nd important features of data and to model its high-level abstractions. The main goal of deep learning is to avoid manual description of a data structure (like hand-written features) by automatic learning from the data. Its name refers to the fact that typically any neural network with two or more hidden layers is called DNN. Most deep learning models are based on an Arti\ufb01cial Neural Network (ANN), even though they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in Deep Belief Networks and Deep Boltzmann Machines. An ANN is a computational nonlinear model based on the neural structure of the brain that is able to learn to perform tasks such as classi\ufb01cation, prediction, decision-making, and visualization. An ANN consists of arti\ufb01cial neurons and is organized into three interconnected layers: input, hidden, and output as illustrated in Fig. 3(b). The input layer contains input neurons that send information to the hidden layer. The hidden layer sends data to the output layer. Every neuron has weighted inputs (synapses), an activation function (de\ufb01nes the output given an input), and one output. Synapses are the adjustable parameters that convert a neural network to a parameterized system. During the training phase, ANNs use backpropagation as an effective learning algorithm to compute quickly a gradient descent with respect to the weights. Backpropagation is a special case of automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent 6 optimization algorithm to adjust the weights of neurons by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers. A DNN is de\ufb01ned as an ANN with multiple hidden layers. There are two typical DNN models, i.e., Feedforward Neural Network (FNN) and Recurrent Neural Network (RNN). In the FNN, the information moves in only one direction, i.e., from the input nodes, through the hidden nodes and to the output nodes, and there are no cycles or loops in the network as shown in Fig. 4. In FNNs, Convolutional Neural Network (CNN) is the most well known model with a wide range of applications especially in image and speech recognition. The CNN contains one or more convolutional layers, pooling or fully connected, and uses a variation of multilayer perceptrons discussed above. Convolutional layers use a convolution operation to the input passing the result to the next layer. This operation allows the network to be deeper with much fewer parameters. Recurrent Neural Network (RNN) Feed-Forward Neural Network (FNN) Fig. 4: RNN vs CNN. Unlike FNNs, the RNN is a variant of a recursive arti\ufb01cial neural network in which connections between neurons make directed cycles. It means that an output depends not only on its immediate inputs, but also on the previous further step\u2019s neuron state. The RNNs are designed to utilize sequential data, when the current step has some relation with the previous steps. This makes the RNNs ideal for applications with a time component, e.g., time-series data, and natural language processing. However, all RNNs have feedback loops in the recurrent layer. This lets RNNs maintain information in memory over time. Nevertheless, it can be dif\ufb01cult to train standard RNNs to solve problems that require learning longterm temporal dependencies. The reason is that the gradient of the loss function decays exponentially with time, which is called the vanishing gradient problem. Thus, Long Short-Term Memory (LSTM) is often used in RNNs to address this issue. The LSTM is designed to model temporal sequences and their long-range dependencies are more accurate than conventional RNNs. The LSTM does not use an activation function within its recurrent components, the stored values are not modi\ufb01ed, and the gradient does not tend to vanish during training. Usually, LSTM units are implemented in \u201cblocks\u201d with several units. These blocks have three or four \u201cgates\u201d, e.g., input gate, forget gate, output gate, that control information \ufb02ow drawing on the logistic function. D. Deep Q-Learning The Q-learning algorithm can ef\ufb01ciently obtain an optimal policy when the state space and action space are small. However, in practice, with complicated system models, these spaces are usually large. As a result, the Q-learning algorithm may not be able to \ufb01nd the optimal policy. Thus, Deep QLearning (DQL) algorithm is introduced to overcome this shortcoming. Intuitively, the DQL algorithm implements a Deep Q-Network (DQN), i.e., a DNN, instead of the Q-table to derive an approximate value of Q\u2217(s, a) as shown in Fig. 3(c). As stated in [16], the average reward obtained by reinforcement learning algorithms may not be stable or even diverge when a nonlinear function approximator is used. This stems from the fact that a small change of Q-values may greatly affect the policy. Thus, the data distribution and the correlations between the Q-values and the target values R + \u03b3 maxa\u2032 Q(s\u2032, a\u2032) are varied. To address this issue, two mechanisms, i.e., experience replay and target Q-network, can be used. \u2022 Experience replay mechanism: The algorithm \ufb01rst initializes a replay memory D, i.e., the memory pool, with transitions (st, at, rt, st+1), i.e., experiences, generated randomly, e.g., through using \u03f5-greedy policy. Then, the algorithm randomly selects samples, i.e., minibatches, of transitions from D to train the DNN. The Q-values obtained by the trained DNN will be used to obtain new experiences, i.e., transitions, and these experiences will be then stored in the memory pool D. This mechanism allows the DNN trained more ef\ufb01ciently by using both old and new experiences. In addition, by using the experience replay, the transitions are more independent and identically distributed, and thus the correlations between observations can be removed. \u2022 Fixed target Q-network: In the training process, the Qvalue will be shifted. Thus, the value estimations can be out of control if a constantly shifting set of values is used to update the Q-network. This leads to the destabilization of the algorithm. To address this issue, the target Q-network is used to update frequently but slowly the primary Q-networks\u2019 values. In this way, the correlations between the target and estimated Qvalues are signi\ufb01cantly reduced, thereby stabilizing the algorithm. The DQL algorithm with experience replay and \ufb01xed target Q-network is presented in Algorithm 2. DQL inherits and promotes advantages of both reinforcement and deep learning techniques, and thus it has a wide range of applications in practice such as game development [3], transportation [17], and robotics [18]. E. Advanced Deep Q-Learning Models 1) Double Deep Q-Learning: In some stochastic environments, the Q-learning algorithm performs poorly due to the large over-estimations of action values [19]. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value as shown in 7 Algorithm 2 The DQL Algorithm with Experience Replay and Fixed Target Q-Network 1: Initialize replay memory D. 2: Initialize the Q-network Q with random weights \u03b8. 3: Initialize the target Q-network \u02c6Q with random weights \u03b8\u2032. 4: for episode=1 to T do 5: With probability \u03f5 select a random action at, otherwise select at = arg max Q\u2217(st, at, \u03b8). 6: Perform action at and observe immediate reward rt and next state st+1. 7: Store transition (st, at, rt, st+1) in D. 8: Select randomly samples c(sj, aj, rj, sj+1) from D. 9: The weights of the neural network then are optimized by using stochastic gradient descent with respect to the network parameter \u03b8 to minimize the loss: \ufffd rj + \u03b3 max aj+1 \u02c6Q(sj+1, aj+1; \u03b8\u2032) \u2212 Q(sj, aj; \u03b8) \ufffd2 . (6) 10: Reset \u02c6Q = Q after every a \ufb01xed number of steps. 11: end for Eq. (3). The reason is that the same samples are used to decide which action is the best, i.e., with highest expected reward, and the same samples are also used to estimate that action-value. Thus, to overcome the over-estimation problem of the Qlearning algorithm, the authors in [20] introduce a solution using two Q-value functions, i.e., Q1 and Q2, to simultaneously select and evaluate action values through the loss function as follows: \ufffd rj + \u03b3Q2 \ufffd sj+1, arg max aj+1Q1 \ufffd sj+1, aj+1; \u03b81 \ufffd ; \u03b82 \ufffd \u2212 Q1(sj, aj; \u03b81) \ufffd2 . Note that the selection of an action, in the arg max, is still due to the online weights \u03b81. This means that, as in Qlearning, we are still estimating the value of the greedy policy according to the current values, as de\ufb01ned by \u03b81. However, the second set of weights \u03b82 is used to evaluate fairly the value of this policy. This second set of weights can be updated symmetrically by switching the roles of \u03b81 and \u03b82. Inspired by this idea, the authors in [20] then develop Double Deep Q-Learning (DDQL) model [21] using a Double Deep QNetwork (DDQN) with the loss function updated as follows: \ufffd rj + \u03b3 \u02c6Q \ufffd sj+1, arg max aj+1 Q \ufffd sj+1, aj+1; \u03b8 \ufffd ; \u03b8\u2032\ufffd \u2212 Q(sj, aj; \u03b8) \ufffd2 . (7) Unlike double Q-learning, the weights of the second network \u03b82 are replaced with the weights of the target networks \u03b8\u2032 for the evaluation of the current greedy policy as shown in Eq. (7). The update to the target network stays unchanged from DQN, and remains a periodic copy of the online network. Due to the effectiveness of DDQL, there are some applications of DDQL introduced recently to address dynamic spectrum access problems in multichannel wireless networks [22] and resource allocation in heterogeneous networks [23]. 2) Deep Q-Learning with Prioritized Experience Replay: Experience replay mechanism allows the reinforcement learning agent to remember and reuse experiences, i.e., transitions, from the past. In particular, transitions are uniformly sampled from the replay memory D. However, this approach simply replays transitions at the same frequency as that the agent was originally experienced, regardless of their signi\ufb01cance. Therefore, the authors in [24] develop a framework for prioritizing experiences, so as to replay important transitions more frequently, and therefore learn more ef\ufb01ciently. Ideally, we want to sample more frequently those transitions from which there is much to learn. As a proxy for learning potential, the proposed Prioritized Experience Replay (PER) [24] samples transitions with probability pt relative to the last encountered absolute error de\ufb01ned as follows: pt \u221d \ufffd\ufffd\ufffdrj + \u03b3 max a\u2032 \u02c6Q(sj+1, a\u2032; \u03b8\u2032) \u2212 Q(sj, aj; \u03b8) \ufffd\ufffd\ufffd \u03c9 , (8) where \u03c9 is a hyper-parameter that determines the shape of the distribution. New transitions are inserted into the replay buffer with maximum priority, providing a bias towards recent transitions. Note that stochastic transitions may also be favoured, even when there is little left to learn about them. Through real experiments on many Atari games, the authors demonstrate that DQL with PER outperforms DQL with uniform replay on 41 out of 49 games. However, this solution is only appropriate to implement when we can \ufb01nd and de\ufb01ne the important experiences in the replay memory D. 3) Dueling Deep Q-Learning: The Q-values, i.e., Q(s, a), used in the Q-learning algorithm, i.e., Algorithm 1, are to express how good it is to take a certain action at a given state. The value of an action a at a given state s can actually be decomposed into two fundamental values. The \ufb01rst value is the state-value function, i.e., V (s), to estimate the importance of being in a particular state s. The second value is the action-value function, i.e., A (a), to estimate the importance of selecting an action a compared with other actions. As a result, the Q-value function can be expressed by two fundamental value functions as follows: Q(s, a) = V (s) + A (a). Stemming from the fact that in many MDPs, it is unnecessary to estimate both values, i.e., action and state values of Q-function Q(s, a), at the same time. For example, in many racing games, moving left or right matters if and only if the agent meets the obstacles or enemies. Inspired by this idea, the authors in [25] introduce an idea of using two streams, i.e., two sequences, of fully connected layers instead of using a single sequence with fully connected layers for the DQN. The two streams are constructed such that they are able to provide separate estimations on the action and state value functions, i.e., V (s) and A (a). Finally, the two streams are combined to generate a single output Q(s, a) as follows: Q(s, a; \u03b1, \u03b2) = V (s; \u03b2) + \ufffd A (s, a; \u03b1) \u2212 \ufffd a\u2032 A (s, a\u2032; \u03b1) |A| \ufffd , (9) where \u03b2 and \u03b1 are the parameters of the two streams V (s; \u03b2) and A (s, a\u2032; \u03b1), respectively. Here, |A| is the total number of actions in the action space A. Then, the loss function is derived in the similar way to (6) as follows: 8 \ufffd rj + \u03b3max aj+1 \u02c6Q(sj+1, aj+1; \u03b1\u2032, \u03b2\u2032) \u2212 Q(sj, aj; \u03b1, \u03b2) \ufffd2 . Through the simulation, the authors show that the proposed dueling DQN can outperform DDQN [21] in 50 out of 57 learned Atari games. However, the proposed dueling architecture only clearly bene\ufb01ts for MDPs with large action spaces. For small state spaces, the performance of dueling DQL is even not as good as that of double DQL as shown in simulation results in [25]. 4) Asynchronous Multi-step Deep Q-Learning: Most of the Q-learning methods such as DQL and dueling DQL rely on the experience replay method. However, such kind of method has several drawbacks. For example, it uses more memory and computation resources per real interaction, and it requires off-policy learning algorithms that can update from data generated by an older policy. This limits the applications of DQL. Therefore, the authors in [26] introduce a method using multiple agents to train the DNN in parallel. In particular, the authors propose a training procedure which utilizes asynchronous gradient decent updates from multiple agents at once. Instead of training one single agent that interacts with its environment, multiple agents are interacting with their own version of the environment simultaneously. After a certain amount of timesteps, accumulated gradient updates from an agent are applied to a global model, i.e., the DNN. These updates are asynchronous and lock free. In addition, to tradeoff between bias and variance in the policy gradient, the authors adopt n-step updates method [1] to update the reward function. In particular, the truncated n-step reward function can be de\ufb01ned by r(n) t = n\u22121 \ufffd k=0 \u03b3(k)rt+k+1. Thus, the alternative loss for each agent will be derived by: \ufffd r(n) j + \u03b3(n) j max a\u2032 \u02c6Q(sj+n, a\u2032; \u03b8\u2032) \u2212 Q(sj, aj; \u03b8) \ufffd2 . (10) The effects of training speed and quality of the proposed asynchronous DQL with multi-step learning are analyzed for various reinforcement learning methods, e.g., 1-step Qlearning, 1-step SARSA, and n-step Q-learning. They show that asynchronous updates have a stabilizing effect on policy and value updates. Also, the proposed method outperforms the current state-of-the-art algorithms on the Atari games while training for half of the time on a single multi-core CPU instead of a GPU. As a result, some recent applications of asynchronous DQL have been developed for handover control problems in wireless systems [27] 5) Distributional Deep Q-learning: All aforementioned methods use the Bellman equation to approximate the expected value of future rewards. However, if the environment is stochastic in nature and the future rewards follow multimodal distribution, choosing actions based on expected value may not lead to the optimal outcome. For example, we know that the expected transmission time of a packet in a wireless network is 20 minutes. However, this information may not be so meaningful because it may overestimate the transmission time most of the time. For example, the expected transmission time is calculated based on the normal transmissions (without collisions) and the interference transmissions (with collisions). Although the interference transmissions are very rare to happen, but it takes a lot of time. Then, the estimation about the expected transmission is overestimated most of the time. This makes estimations not useful for the DQL algorithms. Thus, the authors in [28] introduce a solution using distributional reinforcement learning to update Q-value function based on its distribution rather than its expectation. In particular, let Z(s, a) be the return obtained by starting from state s, executing action a, and following the current policy, then Q(s, a) = E[Z(s, a)]. Here, Z represents the distribution of future rewards, which is no longer a scalar quantity like Qvalues. Then we obtain the distributional version of Bellman equation as follows: Z(s, a) = r + \u03b3Z(s\u2032, a\u2032). For example, if we use the DQN and extract an experience (s, a, r, s\u2032) from the replay buffer, then the sample of the target distribution is Z(s, a) = r + \u03b3Z(s\u2032, a\u2217) with a\u2217 = arg max a\u2032 Q(s, a\u2032). Although the proposed distributional deep Q-learning is demonstrated to outperform the conventional DQL [16] on many Atari 2600 Games (45 out of 57 games), its performance relies much on the distribution function Z. If Z is well de\ufb01ned, the performance of distributional deep Q-learning is much more signi\ufb01cant than that of the DQL. Otherwise, its performance is even worse than that of the DQL. 6) Deep Q-learning with Noisy Nets: In [29], the authors introduce Noisy Net, a type of neural network whose bias and weights are iteratively perturbed during training by a parametric function of the noise. This network basically adds the Gaussian noise to the last (fully-connected) layers of the network. The parameters of this noise can be adjusted by the model during training, which allows the agent to decide when and in what proportion it wants to introduce the uncertainty to its weights. In particular, to implement the noisy network, we \ufb01rst replace the \u03f5-greedy policy by a randomized action-value function. Then, the fully connected layers of the value network are parameterized as a noisy network, where the parameters are drawn from the noisy network parameter distribution after every replay step. For replay, the current noisy network parameter sample is held \ufb01xed across the batch. Since the DQL takes one step of optimization for every action step, the noisy network parameters are re-sampled before every action. After that, the loss function can be updated as follows: L = E \ufffd E(s,a,r,s\u2032)\u223cD \ufffd r+\u03b3 max a\u2032\u2208A \u02c6Q(s\u2032, a\u2032, \u03f5\u2032; \u03b8\u2032)\u2212Q(s, a, \u03f5; \u03b8) \ufffd\ufffd , (11) where the outer and inner expectations are with respect to distributions of the noise variables \u03f5 and \u03f5\u2032 for the noisy value functions \u02c6Q(s\u2032, a\u2032, \u03f5\u2032; \u03b8\u2032) and Q(s, a, \u03f5; \u03b8), respectively. Through experimental results, the authors demonstrate that by adding the Gaussian noise layer to the DNN, the performance of conventional DQL [16], dueling DQL [25], and asynchronous DQL [26] can be signi\ufb01cantly improved for a wide range of Atari games. However, the impact of noise to the performance of the deep DQL algorithms is still under debating in the literature, and thus analysis on the impact of noise layer requires further investigations. 7) Rainbow Deep Q-learning: In [30], the authors propose a solution which integrates all advantages of seven aforementioned solutions (including DQL) into a single learning agent, called Rainbow DQL. In particular, this algorithm \ufb01rst de\ufb01nes 9 TABLE II: Performance comparison among DQL algorithms DQL Algorithms No Operations Human Starts Publish Developer DQL 79% 68% Nature 2015 [16] Google DeepMind DDQL 117% 110% AAAI 2016 [21] Google DeepMind Prioritized DDQL 140% 128% ICLR 2015 [24] Google DeepMind Dueling DDQL 151% 117% ICML 2016 [25] Google DeepMind Asynchronous DQL 116% ICML 2016 [26] Google DeepMind Distributional DQL 164% 125% ICML 2017 [28] Google DeepMind Noisy Nets DQL 118% 102% ICLR 2018 [29] Google DeepMind Rainbow 223% 153% AAAI 2018 [30] Google DeepMind the loss function based on the asynchronous multi-step and distributional DQL. Then, the authors combine the multi-step distributional loss with double Q-learning by using the greedy action in st+n selected according to the Q-network as the bootstrap action a\u2217 t+n, and evaluate the action by using the target network. In standard proportional prioritized replay [24] technique, the absolute TD-error is used to prioritize the transitions. Here, TD-error at a time slot is the error in the estimate made at the time slot. However, in the proposed Rainbow DQL algorithm, all distributional Rainbow variants prioritize transitions by the Kullbeck-Leibler (KL) loss because this loss may be more robust to noisy stochastic environment. Alternatively, the dueling architecture of DNNs is presented in [25]. Finally, the Noisy Net layer [30] is used to replace all linear layers in order to reduce the number of independent noise variables. Through simulation, the authors show that this is the most advanced technique which outperforms almost all current DQL algorithms in the literature over 57 Atari 2600 games. In Table II, we summarize the DQL algorithms and their performance under the parameter settings used in [30]. As observed in Table II, all of the DQL algorithms have been developed by Google DeepMind based on the original work in [16]. So far, through experimental results on Atari 2600 games, the Rainbow DQL presents very impressive results over all other DQL algorithms. However, more experiments need to be further conducted in different domains to con\ufb01rm the real ef\ufb01ciency of the Rainbow DQL algorithm. F. Deep Q-Learning for Extensions of MDPs 1) Deep Deterministic Policy Gradient Q-Learning for Continuous Action: Although DQL algorithm can solve problems with high-dimensional state spaces, it can only handle discrete and low-dimensional action spaces. However, systems in many applications have continuous, i.e., real values, and high dimensional action spaces. The DQL algorithms cannot be straightforwardly applied to continuous actions since they rely on choosing the best action that maximizes the Q-value function. In particular, a full search in a continuous action space to \ufb01nd the optimal action is often infeasible. In [31], the authors introduce a model-free off-policy actorcritic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces. The key idea is based on the deterministic policy gradient (DPG) algorithm proposed in [32]. In particular, the DPG algorithm maintains a parameterized actor function \u00b5(s; \u03b8\u00b5) with parameter vector \u03b8 which speci\ufb01es the current policy by deterministically mapping states to a speci\ufb01c action. The critic Q(s, a) is learned by using the Bellman equation as in Q-learning. The actor is updated by applying the chain rule to the expected return from the start distribution J with respect to the actor parameters as follows: \u2207\u03b8\u00b5J \u2248 Est\u223c\u03c1\u03b2 \ufffd \u2207\u03b8\u00b5Q(s, a; \u03b8Q)|s=st,a=\u00b5(st|\u03b8\u00b5) \ufffd \u2248 Est\u223c\u03c1\u03b2 \ufffd \u2207aQ(s, a; \u03b8Q)|s=st,a=\u00b5(st)\u2207\u03b8\u00b5\u00b5(s; \u03b8\u00b5)|s=st \ufffd . (12) Based on this update rule, the authors then introduce Deep DPG (DDPG) algorithm which can learn competitive policies by using low-dimensional observations (e.g. cartesian coordinates or joint angles) under the same hyper-parameters and network structure. The detail of the DDPG algorithm is presented in 3. The algorithm makes a copy of the actor and critic networks Q\u2032(s, a; \u03b8Q\u2032) and \u00b5\u2032(s; \u03b8\u00b5\u2032), respectively, to calculate the target values. The weights of these target networks are then updated with slowly tracking on the learned networks, i.e., \u03b8\u2032 \u2190 \u03c4\u03b8 + (1 \u2212 \u03c4)\u03b8\u2032 with \u03c4 \u226a 1. This means that the target values are constrained to change slowly, greatly improving the stability of learning. Note that a major challenge of learning in continuous action spaces is exploration. Therefore, in Algorithm 3, an exploration policy \u00b5\u2032 is constructed by adding noise sampled from a noise process N to the actor policy. 2) Deep Recurrent Q-Learning for POMDPs: To tackle problems with partially observable environments by deep reinforcement learning, the authors in [33] propose a framework called Deep Recurrent Q-Learning (DRQN) in which an LSTM layer was used to replace the \ufb01rst post-convolutional fully-connected layer of the conventional DQN. The recurrent structure is able to integrate an arbitrarily long history to better estimate the current state instead of utilizing a \ufb01xedlength history as in DQNs. Thus, DRQNs estimate the function Q(ot, ht\u22121; \u03b8) instead of Q(st, at); \u03b8), where \u03b8 denotes the parameters of entire network, ht\u22121 denotes the output of the LSTM layer at the previous step, i.e., ht = LSTM(ht\u22121, ot). DRQN matches DQN\u2019s performance on standard MDP problems and outperforms DQN in partially observable domains. Regarding the training process, DRQN only considers the convolutional features of the observation history instead of explicitly incorporating the actions. Through the experiments, the authors demonstrate that DRQN is capable of handling partial observability, and recurrency confers bene\ufb01ts when the quality of observations changes during evaluation time. 10 Algorithm 3 DDPG algorithm 1: Randomly initialize critic network Q(s, a; \u03b8Q) and actor \u00b5(s; \u03b8\u00b5) with weights \u03b8Q and \u03b8\u00b5, respectively. 2: Initialize target network Q\u2032 and \u00b5\u2032 with weights \u03b8Q\u2032 \u2190 \u03b8Q, and \u03b8\u00b5\u2032 \u2190 \u03b8\u00b5, respectively. 3: Initialize replay memory D. 4: for episode=1 to M do 5: Initialize a random process N for action exploration 6: Receive initial observation state s1 7: for t=1 to T do 8: Select action at = \u00b5(st; \u03b8\u00b5) + Nt according to the current policy and exploration noise. 9: Execute action at and observe reward rt and new state st+1. 10: Store transition (st, at, rt, st+1) in D. 11: Sample a random mini-batch of N transitions (si, ai, ri, si+1) from D. 12: Set yi = ri + \u03b3Q \u2032\ufffd si+1, \u00b5 \u2032(si+1; \u03b8\u00b5\u2032); \u03b8Q\u2032\ufffd . 13: Update critic by minimizing the loss: L = 1 N \ufffd i(yi \u2212 Q \ufffd si, ai; \u03b8Q) \ufffd2 14: Update the actor policy by using the sampled policy gradient: \u2207\u03b8\u00b5J \u2248 1 N \ufffd i \u2207aQ(s, a; \u03b8Q)|s=si,a=\u00b5(si) \u2207\u03b8\u00b5\u00b5(s|\u03b8\u00b5)|s=si 15: Update the target networks: \u03b8Q\u2032 \u2190 \u03c4\u03b8Q + (1 \u2212 \u03c4)\u03b8Q\u2032 \u03b8\u00b5\u2032 \u2190 \u03c4\u03b8\u00b5 + (1 \u2212 \u03c4)\u03b8\u00b5\u2032 16: end for 17: end for 3) Deep SARSA Learning: In [34], the authors introduce a DQL technique based on SARSA learning to help the agent determine optimal policies in an online fashion. As shown in Algorithm 4, given the current state s, a CNN is used to obtain the current state-action value Q(s, a). Then, the current action a is selected by the \u03f5-greedy algorithm. After that, the immediate reward r and the next state s\u2032 can be observed. In order to estimate the current Q(s, a), the next state-action value Q(s\u2032, a\u2032) is obtained. Here, when the next state s\u2032 is used as the input of the CNN, Q(s\u2032, a\u2032) can be obtained as the output. Then, a label vector related to Q(s, a) is de\ufb01ned as Q(s\u2032, a\u2032) which represents the target vector. The two vectors only have one different component, i.e., r + \u03b3Q(s\u2032, a\u2032) \u2192 Q(s, a). It should be noted that during the training phase, the next action a\u2032 for estimating the current state-action value is never greedy. On the contrary, there is a small probability that a random action is chosen for exploration. 4) Deep Q-Learning for Markov Games: In [35], the authors introduce the general notion of sequential prisoner\u2019s dilemma (SPD) to model real world prisoner\u2019s dilemma (PD) problems. Since SPD is more complicated than PD, existing approaches addressing learning in matrix PD games cannot be directly applied in SPD. Thus, the authors propose a multiagent DRL approach for mutual cooperation in SDP games. The deep multi-agent reinforcement learning towards mutual cooperation consists of two phases, i.e., of\ufb02ine and online phases. The of\ufb02ine phase generates policies with varying coAlgorithm 4 Deep SARSA learning algorithm 1: Initialize data stack D with size of N 2: Initialize parameters \u03b8 of the CNN 3: for episode=1 to M do 4: Initialize state s1 and pre-process state \u03c61 = \u03c6(s1) 5: Select a1 by the \u03f5-greedy method 6: for t=1 to T do 7: Take action at, observe rt and next state st+1 8: \u03c6t+1 = \u03c6(st+1) 9: Store data (\u03c6t, at, rt, \u03c6t+1) into stack D 10: Sample data from stack D 11: Select action a\u2032 by the \u03f5-greedy method 12: if episode terminates at step j + 1 then 13: Set yj = rj 14: else 15: set yj = rj + Q(\u03c6t+1, a\u2032; \u03b8) 16: end if 17: Minimize the loss function: (yj \u2212 Q(\u03c6t, a\u2032; \u03b8))2 18: Update at \u2190 a\u2032 19: end for 20: end for operation degrees. Since the number of policies with different cooperation degrees is in\ufb01nite, it is computationally infeasible to train all the policies from scratch. To address this issue, the algorithm \ufb01rst trains representative policies using actorcritic until it converges, i.e., cooperation and defection baseline policy. Second, the algorithm synthesizes the full range of policies from the above baseline policies. Another task is to detect effectively the cooperation degree of the opponent. The algorithm divides this task into two steps. First, the algorithm trains an LSTM-based cooperation degree detection network of\ufb02ine, which will be then used for real-time detection during the online phase. In the online phase, the agent plays against the opponents by reciprocating with a policy of a slightly higher cooperation degree than that of the opponent. On one hand, intuitively the algorithm is cooperation-oriented and seeks for mutual cooperation whenever possible. On the other hand, the algorithm is also robust against sel\ufb01sh exploitation and resorts to defection strategy to avoid being exploited whenever necessary. Unlike [35] which considers a repeated normal form game with complete information, in [36], the authors introduce an application of DRL for extensive form games with imperfect information. In particular, the authors in [36] introduce Neural Fictitious Self-Play (NFSP), a DRL method for learning approximate Nash equilibria of imperfect-information games. NFSP combines FSP with neural network function approximation. An NFSP agent has two neural networks. The \ufb01rst network is trained by reinforcement learning from memorized experience of play against fellow agents. This network learns an approximate best response to the historical behaviour of other agents. The second network is trained by supervised learning from memorized experience of the agent\u2019s own behaviour. This network learns a model that averages over the agent\u2019s own historical strategies. The agent behaves 11 according to a mixture of its average strategy and best response strategy. In the NSFP, all players of the game are controlled by separate NFSP agents that learn from simultaneous play against each other, i.e., self-play. An NFSP agent interacts with its fellow agents and memorizes its experience of game transitions and its own best response behaviour in two memories, MRL and MSL. NFSP treats these memories as two distinct datasets suitable for DRL and supervised classi\ufb01cation, respectively. The agent trains a neural network, Q(s, a; \u03b8Q), to predict action values from data in MRL using off-policy reinforcement learning. The resulting network de\ufb01nes the agent\u2019s approximate best response strategy, \u03b2 = \u03f5-greedy(Q), which selects a random action with probability \u03f5 and otherwise chooses the action that maximizes the predicted action values. The agent trains a separate neural network \u03a0(s, a; \u03b8\u03a0) to imitate its own past best response behavior by using supervised classi\ufb01cation on the data in MSL. NFSP also makes use of two technical innovations in order to ensure the stability of the resulting algorithm as well as to enable simultaneous self-play learning. Through experimental results, the authors show that the NFSP can converge to approximate Nash equilibria in a small poker game. Summary: In this section, we have presented the basics of reinforcement learning, deep learning, and DQL. Furthermore, we have discussed various advanced DQL techniques and their extensions. Different DQL techniques can be used to solve different problems in different network scenarios. In the next sections, we review DQL related works for various problems in communications and networking. III. NETWORK ACCESS AND RATE CONTROL Modern networks such as IoT become more decentralized and ad-hoc in nature. In such networks, entities such as sensors and mobile users need to make independent decisions, e.g., channel and base station selections, to achieve their own goals, e.g., throughput maximization. However, this is challenging due to the dynamic and the uncertainty of network status. Learning algorithms such as DQL allow to learn and build knowledge about the networks that are used to enable the network entities to make their optimal decisions. In this section, we review the applications of DQL for the following issues: \u2022 Dynamic spectrum access: Dynamic spectrum access allows users to locally select channels to maximize their throughput. However, the users may not have full observations of the system, e.g., channel states. Thus, DQL can be used as an effective tool for dynamic spectrum access. \u2022 Joint user association and spectrum access: User association is implemented to determine which user to be assigned to which Base Station (BS). The joint user association and spectrum access problems are studied in [37] and [38]. However, the problems are typically combinatorial and non-convex which require nearly complete and accurate network information to obtain the optimal strategy. DQL is able to provide distributed solutions which can be effectively used for the problems. \u2022 Adaptive rate control: This refers to bitrate/data rate control in dynamic and unpredictable environments such as Dynamic Adaptive Streaming over HTTP (DASH). Such a system allows clients or users to independently choose video segments with different bitrates to download. The client\u2019s objective is to maximize its Quality of Experience (QoE). DQL can be adopted to effectively solve the problem instead of dynamic programming which has high complexity and demands complete information. A. Network Access This section discusses how to use DQL to solve the spectrum access and user association in networks. 1) Dynamic Spectrum Access: The authors in [39] propose a dynamic channel access scheme of a sensor based on the DQL for IoT. At each time slot, the sensor selects one of M channels for transmitting its packet. The channel state is either in low interference, i.e., successful transmission, or in high interference, i.e., transmission failure. Since the sensor only knows the channel state after selecting the channel, the sensor\u2019s optimization decision problem can be formulated as a POMDP. In particular, the action of sensor is to select one of M channels. The sensor receives a positive reward \u201c+1\u201d if the selected channel is in low interference, and a negative reward \u201c-1\u201d otherwise. The objective is to \ufb01nd an optimal policy which maximizes the sensor\u2019s the expected accumulated discounted reward over time slots. A DQN1 using FNN with experience replay [40] is then adopted to \ufb01nd the optimal policy. The input of the DQN is a state of the sensor which is the combination of actions and observations, i.e., the rewards, in the past time slots. The output includes Q-values corresponding to the actions. To balance the exploration of the current best Q-value with the exploration of the better one, the \u03f5-greedy policy is adopted for the action selection mechanism. The simulation results based on real data from [41] show that the proposed scheme can achieve the average accumulated reward close to the myopic policy [42] without a full knowledge of the system. [39] can be considered to be a pioneer work using the DQL for the channel access. However, the DQL keeps following the learned policy over time slots and stops learning a suitable policy. Actual IoT environments are dynamic, and the DQN in the DQL needs to be re-trained. An adaptive DQL scheme is proposed in [43] which evaluates the accumulated reward of the current policy for every period. When the reward is reduced by a given threshold, the DQN is re-trained to \ufb01nd a new good policy. The simulation results [43] show that when the states of the channels change, the adaptive DQL scheme can detect the change and start re-learning to obtain the high reward. The models in [39] and [43] are constrained to only one sensor. Consider a multi-sensor scenario, the authors in [44] address the joint channel selection and packet forwarding using the DQL. The model is shown in Fig. 5 in which one sensor as a relay forwards packets received from its neighboring sensors to the sink. The sensor is equipped with 1Remind that DQN is the core of the DQL algorithms. 12    Relay (agent) Sensor (neighbor) Sink Channels Sensor (neighbor) Buffer Fig. 5: Joint channel selection and packet forwarding in IoT. a buffer to store the received packets. At each time slot, the sensor selects a set of channels for the packet forwarding so as to maximize its utility, i.e., the ratio of the number of transmitted packets to the transmit power. Similar to [39], the sensor\u2019s problem can be formulated as an MDP. The action is to select a set of channels, the number of packets transmitted on the channels, and a modulation mode. To avoid packet loss, the state is de\ufb01ned as the combination of the buffer state and channel state. The MDP is then solved by the DQL in which the input is the state and the output is the action selection. The DQL uses the stacked autoencoder to reduce the massive calculation and storage in the Q-learning phase. The sensor\u2019s utility function is proved to be bounded which can guarantee the convergence of the algorithm. As shown in the simulation results, the proposed scheme can converge after a certain number of iterations. Also, the proposed scheme signi\ufb01cantly improves the system utility compared with the random action selection scheme. However, as the packet arrival rate increases, the system utility of the proposed scheme decreases since the sensor needs to consume more power to transmit all packets. Consuming more power leads to poor sensor\u2019s performance due to its energy constraint, i.e., a shorter IoT system lifetime. The channel access problem in the energy harvesting-enabled IoT system is investigated in [45]. The model consists of one BS and energy harvesting-based sensors. The BS as a controller allocates channels to the sensors. However, the uncertainty of ambient energy availability at the sensors may make the channel allocation inef\ufb01cient. For example, the channel allocated to the sensor with low available energy may not be fully utilized since the sensor cannot communicate later. Therefore, the BS\u2019s problem is to predict the sensors\u2019 battery states and select sensors for the channel access so as to maximize the total rate. Since the sensors are distributed randomly over a geographical area, the complete statistical knowledge of the system dynamics, e.g., the battery states and channel states, may not be available. Thus, the DQL is used to solve the problem of the BS, i.e., the agent. The DQL uses a DQN consisting of two LSTM-based neural network layers. The \ufb01rst layer generates the predicted battery states of sensors, and the second layer uses the predicted states along with Channel State Information (CSI) to determine the channel access policy. The state space consists of (i) channel access scheduling history, (ii) the history of predicted battery information, (iii) the history of the true battery information, and (iv) the current CSI of the sensors. The action space contains all sets of sensors to be selected for the channel access, and the reward is the difference between the total rate and the prediction error. As shown in the simulation results, the proposed scheme outperforms the myopic policy [42] in terms of total rate. Moreover, the battery prediction error obtained from the proposed scheme is close to zero. The above schemes, e.g., [39] and [45], focus on the rate maximization. In IoT systems such as Vehicle-to-Vehicle (V2V) communications, latency also needs to be considered due to the mobility of V2V transmitters/receivers and vital applications in the traf\ufb01c safety. One of the problems of each V2V transmitter is to select a channel and a transmit power level to maximize its capacity under a latency constraint. Given the decentralized network, a DQN is adopted to make optimal decisions as proposed in [46]. The model consists of V2V transmitters, i.e., agents, which share a set of channels. The actions of each V2V transmitter include choosing channels and transmit power levels. The reward is a function of the V2V transmitter\u2019s capacity and latency. The state observed by the V2V transmitter consists of (i) the instantaneous CSI of the corresponding V2V link, (ii) the interference to the V2V link in the previous time slot, (iii) the channels selected by the V2V transmitter\u2019 neighbors in the previous time slot, and (iv) the remaining time to meet the latency constraint. The state is also an input of the DQN. The output includes Q-values corresponding to the actions. As shown in the simulation results, by dynamically adjusting the power and channel selection when V2V links are likely to violate the latency constraint, the proposed scheme has more V2V transmitters meeting the latency constraint compared with the random channel allocation. To reduce spectrum cost, the above IoT systems often use unlicensed channels. However, this may cause the interference to existing networks, e.g., WLANs. The authors in [47] propose to use the DQN to jointly address the dynamic channel access and interference management. The model consists of Small Base Stations (SBSs) which share unlicensed channels in an LTE network. At each time slot, the SBS selects one of channels for transmitting its packet. However, there may be WLAN traf\ufb01cs on the selected channel, and thus the SBS accesses the selected channel with a probability. The actions of the SBS include pairs of channel selection and channel access probability. The problem of the SBS is to determine an action vector so as to maximize its total throughput, i.e., its utility, over all channels and time slots. The resource allocation problem can be formulated as a non-cooperative game, and the DQN using LSTM can be adopted to solve the game. The input of the DQN is the history traf\ufb01c of the SBSs and the WLAN on the channels. The output includes predicted action vectors of the SBSs. The utility function of each SBS is proved to be convex, and thus the DQN-based algorithm converges to a Nash equilibrium of the game. The simulation results based on real traf\ufb01c data from [48] show that the proposed scheme can improve the average throughput up to 28% compared with the standard Q-learning [15]. Moreover, deploying more SBSs in the LTE network does not allow more airtime fraction for the network. This implies that the proposed scheme can avoid causing performance degradation to the WLAN. However, the 13 proposed scheme requires synchronization between the SBSs and the WLAN which is challenging in real networks. In the same cellular network context, the authors in [22] address the dynamic spectrum access problem for multiple users sharing K channels. At a time slot, the user selects a channel with a certain attempt probability or chooses not to transmit at all. The state is the history of the user\u2019s actions and its local observations, and the user\u2019s strategy is mapping from the history to an attempt probability. The problem of the user is to \ufb01nd a vector of the strategies, i.e., the policy, over time slots to maximize its expected accumulated discounted data rate of the user. The above problem is solved by training a DQN. The input of the DQN includes past actions and the corresponding observations. The output includes estimated Q-values of the actions. To avoid the overestimation in the Q-learning, the DDQN [20] is used. Moreover, the dueling DQN [49] is employed to improve the estimated Q-value. The DQN is then of\ufb02ine trained at a base station. Similar to [47], the multichannel random access is modeled as a non-cooperative game. As proved in [22], the game has a subgame perfect equilibrium. Note that some users can keep increasing their attempt probability to increase their rates. This makes the equilibrium point inef\ufb01cient, and thus the strategy space of the users is restricted to avoid the situation. The simulation results show that the proposed scheme can achieve twice the channel throughput compared with the slotted-Aloha [50]. The reason is that in the proposed scheme, each user only learns from its local observation without an online coordination or carrier sensing. However, the proposed scheme requires the central unit which may raise the message exchanges as the training is frequently updated. In the aforementioned models, the number of users is \ufb01xed in all time slots, and the arrival of new users is not considered. The authors in [51] address the channel allocation to new arrival users in a multibeam satellite system. The multibeam satellite system generates a geographical footprint subdivided into multiple beams which provide services to ground User Terminals (UTs). The system has a set of channels. If there exist available channels, the system allocates a channel to the new arrived UT, i.e., the new service is satis\ufb01ed. Otherwise, the service is blocked. The system\u2019s problem is to \ufb01nd a channel allocation decision to minimize the total service blocking probability of the new UT over time slots without causing the interference to the current UTs. The system\u2019s problem can be viewed as a temporal correlated sequential decision-making optimization problem which is effectively solved by the DQN. Here, the satellite system is the agent. The action is an index indicating which channel is allocated to the new arrived UT. The reward is positive when the new service is satis\ufb01ed and is negative when the service is blocked. The state includes the set of current UTs, the current channel allocation matrix, and the new arrived UT. Note that the state has the spatial correlation feature due to the co-channel interference, and thus it can be represented in an image-like fashion, i.e., an image tensor. Therefore, the DQN adopts the CNN to extract useful features of the state. The simulation results show that the proposed DQN algorithm converges after a certain number of training steps. Also, by allocating available channels to the new arrived UTs, the proposed scheme can improve the system traf\ufb01c up to 24.4% compared with the \ufb01xed channel allocation scheme. However, as the number of current UTs increases, the number of available channels is low or even zero. Therefore, the dynamic channel allocation decisions of the proposed scheme become meaningless, and the performance difference between the two schemes becomes insigni\ufb01cant. For the future work, a joint channel and power allocation algorithm based on the DQL can be investigated. 2) Joint User Association and Spectrum Access: The joint user association and spectrum access problems are typically non-convex. DQL is able to provide distributed solutions, and thus it can be effectively used to solve the problems without requiring complete and accurate network information. The authors in [23] consider a HetNet which consists of multiple users and BSs including macro base stations and femto base stations. The BSs share a set of orthogonal channels, and the users are randomly located in the network. The problem of each user is to select one BS and a channel to maximize its data rate while guaranteeing that the Signalto-Interference-plus-Noise Ratio (SINR) of the user is higher than a minimum Qualtiy of Service (QoS) requirement. The DQL is adopted to solve the problem in which each user is an agent, and its state is a vector including QoS states of all users, i.e., the global state. Here, the QoS state of the user refers to whether its SINR exceeds the minimum QoS requirement or not. At each time slot, the user takes an action. If the QoS is satis\ufb01ed, the user receives utility as its immediate reward. Otherwise, it receives a negative reward, i.e., an action selection cost. Note that the cumulative reward of one user depends on actions of other users, then the user\u2019s problem can be de\ufb01ned as an MDP. Similar to [22], the DDQN and the dueling DQN are used to learn the optimal policy, i.e., the joint BS and channel selections, for the user to maximize its cumulative reward. The simulation results from [23] show that the proposed scheme outperforms the Q-learning implemented in [15] in terms of convergence speed and system capacity. The scheme proposed in [23] is considered to be the \ufb01rst work using the DQL for the joint user association and spectrum access problem. Inspired by this work, the authors in [52] propose to use the DQL for a joint user association, spectrum access, and content caching problem. The network model is an LTE network which consists of UAVs serving ground users. The UAVs are equipped with storage units and can act as cached-enabled LTE-BSs. The UAVs are able to access both licensed and unlicensed bands in the network. The UAVs are controlled by a cloud-based server, and the transmissions from the cloud to the UAVs are implemented by using the licensed cellular band. The problem of each UAV is to determine (i) its optimal user association, (ii) the bandwidth allocation indicators on the licensed band, (iii) the time slot indicators on the unlicensed band, and (iv) a set of popular contents that the users can request to maximize the number of users with stable queue, i.e., users satis\ufb01ed with content transmission delay. The UAV\u2019s problem is combinatorial and non-convex, and 14 Client (agent) (Video segment & bitrate) (Video segment) Buffer Content delivery  network Server Video  segments Server Fig. 6: A dynamic adaptive streaming system based on HTTP standard. the DQL can be used to solve it. The UAVs do not know the users\u2019 content requests, and thus the Liquid State Machine approach (LSM) [53] is adopted to predict the content request distribution of the users and to perform resource allocation. In particular, predicting the content request distribution is implemented at the cloud based on an LSM-based prediction algorithm. Then, given the request distributions, each UAV as an agent uses an LSM-based learning algorithm to \ufb01nd its optimal users association. Speci\ufb01cally, the input of the LSMbased learning algorithm consists of actions, i.e., UAV-user association schemes, that other UAVs take, and the output includes the expected numbers of users with stable queues corresponding to actions that the UAV can take. After the user association is done, the optimal content caching is determined based on the results of [54, Theorem 2], and the optimal spectrum allocation is done by using linear programming. Based on the Gordon\u2019s Theorem [55], the proposed DQL is proved to converge with probability one. The simulation results using content request data from [56] show that the proposed DQL can converge in around 400 iterations. Compared with the Qlearning, the proposed DQN improves the convergence time up to 33% . Moreover, the proposed DQL signi\ufb01cantly improves the number of users with stable queues up to 50% compared with the Q-learning without cache. In fact, energy ef\ufb01ciency is also important for the UAVs, and thus applying the DQL for a joint user association, spectrum access, and power allocation problem needs to be investigated. B. Adaptive Rate Control Dynamic Adaptive Streaming over HTTP (DASH) becomes the dominant standard for video streaming [57]. DASH is able to leverage existing content delivery network infrastructure and is compatible with a multitude of client-side applications. A general DASH system is shown in Fig. 6 in which the videos are stored in servers as multiple segments, i.e., chunks. Each segment is encoded at different compression levels to generate representations with different bitrates, i.e., different video visual quality. At each time slot, the client chooses a representation, i.e., a segment with a certain bitrate, to download. The client\u2019s problem is to \ufb01nd an optimal policy which maximizes its QoE such as maximizing average bitrate and minimizing rebuffering, i.e., the time which the video playout freezes. As presented in [58], the above problem can be modeled as an MDP in which the agent is the client and the action is choosing a representation to download. To maximize the QoE, the reward is de\ufb01ned as a function of (i) visual quality of the video, (ii) video quality stability, (iii) rebuffering event, and (iv) buffer state. Given the reward formulation, the state of the client should include (i) the video quality of the last downloaded segment, (ii) the current buffer state, (iii) the rebuffering time, and (iv) the channel capacities experienced during downloading of segments in the past time slots. The MDP can be solved by using dynamic programming, but the computational complexity rapidly becomes unmanageable as the size of the problem increases. Thus, the authors in [58] adopt the DQL to solve the problem. Similar to [45], the LSTM networks are used in which the input is the state of the client, and the output includes Q-values corresponding to the client\u2019s possible actions. To improve the performance of the standard LSTM, peephole connections are added into the LSTM networks. The simulation results based on dataset from [59] show that the proposed DQL algorithm can converge much faster than Q-learning. Moreover, the proposed DQL improves the video quality and reduces the rebuffering since it is able to dynamically manage the buffer by considering the buffer state and channel capacity. The network model and the optimization problem in [58] are also found in [60]. However, different from [58], the authors in [60] adopt the Asynchronous Advantage ActorCritic (A3C) method [26] for the DQL to further enhance and speed up the training. As presented in Section II-F1, A3C includes two neural networks, namely, actor network and critic network. The actor network is to choose bitrates for the client, and the critic network helps train the actor network. For the actor network, the input is the client\u2019s state, and the output is a policy, i.e., a probability distribution over possible actions given states that the client can take. Here, the action is choosing the next representation, i.e., the next segment with a certain bitrate, to download. For the critic network, the input is the client\u2019s state, and the output is the expected total reward when following the policy obtained from the actor network. The simulation results based on the mobile dataset from [61] show that the proposed DQL can improve the average QoE up to 25% compared with the bitrate control scheme [62]. Also, by having suf\ufb01cient buffer to handle the network\u2019s throughput \ufb02uctuations, the proposed DQL reduces the rebuffering around 32.8% compared with the baseline scheme. In practice, the DQL algorithm proposed in [60] can be easily deployed in a multi-client network since A3C is able to support parallel training for multiple agents. Accordingly, each client, i.e., an agent, is con\ufb01gured to observe its reward. Then, the client sends a tuple including its state, action, and reward to a server. The server uses the actor-critic algorithm to update its actor network model. The server then pushes the newest model to the agent. This update process can happen asynchronously among all agents which improves quality and speeds up the training. Although the parallel training scheme may incur a Round-Trip Time (RTT) between the clients and the server, the simulation results in [60] show that the RTT between the clients and the server reduces the average QoE by only 3.5%. The performance degradation is small, and thus the proposed DQL can be implemented in real network systems. In [58] and [60], the input of the DQL, i.e., the client\u2019s 15 state, includes the video quality of the last downloaded video segment. The video segment is raw which may cause \u201cstate explosion\u201d to the state space [63]. To reduce the state space and to improve the QoE, the authors in [63] propose to use a video quality prediction network. The prediction network extracts useful features from the raw video segments using CNN and RNN. Then, the output of the prediction network, i.e., the predicted video quality, is used as one of the inputs of the DQL which is proposed in [60]. The simulation results based on the broadband dataset from [64] show that the proposed DQL can improve the average QoE up to 25% compared with the Google Hangout, i.e., a communication platform developed by Google. Moreover, the proposed DQL can reduce the average latency of video transmission around 45% due to the small state space. Apart from the DASH systems, the DQL can be effectively used for the rate control in High Volume Flexible Time (HVFT) applications. HVFT applications use cellular networks to deliver IoT traf\ufb01c. The HVFT applications have a large volume of traf\ufb01c, and the traf\ufb01c scheduling, e.g., data rate control, in the HVFT applications is necessary. One common approach is to assign static priority classes per traf\ufb01c type, and then traf\ufb01c scheduling is based on its priority class. However, such an approach does not evolve to accommodate new traf\ufb01c classes. Thus, learning methods such as DQL should be used to provide adaptive rate control mechanisms as proposed in [65]. The network model is a single cell including one BS as a central controller and multiple mobile users. The problem at the BS is to \ufb01nd a proper policy, i.e., data rate for the users, to maximize the amount of transmitted HVFT traf\ufb01c while minimizing performance degradation to existing data traf\ufb01cs. It is shown in [65] that the problem can be formulated as an MDP. The agent is the BS, and the state includes the current network state and the useful features extracted from network states in the past time slots. The network state at a time slot includes (i) the congestion metric, i.e., the cell\u2019s traf\ufb01c load, at the time slot, (ii) the total number of network connections, and (iii) the cell ef\ufb01ciency, i.e., the cell quality. The action that the BS takes is a combination of the traf\ufb01c rate for the users. To achieve the BS\u2019 objective, the reward is de\ufb01ned as a function of (i) the sum of HVFT traf\ufb01c, (ii) traf\ufb01c loss to existing applications due to the presence of the HVFT traf\ufb01c, and (iii) the amount of bytes served below desired minimum throughput. The DQL using the actor and critic networks with LSTM is then adopted. By using the real network data collected in Melbourne, the simulation results show that the proposed DQL increases the HVFT traf\ufb01c up to 2 times compared with the heuristic control scheme. However, how the proposed scheme reduces the traf\ufb01c loss is not shown. In the aforementioned approaches, the maximum number of objectives is constrained, e.g., to 3 in [66]. The authors in [67] show that the DQL can be used for the rate control to achieve multiple objectives in complex communication systems. The network model is a future space communication system which is expected to operate in unpredictable environments, e.g., orbital dynamics, atmospheric and space weather, and dynamic channels. In the system, the transmitter needs to be con\ufb01gured with several transmit parameters, e.g., symbol rate and encoding rate, to achieve multiple con\ufb02ict objectives, e.g., low Bit Error Rate (BER), throughput improvement, power and spectral ef\ufb01ciency. The adaptive coding and modulation schemes, i.e., [68], can be used. However, the methods allow to achieve only limited number objectives. Learning algorithms such as the DQL can be thus used. The agent is the transmitter in the system. The action is a combination of (i) symbol rate, (ii) energy per symbol, (iii) modulation mode, (iv) number of bits per symbol, and (v) encoding rate. The objective is to maximize the system performance. Thus, the reward is de\ufb01ned as a \ufb01tness function of performance parameters including (i) BER estimated at the receiver, (ii) throughput, (iii) spectral ef\ufb01ciency, (iv) power consumption, and (v) transmit power ef\ufb01ciency. The state is the system performance measured by the transmitter, and thus the state is the reward. To achieve multiple objectives, the DQL is implemented by using a set of multiple neural networks in parallel. The input of the DQL is the current state and the channel conditions, and the output is the predicted action. The neural networks are trained by using the Levenberg-Marquardt backpropagation algorithm [69]. The simulation results show that the proposed DQL can achieve the \ufb01tness score, i.e., the weighted sum of different objectives, close to the ideal, i.e., the exhaustive search approach. This implies that the DQL is able to select near-optimal actions and learn the relationship between rewards and actions given dynamic channel conditions. Summary: This section reviews applications of DQL for the dynamic network access and adaptive rate control. The reviewed approaches are summarized along with the references in Table III. We observe that the problems are mostly modeled as an MDP. Moreover, DQL approaches for the IoT and DASH systems receive more attentions than other networks. Future networks, e.g., 5G networks, involve multiple network entities with multiple con\ufb02icting objectives, e.g., provider\u2019s revenue versus users\u2019 utility maximization. This poses a number of challenges to the traditional resource management mechanisms that deserve in-depth investigation. In the next section, we review the adoption of DQL for the emerging services, i.e., of\ufb02oading and caching. IV. CACHING AND OFFLOADING As one of the key features of information-centric networking, in-network caching can ef\ufb01ciently reduce duplicated content transmissions. The studies on wireless caching has shown that access delays, energy consumption, and the total amount of traf\ufb01c can be reduced signi\ufb01cantly by caching contents in wireless devices. Big data analytics [70] also demonstrate that with limited cache size, proactive caching at network edge nodes can achieve 100% user satisfaction while of\ufb02oading 98% of the backhaul traf\ufb01c. Joint content caching and of\ufb02oading can address the gap between the mobile users\u2019 large data demands and the limited capacities in data storage and processing. This motivates the study on Mobile Edge Computing (MEC). By deploying both computational resources and caching capabilities close to end users, MEC signi\ufb01cantly improves energy ef\ufb01ciency and QoS for applications that require intensive computations and low 16 TABLE III: A summary of approaches using DQL for network access and adaptive rate control. ISSUES REF. MODEL LEARNING ALGORITHMS AGENT STATES ACTIONS REWARDS NETWORKS Network access [39] POMDP DQN using FNN Sensor Past channel selections and observations Channel selection Score +1 or -1 IoT [44] MDP DQN using FNN Sensor Current buffer state and channel state Channel, packets, and modulation mode selection Ratio of number of transmitted packets to transmit power IoT [45] MDP DQN with LSTM Base station Channel access history, predicted and true battery information history, and current CSI Sensor selection for channel access Total rate and prediction error IoT [46] MDP DQN with LSTM V2V transmitter Current CSI, past interference, past channel selections, and remaining time to meet the latency constraints Channel and transmit power selection Capacity and latency IoT [47] Game DQN with LSTM Small base station Traf\ufb01c history of small base stations and the WLAN Channel selection and channel access probability Throughput LTE network [22] Game DDQN and dueling DQN Mobile user Past channel selections and observations Channel selection Data rate CRN [51] MDP DQN with CNN Satellite system Current user terminals, channel allocation matrix, and the new arrival user Channel selection Score +1 or -1 Satellite system [23] MDP DDQN and dueling DQN Mobile user QoS states Base station and channel selection Utility HetNet [52] Game DQN with LSM UAV Content request distribution Base station selection Users with stable queues LTE network Rate control [58] MDP DQN with LSTM and peephole connections Client Last segment quality, current buffer state, rebuffering time, and channel capacities Bitrate selection for segment Video quality, rebuffering even, and buffer state DASH system [60] MDP DQN with A3C Client Last segment quality, current buffer state, rebuffering time, and channel capacities Bitrate selection for segment Video quality, rebuffering even, and buffer state DASH system [63] MDP DQN with CNN and RNN Client Predicted video quality, current buffer state, rebuffering time, and channel capacities Bitrate selection for segment Video quality, rebuffering even, and buffer state DASH system [65] MDP DQN using A3C and LSTM Base station Congestion metric, current network connections, and cell ef\ufb01ciency Traf\ufb01c rate decisions for mobile users HVFT traf\ufb01c, traf\ufb01c loss to existing applications, and the amount of served bytes HVFT application [67] MDP DQN using FNN Base station Measurements of BER, throughput, spectral ef\ufb01ciency, power consumption, and transmit power ef\ufb01ciency Symbol rate, energy per symbol, modulation mode, number of bits per symbol, and encoding rate Same as the state Space communication system latency. A uni\ufb01ed study on caching, of\ufb02oading, networking, and transmission control in MEC scenarios involves very complicated system analysis because of strong couplings among mobile users with heterogeneities in application demand, QoS provisioning, mobility pattern, radio access interface, and wireless resources. A learning-based and model-free approach becomes a promising candidate to manage huge state space and optimization variables, especially by using DNNs. In this section, we review the modeling and optimization of caching and of\ufb02oading policies in wireless networks by leveraging the DRL framework. A. Wireless Proactive Caching Wireless proactive caching has attracted great attentions from both academia and industry. Statistically, a few popular contents are usually requested by many users during a short time span, which accounts for most of the traf\ufb01c load. Therefore, proactively caching popular contents can avoid the heavy traf\ufb01c burden of the backhaul links. In particular, this technique aims at pre-caching the contents from the remote content servers at the edge devices or BSs that are close to the end users. If the requested contents are already cached locally, the BS can directly serve the end users with small delay. Otherwise, the BS requests these contents from the original content server and updates the local cache based on the caching policy, which is one of the main design problem for wireless proactive caching. 1) QoS-Aware Caching: Content popularity is the key factor used to solve the content caching problem. With a large number of contents and their time-varying popularities, DQL is an attractive strategy to tackle this problem with high-dimensional state and action spaces. The authors in [70] present a DQL scheme to improve the caching performance. The system model consists of a single BS with a \ufb01xed cache 17 size. For each request, the BS as an agent makes a decision on whether or not to store the currently requested content in the cache. If the new content is kept, the BS determines which local content will be replaced. The state is the feature space of the cached contents and the currently requested content. The feature space consists of the total number of requests for each content in a speci\ufb01c short-, medium-, and long-term. There are two types of actions: (i) to \ufb01nd a pair of contents and exchange the cache states of the two contents and (ii) to keep the cache states of the contents unchanged. The aim of the BS is to maximize the long-term cache hit rate, i.e., reward. The DQL scheme in [70] trains the policy by using the DDPG method [71] and employs Wolpertinger architecture [72] to reduce the size of the action space and avoid missing an optimal policy. The Wolpertinger architecture consists of three main parts: an actor network, K-Nearest Neighbors (K-NN), and a critic network. The actor network is to avoid a large action space. The critic network is to correct the decision made by the actor network. The DDPG method is applied to update both critic and actor networks. K-NN can help to explore a set of actions to avoid poor decisions. The actor and critic networks are then implemented by using FNNs. The simulation results show that the proposed DQL scheme outperforms the \ufb01rst-in \ufb01rst-out scheme in terms of long-term cache hit rate. Maximizing the long-term cache hit rate in [70] implies that the cache stores the most popular contents. In a dynamic environment, contents stored in a cache have to be replaced according to the users\u2019 dynamic requests. An optimization of the placement or replacement of cached contents is studied in [73] by a deep learning method. The optimization algorithm is trained by a DNN in advance and then used for realtime caching or scheduling with minimum delay. The authors in [74] propose an optimal caching policy to learn the cache expiration times, i.e., Time-To-Live (TTL), for dynamically changing requests in content delivery networks. The system includes a cloud database server and multiple mobile devices that can issue queries and update entries in a single database. The query results can be cached for a speci\ufb01ed time interval at server-controlled caches. All cached queries will become invalid if one of the cached records has been updated. A large TTL will strain cache capacities while a small TTL increases latencies signi\ufb01cantly if the database server is physically remote. Unlike the DDPG approach used in [70], the authors in [74] propose to utilize Normalized Advantage Functions (NAFs) for continuous DQL scheme to learn optimal cache expiration duration. The key problem in continuous DQL is to select an action maximizing the Q-function, while avoiding performing a costly numerical optimization at each step. The use of NAFs obviates a second actor network that needs to be trained separately. Instead, a single neural network is used to output both a value function and an advantage term. The DQL agent at the cloud database uses an encoding of a query itself and the query miss rates as the system states, which allows for an easier generalization. The system reward is linearly proportional to the current load, i.e., the number of cached queries divided by the total capacity. This reward function can encourage longer TTLs when fewer queries are cached, and shorter TTLs when the load is close to the system capacity. Considering incomplete measurements for rewards and next-states at runtime, the authors introduce the Delayed Experience Injection (DEI) approach that allows the DQL agent to keep track of incomplete transitions when measurements are not immediately available. The authors evaluate the learning algorithm by Yahoo! cloud serving benchmark with customized web workloads [75]. The simulation results verify that the learning approach based on NAFs and DEI outperforms a statistical estimator. 2) Joint Caching and Transmission Control: The caching policies determine where to store and retrieve the requested content ef\ufb01ciently, e.g., by learning the contents\u2019 popularities [70] and cache expiration time [74]. Another important aspect of caching design is the transmission control of the content delivery from caches to end users, especially for wireless systems with dynamic channel conditions. To avoid mutual interference in multi-user wireless networks, the transmission control decides which cached contents can be transmitted concurrently as well as the most appropriate control parameters, e.g., transmit power, precoding, data rate, and channel allocation. Hence, the joint design of caching and transmission control is required to enable ef\ufb01cient content delivery in multi-user wireless networks. The authors in [76]\u2013[78] propose a DQL framework to address the joint caching and interference alignment to tackle mutual interference in multi-user wireless networks. The authors consider an MIMO system with limited backhaul capacity and the caches at the transmitter. The precoding design for interference alignment requires the global CSI at each transmitter. A central scheduler is responsible for collecting CSI and cache status from each user via the backhaul, scheduling the users\u2019 transmission, and optimizing the resource allocation. By enabling content caching at individual transmitters, we can decrease the demand for data transfer and thus save more backhaul capacity for real-time CSI update and sharing. Using the DQL-based approach at the central scheduler can reduce the explicit demand for CSI and the computational complexity in matrix optimization, especially with time-varying channel conditions. The DQL agent implements the DNN to approximate the Q-function with experience replay in training. To make the learning process more stable, the target Q-network parameter is updated by the Q-network for every a few time instants. The collected information is assembled into a system state and sent to the DQL agent, which feeds back an optimal action for the current time instant. The action indicates which users to be active, and the resource allocation among active users. The system reward represents the total throughput of multiple users. An extended work of [76] and [77] with a similar DQL framework is presented in [78], in which a CNN-based DQN is adopted and evaluated in a more practical conditions with imperfect or delayed CSI. Simulation results show that the performance of the MIMO system is signi\ufb01cantly improved in terms of the total throughput and energy ef\ufb01ciency. Interference management is an important requirement of wireless systems. The application-related QoS or user experience is also an essential metric. Different from [76]\u2013[78], the 18 authors in [79] propose a DQL approach to maximize Quality of Experience (QoE) of IoT devices by jointly optimizing the cache allocation and transmission rate in content-centric wireless networks. The system state is speci\ufb01ed by the nodes\u2019 caching conditions, e.g., the service information and cached contents, as well as the transmission rates of the cached contents. The aim of the DQL agent is to minimize continuously the network cost or maximize the QoE. The proposed DQL framework is further enhanced with the use of PER and DDQN. PER replays important transitions more frequently so that DQN can learn from samples more ef\ufb01ciently. The use of DDQN can stabilize the learning by providing two value functions in separated neural networks. This avoids an overestimation of the DQN with the increasing number of actions. These two neural networks are not completely decoupled as the target network is a periodic copy of estimation network. A discrete simulator ccnSim [80] is used to model the caching behavior in various graph structures. The output data trace of the simulator is then imported to Matlab and used to evaluate the learning algorithm. The simulation results show that the DQL framework by using PER and DDQN outperforms the standard penetration test scheme in terms of QoE. The QoE can be used to characterize the users\u2019 perception of Virtual Reality (VR) services. The authors in [81] address the joint content caching and transmission strategy in a wireless VR network, where UAVs capture videos on live games and transmit them to small-cell BSs servicing the VR users. Millimeter wave (mmWave) downlink backhaul links are used for VR content transmission from the UAVs to BSs. The BSs can also cache the popular contents that may be requested frequently by end users. The joint content caching and transmission problem is formulated as an optimization to maximize the users\u2019 reliability, i.e., the probability that the content transmission delay satis\ufb01es the instantaneous delay target. The maximization involves the control of transmission format, users\u2019 association, the set and format of cached contents. A DQL framework combining the Liquid State Machine (LSM) and Echo State Network (ESN) is proposed for each BS to \ufb01nd the optimal transmission and caching strategies. As a randomly generated spiking neural network [82], LSM can store information about the network environment over time and adjust the users\u2019 association policy, cached contents and formats according to the users\u2019 content requests. It has been used in [83] to predict the users\u2019 content request distribution while having only limited information regarding the network and different users. Conventional LSM uses FNNs as the output function, which demands high complexity in training due to the computation of gradients for all of the neurons. Conversely, the proposed DQL framework uses an ESN as the output function, which uses historical information to \ufb01nd the relationship between the users\u2019 reliability, caching, and content transmission. It also has a lower complexity in training and a better memory for network information. Simulation results show that the proposed DQL framework can yield 25.4% gain in terms of users\u2019 reliability compared to the baseline Q-learning. 3) Joint Caching, Networking, and Computation: Caching and transmission control will become more involved in a HetNet that integrates different communication technologies, e.g., cellular system, device-to-device network, vehicular network, and networked UAVs, to support various application demands. The network heterogeneity raises the problem of complicated system design that needs to address challenging issues such as mutual interference, differentiated QoS provisioning, and resource allocation, hopefully in a uni\ufb01ed framework. Obviously this demands a joint optimization far beyond the extent of joint caching and transmission control. Accordingly, the authors in [84] propose a DQL framework for energy-ef\ufb01cient resource allocation in green wireless networks, jointly considering the couplings among networking, in-network caching and computation. The system consists of a Software-De\ufb01ned Network (SDN) with multiple virtual networks and mobile users requesting for video on-demand \ufb01les that require a certain amount of computational resource at either the content server or at local devices. In each virtual network, an authorized user issues a request to download \ufb01les from a set of available SBSs in its neighborhood area. The wireless channels between each mobile user and the SBSs are characterized as Finite-State Markov Channels (FSMC). The states are the available cache capacity at the SBSs, the channel conditions between mobile users and SBSs, the computational capability of the content servers and mobile users. The DQL agent at each SBS decides an association between each mobile user and SBS, where to perform the computational task, and how to schedule the transmissions of SBSs to deliver the required data. The objective is to minimize the total energy consumption of the system from data caching, wireless transmission, and computation. The DQL scheme proposed in [84] has been applied to improve the performance of Vehicular Ad doc NETworks (VANETs) in [85]\u2013[87]. The network model includes multiple BSs, Road Side Units (RSUs), MEC servers, and content servers. All devices are controlled by a mobile virtual network operator. The vehicles request for video contents that can be cached at the BSs or retrieved from remote content servers. The authors in [85] formulate the resource allocation problem as a joint optimization of caching, networking, and computing, e.g., compressing and encoding operations of the video contents. The system states include the CSI from each BS, the computational capability, and cache size of each MEC/content server. The network operator feeds the system state to the FNN-based DQN and gets the optimal policy that determines the resource allocation for each vehicle. To exploit spatial correlations in learning, the authors in [86] enhance Q-learning by using CNNs in DQN. This makes it possible to extract high-level features from raw input data. Two schemes have been introduced in [87] to improve stability and performance of the ordinary DQN method. Firstly, DDQN is designed to avoid over-estimation of Q-value in ordinary DQN. Hence, the action can be decoupled from the target Q-value generation. This makes the training process faster and more reliable. Secondly, the dueling DQN approach is also integrated in the design with the intuition that it is not always necessary to estimate the reward by taking some action. The state-action Qvalue in dueling DQN is decomposed into one value function representing the reward in the current state, and the advantage 19 Cache Cellular BS Small cell BS Cloud Joint design of caching, networking,  and transmission control strategies Fig. 7: Joint caching, networking, and transmission control to optimize cache hit rate [70], cache expiration time [74], interference alignment [76]\u2013[78], Quality of Experience [79], [81], energy ef\ufb01ciency [84], resource allocation [85]\u2013[87], traf\ufb01c latency, or redundancy [89], [91]. function that measures the relative importance of a certain action compared with other actions. The enhanced DQL agent combining these two schemes can achieve better performance and faster training speed. Considering the huge action space and high complexity with the vehicle\u2019s mobility and service delay deadline Td, a multitime scale DQN framework is proposed in [88] to minimize the system cost by the joint design of communication, caching and computing in VANET. The policy design accounts for limited storage capacities and computational resources at the vehicles and the RSUs. The small timescale DQN is for every time slot and aims to maximize the exact immediate reward. Additionally, the large timescale DQN is designed for every Td time slots within the service delay deadline, and used to estimate the reward considering the vehicle\u2019s mobility in a large timescale. The aforementioned DQL framework for VANETs, e.g., [85]\u2013[87], has also been generalized to smart city applications in [89], which necessitates dynamic orchestration of networking, caching, and computation to meet different servicing requirements. Through Network Function Virtualization (NFV) [90], the physical wireless network in smart cities can be divided logically into several virtual ones by the network operator, which is responsible for network slicing and resource scheduling, as well as allocation of caching and computing capacities. The use cases in smart cities are presented in [91], [92], which apply the generalized DQL framework to improve the security and ef\ufb01ciency for trustbased data exchange, sharing, and delivery in mobile social networks through the resource allocation and optimization of MEC allocation, caching, and D2D (Device-to-Device) networking. B. Data and Computation Of\ufb02oading With limited computation, memory and power supplies, IoT devices such as sensors, wearable devices, and handheld devices become the bottleneck to support advanced applications such as interactive online gaming and face recognition. To address such a challenge, IoT devices can of\ufb02oad the computational tasks to nearby MEC servers, integrated with the BSs, Access Points (APs), and even neighboring Mobile Users (MUs). As a result, data and computation of\ufb02oading can potentially reduce the processing delay, save the battery energy, and even enhance security for computation-intensive IoT applications. However, the critical problem in the computation of\ufb02oading is to determine the of\ufb02oading rate, i.e., the amount of computational workload, and choose the MEC server from all available servers. If the chosen MEC server experiences heavy workloads and degraded channel conditions, it may take even longer time for the IoT devices to of\ufb02oad data and receive the results from the MEC server. Hence, the design of an of\ufb02oading policy has to take into account the time-varying channel conditions, user mobility, energy supply, computation workload and the computational capabilities of different MEC servers. The authors in [93] focus on minimizing the mobile user\u2019s cost and energy consumption by of\ufb02oading cellular traf\ufb01c to WLAN. Each mobile user can either access the cellular network, or the complimentary WLAN as illustrated in Fig. 8(a), but with different monetary costs. The mobile user also has to pay a penalty if the data transmission does not \ufb01nish before the deadline. The mobile user\u2019s data of\ufb02oading decision can be modeled as an MDP. The system state includes the mobile user\u2019s location and the remaining \ufb01le size of all data \ufb02ows. The mobile user will choose to transmit data through either WLAN or cellular network, and decide how to allocate channel capacities to concurrent \ufb02ows. Without knowing the mobility pattern in advance, the DQL is proposed for each mobile user to learn the optimal of\ufb02oading policy from past experiences. CNNs are employed in the DQL to predict a continuous value of the mobile user\u2019s remaining data. Simulation results reveal that the DQN-based scheme generally outperforms the dynamic programming algorithm for the MDP. The reason is that the DQN can learn from experience while the dynamic programming algorithm cannot obtain the optimal policy with incorrect transition probability. The allocation of limited computational resources at the MEC server is critical for cost and energy minimization. The authors in [94] consider an MEC-enabled cellular system, in which multiple mobile users can of\ufb02oad their computational tasks via wireless channels to one MEC server, co-located with the cellular BS as shown in Fig. 8(b). Each mobile user has a computational-intensive task, characterized by the required computational resources, CPU cycles, and the maximum tolerable delay. The capacity of the MEC server is limited to accommodate all mobile users\u2019 task loads. The bandwidth sharing between different mobile users\u2019 of\ufb02oading also affects the overall delay performance and energy consumptions. The DQL is used to minimize the cost of delay and power consumptions for all mobile users, by jointly optimizing the of\ufb02oading decision and computational resource allocation. The system states include the sum of cost of the entire system and the available computational capacity of the MEC server. The action of BS is to determine the resource allocation and 20 of\ufb02oading decision for each mobile user. To limit the size of action space, a pre-classi\ufb01cation step is proposed to check the mobile users\u2019 feasible set of actions. In contrast to [94], multiple BSs in an ultra-dense network is considered in [95] and [96], as shown in Fig. 8(c), with the objective of minimizing the long-term cost of delay in computation of\ufb02oading. All computational tasks are of\ufb02oaded to the shared MEC server via different BSs. Besides the allocation of computational resources and transmission control, the of\ufb02oading policy also has to optimize the association between mobile users and the BSs. With dynamic network conditions, the mobile users\u2019 decision-making can be formulated as an MDP. The system states are the channel conditions between the mobile user and the BSs, the states of energy and task queues. The cost function is de\ufb01ned as a weighted sum of the execution delay, the handover delay and the computational task dropping cost. The authors in [96] \ufb01rstly propose a DDQN-based DQL algorithm to learn the optimal of\ufb02oading policy without knowing the network dynamics. By leveraging the additive structure of the utility function, the Q-function decomposition combined with the DDQN further leads to a novel online SARSA-based DRL algorithm. Numerical experiments show that the new algorithm achieves a signi\ufb01cant improvement in computation of\ufb02oading performance compared with the baseline policies, e.g., the DQN-based DQL algorithm and some heuristic of\ufb02oading strategies without learning. The high density of SBSs can relieve the data of\ufb02oading pressure in peak traf\ufb01c hours but consume a large amount of energy in off-peak time. Therefore, the authors in [97], [98], and [99] propose a DQL-based strategy for controlling the (de)activation of different SBSs to minimize the energy consumption without compromising the quality of provisioning. In particular, in [97], the on/off decision framework uses a DQL scheme to approximate both the policy and value functions in an actor-critic method. The reward of the DQL agent is de\ufb01ned as a cost function relating to energy consumption, QoS degradation, and the switching cost of SBSs. The DDPG approach is also employed together with an action re\ufb01nement scheme to expedite the training process. Through extensive numerical simulations, the proposed scheme is shown to greatly outperform other baseline methods in terms of both energy and computational ef\ufb01ciency. With a similar model to that in [96], computation of\ufb02oading \ufb01nds a proper application for cloud-based malware detection in [100]. A review of the threat models and the RL-based solutions for security and privacy protection in mobile of\ufb02oading and caching are discussed in [101]. With limited energy supply, computational resources, and channel capacity, mobile users cannot always update the local malware database and process all application data in time and thus are vulnerable to zero-day attacks [102]. By leveraging the remote MEC server, all mobile users can of\ufb02oad their application data and detection tasks via different BSs to the MEC/security server with larger and more sophisticated malware database, more computational capabilities, and powerful security services. This can be modeled by a dynamic malware detection game in which multiple mobile users interact with each other in resource competition, e.g., the allocation of wireless channel capacities and the computational capabilities of the MEC/security server. A DQL scheme is proposed for each mobile user to learn its of\ufb02oading data rate to the MEC/security server. The system states include the channel state and the size of application traces. The objective is to optimize the detection accuracy of the security server, which is de\ufb01ned as a concave function in the total amount of malware samples. The Q-value is estimated by using a CNN in the DQL framework. The authors also propose the hotbooting Q-learning technique that provides a better initialization for Q-learning by exploiting the of\ufb02oading experiences in similar scenarios. It can save exploration time at the initial stage and accelerate the learning speed compared with a standard Q-learning algorithm with allzero initialization of the Q-value [103]. The proposed DQL scheme not only improves the detection speed and accuracy, but also increases the mobile users\u2019 battery life. The simulation results reveal that compared with the hotbooting Q-learning, the DQL-based malware detection has the faster learning rate, the higher accuracy, and the shorter detection delay. Multiple MEC servers have been considered in [104], [105], as illustrated in Fig. 8(d). The authors in [104] aim to design optimal of\ufb02oading policy for IoT devices with energy harvesting capabilities. The system consists of multiple MEC servers, such as BSs and APs, with different capabilities in computation and communications. The IoT devices are equipped with energy storage and energy harvesters. They can execute computational tasks locally and of\ufb02oad the tasks to the MEC servers. The IoT device\u2019s of\ufb02oading decision can be formulated as an MDP. The system states include the battery status, the channel capacity, and the predicted amount of harvested energy in the future. The IoT device evaluates the reward based on the overall delay, energy consumption, the task drop loss and the data sharing gains in each time slot. Similar to [100], the authors in [104] enhance Q-learning by the hotbooting technique to save the random exploration time at the beginning of learning. The authors also propose a fast DQL of\ufb02oading scheme that uses hotbooting to initialize the CNN and accelerates the learning speed. The authors in [105] view the MEC-enabled BSs as different physical machines constituting a part of the cloud resources. The cloud optimizes the MUs\u2019 computation of\ufb02oading to different virtual machines residing on the physical machines. A two-layered DQL algorithm is proposed for the of\ufb02oading problem to maximize the utilization of cloud resources. The system state relates to the waiting time of each computational task and the number of virtual machines. The \ufb01rst layer is implemented by a CNN-based DQL framework to estimate an optimal cluster for each computational task. Different clusters of physical machines are generated based on the K-NN algorithm. The second layer determines the optimal serving physical machine within the cluster by Q-learning method. The aforementioned works all focus on data or computation of\ufb02oading in cellular system via BSs to remote MEC servers, e.g., [93]\u2013[96], [100], [104], [105]. In [107] and [106], the authors study QoS-aware computation of\ufb02oading in an ad-hoc mobile network. By making a certain payment, the mobile user can of\ufb02oad its computational tasks to nearby mobile users constituting a mobile cloudlet, as shown in Fig. 8(d). Each 21 Cloud Cellular BS MEC Server (b) MEC Server Cloud (c) Cellular BS WLAN AP Cloud (a) Mobile user Data/Energy  queue Cloud (d) Mobile  cloudlets Fig. 8: Data/computation of\ufb02oading models in cellular networks: (a) Of\ufb02oading cellular traf\ufb01c to WLAN [93], (b) Of\ufb02oading to a single MEC-enabled BS [94], (c) Of\ufb02oading to one shared MEC server via multiple BSs [95], [96], [100], (d) Of\ufb02oading to multiple MECenabled BSs [104], [105] and mobile cloudlets [106], [107]. mobile user has a \ufb01rst-in-\ufb01rst-out queue with limited buffer size to store the arriving tasks arriving as a Poisson process. The mobile user selects nearby cloudlets within D2D communication range for task of\ufb02oading. The of\ufb02oading decision depends on the states including the number of remaining tasks, the quality of the links between mobile users and the cloudlet, and the availability of the cloudlet\u2019s resources. The objective is to maximize a composite utility function, subject to the mobile user\u2019s QoS requirements, e.g., energy consumption and processing delay. The utility function is \ufb01rstly an increasing function of the total number of tasks that have been processed either locally or remotely by the cloudlets. It is also related to the user\u2019s bene\ufb01t such as energy ef\ufb01ciency and payment for task of\ufb02oading. This problem can be formulated as an MDP, which can be solved by linear programming and Q-learning approaches, depending on the availability of information about the state transition probabilities. This work is further enhanced by leveraging DNN or DQN to learn the decision strategy more ef\ufb01ciently. A similar model is studied in [108], where the computation of\ufb02oading is formulated as an MDP to minimize the cost of computation of\ufb02oading. The solution to the MDP can be used to train a DNN by supervised learning. The welltrained DNN is then applied to unseen network conditions for real-time decision-making. Simulation results show that the use of deep supervised learning achieves signi\ufb01cant performance gain in of\ufb02oading accuracy and cost saving. Data and computation of\ufb02oading is also used in fog computing. The mobile application demanding a set of data and computational resources can be hosted in a container, e.g., virtual machine of a fog node. With user\u2019s mobility, the container has to be migrated or of\ufb02oaded to other nodes and dynamically consolidated. With the container migration, some nodes with low resource utilization can be switched off to reduce power consumption. The authors in [109] model the container migration as a multi-dimensional MDP, which is solved by the DQL. The system states consist of the delay, the power consumption and the migration cost. The action includes the selection policy that selects the containers to be emigrated from each source node, and the allocation policy that determines the destination node of each container. The action space can be optimized for more ef\ufb01cient exploration by dividing fog nodes into under-utilization, normal-utilization, and over-utilization groups. By powering off under-utilization nodes, all their containers will be migrated to other nodes to reduce power consumption. The training process is also optimized by using DDQN and PER which assigns different priorities to the transitions in experience memory. This helps the DQL agent at each fog node to perform better in terms of faster learning speed and more stability. Simulation results reveal that the DQL scheme achieves fast decision-making and outperforms the existing baseline approaches signi\ufb01cantly in terms of delay, power consumption, and migration cost. Summary: This section reviews the applications of the DQL for wireless caching and data/computation of\ufb02oading, which are inherently coupled with networking and allocation of channel capacity, computational resources, and caching capabilities, etc. We observe that the DQL framework for caching is typically centralized and mostly implemented at the network controller, e.g., the BS, service provider, and central scheduler, which is more powerful in information collection and cross-layer policy design. On the contrary, the end users have more control over their of\ufb02oading decisions, and hence we observe more popular implementation of the DQL agent at local devices, e.g., mobile users, IoT devices, and fog nodes. Though an orchestration of networking, caching, data and computation of\ufb02oading in one uni\ufb01ed DQL framework is promising for network performance maximization, we face many challenges in designing highly-stable and fastconvergent learning algorithms, due to excessive delay and unsynchronized information collection from different network entities. V. NETWORK SECURITY AND CONNECTIVITY PRESERVATION Future networks become more decentralized and ad-hoc in nature which are vulnerable to various attacks such as Denialof-Service (DoS) and cyber-physical attack. Recently, the DQL has been used as an effective solution to avoid and prevent the attacks. In this section, we review the applications of DQL in addressing the following security issues: \u2022 Jamming attack: In the jamming attack, attackers as jammers transmit Radio Frequency (RF) jamming signals with high power to cause interference to the legitimate communication channels, thus reducing the SINR at legitimate receivers. Anti-jamming techniques such as the frequency hopping [110] and user mobility, i.e., moving out from the heavy jamming area, have been commonly used. However, without being aware of the radio channel 22 TABLE IV: A summary of approaches using DQL for caching and of\ufb02oading. ISSUES REF. MODEL LEARNING ALGORITHMS AGENT STATES ACTIONS REWARDS NETWORKS Wireless proactive caching [70] MDP DQN using actor-critic, DDPG Base station Cached contents and requested content Replace selected content or not Cache hit rate (score 1 or 0) CRN [84] MDP DQN using FNN Base station Channel states and computational capabilities User association, computational unit, content delivery Energy consumption CRN [74] MDP DQN using NAFs Cloud database Encoding of a query, query cache miss rate Cache expiration times Cache hit rates, CDN utilization Cloud database [76] [77] MDP DQN using FNN Central scheduler Channel coef\ufb01cients, cache state Active users and resource allocation Network throughput MU MIMO system [78] MDP DQN using CNN Central scheduler Channel coef\ufb01cients, cache state Active users and resource allocation Network throughput MU MIMO system [79] MDP DDQN Service provider Conditions of cache nodes, transmission rates of content chunks The content chunks to cache and to remove Network cost, QoE Content centric IoT [81] MDP DQN using LSM and ESN Base station Historical content request User association, cached contents and formats Reliability Cellular system [86] [89] MDP DQN using CNN Service provider Available BS, MEC, and cache User association, caching, and of\ufb02oading Composite revenue Vehicular ad hoc network [85] MDP DQN using FNN Service provider Available BS, MEC, and cache User association, caching, and of\ufb02oading Composite revenue Vehicular ad hoc network [87] MDP DDQN and dueling DQN Service provider Available BS, MEC, and cache User association, caching, and of\ufb02oading Composite revenue Vehicular ad hoc network [91] MDP DQN using CNN Base station Channel state, computational capability, content/version indicator, and the trust value User association, caching, and of\ufb02oading Revenue Mobile social network Data and computation of\ufb02oading [93] MDP DQN using CNN Mobile user User\u2019s location and remaining \ufb01le size Idle, transmit via WLAN or cellular network Total data rate Cellular system [94] MDP DQN using FNN Base station Sum of cost and computational capacity of the MEC server Of\ufb02oading decision and resource allocation Sum of cost of delay and energy consumption Cellular system [95] MDP DQN using FNN Mobile user Channel qualities, states of energy and task queues Of\ufb02oading and resource allocation Long term cost function Cellular system [96] MDP DDQN, SARSA Mobile user Channel qualities, states of energy and task queues Of\ufb02oading decision and computational resource allocation Long term cost function Cellular system [100] Game DQN using CNN, hotbooting Q-learning Mobile user Channel states, size of App traces Of\ufb02oading rate Utility related to detection accuracy, response speed, and the transmission cost Cellular system [109] MDP DDQN Fog node Delay, container\u2019s location and resource allocation Container\u2019s next location Composite utility related to delay, power consumption, and migration cost Fog computing model and the jamming methods, it is challenging for the users to choose an appropriate frequency channel as well as to determine how to leave and avoid the attack. DQL enables the users to learn an optimal policy based on their past observations, and thus DQL can be used to address the above challenge. \u2022 Cyber-physical attack: The cyber-physical attack is an integrity attack in which an attacker manipulates data to alter control signals in the system. This attack often happens in autonomous systems such as Intelligent Transportation Systems (ITSs) and increases the risk of accidents to Autonomous Vehicles (AVs). The DQL allows the AVs to learn optimal actions based on their time-varying observations of the attacker\u2019 activities. Thus, the DQL can be used to achieve robust and dynamic control of the AV to the attacks. \u2022 Connectivity preserving: This refers to maintaining the connectivity among the robots, e.g., UAVs, to support the communication and exchange of information among them. The system and network environment is generally dynamic and complex, and thus the DQL which allows each robot to make dynamic decisions based on its state can be effectively used to preserve the connectivity in the system. 23 BS Attacker PU Attacker PU SU  (agent) Receiver BS Move ? Jamming Jamming Fig. 9: Jamming attack in cognitive radio network [111]. A. Network Security This section discusses the applications of DQL to address the jamming attack and the cyber-physical attack. 1) Jamming Attack: A pioneer work using the DQL for the anti-jamming is [111]. The network model is a Cognitive Radio Network (CRN) as shown in Fig. 9 which consists of one Secondary User (SU), multiple Primary Users (PUs), and multiple jammers. The network has a set of frequency channels for hopping. At each time slot, each jammer can arbitrarily select one of the channels to send its jamming signal, and the SU, i.e., the agent, needs to choose a proper action based on the SU\u2019s current state. The action is (i) selecting one of the channels to send its signals or (ii) leaving the area to connect to another BS. The jammers are assumed to avoid causing interference to the PUs, and thus the SU\u2019s current state consists of the number of PUs and the discretized SINR of the SU signal at the last time slot. The objective of the SU is to maximize its expected discounted utility over time slots. Note that when the SU chooses to leave the area to connect to another BS, it spends a mobility cost. Thus, the utility is de\ufb01ned as a function of the SINR of the SU signal and the mobility cost. Since the number of frequency channels may be large that results in a large action set, the CNN is used for the DQL to quickly learn the optimal policy. As shown in the simulation results, the proposed DQL has a faster convergence speed than that of the Q-learning algorithm. Moreover, considering the scenario with two jammers, the proposed DQL outperforms the frequency-hopping method in terms of the SINR and the mobility cost. The model in [111] is constrained to two jammers. As the number of jammers in the network increases, the proposed scheme may not be effective. The reason is that it becomes hard for the SU to \ufb01nd good actions when the number of jammed channels increases. An appropriate solution, as proposed in [112], allows the receiver of the SU to leave its current location. Since the leaving incurs the mobility cost, the receiver, i.e., the agent, needs an optimal policy, i.e., staying at or leaving the current location, to maximize its utility. In this scenario, the DQL based on CNN can be used for the receiver to \ufb01nd the optimal action to maximize its expected utility. Here, the utility and state of the receiver are essentially de\ufb01ned similarly to that of the agent in [111]. In particular, the state includes the discretized SINR of the signal measured by the receiver at the last time slot. The above approaches, i.e., in [111] and [112], de\ufb01ne states of the agents based on raw SINR values of the signals. In practical wireless environments, the number of SINR values may be large and even in\ufb01nite. Moreover, the raw SINR can be inaccurate and noisy. To cope with the challenge of the in\ufb01nite number of states, the DQL can use a recursive Convolutional Neural Network (RCNN) as proposed in [113]. By using the pre-processing layer and recursive convolution layers, the RCNN is able to remove noise from the network environment and extract useful features of the SINR, i.e., discrete spectrum sample values greater than a noise threshold, thus reducing the computational complexity. The network model and the problem formulation considered in [113] are similar to those in [111]. However, instead of directly using the raw SINR, the state of the SU is the extracted features of the SINR. Also, the action of the SU includes only frequency-hopping decision. The simulation results show that the proposed DQL based on the RCNN can converge in both \ufb01xed and dynamic jamming scenarios while the Q-learning cannot converge in the dynamic jamming one. Furthermore, the proposed DQL can achieve the average throughput close to that of the optimal scheme, i.e., an anti-jamming scheme with completely known jamming actions. Instead of \ufb01nding the frequency-hopping decisions, the authors in [114] propose the use of DQL to \ufb01nd an optimal power control policy for the anti-jamming. The model is an IoT network including IoT devices and one jammer. The jammer can observe the communications of the transmitter and chooses a jamming strategy to reduce the SINR at the receiver. Thus, the transmitter chooses an action, i.e., transmit power level, to maximize its utility. Here, the utility is the difference between the SINR and the energy consumption cost due to the transmission. Note that choosing the transmit power impacts the future jamming strategy, and thus the interaction between the transmitter and the jammer can be formulated as an MDP. The transmitter is the agent, and the state is SINR measured at its receiver at the last time slot. The DQN using the CNN is then adopted to \ufb01nd an optimal power control policy for the transmitter to maximize its expected accumulated discounted reward, i.e., the utility, over time slots. The simulation results show that the proposed DQL can improve the utility of the transmitter up to 17.7% compared with the Q-learning scheme. Also, the proposed DQL reduces the utility of the jammer around 18.1% compared with the Q-learning scheme. To prevent the jammer\u2019s observations of communications, the transmitter can change its communication strategy, e.g., by using relays that are far from the jamming area. The relays can be UAVs as proposed in [115]. The model consists of one UAV, i.e., a relay, one jammer, one mobile user and its serving BS (see Fig. 10). The mobile user transmits messages to its server via the serving BS. In the case that the serving BS is heavily jammed, the UAV helps the mobile user to relay the messages to the server through a backup BS. In particular, depending on the SINR and Bit Error Rate (BER) values sent from the serving BS, the UAV as an agent decides the relay power level to maximize its utility, i.e., the difference between the SINR and the relay cost. The relay power level can be considered to be the UAV\u2019s actions, and the SINR and BER are its states. As 24 Serving BS Attacker Jamming User Relay UAV  (agent) Server (receiver) Backup BS  Messages Messages Jamming  Compromised  UAV Messages Fig. 10: Anti-jamming scheme based on UAV [115]. such, the next state observed by the UAV is independent of all the past states and actions. The problem is formulated as an MDP. To quickly achieve the optimal relay policy for the UAV, the DQL based on CNN is then adopted. The simulation results in [115] show that the proposed DQL scheme takes only 200 time slots to converge to the optimal policy, which is 83.3% less than that of the relay scheme based on Q-learning [116]. Moreover, the proposed DQL scheme reduces the BER of the user by 46.6% compared with the hill climbing-based UAV relay scheme [117]. The scheme proposed in [115] assumes that the relay UAV is suf\ufb01ciently far from the jamming area. However, as illustrated in Fig. 10, the attacker can use a compromised UAV close to the relay UAV to launch the jamming attack to the relay UAV. In such a scenario, the authors in [118] show that the DQL can still be used to address the attack. The system model is based on physical layer security and consists of one UAV and one attacker. The attacker is assumed to be \u201csmarter\u201d than that in the model in [115]. This means that the attacker can observe channels that the UAV uses to communicate with the BS in the past time slots and then chooses jamming power levels on the target channels. Therefore, the UAV needs to \ufb01nd a power allocation policy, i.e., transmit power levels on the channels, to maximize the secrecy capacity of the UAVBS communication. Similar to [115], the DQL based on CNN is used which enables the UAV to choose its actions, i.e., transmit power levels on the channels, based on its state, i.e., the attacker\u2019s jamming power level in the last time slot. The reward is the difference between the secrecy capacity of the UAV and BS and the energy consumption cost. The simulation results in [118] show that the proposed DQL can improve the UAV\u2019s utility up to 13% compared with the baseline scheme [119] which uses the Win or Learn FasterPolicy Hill Climbing (WoLF-PHC) to prevent the attack. Also, the safe rate of the UAV, i.e., the probability that the UAV is attacked, obtained by the proposed DQL is 7% higher than that of the baseline. However, the proposed DQL is applied only to a single-UAV system. For the future work, scenarios with multiple UAVs need to be considered. In such a Attacker Faulty data Sensing data Sensor Sensor AV AV(agent) Safe spacing AV Sensor Sensing data Fig. 11: Car-following model with cyber-physical attack. scenario, more computational overhead is expected and multiagent DQL algorithms can be applied. 2) Cyber-Physical Attack: In autonomous systems such as ITSs, the attacker can seek to inject faulty data to information transmitted from the sensors to the AVs. The AVs which receive the injected information may inaccurately estimate the safe spacing among them. This increases the risk of AV accidents. Vehicular communication security algorithms, e.g., [120], can be used to minimize the spacing deviation. However, the attacker\u2019s actions in these algorithms are assumed to be stable which may not be applicable in practical systems. The DQL that enables the AVs to learn optimal actions based on their time-varying observations of the attacker\u2019 actions can be thus used. The \ufb01rst work using the DQL for the cyber-physical attack in an ITS can be found in [121]. The system is a carfollowing model [122] of the General Motors as shown in Fig. 11. In the model, each AV updates its speed based on measurement information received from the closest road smart sensors. The attacker attempts to inject faulty data to the measurement information. However, the attacker cannot inject the measurements of different sensors equally due to its resource constraint. Thus, the AV can choose less-faulty measurements by selecting a vector of measurement weights. The objective of the attacker is to maximize the deviation, i.e., the utility, from the safe spacing between the AV and its nearby AV while that of the AV is to minimize the deviation. The interaction between the attacker and the AV can be modeled as a zero-sum game. The authors in [121] show that the DQL can be used to \ufb01nd the equilibrium strategies. In particular, the action of the AV is to choose a weight vector. Its state includes the past actions, i.e., the weight vectors, and the past deviation values. Since the actions and deviations have continuous values, the state space is in\ufb01nite. Thus, LSTM units that are able to extract useful features are adopted for the DQL to reduce the state space. The simulation results show that by using the past actions and deviations for learning the attacker\u2019s action, the proposed DQL scheme can guarantee a lower steady-state deviation than the Kalmar \ufb01lter-based scheme [120]. Moreover, by using the LSTM units, the results show that the proposed DQL scheme can converge much faster than the baseline scheme. Another work that uses the LSTM to extract useful features from the measurement information to detect the cyber-physical 25 IoT  device LSTM Cloud  LSTM LSTM IoT  device IoT device Signal Probs. of attacking IoT devices Actions on  IoT devices Signal Signal LSTM Fig. 12: Cyber-physical detection in IoT systems using DQL. attack is proposed in [123]. The model is an IoT system including a cloud and a set of IoT devices. The IoT devices generate signals and transmit the signals to the cloud (see Fig. 12). The cloud uses the received signals for estimation and control of the IoT devices\u2019 operation. An attacker can launch the cyber-physical attack by manipulating the IoT devices\u2019 output signals that causes control errors at the cloud and degrades the performance of the IoT system. To detect the attack, the cloud uses LSTM units to extract stochastic features or \ufb01ngerprints such as \ufb02atness, skewness, and kurtosis, of the IoT devices\u2019 signals. The cloud sends the \ufb01ngerprints back to the IoT devices, and the IoT devices embed, i.e., watermark, the \ufb01ngerprints inside the signals. The cloud uses the \ufb01ngerprints to authenticate the IoT devices\u2019 signals to detect the attack. The algorithm proposed in [123] is also called dynamic watermarking [124] which is able to detect the cyber-physical attack and to prevent eavesdropping attacks. However, the algorithm requires large computational resources at the cloud for the IoT device signal authentication. Consequently, the cloud can only authenticate a limited number of vulnerable IoT devices. The cloud can choose the vulnerable IoT devices by observing their security status. However, this can be impractical since the IoT devices may not report their security status. Thus, the authors in [125] propose to use the DQL that enables the cloud to decide which IoT devices to authenticate with the incomplete information. Since IoT devices with more valuable data are likely to be attacked, the reward is de\ufb01ned as a function of data values of IoT devices. The cloud\u2019s state includes attack actions of the attacker on the IoT devices in the past time slots. The actions of the attacker on the IoT devices can be obtained by using the dynamic watermarking algorithm in [123] (see Fig. 12). The DQL then uses an LSTM unit to \ufb01nd the optimal policy. The input of the LSTM unit is the state of the cloud, and the output includes probabilities of attacking the IoT devices. By using a real dataset from the accelerometers, the simulation results show that the proposed DQL can improve the cloud\u2019s utility up to 30% compared with the case in which the cloud chooses the IoT devices with equal probability. Base station Control  signals Connectivity Connectivity Leader Follower Follower Handover Base station Fig. 13: Connectivity preservation of a multi-UAV network. B. Connectivity Preservation Multi-robot systems such as multi-UAV cooperative networks have been widely applied in many \ufb01elds such as military, e.g., enemy detecting. In the cooperative multi-robot system, the connectivity among the robots, e.g., UAVs in Fig 13, is required to enable the communication and exchange of information. To tackle the connectivity preservation problem, the Arti\ufb01cial Potential Field (APF) algorithm [126] has been used. However, the algorithm cannot be directly adopted when the robots are undertaking missions in dynamic and complex environments. The DQL which allows each robot to make dynamic decisions based on its own state can be effectively applied to preserve the connectivity in the multirobot system. Such an approach is proposed in [127]. The model in [127] consists of two robots or UAVs, i.e., one leader robot and one follower robot. In the model, a central control, i.e., a ground BS, adjusts the velocity of the follower such that the follower stays in the communication range of the leader at all time (see Fig 13). The connectivity preservation problem can be thus formulated as an MDP. The agent is the BS, and the states are the relative position and the velocity of the leader with respect to the follower. The action space consists of possible velocity values of the follower. Taking an action returns a reward which is +1 if the follower is in the range of the leader, and -1 otherwise. A DQN using FNN is used which enables the BS to learn an optimal policy to maximize the expected discounted cumulative reward. The input of the DQN includes the states of the two robots, and the output is the action space of the follower. The simulation results show that the proposed scheme can achieve better connectivity between the two robots than that of the APF method. However, a general scenario with more than one leader and one follower needs to be investigated. Considering the general scenario, the authors in [128] address the connectivity preservation between multiple leaders and multiple followers. The robot system is de\ufb01nitely connected if any two robots are connected via a direct link or multi-hop link. To express the connectivity in such a robot system, the authors introduce the concept of algebraic connectivity [129] which is the second smallest eigenvalue of a Laplacian matrix. The robot system is connected if the algebraic connectivity of the system is positive. Thus, the 26 problem is to adjust the velocity of the followers such that the algebraic connectivity is positive over time slots. This problem can be formulated as an MDP in which the agent is the ground BS, the state is a combination of the states of all robots, the action is a set of possible velocity values for the followers. The reward is +1 if the algebraic connectivity of the system increases or holds, and becomes a penalty of -1 if the algebraic connectivity decreases. Similar to [127], a DQN is adopted. Due to the large action space of the followers, the actor-critic neural network [26] is used. The simulation results show that the followers always follow the motion of the leaders even if the leaders\u2019 trajectory dynamically changes. However, the proposed DQN requires more time to converge than that in [127] because of the presence of more followers. The proposed schemes in [127] and [128] do not consider a minimum distance between the leaders and followers. The leaders and followers can collide with each other if the distance between them is too short. Thus, the BS needs to guarantee the minimum distance between them. One solution is to have the minimum distance in the reward as proposed in [130]. In particular, if the leader is too close to its follower, the reward of the system is penalized regarding the minimum distance. The DQL algorithm proposed in [128] is then used such that the BS learns proper actions, e.g., turning left and right, to maximize the cumulative reward. When BSs are densely deployed, the UAVs or mobile users need to trigger a frequent handover to preserve the connectivity. The frequent handover increases communication overhead and energy consumption of the mobile users, and interrupts data \ufb02ows. Thus, it is essential to maintain an appropriate handover rate. The authors in [27] address the handover decision problem in an ultra-density network. The network model consists of multiple mobile users, SBSs, and one central controller. At each time slot, the user needs to decide its serving SBS. The handover decision process can be modeled as an MDP, and the DQL is adopted to \ufb01nd an optimal handover policy for each user to minimize the number of handover occurrences while ensuring certain throughput. The state of the user, i.e., the agent, includes reference signal quality received from candidate SBSs and the last action of the user. The reward is de\ufb01ned as the difference between the data rate of the user and its energy consumption for the handover process. Given a high density of users, the DQL using A3C and LSTM is adopted to \ufb01nd the optimal policy in short training time. The simulation results show that the proposed DQL can achieve better throughput and lower handover rate than those of the upper con\ufb01dence bandit algorithm [131] with similar training time. To enhance the reliability of the communication between the SBSs and the mobile users, the SBSs should be able to handle network faults and failure automatically as selfhealing. The DQL can be applied as proposed in [132] to make optimal parameter adjustments based on the observation of the network performance. The model is the 5G network including one MBS. The MBS as an agent needs to handle network faults such as transmit diversity faults and antenna azimuth change, e.g., because of wind. These faults are represented as the MBS\u2019s state that is the number of active alarms. Based on the alarms, the MBS can take actions including (i) enabling the transmit diversity and (ii) setting the antenna azimuth to default value. The reward that the MBS receives is the scores, e.g., -1, 0, and +1, depending on the number of faults happening. The DQL is used to learn the optimal policy. The simulation results show that the proposed DQL can achieve network throughput close to that of the oracle-based selfhealing, i.e., the upper-performance bound, but incurs less fault message passing overhead. Summary: This section reviews applications of DQL for the network security and connectivity preservation. The reviewed approaches are summarized along with the references in Table V. We observe that the CNN is mostly used for the DQL to enhance the network security. Moreover, DQL approaches for the anonymous system such as robot systems and ITS receive more attentions than other networks. However, the applications of DQL for the cyber-physical security are relatively few and need to be investigated. VI. MISCELLANEOUS ISSUES This section reviews applications of DRL to solve some other issues in communications and networking. The issues include (i) traf\ufb01c engineering and routing, (ii) resource sharing and scheduling, and (iii) data collection. A. Traf\ufb01c Engineering and Routing Traf\ufb01c Engineering (TE) in communication networks refers to Network Utility Maximization (NUM) by optimizing a path to forward the data traf\ufb01c, given a set of network \ufb02ows from source to destination nodes. Traditional NUM problems are mostly model-based. However, with the advances of wireless communication technologies, the network environment becomes more complicated and dynamic, which makes it hard to model, predict, and control. The recent development of DQL methods provides a feasible and ef\ufb01cient way to design experience-driven and model-free schemes that can learn and adapt to the dynamic wireless network from past observations. Routing optimization is one of the major control problems in traf\ufb01c engineering. The authors in [133] present the \ufb01rst attempt to use the DQL for the routing optimization. Through the interaction with the network environment, the DQL agent at the network controller determines the paths for all sourcedestination pairs. The system state is represented by the bandwidth request between each source-destination pair, and the reward is a function of the mean network delay. The DQL agent leverages the actor-critic method for solving the routing problem that minimizes the network delay, by adapting routing con\ufb01gurations automatically to current traf\ufb01c conditions. The DQL agent is trained using the traf\ufb01c information generated by a gravity model [134]. The routing solution is then evaluated by OMNet+ discrete event simulator [135]. The well-trained DQL agent can produce a near-optimal routing con\ufb01guration in a single step and thus the agent is agile for real-time network control. The proposed approach is attractive as the traditional optimization-based techniques require a large number of steps to produce a new con\ufb01guration. The authors in [136] consider 27 TABLE V: A summary of approaches using DQL for network security and connectivity preservation. ISSUES REF. MODEL LEARNING ALGORITHMS AGENT STATES ACTIONS REWARDS NETWORKS Network security [111] Game DQN using CNN Secondary user Number of PUs and signal SINR Channel selection and leaving decision SINR and mobility cost CRN [112] Game DQN using CNN Receiving transducer Signal SINR Staying and leaving decisions SINR and mobility cost Underwater acoustic network [113] MDP DQN using RCNN SU Signal SINR Channel selection SINR and mobility cost CRN [114] MDP DQN using CNN Transmit IoT device Signal SINR Channel selection SINR and energy consumption cost IoT [115] MDP DQN using CNN Relay UAV Signal SINR and BER Relay power SINR and relay cost UAV [118] MDP DQN using CNN Transmit UAV Jamming power Transmit power Secrecy capacity and energy consumption cost UAV [121] Game DQN using LSTM units Autonomous vehicle Deviation values Measurement weight selection Safe spacing deviation ITS [125] Game DQN using LSTM units Cloud Attack actions on IoT devices IoT device set selection IoT devices\u2019 data values IoT Connectivity preservation [127] MDP DQN using FNN Ground base station Relative positions and the velocity of robots Velocity decision Sore +1 and -1 Robot system [128] MDP DQN using A3C Ground base station Relative positions and the velocity of robots Velocity decision Sore +1 and -1 Robot system [130] POMDP DQN using A3C Ground base station Information of distances among robots Turning left and turning right decisions Sore +1 and -1 Robot system [27] MDP DQN using A3C and LSTM Mobile users Reference signal received quality and the last action Serving SBS selection Data rate and energy consumption Ultra-dense network [132] MDP DQN using CNN MBS The number of active alarms Enabling transmit diversity and changing antenna azimuth Score -1, 0, +1, and +5 Selforganization network a similar network model with multiple end-to-end communication sessions. Each source-destination pair has a set of candidate paths that can transport the traf\ufb01c load. Experimental results show that the conventional DDPG method does not work well for the continuous control problem in [136]. One possible explanation is that DDPG utilizes uniform sampling for experience replay, which ignores different signi\ufb01cance of the transition samples. The authors in [136] also combine two new techniques to optimize DDPG particularly for traf\ufb01c engineering problems, i.e., TE-aware exploration and actor-critic-based PER methods. The TE-aware exploration leverages the shortest path algorithm and NUM-based solution as the baseline during exploration. The PER method is conventionally used in DQL, e.g., [79] and [109], while the authors in [136] integrate the PER method with the actor-critic framework for the \ufb01rst time. The proposed scheme assigns different priorities to transitions in the experience replay. Based on the priority, the proposed scheme samples the transitions in each epoch. The system state consists of throughput and delay performance of each communication session. The action speci\ufb01es the amount of traf\ufb01c load going through each of the paths. By learning the dynamics of network environment, the DQL agent aims to maximize the total utility of all the communication sessions, which is de\ufb01ned based on end-to-end throughput and delay [137]. Packet-level simulations using NS-3 [138], tested on well-known network topologies as well as random topologies generated by BRITE [139], reveal that the proposed DQL scheme signi\ufb01cantly reduces the end-to-end delay and improves the network utility, compared with the baseline schemes including DDPG and the NUM-based solutions. The networking and routing optimization become more complicated in the UAV-based wireless communications. The authors in [130] model autonomous navigation of one single UAV in a large-scale unknown complex environment as a POMDP, which can be solved by actor-critic-based DRL method. The system state includes its distances and orientation angles to nearby obstacles, the distance and angle between its present position and the destination. The UAV\u2019s action is to turn left or right or keep ahead. The reward is composed of four parts: an exponential penalty term if it is too close to any obstacles, a linear penalty term to encourage minimum time delay, the transition and direction rewards if the UAV is getting close to the target position in a proper direction. Instead of using conventional DDPG for continuous control, the Recurrent Deterministic Policy Gradient (RDPG) is proposed for the POMDP by approximating the actor and critic using RNNs. Considering that RDPG is not suitable for learning using memory replay, the authors in [130] propose the fastRDPG method by utilizing the actor-critic framework with function approximation [140]. The proposed method derives policy update for POMDP by directly maximizing the expected long-term accumulated discounted reward. Path planning for multiple UAVs connected via cellular systems is studied in [141] and [142]. Each UAV aims to achieve a tradeoff between maximizing energy ef\ufb01ciency and 28 minimizing both latency and interference caused to the ground network along its path. The network state observable by each UAV includes its distances and orientation angles to cellular BSs, the orientation angle to its destination, and the horizontal coordinates of all UAVs. The action of each UAV includes an optimal path, transmit power, and cell association along its path. The interaction among UAVs is cast as a dynamic game and solved by a multi-agent DRL framework. The use of ESN in the DRL framework allows each UAV to retain previous memory states and make a decision for unseen network states, based on the reward obtained from previous states. ESN is a new type of RNNs with feedback connections, consisting of the input, recurrent, and output weight matrices. ESN training is typically quick and computationally ef\ufb01cient compared with other RNNs. Deep ESNs can exploit the advantages of a hierarchical temporal feature representation at different levels of abstraction, hence disentangling the dif\ufb01culties in modeling complex tasks. Simulation results show that the proposed scheme improves the tradeoff between energy ef\ufb01ciency, wireless latency, and the interference caused to the ground network. Results also show that each UAV\u2019s altitude is a function of the ground network density and the UAV\u2019s objective function is an important factor in achieving the UAV\u2019s target. Besides networked UAVs, vehicle-to-infrastructure also constitutes an important part and provides rich application implications in 5G ecosystem. The authors in [143] adopt the DQL to achieve an optimal control policy in communication-based train control system, which is supported by bidirectional trainground communications. The control problem aims to optimize the handoff decision and train control policy, i.e., accelerate or decelerate, based on the states of stochastic channel conditions and real-time information including train position, speed, measured SNR from APs, and handoff indicator. The objective of the DQL agent is to minimize a weighted combination of operation pro\ufb01le tracking error and energy consumption. B. Resource Sharing and Scheduling System capacity is one of the most important performance metrics in wireless communication networks. System capacity enhancements can be based on the optimization of resource sharing and scheduling among multiple wireless nodes. The integration of DRL into 5G systems would revolutionize the resource sharing and scheduling schemes from model-based to model-free approaches and meet various application demands by learning from the network environment. The authors in [144] study the user scheduling in a multiuser massive MIMO system. User scheduling is responsible for allocating resource blocks to BSs and mobile users, taking into account the channel conditions and QoS requirements. Based on this user scheduling strategy, a DRL-based coverage and capacity optimization is proposed to obtain dynamically the scheduling parameters and a uni\ufb01ed threshold of QoS metric. The performance indicators are calculated as the average spectrum ef\ufb01ciency of all the users. The system state is an indicator of the average spectrum ef\ufb01ciency. The action of the scheduler is a set of scheduling parameters to maximize the reward as a function of the average spectrum ef\ufb01ciency. The DRL scheme uses policy gradient method to learn a policy function (instead of a Q-function) directly from trajectories generated by the current policy. The policy network is trained with a variant of the REINFORCE algorithm [140]. The simulation results in [144] show that compared with the optimization-based algorithms that suffer from incomplete network information, the policy gradient method achieves much better performance in terms of network coverage and capacity. In [145], the authors focus on dynamic resource allocation in a cloud radio access network and present a DQL-based framework to minimize the total power consumption while ful\ufb01lling mobile users\u2019 QoS requirements. The system model contains multiple Remote Radio Heads (RRHs) connected to a cloud BaseBand Unit (BBU). The information of RRHs can be shared in a centralized manner. The system state contains information about the mobile users\u2019 demands and the RRHs\u2019 working states, e.g., active or sleep. According to the system state and the result of last execution, the DQL agent at the BBU decides whether to turn on or off certain RRH(s), and how to allocate beamforming weight for each active RRH. The objective is to minimize the total expected power consumption. The authors propose a two-step decision framework to reduce the size of action space. In the \ufb01rst step, the DQL agent determines the set of active RRHs by Q-learning and DNNs. In the second step, the BBU derives the optimal resource allocation for the active RRHs by solving a convex optimization problem. Through the combination of DQL and optimization techniques, the proposed framework results in a relatively small action space and low online computational complexity. Simulation results show that the framework achieves signi\ufb01cant power savings while satisfying user demands and is robust in highly dynamic network environment. The aforementioned works mostly focus on simulations and numerical comparisons. With one step further, the authors in [146] implement a multi-objective DQL framework as the radio-resource-allocation controller for space communications. The implementation uses modular software architecture to encourage re-use and easy modi\ufb01cation for different algorithms, which is integrated into the real spaceground system developed by NASA Glenn Research Center. In emerging and future wireless networks, BSs are deployed with a high density, and thus the interference among the BSs must be considered. The authors in [147] propose to use a DQL scheme which allows the BSs to learn their optimal power control policy. In the proposed scheme, each BS is an agent, the action is choosing power levels, and the state includes interference that the BS caused to its neighbors in the last time slot. The objective is to maximize the BS\u2019s data rate. The DQN using FNN is then adopted to implement the DQL algorithm. For the future work, a joint power control and channel selection can be considered. Network slicing [148] and NFV [90] are two emerging concepts for resource allocation in the 5G ecosystem to provide cost-effective services with better performance. The network infrastructure, e.g., cache, computation, and radio resources, is comparatively static while the upper-layer Virtualized Network Functions (VNFs) are dynamic to support time-varying application-speci\ufb01c service requests. The concept 29 of network slicing is to divide the network resources into multi-layer slices, managed by different service renderers independently with minimal con\ufb02icts. The concept of Service Function Chaining (SFC) is to orchestrate different VNFs to provide required functionalities and QoS provisioning. The authors in [149] propose a DQL scheme for QoS/QoEaware SFC in NFV-enabled 5G systems. Typical QoS metrics are bandwidth, delay, throughput, etc. The evaluation of QoE normally involves the end-user\u2019s participation in rating the service based on direct user perception. The authors quantify QoE by measurable QoS metrics without end-user involvements, according to the Weber-Fechner Law (WFL) [150] and exponential interdependency of QoE and QoS hypothesis [151]. These two principles actually de\ufb01ne nonlinear relationship between QoE and QoS. The system state represents the network environment including network topology, QoS/QoE status of the VNF instances, and the QoS requirements of the SFC request. The DQL agent selects a certain direct successive VNF instance as an action. The reward is a composite function of the QoE gain, the QoS constraint penalty, and the OPEX penalty. A DQL based on CNNs is implemented to approximate the action-value function. The authors in [152] review the application of a DQL framework in two typical resource management scenarios using network slicing. For radio resource slicing, the authors simulate a scenario containing one single BS with different types of services. The reward can be de\ufb01ned as a weighted sum of spectrum ef\ufb01ciency and QoE. For priority-based core network slicing, the authors simulate a scenario with 3 SFCs demanding different computational resources and waiting time. The reward is the sum of waiting time in different SFCs. Simulation results in both scenarios show that the DQL framework could exploit more implicit relationship between user activities and resource allocation in resource constrained scenarios, and enhance the effectiveness and agility for network slicing. Resource allocation and scheduling problems are also important for computer clusters or database systems. This usually leads to an online decision-making problem depending on the information of workload and environment. The authors in [153] propose a DRL-based solution, DeepRM, by employing policy gradient methods [140] to manage resources in computer systems directly from experience. The same policy gradient method is also used in [144] for user scheduling and resource management in wireless systems. DeepRM is a multi-resource cluster scheduler that learns to optimize various objectives such as minimizing average job slowdown or completion time. The system state is the current allocation of cluster resources and the resource pro\ufb01les of jobs in the queue. The action of the scheduler is to decide how to schedule the pending jobs. By simulations with synthetic dataset, DeepRM is shown to perform comparably or better than state-of-the-art heuristics, e.g., Shortest-Job-First (SJF). It adapts to different conditions and converges quickly, without any prior knowledge of system behavior. In [154], the authors use the actor-critic method to address the scheduling problem in a general-purpose distributed data stream processing systems, which deal with processing of continuous data \ufb02ow in real time or near-real-time. The system model contains multiple threads, processes, and machines. The system state consists of the current scheduling decision and the workload of each data source. The scheduling problem is to assign each thread to a process of a machine. The agent at the scheduler determines the assignment of each thread, with the objective of minimizing the average processing time. The DRL framework includes three components, i.e., an actor network, an optimizer producing a K-NN set of the actor network\u2019s output action, and the critic network predicting the Q-value for each action in the set. The action is selected from the K-NN set with the maximum Q-value. The use of optimizer may avoid unstable learning and divergence problems in conventional actor-critic methods [155]. C. Power Control and Data Collection With the prevalence of IoT and smart mobile devices, mobile crowdsensing becomes a cost-effective solution for network information collection to support more intelligent operations of wireless systems. The authors in [156] consider spectrum sensing and power control in non-cooperative cognitive radio networks. There is no information exchange between PUs and SUs. As such, the SU outsources the sensing task to a set of spatially distributed sensing devices to collect information about the PU\u2019s power control strategy. The SU\u2019s power control can be formulated as an MDP. The system state is determined by the Received Signal Strength (RSS) at individual sensing devices. The SU chooses its transmit power from the set of pre-speci\ufb01ed power levels based on the current state. A reward is obtained if both primary and SUs can ful\ufb01ll their SNR requirements. Considering the randomness in RSS measurements, the authors propose a DQL scheme for the SU to learn and adjust its transmit power. The DQL is then implemented by a DQN by using FNN. The simulation results show that the proposed DQL scheme is able to converge to a close-to-optimal solution. The authors in [157] leverage the DQL framework for sensing and control problems in a Wireless Sensor and Actor Network (WSAN), which is a group of wireless devices with the ability to sense events and to perform actions based on the sensed data shared by all sensors. The system state includes processing power, mobility abilities, and functionalities of the actors and sensors. The mobile actor can choose its moving direction, networking, sensing and actuation policies to maximize the number of connected actor nodes and the number of sensing events. The authors in [158] focus on mobile crowdsensing paradigm, where data inference is incorporated to reduce sensing costs while maintaining the quality of sensing. The target sensing area is split into a set of cells. The objective of a sensing task is to collect data (e.g., temperature, air quality) in all the cells. A DQL-based cell selection mechanism is proposed for the mobile sensors to decide which cell is a better choice to perform sensing tasks. The system state includes the selection matrices for a few past decision epochs. The reward function is determined by the sensing quality and cost in the chosen cells. To extract temporal correlations in learning, the authors propose the DRQN that uses LSTM layers in DQL 30 to capture the hidden patterns in state transitions. Considering inter-data correlations, the authors use the transfer learning method to reduce the amount of data in training. That is, the cell selection strategy learned for one task can bene\ufb01t another correlated task. Hence, the parameters of DRQN can be initialized by another DRQN with rich training data. Simulations are conducted based on two real-life datasets collected from sensor networks, i.e., the Sensor-Scope dataset [159] in the EPFL campus and the U-Air dataset of air quality readings in Beijing [160]. The experiments verify that DRQN reduces up to 15% of the sensed cells with the same inference quality guarantee. The authors in [161] combine UAV and unmanned vehicle in mobile crowdsensing for smart city applications. The UAV cruises in the above of the target region for city-level data collection. Meanwhile, the unmanned vehicle carrying mobile charging stations moves on the ground and can charge the UAV at a preset charging point. The target region is divided into multiple subregions and each subregion has a different sample priority. The authors in [161] propose a DQL-based control framework for the unmanned vehicle to schedule its data collection, constrained by limited energy supply. The system state includes information about the sample priority of each subregion, the location of charging point, and the moving trace of the UAV and unmanned vehicle. The UAV and unmanned vehicle can choose the moving direction. The DQL framework utilizes CNNs for extracting the correlation of adjacent subregions, which can increase the convergence speed in training. The DQL algorithm can be enhanced by using a feasible control solution as the baseline during exploration. The PER method is also used in DQL to assign higher priorities to important transitions so that the DQL agent can learn from samples more ef\ufb01ciently. The proposed scheme is evaluated by using real dataset of taxi traces in Rome [162]. Simulation results reveal that the proposed DQL algorithm can obtain the highest data collection rate compared with the MDP and other heuristic baselines. Mobile crowdsensing is vulnerable to faked sensing attacks, as sel\ufb01sh users may report faked sensing results to save their sensing costs and avoid compromising their privacy. The authors in [163] formulate the interactions between the server and a number of crowdsensing users as a Stackelberg game. The server is the leader that sets and broadcasts its payment policy for different sensing accuracy. In particular, the higher payment is set for more sensing accuracy. Based on the server\u2019s sensing policy, each user as a follower then chooses its sensing effort and thus the sensing accuracy to receive the payment. The payment motivates the users to put in sensing efforts and thus the payment decision process can be modeled as an MDP. In a dynamic network, the server uses the DQL to derive the optimal payment to maximize its utility, based on the system state consisting of the previous sensing quality and the payment policy. The DQL uses a deep CNN to accelerate the learning process and improve the crowdsensing performance against sel\ufb01sh users. Simulation results show that the DQL-based scheme produces a higher sensing quality, lower attack rate, and higher utility of the server, exceeding those of both the Q-learning and the random payment strategies. Social networking is an important component of smart city applications. The authors in [164] aim to extract useful information by observing and analyzing the users\u2019 behaviors in social networking. One of the main dif\ufb01culties is that the social behaviors are usually fuzzy and divergent. The authors model pervasive social networking as a monopolistically competitive market, which contains different users as data providers selling information at a certain price. Given the market model, the DQL can be used to estimate the users\u2019 behavior patterns and \ufb01nd the market equilibrium. Considering the costly deep learning structure, the authors in [164] propose a Decentralized DRL (DDRL) framework that decomposes the costly deep component from the RL algorithms at individual users. The deep component can be a feature extractor integrated with the network infrastructure and provide mutual knowledge for all individuals. Multiple RL agents can purchase the most desirable data from the mutual knowledge. The authors combine well-known RL algorithms, i.e., Q-learning and learning automata, to estimate users\u2019 patterns which are described by vectors of probabilities representing the users\u2019 preferences or altitudes to different information. In social networking and smart city applications with human involvement, there can be both labeled and unlabeled data and hence a semisupervised DRL framework can be designed, by combining the strengths of DNNs and statistical modeling to improve the performance and accuracy in learning. Then, the authors in [165] introduce the semi-supervised DRL framework that utilizes variational auto-encoders [166] as an inference engine to infer the classi\ufb01cation of unlabeled data. As a case study, the proposed DRL framework is customized to provide indoor localization based on the RSS from Bluetooth devices. The positioning environment contains a set of positions. Each position is associated with the set of RSS values from the set of anchor devices with known positions. The system state includes a vector of RSS values, the current location, and the distance to the target. The DQL agent, i.e., the positioning algorithm itself, chooses a moving direction to minimize the error distance to the target point. Simulations tested on realworld dataset show an improvement of 23% in terms of the error distance to the target compared with the supervised DRL scheme. Summary: In this section, we review miscellaneous uses of DRL in wireless and networked systems. DRL provides a \ufb02exible tool in rich and diversi\ufb01ed applications, conventionally involving dynamic system modeling and multi-agent interactions. All these imply a huge space of state transitions and actions. These approaches are summarized along with the references in Table VI. We observe that the NUM problems in 5G ecosystem for traf\ufb01c engineering and resource allocation face very diversi\ufb01ed control variables, including discrete indicators, e.g., for BS (de)activation, user/cell association, and path selection, as well as continuous variables such as bandwidth allocation, transmit power, and beamforming optimization. Hence, both DQL and policy gradient methods are used extensively for discrete and continuous control problems, respectively. 31 TABLE VI: A summary of applications of DQL for traf\ufb01c engineering, resource scheduling, and data collection. ISSUES REF. MODEL LEARNING ALGORITHMS AGENT STATES ACTIONS REWARDS SCENARIOS Traf\ufb01c engineering and routing [133] MDP DQN using actor-critic networks Network controller Bandwidth request of each node pair Traf\ufb01c load split on different paths Mean network delay 5G network [136] NUM DQN using actor-critic networks Network controller Throughput and delay performance Traf\ufb01c load split on different paths \u03b1-fairness utility 5G network [130] POMDP DQN using actor-critic networks UAV Local sensory information, e.g., distances and angles Turn left or right Composite reward UAV navigation [141] [142] Game DQN using ESN UAV Coordinates, distances, and orientation angles Path, transmit power, and cell association Weighted sum of energy ef\ufb01ciency, latency, and interference Cellularconnected UAVs [143] MDP DQN using FNN Train scheduler Channel conditions, train position, speed, SNR, and handoff indicator Making handoff of connection, or accelerate or decelerate the train Tracking error and energy consumption Vehicle-toinfrastructure system Resource sharing and scheduling [145] MDP DQN using FNN Cloud baseband unit MUs\u2019 demands and the RRHs\u2019 working states Turn on or off certain RRH(s), and beamforming allocation Expected power consumption Cloud RAN [149] MDP DQN with CNN Network controller Network topology, QoS/QoE status, and the QoS requirements Successive VNF instance Composite function of QoE gain, QoS constraints penalty, and OPEX penalty Cellular system [152] MDP DQN using FNN Network controller The number of arrived packets/the priority and time-stamp of \ufb02ows Bandwidth/SFC allocation Weighted sum of spectrum ef\ufb01ciency and QoE/waiting time in SFCs 5G network [154] MDP DQN using actor-critic networks Central scheduler Current scheduling decision and the workload Assignment of each thread Average processing time Distributed stream data processing Data collection [156] MDP DQN using FNN Secondary user Received signal strength at individual sensors Transmit power Fixed reward if QoS satis\ufb01ed CRN [158] MDP DRQN, LSTM, transfer learning Mobile sensors Cell selection matrices Next cell for sensing A function of the sensing quality and cost WSN [161] MDP DQN using CNN UAV and unmanned vehicle Subregions\u2019 sample priority, charging point\u2019s location, and trace of the UAV and unmanned vehicle Moving direction of the UAV and unmanned vehicle Fixed reward related to subregions\u2019 sample priority UAV and vehicle [163] Game DQN using CNN Crowdsensing server Previous sensing quality and payment policy Current payment policy Utility Mobile crowdsensing [164] Game DDQN Mobile users Current preferences Positive or negative altitude Reward or pro\ufb01t Mobile social network VII. CHALLENGES, OPEN ISSUES, AND FUTURE RESEARCH DIRECTIONS Different approaches reviewed in this survey evidently show that DRL can effectively address various emerging issues in communications and networking. There are existing challenges, open issues, and new research directions which are discussed as follows. A. Challenges 1) State Determination in Density Networks: The DRL approaches, e.g., [23], allow the users to \ufb01nd an optimal access policy without having complete and/or accurate network information. However, the DRL approaches often require the users to report their local states at every time slot. To observe the local state, the user needs to monitor Received Signal Strength Indicators (RSSIs) from its neighboring BSs, and then it temporarily connects to the BS with the maximum RSSI. However, the future networks will deploy a high density of the BSs, and the RSSIs from different BSs may not be different. Thus, it is challenging for the users to determine the temporary BS [167]. 2) Knowledge of Jammers\u2019 Channel Information: The DRL approach for wireless security as proposed in [118] enables the UAV to \ufb01nd optimal transmit power levels to maximize the security capacity of the UAV and the BS. However, to formulate the reward of the UAV, a perfect knowledge of channel information of the jammers is required. This is challenging and even impossible in practice. 3) Multi-agent DRL in Dynamic HetNets: Most of the existing works focus on the customizations of DRL framework for individual network entities, based on locally observed or exchanged network information. Hopefully, the network environment is relatively static to ensure convergent learning results and stable policies. This requirement may be challenged 32 in a dynamic heterogenous 5G network, which consists of hierarchically nested IoT devices/networks with fast changing service requirements and networking conditions. In such a situation, the DQL agents for individual entities have to be light-weighted and agile to the change of network conditions. This implies a reduce to the state and action spaces in learning, which however may compromise the performance of the convergent policy. The interactions among multiple agents also complicate the network environment and cause a considerable increase to the state space, which inevitably slows down the learning algorithms. 4) Training and Performance Evaluation of DRL Framework: The DRL framework requires large amounts of data for both training and performance evaluation. In wireless systems, such data is not easily accessible as we rarely have referential data pools as other deep learning scenarios, e.g., computer vision. Most of the existing works rely on simulated dataset, which undermines the con\ufb01dence of the DRL framework in practical system. The simulated data set is usually generated by a speci\ufb01c stochastic model, which is a simpli\ufb01cation of the real system and may overlook the hidden patterns. Hence, a more effective way for generating simulation data is required to ensure that the training and performance evaluation of the DRL framework are more consistent with practical system. B. Open Issues 1) Distributed DRL Framework in Wireless Networks: The DRL framework requires large amounts of training for DNNs. This may be implemented at a centralized network controller, which has suf\ufb01cient computational capacity and the capability for information collection. However, for massive end users with limited capabilities, it becomes a meaningful task to design distributed implementation for the DRL framework that decomposes resource-demanding basic functionalities, e.g., information collection, sharing, and DNN training, from reinforcement learning algorithms at individual devices. The basic functionalities can be integrated with the network controller. It remains an open issue for the design of network infrastructure that supports these common functionalities for distributed DRL. The overhead of information exchange between end users and network controller also has to be well controlled. 2) Balance between Information Quality and Learning Performance: The majority of the existing works consider the orchestration of networking, transmission control, of\ufb02oading, and caching decisions in one DRL framework to derive the optimal policy, e.g., [84]\u2013[89], [91], [92]. However, from a practical viewpoint, the network system will have to pay substantially increasing cost for information gathering. The cost is incurred from large delay, pre-processing of asynchronous information, excessive energy consumption, reduced learning speed, etc. Hence, an open issue is to \ufb01nd the optimal balance between information quality and learning performance so that the DQL agent does not consume too much resources only to achieve insigni\ufb01cantly marginal increase in the learning performance. C. Future Research Directions 1) DRL for Channel Estimation in Wireless Systems: Massive MIMO will be deployed for 5G to achieve high-speed communications at Gbps. For this, the channel estimation is the prerequisite for realizing massive MIMO. However, in a large-scale heterogeneous cellular network foreseen for 5G or beyond, the required channel estimation is very challenging. Thus, DRL will play an important role in acquiring the channel estimates with regard to dynamic time-varying wireless channels. Also, we expect that the combination of Wireless Power Transfer (WPT) and Mobile Crowd Sensing (MCS), namely Wireless-Powered Crowd Sensing (WPCS) will be a promising technique for the emerging IoT services. To this end, a higher power transfer ef\ufb01ciency of WPT is very critical to enable the deployment of WPCS in low-power wide area network. A \"large-scale array antenna based WPT\" will achieve this goal of higher WPT ef\ufb01ciency, but the channel estimation should be performed with minimal power consumption at a sensor node. This is because of that the sensor must operate with self-powering via WPT from the dedicated energy source, e.g., power beacon, Wi-Fi or small-cell access point, and/or ambient RF sources, e.g., TV tower, Wi-Fi AP and cellular BS. In this regard, the channel estimation based on the receive power measurements at the sensor node is one viable solution, because the receive power can be measured by the passivecircuit power meter with negligible power consumption. DRL can be used for the time-varying wireless channels with temporal correlations over time by taking the receive power measurements from the sensor node as the input for DRL, which will enable the channel estimation for WPT ef\ufb01ciently. 2) DRL for Crowdsensing Service Optimization: In MCS, mobile users contribute sensing data to a crowdsensing service provider and receive an incentive in return. However, due to limited resources, e.g., bandwidth and energy, the mobile user has to decide on whether and how much data to be uploaded to the provider. Likewise, the provider aiming to maximize its pro\ufb01t has to determine the amount of incentive to be given. The provider\u2019s decision depends on the actions of the mobile users. For example, with many mobile users contributing data to the crowdsensing service provider, the provider can lower the incentive. Due to a large state space of a large number of users and dynamic environment, DRL can be applied to obtain an optimal crowdsensing policy similar to [168]. 3) DRL for Cryptocurrency Management in Wireless Networks: Pricing and economic models have been widely applied to wireless networks [169], [170]. For example, wireless users pay money to access radio resources or mobile services. Alternatively, the users can receive money if they contribute to the networks, e.g., offering a relay or cache function. However, using real money and cash in such scenarios faces many issues related to accounting, security, and privacy. Recently, the concept of cryptocurrency based on the blockchain technology has been introduced and adopted in wireless networks, e.g., [171], which has been shown to be a secure and ef\ufb01cient solution. However, the value of cryptocurrency, i.e., token or coin, can be highly dynamic depending on many market ",
    "References": "REFERENCES [1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press Cambridge, 1998. [2] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning. MIT press Cambridge, 2016. [3] (2016, Jan.) Google achieves ai \u201cbreakthrough\u201d by beating go champion. BBC. [Online]. Available: https://www.bbc.com/news/ technology-35420579 [4] M. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. [5] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas, Dynamic programming and optimal control. Athena scienti\ufb01c Belmont, MA, 2005, vol. 1, no. 3. [6] R. Bellman, Dynamic programming. Mineola, NY: Courier Corporation, 2013. [7] Y. Li, \u201cDeep reinforcement learning: An overview,\u201d arXiv preprint arXiv:1701.07274, 2017. [8] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, \u201cA brief survey of deep reinforcement learning,\u201d IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 26\u201338, Nov. 2017. [9] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, \u201cMachine learning for wireless networks with arti\ufb01cial intelligence: A tutorial on neural networks,\u201d arXiv preprint arXiv:1710.02913, 2017. [10] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch, \u201cMulti-agent actor-critic for mixed cooperative-competitive environments,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 6379\u20136390. [11] G. E. Monahan, \u201cState of the art-a survey of partially observable markov decision processes: theory, models, and algorithms,\u201d Management Science, vol. 28, no. 1, pp. 1\u201316, 1982. [12] L. S. Shapley, \u201cStochastic games,\u201d Proceedings of the national academy of sciences, vol. 39, no. 10, pp. 1095\u20131100, 1953. [13] J. Hu and M. P. Wellman, \u201cNash q-learning for general-sum stochastic games,\u201d Journal of machine learning research, vol. 4, no. Nov, pp. 1039\u20131069, 2003. [14] W. C. Dabney, \u201cAdaptive step-sizes for reinforcement learning,\u201d Ph.D. dissertation, 2014. [15] C. J. Watkins and P. Dayan, \u201cQ-learning,\u201d Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992. [16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, no. 7540, p. 529, 2015. [17] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, \u201cAn ef\ufb01cient deep reinforcement learning model for urban traf\ufb01c control,\u201d arXiv preprint arXiv:1808.01876, 2018. [18] S. Gu, E. Holly, T. Lillicrap, and S. Levine, \u201cDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3389\u20133396. [19] S. Thrun and A. Schwartz, \u201cIssues in using function approximation for reinforcement learning,\u201d in Proceedings of Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum, 1993. [20] H. V. Hasselt, \u201cDouble q-learning,\u201d in Advances in Neural Information Processing Systems, 2010, pp. 2613\u20132621. [21] H. Van Hasselt, A. Guez, and D. Silver, \u201cDeep reinforcement learning with double q-learning.\u201d in AAAI, vol. 2, Phoenix, AZ, Feb. 2016, pp. 2094\u20132100. [22] O. Naparstek and K. Cohen, \u201cDeep multi-user reinforcement learning for dynamic spectrum access in multichannel wireless networks,\u201d arXiv preprint arXiv:1704.02613, 2017. [23] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, \u201cDeep reinforcement learning for user association and resource allocation in heterogeneous networks,\u201d in IEEE GLOBECOM, Abu Dhabi, UAE, Dec. 2018, pp. 1\u20136. [24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d arXiv preprint arXiv:1511.05952, 2015. [25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas, \u201cDueling network architectures for deep reinforcement learning,\u201d in International Conference on Machine Learning, New York, NY, Jun. 2016. [26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, \u201cAsynchronous methods for deep reinforcement learning,\u201d in International Conference on Machine Learning, New York City, New York, Jun. 2016, pp. 1928\u20131937. [27] Z. Wang, Y. Xu, L. Li, H. Tian, and S. Cui, \u201cHandover control in wireless systems via asynchronous multi-user deep reinforcement learning,\u201d arXiv preprint arXiv:1801.02077, 2018. [28] M. G. Bellemare, W. Dabney, and R. Munos, \u201cA distributional perspective on reinforcement learning,\u201d arXiv preprint arXiv:1707.06887, 2017. [29] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin et al., \u201cNoisy networks for exploration,\u201d in International Conference on Learning Representations, 2018. [30] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, \u201cRainbow: Combining improvements in deep reinforcement learning,\u201d in The Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d San Juan, Puerto Rico, USA, May 2016. [32] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, \u201cDeterministic policy gradient algorithms,\u201d in ICML, 2014. [33] M. Hausknecht and P. Stone, \u201cDeep recurrent q-learning for partially observable mdps,\u201d CoRR, abs/1507.06527, vol. 7, no. 1, 2015. [34] D. Zhao, H. Wang, K. Shao, and Y. Zhu, \u201cDeep reinforcement learning with experience replay based on sarsa,\u201d in IEEE Symposium Series on Computational Intelligence (SSCI), 2016, pp. 1\u20136. [35] W. Wang, J. Hao, Y. Wang, and M. Taylor, \u201cTowards cooperation in sequential prisoner\u2019s dilemmas: a deep multiagent reinforcement learning approach,\u201d arXiv preprint arXiv:1803.00162, 2018. [36] J. Heinrich and D. Silver, \u201cDeep reinforcement learning from self-play in imperfect-information games,\u201d arXiv preprint arXiv:1603.01121, 2016. [37] D. Fooladivanda and C. Rosenberg, \u201cJoint resource allocation and user association for heterogeneous wireless cellular networks,\u201d IEEE Transactions on Wireless Communications, vol. 12, no. 1, pp. 248\u2013257, 2013. 34 [38] Y. Lin, W. Bao, W. Yu, and B. Liang, \u201cOptimizing user association and spectrum allocation in hetnets: A utility perspective,\u201d IEEE Journal on Selected Areas in Communications, vol. 33, no. 6, pp. 1025\u20131039, 2015. [39] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, \u201cDeep reinforcement learning for dynamic multichannel access,\u201d in International Conference on Computing, Networking and Communications (ICNC), 2017. [40] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller, \u201cPlaying atari with deep reinforcement learning,\u201d CoRR, vol. abs/1312.5602, 2013. [41] R. Govindan. Tutornet: A low power wireless iot testbed. [Online]. Available: http://anrg.usc.edu/www/tutornet/ [42] Q. Zhao, B. Krishnamachari, and K. Liu, \u201cOn myopic sensing for multichannel opportunistic access: structure, optimality, and performance,\u201d IEEE Transactions on Wireless Communications, vol. 7, no. 12, pp. 5431\u20135440, December 2008. [43] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, \u201cDeep reinforcement learning for dynamic multichannel access in wireless networks,\u201d IEEE Transactions on Cognitive Communications and Networking, to appear. [44] J. Zhu, Y. Song, D. Jiang, and H. Song, \u201cA new deep-q-learningbased transmission scheduling mechanism for the cognitive internet of things,\u201d IEEE Internet of Things Journal, 2017. [45] M. Chu, H. Li, X. Liao, and S. Cui, \u201cReinforcement learning based multi-access control and battery prediction with energy harvesting in iot systems,\u201d arXiv preprint arXiv:1805.05929, 2018. [46] H. Ye and G. Y. Li, \u201cDeep reinforcement learning for resource allocation in v2v communications,\u201d arXiv preprint arXiv:1711.00968, 2017. [47] U. Challita, L. Dong, and W. Saad, \u201cProactive resource management in lte-u systems: A deep learning perspective,\u201d arXiv preprint arXiv:1702.07031, 2017. [48] M. Balazinska and P. Castro. (2003) Ibm watson research center. [Online]. Available: https://crawdad.org/ibm/watson/20030219 [49] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas, \u201cDueling network architectures for deep reinforcement learning,\u201d 2015. [50] H. Li, \u201cMultiagent learning for aloha-like spectrum access in cognitive radio systems,\u201d EURASIP Journal on Wireless Communications and Networking, vol. 2010, no. 1, pp. 1\u201315, May 2010. [51] S. Liu, X. Hu, and W. Wang, \u201cDeep reinforcement learning based dynamic channel allocation algorithm in multibeam satellite systems,\u201d IEEE ACCESS, vol. 6, pp. 15 733\u201315 742, 2018. [52] M. Chen, W. Saad, and C. Yin, \u201cLiquid state machine learning for resource allocation in a network of cache-enabled lte-u uavs,\u201d in IEEE GLOBECOM, 2017, pp. 1\u20136. [53] W. Maass, \u201cLiquid state machines: motivation, theory, and applications,\u201d in Computability in context: computation and logic in the real world. World Scienti\ufb01c, 2011, pp. 275\u2013296. [54] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong, \u201cCaching in the sky: Proactive deployment of cache-enabled unmanned aerial vehicles for optimized quality-of-experience,\u201d IEEE Journal on Selected Areas in Communications, vol. 35, no. 5, pp. 1046\u20131061, 2017. [55] I. Szita, V. Gyenes, and A. L\u02ddorincz, \u201cReinforcement learning with echo state networks,\u201d in International Conference on Arti\ufb01cial Neural Networks. Springer, 2006, pp. 830\u2013839. [56] Tyouku of china network video index. [Online]. Available: http: //index.youku.com/ [57] T. Stockhammer, \u201cDynamic adaptive streaming over http\u2013: standards and design principles,\u201d in Proceedings of the second annual ACM conference on Multimedia systems. ACM, 2011, pp. 133\u2013144. [58] M. Gadaleta, F. Chiariotti, M. Rossi, and A. Zanella, \u201cD-dash: A deep q-learning framework for dash video streaming,\u201d IEEE Transactions on Cognitive Communications and Networking, vol. 3, no. 4, pp. 703\u2013718, 2017. [59] J. Klaue, B. Rathke, and A. Wolisz, \u201cEvalvid\u2013a framework for video transmission and quality evaluation,\u201d in International conference on modelling techniques and tools for computer performance evaluation, 2003, pp. 255\u2013272. [60] H. Mao, R. Netravali, and M. Alizadeh, \u201cNeural adaptive video streaming with pensieve,\u201d in Proceedings of the Conference of the ACM Special Interest Group on Data Communication. ACM, 2017, pp. 197\u2013210. [61] H. Riiser, P. Vigmostad, C. Griwodz, and P. Halvorsen, \u201cCommute path bandwidth traces from 3g networks: analysis and applications,\u201d in Proceedings of the 4th ACM Multimedia Systems Conference. ACM, 2013, pp. 114\u2013118. [62] X. Yin, A. Jindal, V. Sekar, and B. Sinopoli, \u201cA control-theoretic approach for dynamic adaptive video streaming over http,\u201d in ACM SIGCOMM Computer Communication Review, vol. 45, no. 4. ACM, 2015, pp. 325\u2013338. [63] T. Huang, R.-X. Zhang, C. Zhou, and L. Sun, \u201cQarc: Video quality aware rate control for real-time video streaming based on deep reinforcement learning,\u201d arXiv preprint arXiv:1805.02482, 2018. [64] (2016) Measuring \ufb01xed broadband report. [Online]. Available: https: //www.fcc.gov/reports-research/reports/measuring-broadband-america/ raw-data-measuring-broadband-america-2016 [65] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra, M. Pavone, and K. Sachin, \u201cCellular network traf\ufb01c scheduling with deep reinforcement learning,\u201d in National Conference on Arti\ufb01cial Intelligence (AAAI), 2018. [66] Z. Zhang, Y. Zheng, M. Hua, Y. Huang, and L. Yang, \u201cCache-enabled dynamic rate allocation via deep self-transfer reinforcement learning,\u201d arXiv preprint arXiv:1803.11334, 2018. [67] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett, S. G. Bil\u00e9n, R. C. Reinhart, and D. J. Mortensen, \u201cMulti-objective reinforcement learning for cognitive satellite communications using deep neural network ensembles,\u201d IEEE Journal on Selected Areas in Communications, 2018. [68] D. Tarchi, G. E. Corazza, and A. Vanelli-Coralli, \u201cAdaptive coding and modulation techniques for next generation hand-held mobile satellite communications,\u201d in IEE ICC, 2013, pp. 4504\u20134508. [69] M. T. Hagan and M. B. Menhaj, \u201cTraining feedforward networks with the marquardt algorithm,\u201d IEEE transactions on Neural Networks, vol. 5, no. 6, pp. 989\u2013993, 1994. [70] C. Zhong, M. C. Gursoy, and S. Velipasalar, \u201cA deep reinforcement learning-based framework for content caching,\u201d arXiv preprint arXiv:1712.08132, 2017. [71] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d CoRR, vol. abs/1509.02971, 2015. [Online]. Available: http://arxiv.org/abs/1509.02971 [72] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, \u201cReinforcement learning in large discrete action spaces,\u201d CoRR, vol. abs/1512.07679, 2015. [Online]. Available: http: //arxiv.org/abs/1512.07679 [73] L. Lei, L. You, G. Dai, T. X. Vu, D. Yuan, and S. Chatzinotas, \u201cA deep learning approach for optimizing content delivering in cache-enabled HetNet,\u201d in Int\u2019l Sym. Wireless Commun. Systems (ISWCS), Aug. 2017, pp. 449\u2013453. [74] M. Schaarschmidt, F. Gessert, V. Dalibard, and E. Yoneki, \u201cLearning runtime parameters in computer systems with delayed experience injection,\u201d arXiv preprint arXiv:1610.09903, 2016. [75] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears, \u201cBenchmarking cloud serving systems with YCSB,\u201d in proc. 1st ACM Sym. Cloud Comput., 2010, pp. 143\u2013154. [76] Y. He and S. Hu, \u201cCache-enabled wireless networks with opportunistic interference alignment,\u201d arXiv preprint arXiv:1706.09024, 2017. [77] Y. He, C. Liang, F. R. Yu, N. Zhao, and H. Yin, \u201cOptimization of cache-enabled opportunistic interference alignment wireless networks: A big data deep reinforcement learning approach,\u201d in IEEE ICC, 2017, pp. 1\u20136. [78] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, and Y. Zhang, \u201cDeep-reinforcement-learning-based optimization for cacheenabled opportunistic interference alignment wireless networks,\u201d IEEE Transactions on Vehicular Technology, vol. 66, no. 11, pp. 10 433\u2013 10 445, 2017. [79] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, \u201cGreen resource allocation based on deep reinforcement learning in contentcentric iot,\u201d IEEE Transactions on Emerging Topics in Computing, to appear. [80] Q. Wu, Z. Li, and G. Xie, \u201cCodingCache: Multipath-aware CCN cache with network coding,\u201d in proc. ACM SIGCOMM Workshop on Information-centric Networking, 2013, pp. 41\u201342. [81] M. Chen, W. Saad, and C. Yin, \u201cEcho-liquid state deep learning for 360 content transmission and caching in wireless vr networks with cellular-connected uavs,\u201d arXiv preprint arXiv:1804.03284, 2018. [82] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, \u201cMachine learning for wireless networks with arti\ufb01cial intelligence: A tutorial 35 on neural networks,\u201d CoRR, vol. abs/1710.02913, 2017. [Online]. Available: http://arxiv.org/abs/1710.02913 [83] M. Chen, W. Saad, and C. Yin, \u201cLiquid state machine learning for resource allocation in a network of cache-enabled LTE-U UAVs,\u201d in IEEE GLOBECOM, Dec. 2017. [84] Y. He, Z. Zhang, and Y. Zhang, \u201cA big data deep reinforcement learning approach to next generation green wireless networks,\u201d in IEEE GLOBECOM, 2017, pp. 1\u20136. [85] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, and Y. Zhang, \u201cResource allocation in software-de\ufb01ned and information-centric vehicular networks with mobile edge computing,\u201d in IEEE Vehicular Technology Conference, 2017, pp. 1\u20135. [86] Y. He, F. R. Yu, N. Zhao, H. Yin, and A. Boukerche, \u201cDeep reinforcement learning (drl)-based resource management in software-de\ufb01ned and virtualized vehicular ad hoc networks,\u201d in Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications, 2017, pp. 47\u201354. [87] Y. He, N. Zhao, and H. Yin, \u201cIntegrated networking, caching, and computing for connected vehicles: A deep reinforcement learning approach,\u201d IEEE Transactions on Vehicular Technology, vol. 67, no. 1, pp. 44\u201355, 2018. [88] T. L. Thanh and R. Q. Hu, \u201cMobility-aware edge caching and computing framework in vehicle networks: A deep reinforcement learning,\u201d IEEE Transactions on Vehicular Technology, to appear. [89] Y. He, F. R. Yu, N. Zhao, V. C. Leung, and H. Yin, \u201cSoftware-de\ufb01ned networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach,\u201d IEEE Communications Magazine, vol. 55, no. 12, pp. 31\u201337, 2017. [90] B. Han, V. Gopalakrishnan, L. Ji, and S. Lee, \u201cNetwork function virtualization: Challenges and opportunities for innovations,\u201d IEEE Communications Magazine, vol. 53, no. 2, pp. 90\u201397, 2015. [91] Y. He, F. R. Yu, N. Zhao, and H. Yin, \u201cSecure social networks in 5g systems with mobile edge computing, caching and device-to-device (d2d) communications,\u201d IEEE Wireless Communications, vol. 25, no. 3, pp. 103\u2013109, Jun. 2018. [92] Y. He, C. Liang, F. R. Yu, and Z. Han, \u201cTrust-based social networks with computing, caching and communications: A deep reinforcement learning approach,\u201d IEEE Transactions on Network Science and Engineering, to appear. [93] C. Zhang, Z. Liu, B. Gu, K. Yamori, and Y. Tanaka, \u201cA deep reinforcement learning based approach for cost-and energy-aware multi\ufb02ow mobile data of\ufb02oading,\u201d IEICE Transactions on Communications, pp. 2017\u20132025. [94] L. Ji, G. Hui, L. Tiejun, and L. Yueming, \u201cDeep reinforcement learning based computation of\ufb02oading and resource allocation for mec,\u201d in IEEE WCNC, 2018, pp. 1\u20135. [95] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, \u201cPerformance optimization in mobile-edge computing via deep reinforcement learning,\u201d arXiv preprint arXiv:1804.00514, 2018. [96] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, \u201cOptimized computation of\ufb02oading performance in virtual edge computing systems via deep reinforcement learning,\u201d arXiv preprint arXiv:1805.06146, 2018. [97] J. Ye and Y.-J. A. Zhang, \u201cDRAG: Deep reinforcement learning based base station activation in heterogeneous networks,\u201d arXiv:1809.02159, Sep. 2018. [98] H. Li, H. Gao, T. Lv, and Y. Lu, \u201cDeep q-learning based dynamic resource allocation for self-powered ultra-dense networks,\u201d in IEEE ICC (ICC Workshops), 2018, pp. 1\u20136. [99] J. Liu, B. Krishnamachari, S. Zhou, and Z. Niu, \u201cDeepnap: Data-driven base station sleeping operations through deep reinforcement learning,\u201d IEEE Internet of Things Journal, 2018. [100] X. Wan, G. Sheng, Y. Li, L. Xiao, and X. Du, \u201cReinforcement learning based mobile of\ufb02oading for cloud-based malware detection,\u201d in IEEE GLOBECOM, 2017, pp. 1\u20136. [101] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, and M. Guizani, \u201cSecurity in mobile edge caching with reinforcement learning,\u201d CoRR, vol. abs/1801.05915, 2018. [Online]. Available: http://arxiv.org/abs/1801. 05915 [102] A. S. Shamili, C. Bauckhage, and T. Alpcan, \u201cMalware detection on mobile devices using distributed machine learning,\u201d in proc. Int\u2019l Conf. Pattern Recognition, Aug. 2010, pp. 4348\u20134351. [103] Y. Li, J. Liu, Q. Li, and L. Xiao, \u201cMobile cloud of\ufb02oading for malware detections with learning,\u201d in IEEE INFOCOM Workshops, Apr. 2015, pp. 197\u2013201. [104] M. Min, D. Xu, L. Xiao, Y. Tang, and D. Wu, \u201cLearning-based computation of\ufb02oading for iot devices with energy harvesting,\u201d arXiv preprint arXiv:1712.08768, 2017. [105] L. Quan, Z. Wang, and F. Ren, \u201cA novel two-layered reinforcement learning for task of\ufb02oading with tradeoff between physical machine utilization rate and delay,\u201d Future Internet, vol. 10, no. 7, 2018. [106] D. V. Le and C. Tham, \u201cQuality of service aware computation of\ufb02oading in an ad-hoc mobile cloud,\u201d IEEE Transactions on Vehicular Technology, pp. 1\u20131, 2018. [107] D. V. Le and C.-K. Tham, \u201cA deep reinforcement learning based of\ufb02oading scheme in ad-hoc mobile clouds,\u201d in Proceedings of IEEE INFOCOM IECCO Workshop, Honolulu, USA., apr 2018. [108] S. Yu, X. Wang, and R. Langar, \u201cComputation of\ufb02oading for mobile edge computing: A deep learning approach,\u201d in IEEE PIMRC, Oct. 2017. [109] Z. Tang, X. Zhou, F. Zhang, W. Jia, and W. Zhao, \u201cMigration modeling and learning algorithms for containers in fog computing,\u201d IEEE Transactions on Services Computing, 2018. [110] P. Popovski, H. Yomo, and R. Prasad, \u201cStrategies for adaptive frequency hopping in the unlicensed bands,\u201d IEEE Wireless Communications, vol. 13, no. 6, pp. 60\u201367, 2006. [111] G. Han, L. Xiao, and H. V. Poor, \u201cTwo-dimensional anti-jamming communication based on deep reinforcement learning,\u201d in Proceedings of the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing,, 2017. [112] L. Xiao, D. Jiang, X. Wan, W. Su, and Y. Tang, \u201cAnti-jamming underwater transmission with mobility and learning,\u201d IEEE Communications Letters, 2018. [113] X. Liu, Y. Xu, L. Jia, Q. Wu, and A. Anpalagan, \u201cAnti-jamming communications using spectrum waterfall: A deep reinforcement learning approach,\u201d IEEE Communications Letters, 2018. [114] Y. Chen, Y. Li, D. Xu, and L. Xiao, \u201cDqn-based power control for iot transmission against jamming,\u201d in IEEE 87th Vehicular Technology Conference (VTC Spring), 2018, pp. 1\u20135. [115] X. Lu, L. Xiao, and C. Dai, \u201cUav-aided 5g communications with deep reinforcement learning against jamming,\u201d arXiv preprint arXiv:1805.06628, 2018. [116] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, and W. Zhuang, \u201cUav relay in vanets against smart jamming with reinforcement learning,\u201d IEEE Transactions on Vehicular Technology, vol. 67, no. 5, pp. 4087\u20134097, 2018. [117] S. Lv, L. Xiao, Q. Hu, X. Wang, C. Hu, and L. Sun, \u201cAnti-jamming power control game in unmanned aerial vehicle networks,\u201d in IEEE GLOBECOM, 2017, pp. 1\u20136. [118] L. Xiao, C. Xie, M. Min, and W. Zhuang, \u201cUser-centric view of unmanned aerial vehicle transmission against smart attacks,\u201d IEEE Transactions on Vehicular Technology, vol. 67, no. 4, pp. 3420\u20133430, 2018. [119] M. Bowling and M. Veloso, \u201cMultiagent learning using a variable learning rate,\u201d Arti\ufb01cial Intelligence, vol. 136, no. 2, pp. 215\u2013250, 2002. [120] Y. Chen, S. Kar, and J. M. Moura, \u201cCyber-physical attacks with control objectives,\u201d IEEE Transactions on Automatic Control, vol. 63, no. 5, pp. 1418\u20131425, 2018. [121] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, \u201cRobust deep reinforcement learning for security and safety in autonomous vehicle systems,\u201d arXiv preprint arXiv:1805.00983, 2018. [122] M. Brackstone and M. McDonald, \u201cCar-following: a historical review,\u201d Transportation Research Part F: Traf\ufb01c Psychology and Behaviour, vol. 2, no. 4, pp. 181\u2013196, 1999. [123] A. Ferdowsi and W. Saad, \u201cDeep learning-based dynamic watermarking for secure signal authentication in the internet of things,\u201d in IEEE ICC, 2018, pp. 1\u20136. [124] B. Satchidanandan and P. R. Kumar, \u201cDynamic watermarking: Active defense of networked cyber\u2013physical systems,\u201d Proceedings of the IEEE, vol. 105, no. 2, pp. 219\u2013240, 2017. [125] A. Ferdowsi and W. Saad, \u201cDeep learning for signal authentication and security in massive internet of things systems,\u201d arXiv preprint arXiv:1803.00916, 2018. [126] P. Vadakkepat, K. C. Tan, and W. Ming-Liang, \u201cEvolutionary arti\ufb01cial potential \ufb01elds and their application in real time robot path planning,\u201d in Proceedings of the 2000 Congress on Evolutionary Computation, vol. 1, 2000, pp. 256\u2013263. [127] W. Huang, Y. Wang, and X. Yi, \u201cDeep q-learning to preserve connectivity in multi-robot systems,\u201d in Proceedings of the 9th International Conference on Signal Processing Systems. ACM, 2017, pp. 45\u201350. 36 [128] W. Huang, Y. Wang, and X. Yi, \u201cA deep reinforcement learning approach to preserve connectivity for multi-robot systems,\u201d in International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2017, pp. 1\u20137. [129] H. A. Poonawala, A. C. Satici, H. Eckert, and M. W. Spong, \u201cCollisionfree formation control with decentralized connectivity preservation for nonholonomic-wheeled mobile robots,\u201d IEEE Transactions on control of Network Systems, vol. 2, no. 2, pp. 122\u2013130, 2015. [130] C. Wang, J. Wang, X. Zhang, and X. Zhang, \u201cAutonomous navigation of uav in large-scale unknown complex environment with deep reinforcement learning,\u201d in IEEE GlobalSIP, 2017, pp. 858\u2013862. [131] C. Shen, C. Tekin, and M. van der Schaar, \u201cA non-stochastic learning approach to energy ef\ufb01cient mobility management,\u201d IEEE Journal on Selected Areas in Communications, vol. 34, no. 12, pp. 3854\u20133868, 2016. [132] M. Faris and E. Brian, \u201cDeep q-learning for self-organizing networks fault management and radio performance improvement,\u201d https://arxiv.org/abs/1707.02329, 2018. [133] G. Stampa, M. Arias, D. Sanchez-Charles, V. Muntes-Mulero, and A. Cabellos, \u201cA deep-reinforcement learning approach for software-de\ufb01ned networking routing optimization,\u201d arXiv preprint arXiv:1709.07080, 2017. [134] M. Roughan, \u201cSimplifying the synthesis of internet traf\ufb01c matrices,\u201d ACM SIGCOMM Computer Communication Review, vol. 35, no. 5, pp. 93\u201396, 2015. [Online]. Available: http://arxiv.org/abs/1710.02913 [135] A. Varga and R. Hornig, \u201cAn overview of the OMNeT++ simulation environment,\u201d in proc. Int\u2019l Conf. Simulation Tools and Techniques for Communications, Networks and Systems & Workshops, 2008. [136] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang, \u201cExperience-driven networking: A deep reinforcement learning based approach,\u201d arXiv preprint arXiv:1801.05757, 2018. [137] K. Winstein and H. Balakrishnan, \u201cTCP ex Machina: Computergenerated congestion control,\u201d in ACM SIGCOMM, 2013, pp. 123\u2013134. [138] R. G.F. and H. T.R., Modeling and Tools for Network Simulation. Springer, Berlin, Heidelberg, 2010, ch. The ns-3 Network Simulator. [139] A. Medina, A. Lakhina, I. Matta, and J. Byers, \u201cBRITE: an approach to universal topology generation,\u201d in IEEE MASCOTS, Aug. 2001, pp. 346\u2013353. [140] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, \u201cPolicy gradient methods for reinforcement learning with function approximation,\u201d in proc. 12th Int\u2019l Conf. Neural Inform. Process. Syst., 1999, pp. 1057\u2013 1063. [141] U. Challita, W. Saad, and C. Bettstetter, \u201cDeep reinforcement learning for interference-aware path planning of cellular connected uavs,\u201d in IEEE ICC, Kansas City, MO, May 2018, pp. 1\u20136. [142] U. Challita, W. Saad, and C. Bettstetter, \u201cCellular-connected uavs over 5g: Deep reinforcement learning for interference management,\u201d arXiv preprint arXiv:1801.05500, 2018. [143] L. Zhu, Y. He, F. R. Yu, B. Ning, T. Tang, and N. Zhao, \u201cCommunication-based train control system performance optimization using deep reinforcement learning,\u201d IEEE Transactions on Vehicular Technology, vol. 66, no. 12, pp. 10 705\u201310 717, 2017. [144] Y. Yang, Y. Li, K. Li, S. Zhao, R. Chen, J. Wang, and S. Ci, \u201cDecco: Deep-learning enabled coverage and capacity optimization for massive mimo systems,\u201d IEEE Access, to appear. [145] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, \u201cA deep reinforcement learning based framework for power-ef\ufb01cient resource allocation in cloud rans,\u201d in IEEE ICC, 2017, pp. 1\u20136. [146] T. M. Hackett, S. G. Bil\u00e9n, P. V. R. Ferreira, A. M. Wyglinski, and R. C. Reinhart, \u201cImplementation of a space communications cognitive engine,\u201d in Cognitive Communications for Aerospace Applications Workshop (CCAA), 2017, pp. 1\u20137. [147] Y. S. Nasir and D. Guo, \u201cDeep reinforcement learning for distributed dynamic power allocation in wireless networks,\u201d arXiv preprint arXiv:1808.00490, 2018. [148] X. Foukas, G. Patounas, A. Elmokash\ufb01, and M. K. Marina, \u201cNetwork slicing in 5g: Survey and challenges,\u201d IEEE Communications Magazine, vol. 55, no. 5, pp. 94\u2013100, 2017. [149] X. Chen, Z. Li, Y. Zhang, R. Long, H. Yu, X. Du, and M. Guizani, \u201cReinforcement learning based qos/qoe-aware service function chaining in software-driven 5g slices,\u201d arXiv preprint arXiv:1804.02099, 2018. [150] P. Reichl, S. Egger, R. Schatz, and A. D\u2019Alconzo, \u201cThe logarithmic nature of qoe and the role of the weber-fechner law in qoe assessment,\u201d in IEEE ICC, Cape Town, South Africa, May 2010, pp. 1\u20135. [151] M. Fiedler, T. Hossfeld, and P. Tran-Gia, \u201cA generic quantitative relationship between quality of experience and quality of service,\u201d IEEE Network, vol. 24, no. 2, pp. 36\u201341, 2014. [152] Z. Zhao, R. Li, Q. Sun, Y. Yang, X. Chen, M. Zhao, H. Zhang et al., \u201cDeep reinforcement learning for network slicing,\u201d arXiv preprint arXiv:1805.06591, 2018. [153] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, \u201cResource management with deep reinforcement learning,\u201d in Proceedings of the 15th ACM Workshop on Hot Topics in Networks, 2016, pp. 50\u201356. [154] T. Li, Z. Xu, J. Tang, and Y. Wang, \u201cModel-free control for distributed stream data processing using deep reinforcement learning,\u201d Proceedings of the VLDB Endowment, vol. 11, no. 6, pp. 705\u2013718, 2018. [155] G. D. Arnold, R. Evans, H. v. Hasselt, P. Sunehag, T. Lillicrap, J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, \u201cDeep reinforcement learning in large discrete action spaces,\u201d arXiv: 1512.07679, 2016. [156] X. Li, J. Fang, W. Cheng, H. Duan, Z. Chen, and H. Li, \u201cIntelligent power control for spectrum sharing in cognitive radios: A deep reinforcement learning approach,\u201d arXiv preprint arXiv:1712.07365, 2017. [157] T. Oda, R. Obukata, M. Ikeda, L. Barolli, and M. Takizawa, \u201cDesign and implementation of a simulation system based on deep q-network for mobile actor node control in wireless sensor and actor networks,\u201d in International Conference on Advanced Information Networking and Applications Workshops (WAINA), 2017, pp. 195\u2013200. [158] L. Wang, W. Liu, D. Zhang, Y. Wang, E. Wang, and Y. Yang, \u201cCell selection with deep reinforcement learning in sparse mobile crowdsensing,\u201d arXiv preprint arXiv:1804.07047, 2018. [159] F. Ingelrest, G. Barrenetxea, G. Schaefer, M. Vetterli, O. Couach, and M. Parlange., \u201cSensorScope: Application-speci\ufb01c sensor network for environmental monitoring,\u201d ACM Transactions on Sensor Networks, vol. 6, no. 2, pp. 1\u201332, 2010. [160] Y. Zheng, F. Liu, and H. P. Hsieh, \u201cU-Air: when urban air quality inference meets big data,\u201d in ACM SIGKDD Int\u2019l Conf. Knowledge Discovery and Data Mining, 2013. [161] B. Zhang, C. H. Liu, J. Tang, Z. Xu, J. Ma, and W. Wang, \u201cLearningbased energy-ef\ufb01cient data collection by unmanned vehicles in smart cities,\u201d IEEE Transactions on Industrial Informatics, vol. 14, no. 4, pp. 1666\u20131676, 2018. [162] L. Bracciale, M. Bonola, P. Loreti, G. Bianchi, R. Amici, and A. Rabuf\ufb01. (2014, Jul.) CRAWDAD dataset roma/taxi (v. 2014-0717). [Online]. Available: http://crawdad.org/roma/taxi/20140717 [163] L. Xiao, Y. Li, G. Han, H. Dai, and H. V. Poor, \u201cA secure mobile crowdsensing game with deep reinforcement learning,\u201d IEEE Transactions on Information Forensics and Security, vol. 13, no. 1, pp. 35\u201347, Jan. 2018. [164] Y. Zhang, B. Song, and P. Zhang, \u201cSocial behavior study under pervasive social networking based on decentralized deep reinforcement learning,\u201d Journal of Network and Computer Applications, vol. 86, pp. 72\u201381, 2017. [165] M. Mohammadi, A. Al-Fuqaha, M. Guizani, and J.-S. Oh, \u201cSemisupervised deep reinforcement learning in support of iot and smart city services,\u201d IEEE Internet of Things Journal, vol. 5, no. 2, pp. 624\u2013635, 2018. [166] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, \u201cSemisupervised learning with deep generative models,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 3581\u20133589. [167] G. Cao, Z. Lu, X. Wen, T. Lei, and Z. Hu, \u201cAif: An arti\ufb01cial intelligence framework for smart wireless network management,\u201d IEEE Communications Letters, vol. 22, no. 2, pp. 400\u2013403, 2018. [168] Y. Zhan, Y. Xia, J. Zhang, T. Li, and Y. Wang, \u201cCrowdsensing game with demand uncertainties: A deep reinforcement learning approach,\u201d submitted. [169] N. C. Luong, P. Wang, D. Niyato, Y. Wen, and Z. Han, \u201cResource management in cloud networking using economic analysis and pricing models: a survey,\u201d IEEE Communications Surveys & Tutorials, vol. 19, no. 2, pp. 954\u20131001, Jan. 2017. [170] N. C. Luong, D. T. Hoang, P. Wang, D. Niyato, D. I. Kim, and Z. Han, \u201cData collection and wireless communication in internet of things (iot) using economic analysis and pricing models: A survey,\u201d IEEE Communications Surveys & Tutorials, vol. 18, no. 4, pp. 2546\u20132590, Jun. 2016. [171] F. Shi, Z. Qin, and J. A. McCann, \u201cOppay: Design and implementation of a payment system for opportunistic data services,\u201d in IEEE International Conference on Distributed Computing Systems, Atlanta, GA, Jul. 2017, pp. 1618\u20131628. [172] Z. Jiang and J. Liang, \u201cCryptocurrency portfolio management with deep reinforcement learning,\u201d in Intelligent Systems Conference (IntelliSys), 2017, pp. 905\u2013913. [173] N. C. Luong, P. Wang, D. Niyato, Y.-C. Liang, F. Hou, and Z. Han, \u201cApplications of economic and pricing models for resource management in 37 5g wireless networks: A survey,\u201d IEEE Communications Surveys and Tutorials, to appear. [174] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He, \u201cDeep reinforcement learning for sponsored search real-time bidding,\u201d arXiv preprint arXiv:1803.00259, 2018. ",
    "title": "Applications of Deep Reinforcement Learning in",
    "paper_info": "1\nApplications of Deep Reinforcement Learning in\nCommunications and Networking: A Survey\nNguyen Cong Luong, Dinh Thai Hoang, Member, IEEE, Shimin Gong, Member, IEEE, Dusit Niyato, Fellow,\nIEEE, Ping Wang, Senior Member, IEEE, Ying-Chang Liang, Fellow, IEEE, Dong In Kim, Senior Member, IEEE\nAbstract\u2014This paper presents a comprehensive literature re-\nview on applications of deep reinforcement learning in com-\nmunications and networking. Modern networks, e.g., Internet\nof Things (IoT) and Unmanned Aerial Vehicle (UAV) networks,\nbecome more decentralized and autonomous. In such networks,\nnetwork entities need to make decisions locally to maximize the\nnetwork performance under uncertainty of network environment.\nReinforcement learning has been ef\ufb01ciently used to enable the\nnetwork entities to obtain the optimal policy including, e.g.,\ndecisions or actions, given their states when the state and\naction spaces are small. However, in complex and large-scale\nnetworks, the state and action spaces are usually large, and the\nreinforcement learning may not be able to \ufb01nd the optimal policy\nin reasonable time. Therefore, deep reinforcement learning, a\ncombination of reinforcement learning with deep learning, has\nbeen developed to overcome the shortcomings. In this survey,\nwe \ufb01rst give a tutorial of deep reinforcement learning from\nfundamental concepts to advanced models. Then, we review deep\nreinforcement learning approaches proposed to address emerging\nissues in communications and networking. The issues include\ndynamic network access, data rate control, wireless caching,\ndata of\ufb02oading, network security, and connectivity preservation\nwhich are all important to next generation networks such as\n5G and beyond. Furthermore, we present applications of deep\nreinforcement learning for traf\ufb01c routing, resource sharing,\nand data collection. Finally, we highlight important challenges,\nopen issues, and future research directions of applying deep\nreinforcement learning.\nKeywords- Deep reinforcement learning, deep Q-learning, net-\nworking, communications, spectrum access, rate control, security,\ncaching, data of\ufb02oading, data collection.\nI. INTRODUCTION\nReinforcement learning [1] is one of the most important\nresearch directions of machine learning which has signi\ufb01cant\nimpacts to the development of Arti\ufb01cial Intelligence (AI) over\nthe last 20 years. Reinforcement learning is a learning process\nin which an agent can periodically make decisions, observe\nthe results, and then automatically adjust its strategy to achieve\nthe optimal policy. However, this learning process, even though\nN. C. Luong and D. Niyato are with School of Computer Science\nand Engineering, Nanyang Technological University, Singapore. E-mails:\nclnguyen@ntu.edu.sg, dniyato@ntu.edu.sg.\nD. T. Hoang is with the Faculty of Engineering and Information Technology,\nUniversity of Technology Sydney, Australia. E-mail: hoang.dinh@uts.edu.au.\nS. Gong is with the Shenzhen Institute of Advanced Technology, Chinese\nAcademy of Sciences, Shenzhen 518055, China. E-mail: sm.gong@siat.ac.cn.\nP. Wang is with Department of Electrical Engineering & Computer Science,\nYork University, Canada. E-mail: pingw@yorku.ca.\nY.-C. Liang is with Center for Intelligent Networking and Communications\n(CINC), with University of Electronic Science and Technology of China,\nChengdu, China. E-mail: liangyc@ieee.org.\nD. I. Kim is with School of Information and Communication Engineering,\nSungkyunkwan University, Korea. Email: dikim@skku.ac.kr.\nproved to converge, takes a lot of time to reach the best policy\nas it has to explore and gain knowledge of an entire system,\nmaking it unsuitable and inapplicable to large-scale networks.\nConsequently, applications of reinforcement learning are very\nlimited in practice. Recently, deep learning [2] has been\nintroduced as a new breakthrough technique. It can overcome\nthe limitations of reinforcement learning, and thus open a new\nera for the development of reinforcement learning, namely\nDeep Reinforcement Learning (DRL). The DRL embraces\nthe advantage of Deep Neural Networks (DNNs) to train\nthe learning process, thereby improving the learning speed\nand the performance of reinforcement learning algorithms.\nAs a result, DRL has been adopted in a numerous applica-\ntions of reinforcement learning in practice such as robotics,\ncomputer vision, speech recognition, and natural language\nprocessing [2]. One of the most famous applications of DRL\nis AlphaGo [3], the \ufb01rst computer program which can beat a\nhuman professional without handicaps on a full-sized 19\u00d719\nboard.\nIn the areas of communications and networking, DRL\nhas been recently used as an emerging tool to effectively\naddress various problems and challenges. In particular, modern\nnetworks such as Internet of Things (IoT), Heterogeneous\nNetworks (HetNets), and Unmanned Aerial Vehicle (UAV)\nnetwork become more decentralized, ad-hoc, and autonomous\nin nature. Network entities such as IoT devices, mobile users,\nand UAVs need to make local and autonomous decisions, e.g.,\nspectrum access, data rate selection, transmit power control,\nand base station association, to achieve the goals of different\nnetworks including, e.g., throughput maximization and energy\nconsumption minimization. Under uncertain and stochastic\nenvironments, most of the decision-making problems can be\nmodeled by a so-called Markov Decision Process (MDP) [4].\nDynamic programming [5], [6] and other algorithms such\nas value iteration, as well as reinforcement learning tech-\nniques can be adopted to solve the MDP. However, the\nmodern networks are large-scale and complicated, and thus the\ncomputational complexity of the techniques rapidly becomes\nunmanageable. As a result, DRL has been developing to be\nan alternative solution to overcome the challenge. In general,\nthe DRL approaches provide the following advantages:\n\u2022 DRL can obtain the solution of sophisticated network\noptimizations. Thus, it enables network controllers, e.g.,\nbase stations, in modern networks to solve non-convex\nand complex problems, e.g., joint user association, com-\nputation, and transmission schedule, to achieve the op-\narXiv:1810.07862v1  [cs.NI]  18 Oct 2018\n",
    "GPTsummary": "- (1): The paper focuses on the application of deep reinforcement learning in communications and networking.\n- (2): The past methods such as reinforcement learning are not suitable for large-scale networks due to their limitations. The paper proposes deep reinforcement learning, a combination of reinforcement learning with deep learning, to solve the shortcomings. The approach is well motivated as it can solve non-convex and complex problems, enable network entities to learn and build knowledge about the communication and networking environment, provide autonomous decision-making, and improve the learning speed.\n- (3): The paper first gives a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, it reviews deep reinforcement learning approaches proposed to address emerging issues in communications and networking, and presents applications of deep reinforcement learning for traffic routing, resource sharing, and data collection. Finally, it highlights important challenges, open issues, and future research directions of applying deep reinforcement learning.\n- (4): The performance achieved by the methods in this paper is not specifically stated as the paper focuses on a comprehensive survey of the applications of deep reinforcement learning in communications and networking. However, the paper emphasizes the potential of deep reinforcement learning to overcome the limitations of past methods and address emerging issues in the field.\n\n\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work is to provide a comprehensive survey of the applications of deep reinforcement learning in communications and networking. It introduces deep reinforcement learning as a combination of reinforcement learning with deep learning, which can solve non-convex and complex problems, enable network entities to learn and build knowledge about the communication and networking environment, provide autonomous decision-making, and improve the learning speed. The paper gives a tutorial of deep reinforcement learning from fundamental concepts to advanced models, reviews existing approaches, and highlights important challenges and future research directions.\n\n- (2): Innovation point: The paper proposes deep reinforcement learning, a new approach that combines reinforcement learning with deep learning to solve the limitations of past methods. Performance: The paper focuses on a comprehensive survey of the applications of deep reinforcement learning in communications and networking, but does not specifically state the performance achieved by the methods. Workload: The paper is well-structured and provides a tutorial on fundamental concepts and advanced models, but some sections may be too technical for general readers.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to provide a comprehensive survey of the applications of deep reinforcement learning in communications and networking. It introduces deep reinforcement learning as a combination of reinforcement learning with deep learning, which can solve non-convex and complex problems, enable network entities to learn and build knowledge about the communication and networking environment, provide autonomous decision-making, and improve the learning speed. The paper gives a tutorial of deep reinforcement learning from fundamental concepts to advanced models, reviews existing approaches, and highlights important challenges and future research directions.\n\n- (2): Innovation point: The paper proposes deep reinforcement learning, a new approach that combines reinforcement learning with deep learning to solve the limitations of past methods. Performance: The paper focuses on a comprehensive survey of the applications of deep reinforcement learning in communications and networking, but does not specifically state the performance achieved by the methods. Workload: The paper is well-structured and provides a tutorial on fundamental concepts and advanced models, but some sections may be too technical for general readers.\n\n\n"
}