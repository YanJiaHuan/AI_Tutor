{
    "Abstract": "Abstract\u2014Reinforcement learning (RL) requires skillful de\ufb01nition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into reinforcement learning is a promising way to improve learning performance. In this paper, a comprehensive human guidance-based reinforcement learning framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the reinforcement learning process is proposed to boost the ef\ufb01ciency and performance of the reinforcement learning algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-art methods suggest the advantages of our algorithm in terms of learning ef\ufb01ciency, performance, and robustness. Index Terms\u2014Reinforcement learning, priority experience replay, human demonstration, autonomous driving. I. ",
    "Introduction": "",
    "Experiments": "Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-art methods suggest the advantages of our algorithm in terms of learning ef\ufb01ciency, performance, and robustness. Index Terms\u2014Reinforcement learning, priority experience replay, human demonstration, autonomous driving. I. INTRODUCTION R EINFORCEMENT learning (RL) has substantially contributed to numerous \ufb01elds [1]\u2013[4] by solving control and optimization problems. As a branch of machine learning methods, RL improves the capability of controlling agents in black-box environments through the exploratory trial-and-error principle [5]. Recent popular RL algorithms, e.g., rainbow deep Q-learning [6], proximal policy optimization (PPO) [7], and soft actor-critic (SAC) [8], have shown ability in handling high-dimensional environment representation and generalization, due to the introduction of deep neural networks. Albeit RL can achieve good performance in complex tasks, its drawback emerges that their interactions with the environment are very inef\ufb01cient [9]. Thus, using RL to solve a problem needs skillful de\ufb01nitions and settings and consumes remarkable computational resources [10]. Combining human guidance with RL can be a promising way to mitigate the above drawback [11]. First, human interJ. Wu, Z. Huang, W. Huang and C. Lv are with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, 639798. (E-mail: {jingda001, zhiyu001, wenhui001}@e.ntu.edu.sg, lyuchen@ntu.edu.sg) Corresponding author: Chen Lv This paper has been published in IEEE Transactions on Neural Networks and Learning Systems. DOI: 10.1109/TNNLS.2022.3177685 The code associated with this paper is available at this link vention has been used to improve RL performance. Intervention is triggered by unfavorable actions and should be avoided by RL. Then, the human demonstration is a powerful tool to enhance RL\u2019s ability [12]. In this context, the objective functions are generally reshaped compatible with supervised learning to improve ef\ufb01ciency [13]. Despite the above human guidance-based methods, RL needs to process numerous data from its self-explorations. The existing methods do not particularly optimize the utilization of human guidance data; consequently, they still need great human workloads to avoid submersion of guidance in exploratory data. Additionally, human guidance, which is variant to pro\ufb01ciency, mental and physical status of participants, should not be equally treated since some low-quality guidance can even impair the RL performance. We propose a priority-based experience replay method on human guidance and put forward the associated human guidance-based RL algorithm to bridge the abovementioned gap. Our approach is off-policy, which leverages the experience replay mechanism [14] to maximize the utilization ef\ufb01ciency of self-exploratory data. The proposed priority replay mechanism can further improve the utilization ef\ufb01ciency of human guidance data by quantifying their values and weighing their utilized probability, which ultimately augments the RL performance. As a result, the ef\ufb01ciency can be improved by over seven times under the adopted task. The schematic diagram of our algorithm is depicted in Fig. 1. To evaluate the training and testing performance of our proposed method, we design two challenging autonomous driving scenarios. The experimental results suggest the advance of the proposed algorithm compared to state-of-the-art baselines in learning ef\ufb01ciency, practical performance, and robustness. The contribution of this report can be summarized into three aspects. 1) We propose a novel prioritized experience utilization mechanism regarding human guidance in the RL process to improve performance. 2) We establish a comprehensive and holistic framework of human guidance-based RL by integrating the human-RL action switch scheme, behavior cloning-based objective function, human-demonstration replay method, and human-intervention reward shaping mechanism.3) We validate the superior performance of the proposed algorithm in solving challenging autonomous driving tasks comprehensively. The remainder is organized as follows: a review of related work is provided in Section II, preliminaries for the proposed arXiv:2109.12516v2  [cs.LG]  29 Nov 2022 ",
    "Related Work": "RELATED WORK Sample ef\ufb01ciency bottlenecks the training and performance of RL. Combining human guidance with RL is a promising way to mitigate the challenge. Three categories of human guidance have been integrated into RL. The \ufb01rst one is human feedback, where the human expert\u2019s prior knowledge about the task could be used to qualitatively or quantitatively score the RL behaviors [15]. In this manner, an RL-based unmanned ground vehicle was guided to run through a maze [16]. However, the feedback is highdemanding on human ability and thus is no longer popular in recent studies. The second branch is human intervention. Intervention is a more direct manifestation of human knowledge than giving feedback. RL agents are devised to reduce their con\ufb01dence in adopted actions if intervention occurs [17]. [18] employed real humans to detect catastrophic actions of DQN in playing Atari games, where humans were required to intervene in the training process to block the risk. It punished the human intervened scenes through the reward-shaping technique to prevent RL from reaching the unfavorable situations again. With a similar idea, [19] devised a reward shaping-based PPO algorithm and made the RL agent complete the drone driving tasks under human interventions. In this report, the abovementioned reward shaping scheme is also adopted, and more importantly, we provide a theoretical derivation and related discussion on the optimality of the human intervention-based reward shaping method. The human demonstration is the other way to enhance RL performance. For discrete-action RL, the DQfD algorithm [12] shaped the value function of DQN using human demonstration. [20] presented a double experience buffer setting to separately store the RL data and human demonstrations. For more complicated RL with actor-critic architecture, the policy function is usually modi\ufb01ed to be compatible with learning from demonstration. The behavior cloning objective has been added to the objective of the policy function to greatly improve learning ef\ufb01ciency, which is a milestone in the \ufb01eld. In this way, dexterous manipulations of high degree-of-freedom robotic arms [21]\u2013[23] and human-level game operation [17] were achieved based on the state-of-the-art RL algorithms. In this report, the behavior cloning objective and its associated human guidance-based actor-critic framework is also integrated into our method. However, it is not reasonable for equal treatment on various demonstrations, which is adopted in existing methods. First, without optimizing the utilization, small-scale human demonstrations would be submerged in the numerous RL-generated data. Second, human guidance is variant due to the pro\ufb01ciency and status of participants, and some low-quality guidance can even impair RL performance. Noticeably, these drawbacks are to be overcome by the proposed prioritized experience utilization mechanism. III. PRELIMINARIES In this section, we \ufb01rst introduce the notation and concept of off-policy actor-critic RL, and we then illustrate the prioritized experience replay mechanism. All three parts in this section are the base for the proposed human-guidance-based RL algorithm. A. Notation We consider a standard RL setting where an RL agent interacts with the controlled environment. Such an interaction can be formulated as a discrete-time Markov decision process (MDP), de\ufb01ned by the tuple (S, A, R, p). The state-space S consists of continuous state variables s and the action space constitutes continuous action variables a. R(\u00b7|s, a) : S \u00d7A \u2192 r is a reward function mapping the state-action pair (s, a) to a deterministic reward value r. The environment dynamics generates state transition probability p(\u00b7|s, a) : S\u00d7A \u2192 P(s\u2032) mapping the state-action pair (s, a) to the probability distribution over the next state s\u2032. At each time step t, the agent observes the state st \u2208 S and sends the action at \u2208 A to the environment, receiving the feedback of a scalar reward rt and next state st+1. The agent\u2019s behavior is determined by a policy \u03c0(at|st) : S \u2192 P(at), which maps a state to the probability distribution over candidate actions. We utilize \u03c1\u03c0 to represent the state-action distribution induced by the policy \u03c0. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 B. Off-policy Actor-critic Architecture The goal of RL is to optimize the policy which maximizes the expected value V over the environment dynamics. A Bellman value function (also called critic) is established to estimate V in a bootstrapping way. This value function is usually called Q. Under an arbitrary policy \u03c0, Q is de\ufb01ned as: Q\u03c0(st, at) = rt + \u03b3 E (st+1,at+1)\u223c\u03c1\u03c0[Q\u03c0(st+1, at+1)], (1) where \u03b3 \u2208 (0, 1) is the discount factor. Then the policy function (also called actor) can be obtained concerning maximized Q, represented as: \u03c0 = arg max \u03c0 \ufffd E (s,a)\u223c\u03c1\u03c0 [Q\u03c0 (s, a)] \ufffd , (2) In practice, value function pursues the evaluation regarding only the optimal policy \u03c0\u22c6, regardless of the policy executing the interaction. Therefore, RL decouples the policy evaluation process and the policy\u2019s behavior, which makes the agent update in an off-policy manner. We use neural networks as the function approximator to formulate the actor and critic, the objectives are then reached through the loss functions. Speci\ufb01cally, the loss function of the critic LQ, and the actor L\u03c0 can be expressed as: LQ (\u03b8) = rt + \u03b3E [Q (st+1, \u03c0 (st+1; \u03c6) ; \u03b8)] \u2212 Q (st, at; \u03b8) , (3) L\u03c0 (\u03c6) = \u2212Q (st, \u03c0 (\u00b7|st; \u03c6) ; \u03b8) , (4) where Q(\u00b7; \u03b8) represents the parameterized critic function and \u03b8 represents the parameters of the critic network, \u03c0(\u00b7; \u03c6) represents the parameterized actor function and \u03c6 represents the parameters of the actor network. Hereinafter, the parameter \u03b8 and \u03c6 can be omitted if no ambiguity exists. C. Prioritized Experience Replay Mechanism The experience replay mechanism establishes an experience buffer to store the data at each interaction. Accordingly, the RL agent can retrieve data generated by previous policies from the buffer for policy evaluation and improvement. Given an arbitrary time step t, the interaction between the RL agent and the environment generates a transition tuple, which is stored into the experience replay buffer as: B \u2190 \u03b6t = (st, at, rt, st+1). (5) Conventionally, the experience in the buffer is retrieved from the buffer using uniform random sampling. In a more ef\ufb01cient method, prioritized experience replay mechanism (PER) [24], the data sample is subjected to a nonuniform distribution I, and its probability mass function pI \u223c I can be expressed as: pI (i) = p\u03b1 i \ufffd k p\u03b1 k , (6) where \u03b1 \u2208 [0, 1] is the scaling coef\ufb01cient, p represents the priority of each tuple i, which is determined by the temporal difference (TD) error \u03b4T D and expressed as: pi = |\u03b4T D i | + \u03b5 = |ri + \u03b3 \u00b7 Q (si+1, \u03c0 (\u00b7|si+1; \u03c6) ; \u03b8) \u2212 Q (si, ai; \u03b8) | + \u03b5, (7) where \u03b5 \u2208 R+ is a small positive constant to guarantee the probability larger than zero. A larger TD error indicates an experience worth learning to a higher extent. Thus, the TD error-based prioritized experience replay mechanism can improve the RL training ef\ufb01ciency. IV. HUMAN-IN-THE-LOOP REINFORCEMENT LEARNING In this section, we \ufb01rst summarize the human behaviors in the RL training process which can be leveraged in the algorithm design. Based on that, we establish an actorcritic framework adapting to human guidance. Then, two modules are proposed to further improve RL in the context of human guidance: a novel prioritized experience replay mechanism concerning human demonstration, and a reward shaping technique concerning human intervention. Finally, a holistic human-in-the-loop RL algorithm is instantiated using the above components. A. Human Guidance Behavior in RL Training We de\ufb01ne two useful human guidance behaviors in the RL training process: intervention and demonstration. Intervention: Human participants recognize RL interaction scenes and identify whether a guidance behavior should be conducted based on their prior knowledge and reasoning abilities. If human participants decide to intervene, they can manipulate the equipment to get the control authority (partially or totally) from the RL agent. The intervention generally happens when the RL agent conducts catastrophic actions or is stuck in local optima traps. Thus, RL could learn to avoid unfavorable situations from the intervention. Demonstration: Human participants perform their actions when an intervention event happens, which generates the corresponded reward signal and next-step state. The generated transition tuple can be seen as a piece of demonstration data since it is induced by human policy instead of the RL\u2019s behavior policy. RL algorithm could learn human behavior from the demonstration. State-of-the-art human-guidance-based RL algorithms have been integrating learning from intervention (LfI) [18], and learning from demonstration (LfD) [25]. In this report, both LfI and LfD will be employed in the proposed architecture. Speci\ufb01cally, LfI based on the reward shaping technique is utilized in the reward function de\ufb01nition, while LfD plays its role in the underlying principles of the algorithm. B. Human-guidance-based Actor-critic Framework In this section, we elaborate on the interaction mechanism and learning objective of the proposed human-guidance-based actor-critic RL algorithm. First, we focus on the interaction mechanism. In the standard interaction between RL and environment, RL\u2019s behavior policy will output actions to explore the environment. Given an off-policy actor-critic RL, the above process is shown as: aRL t = \u03c0(\u00b7|st; \u03c6) + \u03bea \u2299 astd t , (8) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 where astd t \u2208 Rdim(A) is a training-dependent variable that scales the exploration noise, \u2299 represents the Hadamard product and \u03bea \u223c N(0, Idim(A)) We give full authority to human participants whenever they decided to take control in the training loop of RL. Thus, the eventual action is \ufb01ltered by a mask as: at = (Idim(A) \u2212 \u2206t) \u00b7 aRL t + \u2206t \u00b7 aH t , (9) where aH t represents the action from the human participant\u2019s policy, \u2206t \u2208 Rdim(A) is a demonstration mask: it is an identity matrix when human demonstration happens and a zero matrix in the non-demonstrated step. The interaction transition tuple \u03b6 will be recorded and stored into the experience replay buffer once the action is sent to the environment. In particular, actions from the human policy and the RL policy are stored in the same buffer. For this context, the new transition tuple \u03b6 is de\ufb01ned to discriminate human demonstrations from normal RL experiences as: \u03b6i = (si, ai, ri, si+1, \u2206i). (10) Then, we focus on the learning objective. Given a batch of transition tuples with batch size N, there could exist data \u03b6N1 from the RL policy and \u03b6N2=N\u2212N1 from the human policy. The critic network, based on the optimal value function, can learn from both policies. Thus, its loss function is calculated as: LQ(\u03b8) = 1 N1 N1 \ufffd i \u2225ri + \u03b3Q(si+1, \u03c0(\u00b7|si+1); \u03b8) \u2212 Q(si, aRL i ; \u03b8)\u22252 2 + 1 N2 N2 \ufffd j \u2225rj + \u03b3Q(sj+1, \u03c0(\u00b7|sj+1); \u03b8) \u2212 Q(sj, aH j ; \u03b8)\u22252 2. (11) Given the data from the human policy, the actor should learn from these demonstrations in addition to maximizing the critic\u2019s value. Hence, we devise the loss function of the actor network considering behavior cloning as: L\u03c0 (\u03c6) = 1 N1 N1 \ufffd i [\u2212Q(si, \u03c0(\u00b7|si; \u03c6); \u03b8)] + 1 N2 N2 \ufffd j [\u03c9 \u00b7 \u2225aH j \u2212 \u03c0(\u00b7|sj; \u03c6)\u22252 2], (12) where \u03c9 is a manually determined constant that weighs the importance of behavior cloning. It is noticeable that the mean squared error (MSE) losses involved in the above formulas are for exempli\ufb01ed calculation, meaning that they can be alternated by any loss functions. C. Prioritized Human Demonstration Replay In this section, we put forward a novel PER mechanism for human demonstration. Human demonstrations are generally more critical than most exploration from RL\u2019s behavior policy due to prior knowledge and reasoning ability. Thus, a more effective method is needed to weigh human demonstrations among the buffer. We propose an advantage-based metric instead of TD-error of the normal PER to establish the prioritized replay mechanism. First, we de\ufb01ne an advantage measure regarding the human demonstration against the RL\u2019s behavior policy. Since the critic, i.e., value function, can evaluate the policy, we calculate the difference between the Q value of the human action and that of the RL action. Given a human-demonstration transition tuple (si, ai = aH i , ri, si+1), the priority level p is de\ufb01ned as: pi = \u2206 |\u03b4T D i |+\u03b5+exp \ufffd Q(si, aH i ; \u03b8) \u2212 Q(si, \u03c0(\u00b7|si); \u03b8) \ufffd , (13) where exp is the exponential function to guarantee the nonnegative advantage value. We call the last term of the Eq. 13 the Q-advantage term, which evaluates to what extent should a speci\ufb01c humandemonstration tuple be retrieved except the TD-error metric. Through the RL training process, the RL agent\u2019s ability varies and the priority level of one human-demonstration tuple changes accordingly, which gives rise to a dynamic priority mechanism. We abbreviate Q-advantage as QA and call the above mechanism TDQA to illustrate it combines two metrics as the measurement of human guidance. The QA term is removed for non-demonstration tuples when calculating the above equation, thus, the priority levels of non-demonstration data are aligned with those in the conventional PER. In this manner, the experience in the buffer B subjects to a distribution I\u2032, and the probability mass function of the experience distribution pI\u2032 \u223c I\u2032 can be expressed as: pI\u2032(i) = p\u03b1 i \ufffd k p\u03b1 k . (14) We inherent the optimization trick of the conventional PER by using a sum-tree structure to store transition data, and the updating and sampling can be conducted with a complexity of O(log N). The priority mechanism introduces the bias to the estimation of the expectation of the value function since it changes the experience distribution in the buffer. Biased value network Q could have little impact on the RL asymptotic performance, yet it may affect the stability and robustness of the mature policy in some situations. As an optional operation, we can anneal the bias by introducing the importance-sampling weight to the loss function of the value network. The importance-sampling weight of a transition i is calculated as: wIS(i) = [pI\u2032(i)]\u2212\u03b2 . (15) where \u03b2 \u2208 [0, 1] is a coef\ufb01cient: the fully non-uniform sampling occurs if \u03b2 = 1, and fully uniform sampling occurs if \u03b2 = 0. \u03b2 will gradually decrease to zero along with the training process. The importance-sampling weight can be added to the loss function of the value network, expressed as: LQ(\u03b8) = E \u03b6i\u223cI\u2032wIS(i) (ri + \u03b3Q(si+1, \u03c0(\u00b7|si+1); \u03b8) \u2212 Q (si, ai; \u03b8))]. (16) Through the proposed PER, we prioritize human guidance over RL experiences. Moreover, high-quality demonstrations JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 are prioritized to more extents, and the utilization ef\ufb01ciency of human demonstrations can be enhanced. D. Human-intervention-based Reward Shaping In this section, we introduce the human-intervention-based reward shaping technique. Naturally, there is no need for humans to provide guidance if the being-trained RL agent is executing a good policy. Therefore, to minimize the human workload, we assume human participants would intervene in the training process only when RL\u2019s behaviors are unfavorable. In this context, the intervention event can be seen as a negative signal and the corresponding state should be avoided by RL. This negative feedback can be realized by reward shaping, which will be detailed in this section. e universally  Value ss 400 128   5e-4 2e-4 0.996 relu\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd 1\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd 0.05 0.95 alue  1e-3 0.2   1 1 niversally  Value fer 1e5 0.6 1 1e-3 . Value 1e-4 relu 50 128 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd0 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd1 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd3 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd4 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd5 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd0 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd1 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd3 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd4 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd5 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd6 State  RL action (\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd = \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd) Human demonstration action (\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd = \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd) Intervention time ( (\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd = \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd) \u2227 (\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u22121 = \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd) ) Fig. 2. Illustration of the intervention time step. In a time-sequential MDP, the \ufb01rst time step which is controlled by human demonstration is taken as the intervention time. We \ufb01rst identify the intervention event. Recall Eq. 9 de\ufb01nes a mask \u2206t, which is a time-sequential variable recording if the action at is conducted by human demonstration. Hence, the intervention time, i.e., the start time of a period of human demonstrations, can be represented by (\u2206t = I)\u2227(\u2206t\u22121 = 0) in a time-sequential training process of RL, as illustrated in Fig. 2. It is noted that only the intervention time is to be punished by the reward shaping, since the states after humans intervention will be substituted by human demonstrations and cannot be seen as unfavorable. For instance, in Fig. 2, s2 is penalized while s3 and s4 are not. Then, we can shape the vanilla reward function with an additional penalized function: rshape t = rt + rpen[(\u2206t = Idim(A) \u2227 (\u2206t\u22121 = 0dim(A))], (17) where rshape t is the reward after shaping, rpen is a scalar that weighs the intervention penalty. The theoretical performance of this reward shaping scheme is analyzed in Appendix A. E. Prioritized human-in-the-loop RL algorithm In this section, we integrate all the above components and propose a holistic RL algorithm considering human guidance. It is noted that although the human guidance-based actor-critic framework in Section IV-B and reward shaping in Section IV-D are components of the algorithm, they are not the major novelty of this report. To highlight our core idea of the prioritized human-demonstration replay mechanism of Section IV-C, we name the proposed algorithm as Prioritized HumanIn-the-Loop (PHIL) RL. Speci\ufb01cally, we obtain the holistic human-in-the-loop RL con\ufb01guration through equipping the human-guidance-based actor-critic framework with prioritized human-demonstration replay and intervention-based reward shaping mechanisms. We instantiate the PHIL algorithm based on one of the state-of-theart off-policy RLs, i.e., twin delayed deep deterministic policy gradient (TD3) [26]. We also remind the above components are adaptive to various off-policy actor-critic RL algorithms. In TD3, the target networks, namely, the target critic Q\u2032 with parameter \u03b8\u2032 and target actor \u03c0\u2032 with parameter \u03c6\u2032 are utilized to stabilize the algorithm update. And the actor\u2019s output becomes a deterministic value instead of a sample from the probability distribution. Considering the role of human participants in the RL interaction process, the eventual action in the time step t can be expressed as: at = (Idim(A) \u2212 \u2206t) \u00b7 aRL t + \u2206t \u00b7 aH t , (18a) aRL t = \u03c0(\u00b7|st) + clip (\u03f5, \u2212c, c) , \u03f5 \u223c N (0, \u03a3) , (18b) where \u03f5 is a noise coef\ufb01cient vector dependent on the training proceed, c is the bounding of the exploratory action, \u03a3 is the covariance matrix of the Gaussian distribution N. A transition tuple is obtained through the above interaction step and stored into the proposed human-demonstration experience buffer as: B \u2190 \u03b6t = (st, at, rt, st+1, \u2206t). (19) Stored experience tuples will be retrieved for the training of the value and policy networks. An arbitrary transition tuple \u03b6 with index i would be retrieved by the probability p, which is calculated by: p(i) = p\u03b1 i \ufffd k p\u03b1 k , (20) wherein the priority level p is: pt = |\u03b4T D t | + \u03b5 + (\u2206t = Idim(A)) \u00b7 QA, (21a) QA = exp [Q\u2032 (st, at; \u03b8\u2032) \u2212 Q\u2032 (st, \u03c0(\u00b7|st; \u03c6); \u03b8\u2032)] , (21b) It is noticeable that Q-advantage is calculated by the target critic network Q\u2032 to avoid unstable updates. Supposing a tuple with size N contains N1 amount of non demonstration tuples and N2 = N \u2212N1 human demonstration ones, the loss function of the critic can be expressed as: LQk(\u03b8) = 1 N1 N1 \ufffd i \u2225ri + \u03b3Q\u2032 l (si+1, \u03c0\u2032(\u00b7|si+1)) \u2212 Qk \ufffd si, aRL i \ufffd \u22252 2 + 1 N2 N2 \ufffd j \u2225rj + \u03b3Q\u2032 l (sj+1, \u03c0\u2032 (\u00b7|sj+1)) \u2212 Qk \ufffd sj, aH j \ufffd \u22252 2 (22) where k = 1, 2 represents the index of two Q networks. Note the double Q network trick, which utilizes the smaller Q ",
    "Problem Formulation": "PROBLEM FORMULATION The proposed PHIL-TD3, like most RLs, can be universally adapted to any continuous-action decision and control tasks. Here we choose the end-to-end autonomous driving problem as the object, evaluating our algorithm in two challenging driving scenarios. Note that the RL-based autonomous driving problem can be solved by numerous reasonable settings, while the problem formulation in this section is to provide a fair environment for algorithm evaluation and comparison. In this section, two challenging autonomous driving scenarios are introduced to evaluate the control and optimization performance of the proposed algorithm, then the standard optimization setting is established. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 A. Autonomous Driving Scenarios RL is better suited to the challenging driving tasks compared to rule-based or model optimization-based approaches due to its high representational and generalization capabilities. We choose two scenarios, shown in Fig. 3, to evaluate the RL performance. These scenarios are challenging to conventional autonomous driving strategies due to complex combinatorial relationships. Unprotected left-turn: This scenario is illustrated in Figs. 3(a-b). The ego vehicle, i.e., the controlled vehicle, in the side road is trying to make a left turn and merge into the main road. No traf\ufb01c signals guide the vehicles in the intersection. We assume the lateral path of the ego vehicle is planned by other techniques, while the longitudinal control is assigned to the RL agent. Surrounding vehicles are initialized with varying random velocities ranging from [4, 6] m/s and controlled by the intelligent driver model (IDM) [28] to execute lane-keeping behaviors. All surrounding drivers are set with aggressive characteristics, meaning that they would not yield to the ego vehicle. The control interval for all vehicles is set as 0.1 seconds. Highway congestion: This scenario is illustrated in Figs. 3(c-d). The ego vehicle is stuck in severe congestion and tightly surrounded by other vehicles; thus, it is trying to shrink the gap with its leading vehicles and conduct the car-following task with the target velocity. We assume the longitudinal control is completed by IDM with a target velocity of 6 m/s, while the lateral control is assigned to the RL agent. Surrounding vehicles are initialized with the velocity ranging from [4, 6] m/s and controlled by IDM to execute carfollowing behaviors. The control interval for all vehicles is set as 0.1 seconds. The crowded surrounding vehicles cover the lane markings and no speci\ufb01c one leading vehicle in the ego lane, which can lead the conventional lateral-planning approaches to be invalid in such a scenario. Ego vehicle Surrounding vehicles a b Ego vehicle Surrounding vehicles c dd a b Fig. 3. Task environment con\ufb01guration. a, the devised unprotected left-turn scenario in T-intersection, established in CARLA. b, the bird-view of the left-turn scenario, where the dotted line indicates a left-turn trajectory. c, the devised congestion scenario in the highway, established in CARLA. d, the bird-view of the congestion scenario, where the dotted line shows a carfollowing trajectory. 100 10 20 30 PHIL-TD3 IA-TD3 HI-TD3 Vanilla TD3 Mean of average sur Normal setting Tough setting Ratio 100 10 PHIL-TD3 IA-TD3 HI-TD3 Vanilla TD3 Mean of average sur Normal setting Tough setting Ratio a b Fig. 4. State space illustration: bird\u2019s-eye-view semantic graph. a, the left-turn scenario, b, the congestion scenario. B. RL-based problem de\ufb01nition State: : The bird-view semantic graphs are taken as the state information for the RL agent, shown in Fig. 4. Two consecutive frame images are used to constitute one state variable to enable temporal perception. We scale the cameracaptured image to a smaller size to relieve the computational burden. The state variable can be expressed as: st = {pt\u22121, pt|p \u2208 [0, 1]}, (28) where p \u2208 R45\u00d780 is a pixel matrix of which the elements are normalized. Action: : The action variable can be either lateral or longitudinal commands adaptive to different requirements. For the lateral control task in the congestion scenario, we choose the angle of the steering wheel as the action, expressed as: at = [\u03b4t|\u03b4 \u2208 [\u22125\u03bb\u03c0, 5\u03bb\u03c0]] , (29) where \u03b4 \u2208 R1 is the continuous steering command, of which the negative value indicates a left-turn command and the positive value corresponds to a right-turn command, and \u03bb is the scaling factor that limits the steering range. For the longitudinal control in the left-turn scenario, we choose the accelerating/braking pedal aperture, expressed as: at = [\u03b7t |\u03b7 \u2208 [\u22121, 1]] , (30) where \u03b7 \u2208 R1 is the continuous pedal aperture, of which the negative value indicates a braking command and the positive value corresponds to an accelerating command. Reward: : The goal of an autonomous vehicle is to rapidly complete traf\ufb01c scenarios through safe and smooth driving behaviors. RL-based driving strategy achieves this by an appropriate reward function de\ufb01nition. The reward schemes of the two tasks in Fig.3 can be respectively de\ufb01ned as: Rleft-turn (\u00b7|st, at) =rgoal \u00b7 1 (st \u2208 Sgoal) +rfail \u00b7 1 (st \u2208 Sfail) + rspeed (st) , (31) Rcongestion (\u00b7|st, at) =rgoal \u00b7 1 (st \u2208 Sgoal) +rfail \u00b7 1 (st \u2208 Sfail) + rsteer (st) , (32) where rgoal = 10 and Sgoal is the set of goal states where the ego vehicle successfully completes the scenario; rfail = \u221210 and Sfail is the set of failure states where the collision occurs; while rspeed = \u2212\u2225vego \u2212 vtarget\u2225 is the reward that encourages the target speed, i.e., 5m/s set in this section; rsteer = \u2225\u03b4t \u2212 \u03b4t\u22121\u2225 is the reward that discourages frequent steering behaviors. It is noticeable that both rspeed and rsteer JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8 a b \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Convolutional  layers Flatten Dense  layers State: consecutive images Latent feature Output action \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Convolutional layers Dense  layers State: consecutive images Latent feature Output Q value Embedding layer Action: steer/pedal signals Flatten Fig. 5. Neural network approximator illustration. a, the policy function architecture achieved by the neural network, where the target value network owning the same structure is omitted for brevity. b, the value network architecture achieved by the neural network, where the target value network owning the same structure is omitted for brevity. can implicitly play a role in promoting smooth driving. Additionally, we set the penalty term rpen in Eq. 17 the same as rfail and incremented it to the above reward when human intervention occurs. Function approximator: : The function approximators of the value and policy functions are concrete by deep convolutional networks, as shown in Fig. 5. Auxiliary functions: : We de\ufb01ne some auxiliary control functions independent of the RL action to achieve a complete control suit. When RL manipulates the steering wheel, the longitudinal control is achieved by an IDM. When RL manipulates the pedal aperture, the lateral motion target is to track the planned waypoints through a proportional-integral (PI) controller. VII. EXPERIMENTAL VALIDATION A. Baseline Algorithms We employ state-of-the-art in the domain of human-involved RL algorithms as baselines and compare their performance against the proposed algorithm. IA-TD3: This baseline is derived from Intervention Aided Reinforcement Learning (IARL) , which is a representative combination of a continuous-action RL algorithm and human demonstration. The RL\u2019s policy network is modi\ufb01ed to adapt to human demonstrated actions by introducing the behavior cloning objective. Once human intervention happens, the human demonstration will substitute the RL\u2019s exploratory action, and a penalty signal will impose on the reward value. In this study, we devise a modi\ufb01ed IARL by replacing the onpolicy base algorithm with TD3, which essentially augmented the algorithm by improving the sample ef\ufb01ciency. We also implement the prioritized experience replay (PER) in this baseline for a fair comparison. HI-TD3: This baseline is derived from Human Intervention Reinforcement Learning (HIRL) , which is a combination of a discrete-action RL algorithm and human demonstration. Once intervention happens, the human demonstration will substitute the RL\u2019s exploratory action, and a penalty signal will take on the reward signal. In this study, we devise a modi\ufb01ed HIRL by replacing the discrete-action base algorithm with TD3, which augmented the algorithm by improving the representation and control precision. We also implement the PER in this baseline for a fair comparison. RD2-TD3: This baseline is derived from Recurrent Replay Distributed Demonstration-based DQN (R2D3), which is a representative combination of PER mechanism and human demonstration. In this study, we devise a modi\ufb01ed algorithm by replacing DQN with TD3. The original R2D3 utilizes the recurrent neural network to augment performance, which is not the concerned technique in the context of this report, thus, we remove the recurrent network structure and only focus on its replay distributed character regarding human demonstrations. Thus, we devise a Replay Distributed Demonstration-based (RD2) TD3 algorithm, which distributes human demonstration and RL exploratory experience into two experience buffers respectively and retrieves experiences by PER. The probability of utilizing human guidance instead of RL exploratory experience is aligned with the ratio of human guidance amount and total data amount. Furthermore, we use the vanilla PER+TD3 that is shielded from human guidance as an ablated baseline. B. Experimental Setting Multiple experiments are to evaluate the comprehensive performance of PHIL-TD3 against baselines. First, the training efforts of involved algorithms are comparatively evaluated in the two autonomous driving scenarios. Then the well-trained autonomous driving strategies are tested regarding control performance with several metrics. Last, a series of experiments involving both training and testing stages are conducted to analyze the mechanism of PHIL-TD3. The training hardware comprises a driving simulator and a high-performance workstation. The driving simulator is utilized to collect human data to train the human policy model complying with Section IV, and the workstation is dedicated to processing RL training. A high-\ufb01delity autonomous driving simulation platform, CARLA [29], is employed to implement driving scenarios and generate RL-environment interaction information. The schematic diagram of the RL training stage is illustrated in Fig. 6(a). The testing hardware is a robotic vehicle. The post-trained RL policy is implemented on the computation platform of the vehicle, which can communicate with the CARLA server through the wireless network. The on-board RL policy receives state information from CARLA and sends its control command back to remotely complete autonomous driving tasks. The robotic vehicle aims to test whether the RL policy is wellworked under the current onboard calculation and communication situations. The schematic diagram of the RL testing stage is demonstrated in Fig. 6(b). JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 The detailed con\ufb01guration of the above experimental platform is provided in table A1. The algorithms are concreted based on neural networks, of which the architecture is illustrated in Appendix A. And the hyperparameters of the algorithms are also given in Appendix A. C. Evaluation of RL Training Performance In this section, we explore whether human guidance can indeed improve the RL training, and further, which algorithm can achieve the best learning performance given the same human guidance. Additionally, we also investigate the effects of human guidance in dealing with RL tasks of different dif\ufb01culties. To eliminate the deviation brought by participant randomness and obtain repeatable results, we use the identical human model (see Section V) to mimic human guidance behaviors in RL training processes. We \ufb01xate the sequence of random seeds and make the triggering conditions of human interventions invariant in all training attempts, which achieves a fair comparison across different algorithms. Two metrics are employed: the average reward of the training episode (excluding intervention-based shaping term), and the surviving distance of the ego vehicle in the training episode before a goal state or failure state in Eq. 31 occurs. A higher value of both metrics indicates a better learning performance. Fig. 7 visualizes the learning performance through curves, represented with a solid line of the mean value and an error band of the standard deviation. We run each algorithm \ufb01ve times in the unprotected left-turn scenario and demonstrate their learning processes in Figs. 7(a-b). The vanilla TD3 is struggling to improve its policy, while the other three algorithms achieve higher rewards and survive distances in a much shorter time, which indicates the effectiveness of High-fidelity  scenario server Post-trained RL  driving policy \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Real-time onboard  environment High-fidelity  scenario server Action  command State  information Being-trained RL  driving policy \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Pretreat human  model before RL \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Human policy Interaction information (tuple) Update  strategy Human-in-the-loop  model training Workstation  environment State  information State  information Action  command Action command (when needed) RL training stage RL testing stage a b Fig. 6. Experimental work\ufb02ow. a, the experimental work\ufb02ow in the RL training stage. The dotted line represents the human policy model that is not always sending commands. b, the experimental work\ufb02ow in the RL testing stage. e f 0 100 200 300 400 Training episode -3 -2.5 -2 -1.5 -1 -0.5 0 Reward PHIL-TD3(ours) IA-TD3 HI-TD3 Vanilla TD3 0 100 200 300 400 Training episode 0 20 40 60 80 Surviving distance(m) PHIL-TD3(ours) IA-TD3 HI-TD3 Vanilla TD3 0 100 200 300 400 Training episode 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Reward PHIL-TD3(ours) IA-TD3 HI-TD3 Vanilla TD3 0 100 200 300 400 Training episode 10 15 20 Surviving distance (m) PHIL-TD3(ours) IA-TD3 HI-TD3 Vanilla TD3 a b d c 10 15 20 PHIL-T Mean of average surviving distance (m) Normal s a Fig. 7. Learning efforts of different RL algorithms. a-b, curves of training rewards and surviving distances in the left-turn scenario, respectively. c-d, curves of training rewards and surviving distances in the congestion scenario, respectively. human guidance. Among the human-involved algorithms, HITD3 performs the slowest learning process suggested by either reward or surviving distance, and IA-TD3 exhibits a faster convergence but with limited asymptotic performance. In opposite, PHIL rapidly seizes the opportunity of human guidance and learns the best asymptotic policy. It is noticeable that PHIL-TD3 achieves the best asymptotic average reward of the baselines in less than 50 episodes, improving the learning ef\ufb01ciency by over 700%. We also run the congestion scenario \ufb01ve times for each algorithm and plot the learning curves in Figs. 7(c-d). The comparable PHIL and IA-TD3 perform better than the other two baselines when considering the reward. While the metric of surviving distance further con\ufb01rms this advantage and pro\ufb01tably differentiates the algorithm abilities. Speci\ufb01cally, PHIL wins the highest eventual score. IA-TD3 and HI-TD3 manifest comparable levels of asymptotic performance while IA-TD3 has an advantage in 100 150 200 10 20 30 40 50 60 PHIL-TD3 IA-TD3 HI-TD3 Vanilla TD3 Mean of average surviving distance (m) Normal setting Tough setting Ratio 100 150 200 10 15 20 PHIL-TD3 IA-TD3 HI-TD3 Vanilla TD3 Mean of average surviving distance (m) Normal setting Tough setting Ratio b a Ratio (%) Ratio (%) a b Fig. 8. Learning efforts of different RL algorithms in different task dif\ufb01culties. a-b, curves of training rewards and surviving distances in the left-turn scenario, respectively. c-d, curves of training rewards and surviving distances in the congestion scenario, respectively. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 TABLE I COMPARISON OF TIME EFFICIENCY OF DIFFERENT RL ALGORITHMS Algorithm Time consumption (s) per 10000 steps PHIL-TD3 360.70 IA-TD3 348.71 HI-TD3 328.93 Vanilla-TD3 329.39 learning ef\ufb01ciency. In this scenario, PHIL-TD3 achieves the best asymptotic average surviving distance of the baselines in 220 episodes, improving the learning ef\ufb01ciency by over 120%. Overall, the results in this training session highlight the signi\ufb01cant superiority of the proposed algorithm in learning performance. We further explore the learning performance of RLs with different task dif\ufb01culties, which gives rise to Fig. 8. The normal setting complies with the problem de\ufb01nition in Section VI, which is adopted throughout the report, while the tough setting changes consecutive-frame input of Eq. 28 into a single frame input, impairing the temporal perception ability of RL agents. At the high level, the statistical results of the normal setting are aligned with the trends of Figs. 7(a-d). And it is indicated that the tough setting does not change the performance ranking of algorithms despite the degradations in different degrees. At the detail level, the performance difference between the normal and the tough settings, i.e., the ratios in Fig. 8, can manifest more algorithmic characteristics. Speci\ufb01cally, PHIL-TD3 and IA-TD3, which own the behavior-cloning objective, are less affected by the incomplete problem de\ufb01nition of the tough setting, whereas HI-TD3, and vanilla TD3, which less or not rely on human guidance, are signi\ufb01cantly degraded in the same condition. Despite the single-frame state in the autonomous driving task is not fairly reasonable, the \ufb01ndings through this comparison are useful. Since numerous complex real-world tasks are intractable to be well-de\ufb01ned or are only partially observable, the strong integration of human guidance into RL, e.g., behavior-cloning, can play a more remarkable role than pure RL algorithms. Then, we investigate the contributions of different components in improving the performance of the proposed PHIL algorithm. The results are provided in Appendix 1. Three components, the behavior cloning objective of Eq. 12 of Section IV-B, the proposed prioritized experience replay mechanism of Section IV-C, and the intervention-based reward shaping mechanism of Section IV-D, are validated to be effective, respectively. The results show that the proposed prioritized human-demonstration replay mechanism plays a crucial role in improving the ultimate performance. Last, we evaluate the computational ef\ufb01ciency. The CPU clock time of different algorithms is compared in table I. It is shown that the training time consumption of the proposed algorithm is similar to that of IA-TD3. This is because the proposed priority calculation scheme consumes very few computational resources. In all, the proposed PHIL-TD3 greatly improves the training ef\ufb01ciency and performance without requiring signi\ufb01cantly higher computational resources. 30 50 70 90 Congestion Noise-injected Congestion Variant Congestion ** 12 16 20 24 Left-turn Noise-injected Left-turn Variant  Left-turn PHIL-TD3 (ours) IA-TD3 HI-TD3 Vanilla TD3 *** * Surviving distance (m) Surviving distance (m) b c 1 2 3 4 1 2 3 4 5 6 74 78 78 72 66 100 100 100 100 96 90 88 82 98 94 88 94 94 90 90 98 92 84 86 70 75 80 85 90 95 100 C NC VC L NL VL PHIL-TD3 (ours) IA-TD3 HI-TD3 Vanilla TD3 Success rate (%) ** * * a Fig. 9. High-level driving performance of different RL strategies under six autonomous driving scenarios. The two noise-injected scenarios and two variant scenarios are different with the two training scenarios, which can examine the robustness and adaptiveness, respectively. \u201cC\u201d and \u201cL\u201d refer to congestion scene and left-turn scene, respectively, while \u201cN\u201d and \u201cV\u201d denote noise-injected and variant scene, respectively. a, the heatmap of success rate. b, the barplot of surviving distance in the left-turn scenarios. The theoretical maximum surviving distance of the scenario is 21 meters. The error bar describes the standard deviation. c, the barplot of surviving distance in the congestion scenarios. The theoretical maximum surviving distance of the scenario is 80 meters. The error bar describes the standard deviation. The paired t-test is adopted for the statistical test. D. Evaluation of Testing Performance of Driving Strategies In this section, the post-trained driving strategies are tested in terms of autonomous driving performance, adaptiveness, and robustness, which can further evaluate the practicality of the above algorithms. The zero-mean Gaussian noises, of which the standard deviation is 5% of the whole control domain, are injected to output commands of the driving strategies to test the robustness. More types and amounts of surrounding vehicles are added to construct variant scenarios to test the adaptiveness. We conduct 50 runs with the same sequence of random seeds for each post-trained strategy in each scenario. The success rate, which is de\ufb01ned as the number of completed runs divided by the total attempts in the same scenario, is taken as the metric for evaluating the safety performance in Fig. 9(a). Our PHIL-TD3 achieves the highest success rate in all scenarios, showing its superior task-completeness abilities. The vanilla TD3, albeit with its unstable training performance, performs competitively like IA-TD3 and HI-TD3 in the testing stage. Considering the two trained scenes (rows 1, 4) and noise-injected scenes (rows 2, 5), three baseline strategies behave acceptably, nevertheless, the scenario variants (rows 3, 6) signi\ufb01cantly degrade their safety. Our PHIL, instead, maintains the highest ability regardless of varying testing conditions, manifesting itself with good robustness and adaptiveness. In Figs. 9(b-c), PHIL-TD3 once again shows its superiority in safety by the highest average surviving distance, and importantly, its performance stability is con\ufb01rmed due to the lowest variance. Fig. 10 can further evaluate the detailed performance of driving strategies. Time consumption of the episode is the secondary target of RL optimization in the left-turn task, which is implied in the reward function of Eq. 31; thus, the related boxplot is illustrated in Fig. 10(a) to access this objective. It is found that the proposed strategy enjoys minimal JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 TABLE II COMPARISON OF THE SURVIVING DISTANCE OF HUMAN-RELATED DRIVING STRATEGIES IN THREE LEFT-TURN SCENARIOS. Surviving distance, meter, \u2191 Left-turn Noise-injected Left-turn Variant Left-turn PHIL-TD3 (ours) 21.28\u00b10.02 21.27\u00b10.02 21.29\u00b10.02 IA-TD3 20.37\u00b12.58 19.75\u00b13.05 19.46\u00b13.03 HI-TD3 20.87\u00b11.76 20.63\u00b11.74 19.90\u00b12.65 Human policy model 20.70\u00b11.82 20.88\u00b11.32 20.90\u00b11.21 TABLE III COMPARISON OF THE SURVIVING DISTANCE OF HUMAN-RELATED DRIVING STRATEGIES IN THREE CONGESTION SCENARIOS. Surviving distance, meter, \u2191 Congestion Noise-injected Congestion Variant Congestion PHIL-TD3 (ours) 80.15\u00b10.08 79.26\u00b14.30 77.29\u00b111.55 IA-TD3 79.26\u00b15.90 78.64\u00b16.11 78.39\u00b17.66 HI-TD3 76.27\u00b116.00 76.02\u00b113.72 73.57\u00b118.49 Human policy model 80.11\u00b10.07 77.66\u00b112.27 75.15\u00b115.20 30 50 70 90 Congestion Noise-injected Congestion Variant Congestion ** 12 16 20 24 Left-turn Noise-injected Left-turn Variant  Leftturn *** * Surviving distance (m) Surviving distance (m) a b ** * * Left-turn Noise-injected Left-turn Variant  Left-turn Congestion Noise-injected  Congestion Variant  Congestion **** **** **** **** * b a PHIL-RD3 (ours) IA-TD3 HI-TD3 Vanilla-TD3 **** **** **** **** **** **** * Fig. 10. Low-level driving performance of different RL strategies under six autonomous driving scenarios.a, the boxplot of time consumption of the episode without failure in the left-turn scenarios. b, the boxplot of average lateral acceleration of the episode in the congestion scenarios. The paired t-test is adopted for the statistical test. time consumption, which is signi\ufb01cantly different from other candidates. In congestion tasks, smoothness is the secondary target of the reward function of Eq. 32; thus, we choose the lateral acceleration as the smoothness measure and provide the associated boxplot in Fig. 10(b). The comparable humaninvolved strategies show their superior smoothness to vanilla TD3 in the training and noise-injected scenes, while the variant congestion scenario pro\ufb01tably validates the advantage of PHIL-TD3. Additionally, we compare the performance of three human guidance-based RL algorithms to the human guidance itself. Speci\ufb01cally, the surviving distance of these human-involved RLs are compared with the human policy model, and the results are provided in Tables II-III, where the results (mean and standard deviation) are calculated by 50 evaluation seeds. The results suggest the superiority of the proposed PHIL-TD3 over the human policy model. Overall, our PHIL-TD3 perpetuates its predominance of training performance and takes the top spot in the testing stage. E. Discussion on Prioritized Human Experience Mechanism In this section, we explore the effect of PHIL-RL from three aspects: the performance improvement by the TDQA mechanism, the merit of the single-buffer experience replay structure, and the algorithmic robustness to bad demonstrations. TDQA, as the crucial innovation of PHIL-TD3, can improve learning performance in the context of human guidancebased RLs, as suggested in Sections VII-C and VII-D. More speci\ufb01cally, it establishes a novel priority indicator to deal with various human guidance. Thus, we \ufb01rst evaluate TDQA by comparing different priority schemes. \u201cQ-adv\u201d represents the scheme in which the priority of human guidance is calculated based only on Q-advantage. \u201cTD\u201d, i.e., temporal difference, the scheme is inherited from the original PER method, but the TD weights of human demonstrations in it are doubled to highlight the human guidance in the replay buffer. Five learning attempts are conducted with the same sequence of random seeds for each candidate, and the corresponding learning curves are in Fig. 11. We \ufb01nd scheme comparison in two training scenarios shows similar trends when observing results in Figs. 11(a-b) and Figs. 11(c-d). The pure TD scheme learns faster than pure Q-advantage in both scenarios, yet its asymptotic scores (both reward and surviving distance) are signi\ufb01cantly lower than those of the Q-advantage scheme. To be more speci\ufb01c, we evaluate different weigh of \u201cTD\u201d and \u201cQ-adv\u201d and provide the learning performance in Fig. 12. Under the same TD, Q-advantage is weighted with three importance levels. In particular, the equal weighting scheme, i.e., w = 1e0, is the adopted default scheme in the report, whereas the other two variants are for comparison. It is shown that a larger TD (w = 1e\u22121) makes faster convergence but can lead to unfavorable asymptotic performance, while a larger Q-advantage (w = 1e2) can achieve the same-level performance as the default setting, despite sometimes slower learning process. The above results, reveal the same performance trends as Fig. 11. That is, TD error accelerates the convergence speed and Q-advantage contributes to improving convergence performance. Essentially, these two schemes score human guidance based on different indicators, and a better indicator can provide RL with more high-quality guidance to improve learning ef\ufb01ciency. Thus, we \ufb01nd TD indicator, as proved in conventional PER, is indeed bene\ufb01cial to rapidly improve performance, nonetheless, the Q-advantage indicator is superior to the TD indicator in the later stage of the training process. The JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12 0 50 100 150 200 250 300 350 400 Training episode 0.1 0.2 0.3 0.4 Reward PHIL(TD+Q-adv) Q-adv TD 300 350 400 IL RD2-TD3 300 350 400 IL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance (m) PHIL(TD+Q-adv) Q-adv TD c d a 0 50 100 150 200 250 300 350 400 Training episode -1.5 -1 -0.5 0 Reward PHIL(TD+Q-adv) Q-adv TD b 300 350 400 HIL-TD3(adopted, w=1e0) HIL-TD3(w=1e2) HIL-TD3(w=1e-1) 300 350 400 HIL-TD3(adopted, w=1e0) HIL-TD3(w=1e2) HIL-TD3(w=1e-1) 0 50 100 150 200 250 300 350 400 Training episode 12 16 20 Surviving distance (m) PHIL(TD+Q-adv) Q-adv TD Fig. 11. Learning efforts of experience replay mechanisms with different priority indicators.. a-b, training rewards in left-turn and congestion scenario, respectively. c-d, surviving distances in left-turn and congestion scenario, respectively. 0 50 100 150 200 250 300 350 400 Training episode 0.1 0.2 0.3 0.4 Reward PHIL(TD+Q-adv) Q-adv TD 0 50 100 150 200 250 300 350 400 Training episode 0 0.1 0.2 0.3 0.4 Reward PHIL RD2-PHIL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode -3 -2 -1 0 Reward PHIL RD2-PHIL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode 5 10 15 20 Surviving distance (m) PHIL RD2-PHIL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance (m) PHIL RD2-PHIL RD2-TD3 c d a b 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance (m) PHIL(TD+Q-adv) Q-adv TD c d a 0 50 100 150 200 250 300 350 400 Training episode -1.5 -1 -0.5 0 Reward PHIL(TD+Q-adv) Q-adv TD b 0 50 100 150 200 250 300 350 400 Training episode 0.1 0.2 0.3 0.4 Reward PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e-1) PHIL-TD3(w=1e2) 0 50 100 150 200 250 300 350 400 Training episode 10 15 20 Surviving distance (m) PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e-1) PHIL-TD3(w=1e2) 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance(m) PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e2) PHIL-TD3(w=1e-1) 0 50 100 150 200 250 300 350 400 Training episode -2 -1.5 -1 -0.5 0 Reward PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e2) PHIL-TD3(w=1e-1) 0 50 100 150 200 250 300 350 400 Training episode 12 16 20 Surviving distance (m) PHIL(TD+Q-adv) Q-adv TD c d a b Fig. 12. Learning efforts of experience replay mechanisms with different weighting schemes. a-b, training rewards in left-turn and congestion scenario, respectively. c-d, surviving distances in left-turn and congestion scenario, respectively. delayed superiority of Q-advantage complies with intuition since unlike the direct indicator as TD, the evaluation ability of the Q network, i.e., the source of Q-advantage, also needs to be trained. The proposed PHIL, which smartly combines both indicators, achieves the most favorable performance in the two scenarios, showing the effectiveness of the TDQA mechanism. PHIL puts the human guidance and exploratory experience of RL into the same experience replay buffer. This structure 0 50 100 150 200 250 300 350 400 Training episode 0.1 0.2 0.3 0.4 Reward PHIL(TD+Q-adv) Q-adv TD 0 50 100 150 200 250 300 350 400 Training episode 0 0.1 0.2 0.3 0.4 Reward PHIL RD2-PHIL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode -3 -2 -1 0 Reward PHIL RD2-PHIL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode 5 10 15 20 Surviving distance (m) PHIL RD2-PHIL RD2-TD3 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance (m) PHIL RD2-PHIL RD2-TD3 c d a b 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance (m) PHIL(TD+Q-adv) Q-adv TD c d a 0 50 100 150 200 250 300 350 400 Training episode -1.5 -1 -0.5 0 Reward PHIL(TD+Q-adv) Q-adv TD b 0 50 100 150 200 250 300 350 400 Training episode 0.1 0.2 0.3 0.4 Reward PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e-1) PHIL-TD3(w=1e2) 0 50 100 150 200 250 300 350 400 Training episode 10 15 20 Surviving distance (m) PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e-1) PHIL-TD3(w=1e2) 0 50 100 150 200 250 300 350 400 Training episode 0 20 40 60 80 Surviving distance(m) PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e2) PHIL-TD3(w=1e-1) 0 50 100 150 200 250 300 350 400 Training episode -2 -1.5 -1 -0.5 0 Reward PHIL-TD3(adopted, w=1e0) PHIL-TD3(w=1e2) PHIL-TD3(w=1e-1) 0 50 100 150 200 250 300 350 400 Training episode 12 16 20 Surviving distance (m) PHIL(TD+Q-adv) Q-adv TD c d a b Fig. 13. Learning efforts of experience replay mechanism with different buffer structure. a-b, the training rewards of algorithms with different experience replay structures in the left-turn scenario and congestion, respectively. c-d, the training surviving distances of algorithms with different experience replay structures in the left-turn and congestion scenario, respectively. 0% 50% 100% VL NL L VC NC C VL NL L VC NC C VL NL L VC NC C PHIL-TD3 (ours) IA-TD3 HI-TD3 Mean of average surviving  distance (Good guidance) Mean of average surviving  distance (Poor guidance) 50% Fig. 14. Stacked barplot of the surviving distance of different human-guided RL strategies under good/poor guidance in all scenarios. differs from the double distributed scheme which is represented by R2D3. To evaluate the performance of these two schemes under the devised autonomous driving tasks, RD2TD3 is developed which utilizes TD as the indicator to respectively retrieve data from two buffers. Additionally, the TDQA priority mechanism is ported to the RD2-TD3 setting forming the other variant, RD2-PHIL. Five learning attempts with the same sequence of random seeds are conducted by RD2-TD3 and RD2-PHIL. Through learning curves in Figs. 13(a-d), it is found that the double distributed buffer scheme, i.e., RD2-PHIL, fails to achieve the same level of learning ef\ufb01ciency as the proposed PHIL. A possible reason behind this is that human guidance can only be utilized in a chunk way under the double-buffer setting, whereas the single buffer scheme of PHIL is more \ufb02exible and friendly to small-scale human guidance data. The conventional RD2-TD3 is least favorable, which is within expectation due to the lack of the TDQA mechanism. To sum up, the results in Fig. 13 support the single-buffer structure utilized in the PHIL-TD3, and pro\ufb01tably suggest the effectiveness of the proposed TDQA mechanism. A general situation occurs that human guidance is not perfect, and thus an unquali\ufb01ed human participant can sometimes conduct actions that are harmful to the task. We test if the unfavorable guidance of the unquali\ufb01ed human would impair the learning process, that is, evaluating the robustness to harmful guidance. It is noticeable that the robustness discussed here is distinguished from that in Section VII-D: we discuss how the algorithms are affected by poor guidance instead of the anti-noise ability of post-trained driving strategies. The human intervention condition of the training stage keeps the same as foregoing experiments, while one-third of the demonstrations from the human model are replaced with random actions to simulate non-pro\ufb01cient human behaviors. Post-trained driving strategies under poor guidance are tested to conduct 50 runs in each scenario and are compared with those under the good guidance of Fig. 9. The stacked barplots in Fig. 14 provide the adversarial testing performance of three human-guidance-based RL algorithms under good and poor guidance. We take the average surviving distance as the metric and the less performance deterioration by poor guidance suggests better robustness. Our PHIL-TD3 exhibits ",
    "References": "REFERENCES [1] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, and T. Graepel, \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, no. 6419, pp. 1140\u20131144, 2018. [2] X. Gao, J. Si, Y. Wen, M. Li, and H. Huang, \u201cReinforcement learning control of robotic knee with human-in-the-loop by \ufb02exible policy iteration,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2021. [3] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, \u201cNavigating occluded intersections with autonomous vehicles using deep reinforcement learning,\u201d in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, Conference Proceedings, pp. 2034\u20132039. [4] J. Wu, Z. Wei, W. Li, Y. Wang, Y. Li, and D. U. Sauer, \u201cBattery thermal-and health-constrained energy management for hybrid electric bus based on soft actor-critic drl algorithm,\u201d IEEE Transactions on Industrial Informatics, vol. 17, no. 6, pp. 3751\u20133761, 2020. [5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [6] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, \u201cRainbow: Combining improvements in deep reinforcement learning,\u201d in Thirty-second AAAI conference on arti\ufb01cial intelligence, Conference Proceedings. [7] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv preprint arXiv:1707.06347, 2017. [8] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \u201cSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,\u201d in International conference on machine learning. PMLR, Conference Proceedings, pp. 1861\u20131870. [9] E. O. Neftci and B. B. Averbeck, \u201cReinforcement learning in arti\ufb01cial and biological systems,\u201d Nature Machine Intelligence, vol. 1, no. 3, pp. 133\u2013143, 2019. [10] M. L. Littman, \u201cReinforcement learning improves behaviour from evaluative feedback,\u201d Nature, vol. 521, no. 7553, pp. 445\u2013451, 2015. [11] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Roth\u00a8orl, T. Lampe, and M. Riedmiller, \u201cLeveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards,\u201d arXiv preprint arXiv:1707.08817, 2017. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14 [12] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, and I. Osband, \u201cDeep q-learning from demonstrations,\u201d in Thirty-second AAAI conference on arti\ufb01cial intelligence, Conference Proceedings. [13] G. Libardi, G. De Fabritiis, and S. Dittert, \u201cGuided exploration with proximal policy optimization using a single demonstration,\u201d in International Conference on Machine Learning. PMLR, Conference Proceedings, pp. 6611\u20136620. [14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski, \u201cHuman-level control through deep reinforcement learning,\u201d nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [15] S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell, M. Riedl, and A. Thomaz, \u201cLearning from explanations using sentiment and advice in rl,\u201d IEEE Transactions on Cognitive and Developmental Systems, vol. 9, no. 1, pp. 44\u201355, 2017. [16] J. MacGlashan, M. K. Ho, R. Loftin, B. Peng, G. Wang, D. L. Roberts, M. E. Taylor, and M. L. Littman, \u201cInteractive learning from policydependent human feedback,\u201d in International Conference on Machine Learning. PMLR, Conference Proceedings, pp. 2285\u20132294. [17] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei, \u201cReward learning from human preferences and demonstrations in atari,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018. [18] W. Saunders, G. Sastry, A. Stuhlm\u00a8uller, and O. Evans, \u201cTrial without error: Towards safe reinforcement learning via human intervention,\u201d in Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems, 2018, Conference Proceedings, p. 2067\u20132069. [19] F. Wang, B. Zhou, K. Chen, T. Fan, X. Zhang, J. Li, H. Tian, and J. Pan, \u201cIntervention aided reinforcement learning for safe and practical policy optimization in navigation,\u201d in Proceedings of The 2nd Conference on Robot Learning, ser. Proceedings of Machine Learning Research, vol. 87. PMLR, 2018, pp. 410\u2013421. [20] C. Gulcehre, T. Le Paine, B. Shahriari, M. Denil, M. Hoffman, H. Soyer, R. Tanburn, S. Kapturowski, N. Rabinowitz, and D. Williams, \u201cMaking ef\ufb01cient use of demonstrations to solve hard exploration problems,\u201d in International Conference on Learning Representations, Conference Proceedings. [21] E. Senft, S. Lemaignan, P. E. Baxter, M. Bartlett, and T. Belpaeme, \u201cTeaching robots social autonomy from in situ human guidance,\u201d Science Robotics, vol. 4, no. 35, 2019. [22] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, \u201cOvercoming exploration in reinforcement learning with demonstrations,\u201d in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, Conference Proceedings, pp. 6292\u20136299. [23] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine, \u201cLearning complex dexterous manipulation with deep reinforcement learning and demonstrations,\u201d in Proceedings of Robotics: Science and Systems, Conference Proceedings. [24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d arXiv preprint arXiv:1511.05952, 2015. [25] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, \u201cA survey of robot learning from demonstration,\u201d Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009. [26] S. Fujimoto, H. Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d in International Conference on Machine Learning. PMLR, Conference Proceedings, pp. 1587\u20131596. [27] S. Ross, G. Gordon, and D. Bagnell, \u201cA reduction of imitation learning and structured prediction to no-regret online learning,\u201d in Proceedings of the fourteenth international conference on arti\ufb01cial intelligence and statistics. JMLR Workshop and Conference Proceedings, Conference Proceedings, pp. 627\u2013635. [28] M. Liebner, M. Baumann, F. Klanner, and C. Stiller, \u201cDriver intent inference at urban intersections using the intelligent driver model,\u201d in 2012 IEEE Intelligent Vehicles Symposium. IEEE, Conference Proceedings, pp. 1162\u20131167. [29] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, \u201cCarla: An open urban driving simulator,\u201d in Conference on robot learning. PMLR, Conference Proceedings, pp. 1\u201316. [30] A. Y. Ng, D. Harada, and S. Russell, \u201cPolicy invariance under reward transformations: Theory and application to reward shaping,\u201d in Icml, vol. 99, Conference Proceedings, pp. 278\u2013287. APPENDIX Theorem 1 (Policy Optimality Invariance of the Human Intervention-based Reward Shaping): Let the interventionbased reward shaping function F : S \u00d7 A \u00d7 S \u2192 R add a negative constant to the human intervened state as Eq. 17, if the human intervention will certainly occur at state st when the next state st+1 is unacceptable, then the reward shaping function F does not change the policy optimality. Proof 1: According to [30], potential-based reward shaping function F : S \u00d7A\u00d7S is proven to be the only form that can preserve policy optimality. Speci\ufb01cally, F is represented as: F(st, at, st+1) = \u03b3\u03a6(st+1)) \u2212 \u03a6(st), (33) where \u03a6 : S \u2192 R is called the potential function de\ufb01ned over the state space. Thus, the proof converts to construct potential function \u03a6. De\ufb01ne the potential function \u03a6 as: \u03a6(st) = \ufffd rpen \u03b3 , if st is unacceptable 0, otherwise. (34) Then, when humans intervene in the state st (meaning st+1 is unacceptable), F becomes: F(st, at, st+1) = \u03b3\u03a6(st+1) \u2212 \u03a6(st) = rpen \u03b3 \u00b7 \u03b3 \u2212 0 = rpen. (35) And when humans do not intervene the state, F becomes: F(st, at, st+1) = \u03b3\u03a6(st+1) \u2212 \u03a6(st) = 0 \u2212 0 = 0. (36) Lumping Eqs. 35 and 36, F turns into the reward-shaping term of Eq. 17, shown as: rshape t =rt + F(st, at, st+1) =rt + rpen \u00b7 [(\u2206t = I) \u2227 (\u2206t\u22121 = 0)], (37) where [(\u2206t = I)\u2227(\u2206t\u22121 = 0)] refers to the intervention event of the human. Hence, we complete the proof. Remark 1: Theorem 1 is established on the below assumptions: humans are considered to owe invariant judgment on the environment state. In this manner, the \u03a6 can be seen as a stable function de\ufb01ned in the state space. Remark 2: The assumption of Remark 1 is hard to be maintained in practice. This is because 1) the varying mental and physical status of one speci\ufb01c human participant would affect its accurate judgment on the environment state; 2) the judgment on the environment will be varying across different human participants; 3) the state space in the context of deep networks (the image-based one in our manuscript) is intractable to be identi\ufb01ed by humans accurately. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15 TABLE A1 CONFIGURATION OF THE EXPERIMENTAL PLATFORM. Type Description Details Workstation Operation system Ubuntu 20.04 Workstation CPU + RAM AMD Ryzen 3900X + 32GB Workstation GPU NVIDIA RTX 2080S Driving simulator Scenario software CARLA Driving simulator Steering wheel suit Logitech G29 Driving simulator Displays Joint heads-up monitors\u00d73 Driving simulator Other equipment Driver seat suit Robotic vehicle Vehicle brand Wheeled UGV-Hunter Robotic vehicle Size dimension 1000mm \u00d7 740mm \u00d7 400mm Robotic vehicle Communication type ROS publisher-subscriber Robotic vehicle Calculation board Xavier NX Dev Kit Other Programming Python Other Neural network toolbox Pytorch TABLE A2 ARCHITECTURE AND DETAILS OF VALUE NEURAL NETWORK (CRITIC) Parameter Value Input (state + action) shape [80,45,2] + [1] Network convolution Filter feature [6,16] (kernel size 6 \u00d7 6) Network pooling feature Maxpooling (Stride 2) Network fully connected layer feature [256,128,64] TABLE A3 ARCHITECTURE AND DETAILS OF POLICY NEURAL NETWORK (ACTOR) Parameter Value Input (state) shape [80,45,2] Network convolution Filter feature [6,16] (kernel size 6 \u00d7 6) Network pooling feature Maxpooling (Stride 2) Network fully connected layer feature [256,128,64] TABLE A4 ARCHITECTURE AND DETAILS OF DAGGER-BASED HUMAN POLICY MODEL Parameter Value Input (state) shape [80,45,1] Network convolution Filter feature [6,16] (kernel size 6 \u00d7 6) Network pooling feature Maxpooling (Stride 2) Network fully connected layer feature [256,128,64] TABLE A5 HYPERPARAMETERS FOR RL TRAINING Type Description Details Maximum episode Cutoff episode number of the training process 400 Minibatch size (N) Capacity of minibatch 128 Actor learning rate Initial learning rate (policy/actor networks) 5e-4 Critic learning rate Initial learning rate (value/critic networks) 2e-4 Learning rate decay Delay of learning rate (per episode) 0.996 Activation function Activation function of the networks Relu Initial exploration Initial exploration rate of noise in \u03f5 greedy 1 Final exploration Cutoff exploration rate of noise in \u03f5 greedy 0.05 Gamma (\u03b3) Discount factor of the Bellman equation 0.95 Soft updating factor Parameter update frequency to target networks 1e-3 Noise scale (\u03f5) Noise amplitude of action in TD3 0.2 Bounding box (c) Bounding of the exploratory action in TD3 1 Policy delay (d) Update frequency of critic over actor 1 TABLE A6 HYPERPARAMETERS FOR THE PER MECHANISM Type Description Details Replay buffer size Capacity of PER buffer 1e5 Priority factor (\u03b1) Priority scaling factor 0.6 Sample factor (\u03b2) Importance sampling correlation 1 Offset factor (\u03b5) Tiny constant avoiding zero retrieving probability 1e-3 TABLE A7 HYPERPARAMETERS FOR DAGGER-BASED HUMAN POLICY MODEL. Type Description Details Learning rate Initial learning rate 1e-4 Activation function Activation function of the network Relu Maximum episode Cutoff episode number of the training process 50 Batch size Capacity of minibatch 128 0 100 200 300 400 Training episode -3 -2.5 -2 -1.5 -1 -0.5 0 Reward PHIL-TD3(ours) Without Q-adv Without reward shaping Without behavior cloning 0 100 200 300 400 Training episode 0 20 40 60 80 Surviving distance(m) PHIL-TD3(ours) Without Q-adv Without reward shaping Without behavior cloning 0 100 200 300 400 Training episode 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Reward PHIL-TD3(ours) Without Q-adv Without reward shaping Without behavior cloning 0 100 200 300 400 Training episode 10 15 20 Surviving distance (m) PHIL-TD3(ours) Without Q-adv Without reward shaping Without behavior cloning a b d c Fig. 1. Ablation study of the proposed algorithm. a-b, curves of training rewards and surviving distances in the left-turn scenario, respectively. c-d, curves of training rewards and surviving distances in the congestion scenario, respectively. ",
    "title": "Prioritized Experience-based Reinforcement",
    "paper_info": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nPrioritized Experience-based Reinforcement\nLearning with Human Guidance for Autonomous\nDriving\nJingda Wu, Student Member, IEEE Zhiyu Huang, Student Member, IEEE Wenhui Huang, and Chen Lv, Senior\nMember, IEEE\nAbstract\u2014Reinforcement learning (RL) requires skillful de\ufb01-\nnition and remarkable computational efforts to solve optimiza-\ntion and control problems, which could impair its prospect.\nIntroducing human guidance into reinforcement learning is a\npromising way to improve learning performance. In this paper,\na comprehensive human guidance-based reinforcement learning\nframework is established. A novel prioritized experience replay\nmechanism that adapts to human guidance in the reinforcement\nlearning process is proposed to boost the ef\ufb01ciency and per-\nformance of the reinforcement learning algorithm. To relieve\nthe heavy workload on human participants, a behavior model\nis established based on an incremental online learning method\nto mimic human actions. We design two challenging autonomous\ndriving tasks for evaluating the proposed algorithm. Experiments\nare conducted to access the training and testing performance and\nlearning mechanism of the proposed algorithm. Comparative re-\nsults against the state-of-the-art methods suggest the advantages\nof our algorithm in terms of learning ef\ufb01ciency, performance,\nand robustness.\nIndex Terms\u2014Reinforcement learning, priority experience re-\nplay, human demonstration, autonomous driving.\nI. INTRODUCTION\nR\nEINFORCEMENT learning (RL) has substantially con-\ntributed to numerous \ufb01elds [1]\u2013[4] by solving control\nand optimization problems. As a branch of machine learning\nmethods, RL improves the capability of controlling agents in\nblack-box environments through the exploratory trial-and-error\nprinciple [5]. Recent popular RL algorithms, e.g., rainbow\ndeep Q-learning [6], proximal policy optimization (PPO) [7],\nand soft actor-critic (SAC) [8], have shown ability in handling\nhigh-dimensional environment representation and generaliza-\ntion, due to the introduction of deep neural networks. Albeit\nRL can achieve good performance in complex tasks, its draw-\nback emerges that their interactions with the environment are\nvery inef\ufb01cient [9]. Thus, using RL to solve a problem needs\nskillful de\ufb01nitions and settings and consumes remarkable\ncomputational resources [10].\nCombining human guidance with RL can be a promising\nway to mitigate the above drawback [11]. First, human inter-\nJ. Wu, Z. Huang, W. Huang and C. Lv are with the School of Me-\nchanical and Aerospace Engineering, Nanyang Technological University, Sin-\ngapore, 639798. (E-mail: {jingda001, zhiyu001, wenhui001}@e.ntu.edu.sg,\nlyuchen@ntu.edu.sg)\nCorresponding author: Chen Lv\nThis paper has been published in IEEE Transactions on Neural Networks\nand Learning Systems. DOI: 10.1109/TNNLS.2022.3177685\nThe code associated with this paper is available at this link\nvention has been used to improve RL performance. Interven-\ntion is triggered by unfavorable actions and should be avoided\nby RL. Then, the human demonstration is a powerful tool\nto enhance RL\u2019s ability [12]. In this context, the objective\nfunctions are generally reshaped compatible with supervised\nlearning to improve ef\ufb01ciency [13]. Despite the above hu-\nman guidance-based methods, RL needs to process numerous\ndata from its self-explorations. The existing methods do not\nparticularly optimize the utilization of human guidance data;\nconsequently, they still need great human workloads to avoid\nsubmersion of guidance in exploratory data. Additionally,\nhuman guidance, which is variant to pro\ufb01ciency, mental and\nphysical status of participants, should not be equally treated\nsince some low-quality guidance can even impair the RL\nperformance.\nWe propose a priority-based experience replay method\non human guidance and put forward the associated human\nguidance-based RL algorithm to bridge the abovementioned\ngap. Our approach is off-policy, which leverages the experi-\nence replay mechanism [14] to maximize the utilization ef\ufb01-\nciency of self-exploratory data. The proposed priority replay\nmechanism can further improve the utilization ef\ufb01ciency of\nhuman guidance data by quantifying their values and weighing\ntheir utilized probability, which ultimately augments the RL\nperformance. As a result, the ef\ufb01ciency can be improved\nby over seven times under the adopted task. The schematic\ndiagram of our algorithm is depicted in Fig. 1. To evaluate\nthe training and testing performance of our proposed method,\nwe design two challenging autonomous driving scenarios.\nThe experimental results suggest the advance of the proposed\nalgorithm compared to state-of-the-art baselines in learning\nef\ufb01ciency, practical performance, and robustness.\nThe contribution of this report can be summarized into\nthree aspects. 1) We propose a novel prioritized experience\nutilization mechanism regarding human guidance in the RL\nprocess to improve performance. 2) We establish a com-\nprehensive and holistic framework of human guidance-based\nRL by integrating the human-RL action switch scheme, be-\nhavior cloning-based objective function, human-demonstration\nreplay method, and human-intervention reward shaping mech-\nanism.3) We validate the superior performance of the proposed\nalgorithm in solving challenging autonomous driving tasks\ncomprehensively.\nThe remainder is organized as follows: a review of related\nwork is provided in Section II, preliminaries for the proposed\narXiv:2109.12516v2  [cs.LG]  29 Nov 2022\n",
    "GPTsummary": "- (1): The article aims to propose a human guidance-based reinforcement learning framework for autonomous driving and the efficient utilization of human demonstration data to address the inefficiency and heavy computational burden of traditional RL algorithms.\n\n- (2): The past methods of RL algorithms are limited in their utilization of human guidance data and require a significant amount of computational resources. The proposed approach addresses these limitations with a novel prioritized experience replay mechanism that quantifies the value of human guidance data and a behavior model based on incremental online learning to mimic human actions. \n\n- (3): The proposed methodology integrates a human-RL action switch scheme, behavior cloning-based objective function, human-demonstration replay method, and human-intervention reward shaping mechanism to establish a comprehensive and holistic human guidance-based reinforcement learning framework.\n\n- (4): Experiments are conducted on two challenging autonomous driving tasks to evaluate the performance of the proposed algorithm, and comparative results against state-of-the-art methods suggest the advantages of the proposed algorithm in terms of learning efficiency, performance, and robustness. The performance achieved supports the goals of improving the efficiency and accuracy of RL algorithms for autonomous driving tasks.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work is proposing a human guidance-based reinforcement learning framework for autonomous driving that efficiently utilizes human demonstration data to improve the learning efficiency, performance, and robustness of the RL algorithms.\n\n- (2): Innovation point: The proposed method integrates a prioritized experience replay mechanism and behavior model based on incremental online learning to efficiently utilize human guidance data. Moreover, it establishes a comprehensive and holistic human guidance-based reinforcement learning framework by integrating multiple approaches, such as behavior cloning-based objective function, human-demonstration replay method, and human-intervention reward shaping mechanism. Performance: The proposed algorithm outperforms state-of-the-art methods in terms of learning efficiency, performance, and robustness on two challenging autonomous driving tasks. Workload: The article clearly explains the proposed algorithms and conducts comprehensive experiments to demonstrate the effectiveness of the proposed method. However, the real-world implementation of the proposed algorithm may require further study in terms of the computational cost and scalability.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work is proposing a human guidance-based reinforcement learning framework for autonomous driving that efficiently utilizes human demonstration data to improve the learning efficiency, performance, and robustness of the RL algorithms.\n\n- (2): Innovation point: The proposed method integrates a prioritized experience replay mechanism and behavior model based on incremental online learning to efficiently utilize human guidance data. Moreover, it establishes a comprehensive and holistic human guidance-based reinforcement learning framework by integrating multiple approaches, such as behavior cloning-based objective function, human-demonstration replay method, and human-intervention reward shaping mechanism. Performance: The proposed algorithm outperforms state-of-the-art methods in terms of learning efficiency, performance, and robustness on two challenging autonomous driving tasks. Workload: The article clearly explains the proposed algorithms and conducts comprehensive experiments to demonstrate the effectiveness of the proposed method. However, the real-world implementation of the proposed algorithm may require further study in terms of the computational cost and scalability.\n\n\n"
}