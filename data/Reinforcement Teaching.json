{
    "Abstract": "",
    "Introduction": "Introduction As machine learning becomes ubiquitous, there is a growing need for algorithms that generalize better, learn more quickly, and require less data. One way to improve a machine learning algorithm, without hand-engineering the underlying algorithm, is meta-learning. Meta-learning is often thought of as \u201clearning to learn\u201d where the goal is to learn about and improve another machine learning process [1]. A variety of sub-domains have emerged that design hand-crafted solutions for learning about and improving a speci\ufb01c component of a machine learning process. The work in these sub-domains focus on solving one speci\ufb01c problem, whether that be \ufb01nding the best way to augment data [2], sample minibatches [3], adapt objectives, [4] or poison rewards [5]. Consequently, the meta-learning methods used in these domains are handcrafted to solve the problem and cannot be applied to solve new problems in a different domain. Current literature fails to recognize that a more general framework can be used to simultaneously address multiple problems across these varied sub-domains. Therefore, this work takes an important step toward answering the following question: Can we develop a unifying framework for improving machine learning algorithms that can be applied across sub-domains and learning problems? As a crucial step towards this unifying framework, we introduce Reinforcement Teaching: an approach that frames meta-learning in terms of learning in a Markov Decision Process (MDP). In Reinforcement Teaching, a teacher learns a policy via Reinforcement Learning (RL) to improve the learning process of a student. The teacher observes a problem agnostic representation of the student\u2019s behavior and takes actions that adjust components of the student\u2019s learning process that the student is unable to change, such as the objective, optimizer, data, or environment. The teacher\u2019s reward is then based Preprint. Under review. arXiv:2204.11897v2  [cs.LG]  22 May 2022 ",
    "Related Work": "Related Work Learning to Teach Using Reinforcement Learning: Previous works that attempt to learn a policy that controls another learning process have been restricted to speci\ufb01c problems [2; 6\u20139]. Work in this area uses problem-speci\ufb01c heuristics or simple non-Markov state representations [3; 4; 18\u201320]. This is because the Markov state representation, the student\u2019s parameters, is a large and unstructured state representation that makes it dif\ufb01cult to learn an effective policy. As a representative of the heuristic approach, the Learning to Teach (L2T) framework successfully learned to sample minibatches for a supervised learner [3; 4]. In the L2T framework, the teacher\u2019s state representation includes several heuristics about the data and student model, and is heavily designed for the task of minibatch sampling. These works are tailored to the base problems they solve and are unable to generalize to new problems with their state and reward design. Some works have identi\ufb01ed that learning from parameters is theoretically ideal for curriculum learning [21; 22]. When the student is a tabular reinforcement learner, effective policies have been learned from the parameter state representation [5; 23]. Until now, no work has identi\ufb01ed that the behavioral approach proposed in this paper can approximate the true Markov state while enabling tractable, generalizable, and transferable learning algorithms. Learning Progress: Connected to the idea of teaching is a rich literature on learning progress. Learning progress prescribes that a learning agent should focus on tasks for which it can improve on. This mechanism drives the agent to learn easier tasks \ufb01rst, before incrementally learning tasks of increasing complexity [24]. Learning progress has been represented in several ways such as, the change in model loss, model complexity, and prediction accuracy. In addition, learning progress has been successfully applied in a variety of contexts, including curriculum learning [24\u201327], developmental robotics [24; 28; 29], and intelligent tutoring systems [30]. 2 Learned Parameter Representations: Previous work in RL has argued that policies can be represented by a concatenated set of outputs [31; 32]. Policy eValuation Networks (PVN) in reinforcement learning show that representations of a neural policy can be learned through the concatenated outputs of probing states. PVN is similar to the parametric-behavior embedder because it characterizes a neural network by its output behavior. To learn a PVN representation, however, requires a \ufb01xed set of inputs, referred to as probing states. While the probing states can be learned, they are still \ufb01xed after learning and cannot adapt to different policies. In our setting, the student\u2019s neural network is frequently changing due to parameter updates and it is unlikely that the outputs of a \ufb01xed set of inputs can represent the parameters along this trajectory. Furthermore, similar work has shown that learning directly from parameters is more perfomant for policy evaluation [33]. Machine Teaching: The goal of machine teaching is for a teacher to \ufb01nd a training set such that a machine learner (student) can learn a target model. Under the Reinforcement Teaching perspective, machine teaching can be viewed as an RL teacher whose action determines the data that the student uses for learning. Traditional machine teaching assumes the teacher has access to an optimal student model, learning algorithm, and objective function [10]. These assumptions are unrealistic in practice. We show that our parametric-behavior embedded state and learning progress reward allow the teacher to learn a policy while only having access to the student\u2019s inputs/outputs and performance. Meta-Learning: While Reinforcement Teaching does not explicitly build on previous meta-learning work, we point out common meta-learning methods and how they relate to Reinforcement Teaching. Early work in meta-learning with neural networks [34\u201337] inspired follow-up work on learned optimizers [38; 39]. Learned optimizers replace the \ufb01xed learning algorithm with a memory-based parameterization, usually an LSTM [40]. Learning the optimizer through reinforcement learning has also been explored [41; 42]. This work, like L2T [3], employs an ad-hoc state representation and reward function. Optimization-based meta-learning has other applications, such as in few-shot learning [38] and meta-RL [43; 44]. Another approach to meta-learning is gradient-based metalearning, such as Model Agnostic Meta Learning (MAML) [11] and other work in meta-RL [12]. These methods are distinguished from optimization-based meta-learning for the lack of a separately parameterized meta-learner. Instead, meta-information is encoded in \u03b8 by differentiation through gradient descent itself. We show how MAML can be interpreted as a model-based method that differentiates through the Teaching MDP dynamics in Appendix G. 3 Reinforcement Teaching Figure 1: Teaching MDP: The teacher takes actions a \u2208 A, which will in\ufb02uence an aspect of the student, f\u03b8, Alg or its learning domain D. The student will then update its model under the new con\ufb01guration. The student\u2019s learning process will then output r, s\u2032. Before introducing Reinforcement Teaching, we \ufb01rst describe the MDP formalism that underpins reinforcement learning [45\u201347]. An MDP M is de\ufb01ned by the tuple (S, A, r, p, \u00b5, \u03b3), where S is the state space, A denotes the action space, S is the state space, r : A \u00d7 S \u2192 R is the reward function that maps a state and an action to a scalar reward, p : S \u00d7 A \u00d7 S \u2192 [0, 1] is the state transition function, \u00b5 is the initial state distribution, and \u03b3 is the discount factor. Lastly, a Markov reward process (MRP) is an MDP without actions [46]. For an MRP, both the reward function r : S \u2192 R and state transition p : S \u00d7 S \u2192 [0, 1] are no longer explicitly a function of an arbitrary action. Instead, actions are unobserved and selected by some unknown behavior policy. In Reinforcement Teaching, student refers to any learning agent or machine learning model, and teacher refers to an RL agent whose role is to learn about and improve the student\u2019s learning process. We start by de\ufb01ning the components of the student\u2019s learning process. We then identify states and rewards, thereby formulating the student\u2019s learning process as an MRP. This MRP perspective on learning processes allows the Reinforcement Teaching framework to be applied to different types of students with varying data domains, learning algorithms, and goals. Lastly, we introduce an action set for the teacher which allows the teacher to alter the student\u2019s learning process. This induces an MDP, in which the teacher learns a policy that interacts with a student\u2019s learning process to achieve a goal (see Figure 1). 3 3.1 Components of the Learning Process To start, we de\ufb01ne the components of the student\u2019s learning process. Consider a student, f\u03b8, with learnable parameters \u03b8 \u2208 \u0398. The student receives experience from a learning domain D, which can be labelled data (supervised learning), unlabelled data (unsupervised learning), or an MDP (reinforcement learning). How the student interacts with its domain, and how it learns given that interaction, is speci\ufb01ed by a learning algorithm Alg. The learning algorithm updates the student\u2019s parameters, \u03b8t+1 \u223c Alg(f\u03b8t, D), in order to maximize a performance measure that evaluates the student\u2019s current ability, m(f\u03b8, D). One natural choice for m is the objective function directly optimized by Alg, but m can also be a non-differentiable surrogate objective such as accuracy in classi\ufb01cation, or the Monte-Carlo return in RL. The combination of the student, learning domain, learning algorithm, and performance measure is hereafter referred to as the student\u2019s learning process, E = (f\u03b8, D, Alg, m). In the remainder of Section 3, we will outline how the components of the learning process interact as the student learns the optimal parameters that maximize its performance measure \u03b8\u2217 = arg max\u03b8 m(f\u03b8, D). 3.2 States of Reinforcement Teaching We de\ufb01ne the state of the learning process as the student\u2019s current learnable parameters, st = \u03b8t. Therefore, the state space is the set of possible parameters, S = \u0398. The initial state distribution, \u00b5, is determined by the initialization method of the parameters, such as Glorot initialization for neural networks [48]. Lastly, the state transitions, p, are de\ufb01ned through the learning algorithm, \u03b8t+1 = Alg(f\u03b8t, D), which can be stochastic in general. The sequence of learnable parameters, {\u03b8t}t\u22650, form a Markov chain as long as D and Alg do not maintain a state that depends on the parameter history. This is the case, for example, when the learning domain is a dataset 1, D = {xi, yi}N i=1, and the learning algorithm is gradient descent on an objective function, \u03b8\u2032 := Alg(f\u03b8, D) = \u03b8 \u2212 \u03b1\u2207\u03b8 1 N \ufffdN i=1 J(f\u03b8(xi), yi) [49; 50]. While adaptive optimizers violate the Markov property of Alg, we discuss ways to remedy this issue in Appendix D and demonstrate that it is possible to learn a policy that controls Adam [17] in Section 4.2. 3.2.1 Parametric-behavior Embedder Although \u03b8 is a Markov state representation, it is not ideal for learning a policy. To start, the parameter space is large and mostly unstructured, especially for nonlinear function approximators. While there is some structure and symmetry to the weight matrices of neural networks [51; 52], this information cannot be readily encoded as an inductive bias of a meta-learning architecture. Often, the parameter set is de-structured through \ufb02attening and concatenation, further obfuscating any potential regularities in the parameter space. Ideally, the teacher\u2019s state representation should be much smaller than the parameters. In addition, the teacher\u2019s state representation should allow for generalization to new student models with different architectures or activations, which is not feasible with the parameter state representation. See Section 4 for evidence on the dif\ufb01culty of learning from parameters. To avoid learning from the parameters directly, we propose the Parametric-behavior Embedder (PE), a novel method that learns a representation of the student\u2019s parameters from the student\u2019s behavior. To capture the student\u2019s behavior, we use the inputs and outputs of f\u03b8. For example, if the student is a classi\ufb01er, the inputs to f\u03b8 would be the features xi, and the outputs would be the classi\ufb01er\u2019s predictions, f\u03b8(xi). To learn the PE state representation, we \ufb01rst assume that we have a dataset or replay buffer to obtain the student inputs, xi. Then we can randomly sample a minibatch of M inputs, {xi}M i=1, and retrieve the student\u2019s corresponding outputs, f\u03b8(xi). The set of inputs and outputs \u02c6s = {xi, f\u03b8(xi)}M i=1, or mini-state, provides local information about the true underlying state s = \u03b8. To learn a vectorized representation from the mini-state, we recognize that \u02c6s is a set and use a permutation invariant function h to provide the PE state representation h(\u02c6s) [53]. The input-output pair is jointly encoded before pooling, h(\u02c6s) = hpool \ufffd {hjoint(xi, f\u03b8(xi))}M i=1 \ufffd , where hpool is a pooling operation over the minibatch dimension (see Appendix for Figure 6). We argue that the local information provided by the student\u2019s behavior, for a large enough minibatch of inputs and outputs, is enough to summarize pertinent information about \u03b8, while still maintaining 1RL environments are also Markovian learning domains if the environment itself is a Markovian. 4 the Markov property. Methods that attempt to learn directly from the parameters must learn to ignore aspects of the parameters that have no bearing on the student\u2019s progress. This is inef\ufb01cient for even modest neural networks. As we demonstrate in Section 4.2, PE is the only state representation that allows a teacher to learn a step-size adaptation policy that improves Adam. 3.3 Rewards of Reinforcement Teaching Given a reward function, r, we further formalize the learning process as an MRP, E = (S, r, p, \u00b5), where the state-space (S), initial distribution (\u00b5), and state-transition dynamics (p) are de\ufb01ned in Section 3.2. The learning process is formalized as an MRP for two reasons: (1) learning processes are inherently sequential, and therefore an MRP is a natural way to depict the evolution of the student\u2019s parameters and performance, and (2) MRPs provide a unifying framework for all possible students. To specify the reward function, we \ufb01rst identify that reaching a high-level of performance is a common criterion for training and measuring a learner\u2019s performance.2 For ease of reference, let m(\u03b8) := m(f\u03b8, D). A simple approach is the time-to-threshold reward in which a learner is trained until a performance condition is reached, such as a suf\ufb01ciently high performance measure (i.e., m(\u03b8) \u2265 m\u2217 for some threshold m\u2217) [21]. In this case, the reward is constant r(\u03b8) = \u2212I (m(\u03b8) < m\u2217) until the condition, m(\u03b8) \u2265 m\u2217, is reached, which then terminates the episode. Similar to the argument in Section 3.2, the reward function r(\u03b8) = \u2212I (m(f\u03b8, D) < m\u2217) is also Markov as long as the learning domain is Markov. The performance measure itself is always Markov because, by de\ufb01nition, it evaluates the student\u2019s current ability. 3.3.1 Reward Shaping with Learning Progress Under the time-to-threshold reward [21], the teacher is rewarded for taking actions such that the student reaches a performance threshold m\u2217 as quickly as possible. We argue, however, that this binary reward formulation lacks integral information about the student\u2019s learning process. We instead de\ufb01ne a new reward function based on the student\u2019s learning progress. The learning progress signal provides feedback about the student\u2019s relative improvement, and better informs the teacher about how its policy in\ufb02uences the student. We de\ufb01ne Learning Progress (LP) as the change in the student\u2019s performance measure, LP(\u03b8\u2032, \u03b8) = m(\u03b8\u2032) \u2212 m(\u03b8) at subsequent states \u03b8 and \u03b8\u2032 of the student\u2019s learning process. To shape the time-tothreshold reward, we simply add the learning progress term LP(\u03b8\u2032, \u03b8) to the existing reward r(\u03b8\u2032) previously described. Therefore, our resulting LP reward function is r(\u03b8\u2032, \u03b8) = \u2212I (m(\u03b8) < m\u2217) + LP(\u03b8\u2032, \u03b8) until m(\u03b8) \u2265 m\u2217, terminating the episode. It follows that learning progress is a potentialbased reward shaping, given by r\u2032 = r + \u03a6(\u03b8\u2032) \u2212 \u03a6(\u03b8) where the potential is the performance measure \u03a6(\u03b8) = m(\u03b8). This means that combining learning progress with the time-to-threshold reward does not change the optimal policy [54]. Unlike the time-to-threshold reward function, the LP reward provides critical information to the teacher regarding how its actions affected the student\u2019s performance. The LP term indicates the extent to which the teacher\u2019s adjustment (i.e., action) improved or worsened the student\u2019s performance. For example, if the teacher\u2019s action results in a negative LP term, this informs the teacher that with the student\u2019s current skill level (as de\ufb01ned by the student\u2019s parameters), this speci\ufb01c action worsened the student\u2019s performance, thereby deterring the teacher from selecting such an action. We show empirically that compared to the time-to-threshold reward, the LP reward function enables the teacher to learn a more effective teaching policy (See Section 4). 3.4 Actions of Reinforcement Teaching The MRP model demonstrates how the student\u2019s learning process can be viewed as a sequence of parameters, {\u03b8t}t\u22650, with rewards describing the student\u2019s performance at particular points in time, {m(\u03b8t)}t>0. However, the goal of meta-learning is to improve this learning process. The teacher now oversees the student\u2019s learning process and takes actions that intervene on this process, thus transforming the MRP into the Teaching MDP, M = (S, A, p, r, \u00b5). Aside from the action space, A, the remaining elements of the Teaching MDP tuple have been de\ufb01ned in the previous subsections. 2In Appendix C, we outline alternative reward criteria and reward shaping in the Teaching MRP. 5 We now introduce the action set, A, that enables the teacher to control some component of the student learning process. An action can change the student con\ufb01guration or learning domain of the student, as shown in Figure 1. The choice of action space induces different meta-learning problem instances (See Figure B), such as learning to sample, learning to explore, curriculum learning (learning a policy for sequencing tasks), and adaptive optimization (learning to adapt the step-size). Lastly, the action set determines the time-step of the teaching MDP. The base time-step is each application of Alg, which updates the student\u2019s parameters. The teacher can operate at this frequency in settings where it controls an aspect of the learning algorithm, such as the step-size. Acting at a slower rate induces a semi-MDP [55]. If the teacher controls the learning domain, such as setting an episodic goal for an RL agent, then the teacher could operate at a slower rate than the base time-step. 4 Experiments To demonstrate the generality and effectiveness of Reinforcement Teaching, we conduct experiments in both curriculum learning and adaptive optimization domains. In the curriculum learning setting, we show that the teacher using the PE state representation and LP reward function signi\ufb01cantly outperforms other RL teaching baselines in both discrete and continuous environments. For the adaptive optimization setting, we show that a teacher that adapts the step-size of an optimizer must use the PE state representation to outperform Adam with the best constant step-size. We further show that this step-size adapting teacher learns a policy that generalizes to new architectures and datasets. Our results con\ufb01rm that both PE state and LP reward are critical for Reinforcement Teaching, and signi\ufb01cantly improves over previously established baselines. 4.1 Reinforcement Learning In this section, we apply our Reinforcement Teaching framework to the curriculum learning problem. Our goal is for the teacher to learn a policy for sequencing sub-tasks such that the student can solve a target task quickly. In our experiments, we consider both discrete and continuous environments: an 11 x 16 tabular maze, Four Rooms [56] adapted from the MiniGrid suite [56], and Fetch Reach [57]. To formalize curriculum learning through Reinforcement Teaching, we establish the teaching MDP. To start, the teacher\u2019s actions will control an aspect of the student\u2019s environment; Either the start state for the maze and Four Rooms experiments or the goal distribution for Fetch Reach. For the student\u2019s learning algorithm, we used Q learning [46], PPO [58], and DDPG [59] for the maze, Four Rooms, and Fetch Reach environments, respectively. This highlights that Reinforcement Teaching can be useful for a variety of students. For the teacher\u2019s state representation, we consider two variants of PE that use different student outputs f\u03b8. In both cases, the inputs are the states that the student encounters during its learning process. For PE-QValues, the embedded outputs are the state-action values, whereas for PE-Action, the embedded outputs are one-hot encodings of the student\u2019s greedy policy. In addition, for all reward functions, the performance measure is the student\u2019s return on the target task. Now, to train the teacher, we use DQN [60]. See Appendix M.1 for full details on the teacher-student training protocol and hyperparameters. The trained teacher\u2019s policy is evaluated on a newly initialized student to determine the impact of the learned curriculum on the student\u2019s learning ef\ufb01ciency and \ufb01nal performance on the target task. To analyze the effectiveness of the PE state and the LP reward function on the teacher\u2019s policy, we compare against the following baselines: L2T [3], Narvekar et al. (2017) [21], a random teacher policy, and a student learning the target task from scratch (no teacher). Narvekar et al. (2017) uses the parameter state representation with the time-to-threshold reward. L2T uses a heuristic based state representation and a variant of the time-to-threshold reward. All results are averaged over 10 seeds with shaded regions indicating 95 % con\ufb01dence intervals (CI). Experimental Results Across all environments, we found that by using the PE state representation and the LP reward signal together, the teacher is able to learn a superior curriculum policy compared to the baselines. These teacher policies generated a curriculum of start/goal states for the student that improved the student\u2019s learning ef\ufb01ciency and/or \ufb01nal performance, as shown in Figure 2. In addition, in our approach the teacher is able to learn a curriculum policy more ef\ufb01ciently than the baselines (See Appendix J). 6 Figure 2: Left: Maze, Middle: Four Rooms, Right: Fetch Reach. Student learning curve on the target task with the assistance of the respective teacher policies. Purple/orange curves indicate our methods. State Ablation PE (Ours) Parameters Fan et al. (2018) state Four Rooms 26.54 \u00b1 0.917 2.12* \u00b1 2.31 24.61* \u00b1 1.3 Fetch Reach 34.55 \u00b1 3.16 13.94* \u00b1 13.5 31.2 \u00b1 2.39 Reward Ablation LP (Ours) Time-to-thresold Fan et al. (2018) reward Four Rooms 26.54 \u00b1 0.917 20.25* \u00b1 5.18 23.6 \u00b1 4.77 Fetch Reach 34.55 \u00b1 3.16 16.52* \u00b1 9.07 14.18* \u00b1 6.27 Table 1: Top: Ablation of teacher state representations with \ufb01xed LP reward function, Bottom: Ablation of teacher reward functions with \ufb01xed PE state. Reporting mean area under the student\u2019s learning curve +/- 1 CI. * Indicates signi\ufb01cance difference (p <.05) between the baseline and our method (PE state + LP reward). Further, to highlight the importance of our state representation and reward function on the teacher\u2019s learned policy, we ablate over various state representations and reward functions used in the literature. We report the area under the student\u2019s learning curve (AUC) when trained using the teacher\u2019s learned curriculum (See Table 1, learning curves in Appendix I). We \ufb01rst compare our parametric-behavior embedder against the student parameters [21] and a heuristic state representation [3]. In both Four Rooms and Fetch Reach, the PE state representation is essential for the teacher to learn an effective curriculum, resulting in a signi\ufb01cantly higher AUC for the student. We next compare our LP reward against time-to-threshold [21] and the L2T reward [3]. Again, we found that in both environments, the student achieves a signi\ufb01cantly higher AUC value when trained with a teacher utilizing the LP reward. This con\ufb01rms that the PE representation and LP reward are critical for our Reinforcement Teaching method. To that end, we have successfully demonstrated that Reinforcement Teaching can be used to learn effective curricula that improve student learning. 4.2 Supervised Learning For our supervised learning experiments, the student learns a classi\ufb01er using a base optimizer, and the teacher learns a policy that adapts the step-size of that optimizer. For all experiments, the teacher uses Double DQN [61] and the performance measure is the validation accuracy. Results are averaged over either 10 or 30 random seeds, and the shaded regions are 95% CIs. See Appendix K.1 for more details on the experiment protocol, the teacher\u2019s con\ufb01guration, the student\u2019s classi\ufb01cation tasks. Performance gains in this problem are dif\ufb01cult to achieve because of the effectiveness of constant step-sizes in optimizing neural networks, especially with \u201cnatively adaptive\u201d optimizers like Adam [17]. To demonstrate this dif\ufb01culty, we \ufb01rst conduct a study on a synthetic classi\ufb01cation task to show how common approaches for state representation, such as the parameters or heuristics, fail to improve over Adam with the best constant step-size. We further ablate the components of Reinforcement Teaching, showing that both PE and LP are critical for success in this challenging problem. Lastly, we train a single teacher to control Adam\u2019s step-size on a variety of optimization problems. The teaching policy is able to generalize and successfully transfer to new datasets and even new architectures. Ablating State Representations: Using SGD as the base optimizer, we \ufb01rst compare the parametricbehavior embedder against three state representation baselines that have demonstrated success in other circumstances: (1) student parameters, (2) Policy Evaluation Networks (PVNs), and (3) a heuristic which contains the time-step and problem-speci\ufb01c heuristics: train and validation accuracy. For the PE state representation, we \ufb01x the mini-state size at 256 and include three variations: PE-0 which observes only outputs, PE-X which observes inputs and outputs, PE-Y which observes targets and outputs (see Appendix E for more details). In addition to learned policies, we also include the performance of the best constant step-size of SGD as a baseline. Referring to Figure 3 (top left), we \ufb01nd that all PE variants surpass the baselines. Both the PVN and the parameter state representation 7 Figure 3: Ablation experiments where the teacher adapts the step-size for an optimizer. Plots are learning curves for the teacher. The y-axis is the number of gradient steps needed for the student to reach the goal with the help of the teacher\u2019s current policy, as the teacher learns over episodes on the x-axis. For all plots, lower is better. PE and LP signi\ufb01cantly outperform baselines. Top: student\u2019s base optimizer is SGD, problem is a synthetic classi\ufb01cation task. Bottom: student\u2019s base optimizer is Adam, problem is a harder synthetic classi\ufb01cation task. are not better than a simple heuristic. The parameter state representation is the Markov state for this problem, but, generalization in parameter-space is dif\ufb01cult even for this student\u2019s relatively small neural network (19k parameters). The policy learned with PVN also suffers because the set of probing states learned by PVN is \ufb01xed after learning and cannot adapt to the student\u2019s changing parameters. This is evidenced by the student\u2019s accuracy plateauing at 40% (see Appendix N.1). PVN is unable to improve even after increasing the number of states from the suggested 20 to 128 [31]. While the heuristic state representation is widely used, simple and performant relative to other baselines, its improvement over the best constant step-size is minor, whereas our PE state representation is able to make signi\ufb01cant improvements. Ablating Mini-state size: Using the same synthetic classi\ufb01cation problem as before, we now ablate PE\u2019s mini-state size. In Figure 3 (top right), we \ufb01nd that the teacher improves with larger mini-state sizes. However, even a mini-state size of 64 is able to improve over the simple heuristic and parameter state-representation (PE results in Figure 3 (top left) use a mini-state size of 256). Ablating State Representation With Adam as Base Optimizer: We now conduct an experiment with Adam as the base optimizer and with a more dif\ufb01cult synthetic classi\ufb01cation task. Adam maintains parameter momentum, so the reinforcement teaching MDP is no longer Markov in the parameters. To account for this, the mini-state can be augmented to include, in addition to the inputs and outputs, the change in outputs (denoted by -grad in legend, see Appendix D for details). Referring to Figure 3 (bottom-left), we \ufb01nd that PE is the only state representation to considerably improve over Adam with the best constant step-size. Surprisingly, PE-0 is the best performing state representation despite not being a Markov state representation for this problem. The policy learned by Reinforcement Teaching with PE also successfully transfers to new architectures (see Appendix N.2). For the remainder of the experiments, we will use Adam and the PE-0 state representation. Ablating Reward Function: Using Adam, PE-0 and the hard synthetic classi\ufb01cation problem, we now ablate the reward function of Reinforcement Teaching. The earlier experiments were designed to be insensitive to the reward scheme in such a way that a random policy would reach the performance threshold. We note that the policy found in the Adam experiments can reach the performance threshold in under 200 steps. To make the problem even more dif\ufb01cult, we ablate reward shaping with a max steps of only 200. Referring to Figure 3 (bottom right), we \ufb01nd that learning progress is critical in quickly \ufb01nding a well-performing policy. 8 Figure 4: Reinforcement Teaching in the Neural Network Training Gym. Student learning curves use either the best constant step-size or a step-size adaptation policy that was transferred after being learned in the training gym. Left: Teacher learning curves, lower is better. Middle: Student learning curves with CNN on MNIST. Right: Student learning curves with CNN on Fashion MNIST. Transferring the Policy: To learn general optimizers, which are effective across benchmark datasets, the teacher must train students on a large range of tasks. We now conduct experiments where the teacher learns in the \u201cNeural Network Training Gym\u201d environment, where a labelling network is randomly initialized at every episode to provide labels for random features. The teacher then learns to adapt the step-size for a student learning on the generated dataset of features and labels, without knowing the labelling network. While synthetic, this problem covers a large range of optimization problems. After training the teacher\u2019s policy in the NN Training Gym, we apply the policy to control the optimizer for a student learning on benchmark datasets: MNIST [14], Fashion-MNIST [15], and CIFAR-10 [16]. In addition to changing the data and the underlying classi\ufb01cation problem, the batch size and student neural network also change during transfer (see details in Appendix K.1). Referring to Figure 4 (left), we see that Adam with the best constant step-size cannot reach the performance threshold within the maximum number of steps (200). While the heuristic state representation is able to reach the performance threshold for the synthetic data, its teaching policy does not transfer well to benchmark datasets (see Figure 4 middle, right). For our PE state representation, we see in Figure 4 (middle, right) that the teacher is able to transfer the step-size adaptation policy to a new dataset and to a student that is learning with a Convolutional Neural Network (CNN). This is surprising because the NN Training Gym did not provide the teacher with any experience in training students with CNNs. While PE can improve over Adam in MNIST and Fashion-MNIST (feed-forward or CNN), our results show that the best constant step-size has an advantage on CIFAR. This suggests that the NN training gym did not provide experience representative of the challenges posed by CIFAR. More detailed results can be found in Appendix N.6 5 Discussion Our experiments have focused on a narrow slice of Reinforcement Teaching: meta-learning curricula in RL and the global step-size of an adaptive optimizer. However, several meta-learning problems can be formulated using Reinforcement Teaching, such as learning to explore or sample mini-batches. The main limitation of Reinforcement Teaching is the limitation of current RL algorithms. In designing the reward function, we used an episodic formulation because RL algorithms currently struggle in the continuing setting with average reward [62]. Another limitation of the RL approach to meta-learning is that the number of actions cannot be too large, such as directly parameterizing an entire neural network. While we have developed the parametric-behavior embedder to learn indirectly from parameters, an important extension of Reinforcement Teaching would be to learn to represent actions in parameter space. Further, we assumed that the size of the inputs and outputs were the same for all agents observed by the teacher. This is not a limitation and can be avoided by using environment-speci\ufb01c adapters that map inputs and outputs to a shared embedding size. In this paper, we presented Reinforcement Teaching: a general formulation for meta-learning using reinforcement learning. To facilitate learning in the teacher\u2019s MDP, we introduced the parametricbehavior embedder that learns a representation of the student\u2019s local behavior. For credit assignment, we shaped the reward with learning progress. We demonstrated the generality of reinforcement teaching across several meta-learning problems in RL and supervised learning. While an RL approach to meta-learning has certain limitations, Reinforcement Teaching provides a unifying framework for the meta-learning problem formulation. As reinforcement learning algorithms improve, the set of meta-learning problems solvable by Reinforcement Teaching will continue to increase. 9 References [1] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in neural networks: A survey. CoRR, abs/2004.05439, 2020. URL https://arxiv.org/abs/ 2004.05439. [2] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 113\u2013123, 2019. [3] Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. arXiv:1805.03643, 2018. URL http://arxiv.org/abs/1805.03643v1. [4] Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Learning to teach with dynamic loss functions. arXiv:1810.12081, 2018. URL http://arxiv.org/ abs/1810.12081v1. [5] Xuezhou Zhang, Yuzhe Ma, Adish Kumar Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against reinforcement learning. In ICML, 2020. [6] Chen Huang, Shuangfei Zhai, Walter A. Talbott, Miguel \u00c1ngel Bautista, Shi Sun, Carlos Guestrin, and Joshua M. Susskind. Addressing the loss-metric mismatch with adaptive loss alignment. In ICML, 2019. [7] Diogo Almeida, Clemens Winter, Jie Tang, and Wojciech Zaremba. A generalizable approach to learning optimizers. ArXiv, abs/2106.00958, 2021. [8] Francisco M. Garcia and Philip S. Thomas. A meta-mdp approach to exploration for lifelong reinforcement learning. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019. URL https://proceedings.neurips.cc/paper/ 2019/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf. [9] Nataniel Ruiz, Samuel Schulter, and Manmohan Chandraker. Learning to simulate. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=HJgkx2Aqt7. [10] Xiaojin Zhu, Adish Kumar Singla, Sandra Zilles, and Anna N. Rafferty. An overview of machine teaching. ArXiv, abs/1801.05927, 2018. [11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135. PMLR, 2017. [12] Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 2402\u20132413, 2018. URL https://proceedings.neurips. cc/paper/2018/hash/2715518c875999308842e3455eda2fe3-Abstract.html. [13] Khurram Javed and Martha White. Meta-learning representations for continual learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1818\u20131828, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ f4dd765c12f2ef67f98f3558c282a9cd-Abstract.html. [14] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [15] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. 10 [16] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. [17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. [18] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. arXiv:2012.02096, 2020. URL http://arxiv.org/abs/2012.02096v2. [19] Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B. Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv:2006.12122, 2020. URL http://arxiv.org/abs/2006.12122v2. [20] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv:1705.06366, 2017. URL http://arxiv.org/abs/ 1705.06366v5. [21] Sanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for customized curriculum design in reinforcement learning. In Proceedings of the Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-17, pages 2536\u20132542, 2017. doi: 10.24963/ ijcai.2017/353. URL https://doi.org/10.24963/ijcai.2017/353. [22] Sanmit Narvekar and Peter Stone. Learning curriculum policies for reinforcement learning. arXiv:1812.00285, 2018. URL http://arxiv.org/abs/1812.00285v1. [23] \u00d6zg\u00fcr \u00b8Sim\u00b8sek and Andrew G Barto. An intrinsic reward mechanism for ef\ufb01cient exploration. In Proceedings of the 23rd international conference on Machine learning, pages 833\u2013840, 2006. [24] Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2): 265\u2013286, 2007. doi: 10.1109/TEVC.2006.890271. [25] R\u00e9my Portelas, C\u00e9dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. arXiv:1910.07224, 2019. URL http://arxiv.org/abs/1910.07224v1. [26] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. arXiv:1707.00183, 2017. URL http://arxiv.org/abs/1707.00183v2. [27] Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. arXiv:1704.03003, 2017. URL http: //arxiv.org/abs/1704.03003v1. [28] Douglas Blank, Deepak Kumar, Lisa Meeden, and James Marshall. Bringing up robot: Fundamental mechanisms for creating a self-motivated, self-organizing architecture. Cybernetics & Systems, 12 2003. doi: 10.1080/01969720590897107. [29] Oudeyer Pierre-Yves Moulin-Frier Cl\u00e9ment, Nguyen Sao Mai. Self-organization of early vocal development in infants and machines: the role of intrinsic motivation. Frontiers in Psychology, 2014. doi: 10.3389/fpsyg.2013.01006. URL https://www.frontiersin.org/article/ 10.3389/fpsyg.2013.01006. [30] Benjamin Clement, Didier Roy, Pierre-Yves Oudeyer, and Manuel Lopes. Multi-armed bandits for intelligent tutoring systems. Journal of Educational Data Mining, 7(2):20\u201348, Jun. 2015. doi: 10.5281/zenodo.3554667. URL https://jedm.educationaldatamining.org/ index.php/JEDM/article/view/JEDM111. [31] Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks. arXiv:2002.11833, 2020. URL http://arxiv.org/abs/2002.11833v1. 11 [32] Jack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33:18050\u201318062, 2020. [33] Francesco Faccio, Louis Kirsch, and J\u00fcrgen Schmidhuber. Parameter-based value functions. arXiv:2006.09226, 2020. URL http://arxiv.org/abs/2006.09226v4. [34] A Steven Younger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. In IJCNN\u201901. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222), volume 3. IEEE, 2001. [35] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Arti\ufb01cial Neural Networks, pages 87\u201394. Springer, 2001. [36] J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook. Diplomarbeit, Technische Universit\u00e4t M\u00fcnchen, M\u00fcnchen, 1987. [37] Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In AAAI, pages 171\u2013176, 1992. [38] Sachin Ravi and H. Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017. [39] Marcin Andrychowicz, Misha Denil, Sergio G\u00f3mez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/ fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf. [40] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997. [41] Ke Li and Jitendra Malik. Learning to optimize. arXiv:1606.01885, 2016. URL http: //arxiv.org/abs/1606.01885v1. [42] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. [43] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv:1611.02779, 2016. URL http://arxiv.org/abs/1611.02779v2. [44] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763, 2016. URL http://arxiv.org/abs/1611.05763v3. [45] T. Lattimore and C. Szepesv\u00e1ri. Bandit Algorithms. Cambridge University Press, 2020. ISBN 9781108486828. URL https://books.google.ca/books?id=bydXzAEACAAJ. [46] Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive computation and machine learning. MIT Press, 2018. ISBN 0262193981. URL http://www. worldcat.org/oclc/37293240. [47] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. [48] Xavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html. 12 [49] Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic gradient descent as approximate bayesian inference. J. Mach. Learn. Res., 18(1):4873\u20134907, jan 2017. ISSN 1532-4435. [50] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and markov chains. The Annals of Statistics, 48(3):1348\u20131382, 2020. [51] Johanni Brea, Ber\ufb01n Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. arXiv:1907.02911, 2019. URL http://arxiv.org/abs/1907.02911v1. [52] Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss landscapes. arXiv:1906.04724, 2019. URL http://arxiv.org/abs/1906.04724v1. [53] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. arXiv:1703.06114, 2017. URL http://arxiv.org/abs/ 1703.06114v3. [54] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 1999. [55] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2): 181\u2013211, 1999. [56] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018. [57] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018. URL https://arxiv.org/abs/1802.09464. [58] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347v2. [59] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.02971. [60] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. [61] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on arti\ufb01cial intelligence, volume 30, 2016. [62] Yi Wan, Abhishek Naik, and Richard S Sutton. Learning and planning in average-reward markov decision processes. In International Conference on Machine Learning, pages 10653\u201310662. PMLR, 2021. [63] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based of\ufb02ine policy optimization. arXiv:2005.13239, 2020. URL http://arxiv.org/abs/2005.13239v5. [64] Yifan Wu, George Tucker, and O\ufb01r Nachum. Behavior regularized of\ufb02ine reinforcement learning. arXiv:1911.11361, 2019. URL http://arxiv.org/abs/1911.11361v1. [65] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to of\ufb02ine reinforcement learning. arXiv:2106.06860, 2021. URL http://arxiv.org/abs/2106.06860v1. 13 [66] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for of\ufb02ine reinforcement learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html. [67] Martin Riedmiller. Neural \ufb01tted q iteration\u2013\ufb01rst experiences with a data ef\ufb01cient neural reinforcement learning method. In European Conference on Machine Learning, pages 317\u2013328. Springer, 2005. [68] Hado van Hasselt. Double q-learning. In John D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta, editors, Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada, pages 2613\u20132621. Curran Associates, Inc., 2010. URL https://proceedings.neurips. cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html. [69] Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and Satinder Singh. Bootstrapped meta-learning. arXiv:2109.04504, 2021. URL http: //arxiv.org/abs/2109.04504v1. [70] Lucas Willems and Kiran Karra. Pytorch actor-critic deep reinforcement learning algorithms: A2c and ppo, 2020. URL https://github.com/lcswillems/torch-ac/tree/ 85d0b2b970ab402e3ab289a4b1f94572f9368dad. [71] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017. 14 Checklist The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged to include a justi\ufb01cation to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example: \u2022 Did you include the license to the code and datasets? [Yes] See Section ??. \u2022 Did you include the license to the code and datasets? [No] The code and the data are proprietary. \u2022 Did you include the license to the code and datasets? [N/A] Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below. 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately re\ufb02ect the paper\u2019s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 5 (c) Did you discuss any potential negative societal impacts of your work? [No] To the best of our knowledge, no meta-learning algorithms are currently in production and our work provides a framework for building new meta-learning algorithms. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See appendix (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] In addition to the code, which can reproduce the results along with hyperparameters selected, the Appendix contains tables outling the experiment protocol (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identi\ufb01able information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 15 A Code for Experiments The source code to run our experiments can be found in this anonymized dropbox link: https://www.dropbox.com/sh/33lqeqcr9zd2eh3/AABzlkgbXtaWA3y9eVFko_yZa?dl=0 B Teacher\u2019s Action Space Figure 5: The diagram highlights how the choice of action space for the teacher enable the teacher to learn varied policies that can be applied across different domains. C More Details on Reward Functions The reward function discussed in Section 3.3 is a time-to-threshold reward function for some threshold m\u2217. Another common criterion trains the learner for T iterations and records the performance at the end. The learning process in this case is a \ufb01xed horizon, undiscounted, episodic learning problem and the reward is zero everywhere except that rT = m(\u03b8T , D). In this setting, the policy that optimizes the learning progress also optimizes the \ufb01nal performance m(\u03b8T ). Hence, adding learning progress can be seen as balancing the criteria previously discussed and in Section 3.3: reaching a performance threshold and maximizing overall performance. For reward shaping, one issue with a linear potential is that a constant improvement in performance at lower performance levels is treated as equivalent to higher performance levels. Improving the performance of a classi\ufb01er, for example, is much more dif\ufb01cult when the performance is higher. One way to account for this non-linearity in the classi\ufb01cation setting is to introduce a non-linearity into the shaping, \u03a6(\u03b8) = log(1 \u2212 m(\u03b8)). In the non-linear potential function, we may need to add \u03f5 to ensure numerical stability. With this nonlinear learning progress, the agent will receive higher rewards for increasing the performance measure at higher performance levels as opposed to lower ones. In addition to learning progress, we can shape with only the new performance m\u2032. Assuming that the performance measure is bounded, 0 \u2264 m\u2032 \u2264 1, such as for accuracy of a classi\ufb01er, we have that \u22122 \u2265 \u22121 + m\u2032 \u2265 0. Because the reward function is still negative, it still encodes the time-tothreshold objective. This, however, changes the optimal policy. The optimal policy will maximize its discounted sum of the performance measure, which is analogous to the area under the curve. When the performance measure m is not bounded between 0 and 1, as is the case for the sum of rewards when the student is a reinforcement learner, we outline three alternatives. The \ufb01rst is to simply normalize the performance measure if a maximum and minimum is known. The second, when the maximum or minimum is not known, is to clip the shaping term to be between \u22121 and 1. The last possibility, which is used when the scale of the performance measure changes such as in Atari [60], is 16 to treat any increase (resp. any decrease) in the performance measure as equivalent. In this case, we cannot use a potential function and instead shape with a constant, F(s, a, s\u2032) = 2 I(\u03b3m\u2032\u2212m > 0)\u22121. The teacher receives a reward of 1 for increasing the performance measure and a reward of \u22121 for decreasing the reward function. This also respects the structure of the time-to-threshold reward, while still providing limited feedback about the improvement in the agent\u2019s performance measure. D Non-Markov Learning Settings Most components of the learner\u2019s environment will not depend on more than the current parameters. Adaptive optimizers, however, accumulate gradients and hence depend on the history of parameters. In the context of reinforcement learning, this introduces partial observability. To enforce the Markov property in the teaching MDP, we would need to include the state of the optimizer or maintain a history of past states of the teaching MDP. Both appending the state of the optimizer and maintaining a history can be avoided by augmenting the mini-state \u02c6s = {xi, f\u03b8(xi)}M i=1 with additional local information about the change due to a gradient step, g\u03b8(xi) = f\u03b8\u2212\u03b1\u2207\u03b8J(xi) \u2212 f\u03b8(xi) yielding \u02c6sgrad = {xi, f\u03b8(xi), g\u03b8(xi)}M i=1. We will investigate the necessity of this additional state variable in Section 4.2. E Learning From Outputs Alone in Stationary Problems Each of the mini-states is a minibatch of inputs and outputs from the student. This means that training a teacher using stochastic gradient descent involves sampling a minibatch of minibatches. When the inputs are high-dimensional, such as the case of images, the mini-state that approximates the state can still be large. The inputs are semantically meaningful and provide context to the teacher for the outputs. Despite contextualizing the output value, the inputs put a large memory burden on training the teacher. We can further approximate the representation of the parameters by looking at the outputs alone. To see this, suppose hpool is sum pooling and that the joint encoder hjoint is a linear weighting of the concatenated input and output. Then the Parameter Embedder simpli\ufb01es \ufffd i W \ufffd xi, f\u03b8(xi) \ufffd = W \ufffd \ufffd i xi, \ufffd i f\u03b8(xi) \ufffd . For a large enough sample size, and under a stationary distribution x \u223c p(x), \ufffd i xi \u2248 ME[xi] is a constant. Hence, if the minibatch batch size is large enough and the distribution on inputs is stationary, such as in supervised learning, we can approximate the state \u03b8 by the outputs of f\u03b8 alone. While this intuition is for sum pooling and a linear joint encoding, we will verify empirically that this simpli\ufb01cation assumption is valid for both a non-linear encoder and non-linear pooling operation in Section 4.2. F Ef\ufb01ciently Learning to Reinforcement Teach One criterion for a good Reinforcement Teaching algorithm is low sample complexity. Interacting with the teacher\u2019s MDP and evaluating a teacher can be expensive, due to the student, its algorithm or its environment. A teacher\u2019s episode corresponds to an entire training trajectory for the student. Hence, generating numerous teacher episodes involves training numerous students. The teacher agent cannot afford an inordinate amount of interaction with the student. One way to meet the sample complexity needs of the teacher is to use off-policy learning, such as Q-learning. Of\ufb02ine learning can also circumvent the costly interaction protocol, but may not provide enough feedback on the teacher\u2019s learned policy. There is a large and growing literature on of\ufb02ine and off-policy RL algorithms [63\u201366]. However, we found that DQN [67? ] and DoubleDQN [61; 68] were suf\ufb01cient to learn adaptive teaching behaviour and leave investigation of more advanced deep RL algorithms for future work. G Connecting Reinforcement Teaching to MAML Model-Agnostic Meta Learning (MAML) is a meta-learning method that can be applied to any learning algorithm that uses gradient-descent to improve few-shot performance [11] and similar ideas have been extended to continual learning [13] and meta RL [12]. Summarized brie\ufb02y, these 17 approaches learn an initialization \u03b80 for a neural network by backpropagating through T steps of gradient descent. Its broad applicability, relative simplicity and effectiveness demonstrates its continuing success. Here we outline how MAML can be applied in the Reinforcement Teaching framework. When Alg and m are both differentiable, such as when Alg is an SGD update on a \ufb01xed dataset, meta gradient learning unrolls the computation graph to optimize the meta objective directly, m(f\u03b8T , D) = m(Alg(f\u03b8T \u22121, D), D) = m(Alg(\u00b7 \u00b7 \u00b7 Alg(f\u03b80, D)), D). Others have noted, however, that meta gradient learning can have dif\ufb01cult to optimize loss landscapes especially as the unrolling length of the computation graph increases [69]. Because we are providing a general framework for meta-learning in terms of RL, we are able to bootstrap long horizons, avoiding any gradient-based dif\ufb01culties, and optimize non-differentiable performance measures. The solution concepts within RL places some practical limitations, given current RL algorithms, on what the teaching policy can realistically control. Unlike gradient-based meta-learning, a teaching policy cannot directly set parameters because the action space would be very large. We remark, however, that the Reinforcement Teaching approach described in this work is not mutually exclusive to other meta-learning methods. Using the language of Reinforcement Teaching, we can express MAML\u2019s approach to few-show learning. First, the environment e corresponds to many datasets or tasks, some of which are designated for meta-training or meta-testing. The learning algorithm Alg trains a base learner f\u03b8 on a sample of S meta-training tasks with only K samples from each task. MAML proceeds to unroll the computation graph and optimize on the meta-testing tasks which can be thought of as the performance measure m. Because MAML updates the parameters directly, it is using the fact that the student\u2019s learning process is resettable and that we can differentiate through Alg to learn \u03b80. In this sense, MAML is optimizing the start state of the Teaching MDP, so that the autonomous system, i.e. the MRP E, optimizes the performance m for a large range of tasks in a few number of steps. 18 H Diagram of Parameter Embedding architecture Figure 6: Diagram showing an example of the neural network architecture for the Parameter Embedding state representation and max pooling. This architecture is used for all experiments that use the Parameter Embedder. 19 I Ablation over the teacher\u2019s reward and state representations Figure 7: Four Rooms: Ablation over the teacher\u2019s reward function Figure 8: Fetch Reach: Ablation over the teacher\u2019s reward function 20 Figure 9: Four Rooms: Ablation over the teacher\u2019s state representation Figure 10: Fetch Reach: Ablation over the teacher\u2019s state representation 21 J Learning Ef\ufb01ciency of the Teacher Figure 11: Teacher learning curve for the maze environment. Figure 12: Teacher learning curve for the Four Rooms environment. Figure 13: Teacher learning curve for the Fetch Reach environment. Across all environments, we found that using the LP reward function in combination with the Parameter Embedded state representation signi\ufb01cantly improved the teacher\u2019s own learning ef\ufb01ciency. 22 K Environment and Baseline Speci\ufb01cation Figure 14: Left: Four Rooms Right: Tabular maze. The target task in both environments is to travel from the blue start state to the green goal state. The yellow states indicate possible starting states the teacher can select for the student. Environments for RL experiments The Four Rooms domain is adapted from MiniGrid [56], with a \ufb01xed target and goal state. In our implementation, we used the compact state representation and reward function provided by the developers. The state representation is fully observable and encodes the color and objects of each tile in the grid. The reward function is 1\u22120.9\u2217 stepcount maxsteps for successfully reaching the goal, and 0 otherwise. We reduced the maximum number of time-steps to 40. Moreover, there were three actions, turn left, turn right, and go forward. As for the maze domain, the state representation is simply the x,y coordinates on the grid. The reward function is 0.99stepcount for successfully reaching the goal, and 0 otherwise. The maximum time-step was also 40. Lastly, there were four actions, up, down, left and right. RL experiment baselines For the L2T baseline, we used the reward function exactly as described in the paper. For the state representation, we used an approximation of their state which consisted of the teacher\u2019s action, the student\u2019s target task score, source task score, and the student episode number. For the Narvekar (2017) baseline, we used the time-to-threshold reward function which is a variant of their reward function. For the state, we used the student parameters, as described in their paper. 23 K.1 Supervised Learning We describe the classi\ufb01cation datasets used by the student. Note that the teacher\u2019s action is a relative change in the step size, and so we also append the current step-size for all state representations. Synthetic Clustering: At the beginning of each episode, we initialize a student neural network with 2 hidden layers, 128 neurons and relu activations. The batch size is 64. For each episode, we also sample data xi \u223c N(0, I), i = 1, . . . , 1000 and 0 \u2208 R10 and I is the identity matrix. Each xi is labelled yi \u2208 1, . . . , 10 according to its argmax yi = arg max xi. For each step in the environment, the student neural network takes a gradient step with a step size determined by the teacher. We use a relative action set, where the step size can be increased, kept constant or decreased. This problem was designed so that the default step size of the base optimizer would be able to reach the termination condition within the 200 time steps allotted in the episode. Exploration is not a requirement to solve this problem, as we are primarily evaluating the state representations for Reinforcement Teaching and the quality of the resulting policy. \u2022 SGD Variant: Termination condition based on performance threshold of m\u2217 = 0.95, max steps is 200. \u2022 Adam Variant: Termination condition based on performance threshold of m\u2217 = 0.99, max steps is 400. Synthetic Neural Network Transfer Gym: At the beginning of each episode, we initialize a student neural network with 2 hidden layers, 128 neurons and relu activations. The batch size is 128. For each episode, we also sample data xi \u223c N(0, I), i = 1, . . . , 4000 and 0 \u2208 R784 and I is the identity matrix. The data xi are classi\ufb01ed by a randomly initialized labelling neural network yi = f \u2217(xi). The labelling neural network f \u2217 has the same number of layers as the student\u2019s neural network but has 512 neurons per layer and tanh activations to encourage a roughly uniform distribution over the 10 class labels. MNIST: The student\u2019s neural network is a feed-forward neural network with 128 neurons and 2 hidden layers. The CNN Variant uses a LeNet5 CNN. Batch size is 64. Subsampled dataset to 10000 so that an episode covers one epoch of training. Fashion-MNIST: The student\u2019s neural network is a feed-forward neural network with 128 neurons and 2 hidden layers. The CNN Variant uses a LeNet5 CNN. Batch size is 256. Subsampled dataset to 10000 so that an episode covers one epoch of training. CIFAR-10: The student\u2019s neural network is a LeNet5 CNN. Batch size of 128. Subsampled dataset to 10000 so that an episode covers one epoch of training. 24 L Hyperparameters for Experiments L.1 Reinforcement Learning Experiments Teacher Hyperparameters In the tabular maze experiments, for the DQN teacher, we performed a grid search over batch size \u2208 {64, 128, 256}, learning rate \u2208 {.001, .005}, and minibatch \u2208 {75, 100}. The best hyperparameters for each of the state representations X reward function teacher policies are reported below: 1. PE-QValues x LP: Batch size of 128, learning rate of .001, minibatch size of 75 2. PE-Actions x LP: Batch size of 64, learning rate of .001, minibatch of 100 3. Narvekar(2017) (Parameters x Time-to-threshold): Batch size of 128, learning rate of .001 4. Fan(2018): Batch size of 64, learning rate of .005 In the Four Rooms experiments, for the DQN teacher, we performed grid search over batch size \u2208 {128, 256}, and minibatch \u2208 {75, 100}. We use a constant learning rate of .001. The best hyperparameters for each of the state representations X reward function teacher policies are reported below: 1. PE-QValues x LP: Batch size of 128, learning rate of .001, minibatch size of 100 2. PE-QValues x Time-to-threshold: Batch size of 256, learning rate of .001, minibatch size of 100 3. PE-QValues x L2T: Batch size of 256, learning rate of .001, minibatch size of 100 4. PE-Actions x LP: Batch size of 256, learning rate of .001, minibatch of 100 5. Narvekar(2017) (Parameters x Time-to-threshold): Batch size of 256, learning rate of .001 6. Parameters x LP: Batch size of 128, learning rate of .001 7. Fan(2018): Batch size of 256, learning rate of .001 8. L2T state x LP: Batch size of 128, learning rate of .001 In the Fetch Reach experiments, for the DQN teacher, we performed a grid search over batch size \u2208 {128, 256}. We use a constant learning rate of .001 and mini-batch size of 200. The best hyperparameters for each of the state representations X reward function teacher policies are reported below: 1. PE-QValues x LP: Batch size of 128, learning rate of .001, minibatch size of 200 2. PE-QValues x Time-to-threshold: Batch size of 256, learning rate of .001, minibatch size of 200 3. PE-QValues x L2T: Batch size of 256, learning rate of .001, minibatch size of 200 4. PE-Actions x LP: Batch size of 256, learning rate of .001, minibatch size of 200 5. Narvekar(2017) (Parameters x Time-to-threshold): Batch size of 128, learning rate of .001 6. Parameters x LP: Batch size of 128, learning rate of .001 7. Fan(2018): Batch size of 256, learning rate of .001 8. L2T state x LP: Batch size of 256, learning rate of .001 In addition, in all domains we used a vanilla DQN teacher with a decaying epsilon policy, with starting \u03f5 of .5 and a decay rate of .99. Student Hyperparameters For the PPO student, we used the open-source implementation in [70]. For the DDPG student, we used the OpenAI Baselines implementation [71]. We used the existing hyperparameters as in the respective implementations. We did not perform a grid search over the student hyperparameters. 25 . Fetch Reach Student Agent Type DDPG Student Training Iterations 50 # episodes/epochs per student training iteration 1 Max # of environment steps 50 Number of parallel envs (if applicable) 2 Student Performance Threshold .9 # of Teacher Episodes 50 Optimizer ADAM Batch size 256 Learning rate .001 Gamma NA Entropy coef\ufb01cient NA Adam epsilon 1 \u00b7 10-3 Clipping epsilon NA Maximum gradient norm NA GAE NA Value loss coef\ufb01cient NA Polyak-averaging coef\ufb01cient .95 Action L2 norm coef\ufb01cient 1 Cycles per epoch 6 Batches per cycle 5 Test rollouts per epoch 10 Scale of additive Gaussian noise .2 Actor Network 3 layers with 256 units each, ReLU activation Critic Network 3 layers with 256 units each, ReLU activation Table 2: Student Hyperparameters . Maze Four Rooms Student Agent Type Tabular Q Learning PPO Student Training Iterations 100 50 # episodes/epochs per student training iteration 10 25 Max # of environment steps 40 40 Number of parallel envs (if applicable) NA NA Student Performance Threshold .77 .6 # of Teacher Episodes 300 90 Optimizer NA ADAM Batch size NA 256 Learning rate .5 .001 Gamma .99 .99 Entropy coef\ufb01cient NA .01 Adam epsilon NA 1 \u00b7 10-8 Clipping epsilon NA .2 Maximum gradient norm NA .5 GAE NA .95 Value loss coef\ufb01cient NA .5 Polyak-averaging coef\ufb01cient NA NA Action L2 norm coef\ufb01cient NA NA Cycles per epoch NA NA Batches per cycle NA NA Test rollouts per epoch NA NA Scale of additive Gaussian noise NA NA Actor Network NA 3 layers with 64 units each, Tanh activation Critic Network NA 3 layers with 64 units each, Tanh activation Table 3: Student Hyperparameters 26 L.2 Supervised Learning Experiments The teacher in the supervised learning experiment used DoubleDQN with \u03f5-greedy exploration and an \u03f5 value of 0.01. The batch size and hidden neural network size was 256. The action-value network had 1 hidden layer, but the state encoder has 2 hidden layers. There are three actions, one of which keeps the step size the same and the other two increase or decrease the step size by a factor of 2. Optenv Sgd Optenv Adam Optenv Miniabl Init Num Episodes 200 200 200 Optimizer ADAM ADAM ADAM Batch Size 256 256 256 Update Freq 100 100 100 AgentType DoubleDQN DoubleDQN DoubleDQN Num Episodes 200 200 200 Num Env Steps 2 2 2 Hidden Size 256 256 256 Max Num Episodes 200 200 200 Activation Relu Relu Relu Num Grad Steps 1 1 1 Num Layers 1 1 1 Init Policy Random Random Random Gamma 0.99 0.99 0.99 Max Episode Length 200 400 200 Figure 15: Fixed hyperparameter settings for (Left-Right): SGD state ablation experiment, Adam state ablation experiment, Ministate ablation experiment. Optenv Reward Optenv Pooling Optenv Transfer Init Num Episodes 200 200 200 Optimizer ADAM ADAM ADAM Batch Size 256 256 256 Update Freq 100 100 100 AgentType DoubleDQN DoubleDQN DoubleDQN Num Episodes 400 400 400 Num Env Steps 2 2 2 Hidden Size 256 256 256 Max Num Episodes 200 200 200 Activation Relu Relu Relu Num Grad Steps 1 1 1 Num Layers 1 1 1 Init Policy Random Random Random Gamma 0.99 0.99 0.99 Max Episode Length 200 200 200 Figure 16: Fixed hyperparameter settings for (Left-Right): Reward shaping ablation experiment, Pooling Function ablation experiment, Transferring to real data experiment. 27 Optenv Sgd Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0\", \"PD-x\", \"PD-y\", \"heuristic\", \"parameters\", \"PVN_10\", \"PVN_128\"] Num. Seeds 30 EnvType [\"OptEnv-NoLP-syntheticCluster-SGD\"] Figure 17: Other speci\ufb01cation and hyperparameters that are swept over in the SGD state ablation experiment. Optenv Adam Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0\", \"PD-0-grad\", \"PD-x-grad\", \"heuristic\"] Num. Seeds 30 EnvType [\"OptEnv-NoLP-syntheticCluster-ADAM\"] Figure 18: Other speci\ufb01cation and hyperparameters that are swept over in the Adam state ablation experiment. Optenv Miniabl Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0_4\", \"PD-0_8\", \"PD-0_16\", \"PD-0_32\", \"PD-0_64\", \"PD-0_128\"] Num. Seeds 30 EnvType [\"OptEnv-NoLP-syntheticCluster-SGD\"] Figure 19: Other speci\ufb01cation and hyperparameters that are swept over in the ministate size ablation experiment. Optenv Reward Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0 128\"] Num. Seeds 30 EnvType OptEnv-[\"L2T\",\"LP\", \"NoLP\"]-syntheticCluster-ADAM Figure 20: Other speci\ufb01cation and hyperparameters that are swept over in the reward ablation experiment. Optenv Pooling Pooling Func [\"attention\", \"max\", \"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0\"] Num. Seeds 2 EnvType [\"OptEnv-LP-syntheticCluster-ADAM\"] Figure 21: Other speci\ufb01cation and hyperparameters that are swept over in the pooling ablation experiment. 28 Optenv Transfer Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0_128\", \"heuristic\"] Num. Seeds 30 EnvType [\"OptEnv-LP-syntheticNN-ADAM\"] Figure 22: Other speci\ufb01cation and hyperparameters that are swept over in transferring to benchmark datasets experiment. 29 M Additional Experimental Details M.1 Reinforcement Learning Experiments We now describe the teacher-student interaction protocol. At the the beginning of each teacher episode, we initialize a new RL student agent. The teacher will then propose a starting state or goal for the student, for which the student will then train on. This process continues until the student\u2019s performance measure exceeds the performance threshold or until the student reaches its maximum training iterations. One teacher action proposal and subsequent student training equates to one teacher time step. Therefore, a single teacher episode contains the entire training process of the student. As for the teacher\u2019s action space, in all experiments we used a discrete action space. For the maze environment, the teacher\u2019s action set consisted of 11 possible starting positions (x,y coordinates on the grid). The action space for the Four Rooms environment was similar, except the action size was 10. Lastly, in the Fetch Reach environment, the teacher\u2019s action space consisted of 9 (slightly overlapping) goal distributions. \u201cEasier\" tasks in this environment were tasks for which the goal distribution was smaller and closer to the starting con\ufb01guration of the end effector. Each teacher action/goal distribution is slightly larger than the previous one. Therefore, if the student learns the goals in one goal distribution, it can use its knowledge on other goal distributions. 30 ",
    "Experimental Results": "",
    "Experiments": "Experiments We now describe the teacher-student interaction protocol. At the the beginning of each teacher episode, we initialize a new RL student agent. The teacher will then propose a starting state or goal for the student, for which the student will then train on. This process continues until the student\u2019s performance measure exceeds the performance threshold or until the student reaches its maximum training iterations. One teacher action proposal and subsequent student training equates to one teacher time step. Therefore, a single teacher episode contains the entire training process of the student. As for the teacher\u2019s action space, in all experiments we used a discrete action space. For the maze environment, the teacher\u2019s action set consisted of 11 possible starting positions (x,y coordinates on the grid). The action space for the Four Rooms environment was similar, except the action size was 10. Lastly, in the Fetch Reach environment, the teacher\u2019s action space consisted of 9 (slightly overlapping) goal distributions. \u201cEasier\" tasks in this environment were tasks for which the goal distribution was smaller and closer to the starting con\ufb01guration of the end effector. Each teacher action/goal distribution is slightly larger than the previous one. Therefore, if the student learns the goals in one goal distribution, it can use its knowledge on other goal distributions. 30 ",
    "Results": "",
    "Discussion": "Discussion Our experiments have focused on a narrow slice of Reinforcement Teaching: meta-learning curricula in RL and the global step-size of an adaptive optimizer. However, several meta-learning problems can be formulated using Reinforcement Teaching, such as learning to explore or sample mini-batches. The main limitation of Reinforcement Teaching is the limitation of current RL algorithms. In designing the reward function, we used an episodic formulation because RL algorithms currently struggle in the continuing setting with average reward [62]. Another limitation of the RL approach to meta-learning is that the number of actions cannot be too large, such as directly parameterizing an entire neural network. While we have developed the parametric-behavior embedder to learn indirectly from parameters, an important extension of Reinforcement Teaching would be to learn to represent actions in parameter space. Further, we assumed that the size of the inputs and outputs were the same for all agents observed by the teacher. This is not a limitation and can be avoided by using environment-speci\ufb01c adapters that map inputs and outputs to a shared embedding size. In this paper, we presented Reinforcement Teaching: a general formulation for meta-learning using reinforcement learning. To facilitate learning in the teacher\u2019s MDP, we introduced the parametricbehavior embedder that learns a representation of the student\u2019s local behavior. For credit assignment, we shaped the reward with learning progress. We demonstrated the generality of reinforcement teaching across several meta-learning problems in RL and supervised learning. While an RL approach to meta-learning has certain limitations, Reinforcement Teaching provides a unifying framework for the meta-learning problem formulation. As reinforcement learning algorithms improve, the set of meta-learning problems solvable by Reinforcement Teaching will continue to increase. 9 ",
    "References": "References [1] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in neural networks: A survey. CoRR, abs/2004.05439, 2020. URL https://arxiv.org/abs/ 2004.05439. [2] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 113\u2013123, 2019. [3] Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. arXiv:1805.03643, 2018. URL http://arxiv.org/abs/1805.03643v1. [4] Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Learning to teach with dynamic loss functions. arXiv:1810.12081, 2018. URL http://arxiv.org/ abs/1810.12081v1. [5] Xuezhou Zhang, Yuzhe Ma, Adish Kumar Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against reinforcement learning. In ICML, 2020. [6] Chen Huang, Shuangfei Zhai, Walter A. Talbott, Miguel \u00c1ngel Bautista, Shi Sun, Carlos Guestrin, and Joshua M. Susskind. Addressing the loss-metric mismatch with adaptive loss alignment. In ICML, 2019. [7] Diogo Almeida, Clemens Winter, Jie Tang, and Wojciech Zaremba. A generalizable approach to learning optimizers. ArXiv, abs/2106.00958, 2021. [8] Francisco M. Garcia and Philip S. Thomas. A meta-mdp approach to exploration for lifelong reinforcement learning. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019. URL https://proceedings.neurips.cc/paper/ 2019/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf. [9] Nataniel Ruiz, Samuel Schulter, and Manmohan Chandraker. Learning to simulate. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=HJgkx2Aqt7. [10] Xiaojin Zhu, Adish Kumar Singla, Sandra Zilles, and Anna N. Rafferty. An overview of machine teaching. ArXiv, abs/1801.05927, 2018. [11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135. PMLR, 2017. [12] Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 2402\u20132413, 2018. URL https://proceedings.neurips. cc/paper/2018/hash/2715518c875999308842e3455eda2fe3-Abstract.html. [13] Khurram Javed and Martha White. Meta-learning representations for continual learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1818\u20131828, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ f4dd765c12f2ef67f98f3558c282a9cd-Abstract.html. [14] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [15] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. 10 [16] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. [17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. [18] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. arXiv:2012.02096, 2020. URL http://arxiv.org/abs/2012.02096v2. [19] Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B. Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv:2006.12122, 2020. URL http://arxiv.org/abs/2006.12122v2. [20] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv:1705.06366, 2017. URL http://arxiv.org/abs/ 1705.06366v5. [21] Sanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for customized curriculum design in reinforcement learning. In Proceedings of the Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-17, pages 2536\u20132542, 2017. doi: 10.24963/ ijcai.2017/353. URL https://doi.org/10.24963/ijcai.2017/353. [22] Sanmit Narvekar and Peter Stone. Learning curriculum policies for reinforcement learning. arXiv:1812.00285, 2018. URL http://arxiv.org/abs/1812.00285v1. [23] \u00d6zg\u00fcr \u00b8Sim\u00b8sek and Andrew G Barto. An intrinsic reward mechanism for ef\ufb01cient exploration. In Proceedings of the 23rd international conference on Machine learning, pages 833\u2013840, 2006. [24] Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2): 265\u2013286, 2007. doi: 10.1109/TEVC.2006.890271. [25] R\u00e9my Portelas, C\u00e9dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. arXiv:1910.07224, 2019. URL http://arxiv.org/abs/1910.07224v1. [26] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. arXiv:1707.00183, 2017. URL http://arxiv.org/abs/1707.00183v2. [27] Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. arXiv:1704.03003, 2017. URL http: //arxiv.org/abs/1704.03003v1. [28] Douglas Blank, Deepak Kumar, Lisa Meeden, and James Marshall. Bringing up robot: Fundamental mechanisms for creating a self-motivated, self-organizing architecture. Cybernetics & Systems, 12 2003. doi: 10.1080/01969720590897107. [29] Oudeyer Pierre-Yves Moulin-Frier Cl\u00e9ment, Nguyen Sao Mai. Self-organization of early vocal development in infants and machines: the role of intrinsic motivation. Frontiers in Psychology, 2014. doi: 10.3389/fpsyg.2013.01006. URL https://www.frontiersin.org/article/ 10.3389/fpsyg.2013.01006. [30] Benjamin Clement, Didier Roy, Pierre-Yves Oudeyer, and Manuel Lopes. Multi-armed bandits for intelligent tutoring systems. Journal of Educational Data Mining, 7(2):20\u201348, Jun. 2015. doi: 10.5281/zenodo.3554667. URL https://jedm.educationaldatamining.org/ index.php/JEDM/article/view/JEDM111. [31] Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks. arXiv:2002.11833, 2020. URL http://arxiv.org/abs/2002.11833v1. 11 [32] Jack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33:18050\u201318062, 2020. [33] Francesco Faccio, Louis Kirsch, and J\u00fcrgen Schmidhuber. Parameter-based value functions. arXiv:2006.09226, 2020. URL http://arxiv.org/abs/2006.09226v4. [34] A Steven Younger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. In IJCNN\u201901. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222), volume 3. IEEE, 2001. [35] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Arti\ufb01cial Neural Networks, pages 87\u201394. Springer, 2001. [36] J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook. Diplomarbeit, Technische Universit\u00e4t M\u00fcnchen, M\u00fcnchen, 1987. [37] Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In AAAI, pages 171\u2013176, 1992. [38] Sachin Ravi and H. Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017. [39] Marcin Andrychowicz, Misha Denil, Sergio G\u00f3mez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/ fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf. [40] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997. [41] Ke Li and Jitendra Malik. Learning to optimize. arXiv:1606.01885, 2016. URL http: //arxiv.org/abs/1606.01885v1. [42] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. [43] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv:1611.02779, 2016. URL http://arxiv.org/abs/1611.02779v2. [44] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763, 2016. URL http://arxiv.org/abs/1611.05763v3. [45] T. Lattimore and C. Szepesv\u00e1ri. Bandit Algorithms. Cambridge University Press, 2020. ISBN 9781108486828. URL https://books.google.ca/books?id=bydXzAEACAAJ. [46] Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive computation and machine learning. MIT Press, 2018. ISBN 0262193981. URL http://www. worldcat.org/oclc/37293240. [47] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. [48] Xavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html. 12 [49] Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic gradient descent as approximate bayesian inference. J. Mach. Learn. Res., 18(1):4873\u20134907, jan 2017. ISSN 1532-4435. [50] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and markov chains. The Annals of Statistics, 48(3):1348\u20131382, 2020. [51] Johanni Brea, Ber\ufb01n Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. arXiv:1907.02911, 2019. URL http://arxiv.org/abs/1907.02911v1. [52] Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss landscapes. arXiv:1906.04724, 2019. URL http://arxiv.org/abs/1906.04724v1. [53] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. arXiv:1703.06114, 2017. URL http://arxiv.org/abs/ 1703.06114v3. [54] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 1999. [55] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2): 181\u2013211, 1999. [56] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018. [57] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018. URL https://arxiv.org/abs/1802.09464. [58] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347v2. [59] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.02971. [60] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. [61] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on arti\ufb01cial intelligence, volume 30, 2016. [62] Yi Wan, Abhishek Naik, and Richard S Sutton. Learning and planning in average-reward markov decision processes. In International Conference on Machine Learning, pages 10653\u201310662. PMLR, 2021. [63] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based of\ufb02ine policy optimization. arXiv:2005.13239, 2020. URL http://arxiv.org/abs/2005.13239v5. [64] Yifan Wu, George Tucker, and O\ufb01r Nachum. Behavior regularized of\ufb02ine reinforcement learning. arXiv:1911.11361, 2019. URL http://arxiv.org/abs/1911.11361v1. [65] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to of\ufb02ine reinforcement learning. arXiv:2106.06860, 2021. URL http://arxiv.org/abs/2106.06860v1. 13 [66] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for of\ufb02ine reinforcement learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html. [67] Martin Riedmiller. Neural \ufb01tted q iteration\u2013\ufb01rst experiences with a data ef\ufb01cient neural reinforcement learning method. In European Conference on Machine Learning, pages 317\u2013328. Springer, 2005. [68] Hado van Hasselt. Double q-learning. In John D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta, editors, Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada, pages 2613\u20132621. Curran Associates, Inc., 2010. URL https://proceedings.neurips. cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html. [69] Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and Satinder Singh. Bootstrapped meta-learning. arXiv:2109.04504, 2021. URL http: //arxiv.org/abs/2109.04504v1. [70] Lucas Willems and Kiran Karra. Pytorch actor-critic deep reinforcement learning algorithms: A2c and ppo, 2020. URL https://github.com/lcswillems/torch-ac/tree/ 85d0b2b970ab402e3ab289a4b1f94572f9368dad. [71] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017. 14 Checklist The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged to include a justi\ufb01cation to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example: \u2022 Did you include the license to the code and datasets? [Yes] See Section ??. \u2022 Did you include the license to the code and datasets? [No] The code and the data are proprietary. \u2022 Did you include the license to the code and datasets? [N/A] Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below. 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately re\ufb02ect the paper\u2019s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 5 (c) Did you discuss any potential negative societal impacts of your work? [No] To the best of our knowledge, no meta-learning algorithms are currently in production and our work provides a framework for building new meta-learning algorithms. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See appendix (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] In addition to the code, which can reproduce the results along with hyperparameters selected, the Appendix contains tables outling the experiment protocol (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identi\ufb01able information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 15 A Code for Experiments The source code to run our experiments can be found in this anonymized dropbox link: https://www.dropbox.com/sh/33lqeqcr9zd2eh3/AABzlkgbXtaWA3y9eVFko_yZa?dl=0 B Teacher\u2019s Action Space Figure 5: The diagram highlights how the choice of action space for the teacher enable the teacher to learn varied policies that can be applied across different domains. C More Details on Reward Functions The reward function discussed in Section 3.3 is a time-to-threshold reward function for some threshold m\u2217. Another common criterion trains the learner for T iterations and records the performance at the end. The learning process in this case is a \ufb01xed horizon, undiscounted, episodic learning problem and the reward is zero everywhere except that rT = m(\u03b8T , D). In this setting, the policy that optimizes the learning progress also optimizes the \ufb01nal performance m(\u03b8T ). Hence, adding learning progress can be seen as balancing the criteria previously discussed and in Section 3.3: reaching a performance threshold and maximizing overall performance. For reward shaping, one issue with a linear potential is that a constant improvement in performance at lower performance levels is treated as equivalent to higher performance levels. Improving the performance of a classi\ufb01er, for example, is much more dif\ufb01cult when the performance is higher. One way to account for this non-linearity in the classi\ufb01cation setting is to introduce a non-linearity into the shaping, \u03a6(\u03b8) = log(1 \u2212 m(\u03b8)). In the non-linear potential function, we may need to add \u03f5 to ensure numerical stability. With this nonlinear learning progress, the agent will receive higher rewards for increasing the performance measure at higher performance levels as opposed to lower ones. In addition to learning progress, we can shape with only the new performance m\u2032. Assuming that the performance measure is bounded, 0 \u2264 m\u2032 \u2264 1, such as for accuracy of a classi\ufb01er, we have that \u22122 \u2265 \u22121 + m\u2032 \u2265 0. Because the reward function is still negative, it still encodes the time-tothreshold objective. This, however, changes the optimal policy. The optimal policy will maximize its discounted sum of the performance measure, which is analogous to the area under the curve. When the performance measure m is not bounded between 0 and 1, as is the case for the sum of rewards when the student is a reinforcement learner, we outline three alternatives. The \ufb01rst is to simply normalize the performance measure if a maximum and minimum is known. The second, when the maximum or minimum is not known, is to clip the shaping term to be between \u22121 and 1. The last possibility, which is used when the scale of the performance measure changes such as in Atari [60], is 16 to treat any increase (resp. any decrease) in the performance measure as equivalent. In this case, we cannot use a potential function and instead shape with a constant, F(s, a, s\u2032) = 2 I(\u03b3m\u2032\u2212m > 0)\u22121. The teacher receives a reward of 1 for increasing the performance measure and a reward of \u22121 for decreasing the reward function. This also respects the structure of the time-to-threshold reward, while still providing limited feedback about the improvement in the agent\u2019s performance measure. D Non-Markov Learning Settings Most components of the learner\u2019s environment will not depend on more than the current parameters. Adaptive optimizers, however, accumulate gradients and hence depend on the history of parameters. In the context of reinforcement learning, this introduces partial observability. To enforce the Markov property in the teaching MDP, we would need to include the state of the optimizer or maintain a history of past states of the teaching MDP. Both appending the state of the optimizer and maintaining a history can be avoided by augmenting the mini-state \u02c6s = {xi, f\u03b8(xi)}M i=1 with additional local information about the change due to a gradient step, g\u03b8(xi) = f\u03b8\u2212\u03b1\u2207\u03b8J(xi) \u2212 f\u03b8(xi) yielding \u02c6sgrad = {xi, f\u03b8(xi), g\u03b8(xi)}M i=1. We will investigate the necessity of this additional state variable in Section 4.2. E Learning From Outputs Alone in Stationary Problems Each of the mini-states is a minibatch of inputs and outputs from the student. This means that training a teacher using stochastic gradient descent involves sampling a minibatch of minibatches. When the inputs are high-dimensional, such as the case of images, the mini-state that approximates the state can still be large. The inputs are semantically meaningful and provide context to the teacher for the outputs. Despite contextualizing the output value, the inputs put a large memory burden on training the teacher. We can further approximate the representation of the parameters by looking at the outputs alone. To see this, suppose hpool is sum pooling and that the joint encoder hjoint is a linear weighting of the concatenated input and output. Then the Parameter Embedder simpli\ufb01es \ufffd i W \ufffd xi, f\u03b8(xi) \ufffd = W \ufffd \ufffd i xi, \ufffd i f\u03b8(xi) \ufffd . For a large enough sample size, and under a stationary distribution x \u223c p(x), \ufffd i xi \u2248 ME[xi] is a constant. Hence, if the minibatch batch size is large enough and the distribution on inputs is stationary, such as in supervised learning, we can approximate the state \u03b8 by the outputs of f\u03b8 alone. While this intuition is for sum pooling and a linear joint encoding, we will verify empirically that this simpli\ufb01cation assumption is valid for both a non-linear encoder and non-linear pooling operation in Section 4.2. F Ef\ufb01ciently Learning to Reinforcement Teach One criterion for a good Reinforcement Teaching algorithm is low sample complexity. Interacting with the teacher\u2019s MDP and evaluating a teacher can be expensive, due to the student, its algorithm or its environment. A teacher\u2019s episode corresponds to an entire training trajectory for the student. Hence, generating numerous teacher episodes involves training numerous students. The teacher agent cannot afford an inordinate amount of interaction with the student. One way to meet the sample complexity needs of the teacher is to use off-policy learning, such as Q-learning. Of\ufb02ine learning can also circumvent the costly interaction protocol, but may not provide enough feedback on the teacher\u2019s learned policy. There is a large and growing literature on of\ufb02ine and off-policy RL algorithms [63\u201366]. However, we found that DQN [67? ] and DoubleDQN [61; 68] were suf\ufb01cient to learn adaptive teaching behaviour and leave investigation of more advanced deep RL algorithms for future work. G Connecting Reinforcement Teaching to MAML Model-Agnostic Meta Learning (MAML) is a meta-learning method that can be applied to any learning algorithm that uses gradient-descent to improve few-shot performance [11] and similar ideas have been extended to continual learning [13] and meta RL [12]. Summarized brie\ufb02y, these 17 approaches learn an initialization \u03b80 for a neural network by backpropagating through T steps of gradient descent. Its broad applicability, relative simplicity and effectiveness demonstrates its continuing success. Here we outline how MAML can be applied in the Reinforcement Teaching framework. When Alg and m are both differentiable, such as when Alg is an SGD update on a \ufb01xed dataset, meta gradient learning unrolls the computation graph to optimize the meta objective directly, m(f\u03b8T , D) = m(Alg(f\u03b8T \u22121, D), D) = m(Alg(\u00b7 \u00b7 \u00b7 Alg(f\u03b80, D)), D). Others have noted, however, that meta gradient learning can have dif\ufb01cult to optimize loss landscapes especially as the unrolling length of the computation graph increases [69]. Because we are providing a general framework for meta-learning in terms of RL, we are able to bootstrap long horizons, avoiding any gradient-based dif\ufb01culties, and optimize non-differentiable performance measures. The solution concepts within RL places some practical limitations, given current RL algorithms, on what the teaching policy can realistically control. Unlike gradient-based meta-learning, a teaching policy cannot directly set parameters because the action space would be very large. We remark, however, that the Reinforcement Teaching approach described in this work is not mutually exclusive to other meta-learning methods. Using the language of Reinforcement Teaching, we can express MAML\u2019s approach to few-show learning. First, the environment e corresponds to many datasets or tasks, some of which are designated for meta-training or meta-testing. The learning algorithm Alg trains a base learner f\u03b8 on a sample of S meta-training tasks with only K samples from each task. MAML proceeds to unroll the computation graph and optimize on the meta-testing tasks which can be thought of as the performance measure m. Because MAML updates the parameters directly, it is using the fact that the student\u2019s learning process is resettable and that we can differentiate through Alg to learn \u03b80. In this sense, MAML is optimizing the start state of the Teaching MDP, so that the autonomous system, i.e. the MRP E, optimizes the performance m for a large range of tasks in a few number of steps. 18 H Diagram of Parameter Embedding architecture Figure 6: Diagram showing an example of the neural network architecture for the Parameter Embedding state representation and max pooling. This architecture is used for all experiments that use the Parameter Embedder. 19 I Ablation over the teacher\u2019s reward and state representations Figure 7: Four Rooms: Ablation over the teacher\u2019s reward function Figure 8: Fetch Reach: Ablation over the teacher\u2019s reward function 20 Figure 9: Four Rooms: Ablation over the teacher\u2019s state representation Figure 10: Fetch Reach: Ablation over the teacher\u2019s state representation 21 J Learning Ef\ufb01ciency of the Teacher Figure 11: Teacher learning curve for the maze environment. Figure 12: Teacher learning curve for the Four Rooms environment. Figure 13: Teacher learning curve for the Fetch Reach environment. Across all environments, we found that using the LP reward function in combination with the Parameter Embedded state representation signi\ufb01cantly improved the teacher\u2019s own learning ef\ufb01ciency. 22 K Environment and Baseline Speci\ufb01cation Figure 14: Left: Four Rooms Right: Tabular maze. The target task in both environments is to travel from the blue start state to the green goal state. The yellow states indicate possible starting states the teacher can select for the student. Environments for RL experiments The Four Rooms domain is adapted from MiniGrid [56], with a \ufb01xed target and goal state. In our implementation, we used the compact state representation and reward function provided by the developers. The state representation is fully observable and encodes the color and objects of each tile in the grid. The reward function is 1\u22120.9\u2217 stepcount maxsteps for successfully reaching the goal, and 0 otherwise. We reduced the maximum number of time-steps to 40. Moreover, there were three actions, turn left, turn right, and go forward. As for the maze domain, the state representation is simply the x,y coordinates on the grid. The reward function is 0.99stepcount for successfully reaching the goal, and 0 otherwise. The maximum time-step was also 40. Lastly, there were four actions, up, down, left and right. RL experiment baselines For the L2T baseline, we used the reward function exactly as described in the paper. For the state representation, we used an approximation of their state which consisted of the teacher\u2019s action, the student\u2019s target task score, source task score, and the student episode number. For the Narvekar (2017) baseline, we used the time-to-threshold reward function which is a variant of their reward function. For the state, we used the student parameters, as described in their paper. 23 K.1 Supervised Learning We describe the classi\ufb01cation datasets used by the student. Note that the teacher\u2019s action is a relative change in the step size, and so we also append the current step-size for all state representations. Synthetic Clustering: At the beginning of each episode, we initialize a student neural network with 2 hidden layers, 128 neurons and relu activations. The batch size is 64. For each episode, we also sample data xi \u223c N(0, I), i = 1, . . . , 1000 and 0 \u2208 R10 and I is the identity matrix. Each xi is labelled yi \u2208 1, . . . , 10 according to its argmax yi = arg max xi. For each step in the environment, the student neural network takes a gradient step with a step size determined by the teacher. We use a relative action set, where the step size can be increased, kept constant or decreased. This problem was designed so that the default step size of the base optimizer would be able to reach the termination condition within the 200 time steps allotted in the episode. Exploration is not a requirement to solve this problem, as we are primarily evaluating the state representations for Reinforcement Teaching and the quality of the resulting policy. \u2022 SGD Variant: Termination condition based on performance threshold of m\u2217 = 0.95, max steps is 200. \u2022 Adam Variant: Termination condition based on performance threshold of m\u2217 = 0.99, max steps is 400. Synthetic Neural Network Transfer Gym: At the beginning of each episode, we initialize a student neural network with 2 hidden layers, 128 neurons and relu activations. The batch size is 128. For each episode, we also sample data xi \u223c N(0, I), i = 1, . . . , 4000 and 0 \u2208 R784 and I is the identity matrix. The data xi are classi\ufb01ed by a randomly initialized labelling neural network yi = f \u2217(xi). The labelling neural network f \u2217 has the same number of layers as the student\u2019s neural network but has 512 neurons per layer and tanh activations to encourage a roughly uniform distribution over the 10 class labels. MNIST: The student\u2019s neural network is a feed-forward neural network with 128 neurons and 2 hidden layers. The CNN Variant uses a LeNet5 CNN. Batch size is 64. Subsampled dataset to 10000 so that an episode covers one epoch of training. Fashion-MNIST: The student\u2019s neural network is a feed-forward neural network with 128 neurons and 2 hidden layers. The CNN Variant uses a LeNet5 CNN. Batch size is 256. Subsampled dataset to 10000 so that an episode covers one epoch of training. CIFAR-10: The student\u2019s neural network is a LeNet5 CNN. Batch size of 128. Subsampled dataset to 10000 so that an episode covers one epoch of training. 24 L Hyperparameters for Experiments L.1 Reinforcement Learning Experiments Teacher Hyperparameters In the tabular maze experiments, for the DQN teacher, we performed a grid search over batch size \u2208 {64, 128, 256}, learning rate \u2208 {.001, .005}, and minibatch \u2208 {75, 100}. The best hyperparameters for each of the state representations X reward function teacher policies are reported below: 1. PE-QValues x LP: Batch size of 128, learning rate of .001, minibatch size of 75 2. PE-Actions x LP: Batch size of 64, learning rate of .001, minibatch of 100 3. Narvekar(2017) (Parameters x Time-to-threshold): Batch size of 128, learning rate of .001 4. Fan(2018): Batch size of 64, learning rate of .005 In the Four Rooms experiments, for the DQN teacher, we performed grid search over batch size \u2208 {128, 256}, and minibatch \u2208 {75, 100}. We use a constant learning rate of .001. The best hyperparameters for each of the state representations X reward function teacher policies are reported below: 1. PE-QValues x LP: Batch size of 128, learning rate of .001, minibatch size of 100 2. PE-QValues x Time-to-threshold: Batch size of 256, learning rate of .001, minibatch size of 100 3. PE-QValues x L2T: Batch size of 256, learning rate of .001, minibatch size of 100 4. PE-Actions x LP: Batch size of 256, learning rate of .001, minibatch of 100 5. Narvekar(2017) (Parameters x Time-to-threshold): Batch size of 256, learning rate of .001 6. Parameters x LP: Batch size of 128, learning rate of .001 7. Fan(2018): Batch size of 256, learning rate of .001 8. L2T state x LP: Batch size of 128, learning rate of .001 In the Fetch Reach experiments, for the DQN teacher, we performed a grid search over batch size \u2208 {128, 256}. We use a constant learning rate of .001 and mini-batch size of 200. The best hyperparameters for each of the state representations X reward function teacher policies are reported below: 1. PE-QValues x LP: Batch size of 128, learning rate of .001, minibatch size of 200 2. PE-QValues x Time-to-threshold: Batch size of 256, learning rate of .001, minibatch size of 200 3. PE-QValues x L2T: Batch size of 256, learning rate of .001, minibatch size of 200 4. PE-Actions x LP: Batch size of 256, learning rate of .001, minibatch size of 200 5. Narvekar(2017) (Parameters x Time-to-threshold): Batch size of 128, learning rate of .001 6. Parameters x LP: Batch size of 128, learning rate of .001 7. Fan(2018): Batch size of 256, learning rate of .001 8. L2T state x LP: Batch size of 256, learning rate of .001 In addition, in all domains we used a vanilla DQN teacher with a decaying epsilon policy, with starting \u03f5 of .5 and a decay rate of .99. Student Hyperparameters For the PPO student, we used the open-source implementation in [70]. For the DDPG student, we used the OpenAI Baselines implementation [71]. We used the existing hyperparameters as in the respective implementations. We did not perform a grid search over the student hyperparameters. 25 . Fetch Reach Student Agent Type DDPG Student Training Iterations 50 # episodes/epochs per student training iteration 1 Max # of environment steps 50 Number of parallel envs (if applicable) 2 Student Performance Threshold .9 # of Teacher Episodes 50 Optimizer ADAM Batch size 256 Learning rate .001 Gamma NA Entropy coef\ufb01cient NA Adam epsilon 1 \u00b7 10-3 Clipping epsilon NA Maximum gradient norm NA GAE NA Value loss coef\ufb01cient NA Polyak-averaging coef\ufb01cient .95 Action L2 norm coef\ufb01cient 1 Cycles per epoch 6 Batches per cycle 5 Test rollouts per epoch 10 Scale of additive Gaussian noise .2 Actor Network 3 layers with 256 units each, ReLU activation Critic Network 3 layers with 256 units each, ReLU activation Table 2: Student Hyperparameters . Maze Four Rooms Student Agent Type Tabular Q Learning PPO Student Training Iterations 100 50 # episodes/epochs per student training iteration 10 25 Max # of environment steps 40 40 Number of parallel envs (if applicable) NA NA Student Performance Threshold .77 .6 # of Teacher Episodes 300 90 Optimizer NA ADAM Batch size NA 256 Learning rate .5 .001 Gamma .99 .99 Entropy coef\ufb01cient NA .01 Adam epsilon NA 1 \u00b7 10-8 Clipping epsilon NA .2 Maximum gradient norm NA .5 GAE NA .95 Value loss coef\ufb01cient NA .5 Polyak-averaging coef\ufb01cient NA NA Action L2 norm coef\ufb01cient NA NA Cycles per epoch NA NA Batches per cycle NA NA Test rollouts per epoch NA NA Scale of additive Gaussian noise NA NA Actor Network NA 3 layers with 64 units each, Tanh activation Critic Network NA 3 layers with 64 units each, Tanh activation Table 3: Student Hyperparameters 26 L.2 Supervised Learning Experiments The teacher in the supervised learning experiment used DoubleDQN with \u03f5-greedy exploration and an \u03f5 value of 0.01. The batch size and hidden neural network size was 256. The action-value network had 1 hidden layer, but the state encoder has 2 hidden layers. There are three actions, one of which keeps the step size the same and the other two increase or decrease the step size by a factor of 2. Optenv Sgd Optenv Adam Optenv Miniabl Init Num Episodes 200 200 200 Optimizer ADAM ADAM ADAM Batch Size 256 256 256 Update Freq 100 100 100 AgentType DoubleDQN DoubleDQN DoubleDQN Num Episodes 200 200 200 Num Env Steps 2 2 2 Hidden Size 256 256 256 Max Num Episodes 200 200 200 Activation Relu Relu Relu Num Grad Steps 1 1 1 Num Layers 1 1 1 Init Policy Random Random Random Gamma 0.99 0.99 0.99 Max Episode Length 200 400 200 Figure 15: Fixed hyperparameter settings for (Left-Right): SGD state ablation experiment, Adam state ablation experiment, Ministate ablation experiment. Optenv Reward Optenv Pooling Optenv Transfer Init Num Episodes 200 200 200 Optimizer ADAM ADAM ADAM Batch Size 256 256 256 Update Freq 100 100 100 AgentType DoubleDQN DoubleDQN DoubleDQN Num Episodes 400 400 400 Num Env Steps 2 2 2 Hidden Size 256 256 256 Max Num Episodes 200 200 200 Activation Relu Relu Relu Num Grad Steps 1 1 1 Num Layers 1 1 1 Init Policy Random Random Random Gamma 0.99 0.99 0.99 Max Episode Length 200 200 200 Figure 16: Fixed hyperparameter settings for (Left-Right): Reward shaping ablation experiment, Pooling Function ablation experiment, Transferring to real data experiment. 27 Optenv Sgd Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0\", \"PD-x\", \"PD-y\", \"heuristic\", \"parameters\", \"PVN_10\", \"PVN_128\"] Num. Seeds 30 EnvType [\"OptEnv-NoLP-syntheticCluster-SGD\"] Figure 17: Other speci\ufb01cation and hyperparameters that are swept over in the SGD state ablation experiment. Optenv Adam Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0\", \"PD-0-grad\", \"PD-x-grad\", \"heuristic\"] Num. Seeds 30 EnvType [\"OptEnv-NoLP-syntheticCluster-ADAM\"] Figure 18: Other speci\ufb01cation and hyperparameters that are swept over in the Adam state ablation experiment. Optenv Miniabl Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0_4\", \"PD-0_8\", \"PD-0_16\", \"PD-0_32\", \"PD-0_64\", \"PD-0_128\"] Num. Seeds 30 EnvType [\"OptEnv-NoLP-syntheticCluster-SGD\"] Figure 19: Other speci\ufb01cation and hyperparameters that are swept over in the ministate size ablation experiment. Optenv Reward Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0 128\"] Num. Seeds 30 EnvType OptEnv-[\"L2T\",\"LP\", \"NoLP\"]-syntheticCluster-ADAM Figure 20: Other speci\ufb01cation and hyperparameters that are swept over in the reward ablation experiment. Optenv Pooling Pooling Func [\"attention\", \"max\", \"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0\"] Num. Seeds 2 EnvType [\"OptEnv-LP-syntheticCluster-ADAM\"] Figure 21: Other speci\ufb01cation and hyperparameters that are swept over in the pooling ablation experiment. 28 Optenv Transfer Pooling Func [\"mean\"] Lr [0.001, 0.0005, 0.0001] State Representation [\"PD-0_128\", \"heuristic\"] Num. Seeds 30 EnvType [\"OptEnv-LP-syntheticNN-ADAM\"] Figure 22: Other speci\ufb01cation and hyperparameters that are swept over in transferring to benchmark datasets experiment. 29 M Additional Experimental Details M.1 Reinforcement Learning Experiments We now describe the teacher-student interaction protocol. At the the beginning of each teacher episode, we initialize a new RL student agent. The teacher will then propose a starting state or goal for the student, for which the student will then train on. This process continues until the student\u2019s performance measure exceeds the performance threshold or until the student reaches its maximum training iterations. One teacher action proposal and subsequent student training equates to one teacher time step. Therefore, a single teacher episode contains the entire training process of the student. As for the teacher\u2019s action space, in all experiments we used a discrete action space. For the maze environment, the teacher\u2019s action set consisted of 11 possible starting positions (x,y coordinates on the grid). The action space for the Four Rooms environment was similar, except the action size was 10. Lastly, in the Fetch Reach environment, the teacher\u2019s action space consisted of 9 (slightly overlapping) goal distributions. \u201cEasier\" tasks in this environment were tasks for which the goal distribution was smaller and closer to the starting con\ufb01guration of the end effector. Each teacher action/goal distribution is slightly larger than the previous one. Therefore, if the student learns the goals in one goal distribution, it can use its knowledge on other goal distributions. 30 N Additional Experimental Results N.1 Training Curves with Base SGD Optimizer After Meta-Training Figure 23: SGD State Ablation experiment. Top Student training curves with a trained teacher. Top: Step sizes selected by the teacher. Right: Same architecture as training. Center: A narrower but deeper architecture. Right: A wider but shallower architecture. N.2 Training Curves with Base Adam Optimizer After Meta-Training Figure 24: Adam State Ablation experiment. Top Student training curves with a trained teacher. Top: Step sizes selected by the teacher. Right: Same architecture as training. Center: A narrower but deeper architecture. Right: A wider but shallower architecture. Unlike using SGD as the base optimizer, the Reinforcement Teaching Adam optimizer generalizes well in both narrow and wide settings. 31 N.3 Training Curves from Ministate Size Ablation Figure 25: Synthetic Classi\ufb01cation, Adam, Ministate size Ablation. Student training trajectories with a trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture as training, narrower architecture but deeper, wide architecture but shallower. 32 N.4 Training Curves from Reward Ablation Figure 26: Synthetic Classi\ufb01cation, Adam, Reward Ablation. Student training trajectories with a trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture as training, narrower architecture but deeper, wide architecture but shallower. 33 N.5 Training Curves from Pooling Ablation Figure 27: Synthetic Classi\ufb01cation, Adam, Pooling Ablation. Student training trajectories with a trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture as training, narrower architecture but deeper, wide architecture but shallower. 34 N.6 Training Curves from Synthetic NN Transfer Gym Figure 28: Transfer Gym Experiment using Adam as the base optimizer. Student training trajectories with a trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture as training, narrower architecture but deeper, wide architecture but shallower. 35 Figure 29: Transfer Gym Experiment using Adam as the base optimizer. Student training trajectories with a trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: Transfer to MNIST, Transfer to MNIST and CNN, transfer to Fashion MNIST, transfer to Fashion MNIST and CNN, transfer to CIFAR and CNN. 36 Figure 30: Transfer Gym Experiment using Adam as the base optimizer. Stepsizes selected by a trained teacher. 37 ",
    "title": "Reinforcement Teaching",
    "paper_info": "Reinforcement Teaching\nAlex Lewandowski\u22171,2, Calarina Muslimani\u22171,2,\nDale Schuurmans1,3, Matthew E. Taylor1, Jun Luo2\n1Department of Computing Science, University of Alberta, Edmonton, AB, Canada\n2Noah\u2019s Ark Lab, Huawei Technologies Canada Co., Ltd.\n3Google Research, Brain Team\n\u2217Equal Contribution, {lewandowski, musliman}@uablerta.ca\nAbstract\nMeta-learning strives to learn about and improve a student\u2019s machine learning\nalgorithm. However, existing meta-learning methods either only work with differ-\nentiable algorithms or are hand-crafted to improve one speci\ufb01c component of an\nalgorithm. We develop a unifying meta-learning framework, called Reinforcement\nTeaching, to improve the learning process of any algorithm. Under Reinforce-\nment Teaching, a teaching policy is learned, through reinforcement, to improve\na student\u2019s learning. To effectively learn such a teaching policy, we introduce a\nparametric-behavior embedder that learns a representation of the student\u2019s learn-\nable parameters from its input/output behavior. Further, we use learning progress to\nshape the teacher\u2019s reward, allowing it to more quickly maximize the student\u2019s per-\nformance. To demonstrate the generality of Reinforcement Teaching, we conduct\nexperiments where a teacher learns to signi\ufb01cantly improve both reinforcement\nand supervised learning algorithms, outperforming hand-crafted heuristics and\npreviously proposed parameter representations. Results show that Reinforcement\nTeaching is capable of not only unifying different meta-learning approaches, but\nalso effectively leveraging existing tools from reinforcement learning research.\n1\nIntroduction\nAs machine learning becomes ubiquitous, there is a growing need for algorithms that generalize better,\nlearn more quickly, and require less data. One way to improve a machine learning algorithm, without\nhand-engineering the underlying algorithm, is meta-learning. Meta-learning is often thought of as\n\u201clearning to learn\u201d where the goal is to learn about and improve another machine learning process [1].\nA variety of sub-domains have emerged that design hand-crafted solutions for learning about and\nimproving a speci\ufb01c component of a machine learning process. The work in these sub-domains focus\non solving one speci\ufb01c problem, whether that be \ufb01nding the best way to augment data [2], sample\nminibatches [3], adapt objectives, [4] or poison rewards [5]. Consequently, the meta-learning methods\nused in these domains are handcrafted to solve the problem and cannot be applied to solve new\nproblems in a different domain. Current literature fails to recognize that a more general framework\ncan be used to simultaneously address multiple problems across these varied sub-domains. Therefore,\nthis work takes an important step toward answering the following question:\nCan we develop a unifying framework for improving machine learning algorithms that can be\napplied across sub-domains and learning problems?\nAs a crucial step towards this unifying framework, we introduce Reinforcement Teaching: an approach\nthat frames meta-learning in terms of learning in a Markov Decision Process (MDP). In Reinforcement\nTeaching, a teacher learns a policy via Reinforcement Learning (RL) to improve the learning process\nof a student. The teacher observes a problem agnostic representation of the student\u2019s behavior and\ntakes actions that adjust components of the student\u2019s learning process that the student is unable to\nchange, such as the objective, optimizer, data, or environment. The teacher\u2019s reward is then based\nPreprint. Under review.\narXiv:2204.11897v2  [cs.LG]  22 May 2022\n",
    "GPTsummary": "- (1): The background of this article is the need for machine learning algorithms that generalize better, learn more quickly, and require less data. \n\n- (2): Previous meta-learning methods were either limited to differentiable algorithms or were hand-crafted for specific components of algorithms. The Reinforcement Teaching approach in this paper is well motivated as a more general framework that can be applied across sub-domains and learning problems. \n\n- (3): The proposed research methodology is a teacher-student framework, where the teacher learns a policy via Reinforcement Learning (RL) to improve the learning process of a student. The teacher observes the student's behavior and adjusts components of the student's learning process that cannot be changed by the student. \n\n- (4): The methods in this paper were tested in experiments where the teacher was able to significantly improve both reinforcement and supervised learning algorithms, outperforming hand-crafted heuristics and previously proposed parameter representations. The results support the goal of developing a more general framework for improving machine learning algorithms.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in the development of a more general framework for improving machine learning algorithms. It addresses the need for algorithms that can generalize better, learn more quickly, and require less data. The proposed teacher-student framework has the potential to improve learning processes in a wide range of sub-domains and learning problems.\n\n- (2): Innovation point: The Reinforcement Teaching approach is a novel framework that can be applied across sub-domains and learning problems. By using a teacher-student framework, this approach can improve the learning process of a student by adjusting components of the learning process that cannot be changed by the student. \n\nPerformance: The methods in this paper were tested in experiments where the teacher was able to significantly improve both reinforcement and supervised learning algorithms, outperforming hand-crafted heuristics and previously proposed parameter representations. This suggests that the proposed framework has the potential to significantly enhance the performance of machine learning algorithms.\n\nWorkload: The workload of implementing the Reinforcement Teaching approach was not discussed in detail in the article, so it is difficult to assess. However, the approach does require both a teacher and a student, which may increase the computational resources required for training. Further research may be needed to determine the practicality of implementing this approach in real-world scenarios.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in the development of a more general framework for improving machine learning algorithms. It addresses the need for algorithms that can generalize better, learn more quickly, and require less data. The proposed teacher-student framework has the potential to improve learning processes in a wide range of sub-domains and learning problems.\n\n- (2): Innovation point: The Reinforcement Teaching approach is a novel framework that can be applied across sub-domains and learning problems. By using a teacher-student framework, this approach can improve the learning process of a student by adjusting components of the learning process that cannot be changed by the student. \n\nPerformance: The methods in this paper were tested in experiments where the teacher was able to significantly improve both reinforcement and supervised learning algorithms, outperforming hand-crafted heuristics and previously proposed parameter representations. This suggests that the proposed framework has the potential to significantly enhance the performance of machine learning algorithms.\n\nWorkload: The workload of implementing the Reinforcement Teaching approach was not discussed in detail in the article, so it is difficult to assess. However, the approach does require both a teacher and a student, which may increase the computational resources required for training. Further research may be needed to determine the practicality of implementing this approach in real-world scenarios.\n\n\n"
}