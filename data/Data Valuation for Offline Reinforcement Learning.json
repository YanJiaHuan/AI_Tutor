{
    "Abstract": "Abstract\u2014 The success of deep reinforcement learning (DRL) hinges on the availability of training data, which is typically obtained via a large number of environment interactions. In many real-world scenarios, costs and risks are associated with gathering these data. The \ufb01eld of of\ufb02ine reinforcement learning addresses these issues through outsourcing the collection of data to a domain expert or a carefully monitored program and subsequently searching for a batch-constrained optimal policy. With the emergence of data markets, an alternative to constructing a dataset in-house is to purchase external data. However, while state-of-the-art of\ufb02ine reinforcement learning approaches have shown a lot of promise, they currently rely on carefully constructed datasets that are well aligned with the intended target domains. This raises questions regarding the transferability and robustness of an of\ufb02ine reinforcement learning agent trained on externally acquired data. In this paper, we empirically evaluate the ability of the current state-of-the-art of\ufb02ine reinforcement learning approaches to coping with the source-target domain mismatch within two MuJoCo environments, \ufb01nding that current state-of-the-art of\ufb02ine reinforcement learning algorithms underperform in the target domain. To address this, we propose data valuation for of\ufb02ine reinforcement learning (DVORL), which allows us to identify relevant and high-quality transitions, improving the performance and transferability of policies learned by of\ufb02ine reinforcement learning algorithms. The results show that our method outperforms of\ufb02ine reinforcement learning baselines on two MuJoCo environments. I. ",
    "Introduction": "INTRODUCTION Of\ufb02ine Reinforcement Learning (RL) \u2013 also known as batch-constrained RL \u2013 is a class of RL methods that requires the agent to learn from a static dataset of pre-collected experiences without further environment interaction [1]. This learning paradigm disentangles exploration from exploitation, lending itself to the tasks in which exploration for collecting data is costly, time-consuming, or risky [2], [3]. By taking advantage of pre-collected datasets, of\ufb02ine RL can mitigate the technical concerns associated with online data collection, and has potential bene\ufb01ts for a number of real environments, such as human-robot collaboration [4]. Of\ufb02ine reinforcement learning outsources the collection of data to a domain expert and subsequently searches for a batch-constrained optimal policy. However, this task is challenging, as of\ufb02ine RL methods suffer from the extrapolation error [3], [5]. This pathology occurs when of\ufb02ine deep RL methods are trained under one distribution but evaluated on a different one. More speci\ufb01cally, value functions implemented by a function approximator have a tendency to predict unrealistic values for unseen state-action pairs for standard off-policy deep RL algorithms (e. g., DQN and DDPG). This L3S Research Center, Leibniz University Hannover, Germany {abolfazli, gpalmer, kudenko}@l3s.de raises the need for approaches that restrict the action space, forcing the agent to learn a behavior that is close to on-policy with respect to a subset of the given source data [3]. For of\ufb02ine RL methods that are designed to mitigate the extrapolation error [3], [5], [6], there remains the challenge that external data (e. g., purchased from data markets) may not be well aligned with the intended target domain. Therefore, the learned policy induces a different visited state-action distribution that results in a degradation in the performance of the of\ufb02ine RL agent. In recent years there have been a number of efforts within the paradigm of supervised learning for overcoming the source-target domain mismatch problem via valuating data, including data Shapley [7] and data valuation using reinforcement learning (DVRL) [8]. Such methods have shown promising results on several application scenarios such as data-value quanti\ufb01cation, corrupted sample discovery, robust learning with noisy labels, and domain adaptation [8]. This raises the question: data valuation improve the transferability and robustness of an of\ufb02ine RL agent trained on externally acquired data? To investigate the above question, we propose a data valuation approach that selects a subset of samples in the source dataset that are relevant to the target task. Our main contributions can be summarized as follows: \u2022 Inspired by DVRL [8], we propose Data Valuation for Of\ufb02ine Reinforcement Learning (DVORL) that for a given of\ufb02ine RL method, a \ufb01xed source dataset, and a small target dataset, identi\ufb01es those samples of the source buffer that are relevant to the target task. \u2022 We contribute a benchmark on two Gym MuJoCo domains (Hopper and Walker2d) for which parameterizations (friction and mass of torso) for the target domain are different from those of the source domain. \u2022 We show that the state-of-the-art of\ufb02ine RL methods fail to generalize to different target domain con\ufb01gurations. \u2022 We show how our data valuation approach can improve the generalizability of the RL agent to the target domain by outperforming the existing state-of-the-art methods on all considered target domain con\ufb01gurations. The rest of this paper is organized as follows. Section II gives the background on (of\ufb02ine) RL. In Section III, we formally de\ufb01ne the source-target domain mismatch problem for of\ufb02ine RL, and provide a motivating example in Section IV. Section V gives an overview of related work. In Section VI, we introduce our DVORL framework. Section VII describes our experiment setup. We discuss our results in Section VIII, and in Section IX, we discuss our main \ufb01ndings. Finally in Section X we conclude with suggestions for future work. arXiv:2205.09550v1  [cs.LG]  19 May 2022 ",
    "Background": "BACKGROUND A. Reinforcement Learning The RL problem is typically modeled by a Markov decision process (MDP), formulated as a tuple (X, U, p, r, \u03b3), with a state space X, an action space U, and transition dynamics p (x\u2032 | x, u). At each discrete time step the agent performs an action u \u2208 U in a state x \u2208 X, and transitions to a new state x\u2032 \u2208 X based on the transition dynamics p (x\u2032 | x, u), and receives a reward r (x, u, x\u2032). The goal of the agent is to maximize the expectation of the sum of discounted rewards, also known as the return Rt = \ufffd\u221e i=t+1 \u03b3ir (xi, ui, xi+1), which weighs future rewards with respect to the discount factor \u03b3 \u2208 [0, 1), determining the effective horizon. The agent makes decisions via a policy \u03c0 : X \u2192 P(U), which maps a given state x to a probability distribution over the action space U. For a given policy \u03c0, the value function is de\ufb01ned as the expected return of an agent, starting from state x, performing action u, and following the policy Q\u03c0(s, a) = E\u03c0 [Rt | x, u]. The state-action value function can be computed through the Bellman equation of the Q function: Q\u03c0(x, u) = Es\u2032\u223cp [r (x, u, x\u2032) + \u03b3Eu\u2032\u223c\u03c0Q\u03c0(x\u2032, u\u2032)] . (1) Given Q\u03c0, the optimal policy \u03c0\u2217 = maxu Q\u2217(x, u), can be obtained by greedy selection over the optimal value function Q\u2217(x, u) = max\u03c0 Q\u03c0(x, u). For environments confronting agents with the curse of dimensionality the value can be estimated with a differentiable function approximator Q\u03b8(x, u), with parameters \u03b8. In this work, we focus on continuous control problems, where, given a parameterized policy \u03c0\u03d1 our objective is to \ufb01nd an optimal policy \u03c0\u2217 \u03d1, with respect to the parameters \u03d1, which maximizes the expected return from the start distribution J(\u03d1) = Exi\u223cp\u03c0,ui\u223c\u03c0[R0] [9]. The policy parameters \u03d1 can be updated by taking the gradient of the expected return \u2207\u03d1J(\u03d1). A popular approach to optimizing the policy is to use actor-critic methods, where the actor (policy) can be updated through the deterministic policy gradient algorithm [10]: \u2207\u03d1J(\u03d1) = Ex\u223cp\u03c0[\u2207uQ\u03c0 \u03b8 (x, u)|u,\u03c0(x)\u2207\u03d1\u03c0\u03d1(x)], in which the value function Q\u03c0 \u03b8 (x, u) is the critic. B. Of\ufb02ine Reinforcement Learning Standard off-policy deep RL algorithms such as deep Qlearning (DQN) [11] and deep deterministic policy gradient (DDPG) [9] are applicable in batch RL as they are based on more fundamental batch RL algorithms [12]. However, they suffer from a phenomenon, known as extrapolation error, which occurs when there is a mismatch between the given \ufb01xed batch of data and true state-action visitation of the current policy [3]. This is problematic as incorrect values of state-action pairs, which are not contained in the batch of data, are propagated through temporal difference updates of most off-policy algorithms [13], resulting in poor performance of the model [14]. Below we give an overview of approaches designed to address the extrapolation error, which will serve as our baselines. BCQ. Batch-Constrained deep Q-learning [3] is an of\ufb02ine RL method for continuous control that restricts the action space, thereby eliminating actions that are unlikely to be selected by the behavioral policy \u03c0b and therefore rarely observed in the batch. Given a dataset of N transitions D = {xt, ut, rt+1, xt+1}N t=0, collected by a behavior policy \u03c0b, BCQ consists of four parameterized networks: a generative model G\u03c9 : X \u2192 U, parameterized with \u03c9, a perturbation model \u03be\u03c6(x, u, \u03a6), parameterized with \u03c6, and two Q-networks Q\u03d11(x, u), Q\u03d12(x, u), parameterized with \u03d11 and \u03d12, respectively. The generative model G\u03c9 selects the most likely action given the state with respect to the data in the batch. Since modeling the distribution of data in the high dimensional continuous control environments is not straightforward, a variational autoencoder (VAE) is used to approximate it. The policy is de\ufb01ned by sampling n actions from G\u03c9(x) and selecting the highest valued action according to a Q-network as it is easier to sample from \u03c0b(u | x) than modeling \u03c0b(u | x) in a continuous action space. The perturbation model \u03be\u03c6(x, u, \u03a6), parameterized with \u03c6, models the distribution of data in the batch and is a residual added to the sampled actions in the range [\u2212\u03a6, \u03a6]. This model is trained with the DDPG [10] and can be thought of as a behavioral cloning model. Since the perturbation model together with the sampling can be considered as a hierarchical policy, BCQ can also be considered an actorcritic method. CQL. To prevent the training policy from overestimating the Q-values, Conservative Q-Learning (CQL) [6] utilizes a penalized empircal RL objective. More precisely, CQL optimizes the value function not only to minimize the temporal difference error based on the interactions seen on the dataset but also minimizes the value of actions that the currently trained policy takes, while at the same time maximizing the value of actions taken by the behavioral policy during data generation. This results in a conservative Q-function. TD3+BC. Twin Delayed Deep Deterministic (TD3) policy gradient with Behavior Cloning (BC) is a model-free algorithm that does not explicitly learn a model of the behavioral policy, while trains a policy to mimic the behavior policy from the data [15]. It directly penalizes Euclidean distance to the actions that were recorded in the dataset. C. Data Valuation The training samples that machine learning (ML) models are trained with are not all equally valuable [16]. A sample can be considered low-quality due to noisy input values, a noisy label, or the source-target domain mismatch problem. Removing low-quality samples has been shown to increase the performance of ML models [7], [8]. The task of quantifying the quality of individual datum to the overall performance is referred to as data valuation. In supervised learning, data valuation is formally de\ufb01ned as follows. Given a source (training) dataset Ds = {(xi, yi)}N i=1 and a target (test) dataset DT = \ufffd\ufffd xT j , yT j \ufffd\ufffdM j=1 where x \u2208 X is a d-dimensional feature vector, and y \u2208 Y is a corresponding label, the goal is to \ufb01nd a subset D\u2217 = ",
    "Related Work": "RELATED WORK The RL literature contains numerous techniques for dealing with the source-target domain mismatch problem. Noteworthy contributions here include: EPOPT [19], which is a combination of policy transfer through source domain ensemble and learning from limited demonstrations for the fast adaptation to the target domain; UP-OSI [20] trains robust agent policies using a large number of synthetic demonstrations from a simulator to deal with environments with unknown dynamics; CAD2RL [21] learns latent representations from observations in the source domain that are generally applicable to the target domain; DARLA [22], a zero-shot transfer learning method that learns disentangled representations which are robust against domain shifts; and SAFER [23], which accelerates policy learning on complex control tasks by considering safety constraints. Meanwhile, the literature on off-policy RL includes principled experience replay memory sampling techniques. Prioritized Experience Replay (PER) [24] (e.g., [25], [26], [27]) attempts to sample transitions that contribute the most toward learning. prioritized replay with weighted importance sampling can improve BCQ. However, the majority of the work to date on of\ufb02ine RL is focused on preventing the training policy from being too disjoint with the behavioral policy [3], [6], [28]. To increase the generalization capacity of of\ufb02ine RL methods, Kostrikov et al. [29] propose insample Q-learning (IQL), which approximates the policy improvement step by considering the state value function as a random variable with some randomness determined by the action, and then taking a state-conditional expectile of this random variable to estimate the value of the best actions in that state. In contrast to the of\ufb02ine RL methods listed above, our work focuses on valuating the suitability of state transition tuples for a given target domain. Here we draw inspiration from the literature on data valuation for supervised learning. Ghorbani et al. in [30] propose the distributional Shapley, which is a framework in which the value of a point is de\ufb01ned in the context of underlying data distribution. The reformulation of the data Shapley value as a distributional quantity reduces the dependence on the speci\ufb01c draw of data as the valuation function does not depend on a \ufb01xed data set. Ghorbani and Zou propose the Neuron Shapley framework [31] to quantify how individual neurons contribute to ",
    "Method": "METHOD DVORL consists of two learnable functions: (1) a data value estimator (DVE) model v\u03c6 and (2) an of\ufb02ine reinforcement learning model. Inspired by DVRL [8], we adapt the REINFORCE algorithm [33] and use it as the DVE. We use a DNN for the DVE. The goal is to \ufb01nd the parameters \u03c6\u2217 of the DNN so that the network returns the optimal probability distribution over the sample space. The DVE model v\u03c6 : X \u00d7 U \u00d7 X \u2032 \u00d7 R \u00d7 E \u2192 [0, 1] is optimized to output data values corresponding to the relevance of training samples to the target task. We formulate the corresponding optimization problem as: max\u03c6 J (\u03c0\u03c6) = E (xS,uS,x \u2032S)\u223cP S (xT ,uT ,x \u2032T )\u223cP T \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) \ufffd , (3) where r\u03c6 = 1 DKL(P(xS,uS,x\u2032S) \u2225 P(xT ,uT ,x\u2032T )) (4) corresponds to the reciprocal of the KL divergence between the batch of source dataset and target dataset. Therefore, the objective of the network is to assign high probabilities to samples whose reward function value r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) is high. Training. As shown in Algorithm 1 (lines 4 to 10), all the samples of source buffer DS are divided into batches and each batch DS \u2032 is given as input to the DVE (with shared parameters across the batch). The KL divergence between the distribution of the state-action transition (x, u, x \u2032) of the given batch and that of the small target buffer DT is calculated and used as the reward signal rsig for training the DVE. Let w = v\u03c6(xS i , uS i , x\u2032 i S, rS i , eS i ) denote the probability that the sample i of the source buffer is used for training the of\ufb02ine reinforcement learning model. Our adapted version of REINFORCE algorithm, has the following object function for the policy \u03c0\u03c6: J (\u03c0\u03c6) = E (xS,uS,x \u2032S)\u223cP S (xT ,uT ,x \u2032T )\u223cP T w\u223c\u03c0\u03c6(DS \u2032,\u00b7) \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) \ufffd = \ufffd P T \ufffd (xS, uS, x \u2032S) \ufffd \ufffd w\u2208[0,1]N \u03c0\u03c6(DS \u2032, w) \u00b7 \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) \ufffd d \ufffd (xS, uS, x \u2032S) \ufffd . In the above equation, \u03c0\u03c6(DS \u2032, w) is the probability that the selection probability vector w occurs. In this way, the policy directly uses the values output by the DVE. This is different from the DVRL [8], which uses a binary selection vector s = (s1, . . . , sBs) where sBs denotes the batch size, si \u2208 {0, 1}, and P (si = 1) = wi. Thus, in our training, the DVE has no control over exploration and just provides weightings for the given samples and is tuned accordingly. It should be noted that we use the whole input information of the source buffer (i.e., (x, u, x\u2032, r, e)) for calculating the data values (i.e., DS \u2032 in policy \u03c0\u03c6(DS \u2032, w)); however, we only use the information of the state-action transition (x, u, x\u2032) for calculating the reward signal, used for updating the DVE, that is consistent with our formulation of transfer learning where pS \u0338= pT as the transition probability function is conditioned on the state-action transition (x, u, x\u2032). We calculate the gradient of the above objective function in the following. \u2207\u03c6J (\u03c0\u03c6) = \ufffd P T \ufffd (xS, uS, x \u2032S) \ufffd \ufffd w\u2208[0,1]N \u2207\u03c6\u03c0\u03c6(DS \u2032, w) \u00b7 \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T ))) \ufffd d \ufffd (xS, uS, x \u2032S) \ufffd = \ufffd P T \ufffd (xS, uS, x \u2032S) \ufffd \uf8ee \uf8f0 \ufffd x\u2208[0,1]N \u2207\u03c6\u03c0\u03c6(DS \u2032, w) \u03c0\u03c6(DS \u2032, w) \u00b7 \u03c0\u03c6(DS \u2032, w) \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) \ufffd \uf8f9 \uf8fb d \ufffd (xS, uS, x \u2032S) \ufffd = \ufffd P T \ufffd (xS, uS, x \u2032S) \ufffd \uf8ee \uf8f0 \ufffd w\u2208[0,1]N \u2207\u03c6 log \ufffd \u03c0\u03c6(DS \u2032, w) \ufffd \u00b7 \u03c0\u03c6(DS \u2032, w) \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) \ufffd \uf8f9 \uf8fb d \ufffd (xS, uS, x \u2032S) \ufffd = E (xS,uS,x \u2032S)\u223cP S (xT ,uT ,x \u2032T )\u223cP T w\u223c\u03c0\u03c6(DS,\u00b7) \ufffd r\u03c6((xS, uS, x \u2032S), (xT , uT , x \u2032T )) \ufffd \u00b7 \u2207\u03c6 log \ufffd \u03c0\u03c6(DS \u2032, w) \ufffd . To enhance the stability of the DVE, we use the moving average rrolling of the previous signal rewards with the window size \u03c9 as the baseline. The baseline reduces the variance of the gradient estimates [34]. Inference. As shown in Algorithm 1 (lines 11 to 18), after all the samples of the source buffer are used for training the DVE, the fully-trained DVE is used for outputting the data values of the original source buffer. The samples whose corresponding data values are lower than the selection threshold \u03f5 are \ufb01ltered out and the remaining subset of samples form the new source buffer D\u2217 S that is relevant to the target ",
    "Experiments": "EXPERIMENTS A. Baselines In this work, we use the following of\ufb02ine RL methods discussed in Section II as baselines: (Vanilla) BCQ, CQL and TD3+BC. We also evaluated the performance of BEAR [5] on our considered domains. However, as found by [6], we found CQL and TD3+BC outperformed BEAR, and therefore focused our evaluation on the above three methods and our DVORL. We use the (Vanilla) BCQ as the base model in our DVORL, and we refer to it as Data Valuation based BCQ (DVBCQ). The reason for using the Vanilla BCQ is that it is the most commonly used of\ufb02ine RL algorithm, and we also intend to show how the selection of relevant transitions can help a base model, underperforming other methods in most cases, outperform the state-of-the-art methods in terms of transferability of learned policy to different target con\ufb01gurations. We consider two versions of DVBCQ: i) DVBCQ (x) using information of states, and ii) DVBCQ (x, u, x \u2032) using information of state-action transition, for calculating the similarity between source and target buffers. B. Gathering samples for the replay buffer The DVORL agent learns from a dataset collected by a domain expert. In our experiments, for each domain and domain parametrization, we trained a DDPG agent for one million iterations and used the fully-trained agent for generating the buffers with the size one million and ten thousand for the source and target, respectively. C. Domains In this work, we use the following two MuJoCo domains: \u2022 Hopper-v3: The Hopper is a simulated monopod robot with 4 body links including the torso, upper leg, lower leg, and foot, together with 3 actuated joints. This domain has an 11-dimensional state space including joint angles and joint velocities and a 3-dimensional action space corresponding to torques at the joints. The goal is to make the hopper hop forward as fast as possible. \u2022 Walker2d-v3: The Walker is a simulated bipedal robot consisting of 7 body links including to two legs and a torso, along with 6 actuated joints. This domain has a 17-dimensional state space including joint angles and joint velocities and a 4-dimensional action space corresponding to joint torques. The goal is to make the robot walk forward as fast as possible. TABLE I DOMAIN CONFIGURATIONS. Domain Source Con\ufb01g Target Con\ufb01g Name Hopper-v3 F: 2.0 T: 0.05 F: 2.0 T: 0.05 Hopper-Source F: 2.5 T: 0.075 Hopper-Target1 F: 3.0 T: 0.075 Hopper-Target2 Walker2d-v3 F: 0.9 T: 0.05 F: 0.9 T: 0.05 Walker2d-Source F: 1.125 T: 0.075 Walker2d-Target1 F: 1.35 T: 0.075 Walker2d-Target2 D. Source and target domain settings For our experiments, we shall distinguish between source and target domains. The source domain is the one within which the samples are gathered by a fully-trained DDPG. The target domain is the domain within which the DVORL agent is to be deployed. To study the extent to which DVBCQ can cope with modi\ufb01ed domain con\ufb01gurations, we consider two scenarios with respect to the source and target domains: 1) Identical Source and Target Domains: Domain con\ufb01guration for gathering samples and training DVORL agent remain the same. This is the simplest setting where the DDPG agent gathers samples in an environment with a domain parameterization identical to the domain within which the DVORL agent will be deployed. For this setting, we consider two datasets \u201cHopper-Source\u201d and \u201cWalker-Source\u201d. 2) Transfer Learning: Samples are gathered from a source domain with a parameterization that differs from ",
    "Results": "RESULTS A. Evaluation of of\ufb02ine RL methods on source and target domains Figure 2 shows the performance of BCQ, CQL, TD3+BC, DVBCQ (x) and DVBCQ (x, u, x \u2032) on the source domain and two different target domain con\ufb01gurations described in Section VII-D. The models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds. Identical source-domain: For DVBCQ and other baselines, we report the average return achieved by the best policy with respect to checkpoints saved throughout the run. For the identical source-domain setting, both DVBCQ (x) and DVBCQ (x, u, x \u2032) signi\ufb01cantly outperform all baselines on Hopper environment, and their performance is superior to BCQ and CQL, while underperforming TD3+BC on Walker2d environment. Transfer learning: For transfer learning setting, DVBCQ (x) outperforms both target domains whose con\ufb01gurations (mass of torso and friction) differ from those of the source domain, on both Hopper and Walker2d environments. However, DVBCQ (x, u, x \u2032) underperforms CQL on Hopper environment and TD3+BC on Walker environment but it has competitive performance compared with other baselines. It should be noted that the there is a signi\ufb01cant difference between the performance of DVBCQ and its base model BCQ. B. Removing high/low value transitions from source dataset Figure 3 shows the performance of BCQ models trained on the source dataset with different selection thresholds and evaluated on a different target domain con\ufb01guration (Walker2d-Target2), where all the models are trained for 200K iterations, and a \ufb01xed seed is used for the evaluation environment. We consider \ufb01ve thresholds (0.0, 0.1, ..., 0.4) for excluding the high/low-value data samples of the source dataset. In addition, we report the average return of the best policy (with respect to checkpoints saved throughout the run) learned for each point. As shown in Figure 3, removing low-value samples from the source dataset can help the RL agent learn only those transitions relevant to the target domain con\ufb01guration and therefore achieve better performance on the target domain Fig. 2. Performance of BCQ, CQL, TD3+BC, and DVBCQ on the source domain and two different target domain con\ufb01gurations (described in Section VIID), where the models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds. For DVBCQ and other baselines, we report the performance achieved by the best policy with respect to checkpoints saved throughout the run. (green line). On the other hand, removing high-value samples from the source dataset signi\ufb01cantly deteriorates the RL agent\u2019s performance (red line). The \ufb01ndings in Figure 3 support the opening hypothesis that excluding high-value samples worsens the performance of the of\ufb02ine RL methods. Fig. 3. Performance of BCQ models trained on the source dataset with different selection thresholds and evaluated on a different target domain con\ufb01guration (Walker2d-Target2), where all the models are trained for 200.000 iterations, and a \ufb01xed seed is used for the evaluation environment. Excluding high-value samples (red line) aggravates the performance of the of\ufb02ine RL methods. However, excluding low-value samples (green line) does not deteriorate the performance as much as that of the high-value samples. IX. DISCUSSION & FUTURE WORK Our results suggest that DVORL can improve the of\ufb02ine reinforcement learning methods on both identical sourcetarget and transfer learning settings. In addition, our method helps the of\ufb02ine RL methods achieve signi\ufb01cantly higher performance with fewer iterations, making them more ef\ufb01cient. Furthermore, our method can identify the relevant samples of the source domain to different target domain con\ufb01gurations. This is of high importance and has many use cases, such as learning from an externally acquired dataset and safe RL. It should be noted that our goal is not to show that our proposed method outperforms all the state-of-the-art of\ufb02ine RL methods on both source and target domains, but to show that the data valuation for the of\ufb02ine reinforcement learning (DVORL) framework can improve the performance of the baseline algorithms. For future work, we aim to examine whether the size of target buffer plays a role in the performance of DVORL. We intend conduct some experiments on real-world domains and compare our results to other data valuation methods like Data Shapely. Moreover, we plan to improve our reward function by taking into account dynamics of the model. We also aim to investigate the extent to which DVORL can identify the safe transitions within a safe reinforcement learning setting. We also plan to apply the idea of transition valuation to the safe multi-agent reinforcement learning [37], ",
    "Conclusion": "CONCLUSION In this work, we proposed a data valuation framework that selects a subset of samples in the source dataset that are relevant to the target task. The data values are estimated using a deep neural network, trained using reinforcement learning with a reward that corresponds to the similarity between the distribution of the state-action transition of the given data and the target dataset. We show that DVORL outperforms baselines on different target domain con\ufb01gurations and has a competitive performance on the source domain in which the reinforcement learning agent is trained. We \ufb01nd that our method can identify relevant and high-quality transitions and improve the performance and transferability of policy learned by of\ufb02ine RL algorithms. Moreover, we contributed a benchmark on two Gym MuJoCo domains (Hopper and Walker2d) for which domain con\ufb01gurations (friction and mass of torso) for the target domain differ from those of the source domain. ACKNOWLEDGMENT The authors gratefully acknowledge, that the proposed research is a result of the research project \u201cIIP-Ecosphere\u201d, granted by the German Federal Ministry for Economics and Climate Action (BMWK) via funding code 01MK20006A. ",
    "References": "REFERENCES [1] S. Lange, T. Gabel, and M. Riedmiller, \u201cBatch reinforcement learning,\u201d in Reinforcement learning. Springer, 2012, pp. 45\u201373. [2] D. Isele, A. Nakhaei, and K. Fujimura, \u201cSafe reinforcement learning on autonomous vehicles,\u201d in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 1\u20136. [3] S. Fujimoto, D. Meger, and D. Precup, \u201cOff-policy deep reinforcement learning without exploration,\u201d in Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 09\u201315 Jun 2019, pp. 2052\u20132062. [Online]. Available: http://proceedings.mlr.press/v97/fujimoto19a.html [4] C. Breazeal and A. L. Thomaz, \u201cLearning from human teachers with socially guided exploration,\u201d in 2008 IEEE International Conference on Robotics and Automation. IEEE, 2008, pp. 3539\u20133544. [5] A. Kumar, J. Fu, G. Tucker, and S. Levine, \u201cStabilizing offpolicy q-learning via bootstrapping error reduction,\u201d arXiv preprint arXiv:1906.00949, 2019. [6] A. Kumar, A. Zhou, G. Tucker, and S. Levine, \u201cConservative qlearning for of\ufb02ine reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 1179\u20131191, 2020. [7] A. Ghorbani and J. Zou, \u201cData shapley: Equitable valuation of data for machine learning,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 2242\u20132251. [8] J. Yoon, S. Arik, and T. P\ufb01ster, \u201cData valuation using reinforcement learning,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 10 842\u201310 851. [9] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971, 2015. [10] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, \u201cDeterministic policy gradient algorithms,\u201d in International conference on machine learning. PMLR, 2014, pp. 387\u2013395. [11] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [12] S. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau, \u201cBenchmarking batch deep reinforcement learning algorithms,\u201d arXiv preprint arXiv:1910.01708, 2019. [13] R. S. Sutton, \u201cLearning to predict by the methods of temporal differences,\u201d Machine learning, vol. 3, no. 1, pp. 9\u201344, 1988. [14] S. Thrun and A. Schwartz, \u201cIssues in using function approximation for reinforcement learning,\u201d in Proceedings of the Fourth Connectionist Models Summer School. Hillsdale, NJ, 1993, pp. 255\u2013263. [15] S. Fujimoto and S. S. Gu, \u201cA minimalist approach to of\ufb02ine reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021. [16] A. Lapedriza, H. Pirsiavash, Z. Bylinskii, and A. Torralba, \u201cAre all training examples equally valuable?\u201d arXiv preprint arXiv:1311.6510, 2013. [17] S. Durga, R. Iyer, G. Ramakrishnan, and A. De, \u201cTraining data subset selection for regression with controlled generalization error,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 9202\u20139212. [18] Z. Zhu, K. Lin, and J. Zhou, \u201cTransfer learning in deep reinforcement learning: A survey,\u201d arXiv preprint arXiv:2009.07888, 2020. [19] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, \u201cEpopt: Learning robust neural network policies using model ensembles,\u201d arXiv preprint arXiv:1610.01283, 2016. [20] W. Yu, J. Tan, C. K. Liu, and G. Turk, \u201cPreparing for the unknown: Learning a universal policy with online system identi\ufb01cation,\u201d arXiv preprint arXiv:1702.02453, 2017. [21] F. Sadeghi and S. Levine, \u201cCAD2RL: Real single-image \ufb02ight without a single real image,\u201d arXiv preprint arXiv:1611.04201, 2016. [22] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner, \u201cDarla: Improving zeroshot transfer in reinforcement learning,\u201d in International Conference on Machine Learning. PMLR, 2017, pp. 1480\u20131490. [23] D. Slack, Y. Chow, B. Dai, and N. Wichers, \u201cSafer: Data-ef\ufb01cient and safe reinforcement learning via skill acquisition,\u201d arXiv preprint arXiv:2202.04849, 2022. [24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d arXiv preprint arXiv:1511.05952, 2015. [25] Y. Hou, L. Liu, Q. Wei, X. Xu, and C. Chen, \u201cA novel ddpg method with prioritized experience replay,\u201d in 2017 IEEE international conference on systems, man, and cybernetics (SMC). IEEE, 2017, pp. 316\u2013321. [26] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van Hasselt, and D. Silver, \u201cDistributed prioritized experience replay,\u201d in International Conference on Learning Representations, 2018. [27] C. Kang, C. Rong, W. Ren, F. Huo, and P. Liu, \u201cDeep deterministic policy gradient based on double network prioritized experience replay,\u201d IEEE Access, vol. 9, pp. 60 296\u201360 308, 2021. [28] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, \u201cMorel: Model-based of\ufb02ine reinforcement learning,\u201d Advances in neural information processing systems, vol. 33, pp. 21 810\u201321 823, 2020. [29] I. Kostrikov, A. Nair, and S. Levine, \u201cOf\ufb02ine reinforcement learning with implicit q-learning,\u201d in International Conference on Learning Representations, 2022. [30] A. Ghorbani, M. Kim, and J. Zou, \u201cA distributional framework for data valuation,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 3535\u20133544. [31] A. Ghorbani and J. Y. Zou, \u201cNeuron shapley: Discovering the responsible neurons,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 5922\u20135932, 2020. [32] B. Wang, M. Qiu, X. Wang, Y. Li, Y. Gong, X. Zeng, J. Huang, B. Zheng, D. Cai, and J. Zhou, \u201cA minimax game for instance based selective transfer learning,\u201d in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 34\u201343. [33] R. J. Williams, \u201cSimple statistical gradient-following algorithms for connectionist reinforcement learning,\u201d Machine learning, vol. 8, no. 3, pp. 229\u2013256, 1992. [34] E. Greensmith, P. L. Bartlett, and J. Baxter, \u201cVariance reduction techniques for gradient estimates in reinforcement learning.\u201d Journal of Machine Learning Research, vol. 5, no. 9, 2004. [35] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, \u201cOpenai gym,\u201d arXiv preprint arXiv:1606.01540, 2016. [36] E. Todorov, T. Erez, and Y. Tassa, \u201cMujoco: A physics engine for model-based control,\u201d in 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE, 2012, pp. 5026\u20135033. [37] I. ElSayed-Aly, S. Bharadwaj, C. Amato, R. Ehlers, U. Topcu, and L. Feng, \u201cSafe multi-agent reinforcement learning via shielding,\u201d in Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, 2021, pp. 483\u2013491. ",
    "title": "Data Valuation for Of\ufb02ine Reinforcement Learning",
    "paper_info": "Data Valuation for Of\ufb02ine Reinforcement Learning\nAmir Abolfazli, Gregory Palmer and Daniel Kudenko\nAbstract\u2014 The success of deep reinforcement learning (DRL)\nhinges on the availability of training data, which is typically\nobtained via a large number of environment interactions. In\nmany real-world scenarios, costs and risks are associated with\ngathering these data. The \ufb01eld of of\ufb02ine reinforcement learning\naddresses these issues through outsourcing the collection of\ndata to a domain expert or a carefully monitored program\nand subsequently searching for a batch-constrained optimal\npolicy. With the emergence of data markets, an alternative to\nconstructing a dataset in-house is to purchase external data.\nHowever, while state-of-the-art of\ufb02ine reinforcement learning\napproaches have shown a lot of promise, they currently rely\non carefully constructed datasets that are well aligned with\nthe intended target domains. This raises questions regarding\nthe transferability and robustness of an of\ufb02ine reinforcement\nlearning agent trained on externally acquired data. In this\npaper, we empirically evaluate the ability of the current\nstate-of-the-art of\ufb02ine reinforcement learning approaches to\ncoping with the source-target domain mismatch within two\nMuJoCo environments, \ufb01nding that current state-of-the-art\nof\ufb02ine reinforcement learning algorithms underperform in the\ntarget domain. To address this, we propose data valuation for\nof\ufb02ine reinforcement learning (DVORL), which allows us to\nidentify relevant and high-quality transitions, improving the\nperformance and transferability of policies learned by of\ufb02ine\nreinforcement learning algorithms. The results show that our\nmethod outperforms of\ufb02ine reinforcement learning baselines on\ntwo MuJoCo environments.\nI. INTRODUCTION\nOf\ufb02ine Reinforcement Learning (RL) \u2013 also known as\nbatch-constrained RL \u2013 is a class of RL methods that requires\nthe agent to learn from a static dataset of pre-collected\nexperiences without further environment interaction [1]. This\nlearning paradigm disentangles exploration from exploita-\ntion, lending itself to the tasks in which exploration for\ncollecting data is costly, time-consuming, or risky [2], [3].\nBy taking advantage of pre-collected datasets, of\ufb02ine RL can\nmitigate the technical concerns associated with online data\ncollection, and has potential bene\ufb01ts for a number of real\nenvironments, such as human-robot collaboration [4].\nOf\ufb02ine reinforcement learning outsources the collection\nof data to a domain expert and subsequently searches for a\nbatch-constrained optimal policy. However, this task is chal-\nlenging, as of\ufb02ine RL methods suffer from the extrapolation\nerror [3], [5]. This pathology occurs when of\ufb02ine deep RL\nmethods are trained under one distribution but evaluated on a\ndifferent one. More speci\ufb01cally, value functions implemented\nby a function approximator have a tendency to predict\nunrealistic values for unseen state-action pairs for standard\noff-policy deep RL algorithms (e. g., DQN and DDPG). This\nL3S\nResearch\nCenter,\nLeibniz\nUniversity\nHannover,\nGermany\n{abolfazli, gpalmer, kudenko}@l3s.de\nraises the need for approaches that restrict the action space,\nforcing the agent to learn a behavior that is close to on-policy\nwith respect to a subset of the given source data [3].\nFor of\ufb02ine RL methods that are designed to mitigate the\nextrapolation error [3], [5], [6], there remains the challenge\nthat external data (e. g., purchased from data markets) may\nnot be well aligned with the intended target domain. There-\nfore, the learned policy induces a different visited state-action\ndistribution that results in a degradation in the performance\nof the of\ufb02ine RL agent.\nIn recent years there have been a number of efforts within\nthe paradigm of supervised learning for overcoming the\nsource-target domain mismatch problem via valuating data,\nincluding data Shapley [7] and data valuation using rein-\nforcement learning (DVRL) [8]. Such methods have shown\npromising results on several application scenarios such as\ndata-value quanti\ufb01cation, corrupted sample discovery, robust\nlearning with noisy labels, and domain adaptation [8]. This\nraises the question: data valuation improve the transferability\nand robustness of an of\ufb02ine RL agent trained on externally\nacquired data?\nTo investigate the above question, we propose a data\nvaluation approach that selects a subset of samples in the\nsource dataset that are relevant to the target task. Our main\ncontributions can be summarized as follows:\n\u2022 Inspired by DVRL [8], we propose Data Valuation for\nOf\ufb02ine Reinforcement Learning (DVORL) that for a\ngiven of\ufb02ine RL method, a \ufb01xed source dataset, and\na small target dataset, identi\ufb01es those samples of the\nsource buffer that are relevant to the target task.\n\u2022 We contribute a benchmark on two Gym MuJoCo\ndomains (Hopper and Walker2d) for which parameteri-\nzations (friction and mass of torso) for the target domain\nare different from those of the source domain.\n\u2022 We show that the state-of-the-art of\ufb02ine RL methods fail\nto generalize to different target domain con\ufb01gurations.\n\u2022 We show how our data valuation approach can improve\nthe generalizability of the RL agent to the target domain\nby outperforming the existing state-of-the-art methods\non all considered target domain con\ufb01gurations.\nThe rest of this paper is organized as follows. Section II\ngives the background on (of\ufb02ine) RL. In Section III, we for-\nmally de\ufb01ne the source-target domain mismatch problem for\nof\ufb02ine RL, and provide a motivating example in Section IV.\nSection V gives an overview of related work. In Section VI,\nwe introduce our DVORL framework. Section VII describes\nour experiment setup. We discuss our results in Section VIII,\nand in Section IX, we discuss our main \ufb01ndings. Finally in\nSection X we conclude with suggestions for future work.\narXiv:2205.09550v1  [cs.LG]  19 May 2022\n",
    "GPTsummary": "- (1): The background of the article is offline reinforcement learning, a type of reinforcement learning that uses a static dataset of pre-collected experiences to learn from, making it ideal for tasks where exploration is costly or risky. However, current state-of-the-art offline reinforcement learning methods may not perform well when used with externally acquired data that may not be well suited for the target task.\n\n- (2): Previous approaches, including those that restrict the action space of the agent, have been used to mitigate the issue of mismatched external data. However, these methods may not be effective enough. The approach in this article, called Data Valuation for Offline Reinforcement Learning (DVORL), proposes a data valuation approach that selects a subset of relevant samples from the external dataset. \n\n- (3): The proposed methodology, DVORL, allows for the identification of relevant and high-quality transitions in the dataset, improving the transferability and robustness of policies learned by offline reinforcement learning algorithms.\n\n- (4): The methods introduced in this paper were evaluated on two MuJoCo environments, and the results showed that the proposed DVORL approach outperformed several state-of-the-art offline reinforcement learning baselines, improving the transferability and robustness of the learned policies.\n7. Methods: \n\n- (1): The proposed methodology, Data Valuation for Offline Reinforcement Learning (DVORL), aims to address the issue of mismatched external data when using offline reinforcement learning methods. It consists of a data value estimator (DVE) model and an offline reinforcement learning model.\n\n- (2): The DVE is a learnable function that estimates the relevance and quality of transitions in the dataset by assigning them a data value. A deep neural network is used as the DVE and is trained using the REINFORCE algorithm.\n\n- (3): The objective of the DVE is to assign high data values to the most relevant and high-quality transitions in the dataset. The output of the DVE is then used as input to the offline reinforcement learning model to select a subset of samples from the external dataset that are most relevant to the target task.\n\n- (4): The proposed method was evaluated on two MuJoCo environments, and the results showed that it outperformed several state-of-the-art offline reinforcement learning baselines, improving the transferability and robustness of the learned policies.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work lies in proposing a data valuation framework, DVORL, for offline reinforcement learning, which helps identify and select relevant and high-quality samples from external datasets, improving the transferability and robustness of the learned policies.\n\n- (2): Innovation point: The proposed DVORL approach introduces a data valuation estimator model that estimates the relevance and quality of transitions in external datasets for offline reinforcement learning, which is a novel approach to addressing the issue of mismatched external data. \n\nPerformance: The methods introduced in this article were evaluated on two MuJoCo environments, and the results showed that DVORL outperformed several state-of-the-art offline reinforcement learning baselines, improving the transferability and robustness of the learned policies.\n\nWorkload: The proposed methodology is based on a deep neural network and requires training of both the data value estimator model and the offline reinforcement learning model, which may require significant computational resources. However, the workload has not been thoroughly discussed in this article, leaving room for future studies to explore.\n\n\n",
    "GPTmethods": "- (1): The proposed methodology, Data Valuation for Offline Reinforcement Learning (DVORL), aims to address the issue of mismatched external data when using offline reinforcement learning methods. It consists of a data value estimator (DVE) model and an offline reinforcement learning model.\n\n- (2): The DVE is a learnable function that estimates the relevance and quality of transitions in the dataset by assigning them a data value. A deep neural network is used as the DVE and is trained using the REINFORCE algorithm.\n\n- (3): The objective of the DVE is to assign high data values to the most relevant and high-quality transitions in the dataset. The output of the DVE is then used as input to the offline reinforcement learning model to select a subset of samples from the external dataset that are most relevant to the target task.\n\n- (4): The proposed method was evaluated on two MuJoCo environments, and the results showed that it outperformed several state-of-the-art offline reinforcement learning baselines, improving the transferability and robustness of the learned policies.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work lies in proposing a data valuation framework, DVORL, for offline reinforcement learning, which helps identify and select relevant and high-quality samples from external datasets, improving the transferability and robustness of the learned policies.\n\n- (2): Innovation point: The proposed DVORL approach introduces a data valuation estimator model that estimates the relevance and quality of transitions in external datasets for offline reinforcement learning, which is a novel approach to addressing the issue of mismatched external data. \n\nPerformance: The methods introduced in this article were evaluated on two MuJoCo environments, and the results showed that DVORL outperformed several state-of-the-art offline reinforcement learning baselines, improving the transferability and robustness of the learned policies.\n\nWorkload: The proposed methodology is based on a deep neural network and requires training of both the data value estimator model and the offline reinforcement learning model, which may require significant computational resources. However, the workload has not been thoroughly discussed in this article, leaving room for future studies to explore.\n\n\n",
    "GPTconclusion": "- (1): The significance of this work lies in proposing a data valuation framework, DVORL, for offline reinforcement learning, which helps identify and select relevant and high-quality samples from external datasets, improving the transferability and robustness of the learned policies.\n\n- (2): Innovation point: The proposed DVORL approach introduces a data valuation estimator model that estimates the relevance and quality of transitions in external datasets for offline reinforcement learning, which is a novel approach to addressing the issue of mismatched external data. \n\nPerformance: The methods introduced in this article were evaluated on two MuJoCo environments, and the results showed that DVORL outperformed several state-of-the-art offline reinforcement learning baselines, improving the transferability and robustness of the learned policies.\n\nWorkload: The proposed methodology is based on a deep neural network and requires training of both the data value estimator model and the offline reinforcement learning model, which may require significant computational resources. However, the workload has not been thoroughly discussed in this article, leaving room for future studies to explore.\n\n\n"
}