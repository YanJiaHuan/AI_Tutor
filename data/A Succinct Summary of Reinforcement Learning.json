{
    "Abstract": "Abstract This document is a concise summary of many key results in singleagent reinforcement learning (RL). The intended audience are those who already have some familiarity with RL and are looking to review, reference and/or remind themselves of important ideas in the \ufb01eld. Contents 1 Acknowledgements 2 2 Fundamentals 2 2.1 The RL paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.2 Agent and environment . . . . . . . . . . . . . . . . . . . . . . . 2 2.3 Observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.4 Markov processes and Markov reward processes . . . . . . . . . . 3 2.5 Markov decision processes . . . . . . . . . . . . . . . . . . . . . . 3 2.6 Policies, values and models . . . . . . . . . . . . . . . . . . . . . 3 2.7 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . 5 3 Model-free approaches 6 3.1 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Control with action-value functions . . . . . . . . . . . . . . . . . 8 3.3 Value function approximation . . . . . . . . . . . . . . . . . . . . 9 3.4 Policy gradient methods . . . . . . . . . . . . . . . . . . . . . . . 10 3.5 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.6 Compatible function approximation . . . . . . . . . . . . . . . . . 11 3.7 Deterministic policy gradients . . . . . . . . . . . . . . . . . . . . 12 4 Model-based Approaches 12 4.1 Model Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2 Combining model-free and model-based approaches . . . . . . . . 13 5 Latent variables and partial observability 14 5.1 Latent variable models . . . . . . . . . . . . . . . . . . . . . . . . 14 5.2 Partially observable Markov decision processes . . . . . . . . . . 14 6 Deep reinforcement learning 15 6.1 Experience replay . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 6.2 Target networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 \u2217sanjeevanahilan@gmail.com. Much of this work was done at the Gatsby Unit, UCL. 1 1 Acknowledgements I would like to thank Peter Dayan, David Silver, Chris Watkins and ChatGPT for helpful feedback. Much of this work was drawn from David Silver\u2019s UCL course1 and Sutton and Barto\u2019s textbook (Sutton and Barto, 2018) and formed the introductory chapter of my PhD thesis (Ahilan, 2021). 2 Fundamentals 2.1 The RL paradigm The \ufb01eld of reinforcement learning (RL) (Sutton and Barto, 2018) concerns itself with the computational principles underlying goal-directed learning through interaction. Although primarily seen as a \ufb01eld of machine learning, it has a rich history spanning multiple \ufb01elds. In psychology it can be used to model classical (Pavlovian) and operant (instrumental) conditioning. In neuroscience it has been used to model the dopamine system of the brain (Schultz et al., 1997). In economics, it relates to \ufb01elds such as bounded rationality, and in engineering it has extensive overlap with the \ufb01eld of optimal control (Bellman, 1957). In mathematics, investigation has continued under the guise of operations research. The plethora of perspectives ensures that RL continues to be an exciting and extraordinarily interdisciplinary \ufb01eld. 2.2 Agent and environment RL problems typically draw a separation between the agent and the environment. The agent receives observation ot and scalar reward rt from the environment and emits action at, where t indicates the time step. The environment receives action at from the agent and then emits a reward rt+1 and an observation ot+1. The cycle then begins again with the agent emitting its next action. How the environment responds to the agent\u2019s action is determined by the environment state st, which is updated at every time step. The conditional distribution for the next environment state depends only on the present state and action and therefore satis\ufb01es the Markov property: P(st+1|st, at) = P(st+1|s1, . . . , st, a1, . . . , at) (1) The environment state is in general private from the agent, which only receives observations and rewards. The conditional distribution for the next observation given the current observation is not in general Markov, and so it may be bene\ufb01cial for an agent to construct its own notion of state s\u03b1 t , which it uses to determine its next action. This can be de\ufb01ned as s\u03b1 t = f(ht), where ht is the history of the agent\u2019s sequence of observations, actions and rewards: ht = a1, o1, r1, . . . , at, ot, rt (2) 1https://www.davidsilver.uk/teaching/ 2 2.3 Observability A special case exists when the observation received by the agent ot is identical to the environment state st (such that there is no need to distinguish between the two). This is the assumption underlying the formalism of Markov decision processes covered in the next section. An environment is partially observable if the agent cannot observe the full environment state, meaning that the conditional distribution for its next observation given its current observation does not satisfy the Markov property. This assumption underlies the formalism of a partially observable Markov decision process which we describe in Section 5.2. 2.4 Markov processes and Markov reward processes A Markov process (or Markov chain) is a sequence of random states with the Markov property. It is de\ufb01ned in terms of the tuple \u27e8S, P\u27e9 where S is a \ufb01nite set of states and P : S \u00d7 S \u2192 [0, 1] is the state transition probability kernel. A Markov Reward Process (MRP) \u27e8S, P, r, \u03b3\u27e9 extends the Markov process by including a reward function r : S \u00d7 S \u2192 R for each state transition and a discount factor \u03b3. The immediate expected reward in a given state is de\ufb01ned as: r(s) = \ufffd s\u2032 P(s, s\u2032)r(s, s\u2032). The discount factor \u03b3 \u2208 [0, 1] is used to determine the present value of future rewards. Conventionally, a reward received k steps into the future is of worth \u03b3k times what it would be worth if received immediately. As we will shortly see, the cumulative sum of discounted rewards is a quantity RL agents often seek to maximise, and so \u03b3 < 1 ensures that this sum is bounded (assuming r is bounded). 2.5 Markov decision processes Single-agent RL can be formalised in terms of Markov decision processes (MDPs). The idea of an MDP is to capture the key components available to the learning agent; the agent\u2019s sensation of the state of its environment, the actions it takes which can a\ufb00ect the state, and the rewards associated with states and actions. An MDP extends the formalism of an MRP to include a \ufb01nite set of actions on which both P and r depend. Discrete-time, in\ufb01nite-horizon MDPs are described in terms of the 5-tuple \u27e8S, A, P, r, \u03b3\u27e9 where S is the set of states, A is the set of actions, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the state transition probability kernel, r : S\u00d7A\u00d7S \u2192 R is the immediate reward function and \u03b3 \u2208 [0, 1) is the discount factor. The expected immediate reward for a given state and action is de\ufb01ned as r(s, a) = \ufffd s\u2032 P(s, a, s\u2032)r(s, a, s\u2032), which we use for convenience subsequently. 2.6 Policies, values and models Common components of a reinforcement learning agent are a policy, value function and a model. The policy \u03c0 : S \u00d7A \u2192 [0, 1] is the agent\u2019s behaviour function which denotes the probability of taking action a in state s. Agents may also act according to a deterministic policy \u00b5 : S \u2192 A. We will assume that policies are stochastic unless otherwise noted. 3 Given an MDP and a policy \u03c0, the observed state sequence is a Markov process \u27e8S, P\u03c0\u27e9. P\u03c0(s, s\u2032) = \ufffd a\u2208A \u03c0(s, a)P(s, a, s\u2032) (3) Similarly, the state and reward sequence is a MRP \u27e8S, P\u03c0, r\u03c0, \u03b3\u27e9 in which: r\u03c0(s) = \ufffd a\u2208A \u03c0(s, a)r(s, a) (4) Starting from any particular state s at time step t = 0, the value function v\u03c0(s) is a prediction of the expected discounted future reward given that the agent starts in state s and follows policy \u03c0: v\u03c0(s) = E\u03c0 \ufffd \u221e \ufffd t=0 \u03b3trt+1|s0 = s \ufffd (5) where rt+1 = r(st, at, st+1) which is the solution of an associated Bellman expectation equation: v\u03c0(s) = \ufffd a\u2208A \u03c0(s, a) \ufffd r(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032)v\u03c0(s\u2032) \ufffd (6) In matrix form the Bellman expectation equation can be expressed in terms of the induced MRP: v\u03c0 = r\u03c0 + \u03b3P\u03c0v\u03c0 = (I \u2212 \u03b3P\u03c0)\u22121r\u03c0 (7) where v\u03c0 \u2208 R|S| and r\u03c0 \u2208 R|S| are the vector of values and expected immediate rewards respectively for each state under policy \u03c0. We can also de\ufb01ne a Bellman expectation backup operator: T \u03c0(v) = r\u03c0 + \u03b3P\u03c0v (8) which has a \ufb01xed point of v\u03c0. An action-value for a policy \u03c0 can also be de\ufb01ned, which is the expected discounted future reward for executing action a and subsequently following policy \u03c0. q\u03c0(s, a) = r(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032)v\u03c0(s\u2032) = r(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032) \ufffd a\u2032\u2208A \u03c0(s\u2032, a\u2032)q\u03c0(s\u2032, a\u2032) (9) The process of estimating v\u03c0 or q\u03c0 is known as policy evaluation. Policies can be evaluated without directly knowing or estimating a model, using instead the directly sampled experience of the environment, an approach which is known as \u2018model-free\u2019. However a \u2018model-based\u2019 approach is also possible in which a model is used to predict what the environment will do next. A key component of a model is an estimate of P(s, a, s\u2032), the probability of the next state given the current state and action. Another is an estimate of r(s, a), the expected immediate reward. 4 Policy evaluation enables a value function to be learned for a given policy. However, we often wish to learn the best possible policy. The value function for this is known as the optimal value function and corresponds to the maximum value function over all policies: v\u2217(s) = max \u03c0 v\u03c0(s) (10) The de\ufb01nition of the optimal action-value function (which evaluates the immediate action a in state s) is similarly: q\u2217(s, a) = max \u03c0 q\u03c0(s, a) (11) A partial ordering over policies can be de\ufb01ned according to: \u03c0 \u2265 \u03c0\u2032 if v\u03c0(s) \u2265 v\u03c0\u2032(s), \u2200s (12) For any MDP there exists an optimal policy \u03c0\u2217 that is better than or equal to all other policies. All optimal policies achieve the optimal value function and optimal action-value function and there is always a deterministic optimal policy for any MDP. The latter is achieved by selecting: a = arg max a\u2208A q\u2217(s, a) (13) If there are many possible actions which satisfy this, any of these may be chosen to constitute an optimal policy (of which there may be many). The optimal value and state-value functions satisfy Bellman optimality equations: v\u2217(s) = max a\u2208A q\u2217(s, a) v\u2217(s) = max a\u2208A \ufffd r(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032)v\u2217(s\u2032) \ufffd q\u2217(s, a) = r(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032)max a\u2032 q\u2217(s\u2032, a\u2032) (14) The Bellman optimality equation is non-linear with no closed form solution (in general). Solving it therefore requires iterative solution methods. 2.7 Dynamic programming Dynamic programming (DP) (Bertsekas et al., 1995) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as an MDP. In general, DP solves complex problems by breaking them down into subproblems and then combining the solutions. It is particularly useful for overlapping subproblems, the solutions to which reoccur many times when solving the overall problem, making it more computationally e\ufb03cient to cache and reuse them. When applied to MDPs, the recursive decomposition of DP corresponds to the Bellman equation and the cached solution to the value function. DP assumes that the MDP is fully known and therefore does not address the full RL problem but instead addresses the problem of planning. By planning, the prediction problem can be addressed by \ufb01nding the value function v\u03c0 of a given policy 5 \u03c0. This can be evaluated by iterative application of the Bellman Expectation Backup (Equation 8). This leads to convergence to a unique \ufb01xed point v\u03c0, which can be shown using the contraction mapping theorem (also known as the Banach \ufb01xed-point theorem) (Banach, 1922). When a Bellman expectation backup operator T \u03c0 is applied to two value functions u and v over states, we \ufb01nd that it is a \u03b3contraction: ||T \u03c0(u) \u2212 T \u03c0(v)||\u221e = ||(r\u03c0 + \u03b3P\u03c0u) \u2212 (r\u03c0 + \u03b3P\u03c0v)||\u221e = ||\u03b3P\u03c0(u \u2212 v)||\u221e \u2264 ||\u03b3P\u03c01||u \u2212 v||\u221e||\u221e \u2264 \u03b3||u \u2212 v||\u221e (15) where 1 is a vector of ones and the in\ufb01nity norm of a vector a is denoted ||a||\u221e and is de\ufb01ned as the maximum value of its components. This contraction ensures that both u and v converge to the unique \ufb01xed point of T \u03c0 which is v\u03c0. For control, DP can be used to \ufb01nd the optimal value function v\u2217 and in turn the optimal policy \u03c0\u2217. One possibility is policy iteration in which the current policy \u03c0 is \ufb01rst evaluated as described and then subsequently improved to \u03c0\u2032 such that: \u03c0\u2032(s) = arg max a\u2208A q\u03c0(s, a) (16) This improves the value from any state s over one step: q\u03c0(s, \u03c0\u2032(s)) = max a\u2208A q\u03c0(s, a) \u2265 \ufffd a\u2208A \u03c0(s, a)q\u03c0(s, a) = v\u03c0(s) (17) It can be shown that this improves the value function such that that v\u03c0\u2032(s) \u2265 v\u03c0(s) (Silver, 2015). This process is then repeated, with improvements ending when the Bellman optimality equation (14) has been satis\ufb01ed and convergence to \u03c0\u2217 achieved. A generalisation of policy iteration is also possible in which, instead of waiting for policy evaluation to converge, only n steps of evaluation are taken before policy improvement occurs and the process is repeated. If n = 1 this is known as value iteration, as the policy is no longer explicit (being a direct consequence of the value function). Like policy iteration, value iteration is also guaranteed to converge to the optimal value function and policy. This can be demonstrated using the contraction mapping theorem. 3 Model-free approaches 3.1 Prediction As has been outlined, dynamic programming can be used to solve known MDPs enabling optimal value functions and policies to be found. However, in many cases the MDP is not directly known - instead an agent taking actions in the MDP must learn directly from its experiences, as it transitions from state to state and receives rewards accordingly. One approach, known as \u2018model-free\u2019, seeks to solve MDPs without learning transitions or rewards. For prediction, a 6 key quantity to estimate in this setting is the expected discounted future reward. A sampled estimate of this, starting from state st, is known as the return: Rt = rt+1 + \u03b3rt+2 + \u03b32rt+3 + ... = \u221e \ufffd k=0 \u03b3krt+k+1 (18) which depends on the actions sampled from the policy, and states from transitions. Monte-Carlo (MC) methods seek to estimate this directly using complete episodes of experience. Introducing a learning rate \u03b1t, the agent\u2019s value function can therefore be updated according to2: v(st) \u2190 v(st) + \u03b1t \ufffd Rt \u2212 v(st) \ufffd (19) The value function updated in this way will converge to a solution with minimum mean-square error (best \ufb01t to the observed returns), assuming a suitable sequential decrease in the learning rate. Temporal-di\ufb00erence (TD) learning methods learn from incomplete episodes by bootstrapping. For example, if learning occurs after a single step, this is known as TD(0), which has the following update: v(st) \u2190 v(st) + \u03b1t \ufffd rt+1 + \u03b3v(st+1) \u2212 v(st) \ufffd (20) where rt+1 + \u03b3v(st+1) is known as the target. This approximates the full-width Bellman expectation backup (Equation 8) in which every successor state and action is considered, with experiences instead being sampled. TD(0) will converge to the solution of the maximum likelihood Markov model which best \ufb01ts the data (again assuming a suitable sequential decrease in the learning rate). This solution may be di\ufb00erent from the minimum mean-square error solution of MC methods, which do not assume the Markov property. Unlike MC methods, TD methods introduce bias into the estimated return as the currently estimated value function may be di\ufb00erent from the true value function. However, they generally have reduced variance relative to MC methods, as in MC the estimated return depends on a potentially long sequence of random actions, transitions and rewards. The distinction between MC and TD methods can be blurred by considering multi-step TD methods (rather than only TD(0)), in which rewards are sampled for a number of steps before the value function is used to compute an estimate of future rewards. The n-step return is de\ufb01ned as: R(n) t = rt+1 + \u03b3rt+2 + ... + \u03b3n\u22121rt+n + \u03b3nv(st+n) (21) As n \u2192 \u221e it tends towards the unbiased MC return. An algorithm may seek to \ufb01nd a good bias-variance tradeo\ufb00 by estimating a weighted combination of n-step returns; one popular method to do this is known as TD(\u03bb): R\u03bb t = (1 \u2212 \u03bb) \u221e \ufffd n=1 \u03bbn\u22121R(n) t (22) where \u03bb \u2208 [0, 1]. 2assuming a table-based representation rather than use of a function approximator 7 3.2 Control with action-value functions Model free control concerns itself with optimising rather than evaluating the RL objective. Policies may be evaluated according to various objectives. In the case of continuing environments, the objective can be the average value or the average reward per time-step. We focus instead on episodic environments, assuming an initial distribution over starting states p0(s) : S \u2192 [0, 1]. The objective is thus: J(\u03c0) = E\u03c0 \ufffd \u221e \ufffd t=0 \u03b3trt+1|p0(s) \ufffd (23) Note that if the domain of the starting state distribution is only over a single starting state, the objective is simply the value function (Equation 5) in that starting state. This objective can equivalently be expressed as: J(\u03c0) = Es\u223c\u03c1\u03c0,a\u223c\u03c0[r(s, a)] (24) where: \u03c1\u03c0(s) := \ufffd s\u2032 \u221e \ufffd t=0 \u03b3tp(st = s|s\u2032, \u03c0)p0(s\u2032) (25) is the improper discounted state distribution induced by policy \u03c0 starting from an initial state distribution p0(s\u2032). In Section 3.4 we describe policy gradient methods which seek to optimise this objective directly. However, we \ufb01rst consider model-free approaches which rely on an actionvalue function q(s, a) to achieve control (a value function v(s) alone is insu\ufb03cient for model-free control). The optimal action-value function q\u2217(s, a) must be learned, with MC and TD methods both viable. Once it has been learned, an optimal policy may be achieved by selecting the best action in each state (Equation 13). However, unlike dynamic programming, full-width backups are not used and so if actions are selected greedily (meaning those with highest action-values are always chosen) then certain states and actions may never be correctly evaluated. Model-free RL methods must therefore allow for enough exploration during learning before ultimately exploiting this learning to achieve near-optimal cumulative reward. One simple approach, known as \u01eb-greedy is to take a random action with probability \u01eb but otherwise act greedily according to the current estimate of the action-value function. The value of \u01eb can be decreased with the number of episodes. This can satisfy a condition known as greedy in the limit of in\ufb01nite exploration in which all state-action pairs are explored in\ufb01nitely many times and the policy converges to the greedy policy. One popular algorithm for model-free control is known as Q-learning (Watkins and Dayan, 1992), which seeks to learn the optimal action-value function whilst using a policy which also takes exploratory actions (such as epsilon greedy). This learning is termed o\ufb00-policy as the policy used to sample experience is di\ufb00erent from the policy being learned (the optimal policy). The resulting update is: q(st, at) \u2190 q(st, at) + \u03b1 \ufffd rt+1 + \u03b3max a\u2032\u2208A q(st+1, a\u2032) \u2212 q(st, at) \ufffd (26) 8 An alternative to o\ufb00-policy Q-learning is on-policy SARSA (Rummery and Niranjan, 1994). This uses the sampled sampled state st, action at, reward rt+1, next state st+1, and next action at+1 for updates3: q(st, at) \u2190 q(st, at) + \u03b1(rt+1 + \u03b3q(st+1, at+1) \u2212 q(st, at)) (27) 3.3 Value function approximation So far we have assumed a tabular representation of states and actions such that each state is separately updated. However, in practice we would like value functions and policies to generalise to new states and actions, and so it is bene\ufb01cial to use function approximators such as deep neural networks. A common approach is to approximate the value function or action-value function: vw(s) = \u02c6v(s; w) \u2248 v\u03c0(s) qw(s, a) = \u02c6q(s, a; w) \u2248 q\u03c0(s, a) (28) where w are the parameters we wish to learn. If we start by assuming we know the true value function v\u03c0, we can de\ufb01ne a mean square error between the approximate value function and the true function: L(w) = E\u03c0[(v\u03c0(s) \u2212 vw(s))2] (29) Given a distribution of states s \u223c p(s)4, we can minimise this iteratively using stochastic gradient descent: w \u2190 w + \u03b1(v\u03c0(st) \u2212 vw(st))\u2207wvw(st) (30) In reality we can only use a better estimate of v\u03c0 provided by the sampled reward(s). For example, if we use the TD(0) target the update is: w \u2190 w + \u03b1(rt+1 + \u03b3vw(st+1) \u2212 vw(st))\u2207wvw(st) (31) Updates like this are known as \u2018semi-gradient\u2019 as the gradient of the value function used to de\ufb01ne the target is ignored. If we use a linear function approximator vw(s) = x(s)T w (where features x(s) and w are vectors), then we \ufb01nd: w \u2190 w + \u03b1(rt+1 + \u03b3vw(st+1) \u2212 vw(st))x(st) (32) indicating that the linear weights are updated in proportion to the activity of their corresponding features. Non-linear function approximators can also be used, but typically have weaker convergence guarantees than linear function approximators. Nevertheless, due to their \ufb02exibility such approximators have enabled impressive performance in a number of challenging domains, such as Atari games (Mnih et al., 2015) and Go (Silver et al., 2016). 3and also gives SARSA its name 4we later discuss a method for sampling states 9 3.4 Policy gradient methods Parameterised stochastic policies \u03c0\u03b8 may be improved using the policy gradient theorem (Sutton et al., 2000). This can be derived for any of the common RL objectives. To demonstrate a derivation of this result we use a starting state objective J(\u03b8) = v\u03c0\u03b8(s0) with a single starting state s0: \u2207\u03b8J(\u03b8) = \u2207\u03b8v\u03c0(s0) = \u2207\u03b8 \ufffd a \u03c0(s0, a)q\u03c0(s0, a) = \ufffd a \u2207\u03b8\u03c0(s0, a)q\u03c0(s0, a) + \u03c0(s0, a)\u2207\u03b8q\u03c0(s0, a) = \ufffd a \u2207\u03b8\u03c0(s0, a)q\u03c0(s0, a) + \u03c0(s0, a)\u2207\u03b8 \ufffd r(s0, a) + \ufffd s\u2032 \u03b3P(s0, a, s\u2032)v\u03c0(s\u2032) \ufffd = \ufffd a \u2207\u03b8\u03c0(s0, a)q\u03c0(s0, a) + \u03c0(s0, a) \ufffd s\u2032 \u03b3P(s0, a, s\u2032)\u2207\u03b8v\u03c0(s\u2032) (33) We note that we could continue to unroll \u2207\u03b8v\u03c0(s\u2032) on the R.H.S in the same way as we have already done. Considering now transitions from starting state s0 to arbitrary state s we therefore \ufb01nd: \u2207\u03b8v\u03c0(s0) = \ufffd s \u221e \ufffd t=0 \u03b3tp(st = s|s0, \u03c0) \ufffd a \u2207\u03b8\u03c0(s, a)q\u03c0(s, a) (34) where \ufffd\u221e t=0 \u03b3tp(st = s|s0, \u03c0) is the discounted state distribution \u03c1\u03c0(s) from a \ufb01xed starting state s0 (Equation 25). This derivation holds even when there is a distribution over starting states, and gives us the policy gradient theorem: \u2207\u03b8J(\u03b8) = \ufffd s \u03c1\u03c0(s) \ufffd a \u2207\u03b8\u03c0(s, a)q\u03c0(s, a) (35) Using the likelihood ratio trick: \u2207\u03b8\u03c0(s, a) = \u03c0(s, a)\u2207\u03b8\u03c0(s, a) \u03c0(s, a) = \u03c0(s, a)\u2207\u03b8 log \u03c0(s, a) (36) this can be equivalently expressed as: \u2207\u03b8J(\u03b8) = \ufffd s \u03c1\u03c0(s) \ufffd a \u03c0(s, a)q\u03c0(s, a)\u2207\u03b8 log \u03c0(s, a) = E\u03c0[q\u03c0(s, a)\u2207\u03b8 log \u03c0(s, a)] (37) The policy gradient theorem result enables model-free learning as gradients need only be determined for the policy rather than for properties of the environment. There are a variety of approaches for determining q\u03c0. If q\u03c0 is approximated using the sample return (Equation 18), this leads to the algorithm known as REINFORCE (Williams, 1992): \u03b8 \u2190 \u03b8 + \u03b1Rt\u2207\u03b8 log \u03c0(st, at) (38) 10 As there is no bootstrapping here, this is also known as MC policy gradient. An alternative approach is to separately approximate q\u03c0 with a \u2018critic\u2019 qw giving rise to what are commonly known as \u2018actor-critic\u2019 methods. These introduce two sets of parameter updates; the critic parameters w are updated to approximate q\u03c0, and the policy (actor) parameters \u03b8 are updated according to the policy gradient as indicated by the critic. The critic itself can be updated according to the TD error. An example of this approach is SARSA actor-critic: w \u2190 w + \u03b11(rt+1 + \u03b3qw(st+1, at+1) \u2212 qw(st, at))\u2207wqw(st, at) \u03b8 \u2190 \u03b8 + \u03b12qw(st, at)\u2207\u03b8 log \u03c0(st, at) (39) where di\ufb00erent learning rates \u03b11 and \u03b12 may be used for the actor and the critic. 3.5 Baselines Whether we use REINFORCE or an actor-critic based approach to policy gradients, it is possible to reduce the variance further by the introduction of baselines. If this baseline depends only on the state s, then we \ufb01nd it introduces no bias: \ufffd s \u03c1\u03c0(s) \ufffd a \u2207\u03b8\u03c0(s, a)b(s) = \ufffd s \u03c1\u03c0(s)b(s)\u2207\u03b8 \ufffd a \u03c0(s, a) = \ufffd s \u03c1\u03c0(s)b(s)\u2207\u03b81 = 0 (40) A natural choice for the state-dependent baseline is the value function: \u2207\u03b8J(\u03b8) = E\u03c0[(q\u03c0(s, a) \u2212 v\u03c0(s))\u2207\u03b8 log \u03c0(s, a)] = E\u03c0[A\u03c0(s, a)\u2207\u03b8 log \u03c0(s, a)] (41) where A\u03c0 is known as the advantage, which may in some algorithms be approximated directly (rather than approximating both q\u03c0 and v\u03c0). 3.6 Compatible function approximation In the general case, our choice to approximate q\u03c0 with qw introduces bias such that there are no guarantees of convergence to a local optimum. However, in the special case of a compatible function approximator we can introduce no bias and take steps in the direction of the true policy gradient. This becomes possible when the critic\u2019s function approximator reaches a minimum in the mean-squared error: 0 = E\u03c0[\u2207w(q\u03c0(s, a) \u2212 qw(s, a))2] = E\u03c0[(q\u03c0(s, a) \u2212 qw(s, a))\u2207wqw(s, a)] (42) If we choose qw(s, a) such that \u2207wqw(s, a) = \u2207\u03b8 log \u03c0(s, a) we \ufb01nd: E\u03c0[q\u03c0(s, a)\u2207\u03b8 log \u03c0(s, a)] = E\u03c0[qw(s, a)\u2207\u03b8 log \u03c0(s, a)] (43) 11 ",
    "Approaches": "Approaches In model-free RL agents learn to take actions directly from experiences, without ever modelling transitions in the environment or reward functions, whereas in model-based RL the agent attempts to learn these. The key bene\ufb01t is that if the agent can perfectly predict the environment \u2018in its head\u2019, then it no longer needs to interact directly with the environment in order to learn an optimal policy. 4.1 Model Learning Recall that MDPs are de\ufb01ned in terms of the 5-tuple \u27e8S, A, P, r, \u03b3\u27e9. Although models can be predictions about anything, a natural starting point is to approximate the state transition function P\u03b7 \u2248 P and immediate reward function 12 r\u03b7 \u2248 r. We can then use dynamic programming to learn the optimal policy for an approximate MDP \u27e8S, A, P\u03b7, r\u03b7, \u03b3\u27e9, the performance of which may be worse than for the true MDP. Given a \ufb01xed set of experiences, a model can be learned using supervised methods. For predicting immediate expected scalar rewards, this is a regression problem whereas for predicting the distribution over next states this a density estimation problem. Given the simplicity of this framing, a range of function approximators may be employed, including neural networks and Gaussian processes. 4.2 Combining model-free and model-based approaches Once a model is learned it can be used for planning. However, in many situations it is computationally infeasible to do the full-width backups of dynamic programming as the state space is too large. Instead, experiences can be sampled from the model and used as data by a model-free algorithm. A well known architecture which combines model-based and model-free RL is the Dyna architecture (Sutton, 1991). Dyna treats samples of simulated and real experience similarly, using both to learn a value function. Simulated experience is generated by the model which is itself learned from real experience. In Dyna, model-free based updates depend on the state the agent is currently in, whereas for the model-based component starting states can be sampled randomly and then rolled forwards using the model to update the value function using e.g. TD learning. One potential disadvantage of Dyna is that it does not preferentially treat the state the agent is currently in. In many cases, such as deciding on the next move in chess, it is useful to start all rollouts from the current state (the board position) when choosing the next move. This is known as forward search, where a search tree is built with the current state as the root. Forward based search often uses sample based rollouts rather than full-width ones so as to be computationally tractable and this is known as simulation-based search. An e\ufb00ective algorithm for simulation-based search is Monte-Carlo Tree search (Coulom, 2007). It uses the MC return to estimate the action-value function for all nodes in the search tree using the current policy. It then improves the policy, for example by being \u01eb-greedy with respect to the new action-value function (or more commonly handling exploration-exploitation using Upper Con\ufb01dence Trees, see Kocsis and Szepesv\u00b4ari (2006) for a more detailed discussion). MC Tree Search is equivalent to MC control applied to simulated experience and therefore is guaranteed to converge on the optimal search tree. Instead of using MC control for search it is also possible to use TD-based control, which will increase bias but reduce variance. Model-based RL is a highly active area of research. Recent advances include MuZero (Schrittwieser et al., 2020), which extends model-based predictions to value functions and policies, and Dreamer which plans using latent variable models (Hafner et al., 2019). 13 5 Latent variables and partial observability 5.1 Latent variable models Hidden or \u2018latent\u2019 variables correspond to variables which are not directly observed but nevertheless in\ufb02uence observed variables and thus may be inferred from observation. In reinforcement learning, it can be bene\ufb01cial for agents to infer latent variables as these often provide a simpler and more parsimonious description of the world, enabling better predictions of future states and thus more e\ufb00ective control. Latent variable models are common in the \ufb01eld of unsupervised learning. Given data p(x) we may describe a probability distribution over x according to: p(x; \u03b8x|z, \u03b8z) = \ufffd dz p(x|z; \u03b8x|z)p(z; \u03b8z) (48) where \u03b8x|z parameterises the conditional distribution x|z and \u03b8z parameterises the distribution over z. Key aims in unsupervised learning include capturing high-dimensional correlations with fewer parameters (as in probabilistic principal components analysis), generating samples from a data distribution, describing an underlying generative process z which describes causes of x, and \ufb02exibly modelling complex distributions even when the underlying components are simple (e.g. belonging to an exponential family). 5.2 Partially observable Markov decision processes A partially observable Markov decision process (POMDP) (Kaelbling et al., 1998) is a generalisation of an MDP in which the agent cannot directly observe the true state of the system, the dynamics of which is determined by an MDP. Formally, a POMDP is a 7-tuple \u27e8S, A, P, r, O, \u2126, \u03b3\u27e9 where S is the set of states, A is the set of actions, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the state transition probability kernel, r : S \u00d7 A \u00d7 S \u2192 R is the reward function, O is the set of observations, \u2126 : S \u00d7 A \u00d7 O \u2192 [0, 1] is the observation probability kernel and \u03b3 \u2208 [0, 1) is the discount factor. As with MDPs, agents in POMDPs seek to learn a policy \u03c0(s\u03b1 t ) which maximises some notion of cumulative reward, commonly E\u03c0[\ufffd\u221e t=0 \u03b3trt+1]. This policy depends on the agent\u2019s representation of state s\u03b1 t = f(ht), which is a function of its history. One approach to solving POMDPs is by maintaining a belief state over the latent environment state - transitions for which satisfy the Markov property. Maintaining a belief over states only requires knowledge of the previous belief state, the action taken and the current observation. Beliefs may then be updated according to: b\u2032(s\u2032) = \u03b7\u2126(o\u2032|s\u2032, a) \ufffd s\u2208S P(s\u2032|s, a)b(s) (49) where \u03b7 = 1/ \ufffd s\u2032 \u2126(o\u2032|s\u2032, a) \ufffd s\u2208S P(s\u2032|s, a)b(s) is a normalising constant. A Markovian belief state allows a POMDP to be formulated as an MDP where every belief is a state. However, in practice, maintaining belief states in POMDPs will be computationally intractable for any reasonably sized problem. In order to address this, approximate solutions may be used. Alternatively, 14 agents learning using function approximators which condition on the past can construct their own state representations, which may in turn enable relevant aspects of the state to be approximately Markov. 6 Deep reinforcement learning The policies and value functions used in reinforcement learning can be learned using arti\ufb01cial neural network function approximators. When such networks have many layers they are conventionally denoted as \u2018deep\u2019, and are typically trained on large amounts of data using stochastic gradient descent (LeCun et al., 2015). The application of deep networks in model-free reinforcement learning garnered extensive attention when they were successfully used to learn a variety of Atari games from scratch (Mnih et al., 2013). For the particular problem of learning from pixels a convolutional neural network architecture was used (LeCun et al., 1998), which are highly e\ufb00ective at extracting useful features from images. They have been extensively used on supervised image classi\ufb01cation tasks due to their ability to scale to large and complex datasets (LeCun et al., 2015). A deep analysis of deep reinforcement learning (DRL) is beyond the scope of this summary. However we review two key techniques used to overcome the technical challenge of stabilising training. 6.1 Experience replay As an agent interacts with its environment it receives experiences that can be used for learning. However, rather than using those experiences immediately, it is possible to store such experience in a \u2018replay bu\ufb00er\u2019 and sample them at a later point in time for learning. The bene\ufb01ts of such an approach were introduced by Mnih et al. (2013) for their \u2018deep Q-learning\u2019 algorithm. At each timestep, this method stores experiences et = (st, at, rt+1, st+1) in a replay bu\ufb00er over many episodes. After su\ufb03cient experience has been collected, Q-learning updates are then applied to randomly sampled experiences from the bu\ufb00er. This breaks the correlation between samples, reducing the variance of updates and the potential to over\ufb01t to recent experience. Further improvements to the method can be made by prioritised (as opposed to random) sampling of experiences according to their importance, determined using the temporal-di\ufb00erence error (Schaul et al., 2015). 6.2 Target networks When using temporal di\ufb00erence learning with deep function approximators a common challenge is stability of learning. A source of instability arises when the same function approximator is used to evaluate both the value of the current state and the value of the target state for the temporal di\ufb00erence update. After such updates, the approximated value of both current and target state change (unlike tabular methods), which can lead to a runaway target. To address this, deep RL algorithms often make use of a separate target network that remains stable even whilst the standard network is updated. As it is not desirable for the target network to diverge too far from the standard network\u2019s improved 15 predictions, at \ufb01xed intervals the parameters of the standard network can be copied to the target network. Alternatively, this transition is made more slowly using Polyak averaging: \u03c6targ \u2190 \u03c1\u03c6targ + (1 \u2212 \u03c1)\u03c6 (50) where \u03c6 are the parameters of the standard network and \u03c1 is a hyperparameter typically close to 1. 16 ",
    "References": "References Sanjeevan Ahilan. Structures for Sophisticated Behaviour: Feudal Hierarchies and World Models. PhD thesis, UCL (University College London), 2021. Stefan Banach. Sur les op\u00b4erations dans les ensembles abstraits et leur application aux \u00b4equations int\u00b4egrales. Fund. math, 3(1):133\u2013181, 1922. Richard Bellman. A markovian decision process. Journal of mathematics and mechanics, pages 679\u2013684, 1957. Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena scienti\ufb01c Belmont, MA, 1995. R\u00b4emi Coulom. E\ufb03cient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 72\u201383. Springer, 2007. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Arti\ufb01cial intelligence, 101(1-2):99\u2013134, 1998. Levente Kocsis and Csaba Szepesv\u00b4ari. Bandit based monte-carlo planning. In European conference on machine learning, pages 282\u2013293. Springer, 2006. Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Ha\ufb00ner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11): 2278\u20132324, 1998. Yann LeCun, Yoshua Bengio, and Geo\ufb00rey Hinton. Deep learning. nature, 521 (7553):436\u2013444, 2015. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, UK, 1994. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. 17 Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020. Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward. Science, 275(5306):1593\u20131599, 1997. David Silver. Lecture 3: Planning by dynamic programming. Google DeepMind, 2015. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000. Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8 (3-4):279\u2013292, 1992. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992. 18 ",
    "title": "A Succinct Summary of Reinforcement Learning",
    "paper_info": "arXiv:2301.01379v1  [cs.AI]  3 Jan 2023\nA Succinct Summary of Reinforcement Learning\nSanjeevan Ahilan\u2217\nAbstract\nThis document is a concise summary of many key results in single-\nagent reinforcement learning (RL). The intended audience are those who\nalready have some familiarity with RL and are looking to review, reference\nand/or remind themselves of important ideas in the \ufb01eld.\nContents\n1\nAcknowledgements\n2\n2\nFundamentals\n2\n2.1\nThe RL paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.2\nAgent and environment\n. . . . . . . . . . . . . . . . . . . . . . .\n2\n2.3\nObservability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.4\nMarkov processes and Markov reward processes . . . . . . . . . .\n3\n2.5\nMarkov decision processes . . . . . . . . . . . . . . . . . . . . . .\n3\n2.6\nPolicies, values and models\n. . . . . . . . . . . . . . . . . . . . .\n3\n2.7\nDynamic programming . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nModel-free approaches\n6\n3.1\nPrediction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2\nControl with action-value functions . . . . . . . . . . . . . . . . .\n8\n3.3\nValue function approximation . . . . . . . . . . . . . . . . . . . .\n9\n3.4\nPolicy gradient methods . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.5\nBaselines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.6\nCompatible function approximation . . . . . . . . . . . . . . . . .\n11\n3.7\nDeterministic policy gradients . . . . . . . . . . . . . . . . . . . .\n12\n4\nModel-based Approaches\n12\n4.1\nModel Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.2\nCombining model-free and model-based approaches . . . . . . . .\n13\n5\nLatent variables and partial observability\n14\n5.1\nLatent variable models . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5.2\nPartially observable Markov decision processes\n. . . . . . . . . .\n14\n6\nDeep reinforcement learning\n15\n6.1\nExperience replay . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.2\nTarget networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n\u2217sanjeevanahilan@gmail.com. Much of this work was done at the Gatsby Unit, UCL.\n1\n",
    "GPTsummary": "- (1): The article is a concise summary of many key results in single-agent reinforcement learning (RL).\n- (2): The past methods include model-free and model-based approaches. However, maintaining belief states in POMDPs may be computationally intractable for any reasonably sized problem. The approach is well-motivated, but there are potential drawbacks to using Dyna.\n- (3): The research methodology proposed in this paper covers fundamentals such as the RL paradigm, agent and environment, observability, Markov processes, and Markov decision processes, as well as model-free approaches such as prediction, control with action-value functions, value function approximation, policy gradient methods, baselines, and compatible function approximation. The article also covers model-based approaches such as model learning and combining model-free and model-based approaches. Additionally, the paper examines latent variables and partial observability and deep reinforcement learning, including experience replay and target networks.\n- (4): The article provides a summary of important concepts in the field of single-agent reinforcement learning, including both model-free and model-based approaches. The approach is well-motivated and provides an overview of key concepts, but it does not present any new results or experimental data.\n7. Methods: \n\n- (1): The article provides a concise summary of significant findings and concepts in single-agent reinforcement learning (RL), covering fundamentals such as the RL paradigm, agent and environment, observability, Markov processes, and Markov decision processes.\n\n- (2): The paper examines in detail model-free approaches such as prediction, control with action-value functions, value function approximation, policy gradient methods, baselines, and compatible function approximation.\n\n- (3): Additionally, the article discusses model-based approaches such as model learning and combining model-free and model-based approaches, as well as latent variables and partial observability and deep reinforcement learning including experience replay and target networks.\n\n- (4): The article does not present any new experimental data or results but summarizes important concepts in single-agent RL, including both model-free and model-based approaches.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this article lies in its concise summary of important concepts in single-agent reinforcement learning, providing a well-motivated overview of both model-free and model-based approaches to RL.\n\n- (2): Regarding innovation point, while the article does not present any new experimental data or results, it summarizes significant findings in the field of RL. In terms of performance, the paper effectively covers fundamental concepts in RL as well as more advanced topics like deep RL, providing readers with a comprehensive understanding of the subject matter. In terms of workload, the article is concise and to the point, making it an accessible resource for those looking to deepen their understanding of RL.\n\n\n",
    "GPTmethods": "- (1): The article provides a concise summary of significant findings and concepts in single-agent reinforcement learning (RL), covering fundamentals such as the RL paradigm, agent and environment, observability, Markov processes, and Markov decision processes.\n\n- (2): The paper examines in detail model-free approaches such as prediction, control with action-value functions, value function approximation, policy gradient methods, baselines, and compatible function approximation.\n\n- (3): Additionally, the article discusses model-based approaches such as model learning and combining model-free and model-based approaches, as well as latent variables and partial observability and deep reinforcement learning including experience replay and target networks.\n\n- (4): The article does not present any new experimental data or results but summarizes important concepts in single-agent RL, including both model-free and model-based approaches.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this article lies in its concise summary of important concepts in single-agent reinforcement learning, providing a well-motivated overview of both model-free and model-based approaches to RL.\n\n- (2): Regarding innovation point, while the article does not present any new experimental data or results, it summarizes significant findings in the field of RL. In terms of performance, the paper effectively covers fundamental concepts in RL as well as more advanced topics like deep RL, providing readers with a comprehensive understanding of the subject matter. In terms of workload, the article is concise and to the point, making it an accessible resource for those looking to deepen their understanding of RL.\n\n\n",
    "GPTconclusion": "- (1): The significance of this article lies in its concise summary of important concepts in single-agent reinforcement learning, providing a well-motivated overview of both model-free and model-based approaches to RL.\n\n- (2): Regarding innovation point, while the article does not present any new experimental data or results, it summarizes significant findings in the field of RL. In terms of performance, the paper effectively covers fundamental concepts in RL as well as more advanced topics like deep RL, providing readers with a comprehensive understanding of the subject matter. In terms of workload, the article is concise and to the point, making it an accessible resource for those looking to deepen their understanding of RL.\n\n\n"
}