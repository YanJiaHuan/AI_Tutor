{
    "Abstract": "Abstract\u2014 Safety guarantee is essential in many engineering implementations. Reinforcement learning provides a useful way to strengthen safety. However, reinforcement learning algorithms cannot completely guarantee safety over realistic operations. To address this issue, this work adopts control barrier functions over reinforcement learning, and proposes a compensated algorithm to completely maintain safety. Speci\ufb01cally, a sum-of-squares programming has been exploited to search for the optimal controller, and tune the learning hyperparameters simultaneously. Thus, the control actions are pledged to be always within the safe region. The effectiveness of proposed method is demonstrated via an inverted pendulum model. Compared to quadratic programming based reinforcement learning methods, our sum-of-squares programming based reinforcement learning has shown its superiority. I. ",
    "Introduction": "INTRODUCTION Reinforcement learning (RL) is one of the most popular methods for accomplishing the long-term objective of a Markov-decision process [1]\u2013[6]. This learning method aims for an optimal policy that can maximize a long-term reward in a given environment recursively. For the case of RL policy gradient method [7]\u2013[9], this reward-related learning problem is traditionally addressed using gradient descent to obtain superior control policies. RL algorithms might generate desired results in some academic trials, e.g., robotic experiments [10], [11] and autonomous driving contexts [12], [13]. But for realistic situations, how to completely guarantee the safety based on these RL policies is still a problem. The intermediate policies from the RL agent\u2019s exploration and exploitation processes may sometimes ruin the environment or the agent itself, which is unacceptable for safety-critical systems. Moreover, disturbances around the system would confuse the agent over learning process, which make the control tasks more complex and challenging. To cope with the issues above, various methods are proposed to specify dynamical safety during the learning process. Initially, the region of attraction (ROA) from control Lyapunov functions or Lyapunov-like function work [14], [15] are used to certify the dynamical safety. Related and useful work of ROA estimation are introduced in [16]\u2013[19]. In 2017, [20] started to execute the RL\u2019s exploration and exploitation in ROA to maintain safety. Different from region constraint, in 2018 [21], [22] proposed a shield framework to select safe actions repeatedly. Inspired by this work, [23], 1Hejun Huang and Dongkun Han are with the Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong 2Zhenglong Li is with the Department of Electrical and Electronic Engineering, The University of Hong Kong *Email: dkhan@mae.cuhk.edu.hk [24] further combined the control barrier function (CBF) to synthesis the RL-based controller. Note that, the CBF is not new to identify safe regions in control theory, [25], [26] are representative work to combine CBF in biped robot and [27] explored its implementation of quadrotors, etc. Appropriate region constraint for RL is important for building safe RL. Hence, a reliable learning method is expected to construct a controller inside the safe region over the process. In this paper, we will consider a state-space model in the polynomial form, including an unknown term that estimated by Gaussian process. Then, we will apply Deep Deterministic Policy Gradient (DDPG) algorithm to design a controller. The generated control action will impact system dynamics and further in\ufb02uence the sum-of-squares programming (SOSP) solution of control barrier functions. The main contributions of this work are: (1) Formulate the framework to embed SOSP-based CBF into DDPG algorithm; (2) compare the performance of quadratic programming (QP)-based and SOSP-based controller with DDPG algorithm; and (3) demonstrate the safety and ef\ufb01ciency of DDPG-SOSP algorithm via an inverted pendulum model. This paper is organized as follows, we \ufb01rst present some preliminaries about reinforcement learning, Gaussian process and control barrier function in Section II. Then, in Section III, we formulate the steps to compute an SOSPbased controller and introduce a corresponding DDPG-SOSP algorithm. Numerical examples are given for these cases in Section IV, then we discuss the learning ef\ufb01ciency and safety performance of QP-based and SOSP-based strategies, before concluding this paper. II. ",
    "Preliminary": "PRELIMINARY A discrete-time nonlinear control-af\ufb01ne system with state st, st+1 \u2208 S and control action at \u2208 A is considered as: st+1 = f(st) + g(st)at + d(st), \u2200t \u2208 [t, t], (1) where st, st+1 are \ufb01nished within a bounded time t, while t and t are constants. Notation: From the in\ufb01nite-horizon discounted Markov decision process (MDP) theory [28], let r : S \u00d7 A \u2192 R denote the reward of the current state-action pair (st, at) in (1), let P(st, st+1, at) = p(st+1|st, at) denote the probability distribution to next state st+1, and let \u03b3 \u2208 (0, 1) denote a discount factor. Thus, a MDP tuple \u03c4 = (st, st+1, at, rt, Pt, \u03b3) establishes at each state t. arXiv:2206.07915v2  [eess.SY]  29 Jun 2022 A. Reinforcement Learning RL focuses on training an agent to map various situations to the most valuable actions in a long-term process. During the process, the agent will be inspired by a cumulative reward r to \ufb01nd the best policy \u03c0(a|s). Various novel RL algorithms are proposed to select optimal \u03c0(a|s). For more details of selecting \u03c0, we kindly refer interested readers to [1]. Given MDP tuple \u03c4, the RL agent is selecting an optimal policy \u03c0\u2217 by maximizing the expected reward J(\u03c0) J(\u03c0) = E\u03c4[ \u221e \ufffd t=0 \u03b3tr(st, at)]. (2) Furthermore, the action value function Q\u03c0 can be generated from \u03c4 and satis\ufb01es the Bellman equation as follows, Q\u03c0(st, at) = Est,at[rt + \u03b3Eat+1[Q\u03c0(st+1, at+1)]] = Est+1,at+1,...[ \u221e \ufffd l=0 \u03b3lr(st+l, at+l)]. (3) In this paper, we will use DDPG algorithm, a typical actor-critic and off-policy method, to handle the concerned dynamics of (1). Let parameters \u03b8\u03c0 and \u03b8Q denote the neural network of actor and critic, respectively. By interacting with the environment repeatedly, the actor will generate an actor policy \u03c0 : S \u2192 A and be evaluated by the critic via Q\u03c0 : S \u00d7 A \u2192 R. These actions will be stored in replay buffer to update the policy gradient w.r.t \u03b8\u03c0, and the loss function w.r.t \u03b8Q, once if the replay buffer is ful\ufb01lled. However, the state-action of DDPG algorithm generates without a complete safety guarantee, which is unacceptable in safety critical situations. Inspired by the work of [23], we propose a programming assist method to maintain safety over the learning process in Section III. B. Gaussian Process Gaussian processes (GP) estimate the system and further predict dynamics based on the prior data. A GP is a stochastic process that construct a joint Gaussian distribution with concerned states {s1, s2, . . . } \u2282 S. We use GP to estimate the unknown term d in (1) during the system implementation. Mixed with an independent Gaussian noise (0, \u03c32 n), the GP model\u2019s prior data {d0, d1, . . . , dk} can be computed from dk = sk+1 \u2212 f \u2212 g \u00b7 ak indirectly. Then, a high probability statement of the posterior output \u02c6d : S \u2192 R generates as d(s) \u223c N(md(s), \u03c3d(s)), (4) where the mean function md(s) and the variance function \u03c3d(s) can describe the posterior distribution as md(s) \u2212 k\u03b4\u03c3d(s) \u2264 d(s) \u2264 md(s) + k\u03b4\u03c3d(s), (5) with probability greater or equal to (1 \u2212 \u03b4)k, \u03b4 \u2208 (0, 1) [29], where k\u03b4 is a parameter to design interval [(1\u2212\u03b4)k, 1]. Therefore, by learning a \ufb01nite number of data points, we can estimate the unknown term d(s\u2217) of any query state s\u2217 \u2208 S. As the following lemma declaims in [30], [31], there exists a polynomial mean function to approximate the term d via GP, which can predict the output of state s\u2217 straightly here. Lemma 1. Suppose we have access to k measurements of d(s) in (1) that corrupted with Gaussian noise (0, \u03c32 n). If the norm unknown term d(s) bounded, the following GP model of d(s) can be established with polynomial mean function md(s\u2217) and covariance function \u03c32 d(s\u2217), md(s\u2217) = \u03d5(s\u2217)Tw, \u03c32 d(s\u2217) = k(s\u2217, s\u2217) \u2212 kT \u2217 (K + \u03c32 nI)\u22121k\u2217, (6) within probability bounds [(1\u2212\u03b4)k, 1], where \u03b4 \u2208 (0, 1), s\u2217 is a query state, \u03d5(s\u2217) is a monomial vector, w is a coef\ufb01cient vector, [K](i,j) = k(si, sj) is a kernel Gramian matrix and k\u2217 = [k(s1, s\u2217), k(s2, s\u2217), . . . , k(sk, s\u2217)]T. Lemma 1 generates a polynomial expression of d(s) within probabilistic range [(1 \u2212 \u03b4)k, 1]. Meanwhile, the nonlinear term f(s) in (1) can be approximated by Chebyshev interpolants Pk(s) and bounded remainder \u03be(s) in certain domain as f(s) = Pk(s) + \u03be(s) [32], where k is the degree of the polynomial Pk(x) [32]. Now we convert (1) as st+1 = Pk(st) + g(st)at + d\u03be(st). (7) The polynomial (7) is equal to (1) with a new term d\u03be(st) = d(st) + \u03be(st). Consequently, we can obtain a polynomial system within a probability range by learning d\u03be(st), st+1 = Pk(st) + g(st)at + md\u03be(st), (8) which will be used to synthesis the controller in Section III. C. Control Barrier Function The super-level set of the control barrier function (CBF) h : S \u2192 R could validate an safety set C as C = {st \u2208 S : h(st) \u2265 0}. (9) Throughout this paper, we refer to C as a safe region and U = S \u2212 C = {st \u2208 S : h(st) < 0}, (10) as unsafe regions of the dynamical system (8). Then, the state st \u2208 C will not enter into U by satisfying the forward invariant constraint below, \u2200st \u2208 C : h(st) \u2265 0, \u2206h(st, at) \u2265 0, (11) where \u2206h(st, at) = h(st+1)\u2212h(st). Before we demonstrate the computation of h of the system (8), the Positivestellensatz (P-satz) needs to be introduced \ufb01rst [33]. Let P be the set of polynomials and PSOS be the set of sum of squares polynomials, e.g., P(x) = \ufffdk i=1 p2 i (x), where pi(x) \u2208 P and P(x) \u2208 PSOS. Lemma 2. For polynomials {ai}m i=1, {bj}n j=1 and p, de\ufb01ne a set B = {s \u2208 S : {ai(s)}m i=1 = 0, {bj(s)}n j=1 \u2265 0}. Let B be compact. The condition \u2200x \u2208 S, p(s) \u2265 0 holds if the following condition is satis\ufb01ed, \ufffd \u2203r1, . . . , rm \u2208 P, s1, . . . , sn \u2208 PSOS, p \u2212 \ufffdm i=1 riai \u2212 \ufffdn j=1 sjbj \u2208 PSOS. \u25a1 Lemma 2 points out that any strictly positive polynomial p lies in the cone that generated by non-negative polynomials Fig. 1. Work\ufb02ow of the safe RL control with SOS program {bj}n j=1 in the set of B. Based on the de\ufb01nition of C and U, P-satz will be adequately used in the safety veri\ufb01cation. The suf\ufb01cient condition to establish a CBF is the existence of a deterministic controller aCBF : S \u2192 A. Based on Lemma 2, we can compute a polynomial controller aCBF through SOS program to avoid entering into unsafe regions \u00b5i(s) \u2208 U, i = 1, 2, . . . , n at each step t. Lemma 3. Provided a safe region C certi\ufb01ed by the polynomial barrier function h and some unsafe regions \u00b5i \u2208 U, i \u2208 Z+, in a polynomial system (8), if there exists a controller aCBF that satis\ufb01es a\u2217 = arg min \u2225aCBF \u22252 st\u2208S,aCBF \u2208A;L(st),Mi(st)\u2208PSOS; s.t. \u2206h(st, aCBF ) \u2212 L(st)h(st) \u2208 PSOS, \u2212 \u2206h(st, aCBF ) \u2212 Mi(st)\u00b5i(st) \u2208 PSOS, (12) where L(st) and Mi(st) are SOS polynomials for i = 1, 2, . . . , n. Then, action a\u2217 is a minimum polynomial controller that regulates the system (8) safely. Proof. Let us suppose that there exists a CBF h that certi\ufb01ed a safe region C. Then, a deterministic input at = aCBF (st) is needed to establish the necessary condition \u2206h(st, aCBF ) \u2265 0 in (11) of the system (8). Since h is a polynomial function, if aCBF is also a polynomial, \u2206h will maintain in the polynomial form. According to Lemma 2, we can formulate this non-negative condition of \u2206h as part of SOS constraint in the cone of h as \u2206h(st, aCBF ) \u2212 L(st)h(st) \u2208 PSOS, (13) where the auxiliary SOS polynomials L(st) is used to project the related term into an non-negative compact domain C that certi\ufb01ed by the superlevel set of h(st). We continue to leverage the P-satz again to drive the dynamics of (8) far from unsafe areas \u00b5i(st) as follows, \u2212\u2206h(st, aCBF ) \u2212 \u00b5i(st)Mi(st) \u2208 PSOS, (14) where Mi(st) denote the corresponding SOS polynomials. Guided by the optimization (12), we can solve a minimal cost control to maintain system safety with (13) and (14), which completes the proof. This lemma maintains the safety of polynomial system (8) and minimizes the current control aCBF with a SOS program. Multiple unsafe regions \u00b5i(st) are also considered in solving the target input a\u2217 in (12). Since Lemma 1 discussed the probabilistic relationship of (8) and (1), the computed result of (12) of (1) can also be a feasible control to regulate the true dynamics safely. III. CBF GUIDING CONTROL WITH REINFORCEMENT LEARNING In the \ufb01rst part of this section, we will further express the computation steps of (12). In the second part, we will go through the details about the SOSP-based CBF guiding control with DDPG algorithm. A. SOS Program Based CBF Different from [23], our control aCBF is solved by SOS program with polynomial barrier functions, rather than the QP and linear barrier functions. In [23], they initially constructed QP with monotonous linear barrier constraints to cope with RL. Obviously, a safe but more relaxed searching area will enhance the RL training performance. We propose a SOS program to compute a minimal control with polynomial barrier function based on Lemma 3. Lemma 3 declared that we can compute a minimal control aCBF from an approximated polynomial system (8) directly. However, it is hard to de\ufb01ne the minimization of the convex function aCBF in (12). The optimal control can be searched by using the following 2 steps: Step 1: Search an optimal auxiliary factor L(st) by maximizing the scalar \u03f51, L\u2217(st) = arg max \u03f51 \u03f51\u2208R+,L(st)\u2208PSOS s.t. \u2206h(st, aCBF ) \u2212 L(st)h(st) \u2212 \u03f51 \u2208 PSOS \u2212 \u2206h(st, aCBF ) \u2212 Mi(st)\u00b5i(st) \u2208 PSOS, (15) where L(st) is an auxiliary factor to obtain a permissive constraint for the existing action aCBF . Step 2: Given L(st) from last step, search an optimal control of aCBF with minimal control cost by minimizing the scalar \u03f52, a\u2217 t = arg min \u03f52 \u03f52\u2208R+ s.t. \u2206h(st, aCBF ) \u2212 L(st)h(st) \u2212 \u03f52 \u2208 PSOS \u2212 \u2206h(st, aCBF ) \u2212 Mi(st)\u00b5i(st) \u2208 PSOS. (16) Remark. The scalars \u03f51 and \u03f52 in (15) and (16) are used to limit the magnitude of the coef\ufb01cients\u2019 optimization of L(st) and at, respectively. The SOS programs above demonstrate the details of a target controller computation of the system (8), and the solution of (16) regulates the dynamical safety via a control barrier function directly. B. SOS Program Based CBF with Reinforcement Learning The work\ufb02ow of the CBF guiding DDPG control algorithm is shown in Fig. 1, which is akin to the original idea in [23] to push the agent\u2019s action into a safe training architecture. In Fig. 1, we list these \ufb02owing elements into brackets and highlight the corresponding input of each factor computation. We illustrate the core idea of CBF guiding RL control as, at =aRL \u03b8k + \u00afak + aCBF t , (17) where the \ufb01nal action at consists of a RL-based controller aRL \u03b8k , a previous deployed CBF controller \u00afa\u03c6 and a SOS program based CBF controller aCBF t . A clear distinction of the subscript t and k is stated here: t denotes the step number of each policy iteration and k denotes the policy iteration number over the learning process. More speci\ufb01cally, the \ufb01rst term in (17) is an action generated from a standard DDPG algorithm with parameter \u03b8. The second term \u00afak = \ufffdk\u22121 i=0 aCBF i demotes a global consideration of previous CBF controllers. Since it is impractical to compute the exact value of each CBF action aCBF i at each step. Supported by the work of [23], we approximate this term as \u00afa\u03c6k \u2248 \u00afak = \ufffdk\u22121 i=0 aCBF i , where \u00afa\u03c6k denotes an output from the multilayer perceptron (MLP) with hyperparameters \u03c6. Then, we \ufb01t the MLP and update \u03c6k with \ufffdk\u22121 i=0 aCBF i (s, aRL \u03b80 , . . . , aRL \u03b8i\u22121) at each episode. The third term aCBF t in (17) is a real-time value based on the deterministic value st, aRL \u03b8k (st) and \u00afa\u03c6k(st). Although there exists an unknown term d in the system (1), we can still solve the dynamical safety in the approximated polynomial system (8). So, the optimal at in (17) can be computed by the SOS program below with deterministic aRL \u03b8k and \u00afa\u03c6k a\u2217 = arg min \u2225ak\u22252 st\u2208S,ak\u2208A;L(st),Mi(st)\u2208PSOS; s.t. \u2206h(st, ak) \u2212 L(st)h(st) \u2208 PSOS, \u2212 \u2206h(st, ak) \u2212 Mi(st)\u00b5i(st) \u2208 PSOS. (18) Thus, we can establish a controller by satisfying the solution of (18) over the learning process. Theorem 1. Given a barrier function h in (9), a partially unknown dynamical system (1) and a corresponding approximated system (8), while (8) is a stochastic statement of (1) within the probabilistic range [(1 \u2212 \u03b4)k, 1], suppose there exists an action at satisfying (18) in the following form: at(st) =aRL \u03b8k (st, at, st+1, rt) + \u00afa\u03c6k(st, k\u22121 \ufffd i=0 (st, aCBF i )) + aCBF t (st, aRL \u03b8k , \u00afa\u03c6k). (19) Then, the controller (19) guarantees the system (1) inside the safe region within the range of probability [(1 \u2212 \u03b4)n, 1]. Proof. Regarding the system (8), this result follows directly from Lemma 3 by solving the SOS program (18). The only different part of SOS program (18) from the SOS program (12) in Lemma 3 is the action ak here contains additional deterministic value aRL \u03b8k (st) and \u00afa\u03c6k(st). Since the output of (18) can regulate the dynamics of the model (8) in a safe region, while the approximated model (8) is a stochastic statement of the original system (1) with the probability greater or equal to (1 \u2212 \u03b4)k. Then, the solution of (18) can be regarded as a safe control to drive the system (1) far from dangerous, which ends the proof. We display an overview of the whole steps in the combination of the aforementioned factors over the learning process. The detailed work\ufb02ow is outlined in Algorithm 1 Algorithm 1: DDPG-SOSP Input: Origin system (1); barrier function h. Output: RL optimal policy \u03c0. 1 Preprocess (1) into (8). 2 Create the tuple \u02c6D to store {st, at, st+1, rt} in \u02c6D. 3 for k \u2208 {1, 2, . . . , k} do 4 Execute SOSP (18) to obtain the real-time CBF controller aCBF k and store it to train previous CBF controller \u00afaCBF \u03b8k . 5 Actuate the agent with at in (19). 6 Construct \u02c6D and update \u03b8k and \u03c6k directly. 7 return \u03c0. IV. NUMERICAL EXAMPLE A swing-up pendulum model from OpenAI Gym environment (Pendulum-v1) is used to verify the above theorems, ml2\u00a8\u03b8 = mgl sin(\u03b8) + a. (20) Let s1 = \u03b8, s2 = \u02d9\u03b8. The dynamics of (20) are de\ufb01ned as \ufffd \u02d9s1 \u02d9s2 \ufffd = \ufffd s2 \u2212 g l sin (s1) + a ml2 + d \ufffd , (21) where d denotes unknown dynamics that generate by inaccurate parameters \u00afm = 1.4, \u00afl = 1.4, as [23] introduced. When we try to construct a polynomial system (8), the sin(s1) in (21) will be approximated by Chebyshev interpolants within s1 \u2208 [\u22123, 3]. Then, with SOSOPT Toolbox [34], Mosek solver [35] and Tensor\ufb02ow, we implement Algorithm 1 in the pendulum model. Fig. 2. Comparison of the maximum absolute \u03b8 of different algorithms. The dotted solid line, the dashed line, and the solid line denote the performance of DDPG-ONLY, DDPG-QP and DDPG-SOSP, respectively The straight line denotes the safe boundary |s1| = 1. The related model parameters are given in Table 1. TABLE 1: Swing-up Pendulum Model Parameters Model Parameter Symbol Value Units Pendulum mass m 1.0 kg Gravity g 10.0 m/s2 Pendulum length l 1.0 m Input torque a [\u221215.0, 15.0] N Pendulum Angle \u03b8 [\u22121.00, 1.00] rad Angle velocity \u02d9\u03b8 [\u221260.0, 60.0] rad/s Angle acceleration \u00a8\u03b8 \u2014 rad/s2 We want to maintain the pendulum angle \u03b8 always in a safe range [\u22121.0, 1.0] during the learning process. Three RL algorithms are selected and compared in this example, including the DDPG-ONLY algorithm [5], CBF-based DDPG-QP algorithm [23] and CBF-based DDPG-SOSP algorithm (our work). The codes of CBF-based DDPG-SOSP can be found at the url: https://github.com/Wogwan/CCTA2022 SafeRL. All of these algorithms are trained in 150 episodes and each episode contains 200 steps. Each step is around 0.05s. And the reward function r = \u03b82 +0.1 \u02d9\u03b82 +0.001a2 is de\ufb01ned such that the RL agent are expected to keep the pendulum upright with minimal \u03b8, \u02d9\u03b8 and \u00a8\u03b8. Fig. 2 compares the collected maximum |\u03b8| at each episode by using different algorithms. As a baseline algorithm, DDPG-ONLY explored every state without any safety considerations, while DDPG-QP sometimes violated the safe state and DDPG-SOS completely kept in safe regions. The historical maximum value of | \u02d9\u03b8| of these algorithms is compared and shown in Fig. 3. It is found that DDPG-SOS is able to maintain the pendulum into a lower \u02d9\u03b8 easily than others over episodes. In Fig. 4, from the reward curve of these algorithms, it is easy to observe the ef\ufb01ciency of different algorithms: DDPGSOS obtains comparatively lower average reward and thus a better performance. Fig. 5 shows the \ufb01rst (2nd episode) and the \ufb01nal (150th episode) policy guided control performance between DDPGFig. 3. Comparison of the maximum absolute \u02d9\u03b8 of different algorithms. Fig. 4. Comparison of the accumulated reward of different algorithms. QP and DDPG-SOSP algorithm. We observed 50 steps of the pendulum angle to highlight the superiority of these two algorithms. Both in Fig. 5(a) and (b), the \ufb01nal policy can reach a smoother control output with less time. Although DDPG-SOSP takes a quicker and smoother action to stabilize the pendulum than DDPG-QP, but the time consuming will increase due to the dynamics regression and optimization computation under uncertainty. Accordingly, the result of DDPG-SOSP algorithm of model (20) is not only obviously stabler than DDPG-QP, but also maintaining the RL algorithm action\u2019s safety strictly. (a) (b) Fig. 5. The comparison of the 1st and the 150th policy performance of the angle control task between (a) DDPG-QP and (b) DDPG-SOS. ",
    "Conclusion": "CONCLUSION In this paper, we consider a novel approach to guarantee the safety of reinforcement learning (RL) with sum-ofsquares programming (SOSP). The objective is to \ufb01nd a safe RL method toward a partial unknown nonlinear system such that the RL agent\u2019s action is always safe. One of the typical RL algorithms, Deep Deterministic Policy Gradient (DDPG) algorithm, cooperates with control barrier function (CBF) in our work, where CBF is widely used to guarantee the dynamical safety in control theories. Therefore, we leverage the SOSP to compute an optimal controller based on CBF, and propose an algorithm to creatively embed this computation over DDPG. Our numerical example shows that SOSP in the inverted pendulum model obtains a better control performance than the quadratic program work. It also shows that the relaxations of SOSP can enable the agent to generate safe actions more permissively. For the future work, we will investigate how to incorporate SOSP with other types of RL including value-based RL and policy-based RL. Meanwhile, we check the possibility to solve a SOSP-based RL that works in higher dimensional cases. ",
    "References": "REFERENCES [1] R. S. Sutton, A. G. Barto, \u201dReinforcement learning: An introduction,\u201d MIT press, 2018. [2] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, \u201cBenchmarking deep reinforcement learning for continuous control,\u201d in Proc. Int. Conf. on Machine Learning, pp. 1329\u20131338, PMLR, 2016. [3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [4] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016. [5] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d ArXiv:1509.02971, 2015. [6] Li, Z., Cheng, X., Peng, X. B., Abbeel, P., Levine, S., Berseth, G., Sreenath, K., \u201dReinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots,\u201d in Proc. Int. Conf. on Robotics and Automation, pp. 2811\u20132817, 2021. [7] J. Peters, S. Schaal, \u201cReinforcement learning of motor skills with policy gradients,\u201d Neural Networks, vol. 21, no. 4, pp. 682\u2013697, 2008. [8] R. S. Sutton, et al. \u201dPolicy gradient methods for reinforcement learning with function approximation,\u201d Advances in Neural Information Processing Systems, 1999. [9] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller, \u201cDeterministic policy gradient algorithms,\u201d in Proc. Int. Conf. on Machine Learning, pp. 387\u2013395, PMLR, 2014. [10] J. Kober, J. A. Bagnell, J. Peters, \u201cReinforcement learning in robotics: A survey,\u201d Int. Journal of Robotics Research, vol. 32, no. 11, pp. 1238\u2013 1274, 2013. [11] P. Kormushev, S. Calinon, D. G. Caldwell, \u201cReinforcement learning in robotics: Applications and real-world challenges,\u201d Robotics, vol. 2, no. 3, pp. 122\u2013148, 2013. [12] J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel, J. Z. Kolter, D. Langer, O. Pink, V. Pratt, et al., \u201cTowards fully autonomous driving: Systems and algorithms,\u201d IEEE Intelligent Vehicles Symposium, pp. 163\u2013168, 2011. [13] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, P. P\u00b4erez, \u201cDeep reinforcement learning for autonomous driving: A survey,\u201d IEEE Trans. on Intelligent Transportation Systems, 2021. [14] L. Wang, D. Han, M. Egerstedt, \u201cPermissive barrier certi\ufb01cates for safe stabilization using sum-of-squares,\u201d in Proc. Amer. Control Conf., pp. 585\u2013590, 2018. [15] D. Han, D. Panagou, \u201cRobust multitask formation control via parametric Lyapunov-like barrier functions,\u201d IEEE Trans. on Automatic Control, vol. 64, no. 11, pp. 4439\u20134453, 2019. [16] G. Chesi, \u201dDomain of attraction: analysis and control via SOS programming\u201d, Springer Science & Business Media, vol. 415, 2011. [17] D. Han, G. Chesi, Y. S. Hung, \u201cRobust consensus for a class of uncertain multi-agent dynamical systems,\u201d IEEE Trans. on Industrial Informatics, vol. 9, no. 1, pp. 306\u2013312, 2012. [18] D. Han, G. Chesi, \u201cRobust synchronization via homogeneous parameter-dependent polynomial contraction matrix,\u201d IEEE Trans. on Circuits and Systems I: Regular Papers, vol. 61, no. 10, pp. 2931\u2013 2940, 2014. [19] D. Han, M. Althoff, \u201cOn estimating the robust domain of attraction for uncertain non-polynomial systems: An LMI approach,\u201d in Proc. Conf. on Decision and Control, pp. 2176\u20132183, 2016. [20] B. Felix, M. Turchetta, A. Schoellig, A. Krause, \u201dA Safe model-based reinforcement learning with stability guarantees,\u201d Advances in Neural Information Processing Systems, pp. 909\u2013919, 2018. [21] M. Alshiekh, R. Bloem, R. Ehlers, B. K\u00a8onighofer, S. Niekum, U. Topcu, \u201cSafe reinforcement learning via shielding,\u201d in AAAI Conf. on Arti\ufb01cial Intelligence, vol. 32, no. 1, 2018. [22] C. Steven, N. Jansen, S. Junges, U. Topcu, \u201dSafe Reinforcement Learning via Shielding for POMDPs,\u201d ArXiv:2204.00755, 2022. [23] R. Cheng, G. Orosz, R. M. Murray, J. W. Burdick, \u201cEnd-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks,\u201d in AAAI Conf. on Arti\ufb01cial Intelligence, vol. 33, no. 1, 2019. [24] R. Cheng, A. Verma, G. Orosz, S. Chaudhuri, Y. Yue, J. W. Burdick, \u201dControl Regularization for Reduced Variance Reinforcement Learning,\u201d in Proc. Int. Conf. on Machine Learning. pp. 1141\u20131150, 2019. [25] S. C. Hsu, X. Xu, A. D. Ames, \u201cControl barrier function based quadratic programs with application to bipedal robotic walking,\u201d in Proc. Amer. Control Conf., pp. 4542\u20134548, 2015. [26] T. Akshay, Z. Jun, K. Sreenath. \u201dSafety-Critical Control and Planning for Obstacle Avoidance between Polytopes with Control Barrier Functions,\u201d in Proc. Int. Conf. on Robotics And Automation, accepted, 2022. [27] L. Wang, E. A. Theodorou, M. Egerstedt, \u201cSafe learning of quadrotor dynamics using barrier certi\ufb01cates,\u201d in Proc. Int. Conf. on Robotics And Automation, pp. 2460\u20132465, 2018. [28] P. Martin, \u201dMarkov decision processes: discrete stochastic dynamic programming\u201d, John Wiley & Sons, 2014. [29] A. Lederer, J. Umlauft, S. Hirche, \u201cUniform error bounds for Gaussian process regression with application to safe control,\u201d Advances in Neural Information Processing Systems, pp. 659\u2013669, 2019. [30] H. Huang, D. Han, \u201dOn Estimating the Probabilistic Region of Attraction for Partially Unknown Nonlinear Systems: An Sum-ofSquares Approach,\u201d in Proc. Chinese Control and Decision Conf., accepted, 2022. [31] D. Han, H. Huang, \u201dSum-of-Squares Program and Safe Learning on Maximizing the Region of Attraction of Partially Unknown Systems,\u201d in Proc. Asian Control Conf., 2022. [32] L. N. Trefethen, \u201dApproximation Theory and Approximation Practice,\u201d SIAM, 2019. [33] M. Putinar, \u201cPositive Polynomials on Compact Semi-algebraic Sets,\u201d Indiana University Mathematics Journal, vol. 42, no. 3, pp. 969\u2013984, 1993. [34] P. Seiler, \u201dSOSOPT: A toolbox for polynomial optimization,\u201d ArXiv:1308.1889, 2013. [35] M. ApS, \u201dMosek optimization toolbox for Matlab,\u201d User\u2019s Guide And Reference Manual, Ver. 4, 2019. ",
    "title": "Barrier Certi\ufb01ed Safety Learning Control:",
    "paper_info": "Barrier Certi\ufb01ed Safety Learning Control:\nWhen Sum-of-Square Programming Meets Reinforcement Learning\nHejun Huang1, Zhenglong Li2, Dongkun Han1\u2217\nAbstract\u2014 Safety guarantee is essential in many engineering\nimplementations. Reinforcement learning provides a useful\nway to strengthen safety. However, reinforcement learning\nalgorithms cannot completely guarantee safety over realistic op-\nerations. To address this issue, this work adopts control barrier\nfunctions over reinforcement learning, and proposes a compen-\nsated algorithm to completely maintain safety. Speci\ufb01cally, a\nsum-of-squares programming has been exploited to search for\nthe optimal controller, and tune the learning hyperparameters\nsimultaneously. Thus, the control actions are pledged to be\nalways within the safe region. The effectiveness of proposed\nmethod is demonstrated via an inverted pendulum model.\nCompared to quadratic programming based reinforcement\nlearning methods, our sum-of-squares programming based\nreinforcement learning has shown its superiority.\nI. INTRODUCTION\nReinforcement learning (RL) is one of the most popular\nmethods for accomplishing the long-term objective of a\nMarkov-decision process [1]\u2013[6]. This learning method aims\nfor an optimal policy that can maximize a long-term reward\nin a given environment recursively. For the case of RL\npolicy gradient method [7]\u2013[9], this reward-related learning\nproblem is traditionally addressed using gradient descent to\nobtain superior control policies.\nRL algorithms might generate desired results in some\nacademic trials, e.g., robotic experiments [10], [11] and\nautonomous driving contexts [12], [13]. But for realistic\nsituations, how to completely guarantee the safety based on\nthese RL policies is still a problem. The intermediate policies\nfrom the RL agent\u2019s exploration and exploitation processes\nmay sometimes ruin the environment or the agent itself,\nwhich is unacceptable for safety-critical systems. Moreover,\ndisturbances around the system would confuse the agent over\nlearning process, which make the control tasks more complex\nand challenging.\nTo cope with the issues above, various methods are\nproposed to specify dynamical safety during the learning\nprocess. Initially, the region of attraction (ROA) from control\nLyapunov functions or Lyapunov-like function work [14],\n[15] are used to certify the dynamical safety. Related and\nuseful work of ROA estimation are introduced in [16]\u2013[19].\nIn 2017, [20] started to execute the RL\u2019s exploration and\nexploitation in ROA to maintain safety. Different from region\nconstraint, in 2018 [21], [22] proposed a shield framework\nto select safe actions repeatedly. Inspired by this work, [23],\n1Hejun Huang and Dongkun Han are with the Department of Mechanical\nand Automation Engineering, The Chinese University of Hong Kong\n2Zhenglong Li is with the Department of Electrical and Electronic\nEngineering, The University of Hong Kong\n*Email: dkhan@mae.cuhk.edu.hk\n[24] further combined the control barrier function (CBF) to\nsynthesis the RL-based controller. Note that, the CBF is not\nnew to identify safe regions in control theory, [25], [26] are\nrepresentative work to combine CBF in biped robot and [27]\nexplored its implementation of quadrotors, etc. Appropriate\nregion constraint for RL is important for building safe RL.\nHence, a reliable learning method is expected to construct a\ncontroller inside the safe region over the process.\nIn this paper, we will consider a state-space model in the\npolynomial form, including an unknown term that estimated\nby Gaussian process. Then, we will apply Deep Deterministic\nPolicy Gradient (DDPG) algorithm to design a controller.\nThe generated control action will impact system dynam-\nics and further in\ufb02uence the sum-of-squares programming\n(SOSP) solution of control barrier functions.\nThe main contributions of this work are: (1) Formulate\nthe framework to embed SOSP-based CBF into DDPG algo-\nrithm; (2) compare the performance of quadratic program-\nming (QP)-based and SOSP-based controller with DDPG\nalgorithm; and (3) demonstrate the safety and ef\ufb01ciency of\nDDPG-SOSP algorithm via an inverted pendulum model.\nThis paper is organized as follows, we \ufb01rst present some\npreliminaries about reinforcement learning, Gaussian pro-\ncess and control barrier function in Section II. Then, in\nSection III, we formulate the steps to compute an SOSP-\nbased controller and introduce a corresponding DDPG-SOSP\nalgorithm. Numerical examples are given for these cases in\nSection IV, then we discuss the learning ef\ufb01ciency and safety\nperformance of QP-based and SOSP-based strategies, before\nconcluding this paper.\nII. PRELIMINARY\nA discrete-time nonlinear control-af\ufb01ne system with state\nst, st+1 \u2208 S and control action at \u2208 A is considered as:\nst+1 = f(st) + g(st)at + d(st), \u2200t \u2208 [t, t],\n(1)\nwhere st, st+1 are \ufb01nished within a bounded time t, while t\nand t are constants.\nNotation: From the in\ufb01nite-horizon discounted Markov\ndecision process (MDP) theory [28], let r : S \u00d7 A \u2192 R de-\nnote the reward of the current state-action pair (st, at) in (1),\nlet P(st, st+1, at) = p(st+1|st, at) denote the probability\ndistribution to next state st+1, and let \u03b3 \u2208 (0, 1) denote a dis-\ncount factor. Thus, a MDP tuple \u03c4 = (st, st+1, at, rt, Pt, \u03b3)\nestablishes at each state t.\narXiv:2206.07915v2  [eess.SY]  29 Jun 2022\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n- (1): The significance of this piece of work is to propose a novel approach to ensure the safety of reinforcement learning, which is an essential issue for realistic applications. \n- (2): Innovation point: The proposed approach combines control barrier functions with reinforcement learning and uses sum-of-squares programming to ensure the control actions are always safe during the learning process. Performance: The effectiveness of the proposed method is demonstrated via an inverted pendulum model and has shown superior performance compared to previous quadratic programming based reinforcement learning methods. Workload: The workload of the research methodology proposed in this article involves adopting control barrier functions and using sum-of-squares programming to find the optimal controller that ensures the learning process always maintains its safety, and tuning the learning hyperparameters. Overall, this article provides a significant contribution to the reinforcement learning community and offers practical implications for the implementation of safe reinforcement learning in real-world applications.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "\n"
}