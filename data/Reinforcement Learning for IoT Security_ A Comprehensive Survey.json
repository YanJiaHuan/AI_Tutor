{
    "Abstract": "Abstract\u2014The number of connected smart devices has been increasing exponentially for different Internet-of-Things (IoT) applications. Security has been a long run challenge in the IoT systems which has many attack vectors, security \ufb02aws and vulnerabilities. Securing billions of connected devices in IoT is a must task to realize the full potential of IoT applications. Recently, researchers have proposed many security solutions for IoT. Machine learning has been proposed as one of the emerging solutions for IoT security and Reinforcement learning is gaining more popularity for securing IoT systems. Reinforcement learning, unlike other machine learning techniques, can learn the environment by having minimum information about the parameters to be learned. It solves the optimization problem by interacting with the environment adapting the parameters on the \ufb02y. In this paper, we present an comprehensive survey of different types of cyber-attacks against different IoT systems and then we present reinforcement learning and deep reinforcement learning based security solutions to combat those different types of attacks in different IoT systems. Furthermore, we present the Reinforcement learning for securing CPS systems (i.e., IoT with feedback and control) such as smart grid and smart transportation system. The recent important attacks and countermeasures using reinforcement learning in IoT are also summarized in the form of tables. With this paper, readers can have a more thorough understanding of IoT security attacks and countermeasures using Reinforcement Learning, as well as research trends in this area. felix2020sur Index Terms\u2014Reinforcement Learning, IoT, Security I. ",
    "Introduction": "INTRODUCTION Internet of Things (IoT) connects the physical world to the digital world. It is a revolutionary technology in which machines talk to other machines to solve trivial to complex tasks [1\u20134]. Sensors and actuators are the resources from which data is exchanged between the physical world and the digital world. The sensors collect data that are to be stored and processed to provide service to the user. It has brought a drastic change in the lifestyle of humans by bringing smartness to the devices and will eventually increase the quality of human life. IoT has tremendously increased the use of the internet by bringing all the physical devices together in the network. Any Manuscript received Day Month Year. Authors are with the Department of Electrical Engineering and Computer Science at Howard University, Washington, DC 20059, USA. E-mail: db.rawat@ieee.org. This work was supported in part by the US NSF under grants CNS/SaTC 2039583, CNS 1650831 and 1828811, by the DoD Center of Excellence in AI and Machine Learning (CoE-AIML) at Howard University under Contract Number W911NF-20-2-0277 with the US Army Research Laboratory, the DoE\u2019s National Nuclear Security Administration (NNSA) Award # DENA0003946. and the US Department of Homeland Security (DHS) under grant award number, 2017-ST-062-000003. However, any opinion, \ufb01nding, and conclusions or recommendations expressed in this material are those of the author and do not necessarily re\ufb02ect the views of these funding agencies. physical device brought to internet connection that can interact with human can be an IoT device. For example, when cars are connected to each other through the internet and communicate with each other, this is called internet of cars. IoT collects and processes human day to day data and brings automation to the task. With all the easiness provided by IoT, there also exist some pitfalls in using IoT. The major challenge is securing the system from attackers, maintaining the privacy of the user of IoT and making sure that certain IoT devices can be trusted. More the number of connected devices, more is the chance of the vulnerabilities to attack. Security in IoT operation is the major challenge to be faced by IoT designers. The dynamic environment of IoT and runtime communication adds additional security requirements on the IoT design. IoT brings \ufb02exibility and intelligence to the devices providing us usability but at the same time, it is also fearsome to use it. IoT is gaining a status for insecurity. Researchers divulge the dangerous \ufb02aws in IoT which poses a major challenge in IoT success [2, 5, 6]. We are sharing our every personal information through IoT devices and it is very important that our data are con\ufb01dential. Reinforcement learning is a machine learning approach in which the agent interacts with the environment and tries to maximize the numerical reward [7]. Human brain interacts with the outer environment and uses that interaction to understand and sustain in that environment [8]. Reinforcement Learning uses the human brain and sensory processing system [9] as an analogy to learning the environment. It is a process in which an agent has to explore all the system to understand it. Considering the time it takes to converge and get an optimal policy, it is not feasible in many scenarios. Traditional RL suffers a curse of dimensionality. As the environment becomes complex, there is exponential growth in the parameters to be learned by RL agent [10]. As a solution, we have deep reinforcement learning (DRL) which is a combination of deep network and reinforcement learning (RL) [11]. RL has been applied in securing IoT technology in various domains which is the main scope of this paper. IoT is a highly mobile technology and is very vulnerable to many cyber attacks. The sensors and actuators are one point of attack. Network for communication is again another major point of attacks in IoT. Much research work has been done to provide security to IoT system using RL technology. The main scope of this paper is to provide a literature review of research done on securing IoT devices using RL. Along with this, we also provide a background of reinforcement learning. The paper is organized as follows. In Section II, we compare reinforcement learning with other machine learning arXiv:2102.07247v1  [cs.LG]  14 Feb 2021 2 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 techniques. Further, we discuss why RL is suitable for IoT scenarios. Section III is about research works related to RL on securing IoT from several threats. We present the application of RL in speci\ufb01c CPS system in Section IV. At the end some discussion and future research challenge is presented. Table I is the list of abbreviations we used throughout the paper. II. BRIEF OVERVIEW OF REINFORCEMENT LEARNING In this section, we brie\ufb02y introduce reinforcement learning and talk about deep reinforcement learning. The comparison of RL and other machine learning techniques is presented. In the end, we answer why the use of RL is effective in the IoT scenario. A. Reinforcement Learning Reinforcement learning is a kind of machine learning in which AI agent aims to accomplish a task by taking the best next step which can give them overall higher \ufb01nal reward, as shown in Fig. 1. In RL setting, the agent goes through many trial and error steps and tries to maximize the reward it gets from the environment [7]. An agent interacts with an environment, which can be a simulator, a game, the real world etc. Each time step the agent observes the state st from the environment, selects an action at, and then receives a reward rt and the environment changes to st+1. Therefore, each time step the agent gathers experiences (st, at, rt, st+1) from which it can learn. If the action taken was favorable for the given environment, it will get a positive reward. If not it gets a negative reward. The agent continues to collect the reward aiming to maximize expected return from each state [12]. Reinforcement learning is a Markov Decision Process (MDP) in which the output of taking an action from a state depends only on the present state irrespective of past states and actions. MDP is a tuple consisting of \ufb01ve elements as (S,A,P,R,\u03b3). It uses discount factor \u03b3 which is a scalar value between 0 and 1. The discount factor is considered to maximize the future rewards that the agent gets from the environment. Value function in RL is a mapping from states to real numbers, where the value of a state represents the long-term reward achieved starting from that state and executing a particular policy. Value function v(s) is a representation of how good it is for an agent to be in the state s. Bellman equation[13] is the foundation mathematics behind reinforcement learning. v(s) = E[Rt+1 + \u03b3v(St+1)|St = s] In the given Bellman equation, the value function is decomposed as an immediate reward plus the value at the next successor state with discount factor(\u03b3). B. Deep Reinforcement Learning Deep Reinforcement Learning (DRL) is a combination of deep learning and RL. DRL is revolutionary research in RL which is capable to solve complex computational tasks [14]. For the complex environment, an approximation of value function and policy gradient is a complex task. For this, deep network is used to approximate these values. Consider Figure 1. Agent-Environment Interaction in Reinforcement Learning. the set of actions taken by agents that results in a positive reward. In this case, a normal gradient is used to increase the probability of again taking these sets of actions. The deep network adds intelligence to RL agents and hence it accelerates the agent\u2019s capability to optimize the policy. RL is the only machine learning technique that can learn without any dataset. However, as the agent interacts with the environment, it generates the dataset. These datasets are used to train the deep network in DRL. Researchers have proposed many DRL approaches with its application ranging from control [15], resource management [16, 17], robotics [18, 19] and many more. In 2015, Google DeepMind introduced deep Q-network (DQN) [20], delivering results exceeding human in playing Atari games. Deep neural network was used in DQN as the function approximator. In Go games, AlphaGo [21] and AlphaGo Zero [22] also showed an excellent result. Following that, DeepMind team made additional improvements based on DQN which builds a target DQN which calculates the maximum Q value and they named it Double DQN [23]. Dueling DQN [24] is another signi\ufb01cant development. In situations with the exponentially vast environment and continuous action space, DDPG [22] was proposed which uses the actor-critic method. Other approaches are still the center research topic worldwide. C. Comparison of RL with other Machine Learning Machine learning can be classi\ufb01ed as Supervised, Unsupervised and Reinforcement Learning. In supervised learning, the ML model tries to predict the dependencies between training data and actual answer about a problem asked about that data [25]. Basically, the input is given and we know what the model should predict in this kind of learning. It learns based on example. While reinforcement learning is about learning the environment without example. RL is more human like learning approach in which learning does not require large data. Here the agent do not know the target labels. Unsupervised Learning uses unlabeled data to understand the pattern. On the other hand, RL learn through interaction with environment without any prior data. D. Why Reinforcement Learning in IoT IoT connects millions of devices over the network. IoT devices are extremely dynamic and they make a complex 3 Table I ABBREVIATION TABLE Abbreviation De\ufb01nition RL Reinforcement Learning DRL Deep Reinforcement Learning IoT Internet of Things CNN Convolutional Neural Network MDP Markov Decision Process CPS Cyber Physical System DoS Denial of Service DDoS Distributed Denial of Service DQN Deep Q-Network ML Machine Learning SINR Signal-to-inference-plusnoise ratio SDN Software De\ufb01ned Network CRN Cognitive Radio Network WACR Wideband Autonomous CR VANET Vehicular Ad-Hoc Network UAV Unmanned Aerial Vehicle POMDP Partially Observable MDP ICMP Internet Control Message Protocol network [26]. Supervised and unsupervised learning technique have been used in security for intrusion detection [27\u201332], detection of malware [30, 33\u201335], CPS attack detection[36] [37] and also in privacy maintenance task of IoT [38]. However, these techniques can not perform dynamic responses for security in IoT environment [39]. For example for any new and constantly evolving cyber attacks, supervised and unsupervised learning method \ufb01rst need to get the dataset of those attacks and then only \ufb01nd a solution by learning the data. Reinforcement learning is applicable in IoT environment for many reasons. The real-time dynamic environment can be monitored ef\ufb01ciently in a favorable way. RL can continuously learn new information to accommodate to different advanced settings [40, 41]. Some IoT environments are so complex that it is dif\ufb01cult to model it. RL minimizes the effort associated with simulating and solving such complex environment. Consider a complex IoT scenario and we have to come up with a model that can solve a problem in that environment. For using supervised and unsupervised method, \ufb01rst simulation is to be performed to generate dataset and then only dataset is used to train the model. However, reinforcement learning algorithm performs trial and error in the environment and learns a model. This minimizes the complexity involved in simulating and solving a problem in a complex environment. Data collection for some IoT environment is extremely dif\ufb01cult. In such a scenario, there are no datasets to train the model using other machine learning techniques. RL is the only machine learning technique that can learn without prior datasets. E. Reinforcement Learning for Securing IoT Against Adversarial Learning environment Reinforcement learning is regarded as one of the best solutions for securing IoT against adversarial learning environment that incorporates the environment\u2019s behavior into the learning process concurrently [42]. This salient feature of Reinforcement Learning offers IoT security against adversarial learning environment where large number of diverse IoT devices produce huge amount of bursty data or continuous data stream. III. THREATS AND RL BASED SOLUTIONS IN IOT SECURITY The rapid development of smart and mobile devices has made signi\ufb01cant growth in IoT usage in many areas. Nowadays, IoT is incorporated in many domains. Industrial, power, agriculture, vehicles, battle\ufb01eld, homes [43] are the common application domain of IoT. However, IoT is facing security problems with growth in its usage. Privacy and security maintenance is crucial for IoT systems. IoT uses advanced technologies like radio-frequency identi\ufb01cations (RFIDs), wireless sensor networks, Bluetooth, Zigbee, and cloud computing. Privacy protection and securing the system from cyber attacks like DoS attacks, jamming, eavesdropping, malware, and virus injection [44] is a most and at the same time a very challenging task. Privacy leakage is another challenge for security in IoT [45]. For instance, devices that collect and report the actions of elderly people in a smart old care home must have to avoid private information leakage to prevent any harm to elderly people from attackers. IoT system is susceptible to attacks like network, software and physical attack. In this paper, we mainly look at the following IoT threats. A. Denial of Service Attack IoT systems face cyber-attack like Denial of Service (DoS) attack resulting in selective forwarding and eavesdropping [46]. In IoT system attacks, DoS is the most common attack [47]. These attacks on IoT networks place serious threats to human life and direct or indirect \ufb01nancial losses. A Denial-of-Service attack is a serious and most prevalent attack in which the attacker modi\ufb01es the connection of network in such a way that it becomes unavailable to its expected users. DoS attack is achieved by \ufb02ooding the communication network with unnecessary traf\ufb01c. A denial of service attack disables the service in the victim\u2019s side by sending notably huge sizes of packets. The attack traf\ufb01c can use the large portion of available bandwidth resulting in the services not reachable to legitimate users. Another critical challenge for IoT security in the current scenario is protecting the system against distributed denial of service(DDoS) attack. DDoS is typically a DoS attack but is of distributed nature. This results 4 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 in a compromise of a tremendous number of IoT devices at a time. In 2016, a DDoS attack performed by Mirai botnet [48] had affected around 65,000 IoT devices just within the early 20 hours [49]. DoS attack obstructs the usage for the genuine user resulting in the unavailability of network resources. DDoS is the same kind of attack but the only difference is it is drilled from distributed sources. IoT devices have limited power capacity to leverage mechanisms to detect these denial attacks. Network entrance for IoT can be the place to apply detection and protection mechanism from such attacks. 1) IoT layers: IoT architecture is mainly 3-layered [50]. They are perception layer, network layer and application layer, as shown in Fig. 2. Figure 2. Three Layered IoT Architecture. \u2022 Perception Layer: The perception layer is all about sensing the physical characteristics of objects using sensors, actuators and other devices. The process of this perception is reliant on sensing technologies like RFID, GPS, 2-D barcode labels and readers [51]. This layer also takes control of converting sensed information to digital signals. Chips to sense are to be designed and made as small as possible to implant it inside the tiny IoT devices. The main task of this layer is to gather the information by sensing the objects. \u2022 Network Layer: The network layer can be visualized as the neural network or brain of IoT. This layer is accountable for processing the information gathered from the Perception layer [52]. Also, it is responsible for transmitting the information to the application layer using wired/wireless networking technologies. Technologies like Wi\ufb01, Bluetooth, Zigbee, WirelessHART, Ethernet, 3G and so on are used to transmit the information. Because IoT sensors collect a massive amount of data, it is necessary to have a middleware that can handle this huge amount of data. For this, cloud computing is the main technology used in this layer. \u2022 Application Layer: Application Layer is the topmost layer in IoT architecture which is the frontend of the IoT architecture. This layer realizes the application of the overall IoT system. It supports by providing the demanded tools for developers to practicalize IoT vision. The application layer uses the data transmitted to them from previous IoT layers. Automatic sensing device management and node management are handled by this layer [53]. 2) DoS Attack in IoT layers: \u2022 DoS in Perception Layer: RFID is the main sensing technology used in the perception layer. Several attacks like Jamming[54], Kill Command Attack[55], Desynchronizing attack[56] are common in this layer. \u2022 DoS in Network Layer: Attackers perform \ufb02ooding attacks like ICMP \ufb02ood attack, Ampli\ufb01cation based \ufb02ooding, Re\ufb02ection based \ufb02ooding and many more[57]. For instance, Wi\ufb01, which is the major technology in this layer suffers ICMP \ufb02ooding attack. \u2022 DoS in Application Layer: A common attack in the Application layer is Path based DoS attack[58], Reprogramming attacks and so on. 3) Reinforcement Learning against IoT DoS attack: The authors in[59] proposed an approach to protect against DDoS attacks by using a Multiagent Router Throttling. They proposed a model where multiple reinforcement learning agents are involved. These agents are installed on routers. The agents learn to rate-limit or throttle traf\ufb01c towards a victim server. It has been illustrated to work \ufb01ne against DDoS attacks in small-scale network topologies. But this method suffered from scalability problems. To eliminate this issue, they proposed Coordinated Team Learning design on their multi-agent router throttling method [60]. This paper is centered on resolving the scalability issues as mentioned earlier. Here they have proposed an approach that combines mechanisms like hierarchical team-based communication, task decomposition, and team rewards to minimize the DDoS attack traf\ufb01c. They referenced a network model as used by authors in [61] to develop emulator for throttling approaches. By using up to 100 reinforcement learning agents, the scalability of the proposed approach is evaluated. This method is applicable in highly scalable IoT environment. Simulation results showed that the adaptability of the proposed model is highly improved. Rl agents throttles the attacker traf\ufb01c from \ufb02ooding the server. Server is an important component in IoT mechanism. This approach minimizes the DoS attacks in the server. Software De\ufb01ned Network (SDN) is a well known architecture for controlling large network space. SDN allows network administrators to have more control of the network and facilitate the ef\ufb01cient use of network resources [62]. SDN supports the separation of data plane and control plane in switches and routers [63]. The combination of IoT and SDN, commonly known as software de\ufb01ned internet of things, has a potential solution to managing IoT network traf\ufb01c.The work in [64] tried to mitigate DDoS attack using DDPG method which is more scalable than the work proposed in [60]. In this approach, the DRL agents are placed in the central Software De\ufb01ned Network (SDN) instead of distributed router locations. The DRL agent proposed here takes control of the traf\ufb01c that reaches the server and prevents over \ufb02ooding of traf\ufb01c in the server. The mitigating agent is trained using DDPG algorithm and its state space are features of each port of switch and \ufb02ow statistics. The authors have taken eight features in this paper. Action taken by an agent is throttling of traf\ufb01c based on the maximum bandwidth allowed for a speci\ufb01c host. The DDoS attack mitigating agent gets a negative reward if it overloads 5 the server with massive traf\ufb01c. Also, it gets a reward based on the percentage of benign traf\ufb01c and attack traf\ufb01c reaching the server. The agent learns continuously and can take control of the traf\ufb01c \ufb02owing to the server. Hence, it achieves the goal of mitigating DDoS attack on the server. The proposed agent can mitigate DDoS \ufb02ooding attacks of different protocols such as TCP SYN, UDP and ICMP. B. Jamming Attack Jamming is an attack in which an attacker contaminates the original content of information by assigning interruption signals in the network or by barring the original content of the information [65]. This results in the original content not reachable to the desired destination. Jamming is similar to a DoS attack. In wireless networks, the Jamming attack is achieved by decreasing the signal-to-noise ratio at the receiver side. This is achieved by passing interfering wireless signals to the network. The jamming attack can hinder the transmission of information between sender and receiver. Jammers use intentional radio interference to create disturbance in the network. This keeps the communicating medium busy not allowing the transmitter to transfer messages. The jamming attack can be proactive and reactive [66]. In proactive jamming, jammers send the interference signal all the time without taking care of whether there is communication going on in the network. On the other hand, reactive jammers only attack when they sense communication in the network. Intelligent technologies like RL can be the potential research solution to jamming attacks in such IoT networks. 1) Jamming Attack in IoT: IoT is the large scale interconnected system that is vulnerable to numerous attacks due to its large attack surfaces. People are dependent on IoT devices more than ever and any attack on this system is serious to human life in some way. The jamming attack is another serious attack in IoT that can severely disrupt the normal working of the IoT system. Jamming is one of the most dangerous attacks that can interfere in wireless communication channels in the network by injecting false packets and interrupting the radio communication frequencies. Considering this, the jamming attack is a major challenge and threat to IoT networks having nodes with con\ufb01ned energy and power [67]. Reactive jamming is a challenging attack faced by IoT networks compared to another jamming. Reactive jamming consumes the energy of low power devices unnecessarily. Thus, IoT devices being low power are mostly affected by this kind of attack. There are many antijamming techniques proposed for general wireless network [68\u201371]. The anti-jamming solution proposed for this traditional network is not applicable in the IoT network. The reasons are IoT network is highly dynamic, heterogeneous and more demanding. Also, IoT has limited memory, power, and transmission resources [72]. More robust technologies like machine learning can be an effective antijamming solutions in IoT environment. 2) Reinforcement Learning against IoT Jamming attack: IoT technology can perform well only if the communication of information is secure and ef\ufb01cient. So, the demand of wireless medium to support IoT functioning is high. It is challenging to properly assure the management and availability of spectrum resources. Unavailability of spectrum resources can impose a challenge to the sustaining of IoT technology. Cognitive Radio Network (CRN) in IoT somehow manages the spectrum utilization process. But the jamming attack is a serious security threat faced in CNR based IoT devices. Due to limited powered devices, wireless based IoT systems are more suffered by the jamming attack. Several anti-jamming algorithms has been proposed [71, 73\u201379]. In our paper, we brie\ufb02y discuss the anti-jamming technique implemented using RL. The work in [80] proposed a deep reinforcement learning based power control scheme for IoT transmission against jamming. Convolution Neural Network is used as a deep learning algorithm. The DQN-based power control scheme is implemented over the universal software radio peripherals (USRPs). Depending on the present IoT transmission status and strength of the jammer, the agents determine the transmit power unaware of the IoT topology. This approach showed enhanced signal-to-interference-plus-noise (SINR) of the IoT signals compared with anti-jamming using Q-learning. They have used DQN as an RL algorithm. Agent is the transmitter whose action is to choose and set the transmit power. On taking action, the SINR at a time slot is measured and calculated at the end. The authors in [81] have proposed a two dimensional antijamming communication using DRL. CRN is the network model used that has multiple Primary Users (PU) and jammers and a single Secondary User (SU). In this scheme, SU, without interfering with PUs, utilizes both spread spectrum and user mobility to perceive jamming attacks. The authors proposed DQN based scheme to suggest the SU to take one of the two possible actions. First is to leave an area of heavy jamming and reconnect to another base station. Second, use one of the channel to send signals (frequency hopping) to beat the smart jammers. SU obtains an optimal anti-jamming communication policy by using DQN algorithm without having information about the jamming model and radio channel model. They used Convolution Neural Network to accelerate the learning rate. SINR and utility of the SU against cooperative jamming are improved compared to other learning approaches. SU is the agent that choose action based on the system state. State space includes the availability of the number of Primary Users and the discrete SINR value of the SU at that time slot. DQN approach followed in this work converged faster than Q-learning approach. The proposed DQN teaches the SU to choose optimal frequency hopping policy and hence mitigate a jamming attack. The CRN model has many application in IoT [82]. Thus the proposed RL based anti-jamming technique can be applicable in antijamming in CRN based IoT devices. Alternatively, the authors in [83] have proposed a model in which the receiver of SUs can decide to stay or leave the current location to combat jamming attacks. This mobility can cause some overhead, so it should \ufb01nd an optimal policy either to stay at the current location or move. Here, DQN based on 6 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 CNN have been used by the receiver to choose the action that maximizes its utility. RL state space is the discrete measure of SINR of the signal sensed by the receiver at that time slot. Action by the receiver is whether to leave the location or stay there. They concluded that the proposed method achieved faster convergence and higher SINR as compared to Q-learning approach. Both the works in [81] and [83] took account of the discrete SINR value as RL state. But in a scenario of in\ufb01nitely large SINR, these approach is not suitable. Also, the SINR considered in these approaches may be noisy and false. To address this issue, a Recursive Convolutional Neural Network (RCNN) that handles in\ufb01nite state problem was proposed by authors in [84]. An optimal anti-jamming strategy was achieved by the proposed DRL model. The proposed algorithm improved the anti-jamming strategies against dynamic and intelligent jammers. Spectrum waterfall is de\ufb01ned as a state space of the RL environment. Spectrum waterfall utilizes the spectrum information with temporal features. It does not require jamming pattern information so it is applicable against smart jammers who continuously change their jamming pattern. The preprocessing layer in RCNN can remove excess noise from the environment and hence reduce complexity. It \ufb01lters out the SINR with the help of a noise threshold. And recursive convolution layer handles the recursive input state. The simulation result validates the proposed algorithm by showing that the user can avoid jamming even if the jammers change jamming pattern intelligently. The proposed algorithm of DQL with RCNN shows faster convergence that Q-learning against \ufb01xed jamming attacks. The proposed method converged in the presence of dynamic jammers but Qlearning could not converge in this case. However, the work in [85] provided a theoretical proof of a condition in which the method proposed by [84] cannot converge. Here the authors raise a question against the previous DRL-based anti-jamming strategy. When the jammer is intelligent enough that can learn the communication pattern of the user and modify its jamming pattern accordingly, the previous model fails to converge. Here they design an RL agent against DRL anti-jamming. RL agent observes the frequency spectrum and based on that it chooses the frequency band to jam. They have opened a research challenge against intelligent jamming attacks. Wideband autonomous cognitive radio (WACR) based antijamming using RL was proposed by authors in [86]. WACR makes the use of its spectrum sensing ability to locate sweeping jammers. WACR not only senses the active signal but can also classify the signal properties which aids in \ufb01nding such signals [87]. They de\ufb01ne three steps wideband knowledge spectrum acquisition framework. They are wideband spectrum scanning, spectral activity detection, and signal classi\ufb01cation. A reinforcement learning based decision policy is proposed in which a WACR learn an optimal policy to pick the subbands for sensing and transmission. The selection of sub-band is based on the desired contiguous length of idle bandwidth for a sub-band. For the sensed sub-band, Neyman-Pearson (NP) detector is used which allows the WACR to \ufb01nd the frequencies of all active signals in that sub-band. In this Qlearning based RL setting, the action of WACR is either to remain in a sub-band or to switch to other sub-band. WACR on taking each action updates its Q-table on the basis of reward it gets from the environment. Reward, in this case, is dependent on the amount of time WACR can avoid the jammer. Experimental results from the simulation showed that the Q-learning can learn the sweeping jammer pattern and can optimally switch the sub-bands to avoid jamming. A similar Q-learning approach is proposed for the WACR network in the work [88]. The only difference is that the later one uses a multi-agent Rl approach. They considered multiple WACRs and proposed a similar Q-learning approach to achieve antijamming against sweeping jammers and interference from other WACRs. When multiple WACRs are operating in the same spectrum range and there is sweeper jamming, the proposed multi-agent approach avoids sweeping jammers and interference from other WACR. However, both [86] and [88] assumed \ufb01xed jammer. Both did not cover a scenario in which sweeping jammers can also be cognitive and smart enough to adjust its jamming strategy accordingly. C. Spoo\ufb01ng Attack A spoo\ufb01ng attack is a case in which a malicious node impersonates to be another person or device over the network. The main aim of this attack is to get trust from nodes and access the legitimate node to steal information or spread malware [89]. Spoo\ufb01ng attackers trick the user or a node to believe that they are trustworthy and falsely access the information. A spoo\ufb01ng attack is of different types and we will discuss some of them in brief. IP spoo\ufb01ng is done by impersonating the IP address, sending information through that address and trick the receiver to believe that information [90]. ARP spoo\ufb01ng is about sending the falsi\ufb01ed ARP messages in the network [91]. The target of this attack is to falsify the victim node to send the information to a malicious node instead of sending it to a legitimate one. Other spoo\ufb01ng attacks like DNS spoo\ufb01ng, web spoo\ufb01ng, email spoo\ufb01ng, etc are common. 1) Spoo\ufb01ng Attack in IoT: IoT devices are interconnected and they share the information which is privacy critical. The taxonomy of security attacks in the work [92] showed different cyber-attacks in IoT. Spoo\ufb01ng attack is a serious attack which may even lead to DDoS attack and Man-in-the-Middle attack. Let us look at a typical example of how spoo\ufb01ng attacks can disrupt IoT setting. Suppose an IoT scenario of connected multiple UAVs that are deployed in monitoring and controlling battle\ufb01eld information. Spoo\ufb01ng attackers can be any unknown UAV that tries to join the network. On gaining trust from the network, the attacker can fake themselves to be genuine however it is malicious. The attacker UAV on joining the network can sense all the critical information of the battle\ufb01eld. It can also pass false information in the network which will cause a serious disruption of the battle\ufb01eld. 2) Reinforcement Learning against IoT Spoo\ufb01ng Attack: Reinforcement learning is like a game in which the agent plays with the opponent and learn the strategy followed by the opponent. In case of IoT communication, physical layer information like received signal strength, channel state 7 information and channel impulse response can be useful in authenticating the transmitter [93]. Active authentication based on ambient radio signal is one way to authenticate the device and prevent spoo\ufb01ng attack. However, it is hard to obtain the dynamic time-variant channel mode in a real environment. Reinforcement learning was used to obtain this time-variant channel information in the work [94]. Here the authors proposed an active authentication of mobile devices in the indoor environment using reinforcement learning. Here authors considered the PHY-layer information to detect spoo\ufb01ng attacks. The received signal strength at the receiver, which is trust authority, was considered to detect spoo\ufb01ng attack. The receiver formulates a hypothesis test to determine whether a packet is sent from the particular address or not. The receiver on getting a packet computes the test statistics of the hypothesis test. If it is below a threshold, the receiver accepts the packet as authentic otherwise detects the packet as a spoofed packet. The test threshold of hypothesis test in a dynamic environment is chosen by reinforcement learning. Q-learning was used to \ufb01nd the optimal threshold strategy without knowing the model of the arriving packet. State space in this environment is the false alarm rate and miss rate. Based on the observation of states, the receiver chooses a test threshold from L levels. State-action function in Q-learning is updated and utility calculated by the receiver is the reward function. Here the agent uses epsilon-greedy policy to get the optimal test threshold. The simulation result in experimenting with a legitimate user and three spoofers showed that the proposed Q-learning based test threshold strategy gave better utility. Also, the result showed that the proposed approach minimized the convergence of the false alarm rate and miss rate as contrasted to the \ufb01xed threshold approach. Compared to the \ufb01xed threshold approach, the proposed Q-learningbased threshold can ef\ufb01ciently detect the spoo\ufb01ng attacks in a dynamic environment. The work in [95] followed a similar approach as done by authors in [94] to detect spoo\ufb01ng attack. Reinforcement learning was implemented to \ufb01nd the optimal test threshold. But here the authors have compared the performance of the RL agent on the following two algorithms. They have compared the performance of Q-learning and Dyna-Q algorithm. The simulation was implemented in an indoor environment in USRPs. False alarm rate and miss rate are the states for the agent and utility at receiver is given as reward function. The simulation result showed that the error rate with DynaQ is lower than with Q-learning. The detection rate using both algorithms is better than the \ufb01xed threshold approach. Spoo\ufb01ng attack detection in an indoor environment is covered by previous papers. Authors in [96] proposed a rogue edge detection scheme for VANETs (Vehicular Ad Hoc Network) observing the ambient radio signals. Similar to the previous approach, here also authors used Q-learning to allow mobile devices to reach optimal rogue edge attack detection policy without being aware of the dynamic VANET model. Most of the security approaches are reactive i.e. they try to detect the security breach and then only recover from the attack. However, the work in [97] tried to predict the intention of attack. They considered sensor spoo\ufb01ng attacks in one or more sensors of an autonomous vehicle with multiple sensors. The attackers try to take the vehicle in the undesired state by spoo\ufb01ng and hiding inside the sensors. Here authors came up with a Reachability-based approach and Inverse RL to predict the intention of the attacker and detect the compromised sensors. First reachability analysis was used, as done by authors in [98], to \ufb01nd the set of possible states the vehicle can reach on a certain time slot. Inverse RL was used to infer the maximum reward function expected by the attacker. The approach used was to \ufb01nd the group of sensors that deviate the vehicle towards the undesired state. Bayesian Inverse RL (BIRL) can learn the reward function in Markov Decision Process (MDP) if they are given the behavior and dynamics of the system [99]. Given the set of observations, they calculate the posterior probability of all reachable goals. If any of the sensors return a state value such that the variance of the posterior probability is within the user selected threshold, the recovery procedure is initiated. The simulation result showed that when the variance of the posterior probability of goal of attacker reached below the threshold value, the spoofed sensor was detected and omitted from the state estimation and a recovery process is initiated. Similar to this work, BIRL was used in the work [100] to detect the spoofed sensor in an autonomous vehicle environment. In this work, authors have used active exploration policy in which the vehicle explores the environment to reach sensitive states. Active exploration prevented the vehicle to reach states very near to the undesirable state. IV. REINFORCEMENT LEARNING IN CYBER PHYSICAL SYSTEMS A. Security in Smart Grid Smart Grid is an intelligent system to generate and distribute energy in a distributed manner. It is the combination of the traditional power grid and information systems that allows ef\ufb01cient energy generation and consumption. Digital processing in the traditional power grid leads to Smart Grid which gives the capability to control, communicate and monitoring of available energy sources. However, smart grids being online and connected is vulnerable to various cyber attacks. The integration of cyber component exposes it to critical cyberattacks and unauthorized penetration. Cyber Physical Attack also called a blended attack imposes a threat to both the cyber and physical systems of the grid thus causing negative consequences than by the individual attack (cyber attack or physical attack) [101]. Attacks like information tampering and eavesdropping throw a big threat to the security of Smart Grid [102]. Researchers around the world are concerned about the security in this area of CPS and have proposed several security approaches. Here, we will talk about security approaches taken to protect Smart Grid using RL. A sequential attack on the network topology of a smart grid is a serious attack in which the attacker can determine the number and time to attack the component to cause maximum damage. A sequential attack imposes more damage than by a simultaneous attack when attacked on the same victim links [103]. The authors in [104] proposed a Q-learning based vulnerability analysis of smart grid under sequential attack 8 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 admitting the physical system behaviors. Here the authors de\ufb01ned sequential attack as a sequence of coordinated interdiction such that it changes an in-service line into outof-service. By manipulating the control commands or false line status data such an attack can be performed resulting in cascading blackout. The authors proposed a Q-learning based vulnerability analysis in a smart grid under sequential attack. In this RL environment, the agent is the attacker who tries to identify the more vulnerable point in the grid under a sequential attack. State space is either in-service line or outof-service line at a time. Action taken by the attacker is to maliciously turn the in-service line to out of service. The goal of the attacker using Q-learning is to mind the optimal policy to fail the system with the least number of lines attacked. If the attacker can reach blackout by turning of lines equal to or more than a threshold line value and with less action than the threshold value, it will get a positive reward. It gets a negative reward on taking more action than the threshold. Otherwise, the reward given is zero. The experimental results of this approach successfully identi\ufb01ed the critical sequential topology attacks. The blackout sizes in the proposed method showed that with the increase in load, sequential attacks caused more line outages and attack intentions were accomplished quicker. The proposed Q-learning approach tried to learn and \ufb01nd out vulnerable sequences that directed to severe blackouts in the system. Using this vulnerability analysis of the sequential attack, the defender side can follow the security measures to better the situational awareness cyber-security approaches. False Data Injection (FDI) is proven to be a challenging attack to the smart grid in which the attacker injects malicious data to the Supervisory Control and Data Acquisition (SCADA) system resulting in cascading failure of a smart power system. In the work [105], an intelligent FDI attack on a smart grid with automatic voltage control was studied. The authors considered a smart attacker that uses Q-learning approach to \ufb01nd the optimal attack strategy stealthily to manipulate the control system in a compromised substation. In the given paper, the state space is the voltage angle, the amplitude of the buses, the active and reactive power of the generator and, the active and reactive power of the load. An attacker can perform FDI only based on local observation so authors considered the attack as Partially Observable MDP (POMDP). Attacker action is to compromise many measurements of the attacked substation. The reward function is de\ufb01ned such that the attacker\u2019s action makes the bus voltage in compromised substation lower than the desired operational voltage. By giving rewards for no action, the attacker stops injecting FDI and avoids giving a \ufb01xed action pattern. Using Q-learning method with the nearest memory sequence results showed that the proposed FDI, with little knowledge of the complete power system, could generate voltage breakdown events. Online learning helps the attacker to choose probable attack times automatically to make the attack silent. The test result shown in the results section validated the bad data detection and correction method presented against the proposed FDI attack. Some advantages and disadvantages of the some approaches for securing IoT with reinforcement learning is tabulated in Table II. The work in [106] tried to design a defender system against cyber attack in a smart grid using reinforcement learning. The authors proposed a model free RL algorithm that can defend cyber attacks on the \ufb02y without knowing any attack model. A defender is proposed that can detect the low magnitude attack which will be the worst-case scenario for the defender. This makes the defender sensitive to even a very slight deviation of measurement from a normal measurement. The proposed defender system also limits the action space of the attacker. An attacker can only make a lower magnitude attack to be not detected. Such a lower magnitude attack, however, can not make damage to the system. The agent does not know the attacker attack time, so they considered two state i.e. preattack and postattack state. State space is the status of transmission lines in the power system. After observing the measurement, agent (defender) can take two actions. Either they can stop and declare an attack or they can continue to obtain more measurements. The goal of the agent to lessen the detection delays and false alarm rates. Here the reward is the cost associated with the detection delay compared with the false alarm rate. If the agent in preattack state takes action to stop, it gets a unitary reward. While if in postattack state it takes continue action, a cost is given as a reward which is due to the detection delay. SARSA algorithm was used to train the agent and update the Q- table. SARSA is a model free RL algorithm that is shown to have better performance in POMDP environment [107]. Using this learned Q-table, the agent performs online attack detection by choosing the action that leads to the minimum expected future cost. The agent continues to take action until it takes stop action on which it declares that there is attack in the system. They have shown the simulation result of the proposed RL based defender in the presence of different kinds of attacks and compared with Euclidean detector and Cos-Sim detector. The result showed that RL based detector detected the attack with very low detection delay as compared to other approaches. However, here they consider single agent defender which can be extended to multi agent and they have not considered a smart attacker. Ni and Paul [108] proposed a dynamic game between the attacker and the defender to \ufb01nd the optimal attack strategies using reinforcement learning. The attacker learns the attack sequence to be applied in the transmission lines. On the other hand, the defender learns to protect the lines selected. An attacker takes generation loss and line outages as the reward and based on which it plans the next action. Here the attacker \ufb01nds the critical transmission lines in the smart grid based on the action taken by the defender. This learned attack sequence is used by the defender so that it minimizes the action set for the attacker. In this multistage game, they \ufb01rst assumed defender to be passive and attacker to be smart learner. Defender policy was prede\ufb01ned so using that policy information, the attacker performs trial and error action using Q-learning and conducts an attack on the transmission line. Calculation of generation loss and cascade are done and after getting a reward, the Q-table is updated. Later at the end of this multistage game, the defender aligns its action based on ",
    "Approach": "Approach Goal Specialities(+) and Limitations(-) Multiagent Router Throttling [59] Multiple Agent Learn to throttle the traf\ufb01c to victim server + Solves Stability issue -Not Scalable Coordinated Team Learning in Multiagent Router Throttling [60] Hierarchical team based communication to throttle excessive traf\ufb01c reaching server + Scalability is achieved + Improved Adaptability - Consideration of less statistical feature -Lower Data Ef\ufb01ciency Smart mitigation Agent in Software De\ufb01ned Network [64] Mitigation Agent throttles the traf\ufb01c by evaluating the controller of SDN +Highly Scalable +Improved Data Ef\ufb01ciency +Reduces overhead on SDN switches Power Control for IoT against Jamming [80] IoT device decides transmit power in a way to improve SINR and utility in the presence of jammers +More realistic approach (experimented under hardware constraint) +Improved Communication Ef\ufb01ciency - Cost overhead Two-dimensional anti-jamming communication [81] Avoid jamming attack smartly without interfering primary user +Faster Convergence - Cannot handle smart jammers Antijamming in underwater acoustic network [83] To control transmit power against jamming in acoustic network for underwater robots and vehicles. + Higher learning speed - Not scalable - Cannot handle smart jammer Antijamming communication using Spectrum Waterfall [84] To achieve antijamming in dynamic environement in the presence of smart jammer + Less information loss + Reduced complexity - Cannot converge in the presence of RL-based jammer Antijamming with Wideband Autonomous Cognitive Radio [86] Optimal sub-band selection against jammers +Reduced Complexity - Not practical Active Authentication of mobile devices [94] Autheticate mobile devices against spoo\ufb01ng attack + Privacy Protection + Reduced overhead cost - Not Scalable Physical Layer Rogue edge detection in VANET [96] To \ufb01nd rogue edge node based on physical properties of ambient radio signals +Handle dynamic environment Predicting malicious intention under cyber attack [97] To predict the goal of sensor spoo\ufb01ng attack and determine the compromised sensor +More realistic approach -Higher Computation Complexity -Slower convergence speed RL approach for attack intention prediction [100] To predict the intention of attacker and detect the set of compromised sensor +Faster convergence speed - Complex computation the observation and using the sequence learned by the attacker. Here the game was proposed as a zero-sum game in which the reward given to attacker and to defender are opposite to each other. The experimental result showed that total line outages caused by the multistage attack are more consequential than a single stage attack. Also, the result here showed the decreased number of successful attacks and average generation loss on adjusting with the strategy of the defender. The shown case studies in the paper imply that learned information of the attacker can ultimately assist the defenders to plan for better defense policies. B. Security in Smart Transportation System Smart Transportation System (STS) is a CPS system that consists of sensors technology, control, and communication in vehicles and any other transportation infrastructure. The goal is to provide real-time road and other vehicle information for users to improve safety and comfort in transportation. It achieves the smartness in transportation by establishing the connection between vehicle to vehicle (V2V), vehicle to other infrastructures (V2I), vehicles to pedestrian and so on [109]. However, security challenges in STS are posing a threat to this system. It should properly handle issues like privacy protection, authorization, data integrity, data storage, and management [110]. Security is always perceived as one of the most important considerations in realizing STS usecases [111]. Cybersecurity researchers around the world have proposed several methods to secure this transportation. Machine Learning is an emerging technology that have added more smartness to the STS system and also it has been used in securing the system intelligently. VANET and FANET (Flight Ad-Hoc Network) can both be considered as STS. Next we discuss the application of RL for security in STS. The work in [112] presented deep reinforcement learning approach for Unmanned Aerial Vehicle (UAV) against smart attacks with no information on the attack model and accuracy of the system to detect the attack. DQN was used to \ufb01nd the optimal power allocation strategy against a smart attacker. Authors \ufb01rst formulated a prospect theory based smart attack game to \ufb01nd the attack on UAV transmission by a subjective 10 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 Table III REINFORCEMENT LEARNING PARAMETERS FOLLOWED BY SOME OF THE RESEARCH APPROACHES. References Algorithm Agent State-Space Action-Space Reward [59] SARSA Router Traf\ufb01c \ufb02owing towards server Probabilistic throttle of traf\ufb01c coming from host Negative reward if server is overloaded otherwise reward dependent on the rate of legitimate traf\ufb01c [64] DDPG Mitigating Router Flow statistics and features of each port Generate a vector representing maximum bandwidth of speci\ufb01c host Negative reward if load on server exceeds an upper bound otherwise de\ufb01ned by reward function [80] DQN with CNN IoT devices SINR and utility value Decide transmit power at a time slot Positive reward if SINR and utility is improved [81] DQN with CNN Mobile device Presence of PUs and SINR of the signal at previous timeslot Decide to leave or stay at an area and choose channel SINR and utility [83] DQN with CNN Sensor SINR and RSSI at previous timeslot Choose trasmit power and decide to stay or move to another area Utility of the signal [84] DRL with RCNN Sensor Raw frequency spectrum information Choose signal frequency De\ufb01ned by function based on utility and cost of frequency switching [86] Q- learning Radio Required bandwidth length Select a new subband Time taken by jammer to interfere the transmission after switching to a sub band [94] Q-Learning Radio Device False rate and miss rate of authenticated packets Select test threshold value Utility attacker. Then DQN is proposed to \ufb01nd the optimal power allocation strategy in multiple frequency channels. They compared the convergence rate achieved by using Q-learning, DQN and WoLF-PHC (Win or Learn Fast- Policy Hill Climbing) for power allocation against the attacker. UAV sends a signal in each time slot with certain power using DQN approach and observing the state of the network. Observation is the SINR value and utility of the received signal. The results shown in the paper depicts that DQN based power allocation is applicable for UAV having enough resources. On the other hand, WoLF-PHC based strategy can choose a transmission strategy with a lower computational cost. Here UAV can address the Q-learning based smart attack by learning the optimal transmission strategy. VANETs in a large network topology bring high mobility in the onboard units (OBUs). Due to this large scale and dynamic nature, an antijamming strategy like frequency hopping is not ef\ufb01cient. The work in [113] presented a UAV based antijamming approach in VANET using reinforcement learning. This is a follow up research for the proposed UAV relay strategy in the work [114] considering more practical aspects. Here authors proposed a relay game in which UAV learns whether or not to relay the OBUs data to another roadside unit (RSU) and smart jammer decides its jamming power. The authors presented the Nash equilibrium (NE) to show the dependence between the transmission cost and channel model with the UAV relay strategy. Here hotbooting-PHC based strategy was presented for faster UAV relay decisions. Hot booting uses the experimental data generated in advance to update the Q-table. This initializes the Q-value and probability of action-state and hence learning speed is signi\ufb01cantly higher. Here UAV decides relay action based on the observed Bit Error Rate (BER) of the data send by UAV and the channel quality. The experimental result showed the decreased BER of OBU data and increased utility of VANET using the proposed algorithm than by using the Q-learning approach. Thus if the serving RSU for an OBU in VANET is severely jammed, the proposed UAV based relay strategy can transfer that information to another RSU and prevents the VANET from potential jamming affects. Connected and autonomous vehicles (CAVs) and UAVs in IoT can be misused by attackers which directly impose a threat to the STS. The authors in [115] proposed an antijamming V2V communication in an integrated UAV-CAV network with hybrid attackers. They assumed a malicious CAV that can perform smart jamming and a malicious UAV without smartness. Inspired by the predictive-adaptation feature of the human brain, they proposed a research tool called CDS to lead the idea of an anti-jamming technique. The process of channel ",
    "References": "",
    "Conclusion": "CONCLUSION In this paper, we have presented a comprehensive survey on the application of Reinforcement learning for IoT security. First, we have given a brief introduction about Reinforcement learning and background information about several attacks in IoT. Following that, we have presented a survey on various reinforcement learning techniques proposed against IoT attacks such as jamming attack, spoo\ufb01ng attack and denial of service attack. Furthermore, we have presented the Reinforcement learning for securing CPS systems (i.e., IoT with feedback and control) such as smart grid and smart transportation system. Moreover, we have presented some open research challenges and some research direction for IoT security using reinforcement learning. REFERENCES [1] L. Atzori, A. Iera, and G. Morabito, \u201cThe internet of things: A survey,\u201d Computer networks, vol. 54, no. 15, pp. 2787\u20132805, 2010. [2] F. Olowononi, D. B. Rawat, and C. Liu, \u201cResilient Machine Learning for Networked Cyber Physical Systems: A Survey for Machine Learning Security to Securing Machine Learning for CPS,\u201d IEEE Communications Surveys and Tutorials, 2020. Early Access, DoI: https://doi.org/10.1109/COMST.2020.3036778. [3] D. B. Rawat, R. Alsabet, C. Bajracharya, and M. Song, \u201cOn the performance of cognitive internet-of-vehicles with unlicensed usermobility and licensed user-activity,\u201d Computer Networks, vol. 137, pp. 98\u2013106, 2018. [4] D. B. Rawat and C. Bajracharya, \u201cVehicular Cyber Physical Systems: Adaptive Connectivity and Security,\u201d tech. rep., Springer, 2017. [5] S. Mans\ufb01eld-Devine, \u201cOpen source and the internet of things,\u201d Network Security, vol. 2018, no. 2, pp. 14\u201319, 2018. [6] M. Min, X. Wan, L. Xiao, Y. Chen, M. Xia, D. Wu, and H. Dai, \u201cLearning-based privacy-aware of\ufb02oading for healthcare IoT with energy harvesting,\u201d IEEE Internet of Things Journal, vol. 6, no. 3, pp. 4307\u20134316, 2018. 12 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 [7] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [8] F. L. Lewis and D. Vrabie, \u201cReinforcement learning and adaptive dynamic programming for feedback control,\u201d IEEE Circuits and Systems Magazine, vol. 9, no. 3, pp. 32\u201350, 2009. [9] W. Schultz, P. Dayan, and P. R. Montague, \u201cA neural substrate of prediction and reward,\u201d Science, vol. 275, no. 5306, pp. 1593\u20131599, 1997. [10] A. G. Barto and S. Mahadevan, \u201cRecent advances in hierarchical reinforcement learning,\u201d Discrete event dynamic systems, vol. 13, no. 12, pp. 41\u201377, 2003. [11] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d science, vol. 313, no. 5786, pp. 504\u2013507, 2006. [12] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, \u201cAsynchronous methods for deep reinforcement learning,\u201d in International conference on machine learning, pp. 1928\u20131937, 2016. [13] S. Dreyfus, \u201cRichard bellman on the birth of dynamic programming,\u201d Operations Research, vol. 50, no. 1, pp. 48\u201351, 2002. [14] V. Franc\u00b8ois-Lavet, P. Henderson, R. Islam, M. G. Bellemare, J. Pineau, et al., \u201cAn introduction to deep reinforcement learning,\u201d Foundations and Trends\u00ae in Machine Learning, vol. 11, no. 3-4, pp. 219\u2013354, 2018. [15] S. P. K. Spielberg, R. B. Gopaluni, and P. D. Loewen, \u201cDeep reinforcement learning approaches for process control,\u201d in 2017 6th International Symposium on Advanced Control of Industrial Processes (AdCONIP), pp. 201\u2013206, 2017. [16] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, \u201cResource management with deep reinforcement learning,\u201d in Proceedings of the 15th ACM Workshop on Hot Topics in Networks, pp. 50\u201356, 2016. [17] Y. Zhang, J. Yao, and H. Guan, \u201cIntelligent cloud resource management with deep reinforcement learning,\u201d IEEE Cloud Computing, vol. 4, no. 6, pp. 60\u201369, 2017. [18] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Roth\u00a8orl, T. Lampe, and M. Riedmiller, \u201cLeveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards,\u201d arXiv preprint arXiv:1707.08817, 2017. [19] S. Gu, E. Holly, T. Lillicrap, and S. Levine, \u201cDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates,\u201d in 2017 IEEE international conference on robotics and automation (ICRA), pp. 3389\u20133396, IEEE, 2017. [20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [21] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d nature, vol. 529, no. 7587, p. 484, 2016. [22] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971, 2015. [23] H. Van Hasselt, A. Guez, and D. Silver, \u201cDeep reinforcement learning with double q-learning,\u201d in Thirtieth AAAI conference on arti\ufb01cial intelligence, 2016. [24] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas, \u201cDueling network architectures for deep reinforcement learning,\u201d arXiv preprint arXiv:1511.06581, 2015. [25] I. Kachalsky, I. Zakirzyanov, and V. Ulyantsev, \u201cApplying reinforcement learning and supervised learning techniques to play hearthstone,\u201d in 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 1145\u20131148, 2017. [26] M. H. Ling, K.-L. A. Yau, J. Qadir, G. S. Poh, and Q. Ni, \u201cApplication of reinforcement learning for security enhancement in cognitive radio networks,\u201d Applied Soft Computing, vol. 37, pp. 809\u2013829, 2015. [27] P. Garcia-Teodoro, J. Diaz-Verdejo, G. Maci\u00b4a-Fern\u00b4andez, and E. V\u00b4azquez, \u201cAnomaly-based network intrusion detection: Techniques, systems and challenges,\u201d computers & security, vol. 28, no. 1-2, pp. 18\u2013 28, 2009. [28] S. Dua and X. Du, Data mining and machine learning in cybersecurity. CRC press, 2016. [29] A. L. Buczak and E. Guven, \u201cA survey of data mining and machine learning methods for cyber security intrusion detection,\u201d IEEE Communications surveys & tutorials, vol. 18, no. 2, pp. 1153\u20131176, 2015. [30] D. S. Berman, A. L. Buczak, J. S. Chavis, and C. L. Corbett, \u201cA survey of deep learning methods for cyber security,\u201d Information, vol. 10, no. 4, p. 122, 2019. [31] S. K. Biswas, \u201cIntrusion detection using machine learning: A comparison study,\u201d International Journal of Pure and Applied Mathematics, vol. 118, no. 19, pp. 101\u2013114, 2018. [32] Y. Xin, L. Kong, Z. Liu, Y. Chen, Y. Li, H. Zhu, M. Gao, H. Hou, and C. Wang, \u201cMachine learning and deep learning methods for cybersecurity,\u201d IEEE Access, vol. 6, pp. 35365\u201335381, 2018. [33] N. Milosevic, A. Dehghantanha, and K.-K. R. Choo, \u201cMachine learning aided android malware classi\ufb01cation,\u201d Computers & Electrical Engineering, vol. 61, pp. 266\u2013274, 2017. [34] S. KP et al., \u201cA short review on applications of deep learning for cyber security,\u201d arXiv preprint arXiv:1812.06292, 2018. [35] M. Rege and R. B. K. Mbah, \u201cMachine learning for cyber defense and attack,\u201d DATA ANALYTICS 2018, p. 83, 2018. [36] D. Ding, Q.-L. Han, Y. Xiang, X. Ge, and X.-M. Zhang, \u201cA survey on security control and attack detection for industrial cyber-physical systems,\u201d Neurocomputing, vol. 275, pp. 1674\u20131683, 2018. [37] M. Wu, Z. Song, and Y. B. Moon, \u201cDetecting cyber-physical attacks in cybermanufacturing systems with machine learning methods,\u201d Journal of intelligent manufacturing, vol. 30, no. 3, pp. 1111\u20131123, 2019. [38] L. Xiao, X. Wan, X. Lu, Y. Zhang, and D. Wu, \u201cIot security techniques based on machine learning: How do iot devices use ai to enhance security?,\u201d IEEE Signal Processing Magazine, vol. 35, no. 5, pp. 41\u2013 49, 2018. [39] T. T. Nguyen and V. J. Reddi, \u201cDeep reinforcement learning for cyber security,\u201d arXiv preprint arXiv:1906.05799, 2019. [40] W. Wang, A. Kwasinski, D. Niyato, and Z. Han, \u201cA survey on applications of model-free strategy learning in cognitive wireless networks,\u201d IEEE Communications Surveys & Tutorials, vol. 18, no. 3, pp. 1717\u2013 1757, 2016. [41] Y. Wang, Z. Ye, P. Wan, and J. Zhao, \u201cA survey of dynamic spectrum allocation based on reinforcement learning algorithms in cognitive radio networks,\u201d Arti\ufb01cial Intelligence Review, vol. 51, no. 3, pp. 493\u2013 506, 2019. [42] G. Caminero, M. Lopez-Martin, and B. Carro, \u201cAdversarial environment reinforcement learning algorithm for intrusion detection,\u201d Computer Networks, vol. 159, pp. 96\u2013109, 2019. [43] X. Li, R. Lu, X. Liang, X. Shen, J. Chen, and X. Lin, \u201cSmart community: an internet of things application,\u201d IEEE Communications magazine, vol. 49, no. 11, pp. 68\u201375, 2011. [44] I. Andrea, C. Chrysostomou, and G. Hadjichristo\ufb01, \u201cInternet of things: Security vulnerabilities and challenges,\u201d in 2015 IEEE Symposium on Computers and Communication (ISCC), pp. 180\u2013187, IEEE, 2015. [45] R. Roman, J. Zhou, and J. Lopez, \u201cOn the features and challenges of security and privacy in distributed internet of things,\u201d Computer Networks, vol. 57, no. 10, pp. 2266\u20132279, 2013. [46] F. A. Alaba, M. Othman, I. A. T. Hashem, and F. Alotaibi, \u201cInternet of things security: A survey,\u201d Journal of Network and Computer Applications, vol. 88, pp. 10\u201328, 2017. [47] S. Alanazi, J. Al-Muhtadi, A. Derhab, K. Saleem, A. N. AlRomi, H. S. Alholaibah, and J. J. Rodrigues, \u201cOn resilience of wireless mesh routing protocol against dos attacks in iot-based ambient assisted living applications,\u201d in 2015 17th International Conference on E-health Networking, Application & Services (HealthCom), pp. 205\u2013210, IEEE, 2015. [48] M. Antonakakis, T. April, M. Bailey, M. Bernhard, E. Bursztein, J. Cochran, Z. Durumeric, J. A. Halderman, L. Invernizzi, M. Kallitsis, et al., \u201cUnderstanding the mirai botnet,\u201d in 26th {USENIX} Security Symposium ({USENIX} Security 17), pp. 1093\u20131110, 2017. [49] N. Woolf, \u201cDdos attack that disrupted internet was largest of its kind in history, experts say.\u201d [50] I. Romdhani, R. Abdmeziem, and D. Tandjaoui, Architecting the Internet of Things: State of the Art. 07 2015. [51] M. Wu, T.-J. Lu, F.-Y. Ling, J. Sun, and H.-Y. Du, \u201cResearch on the architecture of internet of things,\u201d in 2010 3rd International Conference on Advanced Computer Theory and Engineering (ICACTE), vol. 5, pp. V5\u2013484, IEEE, 2010. [52] P. Wang, S. Chaudhry, L. Li, S. Li, T. Tryfonas, and H. Li, \u201cThe internet of things: a security point of view,\u201d Internet Research, 2016. [53] C.-L. Zhong, Z. Zhu, and R.-G. Huang, \u201cStudy on the iot architecture and gateway technology,\u201d in 2015 14th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES), pp. 196\u2013199, IEEE, 2015. [54] K. Finkenzeller, \u201cKnown attacks on r\ufb01d systems, possible countermeasures and upcoming standardisation activities,\u201d in 5th European Workshop on RFID Systems and Technologies, pp. 1\u201331, 2009. [55] A. Mitrokotsa, M. R. Rieback, and A. S. Tanenbaum, \u201cClassi\ufb01cation of r\ufb01d attacks,\u201d Gen, vol. 15693, no. 14443, p. 14, 2010. 13 [56] H.-Y. Chien and C.-W. Huang, \u201cSecurity of ultra-lightweight r\ufb01d authentication protocols and its improvements,\u201d ACM SIGOPS Operating Systems Review, vol. 41, no. 4, pp. 83\u201386, 2007. [57] K. Sonar and H. Upadhyay, \u201cA survey: Ddos attack on internet of things,\u201d International Journal of Engineering Research and Development, vol. 10, no. 11, pp. 58\u201363, 2014. [58] J. Deng, R. Han, and S. Mishra, \u201cDefending against path-based dos attacks in wireless sensor networks,\u201d in Proceedings of the 3rd ACM workshop on Security of ad hoc and sensor networks, pp. 89\u201396, 2005. [59] K. Malialis and D. Kudenko, \u201cMultiagent router throttling: Decentralized coordinated response against ddos attacks,\u201d in Twenty-Fifth IAAI Conference, 2013. [60] K. Malialis and D. Kudenko, \u201cDistributed response to network intrusions using multiagent reinforcement learning,\u201d Engineering Applications of Arti\ufb01cial Intelligence, vol. 41, pp. 270\u2013284, 2015. [61] D. K. Yau, J. C. Lui, F. Liang, and Y. Yam, \u201cDefending against distributed denial-of-service attacks with max-min fair server-centric router throttles,\u201d IEEE/ACM Transactions on Networking, vol. 13, no. 1, pp. 29\u201342, 2005. [62] F. Hu, Q. Hao, and K. Bao, \u201cA survey on software-de\ufb01ned network and open\ufb02ow: From concept to implementation,\u201d IEEE Communications Surveys & Tutorials, vol. 16, no. 4, pp. 2181\u20132206, 2014. [63] S. Fang, Y. Yu, C. H. Foh, and K. M. M. Aung, \u201cA loss-free multipathing solution for data center network using software-de\ufb01ned networking approach,\u201d IEEE transactions on magnetics, vol. 49, no. 6, pp. 2723\u20132730, 2013. [64] Y. Liu, M. Dong, K. Ota, J. Li, and J. Wu, \u201cDeep reinforcement learning based smart mitigation of ddos \ufb02ooding in software-de\ufb01ned networks,\u201d in 2018 IEEE 23rd International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD), pp. 1\u20136, IEEE, 2018. [65] D. G. Bhoyar and U. Yadav, \u201cReview of jamming attack using game theory,\u201d in 2017 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS), pp. 1\u2013 4, 2017. [66] K. Grover, A. Lim, and Q. Yang, \u201cJamming and anti-jamming techniques in wireless networks: a survey,\u201d International Journal of Ad Hoc and Ubiquitous Computing, vol. 17, no. 4, pp. 197\u2013215, 2014. [67] S. G. Weber, L. Martucci, S. Ries, and M. M\u00a8uhlh\u00a8auser, \u201cTowards trustworthy identity and access management for the future internet,\u201d in 4th International Workshop on Trustworthy Internet of People, Things & Services, 2010. [68] Y. Wu, B. Wang, K. R. Liu, and T. C. Clancy, \u201cAnti-jamming games in multi-channel cognitive radio networks,\u201d IEEE journal on selected areas in communications, vol. 30, no. 1, pp. 4\u201315, 2011. [69] R. El-Bardan, S. Brahma, and P. K. Varshney, \u201cPower control with jammer location uncertainty: A game theoretic perspective,\u201d in 2014 48th Annual Conference on Information Sciences and Systems (CISS), pp. 1\u20136, IEEE, 2014. [70] M. Cagalj, S. Capkun, and J.-P. Hubaux, \u201cWormhole-based antijamming techniques in sensor networks,\u201d IEEE transactions on Mobile Computing, vol. 6, no. 1, pp. 100\u2013114, 2006. [71] B. Wang, Y. Wu, K. R. Liu, and T. C. Clancy, \u201cAn anti-jamming stochastic game for cognitive radio networks,\u201d IEEE journal on selected areas in communications, vol. 29, no. 4, pp. 877\u2013889, 2011. [72] N. Namvar, W. Saad, N. Bahadori, and B. Kelley, \u201cJamming in the internet of things: A game-theoretic perspective,\u201d in 2016 IEEE Global Communications Conference (GLOBECOM), pp. 1\u20136, 2016. [73] R. D. Pietro and G. Oligeri, \u201cJamming mitigation in cognitive radio networks,\u201d IEEE Network, vol. 27, no. 3, pp. 10\u201315, 2013. [74] H. A. Bany Salameh, S. Almajali, M. Ayyash, and H. Elgala, \u201cSpectrum assignment in cognitive radio networks for internet-of-things delaysensitive applications under jamming attacks,\u201d IEEE Internet of Things Journal, vol. 5, no. 3, pp. 1904\u20131913, 2018. [75] J. Heo, J.-J. Kim, S. Bahk, and J. Paek, \u201cDodge-jam: Anti-jamming technique for low-power and lossy wireless networks,\u201d in 2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), pp. 1\u20139, IEEE, 2017. [76] S. Kim, \u201cCognitive radio anti-jamming scheme for security provisioning iot communications.,\u201d KSII Transactions on Internet & Information Systems, vol. 9, no. 10, 2015. [77] D. B. Rawat and M. Song, \u201cSecuring space communication systems against reactive cognitive jammer,\u201d in 2015 IEEE Wireless Communications and Networking Conference (WCNC), pp. 1428\u20131433, IEEE, 2015. [78] S. Djuraev, J.-G. Choi, K.-S. Sohn, and S. Y. Nam, \u201cChannel hopping scheme to mitigate jamming attacks in wireless lans,\u201d EURASIP Journal on Wireless Communications and Networking, vol. 2017, no. 1, p. 11, 2017. [79] J. Becker, \u201cDynamic beamforming optimization for anti-jamming and hardware fault recovery,\u201d 2014. [80] Y. Chen, Y. Li, D. Xu, and L. Xiao, \u201cDqn-based power control for iot transmission against jamming,\u201d in 2018 IEEE 87th Vehicular Technology Conference (VTC Spring), pp. 1\u20135, 2018. [81] G. Han, L. Xiao, and H. V. Poor, \u201cTwo-dimensional anti-jamming communication based on deep reinforcement learning,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2087\u20132091, 2017. [82] A. A. Khan, M. H. Rehmani, and A. Rachedi, \u201cCognitive-radiobased internet of things: Applications, architectures, spectrum related functionalities, and future research directions,\u201d IEEE wireless communications, vol. 24, no. 3, pp. 17\u201325, 2017. [83] L. Xiao, X. Wan, W. Su, Y. Tang, et al., \u201cAnti-jamming underwater transmission with mobility and learning,\u201d IEEE Communications Letters, vol. 22, no. 3, pp. 542\u2013545, 2018. [84] X. Liu, Y. Xu, L. Jia, Q. Wu, and A. Anpalagan, \u201cAnti-jamming communications using spectrum waterfall: A deep reinforcement learning approach,\u201d IEEE Communications Letters, vol. 22, no. 5, pp. 998\u20131001, 2018. [85] Y. Li, X. Wang, D. Liu, Q. Guo, X. Liu, J. Zhang, and Y. Xu, \u201cOn the performance of deep reinforcement learning-based anti-jamming method confronting intelligent jammer,\u201d Applied Sciences, vol. 9, no. 7, p. 1361, 2019. [86] S. Machuzak and S. K. Jayaweera, \u201cReinforcement learning based anti-jamming with wideband autonomous cognitive radios,\u201d in 2016 IEEE/CIC International Conference on Communications in China (ICCC), pp. 1\u20135, 2016. [87] M. Bkassiny, S. K. Jayaweera, Y. Li, and K. A. Avery, \u201cWideband spectrum sensing and non-parametric signal classi\ufb01cation for autonomous self-learning cognitive radios,\u201d IEEE Transactions on Wireless Communications, vol. 11, no. 7, pp. 2596\u20132605, 2012. [88] M. A. Aref, S. K. Jayaweera, and S. Machuzak, \u201cMulti-agent reinforcement learning based cognitive anti-jamming,\u201d in 2017 IEEE Wireless Communications and Networking Conference (WCNC), pp. 1\u20136, 2017. [89] P. R. Babu, D. L. Bhaskari, and C. Satyanarayana, \u201cA comprehensive analysis of spoo\ufb01ng,\u201d International Journal of Advanced Computer Science and Applications, vol. 1, no. 6, pp. 157\u201362, 2010. [90] Z. Duan, X. Yuan, and J. Chandrashekar, \u201cControlling ip spoo\ufb01ng through interdomain packet \ufb01lters,\u201d IEEE Transactions on Dependable and Secure Computing, vol. 5, no. 1, pp. 22\u201336, 2008. [91] S. Whalen, \u201cAn introduction to arp spoo\ufb01ng,\u201d Node99 [Online Document], April, 2001. [92] M. Nawir, A. Amir, N. Yaakob, and O. B. Lynn, \u201cInternet of things (iot): Taxonomy of security attacks,\u201d in 2016 3rd International Conference on Electronic Design (ICED), pp. 321\u2013326, 2016. [93] F. J. Liu, X. Wang, and H. Tang, \u201cRobust physical layer authentication using inherent properties of channel impulse response,\u201d in 2011MILCOM 2011 Military Communications Conference, pp. 538\u2013542, IEEE, 2011. [94] J. Liu, L. Xiao, G. Liu, and Y. Zhao, \u201cActive authentication with reinforcement learning based on ambient radio signals,\u201d Multimedia Tools and Applications, vol. 76, no. 3, pp. 3979\u20133998, 2017. [95] L. Xiao, Y. Li, G. Han, G. Liu, and W. Zhuang, \u201cPhy-layer spoo\ufb01ng detection with reinforcement learning in wireless networks,\u201d IEEE Transactions on Vehicular Technology, vol. 65, no. 12, pp. 10037\u2013 10047, 2016. [96] L. Xiao, W. Zhuang, S. Zhou, and C. Chen, \u201cLearning-based rogue edge detection in vanets with ambient radio signals,\u201d in Learning-based VANET Communication and Security Techniques, pp. 13\u201347, Springer, 2019. [97] N. Bezzo, \u201cPredicting malicious intention in cps under cyber-attack,\u201d in 2018 ACM/IEEE 9th International Conference on Cyber-Physical Systems (ICCPS), pp. 351\u2013352, IEEE, 2018. [98] E. Yel, T. X. Lin, and N. Bezzo, \u201cReachability-based self-triggered scheduling and replanning of uav operations,\u201d in 2017 NASA/ESA Conference on Adaptive Hardware and Systems (AHS), pp. 221\u2013228, 2017. [99] D. Ramachandran and E. Amir, \u201cBayesian inverse reinforcement learning.,\u201d in IJCAI, vol. 7, pp. 2586\u20132591, 2007. [100] M. Elnaggar and N. Bezzo, \u201cAn irl approach for cyber-physical attack intention prediction and recovery,\u201d in 2018 Annual American Control Conference (ACC), pp. 222\u2013227, IEEE, 2018. [101] V. Y. Pillitteri and T. L. Brewer, \u201cGuidelines for smart grid cybersecurity,\u201d tech. rep., 2014. 14 ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957 [102] D. Wang, X. Guan, T. Liu, Y. Gu, Y. Sun, and Y. Liu, \u201cA survey on bad data injection attack in smart grid,\u201d in 2013 IEEE PES Asia-Paci\ufb01c Power and Energy Engineering Conference (APPEEC), pp. 1\u20136, IEEE, 2013. [103] Y. Zhu, J. Yan, Y. Tang, Y. Sun, and H. He, \u201cThe sequential attack against power grid networks,\u201d in 2014 IEEE International Conference on Communications (ICC), pp. 616\u2013621, IEEE, 2014. [104] J. Yan, H. He, X. Zhong, and Y. Tang, \u201cQ-learning-based vulnerability analysis of smart grid against sequential topology attacks,\u201d IEEE Transactions on Information Forensics and Security, vol. 12, no. 1, pp. 200\u2013210, 2016. [105] Y. Chen, S. Huang, F. Liu, Z. Wang, and X. Sun, \u201cEvaluation of reinforcement learning-based false data injection attack to automatic voltage control,\u201d IEEE Transactions on Smart Grid, vol. 10, no. 2, pp. 2158\u20132169, 2018. [106] M. N. Kurt, O. Ogundijo, C. Li, and X. Wang, \u201cOnline cyber-attack detection in smart grid: A reinforcement learning approach,\u201d IEEE Transactions on Smart Grid, vol. 10, no. 5, pp. 5174\u20135185, 2019. [107] J. Loch and S. P. Singh, \u201cUsing eligibility traces to \ufb01nd the best memoryless policy in partially observable markov decision processes.,\u201d in ICML, pp. 323\u2013331, 1998. [108] Z. Ni and S. Paul, \u201cA multistage game in smart grid security: A reinforcement learning solution,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 9, pp. 2684\u20132695, 2019. [109] G. Karagiannis, O. Altintas, E. Ekici, G. Heijenk, B. Jarupan, K. Lin, and T. Weil, \u201cVehicular networking: A survey and tutorial on requirements, architectures, challenges, standards and solutions,\u201d IEEE communications surveys & tutorials, vol. 13, no. 4, pp. 584\u2013616, 2011. [110] N. Alsaffar, H. Ali, and W. Elmedany, \u201cSmart transportation system: A review of security and privacy issues,\u201d in 2018 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT), pp. 1\u20134, 2018. [111] M. A. Javed, E. Ben Hamida, and W. Znaidi, \u201cSecurity in intelligent transport systems for smart cities: From theory to practice,\u201d Sensors, vol. 16, no. 6, p. 879, 2016. [112] L. Xiao, C. Xie, M. Min, and W. Zhuang, \u201cUser-centric view of unmanned aerial vehicle transmission against smart attacks,\u201d IEEE Transactions on Vehicular Technology, vol. 67, no. 4, pp. 3420\u20133430, 2018. [113] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, and W. Zhuang, \u201cUav relay in vanets against smart jamming with reinforcement learning,\u201d IEEE Transactions on Vehicular Technology, vol. 67, no. 5, pp. 4087\u20134097, 2018. [114] X. Lu, D. Xu, L. Xiao, L. Wang, and W. Zhuang, \u201cAnti-jamming communication game for uav-aided vanets,\u201d in GLOBECOM 20172017 IEEE Global Communications Conference, pp. 1\u20136, IEEE, 2017. [115] S. Feng and S. Haykin, \u201cAnti-jamming v2v communication in an integrated uav-cav network with hybrid attackers,\u201d in ICC 2019 - 2019 IEEE International Conference on Communications (ICC), pp. 1\u20136, 2019. [116] L. Weng, \u201cThe multi-armed bandit problem and its solutions,\u201d lilianweng.github.io/lil-log, 2018. [117] P. Auer, N. Cesa-Bianchi, and P. Fischer, \u201cFinite-time analysis of the multiarmed bandit problem,\u201d Machine learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002. [118] A. Lazaric, M. Restelli, and A. Bonarini, \u201cReinforcement learning in continuous action spaces through sequential monte carlo methods,\u201d in Advances in neural information processing systems, pp. 833\u2013840, 2008. [119] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, \u201cHierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,\u201d in Advances in neural information processing systems, pp. 3675\u20133683, 2016. ",
    "title": "Reinforcement Learning for IoT Security:",
    "paper_info": "ACCEPTED FOR PUBLICATION BY IEEE INTERNET OF THINGS JOURNAL. EARLY ACCESS DOI LINK: HTTPS://DOI.ORG/10.1109/JIOT.2020.3040957\n1\nReinforcement Learning for IoT Security:\nA Comprehensive Survey\nAashma Uprety and Danda B. Rawat, Senior Member, IEEE\nAbstract\u2014The number of connected smart devices has been\nincreasing exponentially for different Internet-of-Things (IoT)\napplications. Security has been a long run challenge in the\nIoT systems which has many attack vectors, security \ufb02aws and\nvulnerabilities. Securing billions of\nconnected devices in IoT\nis a must task to realize the full potential of IoT applications.\nRecently, researchers have proposed many security solutions\nfor IoT. Machine learning has been proposed as one of the\nemerging solutions for IoT security and Reinforcement learning is\ngaining more popularity for securing IoT systems. Reinforcement\nlearning, unlike other machine learning techniques, can learn\nthe environment by having minimum information about the\nparameters to be learned. It solves the optimization problem\nby interacting with the environment adapting the parameters\non the \ufb02y. In this paper, we present an comprehensive survey of\ndifferent types of cyber-attacks against different IoT systems and\nthen we present reinforcement learning and deep reinforcement\nlearning based security solutions to combat those different types\nof attacks in different IoT systems. Furthermore, we present the\nReinforcement learning for securing CPS systems (i.e., IoT with\nfeedback and control) such as smart grid and smart transporta-\ntion system. The recent important attacks and countermeasures\nusing reinforcement learning in IoT are also summarized in the\nform of tables. With this paper, readers can have a more thorough\nunderstanding of IoT security attacks and countermeasures using\nReinforcement Learning, as well as research trends in this area.\nfelix2020sur Index Terms\u2014Reinforcement Learning, IoT, Se-\ncurity\nI. INTRODUCTION\nInternet of Things (IoT) connects the physical world to\nthe digital world. It is a revolutionary technology in which\nmachines talk to other machines to solve trivial to complex\ntasks [1\u20134]. Sensors and actuators are the resources from\nwhich data is exchanged between the physical world and the\ndigital world. The sensors collect data that are to be stored and\nprocessed to provide service to the user. It has brought a drastic\nchange in the lifestyle of humans by bringing smartness to\nthe devices and will eventually increase the quality of human\nlife. IoT has tremendously increased the use of the internet by\nbringing all the physical devices together in the network. Any\nManuscript received Day Month Year.\nAuthors are with the Department of Electrical Engineering and Com-\nputer Science at Howard University, Washington, DC 20059, USA. E-mail:\ndb.rawat@ieee.org.\nThis work was supported in part by the US NSF under grants CNS/SaTC\n2039583, CNS 1650831 and 1828811, by the DoD Center of Excellence in\nAI and Machine Learning (CoE-AIML) at Howard University under Contract\nNumber W911NF-20-2-0277 with the US Army Research Laboratory, the\nDoE\u2019s National Nuclear Security Administration (NNSA) Award # DE-\nNA0003946. and the US Department of Homeland Security (DHS) under\ngrant award number, 2017-ST-062-000003. However, any opinion, \ufb01nding,\nand conclusions or recommendations expressed in this material are those of\nthe author and do not necessarily re\ufb02ect the views of these funding agencies.\nphysical device brought to internet connection that can interact\nwith human can be an IoT device. For example, when cars are\nconnected to each other through the internet and communicate\nwith each other, this is called internet of cars.\nIoT collects and processes human day to day data and brings\nautomation to the task. With all the easiness provided by IoT,\nthere also exist some pitfalls in using IoT. The major challenge\nis securing the system from attackers, maintaining the privacy\nof the user of IoT and making sure that certain IoT devices\ncan be trusted. More the number of connected devices, more\nis the chance of the vulnerabilities to attack. Security in IoT\noperation is the major challenge to be faced by IoT designers.\nThe dynamic environment of IoT and runtime communication\nadds additional security requirements on the IoT design. IoT\nbrings \ufb02exibility and intelligence to the devices providing us\nusability but at the same time, it is also fearsome to use it.\nIoT is gaining a status for insecurity. Researchers divulge the\ndangerous \ufb02aws in IoT which poses a major challenge in IoT\nsuccess [2, 5, 6]. We are sharing our every personal information\nthrough IoT devices and it is very important that our data are\ncon\ufb01dential.\nReinforcement learning is a machine learning approach in\nwhich the agent interacts with the environment and tries to\nmaximize the numerical reward [7]. Human brain interacts\nwith the outer environment and uses that interaction to un-\nderstand and sustain in that environment [8]. Reinforcement\nLearning uses the human brain and sensory processing system\n[9] as an analogy to learning the environment. It is a process in\nwhich an agent has to explore all the system to understand it.\nConsidering the time it takes to converge and get an optimal\npolicy, it is not feasible in many scenarios. Traditional RL\nsuffers a curse of dimensionality. As the environment becomes\ncomplex, there is exponential growth in the parameters to\nbe learned by RL agent [10]. As a solution, we have deep\nreinforcement learning (DRL) which is a combination of deep\nnetwork and reinforcement learning (RL) [11]. RL has been\napplied in securing IoT technology in various domains which\nis the main scope of this paper. IoT is a highly mobile\ntechnology and is very vulnerable to many cyber attacks. The\nsensors and actuators are one point of attack. Network for\ncommunication is again another major point of attacks in IoT.\nMuch research work has been done to provide security to IoT\nsystem using RL technology.\nThe main scope of this paper is to provide a literature review\nof research done on securing IoT devices using RL. Along\nwith this, we also provide a background of reinforcement\nlearning. The paper is organized as follows. In Section II, we\ncompare reinforcement learning with other machine learning\narXiv:2102.07247v1  [cs.LG]  14 Feb 2021\n",
    "GPTsummary": "- (4): The proposed approach is evaluated on a physical robot in a peg-in-hole task and achieves comparable performance to other state-of-the-art methods with much less training data. The results show that the node-wise system identification approach can effectively reduce the dimensionality of the reinforcement learning environment and improve the sample efficiency and generalization ability of the robot.\n7. Methods: \n\n- (1): The proposed methodology in this paper is to use a node-wise system identification approach for reinforcement learning in robotic manipulation. The approach involves mapping the raw sensor observations to node states using the system identification technique, followed by incorporating the learned states to a value-based reinforcement learning algorithm.\n \n- (2): The node-wise system identification approach is applied to learn a low-dimensional state representation for the reinforcement learning environment. This approach simplifies the control problem and enables information processing at different levels of abstraction.\n \n- (3): The proposed approach is evaluated on a physical robot in a peg-in-hole task. The performance of the approach is compared with other state-of-the-art methods using much less training data. The evaluation results demonstrate that the node-wise system identification approach can effectively reduce the dimensionality of the reinforcement learning environment and improve the sample efficiency and generalization ability of the robot.\n\n\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant in proposing an efficient reinforcement learning approach for robotic manipulation via node-wise system identification, which can improve the adaptability and reduce the demand for extensive training data in complex robotic environments.\n\n- (2): Innovation point: The authors proposed a node-wise system identification approach to simplify the control problem and generate effective state representations. Performance: The proposed approach achieves comparable performance to other state-of-the-art methods with much less training data. Workload: The approach involves mapping the raw sensor observations to node states, which may require additional computation and time compared to hand-crafted feature extraction methods.\n\n\n",
    "GPTmethods": "- (1): The proposed methodology in this paper is to use a node-wise system identification approach for reinforcement learning in robotic manipulation. The approach involves mapping the raw sensor observations to node states using the system identification technique, followed by incorporating the learned states to a value-based reinforcement learning algorithm.\n \n- (2): The node-wise system identification approach is applied to learn a low-dimensional state representation for the reinforcement learning environment. This approach simplifies the control problem and enables information processing at different levels of abstraction.\n \n- (3): The proposed approach is evaluated on a physical robot in a peg-in-hole task. The performance of the approach is compared with other state-of-the-art methods using much less training data. The evaluation results demonstrate that the node-wise system identification approach can effectively reduce the dimensionality of the reinforcement learning environment and improve the sample efficiency and generalization ability of the robot.\n\n\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant in proposing an efficient reinforcement learning approach for robotic manipulation via node-wise system identification, which can improve the adaptability and reduce the demand for extensive training data in complex robotic environments.\n\n- (2): Innovation point: The authors proposed a node-wise system identification approach to simplify the control problem and generate effective state representations. Performance: The proposed approach achieves comparable performance to other state-of-the-art methods with much less training data. Workload: The approach involves mapping the raw sensor observations to node states, which may require additional computation and time compared to hand-crafted feature extraction methods.\n\n\n",
    "GPTconclusion": "- (1): This piece of work is significant in proposing an efficient reinforcement learning approach for robotic manipulation via node-wise system identification, which can improve the adaptability and reduce the demand for extensive training data in complex robotic environments.\n\n- (2): Innovation point: The authors proposed a node-wise system identification approach to simplify the control problem and generate effective state representations. Performance: The proposed approach achieves comparable performance to other state-of-the-art methods with much less training data. Workload: The approach involves mapping the raw sensor observations to node states, which may require additional computation and time compared to hand-crafted feature extraction methods.\n\n\n"
}