{
    "Abstract": "Abstract Reinforcement learning is about learning agent models that make the best sequential decisions in unknown environments. In an unknown environment, the agent needs to explore the environment while exploiting the collected information, which usually forms a sophisticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving sophisticated problems. It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced. Therefore, derivative-free optimization deals with a similar core issue as reinforcement learning, and has been introduced in reinforcement learning approaches, under the names of learning classi\ufb01er systems and neuroevolution/evolutionary reinforcement learning. Although such methods have been developed for decades, recently, derivative-free reinforcement learning exhibits attracting increasing attention. However, recent survey on this topic is still lacking. In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods. Moreover, we discuss some current limitations and possible future directions, hoping that this article could bring more attentions to this topic and serve as a catalyst for developing novel and ef\ufb01cient approaches. 1 ",
    "Introduction": "Introduction Reinforcement learning [Sutton and Barto, 1998; Wiering and van Otterlo, 2012] aims to enable agents to automatically learn the policy with the maximum long-term reward via interactions with environments. It has been listed as one of the four research directions of machine learning by Professor T. G. Dietterich [Dietterich, 1997]. In recent years, with the fusion of deep learning and reinforcement learning, deep reinforcement learning has made remarkable progress and attracted more and more attention from both the academic and industrial community. To name a few, the deep Q-network (DQN) [Mnih et al., 2015] proposed by DeepMind reaches 1 arXiv:2102.05710v1  [cs.LG]  10 Feb 2021 the human-level control in Atari games, AlphaGo [Silver et al., 2016] proposed also by DeepMind defeats the top human experts in the Go game, and AlphaZero [Silver et al., 2018] defeats a world champion program in the games of chess, shogi and Go without the domain knowledge except the game rules. Due to the strong ability of reinforcement learning, it has been applied in automatic control [Abbeel et al., 2006], automatic machine learning [Zoph and Le, 2017a], computer vision [Huang et al., 2017], natural language processing [Yu et al., 2017], scheduling [Wang and Usher, 2005], \ufb01nance [Choi et al., 2009], commodity search [Shi et al., 2019], and network communication [Boyan and Littman, 1993], etc. In the \ufb01eld of cognition and neuroscience, reinforcement learning also has important research value [Frank et al., 2004; Samejima et al., 2005]. However, as reinforcement learning is being applied to more realistic problems, the complexity of \ufb01nding out an optimal or satisfactory policy is also increasing. The optimization problems encountered in reinforcement learning, especially deep reinforcement learning, are often quite sophisticated. For such optimization problems, standard gradient-based optimization methods may suffer from the dif\ufb01culties of stationary point issues (e.g., a plethora of saddle points or spurious local optima), bad condition number, or \ufb02atness in the activations that could lead to the gradient vanishing problem [Shalev-Shwartz et al., 2017]. These dif\ufb01culties cannot be neglected since they could result in the unsatisfactory performance of gradient-based optimization methods. Therefore, more effective policy learning methods that could make up for the shortcomings of gradient-based ones are quite appealing. Derivative-free optimization [Conn et al., 2009; Kolda et al., 2003; Rios and Sahinidis, 2013], also termed as zeroth-order or black-box optimization, involves a kind of optimization algorithms that do not rely on the gradient information. Given a function f de\ufb01ned over a continuous, discrete, or mixed search space X, it only relies on the objective function value (or \ufb01tness value) f(x) on the sampled solution x. Since the conditions of using derivative-free algorithms are relaxed, they are easy to use and suitable for dealing with the sophisticated optimization tasks, e.g., non-convex and non-differentiable objective functions. Moreover, derivative-free optimization commonly uses a sampling-and-updating framework to iteratively improve the quality of solutions. One of the key issues is to balance the exploration and exploitation in the search space. This key issue is in a quite similar situation of reinforcement learning. Therefore, the fusion of derivative-free optimization and reinforcement learning, termed as derivative-free reinforcement learning in this paper, has many potentialities. Derivative-free reinforcement learning has been developed for decades, under the names of learning classi\ufb01er systems (e.g., [Sigaud and Wilson, 2007]) and neuroevolution/evolutionary reinforcement learning (e.g., [Moriarty et al., 1999; Whiteson, 2012]). Although it has some history, we have noticed that derivative-free reinforcement learning attracts increasing attentions recently. This article reviews some recent advances of derivative-free reinforcement learning in 2 ",
    "Background": "Background This section introduces the background of reinforcement learning and derivative-free optimization. The fusion of them could be an effective way of tackling the hard policy search problems in reinforcement learning. 2.1 Reinforcement learning Reinforcement learning (RL) [Sutton and Barto, 1998; Wiering and van Otterlo, 2012] is an important direction in machine learning [Dietterich, 1997]. It aims to enable an agent to automatically learn the policy with the maximum long-term reward via interactions with an environment. An illustration of interaction structure of reinforcement learning is shown in Figure 1. When an agent is put in an unknown environment, it is told of the action space A in which there are actions it can choose to take, and the state space S in which its observation is contained in. Both S and A can be discrete or continuous. The agent has a policy \u03c0 to determine which action a \u2208 A is chosen at a state s \u2208 S. Generally, a deterministic policy \u03c0 is a mapping from the state space S to the action space A, i.e., a = \u03c0(s); while a stochastic policy \u03c0 chooses the action a 3 Agent Environment Action State Reward Figure 1: Interaction structure of reinforcement learning. according to the probability distribution \u03c0(a|s) at the state s with the constraints \u2200a \u2208 A, \u03c0(a|s) \u2265 0 and \ufffd a\u2208A \u03c0(a|s) = 1. (1) When the agent takes one action at at a state st in time step t, the unknown environment typically responds by transiting to the next state st+1 and feeds back a reward signal. The (unknown) transition function can be represented as a distribution P(st+1|st, at), and the (unknown) reward function can be represented as rt = R(st, at). The agent explores the environment according to a policy \u03c0 by interactions with the environment. Starting from the initial state s0 \u223c \u03c10(\u00b7), where \u03c10(\u00b7) is the start state distribution, the interaction trajectory \u03c4 (also frequently termed as episode or roll-out) is s0, a0 \u223c \u03c0(\u00b7|s0), s1 \u223c P(\u00b7|s0, a0), r0 = R(s0, a0), a1 \u223c \u03c0(\u00b7|s1), . . . (2) The quality of the policy \u03c0 is then evaluated from interaction trajectories, as the expected sum up of the reward along the trajectories. Commonly, the expected total reward, or expected return, of the policy \u03c0 starting from the state s is the expectation over the policy distribution and the transition distribution, e.g., V \u03c0(s) = E \ufffd +\u221e \ufffd t=0 \u03b3trt|s0 = s \ufffd , (3) where \u03b3 \u2208 [0, 1) is the discount factor. Note that reinforcement learning setting shares the key components < A, S, P, R, \u03b3 > with Markov decision process (MDP) [Bellman, 1957], except that all the functions are known under the de\ufb01nition of an MDP. Also note that dynamic programming is a classical and effective method to solve the best policy in MDPs. There are thus two branches of approaches, modelbased ones that recover the transition distribution and reward function to form up a complete MDP for using classical solvers, and model-free ones that learn the policy without recovering the MDP. In this paper, we mainly focus on model-free approaches. Nevertheless, modelassisted model-free reinforcement learning is a promising direction. 4 In model-free reinforcement learning, the policy can be derived from well learned value functions, or can be learned directly. In either ways, the general steps of the learning approaches are the same, i.e., iterating between exploring the environment and updating the policy. The exploration is typically implemented by executing the policy with added noise, such as \u03f5-greedy and Gibbs sampling. In this way, the probability of executing every action and visiting every state is non-zero, and thus diverse trajectory data can be generated for the next step of policy update. To learn the value function, the most popular way might be the temporal difference update [Sutton and Barto, 1998], which is essentially an incremental update rule of arithmetic sum V \u03c0(st) \u2190 V \u03c0(st) + \u03b4 \u00b7 \ufffd rt + V \u03c0(st+1) \u2212 V \u03c0(st) \ufffd , (4) where \u03b4 > 0 denotes the step size. After obtaining the value function, the policy is derived simply as that taking the action of the largest value, i.e., \u03c0(s) = arg maxa Q\u03c0(s, a), where Q\u03c0(s, a) = R(s, a) + Es\u2032\u223cP(\u00b7|s,a)V \u03c0(s\u2032). V \u03c0(s) and Q\u03c0(s, a) are called value functions. The value function based methods may face the problem of policy degradation [Bartlett and Baxter, 2001]. That is to say, a more accurate estimation of the value function may not guarantee a better policy. Another kind of approach is to learn the policy directly, i.e., policy search. In policy search, the policy model is \ufb01rstly parameterized, such as softmax policy for discrete action space \u03c0\u03b8(a|s) = exp \ufffd h\u03b8(s, a) \ufffd \ufffd a\u2032\u2208A exp \ufffd h\u03b8(s, a\u2032) \ufffd, (5) and Gaussian policy for continuous action space \u03c0\u03b8(a|s) = 1 \u221a 2\u03c0\u03c3 exp \ufffd \u2212(a \u2212 h\u03b8(s))2 \u03c32 \ufffd , (6) where h\u03b8(s, a) in Eq. (5) and h\u03b8(s) in Eq. (6) with parameter \u03b8 can be linear, and are nowadays often (deep) neural network models. Note that this parametric policy model is fully differentiable with respect to (w.r.t.) \u03b8, if h\u03b8 is differentiable. Then, consider the objective of learning an optimal policy \u03c0\u2217 = \u03c0\u03b8\u2217 that can maximize the total reward J(\u03b8), i.e., \u03b8\u2217 = arg max \u03b8\u2208\u0398 J(\u03b8), (7) where \u0398 denotes the parameter space. We suppose that both the environment transitions and the policy are stochastic. In episodic environments, J(\u03b8) can be trajectory-wise total reward J(\u03b8) = E\u03c4\u223c\u03c0\u03b8[R(\u03c4)] = \ufffd T p\u03b8(\u03c4)R(\u03c4) d\u03c4, (8) 5 where R(\u03c4) = \ufffdT\u22121 t=0 rt is the total reward (or return) over a trajectory \u03c4, T is a valid trajectory space, and p\u03b8(\u03c4) = \u03c10(s0) \ufffdT\u22121 t=0 P(st+1|st, at)\u03c0\u03b8(at|st) is the probability of generating a Tstep trajectory \u03c4 according to \u03c0\u03b8 in the environment. In continuing environments, J(\u03b8) can be average reward per time-step for one-step MDPs J(\u03b8) = \ufffd S d\u03c0\u03b8(s) \ufffd A \u03c0\u03b8(a|s)R(s, a) da ds, (9) where d\u03c0\u03b8(s) is the stationary probability of visiting the state s following the policy \u03c0\u03b8. We can \ufb01nd that the total reward objective J(\u03b8) is also differentiable w.r.t. \u03b8. A straightforward idea of solving the parameter is to follow the gradient \u2207\u03b8J(\u03b8), which is generally called as policy gradient method. Affected by the deep learning, \ufb02exible and capable deep neural network models are introduced in reinforcement learning. It can be noticed that, although the objective J(\u03b8) is differentiable, the objective function is not simple, particularly when deep neural network models are employed. How to best optimize the objective function is still an open problem with continual progress. 2.2 Derivative-free optimization Optimization, x\u2217 = arg maxx\u2208X f(x) as a general representative, plays a fundamental role in machine learning. With the rapid development of machine learning, it has deeper applications in a broader and more complex scenario. Due to the increasing complexity of machine learning application problems, optimization methods for complex and hard problems are attracting more and more attention from researchers. For complex optimization problems (e.g., non-convex, non-smooth, non-differential, discontinuous, and NP-hard, etc.), gradient-based methods may fall into the local optima or even lose their power, and result in the unsatisfactory performance. In a search space, the objective of optimization is to \ufb01nd the solution with the extreme function value. A general principle of optimization is simply that, given the currently accessed solutions, which can be randomly sampled at the initialization, \ufb01nd the next solution with better function value. For example, gradient ascent method \ufb01nds the next solution follows the gradient direction of the objective function. This procedure requires \ufb01rstly that the objective function is differentiable, and more importantly that there are few local optima and saddle points, so that the gradient direction is informative and can lead to better solutions. Derivative-free optimization (DFO) [Conn et al., 2009; Kolda et al., 2003; Rios and Sahinidis, 2013], also termed as zeroth-order or black-box optimization, \ufb01nds the next solutions in another way. It covers many families of optimization algorithms that do not rely on the gradient. It only relies on the function values (or \ufb01tness values) f(x) on the sampled solution x. Most derivative-free optimization algorithms share a common structure. They \ufb01rstly initialize from 6 some random solutions in the search space. From the accessed solutions, derivative-free optimization algorithms build a model, either explicitly or implicitly, about the underlying objective function. The model could imply an area that contains some potential better solutions. They then sample new solutions from that model and update the model. Derivative-free optimization methods repeat this sampling-and-updating procedure to iteratively improve the quality of solutions. To sum up, the general framework of derivative-free optimization methods involve some key steps below: 1. Randomly sample solutions; 2. Evaluate the objective function value of the sampled solutions; 3. Update the model from the sampled solutions and their function values; 4. Sample new solutions according to the model with a designed mechanism; 5. Repeat from step 2 until some termination conditions are satis\ufb01ed; 6. Return the best found solution and its function value. The termination conditions usually include that the function evaluation budget is exhausted or the goal optimal function value has been reached. Representative derivative-free algorithms include evolutionary algorithms [Holland, 1975; Hansen et al., 2003], Bayesian optimization [Shahriari et al., 2016], cross-entropy method [de Boer et al., 2005], deterministic or stochastic optimistic optimization [Munos, 2014], and classi\ufb01cation-based optimization [Yu et al., 2016], etc. So far, the substantial progress has been made in both the theoretical analysis tools and theoretical guarantees of derivative-free optimization [He and Yao, 2001; Yu and Zhou, 2008; Bull, 2011; Jamieson et al., 2012; Munos, 2014; Yu and Qian, 2014; Duchi et al., 2015; Yu et al., 2015; Kawaguchi et al., 2015; Yu et al., 2016; Kawaguchi et al., 2016]. Some fundamental and crucial concerns in theory, such as the global convergence rate and the function class that can be optimized ef\ufb01ciently, are disclosed progressively. In order to take a deeper insight of the key step 3 and step 4 in the above procedure, we present a snapshot of a canonical genetic algorithm [Holland, 1975; Mitchell, 1998] as an example. Genetic algorithms belong to the family of evolutionary algorithms. Consider maximizing a pseudo-Boolean function arg maxx\u2208{0,1}d f(x). Here the solutions are represented as bit strings of length d. A canonical genetic algorithm maintains a population which includes n solutions, and the model in each iteration/generation is represented by the population (i.e., n solutions in each population). In each iteration, n children (or offspring) are produced on the basis of n parent solutions via the designed mechanism called crossover operator from the parents, e.g., 7 single-point crossover which is shown below. We set d = 5 for illustration. xparent 1 = (0, 1, 0, 0, 1) xparent 2 = (1, 0, 1, 0, 0) crossover \u2212\u2212\u2212\u2212\u2192 xchild 1 = (0, 1, 1, 0, 0) xchild 2 = (1, 0, 0, 0, 1) (10) In the above single-point crossover, a point (or bit) on both parents\u2019 bit strings is picked randomly, and bits to the right of that point are swapped between the two parents, which results in two children. Besides, mutation is another variation operator in the designed mechanism. If mutation takes place, it can be applied to each child that has been produced by crossover. One way of realizing mutation could be \ufb02ipping each bit in a solution xchild with some probability and producing a new solution xchild new , which is shown below. We set d = 5 for illustration. xchild = (0, 1, 1, 0, 0) mutation \u2212\u2212\u2212\u2212\u2192 xchild new = (0, 1, 1, 1, 0) (11) After the crossover and mutation operators (step 4), the objective function values of these newly produced solutions are evaluated (step 2). Then, the model, which is represented by the solutions, is updated via selecting the best n solutions from both the parents and children to form the next generation of population (step 3). From the above algorithmic procedure, some key characteristics of derivative-free optimization algorithms can be observed. Firstly, they can be utilized as long as the quality of solutions can be evaluated. Secondly, the designed mechanisms for sampling solutions and rules for updating model always consider the balance between exploration and exploitation. In optimization, exploration intuitively means gathering more information about objective functions and reducing some uncertainty, while exploitation means choosing the best solutions under current information. In the instance of canonical genetic algorithm, exploration is realized via crossover and mutation operators. The combination of exploration and exploitation could help optimization procedures maintain global and local search, and jump out of the local optima in order to \ufb01nd out the (approximately) global optima with high probability. Thirdly, many derivativefree optimization algorithms are population-based. A population of solutions is maintained and improved iteratively, and the algorithms could share and leverage the information across a population. These key characteristics make derivative-free optimization methods have a low barrier to use as well as the ability of search globally. Since the conditions of using derivative-free optimization methods are relaxed, they are easy to use and general in the continuous, discrete, or mixed search space. Their ability could guarantee the effectiveness of global optimization. Therefore, derivative-free optimization methods are suitable for dealing with the complex and hard optimization problems. They have been applied in the complex learning tasks and achieved impressive empirical results, such as policy search in reinforcement learning [Taylor et al., 2006; Abdolmaleki et al., 2015; Hu et al., 2017; Salimans et al., 2017], automatic machine learning and hyper-parameter tuning [Snoek et al., 2012; 8 Thornton et al., 2013; Real et al., 2017, 2018], objective detection in computer vision [Zhang et al., 2015], subset selection [Qian et al., 2015, 2017], and security games [Brown et al., 2014], etc. 3 When RL meets DFO After a brief recap of reinforcement learning (RL) and derivative-free optimization (DFO), this section explains and emphasizes that the fusion of them, termed as derivative-free reinforcement learning, is necessary and attractive. We explain it from the aspects of optimization, exploration, and computation, respectively. Optimization. A learning task typically involves three components, and they are representation, evaluation, and optimization [Domingos, 2012]. The ability of optimization method has a signi\ufb01cant impact on the complexity of model representation and the type of evaluation function that we can choose for learning. Nowadays, in reinforcement learning, the policy or value function models are often represented by deep neural networks, i.e., deep reinforcement learning. When injecting this sophisticated deep representation into the evaluation function in RL (e.g., Eq. (8) or Eq. (9) in Section 2.1), the resulting optimization problems (e.g., Eq. (7) in Section 2.1) could be non-convex. For such optimization problems, standard gradient-based methods may suffer from the dif\ufb01culties of stationary point issues (e.g., a plethora of saddle points or spurious local optima), bad condition number, or \ufb02atness in the activations that could lead to the gradient vanishing problem [Shalev-Shwartz et al., 2017]. And these dif\ufb01culties could result in the unsatisfactory results. At the same time, derivative-free methods that conduct optimization from samples provide another way of policy learning, and can be complementary with gradient-based ones in reinforcement learning. One straightforward way of applying derivative-free optimization methods is to de\ufb01ne the search space as the policy parameter space and the objective function as the expected long-term reward. Namely, X def = \u0398 and f(x) def = J(\u03b8). For policy learning, derivativefree optimization methods have their own merits of being able to search parameters globally and being easy to train. They do not perform gradient back-propagation, do not care whether rewards are sparse or dense, do not care the length of time horizons, and do not need value function approximation [Salimans et al., 2017]. In Section 4 and 5, we will discuss the recent advances dedicated to the derivative-free model parameter updating as well as model selection in reinforcement learning, respectively. Exploration. Almost all reinforcement learning methods share the exploration-learning framework [Yu, 2018]. Namely, an agent explores and interacts with an unknown environment to learn the optimal policy that maximizes the total reward from the exploration sam9 ples. Generally speaking, the exploration samples involve states, state transitions, actions and rewards. From the exploration samples, the quality of a policy can be evaluated by rewards, and the learning step updates the policy or value function models from the evaluations. This exploration-learning procedure is repeated until some termination conditions are met. Exploration is necessary in reinforcement learning. Because achieving the best total reward on the current trajectory samples is not the ultimate goal, and the agent should visit the states that have not been visited before so as to collect better trajectory samples. This means that the agent should not follow the current policy tightly, and thus the exploration mechanisms need to encourage the agent to deviate from the previous trajectory paths properly. Most existing exploration mechanism mainly suffer from being memoryless and blind, e.g., action space noise or parameter space noise [Plappert et al., 2018], or being dif\ufb01cult to use in real state/action spaces, e.g., curiosity-driven exploration [Pathak et al., 2017]. On the other hand, many mainstream policy gradient methods, such as truncated natural policy gradient (TNPG) [Duan et al., 2016a] and trust region policy optimization (TRPO) [Schulman et al., 2015], seldom touch the exploration. Meanwhile, derivative-free reinforcement learning is naturally equipped with the exploration strategies. Because in the search process of derivative-free optimization methods, the designed mechanisms for sampling solutions and rules for updating model always consider the exploration. Therefore, derivative-free optimization methods can take part of the duty of exploration for reinforcement learning when updating the policy or value function models from samples. Recently, some problem-dependent derivative-free exploration methods that could improve the sample ef\ufb01ciency have been proposed. In Section 6, we will discuss these works dedicated to the derivative-free exploration in reinforcement learning. Computation. Although derivative-free methods could bring some good news to reinforcement learning with respect to optimization and exploration, they mostly suffer from low convergence rate. Derivative-free optimization methods often require to sample a large amount of solutions before convergence, even if the objective function is convex or smooth [Jamieson et al., 2012; Duchi et al., 2015; Bach and Perchet, 2016]. And the issue of slow convergence becomes more serious as the dimensionality of a search space increases [Duchi et al., 2015; Qian and Yu, 2016]. Obviously, this issue will block the further application of derivative-free methods to reinforcement learning. They sample a lot of policy parameters before \ufb01nding out an optimal or satisfactory one, and the quality of policy parameters is evaluated by the trajectory samples. This makes reinforcement learning more sample inef\ufb01cient. Fortunately, many derivative-free optimization methods are population-based. That is to say, a population of solutions is maintained and improved iteratively. This characteristic makes them highly parallel. Thus, derivative-free optimization methods can be accelerated by parallel or distributed computation, which alleviates their slow convergence. Furthermore, for paral10 lel/distributed derivative-free methods, the data communication cost is lower compared with gradient-based ones, since only scalars (\ufb01tness values) instead of gradient vectors or Hessian matrices need to be conveyed. This merit further compensates for the low convergence rate partly. In Section 7, we will discuss the recent works dedicated to the parallel/distributed derivative-free reinforcement learning. DFRL Optimization (Sec. 4&5) Exploration (Sec. 6) Computation (Sec. 7) Parameter Updating (Sec. 4) Model Selection (Sec. 5) Figure 2: The organization of the works reviewed in the article. To sum up, derivative-free reinforcement learning could hopefully result in more effective and powerful algorithms for complex control tasks, more sample ef\ufb01cient exploration in environments, and more time ef\ufb01cient global optimization for better policy. The organization of the following reviewed works is depicted in Figure 2. We should stress that derivative-free optimization methods are not proposed to replace gradient-based ones, e.g, policy gradient algorithms, in reinforcement learning, and they are complementary with each other. In fact, some impressive works that will be discussed in this article are hybrids of derivative-free and gradient-based ones. 4 Derivative-free model parameter updating in reinforcement learning The generic framework of using derivative-free optimization algorithms to update the model parameters in reinforcement learning is quite straightforward. After parameterizing the policy or value function models, the quality of a parameter \u03b8 is evaluated via the total reward J(\u03b8) provided by an environment. For policy search methods in deep reinforcement learning, the parameters \u03b8 of a policy model \u03c0\u03b8 are the weights of a deep neural network. Here the policy model can be softmax policy for discrete action space or Gaussian policy for continuous action space, and this section only considers that the architecture/topology of a (deep) neural network is \ufb01xed. Derivative-free algorithms regard \u03b8 and J(\u03b8) as solutions and objective function values 11 (or \ufb01tness values) respectively, and search the optimal solution \u03b8\u2217. They sample different policy parameters, and learn where to sample in the next iteration. The works dedicated to applying derivative-free methods to optimize the weights of neural networks have been developed for decades, e.g., neuroevolution. In neuroevolution, evolutionary algorithms, inspired from natural evolution, can not only optimize the weights, but also search the topology of neural networks (discussed in Section 5). In each generation of a neuroevolutionary algorithm, each neural network in the population is evaluated by the task, and the best ones are selected. The crossover and mutation operators are used to reproduce the new networks from the selected ones in order to form a new population, and the process iterates. A comprehensive discussion of neuroevolution can be found from the last century review paper [Yao, 1999] or the recent one [Stanley et al., 2019]. The methods of applying neuroevolution for reinforcement learning tasks have been developed for decades. Recently, more and more works have shown that, compared with gradientbased methods, neuroevolution is competitive for not only policy search in RL [Salimans et al., 2017; Such et al., 2017], but also supervised learning tasks [Morse and Stanley, 2016; Zhang et al., 2017]. This con\ufb01rms the power of neuroevolution and renews the increasing interests in it. In addition to the success of neuroevolution RL [Koutn\u00b4\u0131k et al., 2013; Hausknecht et al., 2014; Salimans et al., 2017; Risi and Togelius, 2017; Chrabaszcz et al., 2018], other derivative-free optimization methods for RL could also be promising. For instance, stochastic zeroth-order search could rival the gradient-based methods for the static linear policy optimization on the MuJoCo locomotion tasks [Mania et al., 2018]. And its convergence behavior is analyzed in [Malik et al., 2019]. In this section, we mainly review the recent progress in derivative-free model parameter updating in reinforcement learning, and the involved works include but are not only restricted to neuroevolution RL. For the early works on evolutionary RL, the survey of them can be found in [Moriarty et al., 1999; Whiteson, 2012]. 4.1 Evolution strategies based model parameter updating Evolution strategies (ESs) [Hansen et al., 2015] belong to the family of evolutionary algorithms inspired from natural evolution. In ESs, a parameterized search distribution (e.g., Gaussian or Cauchy distribution) is maintained, and solutions are sampled from this search distribution and then are evaluated by an objective function. The search distribution is iteratively updated according to the evaluated solutions. Mutation is a commonly used variation operator in ESs, and it perturbs solutions in order to generate the new ones. Mutation introduces the diversity of solutions in the population, and may alleviate the problem that optimization algorithms are trapped into the local optima. Due to the different algorithmic implementations, ESs have many 12 variants, and one of the most popular among them might be the covariance matrix adaptation evolution strategy (CMA-ES) [Hansen and Ostermeier, 1996, 2001; Hansen et al., 2003]. In CMA-ES, the search distribution is a multivariate Gaussian, and its covariance matrix adapts over time. The mutation ellipsoids of CMA-ES are not restricted to be axis-parallel, and the overall step size is controlled with the cumulative step size adaptation. Heidrich-Meisner and Igel [Heidrich-Meisner and Igel, 2008, 2009b] proposed to optimize the parameterized policy in RL via CMA-ES. They consider to apply CMA-ES because it is robust (ranking policies based), can detect the correlations among parameters, and can infer the search direction from the scalar signals in RL. In [Heidrich-Meisner and Igel, 2008], CMAES is used to optimize linear policies (e.g., \u03c0\u03b8(s) = \u03b8\u22a4s for the deterministic policies) on the double cart-pole balancing task. Empirical results show that, compared with the episodic natural actor-critic algorithm (NAC) [Peters and Schaal, 2008] which is a gradient-based one, CMA-ES is more robust with respect to noise, policy initialization, and hyper-parameters. On the other hand, NAC could surpass CMA-ES with respect to learning speed under appropriate policy initialization and hyper-parameters. In [Heidrich-Meisner and Igel, 2009b], CMA-ES is used to search neural network policies on the Markovian and non-Markovian variants of the pole balancing task. Compared with the policy gradient methods, value function based methods, random search, and several neuroevolution methods, overall, the empirical performance of CMA-ES is superior. Also, Heidrich-Meisner and Igel [Heidrich-Meisner and Igel, 2009a] enhanced CMA-ES for direct parameterized policy search with an adaptive uncertainty handling mechanism. On the basis of this mechanism, the resulting Race-CMA-ES can dynamically adapt the number of roll-outs for evaluating the parameterized policies such that the ranking of solutions is just reliable enough to push the learning process. Empirical results on the mountain car and swimmer control tasks with linear policies show that, compared with CMA-ES and NAC [Peters and Schaal, 2008], Race-CMA-ES can accelerate the learning process and improve the algorithmic robustness. Stulp and Sigaud [Stulp and Sigaud, 2012] were the \ufb01rst to make the relationship between CMA-ES and the cross-entropy (CE) method [de Boer et al., 2005] explicit under the view of probability-weighted averaging. The ef\ufb01cacy of CE for playing Tetris has been studied in [Szita and L\u00a8orincz, 2006]. They claim that CE can be regarded as a special case of CMAES through setting some CMA-ES parameters to extreme values. Besides, they also inject the covariance matrix adaptation into the policy improvement with path integrals, and result in PI2CMA. PI2-CMA shares the similar way of performing the parameter updating with CMA-ES and CE, but has the more consistent convergence behavior under varying initial conditions. Wierstra et al. [Wierstra et al., 2008, 2014] presented the natural evolution strategies (NESs) where the natural gradient was used to update a parameterized search distribution in the direction of higher expected \ufb01tness. The effectiveness of NESs is empirically veri\ufb01ed on a neu13 roevolutionary control policy design for the non-Markovian double pole balancing task. And the result shows that NESs possess the merits of alleviating oscillations and premature convergence. Salimans et al. [Salimans et al., 2017] from OpenAI proposed to use a simple evolution strategy, based on a simpli\ufb01cation of NESs, to directly optimize the weights \u03b8 of policy neural networks. After initializing the policy parameters \u03b80, OpenAI ES stochastically mutates the parameters \u03b8 of the policy with Gaussian distribution, and evaluates the resulting mutated parameters via the total reward/return J(\u00b7). Then, it combines these returns and updates the parameters \u03b8. This process iterates till the termination condition is met. Let \u03b1 > 0 denote the learning rate, and \u03c3 denote the noise standard deviation. The core steps are summarized as follows: 1. Randomly sample \u03f51, . . . , \u03f5n from the standard Gaussian distribution N(0, I); 2. Compute returns Ji = J(\u03b8t + \u03c3\u03f5i) for i = 1, . . . , n; 3. Update parameters \u03b8t+1 \u2190 \u03b8t + \u03b1 1 n\u03c3 \ufffdn i=1 Ji\u03f5i; 4. Repeat from step 1 till the termination condition is met. Furthermore, some techniques are integrated to facilitate the success of OpenAI ES for learning policy neural networks. To name a few, virtual batch normalization [Salimans et al., 2016] is adopted to enhance the exploration ability, antithetic sampling [Geweke, 1988] (also known as mirrored sampling [Brockhoff et al., 2010]) is adopted to reduce variance, and \ufb01tness shaping [Wierstra et al., 2014] is adopted to decrease the trend of trapping into the local optima in the early training phase. This simple and generic OpenAI ES is very easy to parallelize and only takes low communication cost, which will be discussed in Section 7. The effectiveness of OpenAI ES is empirically investigated on some simulated robotics tasks in MuJoCo and Atari games in OpenAI Gym [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016]. And the results surprisingly show that OpenAI ES is comparable to some state-of-theart gradient-based algorithms, e.g., trust region policy optimization (TRPO) [Schulman et al., 2015] and asynchronous advantage actor-critic (A3C) [Mnih et al., 2016], on both simple and hard environments, but OpenAI ES needs more samples. Notably, this work [Salimans et al., 2017] seems to renew the increasing interests in (deep) neuroevolution RL. Encouraged and inspired by [Salimans et al., 2017], new ideas, discussions and approaches are blooming out. Lehman et al. [Lehman et al., 2018a] claimed that the simple OpenAI ES in [Salimans et al., 2017] was more than just a \ufb01nite-difference approximation of the reward gradient. Because OpenAI ES optimizes the average return of the whole solutions in the population instead of a single solution, it searches parameters that are robust to perturbation in the parameter space and yields more stable policies. Chrabaszcz et al. [Chrabaszcz et al., 2018] 14 compared a simper and basic canonical ES algorithm with OpenAI ES [Salimans et al., 2017]. The empirical results on a subset of 8 Atari games [Bellemare et al., 2013; Brockman et al., 2016] show that this simpler canonical ES is able to match or even surpass the performance of OpenAI ES. This work [Chrabaszcz et al., 2018], on the one hand, further con\ufb01rms that the power of ES-based model parameter updating for policy search may rival that of the gradientbased algorithms. On the other hand, it indicates that the ES-based RL algorithms could be further ameliorated in many aspects by integrating the recent advances made in the \ufb01eld of ES. Choromanski et al. [Choromanski et al., 2018] enhanced ES-based RL model parameter updating by structured evolution and compact policy networks. The resulting algorithm needs less policy neural network parameters, and thus could speed up training and inference. Chen et al. [Chen et al., 2019b] improved ES for training policy neural networks in terms of the mutation strength adaptation and principal search direction update rules. And the number of elitists adaptation and restart procedure are also integrated to tackle the local optima. The proposed algorithm is of linear time complexity and low space complexity, and thus possesses the merit of scalability. Choromanski et al. [Choromanski et al., 2019] applied the ideas of active subspaces [Constantine, 2015], which is one of the popular approaches to dimensionality reduction, to yield the sample-ef\ufb01cient and scalable ES for policy optimization in RL. Liu et al. [Liu et al., 2019] boosted the sample ef\ufb01ciency of ES for RL by reusing the existing sampled data, and proposed the trust region evolution strategies (TRES) algorithm. TRES realizes sample reuse for multiple epochs of updates in the way of iteratively optimizing a surrogate objective function. Tang et al. [Tang et al., 2019] proposed a variance reduction technique for ES-based RL model parameter updating. It utilizes the underlying MDP structure of RL through problem re-parameterization and control variable construction. Fuks et al. [Fuks et al., 2019] proposed a technique called progressive episode lengths (PEL), and injected it into a canonical ES [Chrabaszcz et al., 2018] to result in PEL-ES. Inspired from transfer learning and curriculum learning, PEL-ES \ufb01rstly lets an agent play the easy and short tasks, and then applies the gathered knowledge to further deal with the harder and longer tasks. Compared with the canonical ES [Chrabaszcz et al., 2018], PEL-ES is superior in optimization speed, total rewards, and stability. Houthooft et al. [Houthooft et al., 2018] proposed a meta-learning method for policy learning across different tasks called evolved policy gradient (EPG). EPG encodes the prior knowledge implicitly through a parametrized loss function, and parametrization is realized by temporal convolutions over the agent\u2019s experience. The agent can use the learned loss function to learn on a new task quickly. Thus, the main procedure is to evolve a parametrized and differentiable loss function. The agent optimizes its policy to minimize this loss function so as to gain high returns. To implement this procedure, EPG involves two optimization loops: \u2022 In the inner loop, the agent learns to solve a task through minimizing a loss function 15 provided by the outer loop. \u2022 In the outer loop, the parameters of a loss function are optimized in order to maximize the returns gained after the inner loop. Figure 3: Figure from [Houthooft et al., 2018] that illustrates the high-level procedure of EPG. The inner loop is optimized by the stochastic gradient descent (SGD) method. For the outer loop, since the returns are not explicit functions of the loss function parameters, ES can be applied to optimize the parameters of this loss function. Figure 3 illustrates the main procedure of EPG. Empirical results on several continuous control tasks in MuJoCo [Todorov et al., 2012; Brockman et al., 2016] show that, compared with proximal policy optimization (PPO) [Schulman et al., 2017] that is an off-the-shelf policy gradient algorithm, EPG can generate a learned loss function which trains the agent faster. Besides, EPG also exhibits generalization properties for out-of-distribution test time tasks that surpass other meta-learning algorithms RL2 [Duan et al., 2016b] and MAML [Finn et al., 2017]. Notably, in EPG, ES is used to optimize the parameters of a loss function instead of policy parameters. And the success of EPG largely owes to the hybrid of derivative-free and gradient-based algorithms. The similar scenario also happens in [Ha and Schmidhuber, 2018; Yu et al., 2019], where CMA-ES and gradient-based algorithms are mixed for policy transfer from simulation to reality. 4.2 Genetic algorithms based model parameter updating Similar to evolution strategies (ESs), genetic algorithms (GAs) [Mitchell, 1998] also belong to the family of evolutionary algorithms inspired from natural evolution. Genetic algorithms maintain a population of solutions. Via the operators of mutation, crossover (also called recombination) and selection, a population of solutions can be evolved. One of the main differences between GAs and ESs is that the crossover operator is often used in GAs. Crossover combines two parents to generate new offspring, and the newly generated solutions are usually mutated before adding them to the population. Notably, the crossover operator in GAs could further improve the diversity of solutions in the population. 16 To verify whether GAs can also be effectively applied to the RL tasks, Such et al. [Such et al., 2017] used a very simple GA to update the weights of deep policy neural networks. It iteratively maintains a population of parameter vectors \u03b8, and the mutation operator is realized by the additive Gaussian noise to \u03b8. The elitism technique is employed during evolution. For simplicity, it does not consider crossover. Due to the huge amount of parameters in deep neural networks and the relatively unsatis\ufb01ed scalability of GA, the work proposes an approach to storing large parameter vectors \u03b8 compactly via representing each \u03b8 as an initialization seed plus the list of random seeds. This compact representation approach is reversible (i.e., each parameter vector \u03b8 can be reconstructed from it), and substantially improves the scalability and ef\ufb01ciency of deep GA. The policy neural networks with more than four million free parameters can be successfully evolved. Besides, novelty search (NS) [Lehman and Stanley, 2011] is integrated into this deep GA to avoid the local optima and encourage exploration. And a distributed version of the deep GA on many CPUs across many machines is implemented for acceleration. The experimental results on the Atari and MuJoCo humanoid locomotion tasks [Bellemare et al., 2013; Todorov et al., 2012; Brockman et al., 2016] indicate that the deep GA is effective and competitive. Overall, it performs roughly as well as DQN [Mnih et al., 2015], A3C [Mnih et al., 2016], and OpenAI ES [Salimans et al., 2017]. Gangwani and Peng [Gangwani and Peng, 2018] proposed a genetic policy optimization (GPO) method for sample-ef\ufb01cient deep policy optimization. GPO involves crossover, mutation, and selection. For crossover, instead of directly exchanging the parameter representations of two parents (parameter crossover) that may destroy the hierarchical relationship of the neural networks and lead to a catastrophic drop in performance, GPO employs imitation learning to realize policy crossover in the state space. The state-space crossover operator combines two parent policies \u03c0x and \u03c0y to produce an offspring (or child) policy \u03c0c that shares the same network architecture as parents via two steps, as shown in Figure 4. Firstly, it trains a two-level policy \u03c0H, and \u03c0H(a|s) = \u03c0S(parent = x|s) \u00b7 \u03c0x(a|s) + \u03c0S(parent = y|s) \u00b7 \u03c0y(a|s), (12) where \u03c0S is a binary policy that trains from the parents\u2019 trajectories. Given a state s, \u03c0H selects between the parent policies \u03c0x and \u03c0y, and then outputs the action distribution of the selected parent. Secondly, the trajectories generated from the expert policy \u03c0H is used as the supervised data, and the offspring policy \u03c0c is trained from the supervised data by imitation learning. To avert the compounding errors introduced by the state distribution mismatch between expert and offspring, the dataset aggregation (DAgger) algorithm [Ross et al., 2011] is used for imitation learning. This sample-ef\ufb01cient crossover can distill the knowledge from parents to produce a child that aims to imitate its best parent in generating similar state visiting distributions. For mutation, instead of utilizing the commonly-used Gaussian perturbation, GPO mutates the policy network weights by randomly rolling out trajectories and performing several iterations of 17 Parent-x Parent-y !\"($|&) !(($|&) !) $ & = !+ parent = x & !\" $ & + 4!+ parent = 5 & !(($|&) ? Binary Policy !+(parent = 6|&) Offspring !7 Imitation Learning Figure 4: An illustration of combining the parent policies to produce an offspring policy in GPO [Gangwani and Peng, 2018]. a policy gradient algorithm using these roll-out trajectories. GPO chooses PPO [Schulman et al., 2017] and advantage actor-critic (A2C) algorithm [Sutton and Barto, 1998; Mnih et al., 2016] to ful\ufb01ll the policy gradient mutation. This mutation operator possesses the merits of high ef\ufb01ciency, suf\ufb01cient genetic diversity, and good exploration of state space. For selection, GPO takes both performance and diversity into consideration. The experimental results on some continuous control tasks in MuJoCo [Todorov et al., 2012; Brockman et al., 2016] show that, compared with PPO and A2C which are state-of-the-art policy gradient methods, GPO could be superior in terms of episode reward and sample ef\ufb01ciency. Bodnar et al. [Bodnar et al., 2020] also made progress in developing the variation operators in GAs tailored to policy optimization. They observe that, when the direct genetic encoding for deep neural networks (i.e., the weights of a network are recorded as a list of real numbers) meets the traditional variation operators in GAs, the negative side-effects may occur in RL since deep neural networks are sensitive to the small variation of weights. For instance, the recently proposed evolutionary reinforcement learning (ERL) framework [Khadka and Tumer, 2018], which combines both of them, has the risks of catastrophic forgetting and destructive behaviors, and does not fully address the scalability problem of GAs for RL. Therefore, they propose the learning-based Q-\ufb01ltered distillation crossover, the safe mutation [Lehman et al., 2018b] based proximal mutation , and then integrate them into ERL [Khadka and Tumer, 2018] in a hierarchical manner to result in the proximal distilled evolutionary reinforcement learning (PDERL) framework. PDERL could compensate for the simplicity of the direct genetic en18 coding, satisfy the functional requirements of the genetic variation operators when applied to the directly encoded deep neural networks, and prevent the catastrophic forgetting of parental behaviors. Compared with PPO [Schulman et al., 2017] and twin delayed deep deterministic policy gradients (TD3) algorithm [Fujimoto et al., 2018], as well as ERL, PDERL is superior when evaluated on \ufb01ve robot locomotion tasks in OpenAI Gym [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016]. Notably, in GPO and PDERL, the gradient-based update is injected into GAs to enhance the genetic variation operators. The success of them implies that, by fusing derivative-free and gradient-based algorithms in a clever way, the strengths of both sides could be absorbed, and more powerful policy search algorithms would be expected. 4.3 Bayesian optimization based model parameter updating Bayesian optimization (BO) [Shahriari et al., 2016] is also a kind of widely-used black-box derivative-free approaches aimed at optimizing the dif\ufb01cult functions with relatively few evaluations. BO constructs a probabilistic model for the function being optimized, and then exploits this model to make decisions about the next solution point of the function to evaluate. The newly evaluated solution and the previously evaluated ones are all used to update the probabilistic model so as to improve its accuracy, and this procedure iterates to sample the solutions with increasing quality. There are two ingredients that need to be speci\ufb01ed in BO, i.e., the probabilistic prior over functions that expresses the assumptions of the function being optimized, and the acquisition function that determines the next solution to evaluate. For the probabilistic prior, BO usually uses the Gaussian process (GP) [Rasmussen and Williams, 2006] prior because of its \ufb02exibility and analytical tractability. GP de\ufb01nes a distribution over functions speci\ufb01ed by its mean function and covariance function. The function being optimized is assumed to be drawn from a GP prior, and this prior as well as the sampled data induce a posterior over functions. The acquisition function a(\u00b7), that is constructed from the model posterior, determines which solution should be evaluated next via a proxy optimization \u03b8next = arg max\u03b8 a(\u03b8). The popular choices of acquisition function are probability of improvement [Kushner, 1964], expected improvement [Mo\u02c7ckus et al., 1978], GP upper con\ufb01dence bound [Srinivas et al., 2010; de Freitas et al., 2012] and so on. As BO can exploit the prior information about the expected return and utilize this knowledge to select new policies to execute, more and more works focus on applying BO to RL, and early works on this direction is already reviewed in [Brochu et al., 2010]. Wilson et al. [Wilson et al., 2014] proposed a novel Gaussian process covariance function to measure the similarity between policies using the trajectory data generated from policy executions. Compared with the traditional covariance functions that relate the policy parameters, the proposed covariance function that relates the policy behavior is more reasonable and effective. Furthermore, they also introduce a novel Gaussian process mean function that exploits 19 the learned transition and reward functions to approximate the landscape of the expected return. The developed Bayesian optimization approach tailored to reinforcement learning could recover from model inaccuracies when good transition and reward models cannot be learned. Empirical results on a set of classic control benchmarks verify its effectiveness and ef\ufb01ciency. Calandra et al. [Calandra et al., 2014] applied Bayesian optimization to design the gaits and the corresponding control policies of a bipedal robot. Three popular acquisition functions, as well as the effect of \ufb01xed versus automatic hyper-parameter selection, are analyzed therein. In [Calandra et al., 2016], they additionally formalize the problem of automatic gait optimization, discuss some widely-used optimization methods, and extensively evaluate Bayesian optimization on both simulated tasks and real robots. The evaluation demonstrates that BO is very suitable for robotic applications since it could search a good set of gait parameters with only a few amount of experimental trials. By comparing the different variants of BO algorithms, they observe that the GP upper con\ufb01dence bound acquisition function performs the best among them. Marco et al. [Marco et al., 2017] proposed a Bayesian optimization algorithm that can adaptively select among multiple information sources with different accuracies and evaluation costs, e.g., experiments on a real-world robot and simulators. This BO algorithm exploits the prior knowledge from simulations via maximizing the information gain from each experiment, and automatically integrates the cheap but inaccurate information from simulations with the accurate but expensive real-world experiments in a cost-effective way. The empirical results show that using the prior model information from simulators can reduce the amount of data required to \ufb01nd out the desirable control policies. Letham and Bakshy [Letham and Bakshy, 2019] augmented the on-line experiments with the off-line simulator observations to tune the live machine learning systems on the basis of the multi-task Bayesian optimization [Swersky et al., 2013]. The empirical study on the live machine learning systems indicates that, by directly utilizing a simple and biased off-line simulator together with a small number of on-line experiments, the proposed method can accurately predict the on-line outcomes and achieve the substantial gains. And the empirical result is consistent with the theoretical \ufb01ndings of the multi-task Gaussian process generalization. The work termed as the Bayesian functional optimization (BFO) [Vien et al., 2018] extends the BO methods to the functional policy representations, and its motivation is similar to [Vien et al., 2017]. BFO models the function space as a reproducing kernel Hilbert space (RKHS) [Rasmussen and Williams, 2006], and introduces an ef\ufb01cient update of the functional GP as well as a simple optimization of the acquisition functional. BFO bypasses the problem of manually selecting the features used in the function approximations to de\ufb01ne the parameter space, and thus relaxes the performance reliance on the selected parameter space. The experimental result on the RL task whose policies are modeled in RKHS shows that BFO is able to compactly 20 represent the complex solution functions. Eriksson et al. [Eriksson et al., 2019] proposed the trust region Bayesian optimization (TuRBO) method in order to tackle the high-dimensional RL problems. They notice that the dif\ufb01culty of high-dimensional optimization in RL comes from the plentiful local optima and the heterogeneity of the objective function (e.g., the sparse rewards in RL may lead to the objective function being nearly constant in a large region of the search space). This dif\ufb01culty makes the task of learning a global surrogate model challenging. Thus, TuRBO maintains a collection of simultaneous local probabilistic models. Each local surrogate model shares the same bene\ufb01ts with Bayesian probabilistic modeling, and at the same time enables the heterogeneous modeling of the objective function. In TuRBO, a multi-armed bandit strategy is adopted to globally allocate the samples among the trust regions. The empirical results show that TuRBO outperforms the compared BO, EAs, and stochastic optimization on the RL benchmarks. 4.4 Classi\ufb01cation based model parameter updating The classi\ufb01cation-based derivative-free optimization methods [Lozano et al., 2006; Yu et al., 2016; Hashimoto et al., 2018; Zhou et al., 2019] are recently proposed for non-convex functions with sample-complexity guarantees. The researchers notice that many derivative-free optimization methods are model-based, e.g., BO employs GP to model the function. The modelbased optimization approaches learn a model from the evaluated solutions, and the model is then utilized to guide the sampling of solutions in the next iteration. The classi\ufb01cation-based derivative-free optimization methods use a particular type of model, classi\ufb01cation model from machine learning, to model the objective function. A classi\ufb01cation model learns to classify the solutions in the search space into two categories, the good/positive and the bad/negative ones, according to the quality of the sampled solutions. The learned classi\ufb01er partitions the search space into the good and bad regions. And then the solutions are sampled from the good regions with high probability. Yu et al. [Yu et al., 2016] attempted to answer the crucial questions of classi\ufb01cation-based derivative-free optimization, including which factors of a classi\ufb01er effect the optimization performance, and which function class can be ef\ufb01ciently solved. They identify the critical factors, the error-target dependence and the shrinking rate, and propose the randomized coordinate shrinking (RACOS) classi\ufb01cation algorithm to ef\ufb01ciently learn the classi\ufb01er for both continuous and discrete search space. Given a set of good/positive and bad/negative solutions, RACOS learns an axis-aligned hyper-rectangle to cover all the positive but no negative solutions, and at the same time, the learning process is randomized and the hyper-rectangle is highly shrunk to meet the critical factors disclosed. However, RACOS needs to sample a batch of solutions in each iteration to update the classi\ufb01cation model, while in RL, the environment often only 21 offers the sequential policy evaluation. This means RACOS cannot be directly applied to policy search, where solutions are sampled sequentially. To address this issue, Hu et al. [Hu et al., 2017] further proposed a sequential version of RACOS called SRACOS, which is tailored to direct policy search in RL. SRACOS adapts the classi\ufb01cation-based optimization for sequential sampled solutions by forming the sample batch via reusing the historical solutions. In each iteration, it only samples one solution, replaces a historical solution with the newly sampled one, and then updates the classi\ufb01er as RACOS. They introduce three replacing strategies for maintaining the historical solutions, i.e., replacing the worst one, replacing by a randomized mechanism, and replacing the one that has the largest margin from the best-so-far solution. The empirical effectiveness is veri\ufb01ed on the helicopter hovering task and controlling tasks in OpenAI Gym [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016], and the neural network is used to represent the policy. The results indicate that SRACOS can surpass CMA-ES, CE, BO with respect to the total reward. 5 Derivative-free model selection in reinforcement learning Deep reinforcement learning uses the deep neural networks to model the policy or the value function. The \ufb01nal determination of model depends on two aspects: the weights of connections in a neural network and the topology/architecture of a neural network. In the fourth section, we mainly review the cases when the topology of a neural network is \ufb01xed. Namely, the number of hidden layers, the number of hidden layer neurons, the edge set of connected neurons and the like are all \ufb01xed in advance. Under the condition of \ufb01xed neural network topology, the derivative-free methods are used to optimize the weights of connections to search for the optimal policy. However, relying on the \ufb01xed topology representation imposes some limitations, since it requires the user to correctly specify a good topology representation in advance. Unfortunately, in most tasks, the user cannot correctly guess the appropriate topology representation. Selecting an overly simple topology representation could result in the insuf\ufb01cient model expression ability. Even if the optimization algorithm can \ufb01nd out the optimal solution under this simple representation, the policy corresponding to the found optimal solution may still be unsatisfactory. On the other hand, selecting an overly complex topology representation could signi\ufb01cantly improves the expressive power of the model, but the huge search space makes the optimization algorithm inef\ufb01cient, and thus it is dif\ufb01cult to \ufb01nd out the optimal policy. Therefore, many works are devoted to developing the methods that can automatically \ufb01nding out the appropriate topology representation of neural network. And the review of neuroevolution for learning a neural network architecture can be found from [Yao, 1999; Stanley et al., 2019]. This section mainly reviews the methods for optimizing neural network topologies using derivative-free optimization, where neural networks can represent the policy or the value func22 tion in reinforcement learning. The network topology and connection weights are variable at the same time, which further improves the expressiveness and \ufb02exibility of the reinforcement learning model. However, it challenges the derivative-free optimization algorithms. The search space becomes larger and more complex, and the evaluation of model quality becomes more time consuming and labor intensive. The earliest and simplest work to optimize the neural network topologies using evolutionary algorithms might be traced back to the structured genetic algorithm (sGA) [Dasgupta and McGregor, 1992]. sGA uses a two-part representation to describe each neural network. The \ufb01rst part represents the connectivity of the neural network in the form of a binary matrix. In this binary matrix, the rows and columns correspond to the neuron nodes in the network, and the value of each element in the matrix indicates whether there is an edge connecting a given pair of nodes. The second part represents the weight of each edge in the neural network. Via evolving these binary matrices with the connection weights, sGA can automatically discover the appropriate the network topology. However, sGA still has some restrictions. When a new topology is introduced (e.g., adding a new edge), the quality of the corresponding solution may be poor due to the fact that the weight associated with the new topology has not been optimized. Even if the topology ultimately corresponds to a better policy. Stanley and Miikkulainen [Stanley and Miikkulainen, 2002b,a] proposed the neuroevolution of augmenting topologies (NEAT), which is a well-known approach of topology and weight evolving arti\ufb01cial neural networks (TWEANNs), and achieved a signi\ufb01cant performance gains in RL. To represent networks of different topologies, NEAT uses a \ufb02exible genetic coding. Each network is described by an edge list, and each edge describes the connection between two neuron nodes and the weight. In NEAT, the architecture is evolved in a way of incremental growth from minimal structure. New architecture is added incrementally as structural mutations occur, and only those architectures survive that are found to be useful. During mutation, one can add Input Layer Hidden Layer Output Layer Add a new link Figure 5: A new link (edge) is added between two existing nodes in NEAT [Stanley and Miikkulainen, 2002b,a]. 23 new nodes or new connections to the network, as shown in Figure 5. To avoid the catastrophic crossovers, NEAT introduces the innovation numbers to record and track the historical origin of each individual. Whenever a new individual emerges through mutation, it receives a unique innovation number that belongs to itself. Therefore, the innovation numbers can be regarded as the chronology of all individuals produced during the evolutionary process. Experimental result on the pole-balancing benchmark shows that NEAT can be faster and better than the compared \ufb01xed-topology methods therein. Taylor et al. [Taylor et al., 2006] conducted a detailed empirical comparison between NEAT and Sarsa [Singh and Sutton, 1996] in the Keepaway RL benchmark based on robot soccer. Sarsa is a kind of temporal difference [Sutton and Barto, 1998] methods that learn a value function to estimate the expected total reward for taking a particular action given a state. The results show that NEAT can learn better policies than Sarsa, but NEAT requires more \ufb01tness evaluations to achieve so. The results on two variations of Keepaway show that Sarsa can learn better policies when the task is fully observable, and NEAT can learn faster when the \ufb01tness function is deterministic. Whiteson and Stone [Whiteson and Stone, 2006b,a] enhanced the sample ef\ufb01ciency of evolutionary value function approximation via combining NEAT and a temporal difference method. The proposed algorithm can automatically discover the appropriate topologies for pre-trained neural network function approximators, and exploit the off-policy nature of a temporal difference method. Kohl and Miikkulainen [Kohl and Miikkulainen, 2009] noticed that NEAT could perform poorly on the fractured problems, where the correct action varies discontinuously as the agent moves from state to state. They introduce a method to measure the degree of fracture by utilizing the concept of function variation, and propose RBF-NEAT and Cascade-NEAT to improve the performance on the fractured problems through biasing or constraining the search for network topologies towards the local solutions. Gauci and Stanley [Gauci and Stanley, 2008] proposed the hypercube-based neuroevolution of augmenting topologies (HyperNEAT) that is able to exploit the geometric regularities in the two-dimensional game screen. Hausknecht et al. [Hausknecht et al., 2012] introduced a HyperNEAT-based general game playing (HyperNEATGGP) method to play Atari games. HyperNEAT-GGP reduces the learning complexity from the raw game screen by using a game-independent visual processing hierarchy aimed to identify the objects and entities on the screen. And the identi\ufb01ed ones are input of HyperNEAT. The effectiveness of HyperNEAT-GGP is veri\ufb01ed on Asterix and Freeway. Ebrahimi et al. [Ebrahimi et al., 2017] introduced an approach to learn the control policy for the autonomous driving task aimed to minimize crashes and safety violations during training. The proposed approach learns to generate an optimal network topology from demonstrations by utilizing a new reward function that simultaneously optimizes model size and accuracy. They use a recurrent neural network to sequentially generate the description of layers of an 24 architecture from a given design space, which is inspired by [Zoph and Le, 2017b]. The variable length architectures are searched by an enhanced evolution strategy with a modi\ufb01cation in noise generation. Finally, by combining this derivative-free policy search with demonstrations, the proposed approach can learn a policy that adapts to the new environment based on the rewards in the target domain. The experimental result shows that, when the agent learns to drive in a real simulation environment, this approach can learn more safely than the compared baseline method and has fewer cumulative crash times in the life cycle of the agent. Gaier and Ha [Gaier and Ha, 2019] attempted to answer the question of how important are the weight parameters of a neural network compared to its architecture. To deemphasize the importance of weights in the neural networks, they assign a single shared weight parameter to every network connection from a uniform random distribution, and only search for the architectures that perform well on a wide range of weights without explicit weight training. The proposed search method is inspired by NEAT [Stanley and Miikkulainen, 2002b,a]. The empirical result shows that this method is able to \ufb01nd the minimal neural network architectures that perform well on a set of continuous control tasks only with a random weight parameter. 6 Derivative-free exploration in reinforcement learning In most cases, RL algorithms share the exploration-learning framework [Yu, 2018]. An agent explores and interacts with an unknown environment to learn the optimal policy that maximizes the total reward from the exploration samples. The exploration samples involve states, state transitions, actions and rewards. Exploration is necessary in RL. Because achieving the best total reward on the current trajectory samples is not the ultimate goal, and the agent should visit the states that have not been visited before so as to collect better trajectory samples. This means that the agent should not follow the current policy tightly, and thus the exploration mechanisms need to encourage the agent to deviate from the previous trajectory paths properly and drive it to the regions with uncertainty. Derivative-free RL is naturally equipped with the exploration strategies. In the search process of derivative-free optimization methods, the designed mechanisms for sampling solutions and rules for updating model always consider the exploration. To name a few, the mutation and crossover operators in GAs help to explore the solution space, the GP upper con\ufb01dence bound in BO uses variance to drive the search direction towards the regions with uncertainty, and the classi\ufb01cation-based optimization algorithms usually adopt the global sampling to realize the exploration. Thus, derivative-free optimization methods can take part of the duty of exploration for RL when updating the policy or value function models from samples. Recently, some problem-dependent derivative-free exploration methods that could improve the sample ef\ufb01ciency have been proposed. Conti et al. [Conti et al., 2018] proposed the NSR-ES method, which attempted to enhance 25 the ability of exploration in evolution strategies for deep RL. NSR-ES uses the novelty search (NS) [Lehman and Stanley, 2011] to explore. NS encourages policies to engage in different behaviors than those previously seen. The encouragement of different behaviors is realized through computing the novelty of the current policy with respect to the previously generated policies and then pushing the population distribution towards the regions in parameter space with high novelty. NS is hybridized with ES to improve its performance on sparse or deceptive deep reinforcement learning tasks. The proposed NSR-ES algorithm has been tested in the MuJoCo and Atari [Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016], and the overall performance is superior to the classic evolution strategies. Lehman et al. [Lehman et al., 2018b] noticed that the simple mutation operators, e.g., Gaussian mutation, may lead to the catastrophic consequences on the behavior of an agent in deep RL due to the drastic differences in the sensitivity of parameters. Therefore, they propose a set of safe mutation operators that facilitate exploration without dramatically varying the network behaviors. The safe mutation operators scale the degree of mutation of each individual weight according to the sensitivity of the outputs of network to that weight. And it is realized by computing the gradient of outputs with respect to the weights. Chen and Yu [Chen and Yu, 2019] considered the exploration as an extra optimization problem in reinforcement learning, and realized the exploration component guided by a state-ofthe-art classi\ufb01cation-based derivative-free optimization algorithm, rather than directly applying derivative-free optimization to both exploration and learning. The proposed method searches for the high-quality exploration policy globally by setting the performance of the sampled trajectories as the \ufb01tness value of that exploration policy. The target policy is optimized by the deep deterministic policy gradient (DDPG) algorithm [Lillicrap et al., 2016] on those explored trajectories. This method could overcome the sample inef\ufb01ciency in derivative-free optimization and the exploration locality in gradient-based one. Vemula et al. [Vemula et al., 2019] conducted a theoretical study on when exploring in the parameter space is better/worse than exploring in the action space by the black-box optimizer. They reveal that, when the dimensionality of action space and the horizon length are small, as well as the dimensionality of parameter space is large, exploration in the action space is preferred. Otherwise, when the horizon length is long and the dimensionality of policy parameter space is low, exploration in the parameter space is preferred. Khadka and Tumer [Khadka and Tumer, 2018] proposed the evolutionary reinforcement learning (ERL) method. The core idea behind ERL is to combine the genetic algorithm (GA) and DDPG algorithm [Lillicrap et al., 2016]. The algorithmic \ufb02ow is illustrated in Figure 6. The algorithm can be divided into the genetic algorithm module and the reinforcement learning module. In the genetic algorithm module, ERL generates n actors for parallel sampling, accumulates the cumulative reward of the sampled trajectories as the \ufb01tness of these n actors, and then constructs n new actors according to the mutation process of the genetic algorithm for the 26 Figure 6: Figure from [Khadka and Tumer, 2018] that illustrates the structure of ERL. next sampling. In the reinforcement learning module, ERL collects samples of n actors into the experience replay buffer, uses the policy of reinforcement learning algorithm itself to perform additional sampling, and adds the sampling results to the experience replay buffer. Then, according to the DDPG algorithm which is a gradient-based one, a mini-batch sample with a \ufb01xed number of rounds is selected from the experience replay buffer to optimize the policy. At last, in order to inject the gradient information of policy into the genetic algorithm process, ERL periodically uses the policy to replace the policy with the worst \ufb01tness value among n actors before the genetic algorithm performs further sampling. ERL is tested on multiple environments in MuJoCo [Todorov et al., 2012; Brockman et al., 2016], and the result shows that ERL is superior to the compared derivative-free algorithms and gradient-based ones with respect to both algorithmic effectiveness and sample ef\ufb01ciency. For instance, as shown in Figure 7, we can observe that DDPG can easily solve the standard inverted double pendulum task, while fails on the hard one. Both tasks are similar for the evolutionary algorithm (EA). ERL is able to inherit the merits of both DDPG and EA, and successfully solves both standard and hard tasks similar to EA while utilizing the gradients for better sample ef\ufb01ciency similar to DDPG. Colas et al. [Colas et al., 2018] proposed the GEP-PG algorithm that splits reinforcement learning into two parts: global exploration and policy gradient. In the global exploration process (GEP), it uses a method similar to novelty search to generate a variety of exploration policies based on the policy behavior space. Speci\ufb01cally, the GEP process samples a batch of policy behavior features from the behavior space. The behavior space is an arti\ufb01cially de\ufb01ned mapping from the policy trajectory to a vector, which is used to represent the type of a policy. It then 27 Cumulative*Reward Cumulative*Reward Episodes Episodes Figure 7: Figure from [Khadka and Tumer, 2018] that compares the performance (measured by the cumulative reward) of the proposed ERL, evolutionary algorithm, and DDPG on the standard (left sub-\ufb01gure) and hard (right sub-\ufb01gure) inverted double pendulum task. uses the neighborhood method to \ufb01nd the clustering center for each policy behavior. After that, each clustering center is perturbed to generate a new set of exploration policies. After the new exploration policy is sampled, the behavior features of the exploration policy are marked, and according to the newly added marks, the cluster centers are re-generated by the neighborhood method. This process is repeated so that the generated exploration policies can be placed in different goals as much as possible, which could ensure the diversity of the generated policies. In the policy gradient (PG) process, it combines with the deep deterministic policy gradient (DDPG) algorithm [Lillicrap et al., 2016], and adds the samples collected by the GEP process to the experience replay buffer. PG updates the policy by the \ufb01xed mini-batch. The GEP-PG approach enhances the diversity of exploration policies and is tested on the continuous mountain car and half-cheetah environments in MuJoCo [Todorov et al., 2012; Brockman et al., 2016]. GEP-PG shows the better performance than the compared algorithms. Notably, some of the aforementioned methods, e.g., GEP-PG and ERL, are essentially a combination of derivativefree and gradient-based optimization. The derivative-free ones are used to better explore, and the gradient-based ones are used to better exploit. 7 Parallel and distributed derivative-free reinforcement learning Although derivative-free methods could bring some good news to RL with respect to optimization and exploration, they mostly suffer from low convergence rate. Derivative-free optimization methods often require to sample a large amount of solutions before convergence, even if the objective function is convex or smooth [Jamieson et al., 2012; Duchi et al., 2015; Bach and Perchet, 2016]. And the issue of slow convergence becomes more serious as the dimensionality of a search space increases [Duchi et al., 2015; Qian and Yu, 2016]. Obviously, this issue will block the further application of derivative-free methods to RL. Fortunately, many derivative28 free optimization methods are population-based. A population of solutions is maintained and improved iteratively. This characteristic makes them highly parallel. Thus, derivative-free optimization methods can be accelerated by parallel or distributed computation, which alleviates their slow convergence. Furthermore, for parallel/distributed derivative-free methods, the data communication cost is lower compared with gradient-based ones, since only one-dimensional scalars (\ufb01tness values) instead of gradient vectors or Hessian matrices need to be conveyed. This merit could further compensates for the low convergence rate. The ES method [Salimans et al., 2017] mentioned in the fourth section is a typical parallel case of the derivative-free method. Moreover, by the virtue of high parallelization property of ES, a novel communication strategy based on common random numbers is introduced to further reduce the communication cost, and it can make the algorithm well scaled to a large number of parallel workers. It takes only 10 minutes to reach 6000 long-term reward in the 3D Humanoid environment with 1440 cores. And with the increasing of core amount, the consumption of training time shows a relatively stable linear decline. SRACOS [Hu et al., 2017] is a classi\ufb01cation-based derivative-free optimization algorithm for direct policy search. Its distributed version ZOOpt [Liu et al., 2017], an open-source toolkit, is implemented by the Julia language. And the experimental result shows that the algorithm can support more than 100 processes for parallel computing. The parallel strategies of ES and SRACOS are similar. Both are based on a certain generation module to construct multiple policies simultaneously. ES is based on Gaussian perturbation for current parameters, and SRACOS is based on a randomized coordinate shrinking classi\ufb01er. Policies are run and their \ufb01tness values are evaluated in parallel. The generation module is updated by these \ufb01tness values, and the next batch of policies is constructed. Reinforcement learning algorithms sometimes are sensitive to the hyper-parameters, such as learning rate and entropy penalty coef\ufb01cient. Finely tuned hyper-parameters can accelerate learning and improve the performance of policy. Previous hyper-parameter adjustment methods are parallel search algorithms, which use the performance of \ufb01nal policy under different hyperparameters to adjust hyper-parameters. Such methods require a large amount of computational resources to simultaneously optimize multiple models. Population-based training (PBT) [Jaderberg et al., 2017] is a framework to simultaneously optimize model parameters and adjust hyperparameters with a derivative-free method. The method is shown in Algorithm 1. The step(\u03b8 | h) is a function to conduct model optimization according to the current hyper-parameters h and the model parameters \u03b8. The optimization method depends on tasks (e.g., SGD for supervised learning and DDPG for reinforcement learning). The eval(h) is a function to realize performance evaluation. The evaluation method also depends on tasks (e.g., the loss for supervised learning and the total reward of a policy for reinforcement learning). If the model evaluation result p is better than a threshold, population-based evolution will start to adjust the hyper-parameters 29 Algorithm 1 Population Based Training (PBT) [Jaderberg et al., 2017] Procedure: TRAIN (P) 1: initial population P 2: for (\u03b8, h, p, t) \u2208 P (asynchronously in parallel) do 3: while not end of training do 4: one step of optimization: \u03b8 \u2190 step(\u03b8 | h) 5: current model evaluation: p \u2190 eval(h) 6: if ready(p, t, P) then 7: use the rest of population to \ufb01nd better solution: h\u2032, \u03b8\u2032 \u2190 exploit(h, \u03b8, p, P) 8: if \u03b8 \u0338= \u03b8\u2032 then 9: produce new h: h, \u03b8 \u2190 explore(h\u2032, \u03b8\u2032, P) 10: new model evaluation: p \u2190 eval(\u03b8) 11: end if 12: end if 13: update P with new (\u03b8, h, p, t + 1) 14: end while 15: end for 16: return \u03b8 with the highest p in P h. There are two main functions in the population-based evolution, i.e., exploit(h, \u03b8, p, P) and explore(h\u2032, \u03b8\u2032, P). The exploit(h, \u03b8, p, P) function exploits the best hyper-parameters h and its model parameters \u03b8 to conduct evolution. For example, replacing the worst solution in P with the best one. The explore(h\u2032, \u03b8\u2032, P) function explores the new unknown hyper-parameters h based on the current population P. For example, perturbing h\u2032 with Gaussian noise to generate the new hyper-parameters h. After population evolution, the new hyper-parameters h as well as its model parameters \u03b8 will be added to P. The main characteristic of PBT is that the process of parameter optimization and hyper-parameters adjustment is hybrid, which can not only reduce the computational cost but also help algorithm adjust hyper-parameters in dynamic environments. PBT has succeeded in RL when combining with A3C [Mnih et al., 2016]. The empirical results in the DeepMind Lab 3D environment [Beattie et al., 2016], Atari games [Bellemare et al., 2013; Brockman et al., 2016], and the StarCraft II environment tasks [Vinyals et al., 2017] show that PBT increases the \ufb01nal performance of the agents when trained with the same number of episodes. Ray [Moritz et al., 2018], a distributed framework proposed by the researchers from UC Berkeley, integrates the distributed implementation of the PBT algorithm. Online meta-learning by parallel algorithm competition (OMPAC) [Elfwing et al., 2018] is also a derivative-free distributed hybrid optimization algorithm in RL. This algorithm can be regarded as an improvement based on PBT. The main difference between OMPAC and PBT is the evolution mecha30 ",
    "Conclusion": "Conclusion In this article, we summarize some recent progress in applying derivative-free optimization to reinforcement learning. Since derivative-free optimization is a generally applicable tool, it can be utilized in different levels and aspects of reinforcement learning. Here, we focus on the aspects of parameter updating, model selection, exploration, and parallel/distributed derivativefree methods. Due to the complexity of policy search in reinforcement learning, successful research studies of using derivative-free optimization are noticeable in all of these aspects. Derivative-free reinforcement learning approaches have their own advantages. Firstly, during the optimization process, they do not perform gradient back-propagation, are easy to train, possess the ability of search globally, do not care whether the reward function is sparse or dense, do not care the length of time horizons, do not require to carefully tune the discount factor, and can be applied to optimize the non-differentiable policy functions. Secondly, they could provide better exploration tailored to the problems. Thirdly, they are highly suitable for parallelization and only require the low communication cost. Usually, the amount of information that needs to be exchanged among workers does not rely on the size of neural networks. At the same time, we also notice that the combination of derivative-free optimization and reinforcement learning has several limitations. These are mainly from the issues of current derivative-free optimization methods. The foremost ones could be the issues of sample complexity and scalability. 31 Derivative-free optimization algorithms could be comparable to some state-of-the-art gradientbased ones in reinforcement learning tasks, but usually they require more samples [Salimans et al., 2017]. The low sample ef\ufb01ciency may block the further application of derivative-free optimization algorithms to reinforcement learning. Recently, some emerging works focusing on improving sample ef\ufb01ciency by the way of introducing sample reuse [Pourchot et al., 2018; Liu et al., 2019], surrogate models [Stork et al., 2019], importance sampling [Bibi et al., 2020], and momentum [Chen et al., 2019a; Gorbunov et al., 2020]. However, these advances are far from enough and more future works on this direction are urgently needed. Due to the sampling mechanism, derivative-free optimization methods have limitation of scaling up to the search space with very high dimensionality. This is a crucial issue for deep reinforcement learning. Deep neural network models often have more than a million parameters, for which parameter optimization directly by derivative-free optimization can be inef\ufb01cient. While some recent works dedicated to tackling the scalability issue [Kandasamy et al., 2015; Wang et al., 2016; Qian and Yu, 2016; Qian et al., 2016; Yang et al., 2018; Mutny and Krause, 2018; M\u00a8uller and Glasmachers, 2018; Eriksson et al., 2019; Li et al., 2020], more future works on developing the scalable derivative-free optimization methods tailored to reinforcement learning are appealing. Other issues include noise-sensitivity as well as structure-insensitivity. Since derivative-free optimization relies on solution evaluations, noisy evaluation can badly affect the policy search in reinforcement learning [Wang et al., 2018]. Reinforcement learning usually has inner structures that can help better solve the learning, such as hierarchical policy models and curiosity-driven exploration, while derivative-free methods commonly ignore the inner structure of the problem. How to make the derivative-free optimization well aware of the inner structure and utilizing the structure is also an important direction and remains under explored. For the above issues, the hybrid of derivative-free and gradient-based algorithms could be a potential and promising way. Some existing successful attempts reviewed in this article indicate that, by fusing derivative-free and gradient-based algorithms in a clever manner, the strengths of both sides could be absorbed, and more powerful reinforcement learning algorithms would be expected. With the successful cases and fast progress, we expect in a near future that derivative-free methods will play an even more important role in developing novel and ef\ufb01cient reinforcement learning approaches, and hope that this review article will serve as a catalyst for this goal. Acknowledgments This article has been accepted by Frontiers of Computer Science with DOI: 10.1007/s11704020-0241-4 in 2020. This work is supported by the Program A for Outstanding Ph.D. Candi32 ",
    "References": "References Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y. Ng. An application of reinforcement learning to aerobatic helicopter \ufb02ight. In Advances in Neural Information Processing Systems 19, pages 1\u20138, Vancouver, Canada, 2006. Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu\u00b4\u0131s Paulo Reis, and Gerhard Neumann. Model-based relative entropy stochastic search. In Advances in Neural Information Processing Systems 28, pages 3537\u20133545, Montr\u00b4eal, Canada, 2015. Francis R. Bach and Vianney Perchet. Highly-smooth zero-th order online optimization. In Proceedings of the 29th Conference on Learning Theory, pages 257\u2013283, New York, NY, 2016. James E. Baker. Reducing bias and inef\ufb01ciency in the selection algorithm. In Proceedings of the 2nd International Conference on Genetic Algorithms, pages 14\u201321, Hillsdale, NJ, 1987. P. L. Bartlett and J. Baxter. In\ufb01nite-horizon policy gradient estimation. Journal of Arti\ufb01cial Intelligence Research, 15:319\u2013350, 2001. Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00a8uttler, Andrew Lefrancq, Simon Green, V\u00b4\u0131ctor Vald\u00b4es, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801, 2016. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013279, 2013. Richard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, pages 679\u2013684, 1957. 33 Adel Bibi, El Houcine Bergou, Ozan Sener, Bernard Ghanem, and Peter Richt\u00b4arik. A stochastic derivative-free optimization method with importance sampling: Theory and learning to control. In Proceedings of the 34th AAAI Conference on Arti\ufb01cial Intelligence, pages 3275\u20133282, New York, NY, 2020. Cristian Bodnar, Ben Day, and Pietro Li\u00b4o. Proximal distilled evolutionary reinforcement learning. In Proceedings of the 34th AAAI Conference on Arti\ufb01cial Intelligence, pages 3283\u20133290, New York, NY, 2020. Justin A. Boyan and Michael L. Littman. Packet routing in dynamically changing networks: A reinforcement learning approach. In Advances in Neural Information Processing Systems 6, pages 671\u2013678, Denver, Colorado, 1993. Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv:1012.2599, 2010. Dimo Brockhoff, Anne Auger, Nikolaus Hansen, Dirk V. Arnold, and Tim Hohm. Mirrored sampling and sequential selection for evolution strategies. In Proceedings of the 11th International Conference on Parallel Problem Solving from Nature, pages 11\u201321, Krak\u00b4ow, Poland, 2010. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016. Matthew Brown, Bo An, Christopher Kiekintveld, Fernando Ord\u00b4o\u02dcnez, and Milind Tambe. An extended study on multi-objective security games. Autonomous Agents and Multi-Agent Systems, 28(1):31\u201371, 2014. Adam D. Bull. Convergence rates of ef\ufb01cient global optimization algorithms. Journal of Machine Learning Research, 12:2879\u20132904, 2011. Roberto Calandra, Andr\u00b4e Seyfarth, Jan Peters, and Marc Peter Deisenroth. An experimental comparison of Bayesian optimization for bipedal locomotion. In Proceedings of the 2014 IEEE International Conference on Robotics and Automation, pages 1951\u20131958, Hong Kong, China, 2014. Roberto Calandra, Andr\u00b4e Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimization for learning gaits under uncertainty - An experimental comparison on a dynamic bipedal walker. Annals of Mathematics and Arti\ufb01cial Intelligence, 76(1-2):5\u201323, 2016. 34 Xiong-Hui Chen and Yang Yu. Reinforcement learning with derivative-free exploration. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 1880\u20131882, Montr\u00b4eal, Canada, 2019. Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David D. Cox. ZOAdaMM: Zeroth-order adaptive momentum method for black-box optimization. In Advances in Neural Information Processing Systems 32, pages 7202\u20137213, Vancouver, Canada, 2019. Zefeng Chen, Yuren Zhou, Xiaoyu He, and Siyu Jiang. A restart-based rank-1 evolution strategy for reinforcement learning. In Proceedings of the 28th International Joint Conference on Arti\ufb01cial Intelligence, pages 2130\u20132136, Macao, China, 2019. James J. Choi, David Laibson, Brigitte C. Madrian, and Andrew Metrick. Reinforcement learning and savings behavior. The Journal of Finance, 64(6):2515\u20132534, 2009. Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. In Proceedings of the 35th International Conference on Machine Learning, pages 969\u2013977, Stockholm, Sweden, 2018. Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sindhwani. From complexity to simplicity: Adaptive ES-active subspaces for blackbox optimization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada, 2019. Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. Back to basics: Benchmarking canonical evolution strategies for playing atari. In Proceedings of the 27th International Joint Conference on Arti\ufb01cial Intelligence, pages 1419\u20131426, Stockholm, Sweden, 2018. C\u00b4edric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration and exploitation in deep reinforcement learning algorithms. In Proceedings of the 35th International Conference on Machine Learning, pages 1038\u20131047, Stockholm, Sweden, 2018. Andrew R. Conn, Katya Scheinberg, and Lu\u00b4\u0131s N. Vicente. Introduction to Derivative-Free Optimization. SIAM, Philadelphia, PA, 2009. Paul G. Constantine. Active Subspaces - Emerging Ideas for Dimension Reduction in Parameter Studies, volume 2 of SIAM spotlights. SIAM, Philadelphia, PA, 2015. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Advances in Neural Information Processing Systems 31, pages 5032\u20135043, Montr\u00b4eal, Canada, 2018. 35 D. Dasgupta and D. McGregor. Designing application-speci\ufb01c neural networks using the structured genetic algorithm. In Proceedings of the International Conference on Combinations of Genetic Algorithms and Neural Networks, pages 87\u201396, Baltimore, Maryland, 1992. Pieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y. Rubinstein. A tutorial on the cross-entropy method. Annals of Operations Research, 134(1):19\u201367, 2005. Nando de Freitas, Alexander J. Smola, and Masrour Zoghi. Exponential regret bounds for Gaussian process bandits with deterministic observations. In Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Thomas G. Dietterich. Machine learning research: Four current directions. Arti\ufb01cial Intelligence Magazine, 18(4):97\u2013136, 1997. Pedro M. Domingos. A few useful things to know about machine learning. Communications of the ACM, 55(10):78\u201387, 2012. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33nd International Conference on Machine Learning, pages 1329\u20131338, New York, NY, 2016. Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv:1611.02779, 2016. John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788\u20132806, 2015. Sayna Ebrahimi, Anna Rohrbach, and Trevor Darrell. Gradient-free policy architecture search and adaptation. In Proceedings of the 1st Conference on Robot Learning, pages 505\u2013514, Mountain View, CA, 2017. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Online meta-learning by parallel algorithm competition. In Proceedings of the 2018 Conference on Genetic and Evolutionary Computation, pages 426\u2013433, New York, NY, 2018. David Eriksson, Michael Pearce, Jacob R. Gardner, Ryan Turner, and Matthias Poloczek. Scalable global optimization via local Bayesian optimization. In Advances in Neural Information Processing Systems 32, pages 5497\u20135508, Vancouver, Canada, 2019. 36 Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, pages 1126\u20131135, Sydney, Australia, 2017. Michael J. Frank, Lauren C. Seeberger, and Randall C. O\u2019reilly. By carrot or by stick: Cognitive reinforcement learning in parkinsonism. Science, 306(5703):1940\u20131943, 2004. Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning, pages 1582\u20131591, Stockholm, Sweden, 2018. Lior Fuks, Noor Awad, Frank Hutter, and Marius Lindauer. An evolution strategy with progressive episode lengths for playing games. In Proceedings of the 28th International Joint Conference on Arti\ufb01cial Intelligence, pages 1234\u20131240, Macao, China, 2019. Adam Gaier and David Ha. Weight agnostic neural networks. In Advances in Neural Information Processing Systems 32, pages 5365\u20135379, Vancouver, Canada, 2019. Tanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. In Proceedings of the 6th International Conference on Learning Representations, Vancouver, Canada, 2018. Jason Gauci and Kenneth O. Stanley. A case study on the critical role of geometric regularity in machine learning. In Proceedings of the 23rd AAAI Conference on Arti\ufb01cial Intelligence, pages 628\u2013633, Chicago, IL, 2008. John Geweke. Antithetic acceleration of Monte Carlo integration in Bayesian inference. Journal of Econometrics, 38(1-2):73\u201389, 1988. Eduard A. Gorbunov, Adel Bibi, Ozan Sener, El Houcine Bergou, and Peter Richt\u00b4arik. A stochastic derivative free optimization method with momentum. In Proceedings of the 8th International Conference on Learning Representations, Addis Ababa, Ethiopia, 2020. David Ha and J\u00a8urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems 31, pages 2455\u20132467, Montr\u00b4eal, Canada, 2018. Nikolaus Hansen and Andreas Ostermeier. Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation. In Proceedings of 1996 IEEE International Conference on Evolutionary Computation, pages 312\u2013317, Nayoya University, Japan, 1996. 37 Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159\u2013195, 2001. Nikolaus Hansen, Sibylle D. M\u00a8uller, and Petros Koumoutsakos. Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation, 11(1):1\u201318, 2003. Nikolaus Hansen, Dirk V. Arnold, and Anne Auger. Evolution strategies. In Janusz Kacprzyk and Witold Pedrycz, editors, Springer Handbook of Computational Intelligence, pages 871\u2013 898. Springer, Berlin, Heidelberg, 2015. Tatsunori Hashimoto, Steve Yadlowsky, and John C. Duchi. Derivative free optimization via repeated classi\ufb01cation. In Proceedings of the 2018 International Conference on Arti\ufb01cial Intelligence and Statistics, pages 2027\u20132036, Playa Blanca, Spain, 2018. Matthew J. Hausknecht, Piyush Khandelwal, Risto Miikkulainen, and Peter Stone. HyperNEAT-GGP: A hyperNEAT-based Atari general game player. In Proceedings of the 2012 Conference on Genetic and Evolutionary Computation, pages 217\u2013224, Philadelphia, PA, 2012. Matthew J. Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution approach to general Atari game playing. IEEE Transactions on Computational Intelligence and AI in Games, 6(4):355\u2013366, 2014. Jun He and Xin Yao. Drift analysis and average time complexity of evolutionary algorithms. Arti\ufb01cial Intelligence, 127(1):57\u201385, 2001. Verena Heidrich-Meisner and Christian Igel. Evolution strategies for direct policy search. In Proceedings of the 10th International Conference on Parallel Problem Solving from Nature, pages 428\u2013437, Dortmund, Germany, 2008. Verena Heidrich-Meisner and Christian Igel. Hoeffding and Bernstein races for selecting policies in evolutionary direct policy search. In Proceedings of the 26th International Conference on Machine Learning, pages 401\u2013408, Montr\u00b4eal, Canada, 2009. Verena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforcement learning. Journal of Algorithms, 64(4):152\u2013168, 2009. J. H. Holland. Adaptation in Natural and Arti\ufb01cial Systems. The University of Michigan Press, Ann Arbor, MI, 1975. 38 Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. In Advances in Neural Information Processing Systems 31, pages 5405\u20135414, Montr\u00b4eal, Canada, 2018. Yi-Qi Hu, Hong Qian, and Yang Yu. Sequential classi\ufb01cation-based optimization for direct policy search. In Proceedings of the 31st AAAI Conference on Arti\ufb01cial Intelligence, pages 2029\u20132035, San Francisco, CA, 2017. Chen Huang, Simon Lucey, and Deva Ramanan. Learning policies for adaptive tracking with deep feature cascades. In IEEE International Conference on Computer Vision, pages 105\u2013 114, Venice, Italy, 2017. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. arXiv:1711.09846, 2017. Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Human-level performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859\u2013865, 2019. Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of derivativefree optimization. In Advances in Neural Information Processing Systems 25, pages 2681\u2013 2689, Lake Tahoe, NV, 2012. Whiyoung Jung, Giseung Park, and Youngchul Sung. Population-guided parallel policy search for reinforcement learning. In Proceedings of the 8th International Conference on Learning Representations, Addis Ababa, Ethiopia, 2020. K. Kandasamy, J. Schneider, and B. Poczos. High dimensional Bayesian optimisation and bandits via additive models. In Proceedings of the 32nd International Conference on Machine Learning, pages 295\u2013304, Lille, France, 2015. K. Kawaguchi, L. P. Kaelbling, and T. Lozano-Perez. Bayesian optimization with exponential convergence. In Advances in Neural Information Processing Systems 28, pages 2809\u20132817, Montr\u00b4eal, Canada, 2015. Kenji Kawaguchi, Yu Maruyama, and Xiaoyu Zheng. Global continuous optimization with error bound and fast convergence. Journal of Arti\ufb01cial Intelligence Research, 56:153\u2013195, 2016. 39 Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning. In Advances in Neural Information Processing Systems 31, pages 1196\u20131208, Montr\u00b4eal, Canada, 2018. Nate Kohl and Risto Miikkulainen. Evolving neural networks for strategic decision-making problems. Neural Networks, 22(3):326\u2013337, 2009. Tamara G. Kolda, Robert Michael Lewis, and Virginia Torczon. Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review, 45(3):385\u2013482, 2003. Jan Koutn\u00b4\u0131k, Giuseppe Cuccu, J\u00a8urgen Schmidhuber, and Faustino J. Gomez. Evolving largescale neural networks for vision-based reinforcement learning. In Proceedings of the 2013 Conference on Genetic and Evolutionary Computation, pages 1061\u20131068, Amsterdam, The Netherlands, 2013. H. J. Kushner. A new method of locating the maximum of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering, 86:97\u2013106, 1964. Joel Lehman and Kenneth O. Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary Computation, 19(2):189\u2013223, 2011. Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O. Stanley. ES is more than just a traditional \ufb01nite-difference approximator. In Proceedings of the 2018 Conference on Genetic and Evolutionary Computation, pages 450\u2013457, Kyoto, Japan, 2018. Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O. Stanley. Safe mutations for deep and recurrent neural networks through output gradients. In Proceedings of the 2018 Conference on Genetic and Evolutionary Computation, pages 117\u2013124, Kyoto, Japan, 2018. Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-of\ufb02ine experimentation. arXiv:1904.01049, 2019. Zhenhua Li, Qingfu Zhang, Xi Lin, and Hui-Ling Zhen. Fast covariance matrix adaptation for large-scale black-box optimization. IEEE Transaction on Cybernetics, 50(5):2073\u20132083, 2020. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the 4th International Conference on Learning Representations, San Juan, Puerto Rico, 2016. 40 Yu-Ren Liu, Yi-Qi Hu, Hong Qian, Yang Yu, and Chao Qian. ZOOpt: Toolbox for derivativefree optimization. arXiv:1801.00329, 2017. Guoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Trust region evolution strategies. In Proceedings of the 33rd AAAI Conference on Arti\ufb01cial Intelligence, pages 4352\u20134359, Honolulu, HI, 2019. Jose A. Lozano, Pedro Larranaga, Inaki Inza, and Endika Bengoetxea. Towards a New Evolutionary Computation: Advances on Estimation of Distribution Algorithms. Springer-Verlag, Berlin, Germany, 2006. Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter Bartlett, and Martin J. Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. In Proceedings of the 22nd International Conference on Arti\ufb01cial Intelligence and Statistics, pages 2916\u20132925, Naha, Japan, 2019. Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. In Advances in Neural Information Processing Systems 31, pages 1805\u20131814, Montr\u00b4eal, Canada, 2018. Alonso Marco, Felix Berkenkamp, Philipp Hennig, Angela P. Schoellig, Andreas Krause, Stefan Schaal, and Sebastian Trimpe. Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization. In Proceedings of the 2017 IEEE International Conference on Robotics and Automation, pages 1557\u20131563, Singapore, 2017. Melanie Mitchell. An Introduction to Genetic Algorithms. MIT Press, Cambridge, MA, 1998. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, and Georg Ostrovski. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, pages 1928\u20131937, New York, NY, 2016. J. Mo\u02c7ckus, V. Tiesis, and A. \u02c7Zilinskas. Toward global optimization. In L. C. W. Dixon and G. P. Szego, editors, The Application of Bayesian Methods for Seeking the Extremum, pages 117\u2013128. Elsevier, Amsterdam, Netherlands, 1978. 41 David E. Moriarty, Alan C. Schultz, and John J. Grefenstette. Evolutionary algorithms for reinforcement learning. Journal of Arti\ufb01cial Intelligence Research, 11:241\u2013276, 1999. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributed framework for emerging AI applications. In The 13th USENIX Symposium on Operating Systems Design and Implementation, pages 561\u2013577, Carlsbad, CA, 2018. Gregory Morse and Kenneth O. Stanley. Simple evolutionary optimization can rival stochastic gradient descent in neural networks. In Proceedings of the 2016 Conference on Genetic and Evolutionary Computation, pages 477\u2013484, Denver, CO, 2016. Nils M\u00a8uller and Tobias Glasmachers. Challenges in high-dimensional reinforcement learning with evolution strategies. In Proceedings of the 15th International Conference on Parallel Problem Solving from Nature, pages 411\u2013423, Coimbra, Portugal, 2018. R\u00b4emi Munos. From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization and planning. Foundations and Trends in Machine Learning, 7(1):1\u2013129, 2014. Mojmir Mutny and Andreas Krause. Ef\ufb01cient high dimensional Bayesian optimization with additivity and quadrature fourier features. In Advances in Neural Information Processing Systems 31, pages 9019\u20139030, Montr\u00b4eal, Canada, 2018. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the 34th International Conference on Machine Learning, pages 2778\u20132787, Sydney, Australia, 2017. Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180\u20131190, 2008. Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In Proceedings of the 6th International Conference on Learning Representations, Vancouver, Canada, 2018. Alo\u00a8\u0131s Pourchot, Nicolas Perrin, and Olivier Sigaud. Importance mixing: Improving sample reuse in evolutionary policy search methods. arXiv:1808.05832, 2018. Hong Qian and Yang Yu. Scaling simultaneous optimistic optimization for high-dimensional non-convex functions with low effective dimensions. In Proceedings of the 30th AAAI Conference on Arti\ufb01cial Intelligence, pages 2000\u20132006, Phoenix, AZ, 2016. 42 Chao Qian, Yang Yu, and Zhi-Hua Zhou. Subset selection by pareto optimization. In Advances in Neural Information Processing Systems 28, pages 1765\u20131773, Montr\u00b4eal, Canada, 2015. H. Qian, Y.-Q. Hu, and Y. Yu. Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings. In Proceedings of the 25th International Joint Conference on Arti\ufb01cial Intelligence, pages 1946\u20131952, New York, NY, 2016. Chao Qian, Jing-Cheng Shi, Yang Yu, and Ke Tang. On subset selection with general cost constraints. In Proceedings of the 26th International Joint Conference on Arti\ufb01cial Intelligence, pages 2613\u20132619, Melbourne, Australia, 2017. Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge, Massachusetts, 2006. E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Largescale evolution of image classi\ufb01ers. In Proceedings of the 34th International Conference on Machine Learning, pages 2902\u20132911, Sydney, Australia, 2017. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image classi\ufb01er architecture search. arXiv:1802.01548, 2018. Luis Miguel Rios and Nikolaos V. Sahinidis. Derivative-free optimization: A review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247\u20131293, 2013. Sebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges. IEEE Transactions on Computational Intelligence and AI in Games, 9(1):25\u201341, 2017. St\u00b4ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the 14th International Conference on Arti\ufb01cial Intelligence and Statistics, pages 627\u2013635, Fort Lauderdale, FL, 2011. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems 29, pages 2226\u20132234, Barcelona, Spain, 2016. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017. 43 Kazuyuki Samejima, Yasumasa Ueda, Kenji Doya, and Minoru Kimura. Representation of action-speci\ufb01c reward values in the striatum. Science, 310(5752):1337\u20131340, 2005. John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, pages 1889\u20131897, Lille, France, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148\u2013175, 2016. Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In Proceedings of the 34th International Conference on Machine Learning, pages 3067\u20133075, Sydney, Australia, 2017. Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and Anxiang Zeng. Virtual-Taobao: Virtualizing real-world online retail environment for reinforcement learning. In Proceedings of the 33rd AAAI Conference on Arti\ufb01cial Intelligence, pages 4902\u20134909, Honolulu, HI, 2019. O. Sigaud and S. W. Wilson. Learning classi\ufb01er systems: A survey. Soft Computing, 11(11):1065\u20131078, 2007. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018. Satinder P. Singh and Richard S. Sutton. Reinforcement learning with replacing eligibility traces. Machine Learning, 22(1-3):123\u2013158, 1996. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25, pages 2960\u20132968, Lake Tahoe, NV, 2012. 44 Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015\u20131022, Haifa, Israel, 2010. K. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary Computation, 10(2):99\u2013127, 2002. Kenneth O. Stanley and Risto Miikkulainen. Ef\ufb01cient reinforcement learning through evolving neural network topologies. In Proceedings of the 2002 Conference on Genetic and Evolutionary Computation, pages 569\u2013577, New York, NY, 2002. Kenneth O. Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks through neuroevolution. Nature Machine Intelligence, 1(1):24\u201335, 2019. J\u00a8org Stork, Martin Zaefferer, Thomas Bartz-Beielstein, and A. E. Eiben. Surrogate models for enhancing the ef\ufb01ciency of neuroevolution in reinforcement learning. In Proceedings of the 2019 Conference on Genetic and Evolutionary Computation, pages 934\u2013942, Prague, Czech Republic, 2019. Freek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix adaptation. In Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv:1712.06567, 2017. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Massachusetts, 1998. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems 26, pages 2004\u20132012, Lake Tahoe, NV, 2013. Istvan Szita and Andr\u00b4as L\u00a8orincz. Learning Tetris using the noisy cross-entropy method. Neural Computation, 18(12):2936\u20132941, 2006. Yunhao Tang, Krzysztof Choromanski, and Alp Kucukelbir. Variance reduction for evolution strategies via structured control variates. arXiv:1906.08868, 2019. 45 Matthew E. Taylor, Shimon Whiteson, and Peter Stone. Comparing evolutionary and temporal difference methods in a reinforcement learning domain. In Proceedings of the 2006 Conference on Genetic and Evolutionary Computation, pages 1321\u20131328, Seattle, WA, 2006. C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-WEKA: Combined selection and hyperparameter optimization of classi\ufb01cation algorithms. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 847\u2013855, Chicago, IL, 2013. Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, Vilamoura, Portugal, 2012. Anirudh Vemula, Wen Sun, and J. Andrew Bagnell. Contrasting exploration in parameter and action space: A zeroth-order optimization perspective. In Proceedings of the 22nd International Conference on Arti\ufb01cial Intelligence and Statistics, pages 2926\u20132935, Naha, Japan, 2019. Ngo Anh Vien, Viet-Hung Dang, and TaeChoong Chung. A covariance matrix adaptation evolution strategy for direct policy search in reproducing kernel Hilbert space. In Proceedings of The 9th Asian Conference on Machine Learning, pages 606\u2013621, Seoul, Korea, 2017. Ngo Anh Vien, Heiko Zimmermann, and Marc Toussaint. Bayesian functional optimization. In Proceedings of the 32nd AAAI Conference on Arti\ufb01cial Intelligence, pages 4171\u20134178, New Orleans, LA, 2018. Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00a8uttler, John P. Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. StarCraft II: A new challenge for reinforcement learning. arXiv:1708.04782, 2017. Yi-Chi Wang and John M. Usher. Application of reinforcement learning for agent-based production scheduling. Engineering Applications of Arti\ufb01cial Intelligence, 18(1):73\u201382, 2005. Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. D. Freitas. Bayesian optimization in a billion dimensions via random embeddings. Journal of Arti\ufb01cial Intelligence Research, 55:361\u2013387, 2016. 46 Hong Wang, Hong Qian, and Yang Yu. Noisy derivative-free optimization with value suppression. In Proceedings of the 32nd AAAI Conference on Arti\ufb01cial Intelligence, pages 1447\u2013 1454, New Orleans, LA, 2018. Shimon Whiteson and Peter Stone. Evolutionary function approximation for reinforcement learning. Journal of Machine Learning Research, 7:877\u2013917, 2006. Shimon Whiteson and Peter Stone. Sample-ef\ufb01cient evolutionary function approximation for reinforcement learning. In Proceedings of the 21st AAAI Conference on Arti\ufb01cial Intelligence, pages 518\u2013523, Boston, MA, 2006. Shimon Whiteson. Evolutionary computation for reinforcement learning. In Marco Wiering and Martijn van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 325\u2013355. Springer, Berlin, Heidelberg, 2012. Marco Wiering and Martijn van Otterlo. Reinforcement Learning: State-of-the-Art. Springer, Berlin, Heidelberg, 2012. Daan Wierstra, Tom Schaul, Jan Peters, and J\u00a8urgen Schmidhuber. Natural evolution strategies. In Proceedings of the 2008 IEEE Congress on Evolutionary Computation, pages 3381\u20133387, Hong Kong, China, 2008. Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J\u00a8urgen Schmidhuber. Natural evolution strategies. Journal of Machine Learning Research, 15(1):949\u2013980, 2014. Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data to improve Bayesian optimization for reinforcement learning. Journal of Machine Learning Research, 15(1):253\u2013 282, 2014. Peng Yang, Ke Tang, and Xin Yao. Turning high-dimensional optimization into computationally expensive optimization. IEEE Transactions on Evolutionary Computation, 22(1):143\u2013156, 2018. Xin Yao. Evolving arti\ufb01cial neural networks. Proceedings of the IEEE, 87(9):1423\u20131447, 1999. Yang Yu and Hong Qian. The sampling-and-learning framework: A statistical view of evolutionary algorithms. In Proceedings of the 2014 IEEE Congress on Evolutionary Computation, pages 149\u2013158, Beijing, China, 2014. Y. Yu and Z.-H. Zhou. A new approach to estimating the expected \ufb01rst hitting time of evolutionary algorithms. Arti\ufb01cial Intelligence, 172(15):1809\u20131832, 2008. 47 Yang Yu, Chao Qian, and Zhi-Hua Zhou. Switch analysis for running time analysis of evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 19(6):777\u2013792, 2015. Yang Yu, Hong Qian, and Yi-Qi Hu. Derivative-free optimization via classi\ufb01cation. In Proceedings of the 30th AAAI Conference on Arti\ufb01cial Intelligence, pages 2286\u20132292, Phoenix, AZ, 2016. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In Proceedings of the 31st AAAI Conference on Arti\ufb01cial Intelligence, pages 2852\u20132858, San Francisco, CA, 2017. Wenhao Yu, C. Karen Liu, and Greg Turk. Policy transfer with strategy optimization. In Proceedings of the 7th International Conference on Learning Representations, New Orleans, LA, 2019. Yang Yu. Towards sample ef\ufb01cient reinforcement learning. In Proceedings of the 27th International Joint Conference on Arti\ufb01cial Intelligence, pages 5739\u20135743, Stockholm, Sweden, 2018. Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, and Honglak Lee. Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction. In IEEE Conference on Computer Vision and Pattern Recognition, pages 249\u2013258, Boston, MA, 2015. Xingwen Zhang, Jeff Clune, and Kenneth O. Stanley. On the relationship between the OpenAI evolution strategy and stochastic gradient descent. arXiv:1712.06564, 2017. Aimin Zhou, Jinyuan Zhang, Jianyong Sun, and Guixu Zhang. Fuzzy-classi\ufb01cation assisted solution preselection in evolutionary optimization. In Proceedings of the 33rd AAAI Conference on Arti\ufb01cial Intelligence, pages 2403\u20132410, Honolulu, HI, 2019. Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proceedings of the 5th International Conference on Learning Representations, Toulon, France, 2017. Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proceedings of the 5th International Conference on Learning Representations, Toulon, France, 2017. 48 ",
    "title": "Derivative-Free Reinforcement Learning: A Review",
    "paper_info": "Derivative-Free Reinforcement Learning: A Review\nHong Qian, Yang Yu\u2217\nNational Key Laboratory for Novel Software Technology, Nanjing University, China\nqianh@lamda.nju.edu.cn, yuy@nju.edu.cn\n\u2217Corresponding author.\nAbstract\nReinforcement learning is about learning agent models that make the best sequential deci-\nsions in unknown environments. In an unknown environment, the agent needs to explore\nthe environment while exploiting the collected information, which usually forms a sophis-\nticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving so-\nphisticated problems. It commonly uses a sampling-and-updating framework to iteratively\nimprove the solution, where exploration and exploitation are also needed to be well bal-\nanced. Therefore, derivative-free optimization deals with a similar core issue as reinforce-\nment learning, and has been introduced in reinforcement learning approaches, under the\nnames of learning classi\ufb01er systems and neuroevolution/evolutionary reinforcement learn-\ning. Although such methods have been developed for decades, recently, derivative-free re-\ninforcement learning exhibits attracting increasing attention. However, recent survey on\nthis topic is still lacking. In this article, we summarize methods of derivative-free reinforce-\nment learning to date, and organize the methods in aspects including parameter updating,\nmodel selection, exploration, and parallel/distributed methods. Moreover, we discuss some\ncurrent limitations and possible future directions, hoping that this article could bring more\nattentions to this topic and serve as a catalyst for developing novel and ef\ufb01cient approaches.\n1\nIntroduction\nReinforcement learning [Sutton and Barto, 1998; Wiering and van Otterlo, 2012] aims to enable\nagents to automatically learn the policy with the maximum long-term reward via interactions\nwith environments. It has been listed as one of the four research directions of machine learning\nby Professor T. G. Dietterich [Dietterich, 1997]. In recent years, with the fusion of deep learn-\ning and reinforcement learning, deep reinforcement learning has made remarkable progress\nand attracted more and more attention from both the academic and industrial community. To\nname a few, the deep Q-network (DQN) [Mnih et al., 2015] proposed by DeepMind reaches\n1\narXiv:2102.05710v1  [cs.LG]  10 Feb 2021\n",
    "GPTsummary": "                    - (1): This article reviews recent advances in the use of derivative-free optimization in reinforcement learning.\n\n                    - (2): Past reinforcement learning methods have used gradient-based optimization, which can struggle with highly complex and non-differentiable objective functions. Derivative-free optimization methods do not rely on gradient information and can be easy to use for sophisticated optimization tasks. This article proposes the use of derivative-free reinforcement learning, which balances exploration and exploitation in a similar way to derivative-free optimization methods. \n\n                    - (3): This article summarizes methods of derivative-free reinforcement learning to date and organizes them based on aspects such as parameter updating, model selection, exploration, and parallel/distributed methods. \n\n                    - (4): The paper does not present its own results, but rather reviews existing literature. The authors discuss limitations and future directions for derivative-free reinforcement learning, aiming to bring more attention to this topic and provide a catalyst for the development of novel and efficient approaches in the future.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in its contribution to the field of reinforcement learning by providing a comprehensive review of recent advances in the use of derivative-free optimization methods. It proposes the use of derivative-free reinforcement learning approaches and identifies potential limitations and future research directions in this field.\n                     \n- (2): Innovation point: This article proposes the use of derivative-free reinforcement learning, which is a novel approach to balancing exploration and exploitation in complex optimization tasks. It also organizes existing methods based on aspects such as parameter updating, model selection, exploration, and parallel/distributed methods. \n  Performance: The paper does not present its own results but provides a critical review of literature on derivative-free reinforcement learning. It discusses potential limitations of current derivative-free optimization methods, such as sample complexity and scalability, and identifies future research directions to address these issues. \n  Workload: The article is well-organized and provides a clear and concise overview of recent research on this topic. However, it requires a certain level of knowledge in the field of reinforcement learning and optimization to fully understand the concepts and methods discussed.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in its contribution to the field of reinforcement learning by providing a comprehensive review of recent advances in the use of derivative-free optimization methods. It proposes the use of derivative-free reinforcement learning approaches and identifies potential limitations and future research directions in this field.\n                     \n- (2): Innovation point: This article proposes the use of derivative-free reinforcement learning, which is a novel approach to balancing exploration and exploitation in complex optimization tasks. It also organizes existing methods based on aspects such as parameter updating, model selection, exploration, and parallel/distributed methods. \n  Performance: The paper does not present its own results but provides a critical review of literature on derivative-free reinforcement learning. It discusses potential limitations of current derivative-free optimization methods, such as sample complexity and scalability, and identifies future research directions to address these issues. \n  Workload: The article is well-organized and provides a clear and concise overview of recent research on this topic. However, it requires a certain level of knowledge in the field of reinforcement learning and optimization to fully understand the concepts and methods discussed.\n\n\n"
}