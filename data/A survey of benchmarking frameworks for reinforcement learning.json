{
    "Abstract": "ABSTRACT Reinforcement learning has recently experienced increased prominence in the machine learning community. There are many approaches to solving reinforcement learning problems with new techniques developed constantly. When solving problems using reinforcement learning, there are various di\ufb03cult challenges to overcome. To ensure progress in the \ufb01eld, benchmarks are important for testing new algorithms and comparing with other approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements are accurately judged. This paper provides an overview of di\ufb00erent contributions to reinforcement learning benchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning. The contributions discussed are the most used and recent in the literature. The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks. The survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available and to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for researchers not familiar with the di\ufb00erent tasks that can be used to develop and test new reinforcement learning algorithms. Keywords: reinforcement learning, benchmarking Categories: \u2022 Computing methodologies \u223c Reinforcement learning Email: Belinda Stapelberg belinda.stapelberg@up.ac.za (CORRESPONDING), Katherine M. Malan malankm@unisa.ac.za Article history: Received: Accepted: Available online: 1 ",
    "Introduction": "INTRODUCTION Reinforcement learning (RL) is a sub\ufb01eld of machine learning, based on rewarding desired behaviours and/or punishing undesired ones of an agent interacting with its environment [1]. The agent learns by taking sequential actions in its environment, observing the state of the environment and receiving a reward. The agent needs to learn a strategy, called a policy, to decide which action to take in any state. The goal of RL is to \ufb01nd the policy that maximises the long-term reward of the agent. In recent years RL has experienced dramatic growth in research attention and interest due to promising results in areas including robotics control [2], playing Atari 2600 [3, 4], competitive video Belinda Stapelberg and Katherine M. Malan (2020). A survey of benchmarking frameworks for reinforcement learning. South African Computer Journal Vol TBC(Num TBC), 1\u201335. DOI TBC Copyright \u00a9 the author(s); published under a Creative Commons NonCommercial 4.0 License (CC BY-NC 4.0). SACJ is a publication of the South African Institute of Computer Scientists and Information Technologists. ISSN 1015-7999 (print) ISSN 2313-7835 (online). arXiv:2011.13577v1  [cs.LG]  27 Nov 2020 Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 2 games [5, 6], tra\ufb03c light control [7] and more. In 2016, RL came into the general spotlight when Google DeepMind\u2019s AlphaGo [8] program defeated the Go world champion, Lee Sedol. Even more recently, Google DeepMind\u2019s AlphaStar AI program defeated professional StarCraft II players (considered to be one of the most challenging real-time strategy games) and OpenAI Five defeated professional Dota 2 players. Progress in machine learning is driven by new algorithm development and the availability of high-quality data. In supervised and unsupervised machine learning \ufb01elds, resources such as the UCI Machine Learning repository1, the Penn Treebank [9], the MNIST database of handwritten digits2, the ImageNet large scale visual recognition challenge [10], and Pascal Visual Object Classes [11] are available. In contrast to the datasets used in supervised and unsupervised machine learning, progress in RL is instead driven by research on agent behaviour within challenging environments. Games have been used for decades to test and evaluate the performance of arti\ufb01cial intelligence systems. Many of the benchmarks that are available for RL are also based on games, such as the Arcade Learning Environment for Atari 2600 games [12] but others involve tasks that simulate real-world situations, such as locomotion tasks in Garage (originally rllab) [13]. These benchmarking tasks have been used extensively in research and signi\ufb01cant progress has been made in using RL in ever more challenging domains. Benchmarks and standardised environments are crucial in facilitating progress in RL. One advantage of the use of these benchmarking tasks is the reproducibility and comparison of algorithms to state-of-the-art RL methods. Progress in the \ufb01eld can only be sustained if existing work can be reproduced and accurately compared to judge improvements of new methods [14, 15]. The existence of standardised tasks can facilitate accurate benchmarking of RL performance. This paper provides a survey of the most important and most recent contributions to benchmarking for RL. These are OpenAI Gym [16], the Arcade Learning Environment [12], a continuous control benchmark rllab [13], RoboCup Keepaway soccer [17] and Microsoft TextWorld [18]. When solving RL problems, there are many challenges that need to be overcome, such as the fundamental trade-o\ufb00 problem between exploration and exploitation, partial observability of the environment, delayed rewards, enormous state spaces and so on. This paper discusses these challenges in terms of important RL benchmarking contributions and in what manner the benchmarks can be used to overcome or address these challenges. The rest of the paper is organised as follows. Section 2 introduces the key concepts and terminology of RL, and then discusses the approaches to solving RL problems and the challenges for RL. Section 3 provides a survey on the contributions to RL benchmarking and Section 4 discusses the ways that the di\ufb00erent contributions to RL benchmarking deal with or contribute to the challenges for RL. A conclusion follows in Section 5. 1http://archive.ics.uci.edu/ml/index.php 2http://yann.lecun.com/exdb/mnist/ DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 3 Agent Environment Action at New state st+1 Reward rt+1 Figure 1: Illustration of an RL system. 2 REINFORCEMENT LEARNING RL focuses on training an agent by using a trial-and-error approach. Figure 1 illustrates the workings of an RL system. The agent evaluates a current situation (state), takes an action, and receives feedback (reward) from the environment after each act. The agent is rewarded with either positive feedback (when taking a \u201cgood\u201d action) or negative feedback as punishment for taking a \u201cbad\u201d action. An RL agent learns how to act best through many attempts and failures. Through this type of trial-and-error learning, the agent\u2019s goal is to receive the best so-called long-term reward. The agent gets short-term rewards that together lead to the cumulative, long-term reward. The key goal of RL is to de\ufb01ne the best sequence of actions that allow the agent to solve a problem while maximizing its cumulative long-term reward. That set of optimal actions is learned through the interaction of the agent with its environment and observation of rewards in every state. This section provides the key concepts and terminology of RL used throughout this paper. The challenges of RL are also discussed. 2.1 Concepts and terminology The core idea behind RL is to learn from the environment through interactions and feedback, and \ufb01nd an optimal strategy for solving the problem. The agent takes actions in its environment based on a (possibly partial) observation of the state of the environment and the environment provides a reward for the actions, which is usually a scalar value. The set of all valid actions is referred to as the action space, which can be either discrete (as in Atari and Go) or continuous (controlling a robot in the physical world). The goal of the agent is to maximise its long-term cumulative reward. 2.1.1 Policy A policy of an agent is the control strategy used to make decisions, and is a mapping from states to actions. A policy can be deterministic or stochastic and is denoted by \u03c0. A deterministic policy maps states to actions without uncertainty while a stochastic policy is a probability distribution DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 4 over actions for a given state. Therefore, when an agent follows a deterministic policy it will always take the same action for a given state, whereas a stochastic policy may take di\ufb00erent actions in the same state. The immediate advantage of a stochastic policy is that an agent is not doomed to repeat a looped sequence of non-advancing actions. 2.1.2 On-policy and o\ufb00-policy learning There are two types of policy learning methods. On-policy learning is when the agent \u201clearns on the job\u201d, i.e. it evaluates or improves the policy that is used to make the decisions directly. O\ufb00-policy learning is when the agent learns one policy, called the target policy, while following another policy, called the behaviour policy, which generates behaviour. The o\ufb00-policy learning method is comparable to humans learning a task by observing others performing the task. 2.1.3 Value functions Having a value for a state (or state-action pair) is often useful in guiding the agent towards the optimal policy. The value under policy \u03c0 is the expected return if the agent starts in a speci\ufb01c state or state-action pair, and then follows the policy thereafter. So the state-value function v\u03c0 is a mapping from states to real numbers and represents the long-term reward obtained by starting from a particular state and executing policy \u03c0. The action-value function q\u03c0 is a mapping from state-action pairs to real numbers. The action-value q\u03c0(s, a) of state s and action a (where a is an arbitrary action and not necessarily in line with the policy) is the expected return from starting in state s, taking action a and then following policy \u03c0. The optimal value function v\u2217 gives the expected return starting in a state and then following the optimal policy \u03c0\u2217. The optimal action-value function q\u2217 is the expected return starting in some state, taking an arbitrary action and then following the optimal policy \u03c0\u2217. These state-value and action-value functions all obey so-called Bellman equations, where the idea is that the value of the agent\u2019s starting point is the reward that is expected to be obtained from being there, plus the value of wherever the agent lands next. These Bellman equations are used in most RL approaches where the Bellman-backup is used, i.e. for a state or state-action pair the Bellman-backup is the (immediate) reward plus the next value. 2.1.4 Function approximators In many RL problems the state space can be extremely large. Traditional solution methods where value functions are represented as arrays or tables mapping all states to values are therefore very di\ufb03cult [1]. One approach to this shortcoming is to use features to generalise an estimation of the value of states with similar features. Methods that compute these approximations are called function approximators. There are many techniques used for implementing function approximators including linear combinations of features, neural networks, decision trees, nearest neighbours, etc. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 5 2.1.5 Monte Carlo methods Monte Carlo methods are a class of learning methods where value functions are learned [1]. The value of a state, si, is estimated by running many trials starting from si and then averaging the total rewards received on those trials. 2.1.6 Temporal di\ufb00erence algorithms Temporal di\ufb00erence (TD) learning algorithms are a class of learning methods that are based on the idea of comparing temporally successive predictions [1]. These methods are a fundamental idea in RL and use a combination of Monte Carlo learning and dynamic programming [1]. TD methods learn value functions directly from experience by using the so-called TD error and bootstrapping (not waiting for a \ufb01nal outcome). 2.1.7 Markov decisions processes The standard formalism for RL settings is called a Markov decision process (MDP). MDPs are used to de\ufb01ne the interaction between an agent and its environment in terms of states, actions, and rewards. For an RL problem to be an MDP, it has to satisfy the Markov property: \u201cThe future is independent of the past given the present\u201d. This means that once the current state is known, then the history encountered so far can be discarded and that state completely characterises all the information needed as it captures all the relevant information from the history. Mathematically, an MDP is a tuple: \u27e8S, A, R, P, \u03b3\u27e9, where S is a (\ufb01nite) set of states, A is a (\ufb01nite) set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P is a state transition probability matrix and \u03b3 \u2208 [0, 1] is a discount factor included to control the reward. 2.1.8 Model-free and model-based reinforcement learning approaches There are di\ufb00erent aspects of RL systems that can be learnt. These include learning policies (either deterministic or stochastic), learning action-value functions (so-called Q-functions or Q-learning), learning state-value functions, and/or learning a model of the environment. A model of the environment is a function that predicts state transitions and rewards, and is an optional element of an RL system. If a model is available, i.e. if all the elements of the MDP are known, particularly the transition probabilities and the reward function, then a solution can be computed using classic techniques before executing any action in the environment. This is known as planning: computing the solution to a decision-making problem before executing an actual decision. When an agent does not know all the elements of the MDP, then the agent does not know how the environment will change in response to its actions or what its immediate reward will be. In this situation the agent will have to try out di\ufb00erent actions, observe what happens and in some way \ufb01nd a good policy from doing this. One approach to solve a problem without a complete model is for the agent to learn a model of how the environment works from its observations and then plan a solution using that model. Methods that use the framework of models and planning are referred to as model-based methods. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 6 Another way of solving RL problems without a complete model of the environment is to learn through trial-and-error. Methods that do not have or learn a model of the environment and do not use planning are called model-free methods. The two main approaches to represent and train agents with model-free RL are policy optimisation and Q-learning. In policy optimisation methods (or policy-iteration methods) the agent learns the policy function directly. Examples include policy gradient methods, asynchronous advantage actor-critic (A3C) [19], trust region policy optimization (TRPO) [20] and proximal policy optimization (PPO) [21]. Q-Learning methods include deep Q-networks (DQN) [3], C51 algorithm [22] and Hindsight Experience Replay (HER) [23]. Hybrid methods combining the strengths of Q-learning and policy gradients exist as well, such as deep deterministic policy gradients (DDPG) [2], soft actor-critic algorithm (SAC) [24] and twin delayed deep deterministic policy gradients (TD3) [25]. In the current literature, the most used approaches incorporates a mixture of model-based and model-free methods, such as Dyna and Monte Carlo tree search (MCTS) [1], and temporal di\ufb00erence search [26]. 2.2 Challenges for reinforcement learning This section discusses some of the challenges faced by RL. These challenges will be discussed in terms of how they are addressed by di\ufb00erent contributions in Section 4. 2.2.1 Partially observable environment How the agent observes the environment can have a signi\ufb01cant impact on the di\ufb03culty of the problem. In most real-world environments the agent does not have a complete or perfect perception of the state of its environment due to incomplete information provided by its sensors, the sensors being noisy or some of the state being hidden. However, for learning methods that are based on MDPs, the complete state of the environment should be known. To address the problem of partial observability of the environment, the MDP framework is extended to the partially observable Markov decision process (POMDP) model. 2.2.2 Delayed or sparse rewards In an RL problem, an agent\u2019s actions determine its immediate reward as well as the next state of the environment. Therefore, an agent has to take both these factors into account when deciding which action to take in any state. Since the goal is to learn which actions to take that will give the most reward in the long-run, it can become challenging when there is little or no immediate reward. The agent will consequently have to learn from delayed reinforcement, where it may take many actions with insigni\ufb01cant rewards to reach a future state with full reward feedback. The agent must therefore be able to learn which actions will result in an optimal reward, which it might only receive far into the future. In line with the challenge of delayed or sparse rewards is the problem of long-term credit assignment [27]: how must credit for success be distributed among the sequence of decisions that have been made to produce the outcome? DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 7 2.2.3 Unspeci\ufb01ed or multi-objective reward functions Many tasks (especially real-world problems) have multiple objectives. The goal of RL is to optimise a reward function, which is commonly framed as a global reward function, but tasks with more than one objective could require optimisation of di\ufb00erent reward functions. In addition, when an agent is training to optimise some objective, other objectives could be discovered which might have to be maintained or improved upon. Work on multi-objective RL (MORL) has received increased interest, but research is still primarily devoted to single-objective RL. 2.2.4 Size of the state and action spaces Large state and action spaces can result in enormous policy spaces in RL problems. Both state and action spaces can be continuous and therefore in\ufb01nite. However, even discrete states and actions can lead to infeasible enumeration of policy/state-value space. In RL problems for which state and/or action spaces are small enough, so-called tabular solutions methods can be used, where value functions can be represented as arrays or tables and exact solutions are often possible. For RL problems with state and/or action spaces that are too large, the goal is to instead \ufb01nd good approximate solutions with the limited computational resources available and to avoid the curse of dimensionality [28]. 2.2.5 The trade-o\ufb00 between exploration and exploitation One of the most important and fundamental overarching challenges in RL is the trade-o\ufb00 between exploration and exploitation. Since the goal is to obtain as much reward as possible, an agent has to learn to take actions that were previously most e\ufb00ective in producing a reward. However, to discover these desirable actions, the agent has to try actions that were not tried before. It has to exploit the knowledge of actions that were already taken, but also explore new actions that could potentially be better selections in the future. The agent may have to sacri\ufb01ce short-term gains to achieve the best long-term reward. Therefore, both exploration and exploitation are fundamental in the learning process, and exclusive use of either will result in failure of the task at hand. There are many exploration strategies [1], but a key issue is the scalability to more complex or larger problems. The exploration vs. exploitation challenge is a\ufb00ected by many of the other challenges that are discussed in this section, such as delayed or sparse rewards, and the size of the state or action spaces. 2.2.6 Representation learning Representation (or feature) learning involves automatically extracting features or understanding the representation of raw input data to perform tasks such as classi\ufb01cation or prediction. It is fundamental not just to RL, but to machine learning and AI in general, even with a conference dedicated to it: International Conference on Learning Representations (ICLR). One of the clearest challenges that representation learning tries to solve in an RL context is to e\ufb00ectively reduce the impact of the curse of dimensionality, which results from very large state DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 8 and/or action spaces. Ideally an e\ufb00ective representation learning scheme will be able to extract the most important information from the problem input in a compressed form. 2.2.7 Transfer learning Transfer learning [29, 30] uses the notion that, as in human learning, knowledge gained from a previous task can improve the learning in a new (related) task through the transfer of knowledge that has already been learned. The \ufb01eld of transfer learning has recently been experiencing growth in RL [31] to accelerate learning and mitigate issues regarding scalability. 2.2.8 Model learning Model-based RL methods (Section 2.1.8) are important in problems where the agent\u2019s interactions with the environment are expensive. These methods are also signi\ufb01cant in the trade-o\ufb00 between exploration and exploitation, since planning impacts the need for exploration. Model learning can reduce the interactions with the environment, something which can be limited in practice, but introduces additional complexities and the possibility of model errors. Another challenge related to model learning is the problem of planning using an imperfect model, which is also a di\ufb03cult challenge that has not received much attention in the literature. 2.2.9 O\ufb00-policy learning O\ufb00-policy learning methods (e.g. Q-learning) scale well in comparison to other methods and the algorithms can (in principle) learn from data without interacting with the environment. An agent is trained using data collected by other agents (o\ufb00-policy data) and data it collects itself to learn generalisable skills. Disadvantages of o\ufb00-policy learning methods include greater variance and slow convergence, but are more powerful and general than on-policy learning methods [1]. Advantages of using o\ufb00-policy learning is the use of a variety of exploration strategies, and learning from training data that are generated by unrelated controllers, which includes manual human control and previously collected data. 2.2.10 Reinforcement learning in real-world settings The use of RL in real-world scenarios has been gaining attention due to the success of RL in arti\ufb01cial domains. In real-world settings, more challenges become apparent for RL. Dulac-Arnold et al. [32] provide a list of nine challenges for RL in the real-world, many of which have been mentioned in this section already. Further challenges not discussed here include safety constraints, policy explainability and real-time inference. Many of these challenges have been studied extensively in isolation, but there is a need for research on algorithms (both in arti\ufb01cial domains and real-world settings) that addresses more than one or all of these challenges together, since many of the challenges are present in the same problem. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 9 2.2.11 A standard methodology for benchmarking A diverse range of methodologies is currently common in the literature, which brings into question the validity of direct comparisons between di\ufb00erent approaches. A standard methodology for benchmarking is necessary for the research community to compare results in a valid way and accelerate advancement in a rigorous scienti\ufb01c manner. 3 CONTRIBUTIONS TO REINFORCEMENT LEARNING BENCHMARKING This section discusses some important reinforcement learning benchmarks currently in use. The list of contributions is by no means exhaustive, but includes the ones that are most in use currently in the RL research community. 3.1 OpenAI Gym Released publicly in April 2016, OpenAI\u2019s Gym [16] is a toolkit for developing and comparing reinforcement learning algorithms. It includes a collection of benchmark problems which is continuing to grow as well as a website where researchers can share their results and compare algorithm performance. It provides a tool to standardise reporting of environments in research publications to facilitate the reproducibility of published research. OpenAI Gym has become very popular since its release, with [16] having over 1300 citations on Google Scholar to date. 3.1.1 Implementation The OpenAI Gym library is a collection of test problems (environments) with a common interface and makes no assumptions about the structure of an agent. OpenAI Gym currently supports Linux and OS X running Python 2.7 or 3.5 \u2013 3.7. Windows support is currently experimental, with limited support for some problem environments. OpenAI Gym is compatible with any numerical computation library, such as TensorFlow or Theano. To get started with OpenAI Gym, visit the documentation site3 or the actively maintained GitHub repository4. 3.1.2 Benchmark tasks The environments available in the library are diverse, ranging from easy to di\ufb03cult and include a variety of data. A brief overview of the di\ufb00erent environments is provided here with the full list and descriptions of environments available on the main site3. Classic control and toy text: These small-scale problems are a good starting point for researchers not familiar with the \ufb01eld. The classic control problems include balancing a pole on a moving cart (Figure 2a), driving a car up a steep hill, swinging a pendulum and more. The toy text problems include \ufb01nding a safe path across a grid of ice and water tiles, playing Roulette, Blackjack and more. 3https://gym.openai.com 4https://github.com/openai/gym DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 10 Algorithmic: The objective here is for the agent to learn algorithms such as adding multi-digit numbers and reversing sequences, purely from examples. The di\ufb03culty of the tasks can be varied by changing the sequence length. Atari 2600: The Arcade Learning Environment (ALE) [12] has been integrated into OpenAI Gym in easy-to-install form, where classic Atari 2600 games (see Figure 2b for an example) can be used for developing agents (see Section 3.2 for a detailed discussion). For each game there are two versions: a version which takes the RAM as input and a version which takes the observable screen as the input. MuJoCo: These robot simulation tasks use the MuJoCo proprietary software physics engine [33], but free trial and postgraduate student licences are available. The problems include 3D robot walking or standing up tasks, 2D robots running, hopping, swimming or walking (see Figure 2c for an example), balancing two poles vertically on top of each other on a moving cart, and repositioning the end of a two-link robotic arm to a given spot. Box2D: These are continuous control tasks in the Box2D simulator, which is a free open source 2-dimensional physics simulator engine. Problems include training a bipedal robot (Figure 2d) to walk (even on rough terrain), racing a car around a track and navigating a lunar lander to its landing pad. Roboschool: Most of these problems are the same as in MuJoCo, but use the open-source software physics engine, Bullet. Additional tasks include teaching a 3D humanoid robot to walk as fast as possible (see Figure 2e) as well as a continuous control version of Atari Pong. Robotics: Released in 2018, these environments are used to train models which work on physical robots. It includes four environments using the Fetch5 research platform and four environments using the ShadowHand6 robot. These manipulation tasks are signi\ufb01cantly more di\ufb03cult than the MuJoCo continuous control environments. The tasks for the Fetch robot are to move the end-e\ufb00ector to a desired goal position, hitting a puck across a long table such that it slides and comes to rest on the desired goal, moving a box by pushing it until it reaches a desired goal position, and picking up a box from a table using its gripper and moving it to a desired goal above the table. The tasks for the ShadowHand are reaching with its thumb and a selected \ufb01nger until they meet at a desired goal position above the palm, manipulating a block (see Figure 2f), an egg, and a pen, until the object achieves a desired goal position and rotation. Alongside these new robotics environments, OpenAI also released code for Hindsight Experience Replay (HER), a reinforcement learning algorithm that can learn from failure. Their results show that HER can learn successful policies on most of the new robotics problems from only sparse rewards. A set of requests for research has also been released7 in order to encourage and facilitate research in this area, with a few ideas of ways to improve HER speci\ufb01cally. 5https://fetchrobotics.com/ 6https://www.shadowrobot.com/products/dexterous-hand/ 7https://openai.com/blog/ingredients-for-robotics-research/ DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 11 (a) A screenshot of the classic control task Cart-Pole, with the objective to keep the pole balanced by moving the cart. (b) A screenshot of the Atari 2600 game Breakout. (c) A screenshot of the MuJoCo simulator, where a four-legged 3D robot has to learn to walk. (d) A screenshot of the Box2D simulator, where a bipedal robot has to learn to walk. (e) A screenshot of the 3D humanoid robot learning to walk as fast as possible in the Roboschool simulator. (f) A screenshot of the ShadowHand robot manipulating a block. Figure 2: Some examples of the environments used in OpenAI Gym. 3.2 The Arcade Learning Environment The Atari 2600 gaming console was released in September 1977, with over 565 games developed for it over many di\ufb00erent genres. The games are considerably simpler than modern era video games. However, the Atari 2600 games are still challenging and provide interesting tasks for human players. The Arcade Learning Environment (ALE) [12] is an object-oriented software framework allowing researchers to develop AI agents for the original Atari 2600 games. It is a platform to empirically assess and evaluate AI agents designed for general competency. ALE allows interfacing through DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 12 the Atari 2600 emulator Stella and enables the separation of designing an AI agent and the details of emulation. There are currently over 50 game environments supported in the ALE. The ALE has received a lot of attention since its release in 2013 (over 1200 citations on Google Scholar to date), perhaps the most note-worthy being the success of Deep Q-networks (DQN), which was the \ufb01rst algorithm to achieve human-level control performance in many of the Atari 2600 games [4]. 3.2.1 Implementation The Stella emulator interfaces with the Atari 2600 games by receiving joystick movements and sending screen and/or RAM information to the user. For the reinforcement learning context, ALE has a game-handling layer to provide the accumulated score and a signal for whether the game has ended. The default observation of a single game screen or frame is made up of a two-dimensional array of 7-bit pixels, 160 pixels wide by 210 pixels high. The joystick controller de\ufb01nes 18 discrete actions, which makes up the action space of the problem. Only some actions are needed to play a game and the game-handling layer also provides the minimum set of actions needed to play any particular game. The simulator generates 60 frames per second in real-time and up to 6000 frames per second at full speed. The reward the agent receives depends on each game, but is generally the score di\ufb00erence between frames. A game episode starts when the \ufb01rst frame is shown and ends when the goal of the game has been achieved or after a prede\ufb01ned number of frames. The ALE therefore o\ufb00ers access to a variety of games through one common interface. The ALE also has the functionality of saving and restoring the current state of the emulator. This functionality allows the investigation of topics including planning and model-based reinforcement learning. ALE is free, open-source software8, including the source code for the agents used in associated research studies [12]. ALE is written in C++, but there are many interfaces available that allow the interaction with ALE in other programming languages, with detail provided in [12]. Due to the increase in popularity and importance in the AI literature, another paper was published in 2018 by some of the original proposers of the ALE [15], providing a broad overview of how the ALE is used by researchers, highlighting overlooked issues and discussing propositions for maximising the future use of the testbed. Concerns are raised at how agents are evaluated in the ALE and new benchmark results are provided. In addition, a new version of the ALE was introduced in 2018 [15], which supports multiple game modes and includes so called sticky actions, providing some form of stochasticity to the controller. When sticky actions are used, there is a possibility that the action requested by the agent is not executed, but instead the agent\u2019s previous action is used, emulating a sticky controller. The probability that an action will be sticky can be speci\ufb01ed using a pre-set control parameter. The original ALE is fully deterministic and consequently it is possible for an agent to memorise a good action sequence, instead of learning how to make good decisions. Introducing sticky actions therefore increases the robustness of the policy that the agent has to learn. 8http://arcadelearningenvironment.org DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 13 Originally the ALE only allowed agents to play games in their default mode and di\ufb03culty. In the latest version of the ALE [15] it is possible to select among di\ufb00erent game modes and di\ufb03culty levels for single player games, where each mode-di\ufb03culty pair is referred to as a \ufb02avour. Changes in the mode and di\ufb03culty of the games can impact game dynamics and introduce new actions. 3.2.2 Published benchmark results Bellemare et al. [12] provide performance results on the ALE tasks using an augmented version of the SARSA(\u03bb) [1] algorithm, where linear function approximation is used. For comparison, the performance results of a non-expert human player and three baseline agents (Random, Const and Perturb) are also provided. A set of games is used for training and parameter tuning, and another set for testing. The ALE can also be used to study planning techniques. Benchmark results for two traditional search methods (Breadth-\ufb01rst search and UCT: Upper Con\ufb01dence Bounds Applied to Trees) are provided, as well as the performance results of the best learning agent and the best baseline policy. Machado et al. [15] provide benchmark results for 60 Atari 2600 games with sticky actions for DQN and SARSA(\u03bb) + Blob-PROST [34] (an algorithm that includes a feature representation which enables SARSA(\u03bb) to achieve performance that is comparable to that of DQN). 3.3 Continuous control: rllab The Arcade Learning Environment (Section 3.2) is a popular benchmark to evaluate algorithms which are designed for tasks with discrete actions. Duan et al. [13] present a benchmark of 31 continuous control tasks, ranging in di\ufb03culty, and also implement a range of RL algorithms on the tasks. The benchmark as well as the implementations of the algorithms are available at the rllab GitHub repository9, however this repository is no longer under development but is currently actively maintained at the garage GitHub repository10, which includes many improvements. The documentation11 for garage is a work in progress and the available documentation is currently limited. Both rllab and garage are fully compatible with OpenAI Gym and only support Python 3.5 and higher. Other RL benchmarks for continuous control have also been proposed, but many are not in use anymore. Duan et al. [13] provide a comprehensive list of benchmarks containing low-dimensional tasks as well as a wide range of tasks with high-dimensional continuous state and action spaces. They also discuss previously proposed benchmarks for high-dimensional control tasks do not include such a variety of tasks as in rllab. Where relevant, we mention some of these benchmarks in the next section that have additional interesting tasks. 9https://github.com/rll/rllab 10https://github.com/rlworkgroup/garage 11https://garage.readthedocs.io/en/latest/ DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 14 3.3.1 Benchmark tasks There are four categories for the rllab continuous control tasks: basic, locomotion, partially observable and hierarchical tasks. Basic tasks: These \ufb01ve tasks are widely analysed in the reinforcement learning and control literature. Some of these tasks can also be found in the \u201cClassic control\u201d section of OpenAI Gym (Section 3.1). The tasks are cart-pole balancing, cart-pole swing up, mountain car, acrobot swing up and double inverted pendulum balancing (which can be found in OpenAI Gym Roboschool). A related benchmark involving a 20 link pole balancing task is proposed as part of the Tdlearn package [35]. Locomotion tasks: Six locomotion tasks of varying dynamics and di\ufb03culty are implemented with the goal to move forward as quickly as possible. These tasks are challenging due to high degrees of freedom as well as the need for a lot of exploration, since getting stuck at a local optima (such as staying at the origin or diving forward slowly) can happen easily when the agent acts greedily. These tasks are: Swimmer, Hopper, Walker, Half-Cheetah, Ant, Simple Humanoid and Full Humanoid. Other environments with related locomotion tasks include dotRL [36] with a variable segment octopus arm [37], PyBrain [38], and SkyAI [39] with humanoid robot tasks like jumping, crawling and turning. Partially observable tasks: Realistic agents often do not have access to perfect state information due to limitations in sensory input. To address this, three variations of partially observable tasks are implemented for each of the \ufb01ve basic tasks mentioned above. This leads to 15 additional tasks. The three variations are limited sensors (only positional information is provided, no velocity), noisy observations and delayed actions (Gaussian noise is added to simulate sensor noise, and a time delay is added between taking an action and an action being executed) and system identi\ufb01cation (the underlying physical model parameters vary across di\ufb00erent episodes). These variations are not currently available in OpenAI Gym. Hierarchical tasks: In many real-world situations higher level decisions can reuse lower level skills, for example a robot learning to navigate a maze can reuse learned locomotion skills. Here tasks are proposed where low-level motor controls and high-level decisions are needed, which operate on di\ufb00erent time scales and a natural hierarchy exists in order to learn the task most e\ufb03ciently. The tasks are as follows. Locomotion and food collection: where the swimmer or the ant robot operates in a \ufb01nite region and the goal is to collect food and avoid bombs. Locomotion and maze: the swimmer or the ant robot has the objective to reach a speci\ufb01c goal location in a \ufb01xed maze environment. These tasks are not currently available in OpenAI Gym. 3.3.2 Published benchmark results Duan et al. [13] provide performance results on the rllab tasks. The algorithms implemented are mainly gradient-based policy search methods, but two gradient-free methods are included for comparison. Almost all of the algorithms are batch algorithms and one algorithm is an online algorithm. The batch algorithms are REINFORCE [40], truncated natural policy gradient (TNPG) DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 15 [13], reward-weighted regression (RWR) [41], relative entropy policy search (REPS) [42], trust region policy optimization (TRPO) [20], cross entropy method (CEM) [43] and covariance matrix adaptation evolution strategy (CMA-ES) [44]. The online algorithm used is deep deterministic policy gradient (DDPG) [2]. Direct applications of the batch-based algorithms to recurrent policies are implemented with minor modi\ufb01cations. Of the implemented algorithms, TNPG, TRPO and DDPG were e\ufb00ective in training deep neural network policies. However, all algorithms performed poorly on the hierarchical tasks, which suggest that new algorithms should be developed for automatic discovery and exploitation of the tasks\u2019 hierarchical structure. Recently a new class of reinforcement learning algorithms called proximal policy optimisation (PPO) [21] was released by OpenAI. PPO\u2019s performance is comparable or better than state-of-theart approaches to solving 3D locomotion, robotic tasks (similar to the tasks in the benchmark discussed above) and also Atari 2600, but it is simpler to implement and tune. OpenAI has adopted PPO as its go-to RL algorithm, since it strikes a balance between ease of implementation, sample complexity, and ease of tuning. 3.4 RoboCup Keepaway Soccer RoboCup [45] simulated soccer has been used as the basis for successful international competitions and research challenges since 1997. Keepaway is a subtask of RoboCup that was put forth as a testbed for machine learning in 2001 [17]. It has since been used for research on temporal di\ufb00erence reinforcement learning with function approximation [46], evolutionary learning [47], relational reinforcement learning [48], behaviour transfer [49, 50, 51, 52, 53, 54, 55], batch reinforcement learning [56] and hierarchical reinforcement learning [57]. In Keepaway, one team (the keepers) tries to maintain possession of the ball within a limited region, while the opposing team (the takers) attempts to gain possession [17]. The episode ends whenever the takers take possession of the ball or the ball leaves the region. The players are then reset for another episode with the keepers being given possession of the ball again. Task parameters include the size of the region, the number of keepers, and the number of takers. Figure 3 shows an example episode with 3 keepers and 2 takers (called 3v2) playing in a 20m \u00d7 20m region [17]. In 2005 Stone et al. [58] elevated the Keepaway testbed to a benchmark problem for machine learning and provided infrastructure to easily implement the standardised task. An advantage of the Keepaway subtask is that it allows for direct comparison of di\ufb00erent machine learning algorithms. It is also good for benchmarking machine learning since the task is simple enough to be solved successfully, but complex enough that straightforward solutions are not su\ufb03cient. 3.4.1 Implementation A standardized Keepaway player framework is implemented in C++ and the source code is available for public use at an online repository12. The repository provides implementation for 12http://www.cs.utexas.edu/\u223cAustinVilla/sim/keepaway/ DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 16 all aspects of the Keepaway problem except the learning algorithm itself. It also contains a step-by-step tutorial of how to use the code, with the goal of allowing researchers who are not experts in the RoboCup simulated soccer domain to easily become familiar with the domain. 3.4.2 Standardised task Figure 3: A screen shot from a 3v2 keepaway episode in a 20m \u00d7 20m region from [17]. Robocup simulated soccer (and therefore also Keepaway) is a fully distributed, multiagent domain with both teammates and adversaries [59]. The environment is partially observable for each agent and the agents also have noisy sensors and actuators. Therefore, the agents do not perceive the world exactly as it is, nor can they a\ufb00ect the world exactly as intended. The perception and action cycles of the agent are asynchronous, therefore perceptual input does not trigger actions as is traditional in AI. Communication opportunities are limited, and the agents must make their decisions in realtime. These domain characteristics all result in simulated robotic soccer being a realistic and challenging domain [59]. The size of the Keepaway region, the number of keepers, and the number of takers can easily be varied to change the task. Stone et al. [58] provide a framework with a standard interface to the learner in terms of macro-actions, states, and rewards. 3.4.3 Published benchmark results Stone et al. [58] performed an empirical study for learning Keepaway by training the keepers using episodic SMDP SARSA(\u03bb) [46, 1], with three di\ufb00erent function approximators: CMAC function approximation [60, 61], Radial Basis Function (RBF) [1] networks (a novel extension to CMACs [58]), and neural network function approximation. The RBF network performed comparably to the CMAC method. The Keepaway benchmark structure allows for these results to be quantitatively compared to other learning algorithms to test the relative bene\ufb01ts of di\ufb00erent techniques. 3.4.4 Half Field O\ufb00ense: An extension to Keepaway Half Field O\ufb00ense (HFO) [62, 63] is an extension of Keepaway, which is played on half of the soccer \ufb01eld with more players on each team. The task was originally introduced in 2007 [62], but DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 17 no code was made publicly available. In 2016 [63] the HFO environment was released publicly (open-source)13, however this repository is not currently being maintained. Success in HFO means that the o\ufb00ensive players have to keep possession of the ball (the same as in Keepaway), learn to pass or dribble to get closer to the goal and shoot when possible. Agents can also play defence where they have to prevent goals from being scored. HFO also supports multi-agents which could be controlled manually or automatically. In the same way as the Keepaway environment [58], the HFO environment allows ease of use in developing and deploying agents in di\ufb00erent game scenarios, with C++ and Python interfaces. The performance of three benchmark agents are compared in [63], namely a random agent, a handcoded agent and a SARSA agent. A similar platform to the Arcade Learning Environment (Section 3.2), the HFO environment places less emphasis on generality (the main goal of the ALE) and more emphasis on cooperation and multiagent learning. 3.5 Microsoft TextWorld Recently, researchers from the Microsoft Research Montreal Lab released an open source project called TextWorld [18], which attempts to train reinforcement learning agents using text-based games. In a time where AI agents are mastering complex multi-player games such as Dota 2 and StarCraft II, it might seem unusual to do research on text-based games. Text-based games can play a similar role to multi-player graphic environments which train agents to learn spatial and time-based planning, in advancing conversational skills such as a\ufb00ordance extraction (identifying which verbs are applicable to a given object), memory and planning, exploration etc. Another powerful motivation for the interest in text-based games is that language abstracts away complex physical processes, such as a robot trying not to fall over due to gravity. Text-based games require language understanding and successful play requires skills like long-term memory and planning, exploration (trial and error), common sense, and learning with these challenges. TextWorld is a sandbox environment which enables users to handcraft or automatically generate new games. These games are complex and interactive simulations where text is used to describe the game state and players enter text commands to progress though the game. Natural language is used to describe the state of the world, to accept actions from the player, and to report subsequent changes in the environment. The games are played through a command line terminal and are turn-based, i.e. the simulator describes the state of the game through text and then a player enters a text command to change its state in some desirable way. 3.5.1 Implementation In Figure 4 an example game is shown in order to illustrate the command structure of a typical text-based game generated by TextWorld. 13https://github.com/LARG/HFO DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 18 Figure 4: An example game generated by TextWorld to illustrate the command structure of a game. TextWorld enables interactive playthrough of text-based games and, unlike other text-based environments such as TextPlayer14 and PyFiction15, enables users to handcraft games or to construct games automatically. The TextWorld logic engine automatically builds game worlds, populates them with objects and obstacles, and generates quests that de\ufb01ne a goal state and how to reach it [18]. TextWorld requires Python 3 and currently only supports Linux and macOS systems. The code and documentation are available publicly16 and the learning environment is described in full detail in Section 3 of [18], including descriptions of the two main components of the Python framework: a game generator and a game engine. To interact with TextWorld, the framework provides a simple application programming interface (API) which is inspired by OpenAI Gym. In an RL context, TextWorld games can be seen as partially observable Markov decision processes. The environment state at any turn t contains a complete description of the game state, but much of this is hidden from the agent. Once an agent has issued a command (of at least one word), the environment transitions to a next state with a certain probability. Since the interpreter in parserbased games can accept any sequence of characters (of any length), but only a fraction thereof is recognised, the resulting action space is very large. Therefore, two simplifying assumptions are made in [18]: the commands are sequences of at most L words taken from a \ufb01xed vocabulary V and the commands have to follow a speci\ufb01c structure: a verb, a noun phrase and an adverb phrase. The action space of the agent is therefore the set of all permissible commands from the \ufb01xed vocabulary V followed by a certain special token (\u201center\u201d) that signi\ufb01es the end of the command. The agent\u2019s observation(s) at any time in the game is the text information perceived by the agent. A probability function takes in the environment state and selects what information to show the agent based on the command entered. The agent receives points based on completion of 14https://github.com/danielricks/textplayer 15https://github.com/MikulasZelinka/py\ufb01ction 16http://aka.ms/textworld DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 19 (sub)quests and reaching new locations (exploring). This score could be used as the reward signal if it is available, otherwise positive reward signals can be assigned when the agent \ufb01nishes the game. The agent\u2019s policy maps the state of the environment at any time and words generated in the command so far to the next word, which needs to be added to the command to maximise the reward received. 3.5.2 Benchmark tasks TextWorld was introduced with two di\ufb00erent sets of benchmark tasks [18] and a third task was added in the form of a competition that was available until 31 May 2019. Task 1: A preliminary set of 50 hand-authored benchmark games are described in the original TextWorld paper [18]. These games were manually analysed to ensure validity. Task 2: This benchmark task is inspired by a treasure hunter task which takes place in a 3D environment [64] and was adapted for TextWorld. The agent is randomly placed in a randomly generated map of rooms with two objects on the map. The goal object (the object which the agent should locate) is randomly selected and is mentioned in the welcome message. In order to navigate the map and locate the goal object, the agent may need to complete other tasks, for example \ufb01nding a key to unlock a cabinet. This task assesses the agent\u2019s skills of a\ufb00ordance extraction, e\ufb03cient navigation and memory. There are di\ufb00erent levels for the benchmark, ranging from level 1 to 30, with di\ufb00erent di\ufb03culty modes, number of rooms and quest length. Task 3: The TextWorld environment is still very new: TextWorld was only released to the public in July 2018. A competition \u2013 First TextWorld Problems: A Reinforcement and Language Learning Challenge16, which ran until 31 May 2019, was launched by Microsoft Research Montreal to challenge researchers to develop agents that can solve these text-based games. The challenge is gathering ingredients to cook a recipe. Agents must determine the necessary ingredients from a recipe book, explore the house to gather ingredients, and return to the kitchen to cook up a delicious meal. 3.5.3 Published benchmark results C\u02c6ot\u00b4e et al. [18] evaluate three baseline agents on the benchmark set in Task 1: BYU, Golovin and Simple. The BYU17 agent [65] utilises a variant of Q-learning [66] where word embeddings are trained to be aware of verb-noun a\ufb00ordances. The agent won the IEEE CIG Text-based adventure AI Competition in 2016. The Golovin18 agent [67] was developed speci\ufb01cally for classic text-based games and uses a language model pre-trained on fantasy books to extract important keywords from scene descriptions. The Simple19 agent uniformly samples a command from a prede\ufb01ned set at every step. Results indicated that all three baseline agents achieved low scores in the games. This indicates that there is signi\ufb01cant scope for algorithms to improve on these results. 17https://github.com/danielricks/BYU-Agent-2016 18https://github.com/Kostero/text rpg ai 19https://github.com/Microsoft/TextWorld/tree/master/notebooks DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 20 C\u02c6ot\u00b4e et al. [18] also provide average performance results of three agents (BYU, Golovin and a random agent) on 100 treasure hunter games (task 2) at di\ufb00erent levels of di\ufb03culty. On di\ufb03culty level 1 the Golovin agents had the best average score, but the Random agent completed the game in the least number of steps. As the level of di\ufb03culty increase, the Random agent achieved the best score and also completed the game in the least number of steps. These results can be used as a baseline for evaluating improved algorithms. It is evident that there is still enormous scope for research in the environment of text-based games, and that the generative functionality of the TextWorld sandbox environment is a signi\ufb01cant contribution in the endeavour of researchers trying to solve these problems. 3.6 Summary For the reader\u2019s convenience a summary of the discussed frameworks and algorithms that were shown to be e\ufb00ective are presented in Table 1. It should be noted that since the \ufb01eld moves at a rapid pace, the current state of the art will change (it may also be problem instance dependent within the benchmark class), however the listed algorithms can serve as a reasonable baseline for future research. Framework Benchmark class Recent e\ufb00ective RL algorithm(s) OpenAI Gym Algorithmic UREX [68] Box2D REINFORCE [69] Classic control TNPG and TRPO [13] MuJoCo PPO [21] Roboschool PPO [21] Robotics HER [23]7 Toy text BIRL [70] The ALE Atari 2600 A2C, ACER and PPO [21]; A3C [19]; Distribution DQN, Dueling DDQN, Prioritized DDQN and Rainbow [71] 20 Garage Basic tasks TNPG and TRPO [13] Locomotion tasks PPO [21] Partially observable tasks TNPG and TRPO [13] Hierarchical tasks HIRO [72] Keepaway soccer Keepaway Episodic SMDP SARSA(\u03bb) [46, 1] Half-Field O\ufb00ence SARSA [63] TextWorld Original tasks 1, 2 and 3 BYU and Golovin [18] Generalisation tasks GATA [73] Table 1: A summary of recent algorithms that performed well in di\ufb00erent benchmark sets. DOI TBC ",
    "Discussion": "DISCUSSION This section focuses on the ways that the di\ufb00erent RL benchmarks discussed in Section 3 deal with or facilitate research in addressing the challenges for RL discussed in Section 2.2. 4.1 Partially observable environment In many of the benchmark tasks, such as the classic control tasks in OpenAI Gym, the agent is provided with full information of the environment. The environment in TextWorld games, however, is partially observable since only local information and the player\u2019s inventory are available. The agent might also not be able to distinguish between some states based on observations if only the latest observation is taken into account, i.e. knowledge of past observations are important. In TextWorld games the environment might provide the same feedback for di\ufb00erent commands and some important information about certain aspects of the environment might not be available by a single observation. Additionally, the agent might encounter observations that are time-sensitive, such as only being rewarded when it \ufb01rst examines a clue but not any other time. Controlling the partial observability of the state is also part of TextWorld\u2019s generative functionality. This is done by augmenting the agent\u2019s observations, where the agent can be provided with a list of present objects or even all game state information can be provided. The partially observable tasks introduced in rllab (see Section 3.3.1), provide environments to investigate agents developed for dealing with environments where not all the information is known. In RoboCup, a player can by default only observe objects in a 90-degree cone in front of them. In works from Kuhlmann and Stone [74] and Stone et al. [46] it was shown that it is possible for learning to occur in this limited vision scenario, however players do not perform at an adequate level. For this reason, players in the standardised Keepaway task [58] operate with 360-vision. 4.2 Delayed or sparse rewards The tasks in the ALE and TextWorld are interesting when considering reward structure. In the ALE, reward or feedback may only be seen after thousands of actions. In TextWorld, the agent has to generate a sequence of actions before any change in the environment might occur or a reward is received. This results in sparse and delayed rewards in the games, in cases where an agent could receive a positive reward only after many steps when following an optimal strategy. In Keepaway, there is immediate reward, since the learners receive a positive reward after each action they execute. 4.3 Unspeci\ufb01ed or multi-objective reward functions In HFO (Section 3.4.4) success not only includes maintaining possession of the ball (the main objective in Keepaway), but the o\ufb00ense players also need to learn to pass or dribble to move 20A table summarising the best performance per game can be found at https://github.com/cshenton/atarileaderboard. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 22 towards the goal and shoot when an angle is open. Moreover, success is only evaluated based on a scored goal at the end of an episode, which is rare initially. This aspect of HFO could serve as an ideal environment for investigation into the challenge of problems with multi-objectives. Due the de\ufb01nition of a quest in TextWorld, i.e. a sequence of actions where each action depends on the outcomes of the previous action, quests in TextWorld are limited to simple quests. However, in text adventure games, quests are often more complicated, involving multiple sub-quests. C\u02c6ot\u00b4e et al. [18] remark that this limitation could be overcome by treating a quest as a directed graph of dependent actions rather than a linear chain. If this can be incorporated in TextWorld in the future, the platform can also be used to study problems with multi-objectives and rewards of varying di\ufb03culty. 4.4 Size of the state and action spaces The benchmark tasks that are considered in this paper are ideal to investigate how the size of the state and/or action space challenge can be addressed. The tasks considered all have continuous or large discrete state spaces. In the ALE the number of states in the games are very large and in TextWorld the state space is combinatorially enormous; since the number of possible states increases exponentially with the number of rooms and objects [18]. In most of the tasks in OpenAI Gym, rllab, and in Keepaway, the state space is continuous. In Keepaway, the size of the Keepaway region can be varied along with the number of keepers and takers. This allows for investigation into a problem with various di\ufb03culties due to the size of the state space. In TextWorld, the action space is large and sparse because the set of all possible word strings is much larger than the subset of valid commands. TextWorld\u2019s generative functionality also allows control over the size of the state space, i.e. the number of rooms, objects and commands. Di\ufb00erent problem di\ufb03culties can therefore arise in terms of the size of the state space and this can aid in the investigation of algorithm behaviour with increasing state and action spaces. 4.5 The trade-o\ufb00 between exploration and exploitation In the ALE the challenge of exploration vs. exploitation is di\ufb03cult due to the large state spaces of games and delayed reward. Simple agents sometimes even learn that staying put is the best policy, since exploration can in some cases lead to negative rewards. Recently there has been some e\ufb00ort to address the exploration problem in the ALE, but these e\ufb00orts are mostly successful only in individual games. Exploration is fundamental to TextWorld games as solving them can not be done by learning a purely exploitative or reactive agent. The agent must use directed exploration as its strategy, where it collects information about objects it encounters along the way. This information will provide knowledge about the goal of the game and provide insight into the environment and what might be useful later in the game. Due to this, exploration by curiosity driven agents might fair well in these types of problems. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 23 Overall, there is still much work to be done to try and overcome this di\ufb03cult challenge. Machado et al. [15] suggest a few approaches for the ALE, such as agents capable of exploring in a more abstract manner (akin to humans) and agents not exploring joystick movements, but rather exploring object con\ufb01gurations and game levels. Agents with some form of intrinsic motivation might also be needed in order to continue playing even though achieving any reward might seem impossible. 4.6 Representation learning The original goal of the ALE was to develop agents capable of generalising over many games making it desirable to automatically learn representations instead of hand crafting features. Deep Q-Networks (DQN) [4] and DQN-like approaches are currently the best overall performing methods, despite high sample complexity. However, additional tuning is often required to obtain better performance [75], which suggest that there is still work to be done to improve performance by learning better representation in the ALE. Other di\ufb00erent approaches and directions for representation learning that have been used in the literature are also mentioned in [15] and should still be explored more in the ALE. 4.7 Transfer learning Regarding the ALE, many of the Atari 2600 games have similar game dynamics and knowledge transfer should reduce the number of samples that are required to learn to play games that are similar. Even more challenging would be determining how to use general video game experience and share that knowledge across games that are not necessarily similar. Current approaches in the literature that apply transfer learning in the ALE are restricted to only a limited subset of games that share similarities and the approaches are based on using neural networks to perform transfer, combining representations and policy transfer. Machado et al. [15] point out that it might be interesting to determine whether transferring each of these entities independently could be helpful. To help with the topic of transfer learning in the ALE, the new version includes di\ufb00erent game modes and di\ufb03culty settings called \ufb02avours (see Section 3.2), which introduces many new environments that are very similar. Some of the tasks in rllab and environments in OpenAI Gym have been used in studying the transferring of system dynamics from simulation to robots [76, 77, 78]. These simulation tasks are an ideal way to safely study the transferring of policies for robotic domains. Transfer learning has also been studied in the Keepaway soccer domain [49], which is a \ufb01tting setting since the number of players as well as the size of the action and state spaces can di\ufb00er. TextWorld\u2019s generative functionality (described in full in [18]) allows for control of the size and the partial observability of the state space, and therefore a large number of games with shared characteristics can be generated. This could be used for studying transfer learning in text-based games, since agents can be trained on simpler tasks and behaviour transferred to harder problems. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 24 4.8 Model learning Planning and model learning in complex domains are challenging problems and little research has been conducted on this topic compared to traditional RL techniques to learn policies or value functions. In the ALE, the Stella emulator provides a generative model that can be used in planning and the agent has an exact model of the environment. However, there has not been any success with planning using a learned generative model in the ALE, which is a challenging task since errors start to compound after only a few time steps. A few relatively successful approaches [79, 80] are available, but the models are slower than the emulator. A challenging open problem is to learn a fast and accurate model for the ALE. On the other hand, related to this, is the problem of planning using an imperfect model. On tasks in OpenAI Gym and rllab some research has also been conducted in model learning [81, 82], but the main focus in the literature is on model-free learning techniques. Therefore there is still scope for substantial research to address this problem. Wang et al. [82] attempted to address the lack of a standardised benchmarking framework for model-based RL. They benchmarked 11 model-based RL algorithms and four model-free RL algorithms across 18 environments from OpenAI Gym and have shared the code in an online repository21. They evaluated the e\ufb03ciency, performance and robustness of three di\ufb00erent categories of model-based RL algorithms (Dyna style algorithms, policy search with backpropagation through time and shooting algorithms) and four model-free algorithms (TRPO, PPO, TD3, and SAC \u2013 refer to Section 2.1.8 for these algorithms). They also propose three key research challenges for model-based methods, namely the dynamics bottleneck, the planning horizon dilemma, and the early termination dilemma and show that even with substantial benchmarking, there is no clear consistent best model-based RL algorithm. This again suggests that there is substantial scope and many opportunities for further research in model-based RL methods. 4.9 O\ufb00-policy learning Deep neural networks have become extremely popular in modern RL literature, and the breakthrough work of Mnih et al. [3, 4] demonstrates DQN having human-level performance on Atari 2600 games. However, when using deep neural networks for function approximation for o\ufb00-policy algorithms, new and complex challenges arise, such as instability and slow convergence. While discussing o\ufb00-policy methods using function approximation, Sutton and Barto [1] conclude the following: \u201cThe potential for o\ufb00-policy learning remains tantalizing, the best way to achieve it still a mystery.\u201d Nevertheless, o\ufb00-policy learning has become an active research \ufb01eld in RL. The use of o\ufb00-policy learning algorithms in the ALE in current literature varies with most approaches using experience replay and target networks. This is an attempt at reducing divergence in o\ufb00-policy learning, but these methods are very complex. New proposed algorithms such as GQ(\u03bb) [83] are theoretically sound, but there is still a need for a thorough empirical evaluation or demonstration of these theoretically sound o\ufb00-policy learning RL algorithms. Other contributions 21http://www.cs.toronto.edu/\u223ctingwuwang/mbrl.html DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 25 of using o\ufb00-policy learning in the ALE includes double Q-learning [84] and Q(\u03bb) with o\ufb00-policy corrections [85]. Some of the tasks in rllab and OpenAI Gym have also been used in studying o\ufb00-policy algorithms, for example introducing the soft actor-critic (SAC) algorithm [24] and using the robotics environments from OpenAI Gym to learn grasping [86]. This area of research is still new and there is signi\ufb01cant scope for further research in this domain. 4.10 Reinforcement learning in real-world settings The robotics environments in the OpenAI Gym toolkit can be used to train models which work on physical robots. This can be used to develop agents to safely execute realistic tasks. A request for research from OpenAI7 indicates that work in this area is an active research \ufb01eld with promising results. The Keepaway and HFO soccer tasks are ideal settings to study multi-agent RL [87], an important research area for real-world problems since humans act in an environment where objectives are shared with others. Challenges for RL that are unique to TextWorld games are related to natural language understanding: observation modality, understanding the parser feedback, common-sense reasoning and a\ufb00ordance extraction, and language acquisition. These challenges are explained in more detail in C\u02c6ot\u00b4e et al. [18]. Natural language understanding is an important aspect of arti\ufb01cial intelligence, in order for communication to take place between humans and AI. TextWorld can be used to address many of the challenges described in Section 2.2 in simpler settings and to focus on testing and debugging agents on subsets of these challenges. In addition to the frameworks covered in this survey, there are two further contributions that are focused on multi-agent and distributed RL. The MAgent research platform [88] facilitates research in many-agent RL, speci\ufb01cally in arti\ufb01cial collective intelligence. The platform aims at supporting RL research that scales up from hundreds to millions of agents and is maintained in an online repository22. MAgent also provides a visual interface presenting the state of the environment and agents. A research team from Stanford has introduced the open-source framework SURREAL (Scalable Robotic REinforcementlearning ALgorithms) and the SURREAL Robotics Suite [89], to facilitae research in RL in robotics and distributed RL. SURREAL eliminates the need for global synchronization and improves scalability by decoupling a distributed RL algorithm into four components. The four-layer computing infrastructure can easily be deployed on commercial cloud providers or personal computers, and is also fully replicable from scratch, contributing to the reproducibility of results. The Robotics Suite is developed in the MuJoCo physics engine and provides OpenAI gym-style interfaces in Python. Detailed API documentation and tutorials on importing new robots and the creation of new environments and tasks are also provided, furthering the contribution to research in this \ufb01eld. The Robotics Suite is actively maintained in an online 22https://github.com/geek-ai/MAgent DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 26 repository23. The di\ufb00erent robotics tasks include block lifting and stacking, bimanual peg-in-hole placing and bimanual lifting, bin picking, and nut-and-peg assembly. Variants of PPO and DDPG called SURREAL-PPO and SURREAL-DDPG were developed and examined on the Robotics Suite tasks, and experiments indicate that these SURREAL algorithms can achieve good results. 4.11 A standard methodology for benchmarking The ALE consists of games with similar structure in terms of of inputs, action movements, etc. This makes the ALE an ideal benchmark for comparative studies. A standard methodology is however needed and this is proposed by Machado et al. [15]: \u2022 Episode termination can be standardised by using the game over signal than lives lost. \u2022 Hyperparameter tuning needs to be consistently applied on the training set only. \u2022 Training time should be consistently applied across di\ufb00erent problems. \u2022 There is a need for standard ways of reporting learning performance. These same principles apply to groups of similar tasks in OpenAI Gym and rllab, and to TextWorld and Keepaway soccer. 4.12 Trends in benchmarking of RL It is clear from Section 3 that the number of well thought-out frameworks designed for RL benchmarks has rapidly expanded in recent years, with a general move to fully open source implementations being evident. A notable example is OpenAI Gym re-implementing, to an extent, open source variants of the benchmarks previously provided in the MuJoCo simulation environment. The move to fully open source implementations has had two primary bene\ufb01ts: reproducibility and accessibility. The variety of RL frameworks and benchmark sets may present a challenge to a novice in the \ufb01eld, as there is no clear standard benchmark set or framework to use. This is not a surprising situation as the array of RL application areas has become relatively diverse and so di\ufb00erent types of problems and their corresponding challenges will naturally be more interesting to certain sub-communities within the \ufb01eld. One aspect of modern RL benchmarks that is relatively striking is the increase in problem complexity. While it is not immediately clear how to precisely de\ufb01ne problem di\ufb03culty, it is clear that more and more problem features that are challenging for RL algorithms are being included in proposed benchmarks. Many established benchmark sets have been explicitly expanded to increase the challenge of a given problem instance. Some notable examples include the addition of sticky actions in the ALE and the addition of the partially observable variants of rllab\u2019s continuous control tasks. 23https://github.com/SurrealAI/surreal DOI TBC ",
    "Conclusion": "CONCLUSION This paper provides a survey of some of the most used and recent contributions to RL benchmarking. A number of benchmarking frameworks are described in terms of their characteristics, technical implementation details and the tasks provided. A summary is also provided of published results on the performance of algorithms used to solve these benchmark tasks. Challenges that occur when solving RL problems are also discussed, including the various ways the di\ufb00erent benchmarking tasks address or facilitate research in addressing these challenges. The survey reveals that there has been substantial progress in the endeavour of standardising benchmarking tasks for RL. The research community has started to acknowledge the importance of reproducible results and research has been published to encourage the community to address this problem. However, there is still a lot to be done in ensuring the reproducibility of results for fair comparison. There are many approaches when solving RL problems and proper benchmarks are important when comparing old and new approaches. This survey indicates that the tasks currently used for benchmarking RL encompass a wide range of problems and can even be used to develop algorithms for training agents in real-world systems such as robots. ",
    "References": "References [1] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018. ISBN 978-0262039246. [2] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015. [3] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, A. Antonoglou, A. Wierstra, and M. Riedmiller. Playing Atari with deep reinforcement learning. arXiv:1312.5602, 2013. [4] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529, 2015. doi: https://doi.org/10.1038/nature14236. [5] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani, H. K\u00a8uttler, J. Agapiou, J. Schrittwieser, J. Quan, S. Ga\ufb00ney, S. Petersen, K. Simonyan, DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 28 T. Schaul, H. van Hasselt, D. Silver, T. Timothy Lillicrap, K. Kevin Calderone, P. Paul Keet, A. Brunasso, D. Lawrence, A. Ekermo, J. Repp, and R. Tsing. StarCraft II: A new challenge for reinforcement learning. arXiv:1708.04782, 2017. [6] V. d. N. Silva and L. Chaimowicz. MOBA: A new arena for game AI. arXiv:1705.10443, 2017. [7] I. Arel, C. Liu, T. Urbanik, and A. Kohls. Reinforcement learning-based multi-agent system for network tra\ufb03c signal control. IET Intelligent Transport Systems, 4(2):128\u2013135, 2010. doi: https://doi.org/10.1049/iet-its.2009.0070. [8] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484\u2013489, 2016. doi: https://doi.org/10.1038/nature16961. [9] M. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313\u2013330, jun 1993. ISSN 0891-2017. [10] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, Dec 2015. ISSN 1573-1405. doi: https://doi.org/10.1007/s11263-015-0816-y. [11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88(2): 303\u2013338, Jun 2010. ISSN 1573-1405. doi: https://doi.org/10.1007/s11263-009-0275-4. [12] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47: 253\u2013279, 2013. ISSN 1076-9757. doi: https://doi.org/10.1613/jair.3912. [13] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on International Conference on Machine Learning, ICML\u201916, pages 1329\u20131338, 2016. [14] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, pages 3207\u20133214, 2018. [15] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting the Arcade Learning Environment: Evaluation protocols and open problems for general agents. Journal of Arti\ufb01cial Intelligence Research, 61:523\u2013562, 2018. ISSN 1076-9757. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 29 [16] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. arXiv:1606.01540, 2016. [17] P. Stone and R. S. Sutton. Keepaway soccer: A machine learning test bed. In Robot Soccer World Cup, pages 214\u2013223. Springer, 2001. doi: https://doi.org/10.1007/11780519 9. [18] M. A. C\u02c6ot\u00b4e, \u00b4A K\u00b4ad\u00b4ar, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. J. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. TextWorld: A learning environment for text-based games. arXiv:1806.11532, 2018. [19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on International Conference on Machine Learning, ICML\u201916, pages 1928\u20131937, 2016. [20] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, ICML\u201915, pages 1889\u20131897, 2015. [21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. [22] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, ICML\u201917, pages 449\u2013458, 2017. [23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems 30, pages 5048\u20135058, 2017. [24] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: O\ufb00-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, ICML\u201918, pages 1861\u20131870, 2018. [25] S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actorcritic methods. In Proceedings of the 35th International Conference on Machine Learning, ICML\u201918, pages 1587\u20131596, 2018. [26] D. Silver, R. S. Sutton, and M. M\u00a8uller. Temporal-di\ufb00erence search in computer Go. Machine Learning, 87(2):183\u2013219, 2012. ISSN 1573-0565. doi: https://doi.org/10.1007/s10994-0125280-0. [27] M. Minsky. Steps toward arti\ufb01cial intelligence. Proceedings of the IRE, 49(1):8\u201330, 1961. doi: https://doi.org/10.1109/JRPROC.1961.287775. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 30 [28] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, 1957. ISBN 978-0486428093. [29] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, 2010. ISSN 1041-4347. doi: https://doi.org/10.1109/ TKDE.2009.191. [30] K. Weiss, T. M. Khoshgoftaar, and D. Wang. A survey of transfer learning. Journal of Big data, 3(9), 2016. ISSN 2196-1115. doi: https://doi.org/10.1186/s40537-016-0043-6. [31] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633\u20131685, 2009. ISSN 1532-4435. [32] G. Dulac-Arnold, D. Mankowitz, and T. Hester. Challenges of real-world reinforcement learning. In Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th International Conference on Machine Learning, 2019. arXiv:1904.12901. [33] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012. doi: https://doi.org/10.1109/IROS.2012.6386109. [34] Y. Liang, M. C. Machado, E. Talvitie, and M. Bowling. State of the art control of Atari games using shallow reinforcement learning. In Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent Systems, pages 485\u2013493. International Foundation for Autonomous Agents and Multiagent Systems, 2016. [35] C. Dann, G. Neumann, and J. Peters. Policy evaluation with temporal di\ufb00erences: A survey and comparison. Journal of Machine Learning Research, 15:809\u2013883, 2014. ISSN 1532-4435. [36] B. Papis and P. Wawrzy\u00b4nski. dotRL: A platform for rapid reinforcement learning methods development and validation. In 2013 Federated Conference on Computer Science and Information Systems, pages 129\u2013136. IEEE, 2013. [37] B. G. Woolley and K. O. Stanley. Evolving a single scalable controller for an octopus arm with a variable number of segments. In Parallel Problem Solving from Nature, PPSN XI, pages 270\u2013 279, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. doi: https://doi.org/10.1007/9783-642-15871-1 28. [38] T. Schaul, J. Bayer, D. Wierstra, Y. Sun, M. Felder, F. Sehnke, T. R\u00a8uckstie\u00df, and J. Schmidhuber. PyBrain. Journal of Machine Learning Research, 11(Feb):743\u2013746, 2010. ISSN 1532-4435. [39] A. Yamaguchi and T. Ogasawara. SkyAI: Highly modularized reinforcement learning library. In 2010 10th IEEE-RAS International Conference on Humanoid Robots, pages 118\u2013123. IEEE, 2010. doi: https://doi.org/10.1109/ICHR.2010.5686285. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 31 [40] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229, 1992. ISSN 0885-6125. doi: https://doi.org/10.1007/ BF00992696. [41] J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 745\u2013750, 2007. doi: https://doi.org/10.1145/1273496.1273590. [42] J. Peters, K. Mulling, and Y. Altun. Relative entropy policy search. In Proceedings of the Twenty-Fourth AAAI Conference on Arti\ufb01cial Intelligence, AAAI\u201910, pages 1607\u20131612, 2010. [43] R. Rubinstein. The cross-entropy method for combinatorial and continuous optimization. Methodology And Computing In Applied Probability, 1(2):127\u2013190, 1999. ISSN 1573-7713. doi: https://doi.org/10.1023/A:1010091220143. [44] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159\u2013195, 2001. ISSN 1063-6560. doi: https://doi.org/10. 1162/106365601750190398. [45] H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa. RoboCup: The Robot World Cup Initiative. In Proceedings of the First International Conference on Autonomous Agents, AGENTS \u201997, pages 340\u2013347, 1997. doi: https://doi.org/10.1145/267658.267738. [46] P. Stone, R. S. Sutton, and G. Kuhlmann. Reinforcement learning for RoboCup soccer Keepaway. Adaptive Behavior, 13(3):165\u2013188, 2005. ISSN 1059-7123. doi: https://doi.org/10. 1177/105971230501300301. [47] A. D. Pietro, L. While, and L. Barone. Learning in RoboCup keepaway using evolutionary algorithms. In Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation, GECCO\u201902, pages 1065\u20131072, 2002. [48] T. Walker, J. Shavlik, and R. Maclin. Relational reinforcement learning via sampling the space of \ufb01rst-order conjunctive features. In Proceedings of the ICML Workshop on Relational Reinforcement Learning, 2004. [49] M. E. Taylor and P. Stone. Behavior transfer for value-function-based reinforcement learning. In Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, AAMAS \u201905, pages 53\u201359, 2005. doi: https://doi.org/10.1145/1082473.1082482. [50] S. Didi and G. Nitschke. Multi-agent behavior-based policy transfer. In European Conference on the Applications of Evolutionary Computation, EvoApplications, pages 181\u2013197, 2016. doi: https://doi.org/10.1007/978-3-319-31153-1 13. [51] S. Didi and G. Nitschke. Hybridizing novelty search for transfer learning. In IEEE Symposium Series on Computational Intelligence (SSCI), pages 1\u20138, 2016. doi: https://doi.org/10.1109/ SSCI.2016.7850180. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 32 [52] G. Nitschke and S. Didi. Evolutionary policy transfer and search methods for boosting behavior quality: Robocup keep-away case study. Frontiers in Robotics and AI, 4:62, 2017. ISSN 2296-9144. doi: https://doi.org/10.3389/frobt.2017.00062. [53] S. Didi and G. Nitschke. Policy transfer methods in RoboCup keep-away. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO \u201918, pages 117\u2013118, 2018. ISBN 978-1-4503-5764-7. doi: https://doi.org/10.1145/3205651.3205710. [54] D. Schwab, Y. Zhu, and M. Veloso. Zero shot transfer learning for robot soccer. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS\u201918, pages 2070\u20132072, 2018. [55] Q. Cheng, X. Wang, Y. Niu, and L. Shen. Reusing source task knowledge via transfer approximator in reinforcement transfer learning. Symmetry, 11(1), 2018. ISSN 2073-8994. doi: https://doi.org/10.3390/sym11010025. [56] M. Riedmiller, T. Gabel, R. Hafner, and S. Lange. Reinforcement learning for robot soccer. Autonomous Robots, 27(1):55\u201373, 2009. doi: https://doi.org/10.1007/s10514-009-9120-4. [57] A. Bai and S. Russell. E\ufb03cient reinforcement learning with hierarchies of machines by leveraging internal transitions. In Proceedings of the Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence, pages 1418\u20131424, 2017. doi: https://doi.org/10.24963/ijcai.2017/196. [58] P. Stone, G. Kuhlmann, M. E. Taylor, and Y. Liu. Keepaway soccer: From machine learning testbed to benchmark. In Robot Soccer World Cup, pages 93\u2013105, 2005. doi: https://doi.org/10.1007/11780519 9. [59] P. Stone. Layered learning in multiagent systems: A winning approach to robotic soccer. MIT Press, 2000. ISBN 978-0819428448. [60] J. S. Albus. A new approach to manipulator control: The cerebellar model articulation controller (CMAC). Journal of Dynamic Systems, Measurement, and Control, 97(3):220\u2013227, 1975. doi: https://doi.org/10.1115/1.3426922. [61] J. S. Albus. Brains, Behavior and Robotics. McGraw-Hill, Inc., 1981. ISBN 0070009759. [62] S. Kalyanakrishnan, Y. Liu, and P. Stone. Half Field O\ufb00ense in RoboCup Soccer: A Multiagent Reinforcement Learning Case Study. In RoboCup 2006: Robot Soccer World Cup X, pages 72\u201385, 2007. doi: https://doi.org/10.1007/978-3-540-74024-7 7. [63] M. Hausknecht, P. Mupparaju, S. Subramanian, S. Kalyanakrishnan, and P. Stone. Half \ufb01eld o\ufb00ense: An environment for multiagent learning and ad hoc teamwork. In AAMAS Adaptive Learning Agents (ALA) Workshop, 2016. [64] E. Parisotto and R. Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. arXiv:1702.08360, 2017. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 33 [65] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate. What can you do with a rock? A\ufb00ordance extraction via word embeddings. In Proceedings of the 26th International Joint Conference on Arti\ufb01cial Intelligence, IJCAI\u201917, pages 1039\u20131045, 2017. doi: https://doi.org/10.24963/ ijcai.2017/144. [66] J. C. H. Watkins and P Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992. doi: https://doi.org/10.1007/BF00992698. [67] B. Kostka, J. Kwiecieli, J. Kowalski, and P. Rychlikowski. Text-based adventures of the Golovin AI agent. In 2017 IEEE Conference on Computational Intelligence and Games (CIG), pages 181\u2013188, 2017. doi: https://doi.org/10.1109/CIG.2017.8080433. [68] O Nachum, M. Norouzi, and D. Schuurmans. Improving policy gradient by exploring underappreciated rewards. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. arXiv:1611.09321. [69] D. Ha. Reinforcement learning for improving agent design. Arti\ufb01cial Life, 25(4):352\u2013365, 2019. doi: https://doi.org/10.1162/artl a 00301. [70] C. Cundy and D. Filan. Exploring hierarchy-aware inverse reinforcement learning. arXiv:1807.05037, 2018. [71] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [72] O. Nachum, S. Gu, H. Lee, and S. Levine. Data-e\ufb03cient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems 31, pages 3303\u20133313. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/7591-data-e\ufb03cient-hierarchical-reinforcementlearning.pdf. [73] A. Adhikari, X. Yuan, M. A. C\u02c6ot\u00b4e, M. Zelinka, M. A. Rondeau, R. Laroche, P. Poupart, J. Tang, A. Trischler, and W. L. Hamilton. Learning dynamic knowledge graphs to generalize on text-based games. arXiv preprint arXiv:2002.09127, 2020. [74] G. Kuhlmann and P. Stone. Progress in learning 3 vs. 2 Keepaway. In IEEE International Conference on Systems, Man and Cybernetics, SMC\u201903, pages 52\u201359, 2003. doi: https: //doi.org/10.1109/ICSMC.2003.1243791. [75] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. In ICML Workshop on Reproducibility in Machine Learning, ICML\u201917, 2017. arXiv:1708.04133. [76] D. Held, Z. McCarthy, M. Zhang, F. Shentu, and P. Abbeel. Probabilistically safe policy transfer. In IEEE International Conference on Robotics and Automation (ICRA), pages 5798\u20135805, 2017. doi: https://doi.org/10.1109/ICRA.2017.7989680. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 34 [77] M. Wulfmeier, I. Posner, and P. Abbeel. Mutual alignment transfer learning. In Proceedings of the 1st Annual Conference on Robot Learning, 2017. [78] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In IEEE International Conference on Robotics and Automation (ICRA), pages 1\u20138, 2018. doi: https://doi.org/10.1109/ICRA.2018.8460528. [79] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in Atari games. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2863\u20132871, 2015. [80] S. Chiappa, S. Racaniere, D. Wierstra, and S. Mohamed. Recurrent environment simulators. arXiv:1704.02254, 2017. [81] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for modelbased deep reinforcement learning with model-free \ufb01ne-tuning. In IEEE International Conference on Robotics and Automation (ICRA), pages 7559\u20137566, 2018. doi: https://doi. org/10.1109/ICRA.2018.8463189. [82] T. Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba. Benchmarking model-based reinforcement learning. arXiv:1907.02057, 2019. [83] H. R. Maei and R. S. Sutton. GQ(lambda): A general gradient algorithm for temporaldi\ufb00erence prediction learning with eligibility traces. In 3rd Conference on Arti\ufb01cial General Intelligence (AGI-2010). Atlantis Press, 2010. ISBN 978-90-78677-36-9. doi: https://doi.org/ 10.2991/agi.2010.22. [84] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double Q-learning. In Proceedings of the Thirtieth AAAI Conference on Arti\ufb01cial Intelligence, AAAI\u201916, pages 2094\u20132100, 2016. [85] A. Harutyunyan, M. G. Bellemare, T. Stepleton, and R. Munos. Q(\u03bb) with o\ufb00-policy corrections. In Algorithmic Learning Theory, pages 305\u2013320, 2016. ISBN 978-3-319-46379-7. doi: https://doi.org/10.1007/978-3-319-46379-7 21. [86] D. Quillen, E. Jang, O. Nachum, C. Finn, J. Ibarz, and S. Levine. Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of o\ufb00-policy methods. In IEEE International Conference on Robotics and Automation (ICRA), pages 6284\u20136291, 2018. doi: https://doi.org/10.1109/ICRA.2018.8461039. [87] L. Bu\u00b8soniu, R. Babu\u02c7ska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156\u2013172, 2008. ISSN 1094-6977. doi: https://doi.org/10.1109/TSMCC. 2007.913919. DOI TBC Stapelberg, B. and Malan, K.M.: A survey of benchmarks for reinforcement learning algorithms 35 [88] L. Zheng, J. Yang, H. Cai, M. Zhou, W. Zhang, J. Wang, and Y. Yu. MAgent: A many-agent reinforcement learning platform for arti\ufb01cial collective intelligence. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [89] L. Fan, Y. Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa, S. Savarese, and L. FeiFei. SURREAL: Open-source reinforcement learning framework and robot manipulation benchmark. In Proceedings of The 2nd Conference on Robot Learning, volume 87 of PMLR, pages 767\u2013782, 2018. [90] Y. Lecun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. doi: https://doi.org/10.1109/ 5.726791. DOI TBC ",
    "title": "A survey of benchmarking frameworks for",
    "paper_info": "SACJ Vol TBC(Num TBC) MONTH OF PUBLICATION 2020\nResearch Article\nA survey of benchmarking frameworks for\nreinforcement learning\nBelinda Stapelberg\na, Katherine M. Malan\na\na Department of Decision Sciences, University of South Africa, Pretoria, South Africa\nABSTRACT\nReinforcement learning has recently experienced increased prominence in the machine learning community. There\nare many approaches to solving reinforcement learning problems with new techniques developed constantly. When\nsolving problems using reinforcement learning, there are various di\ufb03cult challenges to overcome.\nTo ensure progress in the \ufb01eld, benchmarks are important for testing new algorithms and comparing with\nother approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements\nare accurately judged. This paper provides an overview of di\ufb00erent contributions to reinforcement learning\nbenchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning.\nThe contributions discussed are the most used and recent in the literature. The paper discusses the contributions\nin terms of implementation, tasks and provided algorithm implementations with benchmarks.\nThe survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available\nand to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for\nresearchers not familiar with the di\ufb00erent tasks that can be used to develop and test new reinforcement learning\nalgorithms.\nKeywords: reinforcement learning, benchmarking\nCategories:\n\u2022 Computing methodologies \u223c Reinforcement learning\nEmail:\nBelinda Stapelberg belinda.stapelberg@up.ac.za (CORRESPONDING),\nKatherine M. Malan malankm@unisa.ac.za\nArticle history:\nReceived:\nAccepted:\nAvailable online:\n1\nINTRODUCTION\nReinforcement learning (RL) is a sub\ufb01eld of machine learning, based on rewarding desired\nbehaviours and/or punishing undesired ones of an agent interacting with its environment [1]. The\nagent learns by taking sequential actions in its environment, observing the state of the environment\nand receiving a reward. The agent needs to learn a strategy, called a policy, to decide which action\nto take in any state. The goal of RL is to \ufb01nd the policy that maximises the long-term reward of\nthe agent.\nIn recent years RL has experienced dramatic growth in research attention and interest due to\npromising results in areas including robotics control [2], playing Atari 2600 [3, 4], competitive video\nBelinda Stapelberg and Katherine M. Malan (2020). A survey of benchmarking frameworks for reinforcement\nlearning. South African Computer Journal Vol TBC(Num TBC), 1\u201335. DOI TBC\nCopyright \u00a9 the author(s); published under a Creative Commons NonCommercial 4.0 License (CC BY-NC 4.0).\nSACJ is a publication of the South African Institute of Computer Scientists and Information Technologists. ISSN\n1015-7999 (print) ISSN 2313-7835 (online).\narXiv:2011.13577v1  [cs.LG]  27 Nov 2020\n",
    "GPTsummary": "                    - (1): The research background of this article is the increasing prominence of reinforcement learning in the machine learning community and the need for benchmarks in testing new algorithms and comparing them with other approaches to ensure progress in the field.\n\n                    - (2): The past methods involve using games and tasks to test and evaluate the performance of artificial intelligence systems, but there was a lack of standardization and reproducibility for accurate benchmarking of RL performance. The approach in the paper is well-motivated as it provides an overview of different contributions to RL benchmarking and how they can assist researchers in addressing the challenges.\n\n                    - (3): The research methodology proposed in this paper is a survey of the most important and most recent contributions to benchmarking for RL, such as OpenAI Gym, the Arcade Learning Environment, and Microsoft TextWorld. The paper discusses the challenges of RL in terms of important RL benchmarking contributions and in what manner the benchmarks can be used to overcome or address these challenges.\n\n                    - (4): The methods in this paper were evaluated on a variety of different tasks in the literature, and their performance was compared against other approaches. The paper aims to encourage research to take place in a standardized manner and bring attention to the wide range of RL benchmarking tasks available. However, the paper does not provide specific numerical results for the performance achieved.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to provide a comprehensive survey of benchmarking frameworks for reinforcement learning and highlight the importance of standardization and reproducibility in RL research. This paper encourages researchers to evaluate RL algorithms on a wide range of benchmark tasks to promote progress and fair comparison in the field.\n\n- (2): Innovation point: This paper provides an overview of the most important and recent contributions to RL benchmarking, which serves as a comprehensive resource for researchers interested in evaluating their RL algorithms. However, the paper does not present any new benchmarking frameworks or novel approaches to RL benchmarking.\n\n    Performance: The paper critically evaluates existing RL benchmarking frameworks and compares their performance on various tasks based on published results. However, the paper does not provide specific numerical results for the performance achieved.\n\n    Workload: The methodology proposed in this paper is a survey of the most important and recent contributions to RL benchmarking, which does not require extensive experimentation or computational resources. However, reviewing and comparing a large number of benchmarking frameworks requires significant effort and expertise.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to provide a comprehensive survey of benchmarking frameworks for reinforcement learning and highlight the importance of standardization and reproducibility in RL research. This paper encourages researchers to evaluate RL algorithms on a wide range of benchmark tasks to promote progress and fair comparison in the field.\n\n- (2): Innovation point: This paper provides an overview of the most important and recent contributions to RL benchmarking, which serves as a comprehensive resource for researchers interested in evaluating their RL algorithms. However, the paper does not present any new benchmarking frameworks or novel approaches to RL benchmarking.\n\n    Performance: The paper critically evaluates existing RL benchmarking frameworks and compares their performance on various tasks based on published results. However, the paper does not provide specific numerical results for the performance achieved.\n\n    Workload: The methodology proposed in this paper is a survey of the most important and recent contributions to RL benchmarking, which does not require extensive experimentation or computational resources. However, reviewing and comparing a large number of benchmarking frameworks requires significant effort and expertise.\n\n\n"
}