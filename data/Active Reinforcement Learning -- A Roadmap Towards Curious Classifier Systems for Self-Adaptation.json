{
    "Abstract": "Abstract: Intelligent systems have the ability to improve their behaviour over time taking observations, experiences or explicit feedback into account. Traditional approaches separate the learning problem and make isolated use of techniques from different \ufb01eld of machine learning such as reinforcement learning, active learning, anomaly detection or transfer learning, for instance. In this context, the fundamental reinforcement learning approaches come with several drawbacks that hinder their application to real-world systems: trial-and-error, purely reactive behaviour or isolated problem handling. The idea of this article is to present a concept for alleviating these drawbacks by setting up a research agenda towards what we call \u201cactive reinforcement learning\u201d in intelligent systems. 1 ",
    "Introduction": "INTRODUCTION Information and communication technology faces a trend towards increasingly complex solutions, e.g., characterised by the laws of Moore (Moore, 1965) and Glass (Glass, 2002). As a consequence, traditional concepts for design, development, and maintenance have reached their limits. Within the last decade, a paradigm shift in engineering such systems has been postulated that claims to master complexity issues by means of self-adaptation and selforganisation. Concepts and techniques emerged that move traditional design-time decisions to runtime and from the system engineer to the systems themselves. As a result, intelligent and autonomously acting systems are targeted, with the self-adapting and selforganising (SASO) systems domain serving as an umbrella for several research initiatives focusing on these issues, including Organic Computing (M\u00a8ullerSchloer and Tomforde, 2017), Autonomic Computing (Kephart and Chess, 2003), Interwoven Systems (Tomforde et al., 2014), or Self-aware Computing Systems (Kounev et al., 2017). The basic idea is in all cases that individual systems react autonomously to changing conditions, \ufb01nd appropriate reactions, and optimise this process over a https://orcid.org/0000-0001-8951-8962 b https://orcid.org/0000-0002-5825-8915 time\u2014resulting in intelligent system behaviour. For the remainder of this article, we de\ufb01ne such an \u201cintelligent system\u201d (according to (Tomforde et al., 2017)) as a computing system that achieves or maintains a certain level of performance, even when operating in environments that change over time and even if it is exposed to disturbances or emergent situations. Such an intelligent system is autonomously alerting its own behaviour with the goal to improve it over time. A keystone in this de\ufb01nition is the ability of an intelligent system to learn autonomously at runtime (D\u2019Angelo et al., 2019). This means that approaches based on massive training data or continuous feedback/supervision by users are not feasible. In turn, the system has to \ufb01gure out what to do in which situation: the classic reinforcement learning (RL) paradigm combined with further mechanisms from the domain of machine learning such as anomaly detection, transfer learning, or collaborative learning (D\u2019Angelo et al., 2020). Several approaches have been presented where varying RL techniques are used for enabling selfimproving runtime adaptation and organisation. However, these approaches come with several drawbacks that hinder their application to real-world systems: trial-and-error, purely reactive behaviour, isolated problem handling, etc. The idea of this article is to present a concept for alleviating these drawbacks by setting up a research agenda towards what we call arXiv:2201.03947v1  [cs.LG]  11 Jan 2022 ",
    "Background": "BACKGROUND This section brie\ufb02y describes basic concepts necessary for understanding the remainder of this article. We initially introduce our model of an intelligent system with an emphasis on learning autonomously at runtime. Afterwards, we explain the basic RL paradigm and the most popular variant as used in intelligent systems. Finally, we summarise the concept of active learning (AL) with a special focus on stream data consideration as most prominent variant for utilisation in intelligent systems. 2.1 System model We assume a SASO system S to consist of a potentially large set A of autonomous subsystems ai in a virtual, physical or hybrid environment. We refer to the term (sub)system using the terminology from the Organic Computing domain (M\u00a8uller-Schloer and Tomforde, 2017), and, for a better readability, omit the sub if it is clear from the context that not the overall system is meant (synonyms are entity or agent). Each ai is equipped with sensors and actuators. Internally, each ai distinguishes between a productive system part (PS, responsible for the basic purpose of the system) and a control mechanism (CM, responsible for controlling the behaviour of the PS and deciding about relations to other subsystems). This corresponds to the separation of concerns between System under Observation and Control (SuOC) and Observer/Controller tandem in the terminology of Organic Computing (OC) (Tomforde et al., 2011b) or Managed Resource and Autonomic Manager in terms of Autonomic Computing (Kephart and Chess, 2003). Figure 1 illustrates this concept with its input and output relations. Each subsystem in the overall system can assume different con\ufb01gurations. Such a con\ufb01guration typically consists of different components. We de\ufb01ne Productive System Control Mechanism (CM) Sensors Actuators ENVIRONMENT Observation  of raw data Execution of interventions Goals User Figure 1: Schematic illustration of a subsystem ai. the entire con\ufb01guration space of a subsystem ai as Cartesian product Ci = ci1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 cim, where cij are the components of the con\ufb01guration. The user guides the behaviour of ai using a utility function U and (in most cases) does not intervene at the decision level: Actual decisions are taken by the productive system and the CM. We model each subsystem to act autonomously, i.e., there are no control hierarchies in the overall system. Consider, e.g., a router a1 in a computer network as an illustrating example following the ideas of (Tomforde et al., 2010). It can take varying con\ufb01gurations into account, such as the processed network protocol or parameter settings (Tomforde et al., 2009). E.g., an interval c11 = [0,100] for the timeout parameter in seconds and the set c12 = {1,2,...,16} for the buffer size in kilobyte. The entire con\ufb01guration space of system a1 would then be C1 = [0,100]\u00d7{1,2,...,16}. Besides the con\ufb01guration space, we consider a local reward. In particular, each subsystem estimates the success of its decisions at runtime\u2014as a response to actions taken before. This is realised based on a feedback mechanism, with feedback possibly stemming from the environment of the subsystem (i.e., direct feedback) or from manual reward assignments (i.e., indirect feedback). This resembles the classic reinforcement model (Sutton and Barto, 1998), where the existence of such a reward is one of the basic assumptions. 2.2 Reinforcement learning for self-adaptation in intelligent systems The basic reinforcement learning (RL) paradigm relies on a continuous interplay of learner and environment. The control mechanism of an intelligent system continuously monitors the environmental conditions and thereby perceives the current state of the environment (st). The index t refers to the point in time of the perception indicating that the RL loop is performed in discrete time steps. Based on this state description, the RL learner decides about the next action (at) that manipulates the environment (e.g., a change of parameters of a controlled productive system). This results in a possibly changed state of the environment. Hence, in the next time step (t +1) an updated state (st+1) is presented to the learner. In addition, the learner receives a feedback signal (rt+1) that quanti\ufb01es the success of the action (if an immediate reward is possible) or is used to assess the \u2019value\u2019 of the current situation. Figure 2 illustrates this process. Prof. Dr.-Ing. Sven Tomforde / Intelligent Systems group 14 Reinforcement model RL Learner Environment action \ud835\udc4e\ud835\udc61 \ud835\udc5f\ud835\udc61+1 \ud835\udc60\ud835\udc61+1 state \ud835\udc60\ud835\udc61 reward \ud835\udc5f\ud835\udc61 Figure 2: The basic RL model Several different RL techniques are available in literature. However, the utilisation of RL in intelligent system (especially for adaptive self-con\ufb01guration, for instance) come with a set of requirements rendering most of the techniques not applicable\u2013the most important are: 1. Decisions are taken based on sensor information, which is typically available in terms of time-series of different real-valued attributes. As a consequence, the RL technique has to deal with real values. 2. An intelligent system typically uses more than just a few sensor values, i.e. it has to master a situation space Rd with d de\ufb01ning the number of dimensions of this space. 3. The ef\ufb01ciency of the RL technique depends directly on the number of possible choices, since learning requires testing different possibilities. Consequently, a simple list of situation-action mappings is not feasible and the learner needs the ability to generalise. Following these requirements, the class of \u201cLearning Classi\ufb01er Systems\u201d (LCS) (Wilson, 2000) in general and the variant \u201cExtended Classi\ufb01er System\u201d (XCS) by Wilson (Wilson, 1995) in particular have proven to provide suitable techniques. An alternative is the usage of Deep Reinforcmeent Learning technology that suffers from the missing interpretability (i.e., this is typically neglected in SASO systems since te user or administrator has no insights in what the system will do and why). An overview of LCS variants can be found in (Sigaud and Wilson, 2007). In contrast, the konlwedge of LCS is stored in an explicit rule-base that is evolved over time. LCS/XCS are \ufb02exible, evolutionary rule-based RL systems\u2014 we rely on the XCS-R variant (\u2019R\u2019 for real values) in the following, since this is the variant used in intelligent/SASO systems (D\u2019Angelo et al., 2019). XCS-R relies on a population [P] of classi\ufb01ers, where each of these classi\ufb01ers is an \u201cIF-THEN\u201d rule with evaluation criteria that partition the input space X and thus approximate the problem space locally. A steady-state niche genetic algorithm (GA) is responsible for improving the coverage in the match set [M] (that contains all classi\ufb01ers where the condition part matches the current conditions). Based on the set of matching classi\ufb01ers, a \ufb01tness-weighted prediction is calculated for all actions proposed by classi\ufb01ers contained in [M]\u2014using these values, a roulette-wheel approach is followed to select the action to be applied. All classi\ufb01ers in [M] promoting the selected action are transferred to the action set [A] for later consideration. For learning purposes, a discount is taken from the classi\ufb01ers contained in [M], a (delayed) reward is considered, and the classi\ufb01ers of the last action set (i.e., those being responsible for the last behaviour) are updated accordingly (i.e., using a learning rate \u03b1). Figure 3 illustrates the process for the variant XCS. The classi\ufb01ers are usually initialised with partially prede\ufb01ned initial but also randomly selected values. A covering mechanism generates new classi\ufb01ers if [M] contains too few or inappropriate classi\ufb01ers. However, this is a rather reactive behaviour. As mentioned before, a steady-state niche GA is constantly creating new knowledge in form of classi\ufb01ers: Successful classi\ufb01ers are selected to be reproduced, recombined, and mutated to search the local neighbourhood of the environmental niche with the result of a better coverage. In particular, this means that a globally optimal subregion of the input space is evolved that is characterised by a high accuracy in predicting the corresponding payoff (or reward). Again, this GA can be considered to act re-actively since it is performed in cycles and only considers \u2019good\u2019 classi\ufb01ers without any kind of creativity. Figure 4 illustrates an example of a population of XCS in a two-dimensional search problem: The space is unevenly covered by classi\ufb01ers with different size of the condition part and different evaluation values. In general, XCS keeps track of the three primary attributes p (prediction of the expected payoff), \u03b5 (prediction error observed over time), and F (\u2019\ufb01tness\u2019 of the classi\ufb01er) and several further \u2019book keeping\u2019 attributes such as the experience (number of occurrences in the action set), numerosity (number of contained sub-classi\ufb01ers after subsumption), etc. Environment Detectors Effectors [(0.12;0.15);\u2026;(0.81;0.89)]   A-12  43   .05   .98  [(0.75;0.85);\u2026;(0.88;0.89)]   A-05  32   .23   .61  [(0.02;0.25);\u2026;(0.80;0.95)]   A-12  23   .66   .51  [(0.10;0.13);\u2026;(0.85;0.88)]   A-19  49   .01   .99  [(0.08;0.21);\u2026;(0.77;0.79)]   A-03  38   .29   .62  [(0.12;0.15);\u2026;(0.81;0.89)]   A-28  09   .47   .42  Condition Action  \ud835\udc5d \ud835\udf00 \ud835\udc39 \u2026 Population [P] Check Copy matching classifiers Match Set [M] [(0.12;0.15);\u2026;(0.81;0.89)]   A-12  43   .05   .98 [(0.02;0.25);\u2026;(0.80;0.95)]   A-12  23   .66   .51  [(0.10;0.13);\u2026;(0.85;0.88)]   A-19  49   .01   .99 [(0.12;0.15);\u2026;(0.81;0.89)]   A-28  09   .47   .42 [(0.13;0.16);\u2026;(0.90;0.96)]   A-19  46   .02   .96  Perception: (0.13,\u2026,0.85) GA A-12    A-19   A-28 36.15  47.52  9.00 Prediction Array [(0.10;0.13);\u2026;(0.85;0.88)]   A-19  49   .01   .99 [(0.13;0.16);\u2026;(0.90;0.96)]   A-19  46   .02   .96 Action Set [A] Action  selection \u201cA-19\u201c Delay=1 Reward \ud835\udc5f\ud835\udc61+1 Previous Action Set [\ud835\udc34]\ud835\udc61\u22121 max + P Update: \u2022 Fitness \u2022 Error \u2022 Prediction Figure 3: Schematic illustration of XCS-R with example population. Accuracy: Strength:  Experience: 0.76 0.75 7 Feature A Feature B Accuracy: Strength:  Experience: 0.86 0.98 12 Accuracy: Strength:  Experience: 0. 36 0.18 17 Accuracy: Strength:  Experience: 0.61 0.82 11 Accuracy: Strength:  Experience: 0.92 0.99 19 Accuracy: Strength:  Experience: 0.55 0.59 14 Accuracy: Strength:  Experience: 0.61 0.48 8 Accuracy: Strength:  Experience: 0.02 0.03 3 Action ignored Action A Action B Legend Action C Figure 4: Example for a population of XCS covering the condition space. Although XCS has proven to be applicable to realworld problems (e.g., (Tomforde et al., 2011a)), it comes with some major drawbacks: 1. Exploration vs. exploitation is done by a roulettewheel approach considering the p and F values of classi\ufb01ers in [M] only. There is no controlled exploration behaviour and no adaptation of the learning rate \u03b1. 2. XCS builds up and manages its population in a purely reactive manner: It generates and adapts classi\ufb01ers only in those parts of the input space where it observes stimuli. It does not generate classi\ufb01ers for other niches pro-actively and, hence, is not prepared for new conditions. 3. XCS generates new classi\ufb01ers either by genetic operations in a niche (i.e., the GA) or randomly (the covering mechanism). It does not use existing knowledge of the entire population or relies on a goal-oriented mechanism for the covering part. 4. The number of classi\ufb01ers contained in the population is limited. Consequently, XCS contains mechanisms to aggregate classi\ufb01ers into one and to delete less promising classi\ufb01ers. However, it does not actively manage its population. 5. The population just stores the best actions following the current experiences. However, it does not make use of knowledge about unsuccessful attempts. 6. The approach can handle concept drifts in a purely passive manner, but it does not contain mechanisms to detect this at runtime and \ufb01nd ef\ufb01cient reactions. The idea of this article is to develop the concept for a novel XCS variant that is able to handle these issues. In particular, this means to turn the passive or reactive XCS into an active or proactively acting system, to which we refer as active reinforcement learning. Therefore, we make use of concepts from the active learning (Settles, 2009; Settles, 2012) domain\u2014 which is introduced in the next paragraphs. 2.3 Active Learning Active Learning (AL) is a semi-supervised learning paradigm that is based on managing the sample selection process in the sense that it selects the most informative samples for labelling. The goal is to achieve a high performance (e.g., classi\ufb01cation accuracy) with as few samples as possible. Application areas where AL has been successfully applied include: drug design (Kangas et al., 2014), text classi\ufb01cation (Chu et al., 2011), or malicious software detection (Nissim et al., 2014). The term AL has initially been de\ufb01ned by Settles as follows: \u201d\u201cActive learning systems attempt to overcome the labelling bottleneck by asking queries in the form of unlabelled instances to be labelled by an oracle. In this way, the active learner aims to achieve high accuracy using as few labelled instances as possible, thereby minimising the cost of obtaining labelled data.\u201d (Settles, 2009). In general, we distinguish between three major AL paradigms: stream-based active learning (SAL) (Atlas et al., 1990), membership query learning (MQL) (Angluin, 1988), and pool-based active learning (PAL) (Lewis and Gale, 1994). The SAL paradigm assumes a stream of data, i.e., the data points show up one after the other and the learner has to decide either to \u2019buy\u2019 (i.e., query an oracle) a label or not. In particular, this means if the learner decides not to buy a label for a speci\ufb01c data point, he will not be able to access that data point again. In the MQL paradigm, the learner works independently from a stream of incoming samples and can possibly request labels for any unlabelled instance in the input space. In particular, this means to allow for queries that the learner generates de novo. In contrast, the standard PAL process starts with a large pool of unlabelled samples and a small set of labelled samples. Figure 5 illustrates such a pool with already labelled samples (blue and orange) and unlabelled samples (grey). The training process is organised in cycles and consists of the following steps: (1) A model such as a classi\ufb01er is trained using the already labelled samples, (2) The selection strategy of the active learner identi\ufb01es a query set (the next samples to be labelled) from the pool of unlabelled samples, (3) The selected samples are presented to the oracle (e.g., a human), which provides label information, and (4) The knowledge model is updated. Finally, (5) the process terminates if a stopping condition is met, otherwise the next learning cycle is started. PAL heavily relies on the particular implementation of the selection strategy. Hence, several alternatives are available in literature\u2014the most prominent examples include the following: \u2022 Random selection, also called passive sampling: Selects instances randomly for labelling, which means that it is free of heuristics and parameters. \u2022 Uncertainty sampling: Select those instances where the learner are least certain about the label, with different measures being used for quanti\ufb01cation of this uncertainty (e.g., posterior, margin, or entropy) (Atlas et al., 1990). \u2022 Ensemble-based strategy: The basic idea here is to train different base classi\ufb01ers by using different subsets of the training data and choose the next sample to be queried by identifying the strongest disagreement between these base classi\ufb01ers (Seung et al., 1992). \u2022 Expected Error Reduction: Estimates the generalisation error of a classi\ufb01cation model given a validation set, which means that no labels are required (Roy and McCallum, 2001). \u2022 Density weighted Uncertainty Sampling (DWUS) An uncertainty score is weighted with a candidate\u2019s density. Thereby, outlier are unlikely to be considered for labelling (Donmez et al., 2007). In general, all these strategies aim at assessing the current knowledge of the classi\ufb01er and, based on this assessment, determine the most informative samples. In the following section, we describe how this basic idea in combination with the different approaches of these basic selection strategies can be used to turn purely reactive RL systems into actively learning RL systems. 3 ACTIVE REINFORCEMENT LEARNING We use the term \u2019active reinforcement learning\u2019 (ARL) for RL systems that are self-aware of their Feature A Feature B Unclassified Class A Class B Legend Figure 5: Example of labelled and unlabelled samples in a pool-based AL setting. knowledge and adapt their own learning behaviour accordingly. This is in contrast to the previously seen purely reactive and passive strategies typically followed by RL systems (see Section 2.2. Figure 6 illustrates an abstract view of an XCS presented in Figure 3 and adds an adaptation mechanism on-top of it that follows the design concept of Organic Computing (Tomforde et al., 2011b). In general, active RL comprises two different aspects that need to be considered subsequently: i) establishing self-awareness of the own knowledge and ii) active control of knowledge discovery parts of the RL system. This corresponds to the observer and controller parts of the design concept. We summarise the corresponding tasks in the following paragraphs. + Observer \u2022 Knowledge assessment \u2022 Uncertainty modelling \u2022 Concept drift detection Controller \u2022 Model-based rule generation \u2022 Adaptive learning rate \u2022 Adaptive exploration Figure 6: Self-adaptive XCS with observer/controller tasks. 3.1 Aspects of ARL Observation tasks: As basis for active management of knowledge and guided reinforcement learning behaviour, the system needs to become aware of its own, currently existing knowledge. This knowledge is stored in the population (i.e., the available classi\ufb01ers as illustrated by Figure 4): in the entirety of the search space covered by condition parts, in the diversity of actions, and in the different evaluation criteria (i.e., prediction, error, \ufb01tness\u2014but also experience, numerosity, etc.). In order to allow for a kind of selfawareness of the existing knowledge, an active reinforcement learning systems needs the following capabilities: \u2022 Assessing the quality (i.e., the accuracy of predictions of rewards/feedback in combination with the strength of the expected reward) of existing knowledge as distribution model for the entire search space: This allows for identifying regions that have insuf\ufb01cient knowledge (which then needs to be addressed actively by the controller part). \u2022 Modelling the uncertainty of the existing knowledge: Assessing the certainty assigned to the different regions of the search space can be done using the aspect \u2019experience\u2019 per classi\ufb01er aggregated over the different classi\ufb01ers covering a niche. This allows for determining a score for each possible niche that\u2014in relation to other niches\u2014 provides the basis for deciding if this niche needs to be explored in more detail. \u2022 Assessing the \u2019appropriateness\u2019 of the knowledge for the next steps: An RL system typically traverses the search space in terms of a trajectory. This means that there is a local dependency between the current situation and the next situation, since this is the result of the chosen action and environmental in\ufb02uences. In order to act actively, the RL system needs to make use of these trajectories and predict the set of next situations (probably probabilistically to quantify the uncertainty assigned to these predictions). Based on this, it can assess in advance if appropriate classi\ufb01ers will be available for the corresponding match and action sets. Otherwise, alternatives need to be generated. \u2022 Detecting meta-features of the learning problem: Typically, RL assumes that the underlying learning problem is static, i.e., challenges such as concept drift and concept shift do not occur. Alternatively, it is assumed that concept drift is so slow that the evolution of the population automatically addresses this problem. However, intelligent systems have to operate under real-world conditions and based on sensor/actuator constellations, which may result in challenges such as i) integration of novel components, ii) hardware characteristics such as wear, or iii) any kind of novel environmental processes. Consequently, the systems needs possibilities to detect such changes in the underlying problem domain, possibly resulting in re-consideration of already learned knowledge. Control tasks: Taking the different mechanisms of the observer part as input, the RL system has to adapt its own learning behaviour accordingly. In general, this means to manage parameters such as learning speed but also to generate promising classi\ufb01ers for inappropriately covered niches of the search space. To allow for a kind of self-management of the learning behaviour, an ARL systems needs the following capabilities: \u2022 Model-based classi\ufb01er generation: Instead of randomly generating new classi\ufb01ers or applying a genetic algorithm to the action or match set, the RL system needs more sophisticated techniques for classi\ufb01er generation that make use of the available knowledge. \u2022 Adaptive exploration rate: Currently, XCS decides about the next action based on the information contained in the prediction array by applying a roulette-wheel approach. This gives a higher probability to better performing actions (i.e., exploitation) and lower probability to worse performing actions (i.e., exploration). However, such a static assignment of probabilities based on the niche is a good idea at startup, it result in undesired exploration when the population is already converged. Consequently, there are different strategies available in literature that aim at controlling the learning rate. An ideal strategy comes with a high learning rate when the population is not appropriate (e.g., startup or after a concept drift/shift) and with no exploration if the population converged to the optimum. Consequently, the RL system needs a mechanism to assess the current exploration needs and to control the exploration probability accordingly. \u2022 Adaptive learning rate: The selection probability is just one means to parameterise the learning behaviour in XCS. A second aspect is given by the learning rate that is responsible for controlling to which degree a classi\ufb01er\u2019s evaluation criteria are shifted into the direction of the actually received reward/feedback signal. Here, the same argumentation holds as above, resulting in the need to control the learning rate dynamically at runtime. \u2022 Population management: The size of the population is limited in XCS to allow for a \u2019selection pressure\u2019, i.e., less performing classi\ufb01ers are continuously replaced by novel candidates. The result of this process is that, after convergence, the population is (theoretically) perfectly optimised to the underlying problem and only stores the most promising responses for occurring situations. However, in the presence of permanent change, deleting information about sub-optimal or even bad behaviour (and consequently the encoding classi\ufb01ers) is highly inef\ufb01cient. Reasons include: i) A concept drift may render deleted classi\ufb01ers necessary again, ii) generating new classi\ufb01ers ef\ufb01ciently means to avoid bad behaviour, etc. As a counter measure, the RL system needs a mechanism for active management of the current population which makes, e.g., use of a backup memory that allows for \u2019constructive forgetting\u2019 and \u2019remembering\u2019. There are further highly important aspects when turning XCS into an ARL system, which need to be taken into consideration: \u2022 XCS is just one RL paradigm, although probably the most prominent in operating intelligent systems. However, the insights gained in developing an active XCS need to be transferred to other RL paradigms for generalisation purposes. \u2022 An intelligent system seldom operates alone. In turn, use cases often contain several similar systems operating in a shared environment. Consequently, the mechanisms identi\ufb01ed before can make use of even more existing knowledge (i.e., the populations of other systems of the same kind) via communication. This, however, may increase the uncertainty related to the knowledge information since the underlying characteristics (such as the sensor equipment) may not be identical. 3.2 State of the art in active reinforcement learning Assessment of the existing knowledge: Stein et al. de\ufb01ned so-called \u2019knowledge gaps\u2019 as parts of the input space (niches) that are not suf\ufb01ciently covered by classi\ufb01ers (Stein et al., 2018). This means that the niche is either not covered at all or only covered by classi\ufb01ers with low performance values. The authors propose to derive an abstract representation of the classi\ufb01er distribution that is used to identify niches where knowledge is needed. In (Stein et al., 2017a), a basic combination with approaches from the \ufb01eld of AL has been derived that should make use of concepts such as query synthesis, uncertainty sampling, and query by committee\u2014however, this just provides the idea and can be seen as preliminary work to this contribution. Considering external knowledge in LCS: Especially for Michgan-style LCS, Urbanowicz et al. presented a concept to integrate the knowledge of external experts to establish a guided discovery process of novel classi\ufb01ers (Urbanowicz et al., 2012; Urbanowicz and Moore, 2015). In general, the approach is to make use of a set of hand-crafted heuristics (i.e., \u2019expert knowledge\u2019) to improve the generation of novel classi\ufb01ers in comparison to the random-based strategy typically used. This means that the probabilities of actions covered in situations of classi\ufb01ers under construction are pre-de\ufb01ned (i.e., derived in an of\ufb02ine pre-processing step) and not adaptive to the underlying problem. As an alternative, Najar et al. presented a learning model where a human \u2019teacher\u2019 guides the RL system (a robot) by using teaching signals (i.e., showing what to do) (Najar et al., 2015a; Najar et al., 2015b). The idea is to allow the learner to imitate the behaviour of the teacher and therefore improve its learning ef\ufb01ciency. However, the approach is limited to cases where learner and teacher can explicitly do the same task, where a (human) teacher is available during the entire learning process, and where knowledge is already available by teachers. In the context of Organic Computing (M\u00a8ullerSchloer and Tomforde, 2017), a safety-based generation of novel classi\ufb01ers has been presented (Tomforde et al., 2011a). Here, the LCS-variant XCS is not allowed to generate new behaviour. In contrast, it operates on the existing classi\ufb01ers only. The GA responsible for discovering appropriate actions in speci\ufb01c situations has been removed from the online component and coupled with a simulation of the underlying intelligent system. As a result, only tested behaviour is added to the population. To remain operable (i.e., \ufb01nd answers in case of missing knowledge), the covering component copies the nearest classi\ufb01er (i.e., based on the Euclidean distance de\ufb01ned in the search space of the condition parts), widens its condition part to \ufb01t the new situation, and uses this as default classi\ufb01er (Prothmann et al., 2011). However, the approach requires the existence of a \u2019digital twin\u2019 during the discovery process. Also in the context of OC, Stein et al. proposed to make use of interpolation techniques to generate new classi\ufb01ers (Stein et al., 2016). The basic idea is that the best action is probably a compromise of the actions proposed by the best surrounding classi\ufb01ers in the niche. Consequently, novel classi\ufb01ers are generated not by using a simulation-coupled GA as in (Tomforde et al., 2011a; Prothmann et al., 2011) but by interpolation of the neighbours\u2014which is much faster. The approach has been further extended to alternative usage patterns of interpolation (Stein et al., 2017b) and a concept for proactive knowledge construction that, however, has not been realised yet (Stein et al., 2017a). Still, this assumes a gradual change in the action rather than allowing for shifts in the action distributions. Following a more generic approach, Nakata et al. developed an example of a weighted complete action map, which is intended to evolve a population that is complete in terms of learning the entire situation-toaction-mapping. In particular, this means to assign more classi\ufb01ers to the highest-return actions for each state, for instance (Nakata et al., 2015). This continues work by Kovac (see e.g. (Kovacs, 1999), where he investigates the trade-off between strength and accuracy and consequently emphasises issues related to the change in \ufb01tness. Management of the population: In (Butz and Sigaud, 2011), Butz and Sigaud proposed a different approach to classi\ufb01er deletion. They handled the selection problem (i.e., identi\ufb01cation of classi\ufb01ers to be deleted to keep the population size below a given threshold) locally rather than globally as before. This is intended to avoid deleting classi\ufb01ers in niches that can be considered as knowledge gaps using the wording of Stein et al. from (Stein et al., 2018). However, this does not tackle the problem of actively managing the population. In XCS, a subsumption mechanism combines similar rules, and a randomised deletion mechanism removes classi\ufb01ers of a low \ufb01tness from the population. In (Fredivianus et al., 2010), the discovery component is altered by introducing a modi\ufb01ed rule combining technique. The goal is to create maximally general classi\ufb01ers that match as many inputs as possible while still being exact in their predictions. The approach considers previously learnt knowledge and infers generalised classi\ufb01ers from the existing population. It has been extended to real-valued instances of XCS in (Fredivianus et al., 2012). Despite providing a heuristic for rule combining, the approach covers just one aspect of the management problem. Imbalanced data: RL systems in intelligent systems learn in a reactive manner by considering the current environmental conditions as input. In other words, the different input situations the RL system has to react on are highly imbalanced, with some regions of the search space being never or only extremely seldom covered. The learning mechanism of XCS under imbalanced data has been investigated and theoretically modelled by Orriols-Puig et al. in (Orriols-Puig and Bernad\u00b4o-Mansilla, 2009). The authors focus on rare cases and react to this challenge by adapting XCS\u2019 parameters (i.e., the learning rate and the threshold for GA activation). However, this does not explicitly address the problem of identifying limited knowledge or steering the exploration\u2014but it provides a basis for techniques to modify XCS parameters at runtime. Novelty search: LCS are part of the overall \ufb01eld of evolutionary algorithms (EAs). Here, approaches such as the \u2019Novelty Search Algorithm\u2019 (Lehman and Stanley, 2008) by Lehman and Stanley have been presented that exchange the traditional evolution process based on a \ufb01tness function by one that puts an emphasis on searching for novel behaviours. This may be bene\ufb01cial if the search space of possible actions is unknown and consequently can provide an element for action discovery in active RL systems. Exploration strategies: Initially, RL systems used static exploration strategies with the \u03b5-greedy strategy as probably most popular one (Sutton and Barto, 1998). Here, a certain fraction of attempts (\u03b5) are dedicated to exploration, while the remaining 1 \u2212 \u03b5 fraction is used for exploiting the currently best known action. Variants of static exploration schemes include a random strategy (also called random walk, i.e., selecting randomly which action to use), or a softmax strategy (i.e., an \u03b5-greedy approach with modi\ufb01ed selection probabilities to better re\ufb02ect the currently learned reinforcement values) (Sutton and Barto, 1998). In contrast, the idea of the \u2019decreasing \u03b5-strategy\u2019 (Vermorel and Mohri, 2005) is to have a higher probability for an explorative action selection at the beginning and to decrease this towards the end of the learning process. In this way, an attempt is made to ensure appropriate exploration of the search space before the agent \ufb01nally makes primarily exploitative action selections. The limitations of this approach include the static decreasing behaviour (i.e., no real adaptiveness) and the characteristics of real-world problems, since there are typically no \ufb01nal states. A similar alternative is the \u2019\u03b5 \ufb01rst\u2019 strategy (Vermorel and Mohri, 2005)\u2013 here, only two different values for \u03b5 are considered: a high one (e.g., 1) at the beginning and a low one (e.g., 0 or close to 0) after suf\ufb01cient exploration. The limitations are similar as before but further emphasise the need to de\ufb01ne \u2019suf\ufb01cient\u2019 exploration. A third category of exploration strategies is de\ufb01ned by the \u2019meta-softmax-strategy\u2019 (Schweighofer and Doya, 2003). The idea is to adapt the learning rate, the discount factor and the exploration probability dynamically in response to the difference of two values calculated from the feedback signal: a \u2019midterm reward\u2019 (i.e., considering a few rewards) and a \u2019long-term reward\u2019 (i.e., considering the rewards of a longer window). For both, the averaged rewards are determined for the considered window and if the difference is above a pre-de\ufb01ned threshold, the exploration criteria (i.e., learning rate, discount factor, and exploration probability) are increased. This is already adaptive in the sense of our notion of ARL, but it still follows a reactive mechanism rather than a wellde\ufb01ned active decision. Closely related to the \u2019meta softmax\u2019 strategy is the class of \u2019value-differencebased-exploration\u2019 strategies (Tokic, 2010). Here, the exploration parameters of the learning strategy are determined based on the difference of the \u2019values\u2019 of the individual actions. In particular, this value difference denotes the product of the learning rate and the temporal difference error. It can not be mapped directly to the evaluation criteria of XCS, since there is no individual \u2019value\u2019 of a classi\ufb01er. However, the prediction error in XCS can be understood as a corresponding indicator. We have to consider differences between expected reward/feedback and observation as one aspect of the strategy to decide about adaptations of the explorative parameters. 4 RESEARCH ROADMAP To realise ARL based on XCS, different challenges need to be addressed. In this section, we outline a corresponding research roadmap. Therefore, we de\ufb01ne the most urgent challenges that contribute to the aspects of ARL as introduced previously. We organise these challenges in terms of observer, controller, or overall system tasks. For each challenge, we initially discuss the goal and subsequently propose a concept how this challenge can be addressed using. Thereby, we either make use of concepts from the state of the art or we put an emphasis on possible strategies to to close the gap in research. 4.1 Observer-related challenges The goal of the observer part is to establish a selfassessment of the existing knowledge and a subsequent prediction of possible next states. This mainly comprises the challenges assessment of the population, prediction of the next states, and modelling the entire learning problem. Challenge 1 \u2013 Assessment of the existing population: The goal of the \ufb01rst challenge is to establish a continuous self-assessment of the existing population. This mainly refers to the task of identifying less covered niches or niches with inappropriate knowledge. The different AL strategies as outlined in Section 2.3 are already ful\ufb01lling the task of assessing the input space. However, the major difference is that the population is a set of hyper-rectangles covering the search space (see Figure 4) rather than the individual points in the AL approach (i.e., the samples in Figure 5). This means that we have to turn the sampledistribution into a continuous distribution, e.g. by using kernel density estimation techniques. However, the hyper-rectangles de\ufb01ning the condition parts of the classi\ufb01ers are combined with a kind of label information (i.e., actions), but they also consider uncertainty values (i.e., the evaluation criteria such as accuracy, strength, experience or numerosity). The assessment must be able to derive values for these aspects as well. Further, an ordering of knowledge gaps (i.e. a ranking) will most certainly be based on several criteria aggregated to an individual score. Here, aspects such as described for the 4DS strategy in AL can play a major role, see (Reitmaier and Sick, 2013). Challenge 2 \u2013 Prediction of the next match sets: The goal of the second challenge is to predict the next observations to check whether the match set will be suitable or not. This can then serve as basis to proactively generate new classi\ufb01ers. A possible approach is to model the sequence of situations as a sliding window approach. Within this window, a trajectory can be determined \u2014 which is then used to predict possible next situations. To further improve the predictions, this has to take aspects such as velocity and probability of occurrence into account. In addition, the prediction may become subject to runtime learning as well by using state-of-the-art techniques. Based on this, the match set can be generated and an analysis of the contained classi\ufb01ers can be performed: Is there enough diversity? Is the overall \ufb01tness-weighted prediction above a certain threshold? Is the niche suf\ufb01ciently covered? Challenge 3 \u2013 Learning problem modelling: The goal of the third challenge is to generalise the \ufb01rst challenge by explicitly modelling the problem space next to the knowledge stored in the population. A possible approach lies in the integration of an additional, continuously-de\ufb01ned representation of the learning problem that is update based on any received feedback. This can be done, for instance, by training a neural network taking the situation as input and providing the most appropriate action as output (or estimating the payoff for a situation-action pair). Such a solution \u2013 and therefore the reason why we favour the XCS-based approach \u2013 suffers from the representation being not interpretable by humans and not explaining the behaviour. As an additional knowledge base, this may serve as input for more ef\ufb01cient and reliable classi\ufb01er generation and management. 4.2 Controller-related challenges The goal of the controller part is to increase the degree of autonomy of (reinforcement-based) learning in intelligent technical systems. This actually turns the RL system into an ARL system by making use of the information provided by the observer part\u2014and it mainly comprises the challenges population management, controlled classi\ufb01er generation, adaptive exploration probability, adaptive learning rate, and control of the adaptation speed. Challenge 4 \u2013 Population management: The goal of population management is to turn the reactive replacement approach with a de\ufb01ned maximum number of classi\ufb01ers in the population into a proactive management. A possible approach is to introduce external of\ufb02ine memories that serve as reservoir for removed classi\ufb01ers. The knowledge encoded in these classi\ufb01ers can also be used as reference when generating new ones. Especially if a change in the structure of the learning problem is noticed, the system can check if it could switch back to previously removed classi\ufb01ers, which would perfectly cover oscillating behaviour, for instance. Such a mechanism needs to be augmented with a suitable selection scheme as well as a mechanism that ideally abstracts from the individual classi\ufb01ers. Thereby, not only the memorisation of older classi\ufb01ers plays an important role, but also techniques to realise \u2019constructive forgetting\u2019, i.e. explicitly select knowledge to be removed to allow for novel behaviour (which not necessarily refers to \u2019bad\u2019 classi\ufb01ers as currently). Challenge 5 \u2013 Controlled generation of new rules: The goal of this aspect is to proactively generate new classi\ufb01ers. There are already a few attempts in literature: (Tomforde et al., 2011a) uses a simulator to generate new classi\ufb01ers and (Stein et al., 2017a) describes interpolation-based ideas. Following these ideas, new classi\ufb01ers can be generated either using sideknowledge (i.e., the external neural network mentioned above), simulation, external memories of replaced classi\ufb01ers or the neighboured classi\ufb01ers. The scope can be extended towards incorporation of other opportunistically available knowledge sources as explained in (Calma et al., 2017). This is closely related to the concept of curiosity, which is discussed in detail in (Wu and Miao, 2013). Challenge 6 \u2013 Active con\ufb01guration of exploration probability: The goal of this challenge is to replace the roulette-wheel exploration approach by an adaptive mechanism. As outlined in Section 3.2, there are several adaptive exploration schemes available in literature. They can serve as starting point for altering the decision logic of XCS as well. However, this needs to be combined with change detection techniques or novelty detection techniques (such as (Gruhl et al., 2021) taking, e.g., the trajetories through the search space as input) to identify conditions where the reinforcement behaviour is differing from previous and expected behaviour. Challenge 7 \u2013 Adaptive control of the learning rate: The goal of this aspect is to replace the static de\ufb01nition of the learning rate by adaptive techniques. Similar to the challenge before, cases where the underlying learning problem is changing require a faster adaptation of the existing knowledge. In turn, already settled knowledge bases require smaller or no update at all to avoid oscillations due to noise feedback. A solution can adapt the reinforcement rate in a way that \u03b1 is increased in case of changing conditions (e.g. detected drift). Alternatively, the value of \u03b1 depends on the niche of the search space (number of classi\ufb01ers, \ufb01tness, numerosity, etc) and can be de\ufb01ned per-niche rather then globally. Challenge 8 \u2013 Adaptation speed: The goal of this aspect is to replace the static cycle-based activation scheme of the learning technique with an adaptive variant. Currently, the RL loop is performed in given cycles of \ufb01xed duration. However, the frequency of adaptation performed by the intelligent system is not necessarily desirable to be constant. For instance, in traf\ufb01c control the conditions are almost constant during the night, resulting in seldom adaptation needs. On the other hand, rush hour handling would bene\ufb01t from even shorter adaptation cycles. This maps also to the idea that the learner is able to dynamically change the number of observations until it decides to adapt. Consequently, an approach could rely on assessing the stability of the observed condition and \u2013 based on such a score \u2013 adapt the frequency of adaptation. However, this has to avoid self-lock-in problems, for instance. 4.3 System-related challenges Besides the observer and controller tasks, there are system-wide challenges for ARL. This mainly comprises the challenges multi-dimensional feedback signals, collaborative awareness, and indirect feedback. Challenge 9 \u2013 Active reward requests: The goal of this aspect is to integrate human users as additional knowledge source. In accordance with the basic idea of AL, we introduce the user or administrator of the system as additional knowledge source. For unknown conditions or alternative classi\ufb01ers, the system may query the user for a feedback signal that is not stemming from the observed environment. This means to fully integrate the self-assessment of the knowledge distribution combined with the uncertainty estimation as known in AL and incorporate the corresponding selection strategies, possibly with a given budget and only if a user is available. This entails the need for a availability models for the user, maybe even augmented with (learned) models of their expertise. Challenge 10 \u2013 Multi-dimensional feedback signals: The goal of this aspect is to replace the reward signal by a vector comprising several utility functions at the same time. A typical intelligent system has to tackle more than one goal at the same time. Integrating the different utility values into one score is always a trade-off and looses information. Consequently, the algorithmic logic needs to be adapted in a way that the single value is replaced by a vector representation. This has impact on all stages of the learning process. However, there is a \ufb01rst solution available (Becker et al., 2012) that serves as a basis for tackling the problem. Challenge 11 \u2013 Indirect feedback signals and feedback with uncertainty: The goal of the last aspect is to combine the explicit with possible further implicit feedback signals. A possible approach establishes a mechanism that tries to determine additional feedback signals for consideration. An intuitive approach relies on using neighbouring systems of the same kind that act in a shared environment: Thy can provide (negative) feedback about adaptation decisions that have a negative impact on the utility of the neighbours. This would turn the methodology as de\ufb01ned in (Rudolph et al., 2019) into a direct representation. However, this also ",
    "Conclusion": "CONCLUSION This paper discussed the limitations of current techniques applied to the self-adaptation task in intelligent systems. The major observation is that currently the speci\ufb01c learning parts are processed in an isolated manner. In particular, the main learning technique is typically a reinforcement learner that learns in a purely reactive manner. On the other hand, machine learning paradigms such as anomaly/novelty detection or active learning are able to provide a better selfawareness of the underlying observed behaviour and processes. Based on this observation, we proposed a concept to integrate current sophisticated reinforcement learning techniques \u2014 in particular, the class of Learning Classi\ufb01er Systems \u2014 with concepts from the other domain. The goal is to establish an integrated approach, which we called \u2019active reinforcement learning\u2019. The major advantage of such a technique lies in the \u2019proactiveness\u2019, i.e. the possibility to act selfdetermined (optimised, planned) rather than purely reactive. We used the basic design pattern from the domain of Organic Computing, i.e. the Observer/Controller pattern, as a reference model for intelligent systems. Based on the constituent parts, we derived a research roadmap towards closing the gap to an active reinforcement learning system. This resulted in the de\ufb01nition of nine challenges. However, this list does not claim to be exhaustive, but re\ufb02ects the most urgent steps towards an ARL approach. In our current and future work, we focus on these challenges. ",
    "References": "REFERENCES Angluin, D. (1988). Queries and concept learning. Machine learning, 2(4):319\u2013342. Atlas, L. E., Cohn, D. A., and Ladner, R. E. (1990). Training connectionist networks with queries and selective sampling. In Advances in neural information processing systems, pages 566\u2013573. Becker, C., H\u00a8ahner, J., and Tomforde, S. (2012). Flexibility in organic systems - remarks on mechanisms for adapting system goals at runtime. In Proc. of 9th Int. Conf. on Inf. in Control, Automation and Robotics, pages 287\u2013292. Butz, M. V. and Sigaud, O. (2011). Xcsf with local deletion: preventing detrimental forgetting. In Proc. of 13th An. Conf. Companion on Genetic and Evolutionary Computation, pages 383\u2013390. ACM. Calma, A., Kottke, D., Sick, B., and Tomforde, S. (2017). Learning to learn: Dynamic runtime exploitation of various knowledge sources and machine learning paradigms. In Proc. 2nd IEEE Int. Workshops on Foundations and Applications of Self* Systems, pages 109\u2013116. Chu, W., Zinkevich, M., Li, L., Thomas, A., and Tseng, B. (2011). Unbiased online active learning in data streams. In Proc. of 17th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining, pages 195\u2013 203. ACM. D\u2019Angelo, M., Gerasimou, S., Ghahremani, S., Grohmann, J., Nunes, I., Pournaras, E., and Tomforde, S. (2019). On learning in collective self-adaptive systems: state of practice and a 3d framework. In Proc of 14th SEAMS@ICSE, pages 13\u201324. D\u2019Angelo, M., Ghahremani, S., Gerasimou, S., Grohmann, J., Nunes, I., Tomforde, S., and Pournaras, E. (2020). Learning to learn in collective adaptive systems: Mining design patterns for data-driven reasoning. In 2020 IEEE Int. Conf. on Autonomic Computing and SelfOrganizing Systems, Companion, pages 121\u2013126. Donmez, P., Carbonell, J. G., and Bennett, P. N. (2007). Dual strategy active learning. In European Conference on Machine Learning, pages 116\u2013127. Springer. Fredivianus, N., Kara, K., and Schmeck, H. (2012). Stay real!: XCS with rule combining for real values. In Genetic and Evolutionary Computation Conference, pages 1493\u20131494. Fredivianus, N., Prothmann, H., and Schmeck, H. (2010). XCS revisited: A novel discovery component for the extended classi\ufb01er system. In Simulated Evolution and Learning - 8th Int. Conf., pages 289\u2013298. Glass, R. L. (2002). Facts and Fallacies of Software Engineering. Agile Software Development. Addison Wesley, Boston, US. Gruhl, C., Sick, B., and Tomforde, S. (2021). Novelty detection in continuously changing environments. Future Gener. Comput. Syst., 114:138\u2013154. Kangas, J. D., Naik, A. W., and Murphy, R. F. (2014). Ef\ufb01cient discovery of responses of proteins to compounds using active learning. BMC bioinformatics, 15(1):143. Kephart, J. and Chess, D. (2003). The Vision of Autonomic Computing. IEEE Computer, 36(1):41\u201350. Kounev, S., Lewis, P., Bellman, K. L., Bencomo, N., Camara, J., Diaconescu, A., Esterle, L., Geihs, K., Giese, H., G\u00a8otz, S., Inverardi, P., Kephart, J. O., and Zisman, A. (2017). The Notion of Self-aware Computing. In Self-Aware Computing Systems, pages 3\u201316. Springer. Kovacs, T. (1999). Strength or accuracy? \ufb01tness calculation in learning classi\ufb01er systems. In Int. Worksh. on Learning Classi\ufb01er Sys., pages 143\u2013160. Springer. Lehman, J. and Stanley, K. O. (2008). Exploiting openendedness to solve problems through the search for novelty. In ALIFE, pages 329\u2013336. Lewis, D. D. and Gale, W. A. (1994). A sequential algorithm for training text classi\ufb01ers. In SIGIR\u201994, pages 3\u201312. Springer. Moore, G. E. (1965). Cramming more components onto integrated circuits. Electronics Mag., 38(8):114 \u2013 117. M\u00a8uller-Schloer, C. and Tomforde, S. (2017). Organic Computing \u2013 Techncial Systems for Survival in the Real World. Autonomic Systems. Birkh\u00a8auser Verlag. Najar, A., Sigaud, O., and Chetouani, M. (2015a). Socialtask learning for hri. In Int. Conf. on Social Robotics, pages 472\u2013481. Springer. Najar, A., Sigaud, O., and Chetouani, M. (2015b). Socially guided xcs: using teaching signals to boost learning. In Proc. of Companion to 2015 An. Conf. on Genetic and Evolutionary Comp., pages 1021\u20131028. ACM. Nakata, M., Lanzi, P. L., Kovacs, T., Browne, W. N., and Takadama, K. (2015). How should learning classi\ufb01er systems cover a state-action space? In Proc. of CEC15, pages 3012\u20133019. IEEE. Nissim, N., Moskovitch, R., Rokach, L., and Elovici, Y. (2014). Novel active learning methods for enhanced pc malware detection in windows os. Expert Systems with Applications, 41(13):5843\u20135857. Orriols-Puig, A. and Bernad\u00b4o-Mansilla, E. (2009). Evolutionary rule-based systems for imbalanced data sets. Soft Computing, 13(3):213. Prothmann, H., Tomforde, S., Branke, J., H\u00a8ahner, J., M\u00a8uller-Schloer, C., and Schmeck, H. (2011). Organic Traf\ufb01c Control. In Organic Computing \u2013 A Paradigm Shift for Complex Systems, pages 431 \u2013 446. Birkh\u00a8auser, Basel. Reitmaier, T. and Sick, B. (2013). Let us know your decision: Pool-based active training of a generative classi\ufb01er with the selection strategy 4ds. Information Sciences, 230:106\u2013131. Roy, N. and McCallum, A. (2001). Toward optimal active learning through sampling estimation of error reduction. In Proc. of 18th Int. Conf. on Machine Learning, pages 441\u2013448. Morgan Kaufman. Rudolph, S., Tomforde, S., and H\u00a8ahner, J. (2019). Mutual in\ufb02uence-aware runtime learning of self-adaptation behavior. ACM Trans. Auton. Adapt. Syst., 14(1):4:1\u2013 4:37. Schweighofer, N. and Doya, K. (2003). Meta-learning in reinforcement learning. Neural Networks, 16(1):5\u20139. Settles, B. (2009). Active learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences. Settles, B. (2012). Active learning. Synthesis Lect. on Art. Int. and Machine Learning, 6(1):1\u2013114. Seung, H. S., Opper, M., and Sompolinsky, H. (1992). Query by committee. In Proceedings of the \ufb01fth annual workshop on Computational learning theory, pages 287\u2013294. ACM. Sigaud, O. and Wilson, S. (2007). Learning classi\ufb01er systems: a survey. Soft Comp., 11(11):1065\u20131078. Stein, A., Maier, R., and H\u00a8ahner, J. (2017a). Toward curious learning classi\ufb01er systems: Combining xcs with active learning concepts. In GECCO17 Companion, pages 1349\u20131356. ACM. Stein, A., Rauh, D., Tomforde, S., and H\u00a8ahner, J. (2016). Augmenting the algorithmic structure of XCS by means of interpolation. In Architecture of Computing Systems 2016, pages 348\u2013360. Stein, A., Rauh, D., Tomforde, S., and H\u00a8ahner, J. (2017b). Interpolation in the extended classi\ufb01er system: An architectural perspective. Journal of Systems Architecture, 75:79\u201394. Stein, A., Tomforde, S., Diaconescu, A., H\u00a8ahner, J., and M\u00a8uller-Schloer, C. (2018). A concept for proactive knowledge construction in self-learning autonomous systems. In Proc. of 3rd Int. Worksh. on Foundations and Applications of Self* Sys., pages 204\u2013213. Sutton, R. S. and Barto, A. G. (1998). Introduction to Reinforcement Learning. MIT Press, 1st edition. Tokic, M. (2010). Adaptive \u03b5-greedy exploration in reinforcement learning based on value differences. In An. Conf. on Art. Int., pages 203\u2013210. Springer. Tomforde, S., Brameshuber, A., H\u00a8ahner, J., and M\u00a8ullerSchloer, C. (2011a). Restricted On-line Learning in Real-world Systems. In Proc. of CEC11, pages 1628 \u2013 1635. IEEE. Tomforde, S., H\u00a8ahner, J., and Sick, B. (2014). Interwoven Systems. Informatik-Spektrum, 37(5):483\u2013487. Aktuelles Schlagwort. Tomforde, S., Hurling, B., and H\u00a8ahner, J. (2010). Dynamic control of mobile ad-hoc networks - Network protocol parameter adaptation using Organic Network Control. In Proc. of 7th Int. Conf. on Inf. in Control, Automation, and Robotics, pages 28\u201335. INSTICC. Tomforde, S., Prothmann, H., Branke, J., H\u00a8ahner, J., Mnif, M., M\u00a8uller-Schloer, C., Richter, U., and Schmeck, H. (2011b). Observation and Control of Organic Systems. In Organic Computing - A Paradigm Shift for Complex Systems, pages 325 \u2013 338. Birkh\u00a8auser. Tomforde, S., Sick, B., and M\u00a8uller-Schloer, C. (2017). Organic computing in the spotlight. CoRR, abs/1701.08125. Tomforde, S., Steffen, M., H\u00a8ahner, J., and M\u00a8uller-Schloer, C. (2009). Towards an Organic Network Control System. In Proc. of the 6th ATC, pages 2 \u2013 16. Springer. Urbanowicz, R. and Moore, J. (2015). Exstracs 2.0: description and evaluation of a scalable learning classi\ufb01er system. Ev. int., 8(2-3):89\u2013116. Urbanowicz, R. J., Granizo-Mackenzie, D., and Moore, J. H. (2012). Using expert knowledge to guide covering and mutation in a michigan style learning classi\ufb01er system to detect epistasis and heterogeneity. In Int. Conf. on Parallel Problem Solving from Nature, pages 266\u2013275. Springer. Vermorel, J. and Mohri, M. (2005). Multi-armed bandit algorithms and empirical evaluation. In ECML05, pages 437\u2013448. Springer. Wilson, S. W. (1995). Classi\ufb01er Fitness Based on Accuracy. Evolutionary Computation, 3(2):149\u2013175. Wilson, S. W. (2000). Get real! xcs with continuous-valued inputs. In Learning Classi\ufb01er Systems, pages 209\u2013 219. Springer. Wu, Q. and Miao, C. (2013). Curiosity: From psychology to computation. ACM Computing Surveys, 46(2):18. ",
    "title": "ACTIVE REINFORCEMENT LEARNING",
    "paper_info": "ACTIVE REINFORCEMENT LEARNING\nA Roadmap Towards Curious Classi\ufb01er Systems for Self-Adaptation\nSimon Reichhuber1\na and Sven Tomforde1\nb\n1Intelligent Systems, University of Kiel, Hermann-Rodewald-Str. 3, Kiel, Germany\n{sir,st}@informatik.uni-kiel.de\nKeywords:\nOrganic Computing, active learning, reinforcement learning, intelligent systems, active reinforcement\nlearning, learning classi\ufb01er systems\nAbstract:\nIntelligent systems have the ability to improve their behaviour over time taking observations, experiences or\nexplicit feedback into account. Traditional approaches separate the learning problem and make isolated use of\ntechniques from different \ufb01eld of machine learning such as reinforcement learning, active learning, anomaly\ndetection or transfer learning, for instance. In this context, the fundamental reinforcement learning approaches\ncome with several drawbacks that hinder their application to real-world systems: trial-and-error, purely reac-\ntive behaviour or isolated problem handling. The idea of this article is to present a concept for alleviating these\ndrawbacks by setting up a research agenda towards what we call \u201cactive reinforcement learning\u201d in intelligent\nsystems.\n1\nINTRODUCTION\nInformation and communication technology faces a\ntrend towards increasingly complex solutions, e.g.,\ncharacterised by the laws of Moore (Moore, 1965)\nand Glass (Glass, 2002). As a consequence, tradi-\ntional concepts for design, development, and main-\ntenance have reached their limits.\nWithin the last\ndecade, a paradigm shift in engineering such sys-\ntems has been postulated that claims to master com-\nplexity issues by means of self-adaptation and self-\norganisation. Concepts and techniques emerged that\nmove traditional design-time decisions to runtime and\nfrom the system engineer to the systems themselves.\nAs a result, intelligent and autonomously acting sys-\ntems are targeted, with the self-adapting and self-\norganising (SASO) systems domain serving as an\numbrella for several research initiatives focusing on\nthese issues, including Organic Computing (M\u00a8uller-\nSchloer and Tomforde, 2017), Autonomic Comput-\ning (Kephart and Chess, 2003), Interwoven Systems\n(Tomforde et al., 2014), or Self-aware Computing\nSystems (Kounev et al., 2017).\nThe basic idea is in all cases that individual sys-\ntems react autonomously to changing conditions, \ufb01nd\nappropriate reactions, and optimise this process over\na\nhttps://orcid.org/0000-0001-8951-8962\nb\nhttps://orcid.org/0000-0002-5825-8915\ntime\u2014resulting in intelligent system behaviour. For\nthe remainder of this article, we de\ufb01ne such an \u201cintel-\nligent system\u201d (according to (Tomforde et al., 2017))\nas a computing system that achieves or maintains a\ncertain level of performance, even when operating in\nenvironments that change over time and even if it is\nexposed to disturbances or emergent situations. Such\nan intelligent system is autonomously alerting its own\nbehaviour with the goal to improve it over time.\nA keystone in this de\ufb01nition is the ability of an\nintelligent system to learn autonomously at runtime\n(D\u2019Angelo et al., 2019). This means that approaches\nbased on massive training data or continuous feed-\nback/supervision by users are not feasible. In turn,\nthe system has to \ufb01gure out what to do in which\nsituation:\nthe classic reinforcement learning (RL)\nparadigm combined with further mechanisms from\nthe domain of machine learning such as anomaly de-\ntection, transfer learning, or collaborative learning\n(D\u2019Angelo et al., 2020).\nSeveral approaches have been presented where\nvarying RL techniques are used for enabling self-\nimproving\nruntime\nadaptation\nand\norganisation.\nHowever, these approaches come with several draw-\nbacks that hinder their application to real-world sys-\ntems: trial-and-error, purely reactive behaviour, iso-\nlated problem handling, etc. The idea of this article\nis to present a concept for alleviating these drawbacks\nby setting up a research agenda towards what we call\narXiv:2201.03947v1  [cs.LG]  11 Jan 2022\n",
    "GPTsummary": "- (1): The article presents a research agenda, called \"active reinforcement learning\", towards alleviating the drawbacks of traditional reinforcement learning approaches and promoting the self-adaptation and self-organisation in intelligent systems.\n \n- (2): Traditional approaches, such as reinforcement learning, active learning, anomaly detection, or transfer learning, have limitations in dealing with the complex conditions of real-world systems, including trial-and-error, purely reactive behaviour, or isolated problem handling. The proposed approach aims to integrate the strong points of these different techniques and provide a roadmap for the development of curious classifier systems that can achieve self-adaptation and self-organisation in intelligent systems. The approach is motivated by the growing trend of self-adapting and self-organising systems to master complexity issues from traditional design-time decisions to runtime and from the system engineer to the systems themselves. \n\n- (3): The research methodology proposed in this article revolves around the idea of \"active reinforcement learning\", which refers to integrating reinforcement learning with further mechanisms from diverse domains of machine learning, such as anomaly detection, transfer learning, or collaborative learning. The approach aims to enable self-improving runtime adaptation and organisation by allowing the intelligent systems to learn autonomously at runtime based on observations, experiences, or explicit feedback.\n\n- (4): The paper does not present experimental results or performance evaluation, but instead proposes a research roadmap towards the development of curious classifier systems for self-adaptation and self-organisation in intelligent systems. The article suggests that the use of active reinforcement learning can provide a promising direction for these systems to achieve autonomy, scalability, robustness, and sustainability in various application domains.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in proposing a research agenda, called \"active reinforcement learning\", towards promoting the self-adaptation and self-organisation in intelligent systems. The proposed approach can integrate diverse machine learning techniques and provide a roadmap for the development of curious classifier systems that can achieve autonomy, scalability, robustness, and sustainability in various application domains.\n\n- (2): Innovation point: The integration of different machine learning techniques in active reinforcement learning represents an innovative approach to address the limitations of traditional reinforcement learning techniques in dealing with the complex conditions of real-world systems. \n\nPerformance: The article does not present experimental results or performance evaluation, but instead focuses on proposing a research roadmap towards the development of curious classifier systems for self-adaptation and self-organisation in intelligent systems.\n\nWorkload: The proposed research roadmap involves multiple challenges that need to be tackled, such as anomaly detection, transfer learning, or collaborative learning, which require significant efforts from the research community. However, the potential benefits of achieving self-adaptation and self-organisation in intelligent systems can justify the workload.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in proposing a research agenda, called \"active reinforcement learning\", towards promoting the self-adaptation and self-organisation in intelligent systems. The proposed approach can integrate diverse machine learning techniques and provide a roadmap for the development of curious classifier systems that can achieve autonomy, scalability, robustness, and sustainability in various application domains.\n\n- (2): Innovation point: The integration of different machine learning techniques in active reinforcement learning represents an innovative approach to address the limitations of traditional reinforcement learning techniques in dealing with the complex conditions of real-world systems. \n\nPerformance: The article does not present experimental results or performance evaluation, but instead focuses on proposing a research roadmap towards the development of curious classifier systems for self-adaptation and self-organisation in intelligent systems.\n\nWorkload: The proposed research roadmap involves multiple challenges that need to be tackled, such as anomaly detection, transfer learning, or collaborative learning, which require significant efforts from the research community. However, the potential benefits of achieving self-adaptation and self-organisation in intelligent systems can justify the workload.\n\n\n"
}