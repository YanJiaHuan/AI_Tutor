{
    "Abstract": "Abstract\u2014We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL). Previous attempts to fuse classical control and RL are variations on imitation learning or human-in-the-loop learning. Also, in their approaches, the training acceleration comes with an implicit cap on what is attainable by the RL agent, therefore it is vital to have high-quality controllers. We ask if it is possible to accelerate RL with even a primitive hand-tuned PID controller, and we draw inspiration from the relationship between athletes and their coaches. At the top level of the athletic world, a coach\u2019s job is not to function as a template to be imitated after, but rather to provide conditions for the athletes to collect critical experiences. We seek to construct a coaching relationship between the PID controller and the RL agent, where the controller helps the agent experience the most pertinent states for the task. We conduct experiments in Mujoco locomotion simulation, but the setup can be easily converted into real-world circumstances. We conclude from the data that when the coaching structure between the PID controller and its respective RL agent is set at its goldilocks spot, the agent\u2019s training can be accelerated by up to 37%, yielding uncompromised training results in the meantime. This is an important proof of concept that controller-based coaching can be a novel and effective paradigm for merging classical control with learning and warrants further investigations in this direction. All the code and data can be found at github/BaiLiping/Coaching Index Terms\u2014Reinforcement Learning, Control, Learning for Dynamic Control, L4DC I. ",
    "Introduction": "",
    "Approach": "Approach for Merging Control with Reinforcement Learning Liping Bai Abstract\u2014We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL). Previous attempts to fuse classical control and RL are variations on imitation learning or human-in-the-loop learning. Also, in their approaches, the training acceleration comes with an implicit cap on what is attainable by the RL agent, therefore it is vital to have high-quality controllers. We ask if it is possible to accelerate RL with even a primitive hand-tuned PID controller, and we draw inspiration from the relationship between athletes and their coaches. At the top level of the athletic world, a coach\u2019s job is not to function as a template to be imitated after, but rather to provide conditions for the athletes to collect critical experiences. We seek to construct a coaching relationship between the PID controller and the RL agent, where the controller helps the agent experience the most pertinent states for the task. We conduct experiments in Mujoco locomotion simulation, but the setup can be easily converted into real-world circumstances. We conclude from the data that when the coaching structure between the PID controller and its respective RL agent is set at its goldilocks spot, the agent\u2019s training can be accelerated by up to 37%, yielding uncompromised training results in the meantime. This is an important proof of concept that controller-based coaching can be a novel and effective paradigm for merging classical control with learning and warrants further investigations in this direction. All the code and data can be found at github/BaiLiping/Coaching Index Terms\u2014Reinforcement Learning, Control, Learning for Dynamic Control, L4DC I. INTRODUCTION L EARNING for Dynamic Control is an emerging \ufb01eld of research located at the interaction between classic control and reinforcement learning (RL). Although RL community routinely generate jaw-dropping results that seem out of reach to the control community[1][2][3], the theories that undergird RL are as bleak as it was \ufb01rst introduced[4]. Today, those de\ufb01ciencies can be easily papered over by the advent of Deep Neural Networks (DNN) and ever faster computational capacities. For RL to reach its full potential, existing control theories and strategies have to be part of that new combined formulation. There are three ways that classic control \ufb01nds its way into RL. First, theorists who are well versed in optimization techniques and mathematical formalism can provide systematic perspectives to RL and invent the much needed analytical tools[5][6][7][8][9]. Second, system identi\ufb01cation researchers are exploring all possible con\ufb01gurations to combine existing system models with DNN and its variations[10][11][12][13][14]. Third, proven controllers can provide Nanjing Unversity of Posts and Telecommunications, College of Automation & College of Arti\ufb01cial Intelligence, Nanjing, Jiangsu,210000 China email:zqpang@njupt.edu.cn data on successful control trajectories to be used in imitation learning, reverse reinforcement learning, and \"human\"-in-theloop learning[15][16][17][18][19]. Our approach is an extension of the third way of combining classing control with RL. Previous researches[20][21][22] are about making a functioning controller works better. To begin with, they require high-quality controllers, and the improvements brought about by the RL agents are merely icing on the cake. In addition, the controllers inadvertently impose limits on what can be achieved by the RL agents. If, unfortunately, a bad controller is chosen, then the RL training process would be hindered rather than expedited. We ask the question, can we speed up RL training with hand-tuned PID controllers, which are primitive but still captures some of our understanding of the system? This inquiry leads us to the relationship between athletes and their coaches. Professional athletes don\u2019t get where they are via trial-anderror. Their skillsets are forged through painstakingly designed coaching techniques. At the top level, a coach\u2019s objective is not to be a template for the athletes to imitate after, but rather is to facilitate data collection on critical states. Top athletes are not necessarily good coaches and vice versa. In our approach, the \u2019coaches\u2019 are PID controllers which we deliberately tuned to be barely functioning, as shown by Table I. Yet, even with such bad controllers, when appropriately structured, training acceleration is still observed in our experiments, as shown by Table II. The critical idea of coaching is for the PID controllers to take over when the RL agents deviated from the essential states. Our approach differs from previous researches in one signi\ufb01cant way: controllers\u2019 interventions and the rewards associated with such interventions are hidden from the RL agents. They are not part of the training data. We also restrain from reward engineering and leave everything as it is, other than the coaching intervention. This way, we can be con\ufb01dent that the observed acceleration does not stem from other alterations. The implementations would be detailed in subsequent sections. Environment PID Controller RL Agent PID/RL Inverted Pendulum 240 1000 24.0% Double Pendulum 1107 9319 11.9% Hopper 581 989 58.7% Walker 528 1005 52.5% TABLE I: Performance Comparison between PID controller and its respective RL agent. We interfaced with Mujoco simulation through OpenAI GYM, and every simulated environment comes with predetermined maximum episode steps. The scores achieved by the RL agents would probably be high if not for this reason. arXiv:2210.00770v1  [eess.SY]  3 Oct 2022 2 Environment Target Measure With PID Without Percentage Name Score Coaching Coaching Increase Inverted 800 Win Streak 100 160 37.5% Pendulum Average 104 159 34.6% Double 5500 5 Wins 908 1335 31.9% Pendulum Average 935 1370 29.9% Hopper 800 5 Wins 2073 2851 27.3% Average 2155 2911 25.9% Walker 800 5 Wins 4784 5170 7.5% Average 5659 7135 20.7% TABLE II: Comparison Between Agents Trained With and Without PID Controller Coaching. Even though the PID controllers are less capable than the eventual RL agent, they are still useful and can accelerate the RL agent training. There two measures we used to gauge training acceleration. The \ufb01rst is \ufb01ve consecutive wins, and the second is the scoring average. The \"win\" is a predetermined benchmark. In section II, we present the idea of controller-based coaching. In section III, we present the results of our experiments and their detailed implementations. We conclude what we have learned and layout directions for further research in section IV. II. CONTROLLER BASED COACHING Reinforcement Learning is the process of cycling between interaction with an environment and re\ufb01nement of the understanding of that environment. RL agents methodically extract information from experiences, gradually bounding system models, policy distributions, or cost-to-go approximations to maximize the expected rewards along a trajectory, as shown by Figure1 which is an adaption of Benjamin Recht\u2019s presentation[23]. Fig. 1: From Optimization to Learning. Model-Based or Model-Free learning refers to whether or not learning is used to approximate the system dynamics function. If there is an explicit action policy, it is called on-policy learning. Otherwise, the optimal action would be implicitly captured by the Q value function, and that would be called off-policy learning instead. Importance sampling allows \"limited off-policy learning\" capacity, which enables data reuse in a trusted region. Online learning means interleaving data collection and iterative network parameters update. Of\ufb02ine learning means the data is collected in bulk \ufb01rst, and then the network parameters are set with regression computation. Batch learning, as the name suggested, is in between online and of\ufb02ine learning. An agent would \ufb01rst generate data that \ufb01ll its batch memory and then sample from the batch memories for iterative parameter update. New data would be generated with the updated parameters to replace older data in the memory. This taxonomy is somewhat outdated now. When Richard Sutton wrote his book, the algorisms he had in mind fall nicely into various categories. Today, however, the popular algorisms would combine more than one route to derive superior performance and can\u2019t be pigeonholed. A fundamental concept for RL is convergence through bootstrap. Instead of asymptotically approaching a known target function2a, bootstrap methods approach an assumed target \ufb01rst and then update the target assumption based on collected data2b. When evaluating estimation functions with belief rather than of the real value, things could just run around in circles and never converge. Without any guidance, the RL agent would (a) With Known Evaluation Function (b) Bootstrap have just explored all the possible states, potentially resulting in this unstable behavior. One method to produce more ef\ufb01cient exploration and avoid instability is to give more weight to critical states. Not all observational states are created equal. Some are vital, while others have nothing to do with the eventual objective. For instance, in the inverted pendulum task, any states outside of the Lyapunov stability bounds should be ignored since they can\u2019t be properly controlled anyway. There are statistical techniques to distinguish critical states from the non-essential ones, and imitation learning works by marking crucial states with demonstrations. However, the former approach is hard to implement, and the latter one requires high-quality controllers. Our proposed controller-based coaching method is easy to implement and does not have stringent requirements on the controllers it uses. Controller-based coaching works by adjusting the trajectory of the RL agent and avoid wasting valuable data collection cycle on the states that merely lead to eventual termination. When the agent is about to deviate from essential states, the controller will apply a force to nudge the agent back to where it should be, much like a human coach course-corrects on behalf of the athlete. Crucially, the agent is oblivious to this intervention step, and it would not be part of the agent\u2019s data. Even if the controller didn\u2019t adjust the agent to where it should be, it would not have mattered since it is unaware of it because it is a high-quality controller. On the other hand, if the controller successfully adjusts the trajectory, the RL agent\u2019s next data collection cycle will be spent in a critical state. We test our approach on four mujoco locomotion environments as a proof of concept, and in all four experiments, the hypothesized acceleration on RL training is observed. III. EXPERIMENT SETUP Mujoco physics engine[24], is one of many such simulation tools. We interface with it through a python wrapper provided by the OpenAI Gym[25] team. We choose four environments for our experiments: inverted pendulum, double inverted pendulum, hopper, and walker. Every environment comes with a set of predetermined rewards and maximum episode steps. We did not tinker with those parameters. The only change we made to each environment is a controller-based coach ready to intervene when the agent steps out of the predetermined critical states. We use tensorforce\u2019s[26] implementation of RL agents, speci\ufb01cally the Proximal Policy Optimization (PPO) agent because the learning curves generated by PPO agent are smoother, 3 Fig. 3: Inverted Pendulum system controlled by RL agent and its PID coach. The left plot is RL agent and the right plot is the PID controller. The maximum episode steps are 1000 and each step without termination is scored 1 point. The average score achieved by the RL agent is 1000 and the average score achieved by the PID controller is 240. as shown by the spinning up[27] team. Our paper aims to indicate the controller-based coaching method\u2019s feasibility, and a smoother curve makes things easier. In this paper, human judgment is the basis for the determination of critical states. In future works, we would like to provide a more systematic process for critical states\u2019 demarkation. We will provide our reasonings when we discuss our experiments in each environment. Our experiments\u2019 code and data can be accessed via this deposite. The folders are named after each environment, and in each folder, you will \ufb01nd a data record, an RL agent record, a agent.json \ufb01le that indicates all the hyperparameters of the RL agent, a Normal \ufb01le that trains an RL agent with the normal method, a Coached \ufb01le which trains an agent based on the controller based coaching method, a PID \ufb01le which is the PID coach. In the PIDvsRL folder, you will \ufb01nd \ufb01les that generate all the plots shown in the following section. A. Inveretd Pendulum The observation space of the inverted pendulum environment is: [Cart Position on X-axis, Cart Velocity, Pole Angle, Pole Angular Velocity]. The continuous action space is an action ranging from -10 to 10, where -10 means the actuator moves the cart to the left with maximum power and 10 means the actuator moves the cart to the right with full force. The maximum episode step number is 1000. The terminal state for the inverted pendulum is an angle of absolute value greater than 0.2 radius. The reward is 1 for each non-terminal step and 0 for the terminal step. Figure3 shows how the RL agent and the PID controller manage the pole angle and the angular velocity. The left plot is the RL agent, and the right plot is the PID controller coach. While the PID controller tries hard to converge to zero, eventually, there would be too much accumulation on the integral term, and the equilibrium breaks down. The average score achieved by the RL agent is 1000, and the average score achieved by the PID controller is 240. Based on our observation of the system, we decided to put the boundary between critical and noncritical states on the angular velocity of the absolute value of 0.4. The RL agent is free to explore with the angular velocity with an absolute value smaller than 0.4, but once it goes above this bound, the PID controller will kick in, trying to decrease the velocity back to the 0.4 bound. Fig. 4: Inverted Pendulum Coaching Result The experiment result is presented by Figure4. The black line indicates the agent trained normally, and the blue line indicates the agent trained with a PID coach. A win is set to be with a score greater than 800. It takes the normal method 160 episodes to get \ufb01ve consecutive wins, and it takes the controller-based coaching method 100 episodes to do the same. As measured by averaging over 10 episodes, it takes the normal method 159 episodes to go beyond 800, and it takes controller-based coaching method 104 episodes to do the same. The acceleration is roughly 35%. The agents trained with both methods pass the evaluation, and their respective average scores are presented in the upper left corner. B. Inverted Double Pendulum The inverted double pendulum has observation space of the following: [x position of the cart, sin(\u03b81), sin(\u03b82),cos(\u03b81),cos(\u03b82),velocity of x, angular velocity of \u03b81, angular velocity of \u03b82, constraint force on x, constraint force on \u03b81, constraint force on \u03b82]. \u03b81 and \u03b82 are the angles of the upper and lower pole perspectively. The action space for Inverted Double Pendulum is an action ranging from -10 to 10, where -10 means the actuator moves the cart to the left with maximum power and 10 means the actuator moves the cart to the right with maximum power. A score of roughly 10 points is assigned to non-terminal states, based on the velocity on the x-axis. The detailed formular for score computation can be found at the OpenAI site. Figure5 shows how the RL agent and the PID controller manage the lower pole angle and its angular velocity. The left plot is the RL agent and the right plot is the PID controller coach. The RL agent seems to settle on continuously oscillating from the left limit to the right limit until the maximum episode steps are reached. The PID controller functions well until the equilibrium breaks down with too much accumulation on the integral term. The average score achieved by the RL agent is 9319, and the average score achieved by the PID controller is 1107. Based on our observation of the system, we decided to put the boundary between critical and noncritical states on the lower angle of the absolute value of 0.2. The RL agent is free to explore with the lower angle with an absolute value smaller than 0.2, but once it goes above this bound, the PID controller 4 Fig. 5: Inverted Double Pendulum system controlled by RL agent and its PID coach. The left plot is RL agent and the right plot is the PID controller. The maximum episode steps are 1000 and each step without termination is scored at around 10 points, based on velocity on the x axis. The average score achieved by the RL agent is 9319 and the average score achieved by the PID controller is 1107. Fig. 6: Double Inverted Pendulum Coaching Result. will kick in, trying to nudge the lower angle back to the 0.2 bound. The experiment result is presented by Figure6. The black line indicates the agent trained normally, and the blue line indicates the agent trained with a PID coach. A win is set to be with a score greater than 5500. It takes the normal method 1335 episodes to get \ufb01ve consecutive wins, and it takes controller-based coaching method 908 episodes to do the same. As measured by averaging over 100 episodes, it takes normal method 1370 episodes to go beyond 5500, and it takes controller-based coaching method 935 episodes to do the same. The acceleration is roughly 30%. The agents trained with both methods pass the evaluation, and their respective average scores are presented in the upper left corner. C. Hopper The observation space of hopper is the following vector: [z position, y position, thigh joint angle, leg joint angle, foot joint angle, velocity at x-axis, velocity at z-axis, velocity at y-axis, angular velocity of thigh joint, angular velocity of leg joint, angular velocity of foot joint]. The hopper\u2019s action space is three continuous action choices for three actuators [thigh actuator, leg actuator, foot actuator]. The range of actuators is -10, which means applying force towards the negative direction with maximum power, and 10, which means applying force towards the positive direction with maximum power. The terminal states for hopper are when the absolute y position is greater than 0.2. Based on our observation of the system, we decided to put the boundary between critical and noncritical states on the y axis velocity of the absolute value of 0.3. The RL agent is Fig. 7: Hopper system controlled by RL agent and its PID coach. The left plot is RL agent and the right plot is the PID controller. The maximum episode steps are 1000 and each step without termination is scored at around 1 point, depending on the x-axis velocity. The average score achieved by the RL agent is 989 and the average score achieved by the PID controller is 581. Fig. 8: Hopper Coaching Result. Benchmarked against training without coaching, indicated by the black dotted line. free to explore with the y axis velocity with an absolute value smaller than 0.3, but once it goes above this bound, the PID controller will kick in, trying to decrease the y axis velocity back to the 0.3 bound. Figure7 shows how the RL agent and the PID controller manage they position and its velocity. The left plot is the RL agent and the right plot is the PID controller coach. The RL agent seems to settle on doing nothing until the maximum episode steps are reached. The PID controller can only manage they position into an oscillation with ever-increasing magnitude. The average score achieved by the RL agent is 989, and the average score achieved by the PID controller is 581. The experiment result is presented by Figure8. The agent trained normally is indicated by the black line, and the agent trained with PID coach is indicated by the blue line. A win is set to be with a score greater than 800. It takes the normal method 2851 episodes to get \ufb01ve consecutive wins, and it takes controller-based coaching method 2073 episode to do the same. As measured by averaging over 100 episodes, it takes the normal method 2911 episodes to go beyond 800, and it takes the controller-based coaching method 2155 episodes to do the same. The acceleration is roughly 25%. The agents trained with both methods pass the evaluation, and their respective average scores are presented in the upper left corner. D. Walker The walker system is just a two-legged hopper. The observation space is the same as listed in the hopper environment but for both legs. The terminal state is when the z position falls below 0.8. ",
    "References": "REFERENCES [1] O. M. Andrychowicz, B. Baker, M. Chociej, R. J\u00f3zefowicz, B. McGrew, J. W. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba, \u201cLearning dexterous in-hand manipulation,\u201d The International Journal of Robotics Research, vol. 39, pp. 20 \u2013 3, 2020. [2] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine, \u201cQt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation,\u201d ArXiv, vol. abs/1806.10293, 2018. [3] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, \u201cLearning quadrupedal locomotion over challenging terrain,\u201d Science Robotics, vol. 5, 2020. [4] D. Bertsekas and J. Tsitsiklis, \u201cNeuro-dynamic programming,\u201d in Encyclopedia of Machine Learning, 1996. [5] M. Han, L. Zhang, J. Wang, and W. Pan, \u201cActor-critic reinforcement learning for control with stability guarantee,\u201d IEEE Robotics and Automation Letters, vol. 5, pp. 6217\u20136224, 2020. [6] E. Weinan, \u201cA proposal on machine learning via dynamical systems,\u201d 2017. [7] E. Dupont, A. Doucet, and Y. Teh, \u201cAugmented neural odes,\u201d in NeurIPS, 2019. [8] M. Betancourt, M. I. Jordan, and A. Wilson, \u201cOn symplectic optimization,\u201d arXiv: Computation, 2018. [9] O. Nachum and B. Dai, \u201cReinforcement learning via fenchel-rockafellar duality,\u201d ArXiv, vol. abs/2001.01866, 2020. [10] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, \u201cLearningbased model predictive control: Toward safe learning in control,\u201d 2020. [11] A. Mohan, N. Lubbers, D. Livescu, and M. Chertkov, \u201cEmbedding hard physical constraints in neural network coarse-graining of 3d turbulence.\u201d arXiv: Computational Physics, 2020. 6 [12] B. Lusch, J. N. Kutz, and S. Brunton, \u201cDeep learning for universal linear embeddings of nonlinear dynamics,\u201d Nature Communications, vol. 9, 2018. [13] S. Bai, J. Z. Kolter, and V. Koltun, \u201cDeep equilibrium models,\u201d ArXiv, vol. abs/1909.01377, 2019. [14] F. de Avila Belbute-Peres, T. D. Economon, and J. Z. Kolter, \u201cCombining differentiable pde solvers and graph neural networks for \ufb02uid \ufb02ow prediction,\u201d ArXiv, vol. abs/2007.04439, 2020. [15] W. B. Knox and P. Stone, \u201cInteractively shaping agents via human reinforcement: the tamer framework,\u201d in K-CAP \u201909, 2009. [16] \u2014\u2014, \u201cCombining manual feedback with subsequent mdp reward signals for reinforcement learning,\u201d in AAMAS, 2010. [17] X. Peng, P. Abbeel, S. Levine, and M. V. D. Panne, \u201cDeepmimic: Example-guided deep reinforcement learning of physics-based character skills,\u201d ACM Trans. Graph., vol. 37, pp. 143:1\u2013143:14, 2018. [18] X. Peng, E. Coumans, T. Zhang, T. Lee, J. Tan, and S. Levine, \u201cLearning agile robotic locomotion skills by imitating animals,\u201d ArXiv, vol. abs/2004.00784, 2020. [19] T. Paine, S. G. Colmenarejo, Z. Wang, S. Reed, Y. Aytar, T. Pfaff, M. W. Hoffman, G. Barth-Maron, S. Cabi, D. Budden, and N. D. Freitas, \u201cOneshot high-\ufb01delity imitation: Training large-scale deep nets with rl,\u201d ArXiv, vol. abs/1810.05017, 2018. [20] L. Xie, S. Wang, S. Rosa, A. Markham, and A. Trigoni, \u201cLearning with training wheels: Speeding up training with a simple controller for deep reinforcement learning,\u201d 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 6276\u20136283, 2018. [21] I. Carlucho, M. D. Paula, S. A. Villar, and G. G. Acosta, \u201cIncremental q-learning strategy for adaptive pid control of mobile robots,\u201d Expert Syst. Appl., vol. 80, pp. 183\u2013199, 2017. [22] B. S. Pavse, F. Torabi, J. P. Hanna, G. Warnell, and P. Stone, \u201cRidm: Reinforced inverse dynamics modeling for learning from a single observed demonstration,\u201d IEEE Robotics and Automation Letters, vol. 5, pp. 6262\u20136269, 2020. [23] B. Recht, \u201cA tour of reinforcement learning: The view from continuous control,\u201d ArXiv, vol. abs/1806.09460, 2018. [24] E. Todorov, T. Erez, and Y. Tassa, \u201cMujoco: A physics engine for modelbased control,\u201d in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 5026\u20135033. [25] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, \u201cOpenai gym,\u201d ArXiv, vol. abs/1606.01540, 2016. [26] A. Kuhnle, M. Schaarschmidt, and K. Fricke, \u201cTensorforce: a tensor\ufb02ow library for applied reinforcement learning,\u201d Web page, 2017. [Online]. Available: https://github.com/tensorforce/tensorforce [27] J. Achiam, \u201cSpinning Up in Deep Reinforcement Learning,\u201d 2018. [28] E. Kaufmann, A. Loquercio, R. Ranftl, M. M\u00fcller, V. Koltun, and D. Scaramuzza, \u201cDeep drone acrobatics,\u201d ArXiv, vol. abs/2006.05768, 2020. ",
    "title": "Coaching with PID Controllers: A Novel Approach",
    "paper_info": "1\nCoaching with PID Controllers: A Novel Approach\nfor Merging Control with Reinforcement Learning\nLiping Bai\nAbstract\u2014We propose a Proportional Integral Derivative (PID)\ncontroller-based coaching scheme to expedite reinforcement learn-\ning (RL). Previous attempts to fuse classical control and RL are\nvariations on imitation learning or human-in-the-loop learning.\nAlso, in their approaches, the training acceleration comes with\nan implicit cap on what is attainable by the RL agent, therefore it\nis vital to have high-quality controllers. We ask if it is possible to\naccelerate RL with even a primitive hand-tuned PID controller,\nand we draw inspiration from the relationship between athletes\nand their coaches. At the top level of the athletic world, a coach\u2019s\njob is not to function as a template to be imitated after, but rather\nto provide conditions for the athletes to collect critical experiences.\nWe seek to construct a coaching relationship between the PID\ncontroller and the RL agent, where the controller helps the agent\nexperience the most pertinent states for the task. We conduct\nexperiments in Mujoco locomotion simulation, but the setup can\nbe easily converted into real-world circumstances. We conclude\nfrom the data that when the coaching structure between the PID\ncontroller and its respective RL agent is set at its goldilocks spot,\nthe agent\u2019s training can be accelerated by up to 37%, yielding\nuncompromised training results in the meantime. This is an\nimportant proof of concept that controller-based coaching can\nbe a novel and effective paradigm for merging classical control\nwith learning and warrants further investigations in this direction.\nAll the code and data can be found at github/BaiLiping/Coaching\nIndex Terms\u2014Reinforcement Learning, Control, Learning for\nDynamic Control, L4DC\nI. INTRODUCTION\nL\nEARNING for Dynamic Control is an emerging \ufb01eld of\nresearch located at the interaction between classic control\nand reinforcement learning (RL). Although RL community\nroutinely generate jaw-dropping results that seem out of reach\nto the control community[1][2][3], the theories that undergird\nRL are as bleak as it was \ufb01rst introduced[4]. Today, those\nde\ufb01ciencies can be easily papered over by the advent of\nDeep Neural Networks (DNN) and ever faster computational\ncapacities. For RL to reach its full potential, existing control\ntheories and strategies have to be part of that new combined\nformulation.\nThere are three ways that classic control \ufb01nds its way\ninto RL. First, theorists who are well versed in optimiza-\ntion techniques and mathematical formalism can provide\nsystematic perspectives to RL and invent the much needed\nanalytical tools[5][6][7][8][9]. Second, system identi\ufb01cation\nresearchers are exploring all possible con\ufb01gurations to com-\nbine existing system models with DNN and its varia-\ntions[10][11][12][13][14]. Third, proven controllers can provide\nNanjing Unversity of Posts and Telecommunications, College of Automa-\ntion & College of Arti\ufb01cial Intelligence, Nanjing, Jiangsu,210000 China\nemail:zqpang@njupt.edu.cn\ndata on successful control trajectories to be used in imitation\nlearning, reverse reinforcement learning, and \"human\"-in-the-\nloop learning[15][16][17][18][19].\nOur approach is an extension of the third way of combining\nclassing control with RL. Previous researches[20][21][22]\nare about making a functioning controller works better. To\nbegin with, they require high-quality controllers, and the\nimprovements brought about by the RL agents are merely\nicing on the cake. In addition, the controllers inadvertently\nimpose limits on what can be achieved by the RL agents. If,\nunfortunately, a bad controller is chosen, then the RL training\nprocess would be hindered rather than expedited. We ask the\nquestion, can we speed up RL training with hand-tuned PID\ncontrollers, which are primitive but still captures some of our\nunderstanding of the system? This inquiry leads us to the\nrelationship between athletes and their coaches.\nProfessional athletes don\u2019t get where they are via trial-and-\nerror. Their skillsets are forged through painstakingly designed\ncoaching techniques. At the top level, a coach\u2019s objective is\nnot to be a template for the athletes to imitate after, but rather\nis to facilitate data collection on critical states. Top athletes\nare not necessarily good coaches and vice versa.\nIn our approach, the \u2019coaches\u2019 are PID controllers which\nwe deliberately tuned to be barely functioning, as shown by\nTable I. Yet, even with such bad controllers, when appropriately\nstructured, training acceleration is still observed in our experi-\nments, as shown by Table II. The critical idea of coaching is for\nthe PID controllers to take over when the RL agents deviated\nfrom the essential states. Our approach differs from previous\nresearches in one signi\ufb01cant way: controllers\u2019 interventions\nand the rewards associated with such interventions are hidden\nfrom the RL agents. They are not part of the training data. We\nalso restrain from reward engineering and leave everything as\nit is, other than the coaching intervention. This way, we can\nbe con\ufb01dent that the observed acceleration does not stem from\nother alterations. The implementations would be detailed in\nsubsequent sections.\nEnvironment\nPID Controller\nRL Agent\nPID/RL\nInverted Pendulum\n240\n1000\n24.0%\nDouble Pendulum\n1107\n9319\n11.9%\nHopper\n581\n989\n58.7%\nWalker\n528\n1005\n52.5%\nTABLE I: Performance Comparison between PID controller and its respective RL agent.\nWe interfaced with Mujoco simulation through OpenAI GYM, and every simulated\nenvironment comes with predetermined maximum episode steps. The scores achieved\nby the RL agents would probably be high if not for this reason.\narXiv:2210.00770v1  [eess.SY]  3 Oct 2022\n",
    "GPTsummary": "- (1): The proposed methodology is a coaching scheme between a PID controller and an RL agent, where the PID controller helps the RL agent experience the most critical states for the task.\n- (2): The controller intervenes based on a set of rules without giving any explicit reward signal to the RL agent.\n- (3): The experiments conducted in Mujoco locomotion simulation show that the training acceleration can be up to 37%, yielding uncompromised training results. \n- (4): The critical idea of coaching is for the PID controllers to take over when the RL agents deviated from the essential states. Our approach differs from previous researches in one significant way: controllers' interventions and the rewards associated with such interventions are hidden from the RL agents. Special attention is paid to the lack of reward engineering, which allows us to be confident that the observed acceleration does not stem from other alterations. The implementations are detailed in subsequent sections.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant as it proposes a novel and effective paradigm for combining classical control with reinforcement learning, which can accelerate the training of RL agents without affecting the quality of the learning results. \n\n- (2): Innovation point: The article innovatively proposes a coaching relationship between a PID controller and an RL agent, inspired by the coaching relationship between athletes and coaches. The use of a low-quality controller to help the RL agent experience critical states represents a significant departure from previous research, which either relied on imitation learning or human-in-the-loop learning. \n\nPerformance: The experiments conducted in the Mujoco locomotion simulation show that the coaching-based methodology can accelerate the training of RL agents by up to 37% while yielding uncompromised training results. \n\nWorkload: The article provides detailed descriptions of the experimental procedures and code, making it easy to replicate the study. However, the article could benefit from more in-depth analysis and discussions of the limitations and potential applications of the proposed coaching-based methodology.\n\n\n",
    "GPTmethods": "- (1): The proposed methodology is a coaching scheme between a PID controller and an RL agent, where the PID controller helps the RL agent experience the most critical states for the task.\n- (2): The controller intervenes based on a set of rules without giving any explicit reward signal to the RL agent.\n- (3): The experiments conducted in Mujoco locomotion simulation show that the training acceleration can be up to 37%, yielding uncompromised training results. \n- (4): The critical idea of coaching is for the PID controllers to take over when the RL agents deviated from the essential states. Our approach differs from previous researches in one significant way: controllers' interventions and the rewards associated with such interventions are hidden from the RL agents. Special attention is paid to the lack of reward engineering, which allows us to be confident that the observed acceleration does not stem from other alterations. The implementations are detailed in subsequent sections.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant as it proposes a novel and effective paradigm for combining classical control with reinforcement learning, which can accelerate the training of RL agents without affecting the quality of the learning results. \n\n- (2): Innovation point: The article innovatively proposes a coaching relationship between a PID controller and an RL agent, inspired by the coaching relationship between athletes and coaches. The use of a low-quality controller to help the RL agent experience critical states represents a significant departure from previous research, which either relied on imitation learning or human-in-the-loop learning. \n\nPerformance: The experiments conducted in the Mujoco locomotion simulation show that the coaching-based methodology can accelerate the training of RL agents by up to 37% while yielding uncompromised training results. \n\nWorkload: The article provides detailed descriptions of the experimental procedures and code, making it easy to replicate the study. However, the article could benefit from more in-depth analysis and discussions of the limitations and potential applications of the proposed coaching-based methodology.\n\n\n",
    "GPTconclusion": "- (1): This piece of work is significant as it proposes a novel and effective paradigm for combining classical control with reinforcement learning, which can accelerate the training of RL agents without affecting the quality of the learning results. \n\n- (2): Innovation point: The article innovatively proposes a coaching relationship between a PID controller and an RL agent, inspired by the coaching relationship between athletes and coaches. The use of a low-quality controller to help the RL agent experience critical states represents a significant departure from previous research, which either relied on imitation learning or human-in-the-loop learning. \n\nPerformance: The experiments conducted in the Mujoco locomotion simulation show that the coaching-based methodology can accelerate the training of RL agents by up to 37% while yielding uncompromised training results. \n\nWorkload: The article provides detailed descriptions of the experimental procedures and code, making it easy to replicate the study. However, the article could benefit from more in-depth analysis and discussions of the limitations and potential applications of the proposed coaching-based methodology.\n\n\n"
}