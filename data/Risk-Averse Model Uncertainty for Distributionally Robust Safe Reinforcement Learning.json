{
    "Abstract": "Abstract Many real-world domains require safe decision making in the presence of uncertainty. In this work, we propose a deep reinforcement learning framework for approaching this important problem. We consider a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures, and we show that our formulation is equivalent to a distributionally robust safe reinforcement learning problem with robustness guarantees on performance and safety. We propose an ef\ufb01cient implementation that only requires access to a single training environment, and we demonstrate that our framework produces robust, safe performance on a variety of continuous control tasks with safety constraints in the Real-World Reinforcement Learning Suite. 1. ",
    "Introduction": "Introduction In many real-world decision making applications, it is important to satisfy safety requirements while achieving a desired goal. In addition, real-world environments often involve uncertain or changing conditions. Therefore, in order to reliably deploy data-driven decision making methods in these settings, they must produce safe performance even in the presence of uncertainty. Deep reinforcement learning (RL) represents a powerful tool for data-driven sequential decision making in complex environments. Motivated by the need for safety in real-world decision making, several algorithms have incorporated safety constraints into the deep RL framework. However, these safe RL algorithms typically focus on satisfying safety requirements in a single training environment, and do not consider the issue of environment (i.e., model) uncertainty. 1Division of Systems Engineering, Boston University, Boston, MA, USA (work partly done during an internship at MERL) 2Mitsubishi Electric Research Laboratories, Cambridge, MA, USA. Correspondence to: James Queeney <jqueeney@bu.edu>, Mouhacine Benosman <benosman@merl.com>. Preprint. In order to account for model uncertainty in the deep RL training process, a popular approach is to consider a distribution of training environments instead of training on a single environment. Domain randomization (Peng et al., 2018), for example, collects training data from a variety of environments by altering important simulator parameters such as mass or friction in robotics applications. By interacting with many different environments during training and applying domain knowledge to de\ufb01ne the training distribution in a way that captures key sources of uncertainty, domain randomization has achieved empirical success across a range of complex tasks (Andrychowicz et al., 2020). Unfortunately, high-\ufb01delity simulators are not readily available in many real-world applications, such as tactile sensing in robotics (Xu et al., 2022). In other applications, such as \ufb02uid dynamics (Mowlavi et al., 2022) and circuit design (Cao et al., 2022), the computation required for simulation can make data collection across many environments impractical. In these cases, we require methods that can incorporate model uncertainty while only leveraging data from a single training environment. This training environment may represent a simulator with a \ufb01xed set of parameters, or data collection in a controlled real-world setting. In the context of safe decision making, it is not only important to account for model uncertainty in the training process, but also to learn policies that are robust to this uncertainty. Existing methods such as domain randomization typically optimize for average performance over a distribution of environments, which results in a risk-neutral approach to uncertainty that works well in practice but lacks robustness guarantees. Robust RL methods address this issue by focusing on worst-case environments in an uncertainty set, but as a result require solving dif\ufb01cult minimax optimization problems throughout training that can lead to instability and overly-conservative behavior. In this work, we propose a general approach to safe RL in the presence of model uncertainty that addresses the main shortcomings of existing methods. In particular, we apply a risk-averse perspective towards model uncertainty, which leads to a framework for learning robust and safe policies that can be ef\ufb01ciently implemented using only data collected from a single training environment. By doing so, we guararXiv:2301.12593v1  [cs.LG]  30 Jan 2023 Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning antee safety in the presence of model uncertainty, without the need for prior domain knowledge or detailed simulator access. Our main contributions are as follows: 1. We reformulate the safe RL problem to incorporate a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures, and we introduce the corresponding Bellman operators. 2. We provide robustness guarantees for our risk-averse formulation by showing that it is equivalent to a distributionally robust safe RL problem. 3. We propose an ef\ufb01cient deep RL implementation that uses data collected from a single training environment and avoids the dif\ufb01cult minimax formulation present in robust RL methods. 4. We demonstrate the robust, safe performance of our framework through experiments on continuous control tasks with safety constraints in the Real-World RL Suite (Dulac-Arnold et al., 2020; 2021). 2. Preliminaries 2.1. Safe Reinforcement Learning In this work, we consider RL in the presence of safety constraints. We model this sequential decision making problem as an in\ufb01nite-horizon, discounted Constrained Markov Decision Process (CMDP) (Altman, 1999) de\ufb01ned by the tuple (S, A, p, r, c, d0, \u03b3), where S is the set of states, A is the set of actions, p : S \u00d7 A \u2192 P(S) is the transition model where P(S) represents the space of probability measures over S, r, c : S \u00d7 A \u2192 R are the reward function and cost function used to de\ufb01ne the objective and constraint, respectively, d0 \u2208 P(S) is the initial state distribution, and \u03b3 is the discount rate. We focus on the setting with a single constraint, but all results can be extended to the case of multiple constraints. We model the agent\u2019s decisions as a stationary policy \u03c0 : S \u2192 P(A). For a given CMDP, the goal of safe RL is to \ufb01nd a policy \u03c0 that maximizes the expected total discounted rewards subject to a constraint B on the expected total discounted costs. We formulate this goal as the constrained optimization problem max \u03c0 Jp,r(\u03c0) s.t. Jp,c(\u03c0) \u2264 B, (1) where Jp,r(\u03c0) = E\u03c4\u223c(\u03c0,p) [\ufffd\u221e t=0 \u03b3tr(st, at)], Jp,c(\u03c0) = E\u03c4\u223c(\u03c0,p) [\ufffd\u221e t=0 \u03b3tc(st, at)], and \u03c4 \u223c (\u03c0, p) represents a trajectory sampled according to s0 \u223c d0, at \u223c \u03c0( \u00b7 | st), and st+1 \u223c p( \u00b7 | st, at). We write the corresponding stateaction value functions (i.e., Q functions) as Q\u03c0 p,r(s, a) and Q\u03c0 p,c(s, a), respectively. 2.2. Model Uncertainty in Reinforcement Learning Rather than focusing on a single CMDP with transition model p, we incorporate uncertainty about the transition model by considering a distribution \u00b5 over models. We focus on distributions of the form \u00b5 = \ufffd (s,a)\u2208S\u00d7A \u00b5s,a, where \u00b5s,a represents a distribution over transition models ps,a = p( \u00b7 | s, a) \u2208 P(S) at a given state-action pair and \u00b5 is the product over all \u00b5s,a. This is known as rectangularity, and is a common assumption in the literature (Xu & Mannor, 2010; Yu & Xu, 2016; Derman et al., 2018; Derman & Mannor, 2020). Note that \u00b5s,a \u2208 P(M), where we write M = P(S). 2.3. Risk Measures Consider a random variable Z \u2208 Z, where Z is a space of random variables de\ufb01ned on a given probability space. A real-valued risk measure \u03c1 : Z \u2192 R summarizes a random variable as a value on the real line. In this section, we consider cost random variables where a lower value of \u03c1(Z) is better. We can de\ufb01ne a corresponding risk measure \u03c1+ for reward random variables through an appropriate change in sign, where \u03c1+(Z) = \u2212\u03c1(\u2212Z). Recently, Majumdar & Pavone (2020) proposed a set of six axioms to characterize desirable properties of risk measures in the context of robotics. A1. Monotonicity: If Z, Z\u2032 \u2208 Z and Z \u2264 Z\u2032 almost everywhere, then \u03c1(Z) \u2264 \u03c1(Z\u2032). A2. Translation invariance: If \u03b1 \u2208 R and Z \u2208 Z, then \u03c1(Z + \u03b1) = \u03c1(Z) + \u03b1. A3. Positive homogeneity: If \u03c4 \u2265 0 and Z \u2208 Z, then \u03c1(\u03c4Z) = \u03c4\u03c1(Z). A4. Convexity: If \u03bb \u2208 [0, 1] and Z, Z\u2032 \u2208 Z, then \u03c1(\u03bbZ + (1 \u2212 \u03bb)Z\u2032) \u2264 \u03bb\u03c1(Z) + (1 \u2212 \u03bb)\u03c1(Z\u2032). A5. Comonotonic additivity: If Z, Z\u2032 \u2208 Z are comonotonic, then \u03c1(Z + Z\u2032) = \u03c1(Z) + \u03c1(Z\u2032). A6. Law invariance: If Z, Z\u2032 \u2208 Z are identically distributed, then \u03c1(Z) = \u03c1(Z\u2032). See Majumdar & Pavone (2020) for a discussion on the intuition behind these axioms. Risk-sensitive methods typically focus on classes of risk measures that satisfy some or all of these axioms. Two popular choices include coherent risk measures (Artzner et al., 1999) and distortion risk measures (Wang, 1996; Dhaene et al., 2012). De\ufb01nition 2.1 (Coherent Risk Measure). A risk measure \u03c1 is a coherent risk measure if it satis\ufb01es Axioms A1\u2013A4. ",
    "Related Work": "Related Work 3.1. Safe Reinforcement Learning The CMDP framework is the most popular approach to safety in RL, and several deep RL algorithms have been developed to solve the constrained optimization problem in (1). These include primal-dual methods that consider the Lagrangian relaxation of (1) (Ray et al., 2019; Tessler et al., 2019; Stooke et al., 2020), algorithms that compute closedform solutions to related or approximate versions of (1) (Achiam et al., 2017; Liu et al., 2022), and direct methods for constraint satisfaction such as the use of barriers (Cheng et al., 2019; Liu et al., 2020), safety layers (Dalal et al., 2018), and immediate switching between the objective and constraint (Xu et al., 2021). All of these approaches are designed to satisfy expected cost constraints for a single CMDP observed during training. In our work, on the other hand, we consider a distribution over possible transition models. 3.2. Uncertainty in Reinforcement Learning There are multiple sources of uncertainty in RL that have received attention: (i) model or transfer uncertainty, (ii) epistemic uncertainty from sample-based estimation, and (iii) aleatoric uncertainty due to the stochasticity of the environment. We focus on model uncertainty in this work. The most popular approach to model uncertainty is domain randomization (Tobin et al., 2017; Peng et al., 2018), which is based on the goal of sim-to-real transfer. Domain randomization considers parametric uncertainty by randomizing over parameters in a simulator, and trains a policy to maximize average performance over this training distribution. Parametric uncertainty is the most common method for implementing model uncertainty in RL, and has been combined with both risk-neutral (Derman et al., 2018) and robust objectives (Rajeswaran et al., 2017; Mankowitz et al., 2020; 2021). Compared to these approaches, we propose an implementation of model uncertainty that does not require access to a range of simulated training environments. Epistemic uncertainty has been considered in the estimation of Q functions (Osband et al., 2016; 2018; Bharadhwaj et al., 2021) and learned transition models (Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020; As et al., 2022), and has been applied to promote both exploration and safety. Compared to model uncertainty, epistemic uncertainty can be reduced during training through data collection. Finally, risk-sensitive methods typically focus on the aleatoric uncertainty in RL. Rather than considering the standard expected value objective, they learn risk-sensitive policies over the distribution of possible outcomes in a \ufb01xed MDP (Shen et al., 2014; Chow et al., 2015; Tamar et al., 2015; Keramati et al., 2020; L.A. & Fu, 2022). Distributional RL (Bellemare et al., 2017) trains critics that estimate the full distribution of future returns due to aleatoric uncertainty, and risk measures can be applied to these distributional critics for risk-sensitive learning (Dabney et al., 2018; Ma et al., 2020). We also consider the use of risk measures in our work, but different from standard risk-sensitive RL methods we apply a risk measure over model uncertainty instead of aleatoric uncertainty. 4. Risk-Averse Model Uncertainty for Safe Reinforcement Learning In the standard safe RL setting with \ufb01xed transition model p, policy optimization techniques (Xu et al., 2021; Liu et al., 2022) \ufb01nd a policy that maximizes (1) by solving at each iteration the related optimization problem max \u03c0 E s\u223cD \ufffd E a\u223c\u03c0(\u00b7|s) \ufffd Q\u03c0k p,r(s, a) \ufffd\ufffd s.t. E s\u223cD \ufffd E a\u223c\u03c0(\u00b7|s) \ufffd Q\u03c0k p,c(s, a) \ufffd\ufffd \u2264 B, (2) where \u03c0k represents the current policy and D represents data previously collected in the training environment. Note that the cost Q function Q\u03c0 p,c(s, a) can be written recursively as Q\u03c0 p,c(s, a) = c(s, a) + \u03b3 E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) \ufffd Q\u03c0 p,c(s\u2032, a\u2032) \ufffd\ufffd , Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning and is the \ufb01xed point of the corresponding Bellman operator T \u03c0 p,cQ(s, a) := c(s, a) + \u03b3 E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd . Similarly, the reward Q function Q\u03c0 p,r(s, a) is the \ufb01xed point of the Bellman operator T \u03c0 p,r. In this work, however, we are interested in a distribution of possible transition models sampled from \u00b5 rather than a \ufb01xed transition model. The distribution \u00b5 provides a natural way to capture our uncertainty about the unknown transition model at deployment time. Next, we must incorporate this model uncertainty in the de\ufb01nition of our Q functions. Prior methods (Derman et al., 2018) have done this by applying the risk-neutral expectation operator over \u00b5s,a at every transition. Due to the linearity of the expectation operator, this is equivalent to considering a \ufb01xed CMDP with the mixture transition model \u00afps,a = Eps,a\u223c\u00b5s,a [ps,a] at every state-action pair. Instead, we adopt a risk-averse view towards model uncertainty in order to learn robust and safe policies. We accomplish this by applying a coherent distortion risk measure \u03c1 with respect to model uncertainty at every transition. For a given policy \u03c0, this results in the riskaverse model uncertainty (RAMU) cost Q function de\ufb01ned as Q\u03c0 \u03c1,c(s, a) := c(s, a) + \u03b3 \u03c1 ps,a\u223c\u00b5s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) \ufffd c(s\u2032, a\u2032) + \u03b3 \u03c1 ps\u2032,a\u2032\u223c\u00b5s\u2032,a\u2032 (. . .) \ufffd\ufffd\ufffd , where the notation \u03c1 ps,a\u223c\u00b5s,a ( \u00b7 ) makes clear the fact that the stochasticity of the random variable is with respect to the transition model sampled from \u00b5s,a. We can de\ufb01ne the RAMU reward Q function Q\u03c0 \u03c1+,r(s, a) similarly, with the use of \u03c1+ to account for reward random variables. It is important to note that we still apply expectations over the aleatoric uncertainty of the CMDP (i.e., the randomness associated with a stochastic transition model and stochastic policy), while being risk-averse with respect to model uncertainty. Therefore, unlike standard risk-sensitive RL methods that consider risk measures over aleatoric uncertainty, we recover the standard RL framework as model uncertainty disappears. Also note that it would not be possible to separate aleatoric and model uncertainty in this way with a distributional critic (Bellemare et al., 2017), which is a popular design choice in risk-sensitive RL. By compounding coherent distortion risk measures at every timestep, we have that Q\u03c0 \u03c1+,r(s, a) and Q\u03c0 \u03c1,c(s, a) are timeconsistent, dynamic risk measures (Ruszczy\u00b4nski, 2010). We can write these RAMU Q functions recursively, where Q\u03c0 \u03c1,c(s, a) = c(s, a) + \u03b3 \u03c1 ps,a\u223c\u00b5s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) \ufffd Q\u03c0 \u03c1,c(s\u2032, a\u2032) \ufffd\ufffd\ufffd , and Q\u03c0 \u03c1+,r(s, a) can be written recursively in a similar fashion. These recursive de\ufb01nitions motivate corresponding RAMU Bellman operators. De\ufb01nition 4.1. For a given policy \u03c0, the RAMU cost Bellman operator is de\ufb01ned as T \u03c0 \u03c1,cQ(s, a) := c(s, a) + \u03b3 \u03c1 ps,a\u223c\u00b5s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd , and the RAMU reward Bellman operator T \u03c0 \u03c1+,r is de\ufb01ned similarly. Note that the RAMU Bellman operators can also be interpreted as applying a coherent distortion risk measure over standard Bellman values, which are random variables with respect to the transition model ps,a \u223c \u00b5s,a for a given stateaction pair. Lemma 4.2. The RAMU Bellman operators can be written in terms of standard Bellman operators as T \u03c0 \u03c1+,rQ(s, a) = \u03c1+ ps,a\u223c\u00b5s,a \ufffd T \u03c0 p,rQ(s, a) \ufffd , (3) T \u03c0 \u03c1,cQ(s, a) = \u03c1 ps,a\u223c\u00b5s,a \ufffd T \u03c0 p,cQ(s, a) \ufffd . (4) Proof. The results follow by using the de\ufb01nitions of T \u03c0 p,r and T \u03c0 p,c, along with the translation invariance (Axiom A2) and positive homogeneity (Axiom A3) of coherent distortion risk measures. See the Appendix for details. In the next section, we show that T \u03c0 \u03c1+,r and T \u03c0 \u03c1,c are contraction operators, so we can apply standard temporal difference learning techniques to learn the RAMU Q functions Q\u03c0 \u03c1+,r(s, a) and Q\u03c0 \u03c1,c(s, a). Then, by replacing the standard Q functions in (2) with RAMU Q functions, we can learn a safe policy that is risk-averse to model uncertainty by iteratively optimizing max \u03c0 E s\u223cD \ufffd E a\u223c\u03c0(\u00b7|s) \ufffd Q\u03c0k \u03c1+,r(s, a) \ufffd\ufffd s.t. E s\u223cD \ufffd E a\u223c\u03c0(\u00b7|s) \ufffd Q\u03c0k \u03c1,c(s, a) \ufffd\ufffd \u2264 B. (5) Note that we can use different risk measures for the RAMU reward and cost Q functions, including the risk-neutral expectation operator as a special case. Therefore, if we only Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning care about the potential negative impacts of model uncertainty with respect to safety, we can choose to be risk-averse in the safety constraint while remaining risk-neutral in the objective. 5. Robustness Guarantees Intuitively, our risk-averse perspective places more emphasis on potential transition models that result in higher costs or lower rewards under the current policy, which should result in learning safe policies that are robust to model uncertainty. Next, we formalize the robustness guarantees of our RAMU framework by showing that it is equivalent to a distributionally robust safe RL problem for appropriate choices of uncertainty sets. Theorem 5.1. The RAMU Bellman operators T \u03c0 \u03c1+,r and T \u03c0 \u03c1,c are equivalent to distributionally robust Bellman operators with respective uncertainty sets U+ = \ufffd (s,a)\u2208S\u00d7A U+ s,a and U = \ufffd (s,a)\u2208S\u00d7A Us,a, where U+ s,a, Us,a \u2286 {\u03b2s,a \u2208 P(M) | \u03b2s,a = \u03bes,a\u00b5s,a} are the sets of feasible reweightings of \u00b5s,a with \u03bes,a, which depend on the choice of \u03c1+ and \u03c1, respectively. Proof. For a given state-action pair, we can rewrite the risk measures that appear in T \u03c0 \u03c1+,rQ(s, a) and T \u03c0 \u03c1,cQ(s, a) by applying duality results for coherent risk measures (Shapiro et al., 2014). By doing so, we see that the application of \u03c1+ and \u03c1 are equivalent to solving distributionally robust optimization problems over the uncertainty sets of distributions U+ s,a and Us,a, respectively, which can be interpreted as adversarially reweighting \u00b5s,a with \u03bes,a. See the Appendix for details. Because T \u03c0 \u03c1+,r and T \u03c0 \u03c1,c are equivalent to distributionally robust Bellman operators according to Theorem 5.1, they are also equivalent to robust Bellman operators (Xu & Mannor, 2010; Yu & Xu, 2016). Corollary 5.2. The RAMU Bellman operators T \u03c0 \u03c1+,r and T \u03c0 \u03c1,c are equivalent to robust Bellman operators for appropriate choices of uncertainty sets over M = P(S). In addition, given the equivalence established in Theorem 5.1, we can leverage previous contraction results for distributionally robust Bellman operators (Xu & Mannor, 2010; Yu & Xu, 2016) to show that T \u03c0 \u03c1+,r and T \u03c0 \u03c1,c are contraction operators. Corollary 5.3. The RAMU Bellman operators T \u03c0 \u03c1+,r and T \u03c0 \u03c1,c are \u03b3-contractions in the sup-norm. See the Appendix for details on Corollary 5.2 and Corollary 5.3. Therefore, we have that Q\u03c0 \u03c1+,r(s, a) and Q\u03c0 \u03c1,c(s, a) can be interpreted as distributionally robust Q functions by Theorem 5.1 (or robust Q functions by Corollary 5.2), and we can apply standard temporal difference methods to learn these RAMU Q functions as a result of Corollary 5.3. In particular, we see from Theorem 5.1 that the use of a coherent distortion risk measure is equivalent to selecting an adversarial distribution over transition models by reweighting \u00b5s,a with \u03bes,a at every state-action pair. Therefore, our risk-averse perspective results in a distributionally robust safe RL framework that provides guarantees on robust and safe performance in the presence of model uncertainty. Importantly, we achieve these robustness guarantees without having to actually compute the adversarial distributions over transition models in U+ s,a and Us,a, which would require solving an inner optimization problem over distributions at every transition. As a result, our framework guarantees robust and safe performance, while also leading to an ef\ufb01cient implementation that is compatible with deep RL as we describe in the following section. 6. Model-Free Implementation with a Single Training Environment The risk-averse policy update in (5) takes the same form as the standard safe RL update in (2), except for the use of Q\u03c0 \u03c1+,r(s, a) and Q\u03c0 \u03c1,c(s, a). Because our RAMU Bellman operators are contractions, we can learn these RAMU Q functions by applying standard temporal difference loss functions that are used throughout deep RL. In particular, we consider parametric critics Q\u03b8r and Q\u03b8c, and we optimize their parameters during training to minimize the loss functions L(\u03b8r) = E (s,a)\u223cD \ufffd\ufffd Q\u03b8r(s, a) \u2212 \u02c6T \u03c0 \u03c1+,r \u00afQ\u03b8r(s, a) \ufffd2\ufffd , L(\u03b8c) = E (s,a)\u223cD \ufffd\ufffd Q\u03b8c(s, a) \u2212 \u02c6T \u03c0 \u03c1,c \u00afQ\u03b8c(s, a) \ufffd2\ufffd , where \u02c6T \u03c0 \u03c1+,r and \u02c6T \u03c0 \u03c1,c represent sample-based estimates of the RAMU Bellman operators applied to target Q functions denoted by \u00afQ. Therefore, we must be able to ef\ufb01ciently estimate the RAMU Bellman targets, which involve calculating coherent distortion risk measures that depend on the distribution \u00b5s,a. 6.1. Sample-Based Estimation of Risk Measures Using the formulation of our RAMU Bellman operators from Lemma 4.2, we can leverage properties of distortion risk measures to ef\ufb01ciently estimate (3) and (4) using sample-based weighted averages of standard Bellman values. For n transition models p(i), i = 1, . . . , n, sampled Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning w(1) w(2) w(3) w(4) w(5) 0.0 0.2 0.4 Weight Risk-Neutral Wang ( = 0.75) Figure 1. Weights for sample-based estimates in (6) and (7) when n = 5 and \u03c1 is the Wang transform with \u03b7 = 0.75. independently from \u00b5, consider the weights w(i) \u03c1 = g \ufffd i n \ufffd \u2212 g \ufffdi \u2212 1 n \ufffd , where g de\ufb01nes the distortion risk measure \u03c1 according to De\ufb01nition 2.2. See Figure 1 for an example using the Wang transform (Wang, 2000). Then, from Jones & Zitikis (2003) we have that n \ufffd i=1 w(i) \u03c1+T \u03c0 p(i),rQ(s, a), n \ufffd i=1 w(i) \u03c1 T \u03c0 p(i),cQ(s, a), are consistent estimators of (3) and (4), respectively, where T \u03c0 p(i),rQ(s, a) are sorted in ascending order and T \u03c0 p(i),cQ(s, a) are sorted in descending order. Finally, we can replace T \u03c0 p(i),cQ(s, a) with the standard unbiased sample-based estimate \u02c6T \u03c0 p(i),cQ(s, a) = c(s, a) + \u03b3Q(s\u2032, a\u2032), where s\u2032 \u223c p(i) s,a and a\u2032 \u223c \u03c0( \u00b7 | s\u2032). Similarly, we replace T \u03c0 p(i),rQ(s, a) with \u02c6T \u03c0 p(i),rQ(s, a). This leads to the samplebased estimates \u02c6T \u03c0 \u03c1+,rQ(s, a) = n \ufffd i=1 w(i) \u03c1+ \u02c6T \u03c0 p(i),rQ(s, a), (6) \u02c6T \u03c0 \u03c1,cQ(s, a) = n \ufffd i=1 w(i) \u03c1 \u02c6T \u03c0 p(i),cQ(s, a), (7) which we use to train our RAMU Q functions. Note that (6) and (7) can be computed very ef\ufb01ciently, which is a major bene\ufb01t of our RAMU framework compared to robust RL methods. In the next section, we describe how we can sample models p(i) s,a, i = 1, . . . , n, from \u00b5s,a, and generate the next state transitions from these models that are used in the calculation of our sample-based Bellman targets in (6) and (7). 6.2. Generative Distribution of Transition Models Note that our framework holds for any choice of distribution \u00b5 over transition models. Therefore, it is possible to de\ufb01ne \u00b5 in a way that incorporates domain knowledge and leverages detailed simulator access for sampling transition models p(i) \u223c \u00b5 and corresponding state transitions s\u2032 \u223c p(i) s,a. However, in order for our methods to be broadly applicable to any setting where data can be collected under a single training environment ptrain, we propose a generative approach to sampling transition models and corresponding state transitions by de\ufb01ning the distribution \u00b5 over perturbed versions of the training environment. In particular, consider a function fx : S \u00d7 S \u2192 S that takes as input a state transition (s, s\u2032) and outputs a perturbed next state \u02dcs\u2032 = fx(s, s\u2032), where fx depends on a latent variable x \u223c X. We also de\ufb01ne a transition model ps,a(x) for every x \u223c X that shifts the probability of s\u2032 under ptrain s,a to \u02dcs\u2032 = fx(s, s\u2032). Then, by considering a distribution over latent space, we implicitly de\ufb01ne a distribution \u00b5s,a over transition models ps,a(x). By de\ufb01ning \u00b5s,a in this way, it becomes easy to generate the next state samples needed to calculate the Bellman target estimates in (6) and (7). We sample a latent variable, which de\ufb01nes the transition model ps,a(x). Then, we can generate a state transition under this sampled model by simply perturbing the next state observed in the training environment according to fx. In particular, for data collected in the training environment we have that s\u2032 \u223c ptrain s,a , so \u02dcs\u2032 = fx(s, s\u2032) represents the corresponding sample from the transition model ps,a(x). In our experiments, we consider a simple implementation for the common case where S = Rd. We use uniformly distributed latent variables x \u223c U([\u22122\u03f5, 2\u03f5]d), and we de\ufb01ne the perturbation function as fx(s, s\u2032) = s + (s\u2032 \u2212 s)(1 + x), where all operations are performed per-coordinate. Therefore, the latent variable x \u223c U([\u22122\u03f5, 2\u03f5]d) can be interpreted as the percentage change in each dimension of a state transition observed in the training environment, where the average magnitude of the percentage change is \u03f5. The hyperparameter \u03f5 determines the distribution \u00b5s,a over transition models, where a larger value of \u03f5 leads to transition models that vary more signi\ufb01cantly from the training environment. The structure of fx provides an intuitive, scale-invariant meaning for the hyperparameter \u03f5, which makes it easy to tune in practice. 6.3. Algorithm We summarize the implementation of our RAMU framework in Algorithm 1. Given data collected in a single training environment, we can ef\ufb01ciently calculate the samplebased RAMU Bellman targets in (6) and (7) by (i) sampling from a latent variable x \u223c X, (ii) computing the corresponding next state samples fx(s, s\u2032), and (iii) sorting the ",
    "Experiments": "Experiments In order to evaluate the performance and safety of our RAMU framework, we conduct experiments on 5 continuous control tasks with safety constraints from the RealWorld RL Suite (Dulac-Arnold et al., 2020; 2021): Cartpole Swingup, Walker Walk, Walker Run, Quadruped Walk, and Quadruped Run. Each task has a horizon length of 1,000 with r(s, a) \u2208 [0, 1] and c(s, a) \u2208 {0, 1}, and we consider a safety budget of B = 100. Unless noted otherwise, we train these tasks on a single training environment for 1 million steps across 5 random seeds, and we evaluate performance of the learned policies across a range of perturbed test environments via 10 trajectory rollouts. See the Appendix for details on the safety constraints and environment perturbations that we consider. Our RAMU framework can be combined with several choices of safe RL algorithms. We consider the safe RL algorithm Constraint-Recti\ufb01ed Policy Optimization (CRPO) (Xu et al., 2021), which immediately switches between maximizing the objective and minimizing the safety constraint based on the current value of the constraint. We apply Maximum a Posteriori Policy Optimization (MPO) (Abdolmaleki et al., 2018) as the unconstrained policy optimization algoTable 1. Summary of performance across all tasks and environment perturbations. \u201c% Safe\u201d denotes percentage of policies that satisfy the safety constraint across all tasks and environment perturbations. Total rewards and costs are normalized relative to the average performance of CRPO for each task and environment perturbation. NORMALIZED AVE. ALGORITHM % SAFE REWARD COST CRPO 51% 1.00 1.00 RAMU (WANG 0.75) 80% 1.08 0.51 RAMU (EXPECTATION) 74% 1.05 0.67 DOMAIN RAND. (IN) 76% 1.14 0.72 DOMAIN RAND. (OUT) 55% 1.02 1.02 rithm used in CRPO. We consider a multivariate Gaussian policy with learned mean and diagonal covariance at each state, along with separate reward and cost critics. We parameterize our policy and critics using neural networks. See the Appendix for implementation details.1 Finally, we consider n = 5 samples of transition models with latent variable hyperparameter \u03f5 = 0.10 in order to calculate Bellman targets in our RAMU framework. 7.1. Comparison to Safe Reinforcement Learning First, we analyze the impact of our RAMU framework compared to standard safe RL. In both cases, we train policies using data collected from a single training environment, so the only difference comes from our use of risk-averse model uncertainty to learn RAMU Q functions. We apply our RAMU framework using the Wang transform with \u03b7 = 0.75 as the risk measure in both the objective and constraint, which uses the weights in Figure 1 to compute sample-based estimates. In order to understand the impact of being risk-averse to model uncertainty, we also consider the risk-neutral special case of our framework where expectations are applied to the objective and constraint. This special case is similar to existing methods such as domain randomization in the sense of using expectation over model uncertainty, but only uses data from a single training environment. The bene\ufb01ts of our RAMU approach are summarized in Figure 2 and Table 1. By evaluating the learned policies in perturbed test environments different from the training environment, we see that our framework provides robustness in terms of both total rewards and safety. In particular, the risk-averse implementation of our algorithm leads to safety constraint satisfaction in 80% of test environments, compared to only 51% with standard safe RL. In addition, this implementation results in higher total rewards (1.08x) and lower total costs (0.51x), on average. Note that the use 1Code is publicly available at https://github.com/ jqueeney/robust-safe-rl. ",
    "Conclusion": "Conclusion We have presented a framework for safe RL in the presence of model uncertainty, an important setting for many realworld decision making applications. Compared to existing approaches to model uncertainty in deep RL, our formulation applies a risk-averse perspective through the use of coherent distortion risk measures. We show that this results in robustness guarantees, while still leading to an ef\ufb01cient deep RL implementation. Importantly, our method only requires data collected from a single training environment, so it can be applied to real-world domains where high-\ufb01delity simulators are not readily available or are computationally expensive. Therefore, our framework represents an attractive approach to safe decision making under model uncertainty that can be deployed across a range of applications. ",
    "References": "References Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. Maximum a posteriori policy optimisation. In Sixth International Conference on Learning Representations, 2018. Abdolmaleki, A., Huang, S., Hasenclever, L., Neunert, M., Song, F., Zambelli, M., Martins, M., Heess, N., Hadsell, R., and Riedmiller, M. A distributional view on multiobjective policy optimization. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp. 11\u201322. PMLR, 2020. Achiam, J., Held, D., Tamar, A., and Abbeel, P. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 22\u201331. PMLR, 2017. Altman, E. Constrained Markov Decision Processes. CRC Press, 1999. Andrychowicz, M., Baker, B., Chociej, M., J\u00b4ozefowicz, R., McGrew, B., Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., Schneider, J., Sidor, S., Tobin, J., Welinder, P., Weng, L., and Zaremba, W. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020. doi: 10.1177/0278364919887447. Artzner, P., Delbaen, F., Eber, J.-M., and Heath, D. Coherent measures of risk. Mathematical Finance, 9(3):203\u2013228, 1999. doi: 10.1111/1467-9965.00068. As, Y., Usmanova, I., Curi, S., and Krause, A. Constrained policy optimization via Bayesian world models. In Tenth International Conference on Learning Representations, 2022. Bellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 449\u2013458. PMLR, 2017. Bharadhwaj, H., Kumar, A., Rhinehart, N., Levine, S., Shkurti, F., and Garg, A. Conservative safety critics for exploration. In Ninth International Conference on Learning Representations, 2021. Cao, W., Benosman, M., and Ma, R. Domain knowledgebased automated analog circuit design with deep reinforcement learning. In The 59th ACM/IEEE Design Automation Conference, 2022. Cheng, R., Orosz, G., Murray, R. M., and Burdick, J. W. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 33, pp. 3387\u20133395, 2019. Chow, Y., Tamar, A., Mannor, S., and Pavone, M. Risksensitive and robust decision-making: a CVaR optimization approach. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Dabney, W., Ostrovski, G., Silver, D., and Munos, R. Implicit quantile networks for distributional reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 1096\u20131105. PMLR, 2018. Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y. Safe exploration in continuous action spaces. arXiv preprint, 2018. arXiv:1801.08757. Derman, E. and Mannor, S. Distributional robustness and regularization in reinforcement learning. arXiv preprint, 2020. arXiv:2003.02894. Derman, E., Mankowitz, D. J., Mann, T. A., and Mannor, S. Soft-robust actor-critic policy-gradient. arXiv preprint, 2018. arXiv:1803.04848. Dhaene, J., Kukush, A., Linders, D., and Tang, Q. Remarks on quantiles and distortion risk measures. European Actuarial Journal, 2:319\u2013328, 2012. doi: 10.1007/ s13385-012-0058-0. Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru, C., Gowal, S., and Hester, T. An empirical investigation of the challenges of real-world reinforcement learning. arXiv preprint, 2020. arXiv:2003.11881. Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru, C., Gowal, S., and Hester, T. Challenges of realworld reinforcement learning: de\ufb01nitions, benchmarks and analysis. Machine Learning, 110:2419\u20132468, 2021. doi: 10.1007/s10994-021-05961-4. Hoffman, M. W., Shahriari, B., Aslanides, J., Barth-Maron, G., Momchev, N., Sinopalnikov, D., Sta\u00b4nczyk, P., Ramos, S., Raichuk, A., Vincent, D., Hussenot, L., Dadashi, R., Dulac-Arnold, G., Orsini, M., Jacq, A., Ferret, J., Vieillard, N., Ghasemipour, S. K. S., Girgin, S., Pietquin, O., Behbahani, F., Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., Henderson, S., Friesen, A., Haroun, R., Novikov, A., Colmenarejo, S. G., Cabi, S., Gulcehre, C., Paine, T. L., Srinivasan, S., Cowie, A., Wang, Z., Piot, B., and de Freitas, N. Acme: A research framework for distributed reinforcement learning. arXiv preprint, 2020. arXiv:2006.00979. Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning Iyengar, G. N. Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280, 2005. doi: 10. 1287/moor.1040.0129. Janner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Jones, B. L. and Zitikis, R. Empirical estimation of risk measures and related quantities. North American Actuarial Journal, 7(4):44\u201354, 2003. doi: 10.1080/10920277. 2003.10596117. Keramati, R., Dann, C., Tamkin, A., and Brunskill, E. Being optimistic to be conservative: Quickly learning a CVaR policy. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pp. 4436\u20134443, 2020. Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. Model-ensemble trust-region policy optimization. In Sixth International Conference on Learning Representations, 2018. L.A., P. and Fu, M. C. Risk-sensitive reinforcement learning via policy gradient search. Foundations and Trends\u00ae in Machine Learning, 15(5):537\u2013693, 2022. ISSN 19358237. doi: 10.1561/2200000091. Liu, Y., Ding, J., and Liu, X. IPO: Interior-point policy optimization under constraints. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pp. 4940\u20134947, 2020. Liu, Z., Cen, Z., Isenbaev, V., Liu, W., Wu, S., Li, B., and Zhao, D. Constrained variational policy optimization for safe reinforcement learning. In Proceedings of the 39th International Conference on Machine Learning, pp. 13644\u201313668. PMLR, 2022. Ma, X., Xia, L., Zhou, Z., Yang, J., and Zhao, Q. DSAC: Distributional soft actor critic for risk-sensitive reinforcement learning. arXiv preprint, 2020. arXiv:2004.14547. Majumdar, A. and Pavone, M. How should a robot assess risk? Towards an axiomatic theory of risk in robotics. In Robotics Research, pp. 75\u201384. Springer International Publishing, 2020. ISBN 978-3-030-28619-4. Mankowitz, D. J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg, J. T., Shi, Y., Kay, J., Hester, T., Mann, T., and Riedmiller, M. Robust reinforcement learning for continuous control with model misspeci\ufb01cation. In Eighth International Conference on Learning Representations, 2020. Mankowitz, D. J., Calian, D. A., Jeong, R., Paduraru, C., Heess, N., Dathathri, S., Riedmiller, M., and Mann, T. Robust constrained reinforcement learning for continuous control with model misspeci\ufb01cation. arXiv preprint, 2021. arXiv:2010.10644. Mowlavi, S., Benosman, M., and Nabi, S. Reinforcement learning state estimation for high-dimensional nonlinear systems. In Tenth International Conference on Learning Representations, 2022. Nilim, A. and Ghaoui, L. E. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780\u2013798, 2005. doi: 10.1287/ opre.1050.0216. Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. Osband, I., Aslanides, J., and Cassirer, A. Randomized prior functions for deep reinforcement learning. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 3803\u20133810, 2018. doi: 10.1109/ICRA.2018.8460528. Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S. EPOpt: Learning robust neural network policies using model ensembles. In 5th International Conference on Learning Representations, 2017. Rajeswaran, A., Mordatch, I., and Kumar, V. A game theoretic framework for model based reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp. 7953\u20137963. PMLR, 2020. Ray, A., Achiam, J., and Amodei, D. Benchmarking safe exploration in in deep reinforcement learning, 2019. Ruszczy\u00b4nski, A. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming, 125:235\u2013261, 2010. doi: 10.1007/s10107-010-0393-3. Shapiro, A., Dentcheva, D., and Ruszczy\u00b4nski, A. Lectures on Stochastic Programming: Modeling and Theory, Second Edition. Society for Industrial and Applied Mathematics, 2014. ISBN 1611973422. Shen, Y., Tobia, M. J., Sommer, T., and Obermayer, K. Risksensitive reinforcement learning. Neural Computation, 26(7):1298\u20131328, 2014. doi: 10.1162/NECO a 00600. Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by PID Lagrangian methods. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp. 9133\u20139143. PMLR, 2020. Tamar, A., Chow, Y., Ghavamzadeh, M., and Mannor, S. Policy gradient for coherent risk measures. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. Tessler, C., Mankowitz, D. J., and Mannor, S. Reward constrained policy optimization. In Seventh International Conference on Learning Representations, 2019. Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 23\u201330, 2017. doi: 10. 1109/IROS.2017.8202133. Wang, S. Premium calculation by transforming the layer premium density. ASTIN Bulletin, 26(1):71\u201392, 1996. doi: 10.2143/AST.26.1.563234. Wang, S. S. A class of distortion operators for pricing \ufb01nancial and insurance risks. The Journal of Risk and Insurance, 67(1):15\u201336, 2000. ISSN 00224367, 15396975. doi: 10.2307/253675. Wirch, J. L. and Hardy, M. R. Distortion risk measures: Coherence and stochastic dominance. Insurance Mathematics and Economics, 32:168\u2013168, 2003. Xu, H. and Mannor, S. Distributionally robust Markov decision processes. In Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010. Xu, J., Kim, S., Chen, T., Rodriguez, A., Agrawal, P., Matusik, W., and Sueda, S. Ef\ufb01cient tactile simulation with differentiability for robotic manipulation. In The Conference on Robot Learning (CoRL), 2022. Xu, T., Liang, Y., and Lan, G. CRPO: A new approach for safe reinforcement learning with convergence guarantee. In Proceedings of the 38th International Conference on Machine Learning, pp. 11480\u201311491. PMLR, 2021. Yu, P. and Xu, H. Distributionally robust counterpart in Markov decision processes. IEEE Transactions on Automatic Control, 61(9):2538\u20132543, 2016. doi: 10.1109/ TAC.2015.2495174. Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning A. Proofs In this section, we prove all results related to the RAMU cost Bellman operator T \u03c0 \u03c1,c. Using the fact that \u03c1+(Z) = \u2212\u03c1(\u2212Z) for a coherent distortion risk measure \u03c1 on a cost random variable, all results related to the RAMU reward Bellman operator follow by an appropriate change in sign. A.1. Proof of Lemma 4.2 Proof. Starting from the de\ufb01nition of T \u03c0 \u03c1,c in De\ufb01nition 4.1, we have that T \u03c0 \u03c1,cQ(s, a) = c(s, a) + \u03b3 \u03c1 ps,a\u223c\u00b5s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd = c(s, a) + \u03c1 ps,a\u223c\u00b5s,a \ufffd \u03b3 E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd = \u03c1 ps,a\u223c\u00b5s,a \ufffd c(s, a) + \u03b3 E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd = \u03c1 ps,a\u223c\u00b5s,a \ufffd T \u03c0 p,cQ(s, a) \ufffd , which proves the result in (4). Note that the second equality follows from the positive homogeneity of \u03c1 (Axiom A3), the third equality follows from the translation invariance of \u03c1 (Axiom A2), and the fourth equality follows from the de\ufb01nition of the standard Bellman cost operator T \u03c0 p,c. A.2. Proof of Theorem 5.1 Consider the probability space (M, F, \u00b5s,a), where F is a \u03c3-algebra on M and \u00b5s,a \u2208 P(M) de\ufb01nes a probability measure over M. Let Z be a space of random variables de\ufb01ned on this probability space, and let Z\u2217 be its corresponding dual space. In order to prove Theorem 5.1, we make use of the following dual representation of coherent risk measures applied to our probability space of interest. Lemma A.1 (Shapiro et al. 2014). Let \u03c1 be a proper, real-valued coherent risk measure. Then, for any Z \u2208 Z we have that \u03c1(Z) = sup \u03b2s,a\u2208Us,a E\u03b2s,a [Z] , where E\u03b2s,a [ \u00b7 ] represents expectation with respect to the probability measure \u03b2s,a \u2208 P(M), and Us,a \u2286 {\u03b2s,a \u2208 P(M) | \u03b2s,a = \u03bes,a\u00b5s,a, \u03bes,a \u2208 Z\u2217} is a convex, bounded, and weakly* closed set that depends on \u03c1. See Shapiro et al. (2014) for a general treatment of this result. Using Lemma A.1, we now prove Theorem 5.1 for the RAMU cost Bellman operator T \u03c0 \u03c1,c. Proof of Theorem 5.1. For a given state-action pair, we apply Lemma A.1 to the risk measure that appears in the de\ufb01nition of T \u03c0 \u03c1,c. By doing so, we have that T \u03c0 \u03c1,cQ(s, a) = c(s, a) + \u03b3 \u03c1 ps,a\u223c\u00b5s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd = c(s, a) + \u03b3 sup \u03b2s,a\u2208Us,a E ps,a\u223c\u03b2s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd , where Us,a is de\ufb01ned in Lemma A.1. Therefore, T \u03c0 \u03c1,c has the same form as a distributionally robust Bellman operator (Xu & Mannor, 2010; Yu & Xu, 2016) with the uncertainty set U = \ufffd (s,a)\u2208S\u00d7A Us,a. Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning A.3. Proof of Corollary 5.2 Given the equivalence to a distributionally robust Bellman operator as shown in Theorem 5.1, Corollary 5.2 follows from results in Xu & Mannor (2010) and Yu & Xu (2016). We include a proof for completeness. Proof. Due to the linearity of the expectation operator, for a given \u03b2s,a \u2208 Us,a we have that E ps,a\u223c\u03b2s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd = E s\u2032\u223c\u00afp\u03b2 s,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd , where \u00afp\u03b2 s,a = Eps,a\u223c\u03b2s,a [ps,a] \u2208 P(S) represents a mixture transition model determined by \u03b2s,a. Therefore, starting from the result in Theorem 5.1, we can write T \u03c0 \u03c1,cQ(s, a) = c(s, a) + \u03b3 sup \u03b2s,a\u2208Us,a E ps,a\u223c\u03b2s,a \ufffd E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd\ufffd = c(s, a) + \u03b3 sup \u00afp\u03b2 s,a\u2208Ps,a E s\u2032\u223c\u00afp\u03b2 s,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd , where Ps,a = \ufffd \u00afp\u03b2 s,a \u2208 P(S) | \u00afp\u03b2 s,a = E ps,a\u223c\u03b2s,a [ps,a] , \u03b2s,a \u2208 Us,a \ufffd . As a result, T \u03c0 \u03c1,c has the same form as a robust Bellman operator (Iyengar, 2005; Nilim & Ghaoui, 2005) with the uncertainty set P = \ufffd (s,a)\u2208S\u00d7A Ps,a. Similarly, the RAMU reward Bellman operator T \u03c0 \u03c1+,r has the same form as a robust Bellman operator with the uncertainty set P+ = \ufffd (s,a)\u2208S\u00d7A P+ s,a, where P+ s,a = \ufffd \u00afp\u03b2 s,a \u2208 P(S) | \u00afp\u03b2 s,a = E ps,a\u223c\u03b2s,a [ps,a] , \u03b2s,a \u2208 U+ s,a \ufffd . A.4. Proof of Corollary 5.3 Corollary 5.3 also follows from previous results on distributionally robust Bellman operators (Xu & Mannor, 2010; Yu & Xu, 2016) and robust Bellman operators (Iyengar, 2005; Nilim & Ghaoui, 2005) due to the equivalences shown in Theorem 5.1 and Corollary 5.2. Again, we include a proof for completeness. Proof. From Corollary 5.2, we can write T \u03c0 \u03c1,cQ(s, a) = c(s, a) + \u03b3 sup ps,a\u2208Ps,a E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) [Q(s\u2032, a\u2032)] \ufffd . Consider Q functions Q(1) and Q(2), and denote the sup-norm by \u2225Q(1) \u2212 Q(2)\u2225\u221e = sup (s,a)\u2208S\u00d7A \ufffd\ufffd\ufffdQ(1)(s, a) \u2212 Q(2)(s, a) \ufffd\ufffd\ufffd . Fix \u03f5 > 0 and consider (s, a) \u2208 S \u00d7 A. Then, there exists p(1) s,a \u2208 Ps,a such that E s\u2032\u223cp(1) s,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) \ufffd Q(1)(s\u2032, a\u2032) \ufffd\ufffd \u2265 sup ps,a\u2208Ps,a E s\u2032\u223cps,a \ufffd E a\u2032\u223c\u03c0(\u00b7|s\u2032) \ufffd Q(1)(s\u2032, a\u2032) \ufffd\ufffd \u2212 \u03f5. ",
    "Experimental Results": "Experimental ",
    "Results": "Results B.1. Safety Constraints and Environment Perturbations In all of our experiments, we consider the problem of optimizing a task objective while satisfying a safety constraint. We focus on a single safety constraint corresponding to a cost function de\ufb01ned in the RWRL Suite for each task, and we consider a safety budget of B = 100. The safety constraints used for each task are described in Table 2. In the Cartpole domain, costs Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning Table 3. Perturbation ranges for test environments across domains. PERTURBATION NOMINAL DOMAIN PARAMETER VALUE TEST RANGE CARTPOLE POLE LENGTH 1.00 [0.75, 1.25] WALKER TORSO LENGTH 0.30 [0.10, 0.50] QUADRUPED TORSO DENSITY 1,000 [500, 1,500] Table 4. Perturbation parameters and ranges for domain randomization across domains. IN-DISTRIBUTION OUT-OF-DISTRIBUTION PERTURBATION NOMINAL TRAINING PERTURBATION NOMINAL TRAINING DOMAIN PARAMETER VALUE RANGE PARAMETER VALUE RANGE CARTPOLE POLE LENGTH 1.00 [0.875, 1.125] POLE MASS 0.10 [0.05, 0.15] WALKER TORSO LENGTH 0.30 [0.20, 0.40] CONTACT FRICTION 0.70 [0.40, 1.00] QUADRUPED TORSO DENSITY 1,000 [750, 1,250] CONTACT FRICTION 1.50 [1.00, 2.00] are applied when the slider is outside of a speci\ufb01ed range. In the Walker domain, costs are applied for large joint velocities. In the Quadruped domain, costs are applied for large joint angles. See Dulac-Arnold et al. (2021) for detailed de\ufb01nitions of each safety constraint. The de\ufb01nitions of these cost functions depend on a safety coef\ufb01cient in [0, 1], which determines the range of outcomes that lead to constraint violations and therefore controls how dif\ufb01cult it will be to satisfy safety constraints corresponding to these cost functions. As the safety coef\ufb01cient decreases, the range of safe outcomes also decreases and the safety constraint becomes more dif\ufb01cult to satisfy. In order to consider safe RL tasks with dif\ufb01cult safety constraints where strong performance is still possible, we selected the value of this safety constraint in the range of [0.15, 0.20, 0.25, 0.30] for each task based on the performance of the baseline safe RL algorithm CRPO compared to the unconstrained algorithm MPO. Figure 4 shows total rewards throughout training for each task across this range of safety coef\ufb01cients. We selected the most dif\ufb01cult cost de\ufb01nition in this range (i.e., lowest safety coef\ufb01cient value) where CRPO is still able to achieve the same total rewards as MPO (or the value that leads to the smallest gap between the two in the case of Walker Run and Quadruped Run). The resulting safety coef\ufb01cients used for our experiments are listed in Table 2. In order to evaluate the robustness of our learned policies, we generate a range of test environments for each task based on perturbing a simulator parameter in the RWRL Suite. See Table 3 for the perturbation parameters and corresponding ranges considered in our experiments. The test range for each domain is centered around the nominal parameter value that de\ufb01nes the training environment used in CRPO and our RAMU framework. B.2. Domain Randomization Domain randomization requires a training distribution over a range of environments, which is typically de\ufb01ned by considering a range of simulator parameters. For the in-distribution version of domain randomization considered in our experiments, we apply a uniform distribution over a subset of the test environments de\ufb01ned in Table 3. In particular, we consider the middle 50% of test environment parameter values centered around the nominal environment value for training. In the out-of-distribution version of domain randomization, on the other hand, we consider a different perturbation parameter from the one varied at test time. We apply a uniform distribution over a range of values for this alternate parameter centered around the value in the nominal environment. Therefore, the only environment shared between the set of test environments and the set of training environments used for out-of-distribution domain randomization is the nominal environment. See Table 4 for details on the parameters and corresponding ranges used for training in domain randomization. We include the results for domain randomization across all tasks in Figure 5. Across all tasks, we observe that our RAMU framework leads to similar or improved constraint satisfaction compared to in-distribution domain randomization, while only using one training environment. In addition, our framework consistently outperforms out-of-distribution domain randomization, which provides little bene\ufb01t compared to standard safe RL due to its misspeci\ufb01ed training distribution. Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning 700 800 Total Reward Cartpole Swingup 400 600 800 1000 Walker Walk 200 400 600 Walker Run 600 800 1000 Quadruped Walk 800 850 Quadruped Run 0.8 0.9 1.0 1.1 1.2 Pole Length 0 100 200 Total Cost 0.1 0.2 0.3 0.4 0.5 Torso Length 0 100 200 0.1 0.2 0.3 0.4 0.5 Torso Length 0 100 200 CRPO RAMU (Wang 0.75) DR (In) DR (Out) 600 800 1000 1200 1400 Torso Density 0 100 200 600 800 1000 1200 1400 Torso Density 0 100 200 Figure 5. Comparison with domain randomization across all tasks over a range of environment perturbations. Grey shaded area denotes the training range for DR (In). Shading denotes half of one standard error across policies. Vertical dotted lines represent nominal training environment. Top: Total reward. Bottom: Total cost, where horizontal dotted lines represent safety budget. B.3. Network Architectures and Algorithm Hyperparameters In our experiments, we consider neural network representations of the policy and critics. Each of these neural networks contains 3 hidden layers of 256 units with ELU activations. In addition, we apply layer normalization followed by a tanh activation after the \ufb01rst layer of these networks as proposed in Abdolmaleki et al. (2020). We consider a multivariate Gaussian policy, where at a given state we have \u03c0(a | s) = N(\u00b5(s), \u03a3(s)) where \u00b5(s) and \u03a3(s) represent outputs of the policy network. \u03a3(s) is a diagonal covariance matrix, whose diagonal elements are calculated by applying the softplus operator to the outputs of the neural network. We parameterize the reward and cost critics with separate neural networks. In addition, we consider target networks that are updated as an exponential moving average with parameter \u03c4 = 5e\u22123. We consider CRPO (Xu et al., 2021) as the baseline safe RL algorithm in all of our experiments, which immediately switches between maximizing rewards and minimizing costs at every update based on the value of the safety constraint. If the sample-average estimate of the safety constraint for the current batch of data satis\ufb01es the safety budget, we update the policy to maximize rewards. Otherwise, we update the policy to minimize costs. After CRPO determines the appropriate objective for the current batch of data, we apply MPO (Abdolmaleki et al., 2018) to calculate policy updates. MPO calculates a non-parametric policy update based on the KL divergence parameter \u03f5KL, and then takes a step towards this non-parametric policy while constraining the KL divergence from updating the mean by \u03b2\u00b5 and the KL divergence from updating the covariance matrix by \u03b2\u03a3. We consider per-dimension KL divergence constraints by dividing these parameter values by the number of action dimensions, and we penalize actions outside of the feasible action limits using the multi-objective MPO framework (Abdolmaleki et al., 2020) as suggested in Hoffman et al. (2020). In order to avoid potential issues related to the immediate switching between reward and cost objectives throughout training, we completely solve for the temperature parameter of the non-parametric target policy in MPO at every update as done in Liu et al. (2022). See Table 5 for the default hyperparameter values used in our experiments, which are based on default values considered in Hoffman et al. (2020). For our RAMU framework, the latent variable hyperparameter \u03f5 controls the de\ufb01nition of the distribution \u00b5s,a over transition models. Figure 6 shows the performance of our RAMU framework in Walker Run and Quadruped Run for \u03f5 \u2208 [0.05, 0.10, 0.15, 0.20]. A larger value of \u03f5 leads to a distribution over a wider range of transition models, which results in a more robust approach when combined with a risk-averse perspective on model uncertainty. We see in Figure 6 that our algorithm more robustly satis\ufb01es safety constraints as \u03f5 increases, but this robustness also leads to a decrease in total rewards. We consider \u03f5 = 0.10 in our experiments, as it achieves strong constraint satisfaction without a meaningful decrease in rewards. Finally, for computational ef\ufb01ciency we consider n = 5 samples of transition models per data point to calculate sample-based Bellman targets in our RAMU framework, as we did not observe meaningful improvements in performance from considering a larger number of samples. Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning Table 5. Network architectures and algorithm hyperparameters used in experiments. GENERAL BATCH SIZE PER UPDATE 256 UPDATES PER ENVIRONMENT STEP 1 DISCOUNT RATE (\u03b3) 0.99 TARGET NETWORK EXPONENTIAL MOVING AVERAGE (\u03c4) 5E-3 POLICY LAYER SIZES 256, 256, 256 LAYER ACTIVATIONS ELU LAYER NORM + TANH ON FIRST LAYER YES INITIAL STANDARD DEVIATION 0.3 LEARNING RATE 1E-4 NON-PARAMETRIC KL (\u03f5KL) 0.10 ACTION PENALTY KL 1E-3 ACTION SAMPLES PER UPDATE 20 PARAMETRIC MEAN KL (\u03b2\u00b5) 0.01 PARAMETRIC COVARIANCE KL (\u03b2\u03a3) 1E-5 PARAMETRIC KL DUAL LEARNING RATE 0.01 CRITICS LAYER SIZES 256, 256, 256 LAYER ACTIVATIONS ELU LAYER NORM + TANH ON FIRST LAYER YES LEARNING RATE 1E-4 RAMU TRANSITION MODEL SAMPLES PER DATA POINT (n) 5 LATENT VARIABLE HYPERPARAMETER (\u03f5) 0.10 200 400 600 Total Reward Walker Run 750 800 850 Quadruped Run 0.1 0.2 0.3 0.4 0.5 Torso Length 0 100 200 Total Cost 600 800 1000 1200 1400 Torso Density 0 100 200 CRPO RAMU ( = 0.05) RAMU ( = 0.10) RAMU ( = 0.15) RAMU ( = 0.20) Figure 6. Hyperparameter sweep of latent variable hyperparameter \u03f5 on Walker Run and Quadruped Run. RAMU algorithms use the Wang transform with \u03b7 = 0.75 applied to both the objective and constraint. Shading denotes half of one standard error across policies. Vertical dotted lines represent nominal training environment. Top: Total reward. Bottom: Total cost, where horizontal dotted lines represent safety budget. ",
    "title": "Risk-Averse Model Uncertainty for",
    "paper_info": "Risk-Averse Model Uncertainty for\nDistributionally Robust Safe Reinforcement Learning\nJames Queeney 1 Mouhacine Benosman 2\nAbstract\nMany real-world domains require safe decision\nmaking in the presence of uncertainty. In this\nwork, we propose a deep reinforcement learning\nframework for approaching this important prob-\nlem. We consider a risk-averse perspective to-\nwards model uncertainty through the use of co-\nherent distortion risk measures, and we show that\nour formulation is equivalent to a distributionally\nrobust safe reinforcement learning problem with\nrobustness guarantees on performance and safety.\nWe propose an ef\ufb01cient implementation that only\nrequires access to a single training environment,\nand we demonstrate that our framework produces\nrobust, safe performance on a variety of contin-\nuous control tasks with safety constraints in the\nReal-World Reinforcement Learning Suite.\n1. Introduction\nIn many real-world decision making applications, it is im-\nportant to satisfy safety requirements while achieving a\ndesired goal. In addition, real-world environments often in-\nvolve uncertain or changing conditions. Therefore, in order\nto reliably deploy data-driven decision making methods in\nthese settings, they must produce safe performance even in\nthe presence of uncertainty.\nDeep reinforcement learning (RL) represents a powerful tool\nfor data-driven sequential decision making in complex envi-\nronments. Motivated by the need for safety in real-world de-\ncision making, several algorithms have incorporated safety\nconstraints into the deep RL framework. However, these\nsafe RL algorithms typically focus on satisfying safety re-\nquirements in a single training environment, and do not\nconsider the issue of environment (i.e., model) uncertainty.\n1Division of Systems Engineering, Boston University, Boston,\nMA, USA (work partly done during an internship at MERL)\n2Mitsubishi Electric Research Laboratories, Cambridge, MA,\nUSA. Correspondence to: James Queeney <jqueeney@bu.edu>,\nMouhacine Benosman <benosman@merl.com>.\nPreprint.\nIn order to account for model uncertainty in the deep RL\ntraining process, a popular approach is to consider a distribu-\ntion of training environments instead of training on a single\nenvironment. Domain randomization (Peng et al., 2018),\nfor example, collects training data from a variety of environ-\nments by altering important simulator parameters such as\nmass or friction in robotics applications. By interacting with\nmany different environments during training and applying\ndomain knowledge to de\ufb01ne the training distribution in a\nway that captures key sources of uncertainty, domain ran-\ndomization has achieved empirical success across a range\nof complex tasks (Andrychowicz et al., 2020).\nUnfortunately, high-\ufb01delity simulators are not readily avail-\nable in many real-world applications, such as tactile sensing\nin robotics (Xu et al., 2022). In other applications, such as\n\ufb02uid dynamics (Mowlavi et al., 2022) and circuit design\n(Cao et al., 2022), the computation required for simulation\ncan make data collection across many environments imprac-\ntical. In these cases, we require methods that can incorporate\nmodel uncertainty while only leveraging data from a sin-\ngle training environment. This training environment may\nrepresent a simulator with a \ufb01xed set of parameters, or data\ncollection in a controlled real-world setting.\nIn the context of safe decision making, it is not only impor-\ntant to account for model uncertainty in the training process,\nbut also to learn policies that are robust to this uncertainty.\nExisting methods such as domain randomization typically\noptimize for average performance over a distribution of\nenvironments, which results in a risk-neutral approach to\nuncertainty that works well in practice but lacks robustness\nguarantees. Robust RL methods address this issue by focus-\ning on worst-case environments in an uncertainty set, but\nas a result require solving dif\ufb01cult minimax optimization\nproblems throughout training that can lead to instability and\noverly-conservative behavior.\nIn this work, we propose a general approach to safe RL in\nthe presence of model uncertainty that addresses the main\nshortcomings of existing methods. In particular, we apply a\nrisk-averse perspective towards model uncertainty, which\nleads to a framework for learning robust and safe policies\nthat can be ef\ufb01ciently implemented using only data collected\nfrom a single training environment. By doing so, we guar-\narXiv:2301.12593v1  [cs.LG]  30 Jan 2023\n",
    "GPTsummary": "- (1): The paper addresses the problem of safe decision making in the presence of model uncertainty in real-world domains by reformulating the RL problem from a risk-averse perspective.\n \n- (2): Existing RL methods typically focus on satisfying safety requirements in a single training environment and do not account for environment uncertainty. Methods that do address uncertainty, such as domain randomization and robust RL, have limitations. This paper proposes a risk-averse approach that produces robust and safe performance using data collected from a single training environment, without requiring prior domain knowledge or detailed simulator access.\n \n- (3): The proposed methodology is based on incorporating uncertainty about the transition model by considering a distribution of training environments instead of training on a single environment. The paper introduces coherent distortion risk measures and corresponding Bellman operators to reformulate the safe RL problem from a risk-averse perspective. An efficient deep RL implementation is proposed to avoid the difficult minimax formulation present in robust RL methods.\n  \n- (4): The framework is demonstrated through experiments on continuous control tasks with safety constraints in the Real-World RL Suite, producing robust, safe performance. The proposed approach addresses the issue of model uncertainty in RL and provides robustness guarantees on safety and performance.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in addressing the problem of safe decision making in the presence of model uncertainty in real-world domains, a crucial issue in the field of reinforcement learning. The proposed methodology uses a risk-averse approach, producing robust and safe performance using data collected from a single training environment, without requiring prior domain knowledge or detailed simulator access.\n                        \n- (2): Innovation point: The paper innovatively introduces coherent distortion risk measures and corresponding Bellman operators to reformulate the safe RL problem from a risk-averse perspective, incorporating uncertainty about the transition model by considering a distribution of training environments instead of training on a single environment. \nPerformance: The proposed approach is demonstrated through experiments on continuous control tasks with safety constraints in the Real-World RL Suite, producing robust and safe performance. \nWorkload: The paper proposes an efficient deep RL implementation to avoid the difficult minimax formulation present in robust RL methods, resulting in a promising approach to safe decision-making under model uncertainty that can be deployed across a range of applications.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in addressing the problem of safe decision making in the presence of model uncertainty in real-world domains, a crucial issue in the field of reinforcement learning. The proposed methodology uses a risk-averse approach, producing robust and safe performance using data collected from a single training environment, without requiring prior domain knowledge or detailed simulator access.\n                        \n- (2): Innovation point: The paper innovatively introduces coherent distortion risk measures and corresponding Bellman operators to reformulate the safe RL problem from a risk-averse perspective, incorporating uncertainty about the transition model by considering a distribution of training environments instead of training on a single environment. \nPerformance: The proposed approach is demonstrated through experiments on continuous control tasks with safety constraints in the Real-World RL Suite, producing robust and safe performance. \nWorkload: The paper proposes an efficient deep RL implementation to avoid the difficult minimax formulation present in robust RL methods, resulting in a promising approach to safe decision-making under model uncertainty that can be deployed across a range of applications.\n\n\n"
}