{
    "Abstract": "Abstract We propose a general agent population learning system, and on this basis, we propose lineage evolution reinforcement learning algorithm. Lineage evolution reinforcement learning is a kind of derivative algorithm which accords with the general agent population learning system. We take the agents in DQN and its related variants as the basic agents in the population, and add the selection, mutation and crossover modules in the genetic algorithm to the reinforcement learning algorithm. In the process of agent evolution, we refer to the characteristics of natural genetic behavior, add lineage factor to ensure the retention of potential performance of agent, and comprehensively consider the current performance and lineage value when evaluating the performance of agent. Without changing the parameters of the original reinforcement learning algorithm, lineage evolution reinforcement learning can optimize different reinforcement learning algorithms. Our experiments show that the idea of evolution with lineage improves the performance of original reinforcement learning algorithm in some games in Atari 2600. 1 ",
    "Introduction": "INTRODUCTION Deep reinforcement learning has been widely concerned since it was proposed. Deep Q-Networks (Mnih, 2013, 2015) algorithm combines Q-learning with convolutional neural network, introduces the concept of deep reinforcement learning, and achieves excellent results in Atari games (Bellemare, 2013). Double DQN (Hasselt, 2016) solves the overestimate problem in the original DQN by separating the selection function and the evaluation function of the action. Prioritized experience replay (Schaul, 2015) improves the ef\ufb01ciency of sampling by marking the priority of sample data. Dueling DQN (Wang, 2016) separates the value of environment and action, and improves the accuracy of value evaluation of action. A3C (Mnih, 2016) integrates DQN with policy gradient, absorbs the idea of asynchronous and multi-step bootstrap targets (Sutton, 1988), adjusts the balance between variance and deviation, and improves the convergence speed of agent performance. NAF (Shixiang Gu, 2016) enables agents to be used in continuous action space by using positive de\ufb01nite matrix to represent advantage function. Distribution RL (Bellemare, 2017) changes the value evaluation object from average value to distribution, avoiding the evaluation deviation caused by average value \ufb01tting. Noisy DQN (Fortunato, 2017) adds noisy linear layer to DQN algorithm to adjust the exploratory strategy so that the agent can explore stably. Rainbow (Hessel, 2017) integrates six DQN extensions, including DDQN, Prioritized replay, Dueling networks, Multistep learning, Distributional RL and Noisy Nets, which further improves the performance of the agent on Atari 2600, and analyzes the impact of different components on the performance. QR-DQN (Dabney, 2017) added the concept of quantile regression to Distributional RL and proposed. By introducing risk neutral policy, IQN (Dabney, 2018) was proposed based on QR-DQN, and further improves the performance of quantile network. Genetic algorithm, like reinforcement learning algorithm, is also an algorithm that borrows from natural biological characteristics. Genetic algorithm (Holland, 1975) was \ufb01rst proposed in 1975. This paper systematically expounded the theoretical basis of genetic algorithm. Then, Goldberg further improved the theory of genetic algorithm (Goldberg, 1989), and laid the foundation of modern genetic algorithm research. In recent years, the ideas of genetic algorithm, evolution strategy and neural network are gradually integrated. Kenneth proposed NEAT (Kenneth, 2002, 2009, 2012) by combining genetic algorithm with neural network, and furarXiv:2010.14616v1  [cs.NE]  26 Sep 2020 ",
    "Background": "BACKGROUND This part will introduce the core idea and principle of reinforcement learning and genetic algorithm. 2.1 REINFORCEMENT LEARNING Markov Decision Process. Markov Decision Process (MDP), which is a tuple (S; A; T; r; \u03b3), is a kind of mathematical model to simulate agent strategy and reward in the environment. In the tuple, S stands for the state space, A stands for the action space, T is the transition function, r is the reward function, and \u03b3 is a discount factor. Its theoretical basis is Markov chain. According to the different degree of perception of the environment, there are some variants of Markov decision process, such as partial Markov decision process and constrained Markov decision process. Markov decision process has Markov property, that is, the state of the current moment is only related to the state of the last moment and the action taken at the last moment, but not to the state before the last moment. Its conditional probability formula is p(si+1|si, ai, ...s0, a0) = p(si+1|si, ai). (1) Deep Q-Networks. Deep Q-Networks (DQN) is the \ufb01rst algorithm that combines Q learning with neural network. Most of the original Q-learning algorithms access the state and corresponding rewards in the form of tables, which is dif\ufb01cult to deal with the complex environment of state and action. Neural network just makes up for its shortcomings. In DQN, the selection of agent action is decided by neural network. When an agent makes a decision, the current observation of the environment is the input of the neural network, and the Q value of each discrete action is the output of the neural network. The action with the largest Q value is selected to execute. The loss function of training DQN is (Rt+1 + \u03b3t+1maxq\u00af\u03b8(St+1, a\u2032) \u2212 q\u03b8(St, At))2. (2) Distributional RL. Distributional RL is a kind of reinforcement learning algorithm which can \ufb01t the distribution of action value. In the original dqn, only one value is output for each action, which represents the mean value of the action. Other attributes of the action value distribution are not considered too much. However, some values with large distribution gap may have the same mean value, which makes a lot of important information lost in value function \ufb01tting. In order to solve this problem, the output of distribution RL is no longer the traditional mean value, but the value distribution of actions. After setting the upper and lower limits of all action values, the network will output the value distribution of an action. In their paper, the most representative algorithm is C51, which outputs a 51 dimensional vector for each action, and each element represents the probability of its corresponding value. Rainbow. In Rainbow (Dopamine), researchers implemented the three most important modules in the original Rainbow algorithm: n-step Berman update, priority experience replay and distributional RL. Among them, prioritized replay means that the algorithm de\ufb01nes the priority of training data according to absolute TD error. The algorithm is more inclined to take out the samples with high priority when sampling, which greatly improves the ef\ufb01ciency of agent learning. Implicit Quantile Networks. Based on C51, Dabney proposed QR-DQN. QR-DQN takes quantile as \ufb01tting object, which improves the accuracy and robustness of value distribution evaluation. After that, Dabney added an extra neural network to the original QR-DQN to dynamically determine the \ufb01tting accuracy, and proposed Implicit Quantile Networks (IQN), which has risk sensitive property. 2.2 GENETIC ALGORITHM The idea of genetic algorithm comes from Darwin\u2019s law of natural selection. Genetic algorithm encodes the solution of the problem \ufb01rst, and then optimizes the solution of binary encoding format through selection, mutation and crossover. Genetic algorithm keeps the individuals with high \ufb01tness through the \ufb01tness evaluation function, and takes the retained individuals as the parents of the new population to generate new solutions. Genetic algorithm mainly includes \ufb01ve parts: \ufb01tness evaluation, selection operation, mutation operation, crossover operation and judgment of termination conditions. Fitness evaluation. Fitness evaluation is the basis of selection, crossover and mutation in genetic algorithm, which is used to measure the performance of an individual in the current generation. Generally, the \ufb01tness function needs to be adjusted according to the application environment. Selection operation. After getting the \ufb01tness evaluation results of all individuals in the population, genetic algorithm will select the individuals with higher \ufb01tness from the current population according to certain rules as the parent samples of the next generation population. The purpose of this operation is to retain genes with high \ufb01tness in the evolution process. Mutation operation. The mutation operation originates from the mutation of biological chromosomes in nature. In mutation operation, genetic algorithm determines whether to perform mutation operation according to mutation probability, and randomly selects one or a group of basic bits for mutation. Crossover operation. The object of traditional crossover operation is two string binary codes. Following the crossover and recombination of biological chromosomes, two binary codes are used to realize crossover operation. Judgment of termination conditions. The goal of genetic algorithm is to \ufb01nd the local optimal solution rather than the global optimal solution in the high-dimensional solution space. Therefore, it is dif\ufb01cult for genetic algorithm to stop by \ufb01nding the global optimal solution. There are two common termination conditions. The \ufb01rst is to limit the number of evolutions. The second is to terminate the algorithm when the performance reaches a certain score or the performance \ufb02uctuation is small. 3 AGENT POPULATION LEARNING SYSTEM At present, a lot of arti\ufb01cial intelligence algorithms are based on the speci\ufb01c problems and analyze the characteristics of the problems. For example, natural language processing focuses more on the structure of language, and image recognition focuses more on the processing of image features. If we analyze these tasks from a more abstract perspective, we can get a more general algorithm design pattern. On the one hand, a general algorithm system can reduce the limitations of algorithm application, on the other hand, it can improve the ef\ufb01ciency of algorithm design when researchers deal with new tasks. The inspiration of agent population learning system proposed in this paper comes from human perception, learning and evolution. In the general agent population learning system (GAPLS), we divide the architecture of agent into three levels, from bottom to top, they are perception layer, thinking layer and population layer. In the perception layer, the agents mainly complete the task of information processing. The agents use some algorithms directly deal with the external information obtained by a certain perception medium, abstract this information, and transmit the information obtained from the abstraction to the thinking level. The function of perception layer corresponds to the function of perception organ in human intelligence. The machine learning of this module focuses on improving the recognition ability of agents to environmental information, aiming to extract higher-level features through complex and largescale representation information. The task of the thinking layer is to study and explore. In many current algorithms, the concept of thinking is usually blurred or bound with the perception layer. In fact, there is a certain deviation from the way of thinking of human intelligence. Although human intelligence is essentially the transmission of neurotransmitters, perception and decision-making are realized by different brain regions. Therefore, we think that the thinking layer should be treated as a single layer. At present, many algorithms can be regarded as thinking layer algorithms. The input of the thinking layer is a feature group transmitted from the perception layer, and its output is the decision-making of the agent. The task goal of the thinking layer is to gradually improve the adaptability of agents to the environment without changing the input characteristics, and improve their performance in a speci\ufb01c environment or task through existing data. The exploration of agents in the thinking layer is relatively Realization Interaction Population Thinking Perception Evolution Learning and  Exploration Information  Processing  Reinforcement Learning (Q-Learning, Hierarchical  Reinforcement Learning, Course Learning, etc.),  Supervised Learning, Semi-Supervised Learning, etc. Neural Network (DNN, DBN, CNN, etc.),  Decision Tree, Bayes Classifier, etc. 1. Same Task (PSO, ACO, etc.) 2. Different Tasks (Cooperation and  Competition of Multi-Agent) 1. Multi Feature Collaborative Processing  (Image, Sound, Temperature, Pressure) 2. Imagination and Recollection  (On Model,  RNN, etc.) Genetic Algorithm  (Selection, Crossover, Mutation),  Ancestry, etc. Figure 1: General Agent Population Learning System stable. The population layer is at the top of the whole agent population learning system, and it is also the layer with the most exploratory and the least call times in the learning process of agent population. The population layer of agents corresponds to the population of human beings, both of which take independent intelligent units as basic computing units. The most typical algorithm in the population layer of the same task is genetic algorithm, and the algorithms of different tasks are mainly focused on the \ufb01eld of game theory. The goal of population level is to break the limitation of current learning, jump out of local optimal solution, and try to make breakthroughs to improve the adaptability of agents to the environment. In addition, if the population layer adopts the elite strategy, the excellent and rapid evolution of a single agent can improve the average performance of the whole agent population in a limited time, and drive other agents to make leaping progress. Task objectives at different layers are relatively independent. However, due to the difference of perception information, task characteristics and the de\ufb01nition of environment, we still need to supplement the interaction modules between different layers to improve the whole agent population learning system and ensure its universality. The interaction between population layer and thinking layer can be divided into the same task and different tasks according to different task settings. To some extent, interaction patterns will affect the way of evolution. In the interaction process of thinking layer and perception layer, if the perception mode is not unique, the interaction process may need to preprocess the feature groups acquired by different perceptions to ensure the unity of dimensions. In addition, some tasks involving prediction or timing can be supplemented with imagination and recall modules in the interaction process to improve the pertinence of tasks. According to the design of agent population learning system, we give the corresponding algorithm \ufb02ow. In Algorithm 1, we de\ufb01ne the structure of perceptions, agents and population.In the evolution operation, we keep the evaluation, selection, mutation and crossover in genetic algorithm, and adopt the elite strategy in the agent selection. In order to ensure the accuracy of the evaluation, we add a potential performance factor in the evaluation process. Potential performance can be derived directly from past performance or predicted based on the mapping relationship between past environment and past performance, and updated in each generation. In the general agent population learning algorithm, we train the mapping function of thinking layer and perception layer independently according to the structure of agent population learning system. Taking the neural network algorithm as an example, we divide the agent network into two parts. The perception part can generate as many different features as possible through semi supervised learning, while the thinking part uses another completely independent network for supervised learning. When the network of the thinking part is trained, the network parameters of the perception part remain unchanged. When the agent interacts with the environment, we make the thinking part and the perception part work together. This training mode has the following advantages: 1) The perception function can be reused. 2) In line with the design concept of agent population learning system. 3) Training is more ef\ufb01cient. In the general agent population learning algorithm, if ip is equal to 1, the agent\u2019s thinking layer and perception layer can be regarded as bundling training, which is consistent with the current DQN and its variant algorithms.Considering the different types of tasks, we take environment interaction as an option in the learning pro",
    "Evaluation": "EVALUATION The evaluation of agent performance is the \ufb01rst step of agent evolution. The evaluation process occurs before each selection, mutation and crossover, and after the learning of all agents of the previous generation. Agents perform evolution operations on a regular basis in the process of reinforcement learning. When the agent population is in the evolution generation, we evaluate the agent performance \u0393. Because of the parallelism of the algorithm, the time for each agent to reach the evolutionary operation is different, so we also set the evolution lock for the agent population. When an agent reaches the evolutionary operation, the evolution lock is closed. When all agents are ready for evolution operation, the evolution lock is opened, and the algorithm evaluates the performance of all agents. Due to the randomness and complexity of the environment, only using the current performance of agents for evaluation will not be able to accurately measure the performance of agents in the following situations: 1) The current performance of agent is accidental, which is far from the result of re running in the same environment. Due to the uncertainty of the environment, the performance of the current agent may be accidental, and the agent cannot guarantee that the performance of the next run is the same as the current one. 2) Although the current performance of the agent may be good, its mobility is poor. Reinforcement learning is a kind of unsupervised learning algorithm, but due to the limitation of learning samples, its learning process will also have the phenomenon of over \ufb01tting. When over \ufb01tting occurs, the agent can perform well in the same or similar environment as the current learning samples. However, the agent may not perform well in other environments under the same settings, its learning potential is low. 3) There is a big gap between the current learning samples and the historical samples. Although the current performance of the agent is not good, it has great potential. As we all know, with the running of reinforcement learning algorithm, the replay buffer used for agent training will be constantly updated. Although the agent will follow certain random principle to extract samples during training, it is undeniable that the learning samples in different periods have different characteristics. In the learning process, some agents with good performance may encounter an environment having a large gap with the samples in the current buffer. In this case, the current performance of these agents may be lower than that of history or other agents of the same generation. However, this kind of agent may have high potential. After training in the current unfamiliar environment, this kind of agent will have stronger robustness. In order to solve the above problems, based on the general agent population learning system, we imitated the lineage theory in human reproduction, and added the lineage factor as one of the evaluation indexes. The new evaluation method comprehensively considers the current performance and implicit performance of the agent, thus improving the accuracy of agent evaluation. The evaluation of agent can be divided into two sub steps. The \ufb01rst step is to normalize the current performance of the agent. The current performance \u03c1 of an agent is represented by the score of the agent in the environment, so the performance distribution in different environments is different. During the comprehensive evaluation, it is necessary to ensure that the value range of lineage is consistent with that of current performance. After normalizing the current performance of the agent, the performance and lineage have the same value range [0,1]. The second step is computing the comprehensive evaluation value \u0393 of the agent according to the lineage value \u03c6 and the current performance \u03c1. The weight of lineage value and current performance is w\u03c6 and w\u03c1 respectively. 4.2 LINEAGE VALUE UPDATING After the evaluation of the agent, the algorithm enters the lineage value updating phase. In LERL, lineage will be updated with the learning of agents, and the new lineage value will be determined by the historical lineage value and the current performance of agents. The process of lineage value renewal can be divided into three steps. The \ufb01rst step is to give the lineage update value \u2206\u03c6 according to the current performance of the agents. The second step is to generate new lineage value according to the old lineage value and lineage renewal value. The third step is to normalize the new lineage value. When calculating the lineage update value, we rank the agents according to their current performance. The higher the performance of the agent, the higher the lineage update value. The maximum lineage update value is 1, and all of them are positive. \u2206\u03c6i = An \u2212 Arank i + 1 An (3) \u03c6i = (\u2206\u03c6i + \u03c6i \u2217 \u03b6o) \u2212 min(\u2206\u03c6i + \u03c6i \u2217 \u03b6o) max(\u2206\u03c6i + \u03c6i \u2217 \u03b6o) \u2212 min(\u2206\u03c6i + \u03c6i \u2217 \u03b6o) (4) It should be noted that the renewal of lineage value must be strictly carried out after the comprehensive evaluation. Because if the lineage value update is performed \ufb01rst, the comprehensive evaluation value will consider the current performance twice. Although from the perspective of algorithm execution, only adjusting the weight of performance can achieve the same effect as the current algorithm \ufb02ow, but multiple consideration of performance will increase the complexity of lineage update function and increase the dif\ufb01culty of parameter adjustment. 4.3 SELECTION In the selection operation part, we adopt the elite strategy to select the agents to be retained. According to the comprehensive evaluation value of agents, the agent population can be divided into three parts. The \ufb01rst part is the elite agents E1. Each time a selection operation is performed, several agents with the best performance are selected as elite agents. As the parents of new agents, elite agents will generate new agents through mutation or crossover operation.The second part is general agents E2. The performance evaluation of general agent is in the middle of the whole population. In the evolution, the part of agent is directly a member of the next generation population.The third part is to eliminate the agents Em and Ec. The performance of the eliminated agents is at the end of the current population, which will be discarded in evolution due to their poor performance. Because the population size is \ufb01xed, the positions of the eliminated agents will be replaced by the new agents. The new agents come from the mutation operation and crossover operation of elite agents. The elite strategy has the following functions: 1) It ensures that the excellent agents will not be lost due to the interference of mutation or crossover. 2) It ensures that the upper performance limit of LERL is not lower than that of the original algorithm. 3) If evolution operation cannot improve the performance of the agent, the whole learning process of the agent will not be affected. 4) If the performance of the agent declines due to the uncertainty of reinforcement learning, the corresponding agent can be discarded in time. 4.4 MUTATION Mutation operation and cross operation are the key to speed up learning and improve performance. Mutation operation can be divided into three parts: agent replication, network mutation and lineage inheritance. Agent replication refers to the online network replication of an elite agent. It should be noted that in the agent replication phase, the mutation operation only deals with the network parameters, and does not copy the data in the replay buffer. In agent mutation, we disturb the parameters of the network, and the disturbance range is all the layers of online network. The disturbance operation includes two parameters: the \ufb01rst is the probability of disturbance vpart, and the second is the amplitude of disturbance vrange. When the network is disturbed, we traverse the network layer by layer, and judge whether the corresponding layer is disturbed according to the disturbance probability. When a layer of the network is disturbed, the shape of the layer network is obtained \ufb01rst, and the disturbed network is generated according to its shape and disturbance amplitude. The parameters in the disturbed network are uniformly distributed. Vij \u223c U(1 \u2212 vrange, 1 + vrange) (5) After the disturbed network is generated, we get a new network by Hadamard product between the online parameter matrix and the disturbed parameter matrix. We introduce crossover operation Om, which is de\ufb01ned as follows: Om(Q(x, a; \u03b8)) := Q(x, a; \u03b8 \u2217 V ) (6) The disturbance amplitude vrange is very important for mutation operation. If vrange is too large, it will lead to the loss of previous learning. If vrange is too small, the exploration of mutation operation cannot be guaranteed. In addition, because of the less randomness of the agent in the learning process and the gradual convergence of the algorithm, vrange should have different values in different periods. In LERL, with the increase of the number of iterations, vrange will decrease. vrange = min(vrange0 \u2212 Gn \u2217 vrange decay, vrange min) (7) When the disturbance amplitude decays to a certain extent, the disturbance parameters will not change, so as to ensure that the mutation operation remains exploratory. After the network disturbance, the new mutation agent needs to inherit the lineage value of the original agent. Due to the in\ufb02uence of mutation operation, the inherited lineage value should be attenuated, and its attenuation parameter is \u03b6m. 4.5 CROSSOVER The \ufb01rst step in crossover operation is similar to mutation operation, which is to select and copy agents from elite agents as the parent agents in crossover operation. Then, we carry out network crossover operation on the selected two replicated elite agents. No matter the binary code crossover operation in the traditional genetic algorithm or the expected value crossover operation in the evolution strategy, it is undoubtedly dif\ufb01cult to be directly used in the neural network. Referring to the description of human neural in cognitive neuroscience (Gazzaniga, 2000), we think that convolution network is more emphasis on feature extraction, while full connection layer is more emphasis on decision-making according to the extracted features. The former corresponds to the perception layer in the agent population learning system, and the latter to the thinking layer. Therefore, we split the convolution network and the full connection network in the online network as two basic units of crossover operation. In the crossover operation, we crossover the convolution network part and the full connection network part of the two elite agents, and take the two new networks as the online networks of the new agents. We introduce crossover operation Oc, which is de\ufb01ned as follows: Q(x, a) := Qf(Qc(x; \u03b8c), a; \u03b8f) (8) Oc(Qj, Qk) := Qf j (Qc k(x; \u03b8c k), a; \u03b8f j ) (9) Similar to mutation operation, crossover operation also needs to inherit and decay the lineage value of the parent agents, and its decay parameter is \u03b6c. The lineage decay formula of crossover operation is different from that of mutation operation. There are two parent agents of crossover operation, so it is necessary to calculate the average lineage value of parent agents before inheriting and decaying. In addition, because mutation and crossover do not affect each other and there is no strict logical order, the two operations can run in parallel. ",
    "Experiment": "EXPERIMENT We use the data and algorithm in the Dopamine framework for experimental analysis. Among them, the reinforcement learning algorithm for LERL includes DQN, C51, Rainbow and IQN in Dopamine. The running environment includes Asterix, Assault, ChopperCommand and KungFuMaster. The baseline data of each algorithm is provided by Dopamine project. Figure 2 shows the performance of LE Rainbow, which uses Rainbow algorithm as the basic reinforcement learning algorithm, in four games. The images in the \ufb01rst row show the original performance of the agents in Rainbow and LE Rainbow. Among them, the agent population of LE Rainbow includes 7 agents. In evolution operations, the number of elite agents is 2, the number of general agents is 2, and the number of eliminated agents is 3. Rainbow\u2019s curve is the performance data of agents running independently \ufb01ve times. The evolution cycle of LE Rainbow is 5 iterations. The images in the second row are the average performance curve of the two algorithms. In addition, due to the randomness of evolution, the performance of LE Rainbow algorithm changes greatly, so its average performance is not representative. We introduce the smooth average performance curve LE Rainbow Smooth for LE Rainbow. The images in the third row show the best performance curve and median performance curve of the two algorithms. The best performance curve of the agent represents the best performance of the agent in each iteration, which can better help us to analyze the performance of LE Rainbow. As can be seen from Figure 2, LE Rainbow improves the performance of Rainbow greatly in Asterix and KungFuMaster. Whether it\u2019s average performance, best performance or median performance, LE Rainbow has always been higher than Rainbow since the early stage. In Assault and ChopperCommand, although the performance of LE Rainbow is not signi\ufb01cantly improved, it also ensures that the performance is not lower than that of Rainbow. In addition, from the best performance curve, we can see that LE Rainbow is more exploratory. In the process of exploration, the best performance LE Rainbow ",
    "Conclusion": "CONCLUSION We propose a general agent population learning system (GAPLS) and give the corresponding algorithm \ufb02ow according to its system structure. On this basis, we propose lineage evolutionary reinforcement learning algorithm (LERL) which combines genetic algorithm, reinforcement learning algorithm and lineage factor. The performance of LERL in the experiment reached our overdue and veri\ufb01ed our theoretical analysis. Compared with the original reinforcement learning algorithm, the performance and learning speed of the algorithm with LE are improved. In the future research, we think that some improvements in mutation operations and crossover operations may be able to reduce the performance \ufb02uctuations caused by evolution operations. In addition, the initial values of some parameters in evolution operations also can be further studied. ",
    "References": "References [1] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018. [2] Marc G Bellemare, Will Dabney, and R\u00b4emi Munos. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 449\u2013458. JMLR. org, 2017. [3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013 279, 2013. [4] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018. [5] Will Dabney, Georg Ostrovski, David Silver, and R\u00b4emi Munos. Implicit quantile networks for distributional reinforcement learning. arXiv preprint arXiv:1806.06923, 2018. [6] Will Dabney, Mark Rowland, Marc G Bellemare, and R\u00b4emi Munos. Distributional reinforcement learning with quantile regression. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [7] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017. [8] Michael S Gazzaniga. The new cognitive neurosciences. MIT press, 2000. [9] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pages 2829\u20132838, 2016. [10] Nikolaus Hansen. The cma evolution strategy: a comparing review. In Towards a new evolutionary computation, pages 75\u2013102. Springer, 2006. [11] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018. [12] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937, 2016. [13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [14] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. [15] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [16] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge, 1998. [17] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Thirtieth AAAI conference on arti\ufb01cial intelligence, 2016. [18] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015. [1\u201318] ",
    "title": "Lineage Evolution Reinforcement Learning",
    "paper_info": "Lineage Evolution Reinforcement Learning\nZeyu Zhang, Guisheng Yin\nAbstract\nWe propose a general agent population learn-\ning system, and on this basis, we propose lin-\neage evolution reinforcement learning algo-\nrithm. Lineage evolution reinforcement learn-\ning is a kind of derivative algorithm which ac-\ncords with the general agent population learn-\ning system. We take the agents in DQN and its\nrelated variants as the basic agents in the pop-\nulation, and add the selection, mutation and\ncrossover modules in the genetic algorithm to\nthe reinforcement learning algorithm. In the\nprocess of agent evolution, we refer to the\ncharacteristics of natural genetic behavior, add\nlineage factor to ensure the retention of po-\ntential performance of agent, and comprehen-\nsively consider the current performance and\nlineage value when evaluating the performance\nof agent. Without changing the parameters of\nthe original reinforcement learning algorithm,\nlineage evolution reinforcement learning can\noptimize different reinforcement learning al-\ngorithms. Our experiments show that the idea\nof evolution with lineage improves the perfor-\nmance of original reinforcement learning algo-\nrithm in some games in Atari 2600.\n1\nINTRODUCTION\nDeep reinforcement learning has been widely concerned\nsince it was proposed. Deep Q-Networks (Mnih, 2013,\n2015) algorithm combines Q-learning with convolutional\nneural network, introduces the concept of deep reinforce-\nment learning, and achieves excellent results in Atari\ngames (Bellemare, 2013). Double DQN (Hasselt, 2016)\nsolves the overestimate problem in the original DQN by\nseparating the selection function and the evaluation func-\ntion of the action. Prioritized experience replay (Schaul,\n2015) improves the ef\ufb01ciency of sampling by marking\nthe priority of sample data. Dueling DQN (Wang, 2016)\nseparates the value of environment and action, and im-\nproves the accuracy of value evaluation of action. A3C\n(Mnih, 2016) integrates DQN with policy gradient, ab-\nsorbs the idea of asynchronous and multi-step bootstrap\ntargets (Sutton, 1988), adjusts the balance between vari-\nance and deviation, and improves the convergence speed\nof agent performance.\nNAF (Shixiang Gu, 2016) en-\nables agents to be used in continuous action space by us-\ning positive de\ufb01nite matrix to represent advantage func-\ntion.\nDistribution RL (Bellemare, 2017) changes the\nvalue evaluation object from average value to distribu-\ntion, avoiding the evaluation deviation caused by average\nvalue \ufb01tting. Noisy DQN (Fortunato, 2017) adds noisy\nlinear layer to DQN algorithm to adjust the exploratory\nstrategy so that the agent can explore stably. Rainbow\n(Hessel, 2017) integrates six DQN extensions, includ-\ning DDQN, Prioritized replay, Dueling networks, Multi-\nstep learning, Distributional RL and Noisy Nets, which\nfurther improves the performance of the agent on Atari\n2600, and analyzes the impact of different components\non the performance. QR-DQN (Dabney, 2017) added\nthe concept of quantile regression to Distributional RL\nand proposed. By introducing risk neutral policy, IQN\n(Dabney, 2018) was proposed based on QR-DQN, and\nfurther improves the performance of quantile network.\nGenetic algorithm, like reinforcement learning algo-\nrithm, is also an algorithm that borrows from natural\nbiological characteristics. Genetic algorithm (Holland,\n1975) was \ufb01rst proposed in 1975. This paper system-\natically expounded the theoretical basis of genetic algo-\nrithm. Then, Goldberg further improved the theory of\ngenetic algorithm (Goldberg, 1989), and laid the foun-\ndation of modern genetic algorithm research. In recent\nyears, the ideas of genetic algorithm, evolution strategy\nand neural network are gradually integrated. Kenneth\nproposed NEAT (Kenneth, 2002, 2009, 2012) by com-\nbining genetic algorithm with neural network, and fur-\narXiv:2010.14616v1  [cs.NE]  26 Sep 2020\n",
    "GPTsummary": "                    - (1): The background of this article is to improve the performance of reinforcement learning (RL) algorithms for Atari 2600 games by borrowing ideas from natural genetic behavior to create an agent population learning system with a lineage evolution RL algorithm.\n\n                    - (2): Previous methods for RL algorithms include Deep Q-Networks (DQN), Double DQN, Prioritized experience replay, and Dueling DQN. While they have achieved good results in Atari 2600 games, they have limitations such as overestimation and evaluation deviation. The approach proposed in this paper is well motivated since it introduces a derivative algorithm based on the general agent population learning system with selection, mutation, and crossover modules from the genetic algorithm.\n\n                    - (3): The research methodology proposed in this paper is to add lineage factors to ensure the retention of potential performance of the agent and comprehensively consider the current performance and lineage value when evaluating the performance of the agent. This is done without changing the parameters of the original RL algorithm and can be used to optimize different RL algorithms.\n\n                    - (4): The methods proposed in this paper achieve better performance for some games in Atari 2600, indicating that lineage evolution can improve the performance of the original RL algorithm. However, the performance improvement is not consistent across all games, and more research is needed to evaluate the effectiveness of the approach more comprehensively.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to improve the performance of reinforcement learning algorithms for Atari 2600 games by introducing lineage evolution to create a general agent population learning system with a lineage evolution reinforcement learning algorithm. This approach is well motivated and can be applied to optimize different RL algorithms. \n\n- (2): Innovation point: The use of lineage evolution and a general agent population learning system in reinforcement learning for Atari 2600 games is a unique and innovative approach. Performance: The methods proposed in this paper achieve better performance for some games in Atari 2600, indicating that lineage evolution can improve the performance of the original RL algorithm. However, the performance improvement is not consistent across all games, and more research is needed to evaluate the effectiveness of the approach more comprehensively. Workload: The workload of implementing the lineage evolution reinforcement learning algorithm is not significantly different from traditional reinforcement learning algorithms, as the original RL algorithm parameters do not need to be changed.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to improve the performance of reinforcement learning algorithms for Atari 2600 games by introducing lineage evolution to create a general agent population learning system with a lineage evolution reinforcement learning algorithm. This approach is well motivated and can be applied to optimize different RL algorithms. \n\n- (2): Innovation point: The use of lineage evolution and a general agent population learning system in reinforcement learning for Atari 2600 games is a unique and innovative approach. Performance: The methods proposed in this paper achieve better performance for some games in Atari 2600, indicating that lineage evolution can improve the performance of the original RL algorithm. However, the performance improvement is not consistent across all games, and more research is needed to evaluate the effectiveness of the approach more comprehensively. Workload: The workload of implementing the lineage evolution reinforcement learning algorithm is not significantly different from traditional reinforcement learning algorithms, as the original RL algorithm parameters do not need to be changed.\n\n\n"
}