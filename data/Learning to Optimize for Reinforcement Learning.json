{
    "Abstract": "Abstract In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning optimization, outperforming classical hand-designed optimizers. However, in practice, these learned optimizers fail to generalize to reinforcement learning tasks due to unstable and complex loss landscapes. Moreover, neither handdesigned optimizers nor learned optimizers have been speci\ufb01cally designed to address the unique optimization properties in reinforcement learning. In this work, we take a data-driven approach to learn to optimize for reinforcement learning using meta-learning. We introduce a novel optimizer structure that signi\ufb01cantly improves the training ef\ufb01ciency of learned optimizers, making it possible to learn an optimizer for reinforcement learning from scratch. Although trained in toy tasks, our learned optimizer demonstrates its generalization ability to unseen complex tasks. Finally, we design a set of small gridworlds to train the \ufb01rst general-purpose optimizer for reinforcement learning. 1. ",
    "Introduction": "Introduction Deep learning has achieved great success in many areas (LeCun et al., 2015), which is largely attributed to the automatically learned features that surpass handcrafted expert features. The use of gradient descent enables automatic adjustments of parameters within a model, yielding highly effective features. Despite these advancements, as another important component in deep learning, optimizers are still largely hand-designed and heavily reliant on expert knowledge, such as RMSprop (Tieleman & Hinton, 2012) and *This work was done during an internship at Sea AI Lab, Singapore. Code release: https://github.com/ sail-sg/optim4rl 1Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada 2Sea AI Lab, Singapore 3CIFAR AI Chair, Amii. Correspondence to: Qingfeng Lan <qlan3@ualberta.ca>, Zhongwen Xu <zhongwen.s.xu@gmail.com>. Preprint. Adam (Kingma & Ba, 2015). To reduce the burden of hand-designing optimizers, researchers propose to learn to optimize (Sutton, 1992; Andrychowicz et al., 2016; Wichrowska et al., 2017; Maheswaranathan et al., 2021) with the help of meta-learning. A learned optimizer is typically represented by a neural network, named the meta-network. Its parameters and gradients are called meta-parameters and meta-gradients, respectively. The meta-network can be used to compute the hyper-parameters of classical optimizers (Xu et al., 2017; Metz et al., 2019) or the changes of parameters in a learning model (Lv et al., 2017; Li & Malik, 2017). During meta-learning, the meta-network is updated to achieve better optimization. For example, hyper-parameters (e.g., the learning rate) can be adjusted automatically. Even the parameter update rule can be directly learned from training data. Compared to designing optimizers with human expert knowledge, learning an optimizer is a data-driven approach, reducing the reliance on expert knowledge. Despite the signi\ufb01cant progress made in learning optimizers, previous works only present learned optimizers under supervised learning (SL) settings. These learned optimizers usually have complex neural network structures and incorporate numerous human-designed input features, requiring a large amount of computation and human effort to design and train them. However, they have been shown to perform poorly in reinforcement learning (RL) tasks (Metz et al., 2020b; 2022b). Learning to optimize for RL remains an open and challenging problem. Classical optimizers are typically designed for optimization in SL tasks and then applied to RL tasks. However, RL tasks possess unique properties that are largely overlooked by classical optimizers. For example, unlike SL, the input distribution of an RL agent is non-stationary and non-independent and identically distributed (non-i.i.d.) due to locally correlated transition dynamics (Alt et al., 2019). Additionally, the target function and the loss landscapes in RL are constantly changing throughout the learning process, as a result of policy and value iterations. These unique properties make the optimization process in RL much more unstable and complex. It is also inappropriate to apply optimization algorithms designed for SL to RL directly (Bengio et al., 2020a;b). Yet we still lack optimizers speci\ufb01cally designed for RL tasks. arXiv:2302.01470v1  [cs.LG]  3 Feb 2023 ",
    "Related Work": "",
    "Background": "Background 2.1. Reinforcement Learning In reinforcement learning (RL), an agent learns by trial and error to maximize a reward signal. The process of reinforcement learning can be formalized mathematically as a Markov Decision Process (MDP). Formally, let M = (S, A, P, r, \u03b3) be an MDP which includes a state space S, an action space A, a state transition probability function P : S \u00d7 A \u00d7 S \u2192 R, a reward function r : S \u00d7 A \u2192 R, and a discount factor \u03b3 \u2208 [0, 1). For a given MDP, the agent interacts with the MDP according to its policy \u03c0, which maps a state to a distribution over the action space. At each time-step t, the agent observes a state St \u2208 S and samples an action At from \u03c0(\u00b7|St). Then it observes the next state St+1 \u2208 S according to the transition function P and receives a scalar reward Rt+1 = r(St, At) \u2208 R. The agents aim to \ufb01nd an optimal policy \u03c0\u2217 to maximize the expectation of the return, Gt = \ufffd\u221e k=t \u03b3k\u2212tRk+1. The state-value function v\u03c0 induced by the policy \u03c0 maps states to expected returns. Moreover, it can be written recursively in form of a Bellman equation: v\u03c0(s) = E\u03c0[Rt+1 + \u03b3v\u03c0(St+1) | St = s]. In practice, we approximate v\u03c0 with v. The recursive form of v\u03c0 allows us to approximate v\u03c0 by bootstrapping from the current estimate of the value function v, which is also known as temporal difference (TD) learning: v(St) \u2190 v(St) + \u03b1( the TD target \ufffd \ufffd\ufffd \ufffd Rt+1 + \u03b3v(St+1) \u2212v(St)), (1) where \u03b1 is the learning rate and Rt+1 + \u03b3v(St+1) is named the TD target. 2.2. Learning to Optimize with Meta-learning We aim to learn an optimizer using meta-learning. Let \u03b8 be the agent-parameters that we aim to optimize with an optimizer. A (learned) optimizer is de\ufb01ned as an update function U which maps input gradients to parameter updates, implemented as a meta-network, parameterized by the metaparameters \u03c6. Let z be the input features which may include gradients g, losses L, and exponential moving average of gradients. Let h be an optimizer state which stores historical values. We can then compute agent-parameters updates \u2206\u03b8 as well as the updated agent-parameters \u03b8\u2032 with \u2206\u03b8, h\u2032 = U\u03c6(z, h), and \u03b8\u2032 = \u03b8 + \u2206\u03b8. (2) Note that all classical \ufb01rst-order optimizers can be written in the form of Equation 2 with \u03c6 = \u2205. For example, in SGD, h\u2032 = h = \u2205, z = g, and USGD(g, \u2205) = (\u2212\u03b1g, \u2205), where \u03b1 is the learning rate. In RMSProp (Tieleman & Hinton, 2012), let z = g, and h is used to store the average of squared gradients. Then URMSProp(g, h) = (\u2212 \u03b1g \u221ah\u2032+\u03f5, h\u2032), where h\u2032 = \u03b2h+(1\u2212\u03b2)g2, \u03b1 is the learning rate, \u03b2 \u2208 [0, 1], and \u03f5 is a tiny positive number for numerical stability. Similar to Xu et al. (2020), we apply two-level optimization to optimize \u03b8 and \u03c6. First, we collect a sequence of M + 1 trajectories T = {\u03c4i, \u03c4i+1, \u00b7 \u00b7 \u00b7 , \u03c4i+M\u22121, \u03c4i+M}. For the inner update, we \ufb01x \u03c6 and apply multiple steps of gradient descent updates to \u03b8 by minimizing an inner loss Linner. Speci\ufb01cally, for each trajectory \u03c4i \u2208 T , we have \u2206\u03b8i \u221d \u2207\u03b8Linner(\u03c4i; \u03b8i, \u03c6) and \u03b8i+1 \u2190 \u03b8i + \u2206\u03b8i, where \u2207\u03b8Linner are agent-gradients of \u03b8. By repeating the above process for M times, we get \u03b8i \u03c6\u2212\u2192 \u03b8i+1 \u00b7 \u00b7 \u00b7 \u03c6\u2212\u2192 \u03b8i+M. Here, \u03b8i+M are actually functions of \u03c6. For simplicity, we abuse the notation and still use \u03b8i+M. Next, we use \u03c4i+M as a validation trajectory to optimize \u03c6 with an outer loss Louter: \u2206\u03c6 \u221d \u2207\u03c6Louter(\u03c4i+M; \u03b8i+M, \u03c6) and \u03c6 \u2190 \u03c6 + \u2206\u03c6. Note that \u2207\u03c6Louter are meta-gradients. Since \u03b8i+M are functions of \u03c6, we can apply the chain rule to compute metagradients. In practice, this can be done with the help of automatic differentiation packages. 3. Related Work Our work is closely related to two areas: optimization in reinforcement learning (RL) and learning to optimize in supervised learning (SL). 3.1. Optimization in RL Henderson et al. (2018) tested different optimizers in RL settings and pointed out that classical adaptive optimizers may Learning to Optimize for Reinforcement Learning \u221215.0 \u221212.5 \u221210.0 \u22127.5 \u22125.0 \u22122.5 0.0 2.5 log(|g| + 10\u221216) 0K 25K 50K 75K 100K 125K 150K 175K 200K Counts in the bin Figure 1. A visualization of agent-gradients collected during training A2C in big dense long , optimized by RMSProp. We compute log(|g| + 10\u221216) to avoid the error of applying log function to non-positive agent-gradients. The y-axis shows the number of elements in corresponding bins. not always consider the complex interactions between RL algorithm properties and the dynamics of the environment. Sarig\u00a8ul & Avci (2018) benchmarked different momentum strategies in deep RL and found that Nesterov momentum is better at generalization. Bengio et al. (2020a) took one step further and showed that unlike SL, momentum in TD learning becomes doubly stale due to changing parameter updates and bootstrapping. By correcting momentum in TD learning, the sample ef\ufb01ciency is improved in policy evaluation. Later, Bengio et al. (2020b) studied the connection between TD learning and generalization and interference. They found that, due to the interplay between the dynamics of interference and bootstrapping in TD learning, low-interference parameters often memorize when TD learning is involved, while low-interference parameters tend to generalize in supervised classi\ufb01cation tasks. These works together hint that it may not always be appropriate to bring optimization methods in SL directly to RL, without considering the unique properties in RL. In this work, we adopt a data-driven approach. Instead of studying these properties and hand-designing new optimizers for RL tasks, we apply meta-learning to learn an optimizer for RL from data generated in the interactions of agents and environments. 3.2. Learning to Optimize in SL Initially, learning to optimize is only applied to tune hyperparameters in classical optimizers, such as learning rates (Jacobs, 1988; Sutton, 1992; Schraudolph & Sejnowski, 1995; Mahmood et al., 2012). Recently, researchers have started to learn an optimizer completely from scratch. Andrychowicz et al. (2016) implemented learned optimizers with long short-term memory networks (LSTMs) (Hochreiter 0 200 400 600 800 1000 Epoch 0.2 0.4 0.6 0.8 1.0 Accurcy W.o. gradient processing With gradient processing Figure 2. The accuracy of RNN models approximating the identity function, with or without gradient processing. & Schmidhuber, 1997), trained by meta-learning. They showed that learned optimizers could generalize to similar but unseen tasks, outperforming hand-designed optimizers. Li & Malik (2017) represented an optimization method as a policy in RL and applied a guided policy search method to \ufb01nd a good optimizer. Wichrowska et al. (2017) introduced a novel hierarchical recurrent neural network (RNN) (Medsker & Jain, 2001) architecture, which greatly reduces memory and computation, and was shown to generalize to neural networks of different structures. They also developed a dataset consisting of small tasks with diverse loss landscapes for meta-learning. Instead of using RNNs, Metz et al. (2022a) developed learned optimizers with multilayer perceptrons (MLPs) only, which achieves a better balance among memory, computation, and performance. Learned optimizers are known to be hard to train. Part of the reason is that they are usually trained by truncated backpropagation through time, which leads to strongly biased gradients or exploding gradients. To overcome these issues, Metz et al. (2019) presented a method to dynamically weigh a reparameterization gradient estimator and an evolutionary strategy style gradient estimator, stabilizing the training of learned optimizers. Vicol et al. (2021) resolved the issues by dividing the computation graph into truncated unrolls and computing unbiased gradients with evolution strategies and gradient bias corrections. Harrison et al. (2022) investigated the training stability properties of optimization algorithms with tools from dynamical systems and proposed to improve the stability of learned optimizers by adding adaptive nominal terms based on Adam (Kingma & Ba, 2015) and AggMo (Lucas et al., 2019). Metz et al. (2020a) trained a general-purpose optimizer, STAR, by training optimizers on thousands of tasks with a large amount of computation. Following the same Learning to Optimize for Reinforcement Learning m h1 RNN1 MLP1 h\u2032 1 v h2 RNN2 MLP2 h\u2032 2 g \u0394\u03b8 = \u2212 \u03b1 m v + \u03f5 g2 Figure 3. The network structure of Optim4RL. spirit, Metz et al. (2022b) continued to perform large-scale optimizer training, leveraging more computation (4, 000 TPU-months) and more diverse SL tasks. The learned optimizer, VeLO, requires no hyperparameter tuning and works well on a wide range of SL tasks. VeLO is the precious outcome of long-time research in the area of learning to optimize, building on the wisdom and effort of many generations and a large amount of computation. Although marking a milestone for the success of learned optimizers in SL tasks, VeLO still performs poorly in RL tasks, as shown in Section 4.4.4 in Metz et al. (2022b). The failure of VeLO in RL tasks suggests that designing learned optimizers for RL tasks is still a challenging problem. Note that all previously mentioned works focus on training learned optimizers for SL. Our work is inspired by them but focuses on learning to optimize for RL. Compared to these works, our method (i.e., Optim4RL) is simple, stable, and effective, without using complex neural network structures or incorporating numerous human-designed input features. As far as we know, our work is the \ufb01rst to demonstrate the success of learned optimizers in deep RL. 4. Learning to Optimize in RL is Much Harder In supervised learning (SL), a training dataset consists of pairs {(xi, yi)} where xi is the input and yi is the label for input xi. The goal is to train a model that maps the input data to the correct labels. During training, a minibatch is sampled from the dataset and fed into the model to generate predictions \u02c6ys for input xs. A loss is computed given the predicted labels \u02c6ys and the true labels ys. A lower loss usually indicates better performance (e.g., higher classi\ufb01cation accuracy). The parameters of this model are then optimized given the loss function with gradient descent methods. Note that in the training process, it is assumed that the training set consists of i.i.d. samples. Moreover, the true labels ys are usually noiseless and stationary (i.e., time-invariant). For example, the true label of a written digit 2 in MNIST (Deng, 2012) is always y = 2 which does not change during training. However, in reinforcement learning (RL), the input training data distribution is non-i.i.d., which makes the whole training process much more unstable and complex, especially when learning to optimize is involved. Compared with SL, one signi\ufb01cant difference in RL training is that the agent-environment interactions change input data distribution, such as visited states and rewards. TD learning is widely used in RL, and TD targets (see Equation 1) in RL play a similar role as labels in SL. However, unlike labels in SL, TD targets are usually biased, non-stationary, and noisy due to changing state-values, complex state transitions, and noisy reward signals. This leads to a changing loss landscape that evolves as the policy and the state-value function change. Unlike SL, a lower loss is not necessarily a good indicator of better performance (i.e., higher return) in RL for this reason. The randomness from state transitions, reward signals, and agent-environment interactions, together with biased TD targets, makes the bias and variance of agent-gradients relatively high. Learned optimizers for SL are infamously hard to train, suffering from high training instability (Wichrowska et al., 2017; Metz et al., 2019; 2020a; Harrison et al., 2022). Learning an optimizer for RL tasks is even harder, due to the high bias and variance of agent-gradients in RL. During learning to optimize in RL, meta-gradients are af\ufb02icted with large noise induced by the high bias and variance of agentgradients. Since meta-gradients are noisy and inaccurate, the improvement of the learned optimizer becomes unstable and slow. With a poorly performed optimizer, the agent policy improvement is no longer guaranteed. Due to the agent-environment interactions, a poor agent policy is unlikely to collect \u201chigh-quality\u201d data to boost the performance of both the agent and the learned optimizer. In the end, this two-level optimization gets stuck in a vicious spiral: a poor optimizer \u2192 a poor agent policy \u2192 collected data of low-quality \u2192 a poor optimizer \u2192 \u00b7 \u00b7 \u00b7 . Note that this phenomenon also occurs to learning to optimize in SL. It is the agent-gradients of high bias and variance in RL and the agent-environment interactions that make learning to optimize in RL much harder. 5. Optim4RL: A Learned Optimizer for RL To overcome the hardness of learning an optimizer in RL, we introduce a dual-RNN structure optimizer, named Optim4RL, that is more robust and sample-ef\ufb01cient than previous methods. To achieve this, we \ufb01rst employ a technique Learning to Optimize for Reinforcement Learning iteration t unit n reset interval m = 3 1 2 3  0     1     2     3     4     5     6     7     8 reset unit 1  at t=0 reset unit 2  at t=4 reset unit 3  at t=8 Figure 4. An illustration of pipeline training where reset interval m = 3 and the number of units n = 3. All training units are reset at regular intervals to diversify input training data. for generating improved gradient representations, which is crucial for the success of Optim4RL. We then describe the key features in the network structure of Optim4RL. 5.1. Gradient Processing Before feeding agent-gradients into neural networks, we process them to get new gradient representations. Speci\ufb01cally, for each scalar gradient g, we map it into a pair of real values: g \u2192 [sgn(g), log(|g| + \u03b5)] (3) where \u03b5 is a small value to prevent the error of computing log(0). In this work, we set it to 10\u221218 in all experiments. We \ufb01nd that gradient processing signi\ufb01cantly improves the training stability and performance of learned optimizers. Note that there is a similar gradient processing method proposed in Andrychowicz et al. (2016). However, we \ufb01nd our approach is simpler and better in practice. In Figure 1, we visualize collected agent-gradients during training A2C (Mnih et al., 2016) in a gridworld, optimized by RMSProp (Tieleman & Hinton, 2012). The gridworld is named big dense long (see the appendix for details) in which the agent is required to move and collect objects. We plot these agent-gradients with logarithmic x-axis. The yaxis shows the number of elements in corresponding bins. Notice that the agent-gradients vary across a wide range in logarithms, from \u2212\u221e (we truncate it to \u221216 in the plot) to 0, while their absolute values are in a small range (i.e., in [0, 1]). This requires a learned optimizer to distinguish small value differences between agent-gradients in order to generate accurate parameter updates. Furthermore, there are two peaks in the distribution, which makes it harder to approximate an optimizer update function with neural networks. By transforming g to [sgn(g), log(|g|+\u03b5)], neural networks would be able to recover g since no information is lost. The new gradient representations also allow neural networks to distinguish small value differences of agentgradients due to the log function. Surprisingly, we \ufb01nd that it is hard for RNNs to approximate the identity function without gradient processing. Consider the identity function f(g) = g where g is the input agentgradient. This is a special case that a learned optimizer approximates SGD with a learning rate \u22121. We train two RNN models to approximate the identity function for 1, 000 epochs by minimizing the mean squared error (\u02c6g \u2212 g)2, where \u02c6g is the predicted value. Each RNN model consists of an LSTM and an MLP with two hidden layers of size 16, with or without gradient processing. We measure model performance by accuracy. Speci\ufb01cally, the model makes a correct prediction if min((1 \u2212 \u03f5\u2032)g, (1 + \u03f5\u2032)g) \u2264 \u02c6g \u2264 max((1 \u2212 \u03f5\u2032)g, (1 + \u03f5\u2032)g) where \u03f5\u2032 = 0.1; otherwise, \u02c6g is a wrong prediction. We plot the training accuracies of two approaches, averaging over 10 runs. As shown in Figure 2, there is a large performance gap between the two approaches, showing the advantage of gradient processing. Moreover, we \ufb01nd that the convergence accuracy with gradient processing is slightly above 90%, far from 100%. This result again validates the dif\ufb01culty of learning a good optimizer: it is hard for RNN models to approximate the identity function accurately, let alone learn a much more complex parameter update function. 5.2. A Dual-RNN Structure In general, we want to learn a good update function in a reasonable hypothesis space. This hypothesis space should be large enough to include as many good known optimizers as possible, such as Adam (Kingma & Ba, 2015) and RMSProp (Tieleman & Hinton, 2012). Meanwhile, we should also rule out obviously bad choices so that a good candidate can be found ef\ufb01ciently during training. Inspired by the success of classical optimizers, we parameterize the parameter update function in a similar form to adaptive optimizers (i.e., Adam, RMSProp, etc.): \u2206\u03b8 = \u2212\u03b1 m \u221av + \u03f5, (4) where \u03b1 is the learning rate, \u03f5 is a small positive value, and m and v are the processed outputs of two RNNs followed by two MLPs, as shown in Figure 3. In practice, it is very hard for neural networks to approximate mathematical operations accurately without additional techniques involved (Telgarsky, 2017; Yarotsky, 2017; Boull\u00b4e et al., 2020; Lu et al., 2021). Besides applying gradient preprocessing, we mitigate this issue by building inductive biases into the design of learned optimizers. By parameterizing the parameter update function as Equation 4, we reduce the burden of approximating square root and division for neural networks. Note that we also test another ",
    "Experiment": "Experiment In this section, we \ufb01rst design experiments to verify that Optim4RL can be learned from scratch in RL tasks. Then we investigate the generalization ability of Optim4RL in different RL tasks. Based on the insights from the investigation, we show how to train a general-purpose learned optimizer for RL that generalizes to complex RL tasks. Similar to Oh et al. (2020), we design 8 gridworlds with diverse properties: small sparse short , small sparse long , small dense long , big sparse short , big sparse long , big dense short , small dense short , and big dense long . These gridworlds are designed to be different regarding horizons, reward functions, and state-action spaces. The details of all gridworlds are described in the appendix. Besides gridworlds, we also test our method in several Brax tasks (Freeman et al., 2021). We mainly consider two RL algorithms \u2014 A2C (Mnih et al., 2016) and PPO (Schulman et al., 2017). For all experiments, we train A2C in gridworlds and train PPO in Brax tasks. For Optim4RL, we use two GRUs (Cho et al., 2014) with hidden size 8; both MLPs have two hidden layers with size 16. To meta-learn optimizers, we set M = 4 in all experiments; that is, for every outer update, we do inner update 4 times. We use Adam as the meta-optimizer to optimize learned optimizers. We compare Optim4RL with two classical optimizers, Adam and RMSProp. For A2C, the inner loss is the standard A2C loss, while the outer loss is the actor loss in A2C loss. For PPO, both the inner loss and outer loss are the standard PPO loss. Please check the appendix for more implementation details. 6.1. Learning an Optimizer for RL from Scratch In this section, we show that it is feasible to train Optim4RL in RL tasks from scratch, and that directly applying learned optimizers for SL to optimize in RL tasks does not work, especially for hard RL tasks. Speci\ufb01cally, two easy gridworlds (small dense short and big dense long ) and two complex Brax tasks (Ant and Humanoid) are selected as test environments. Besides Adam and RMSProp, we consider three other baselines: L2LGD2 (Andrychowicz et al., 2016), a classical learned optimizer in SL; STAR (Harrison et al., 2022) and VeLO (Metz et al., 2022b), two state-of-the-art learned optimizers in SL. Learning to Optimize for Reinforcement Learning 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Step 1e7 0 1 2 3 4 Return RMSProp Adam Optim4RL (a) ur5e 0 1 2 3 4 Step 1e7 0 2000 4000 6000 8000 10000 Return RMSProp Adam Optim4RL (b) Humanoid 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e8 1000 2000 3000 4000 5000 6000 7000 8000 Return RMSProp Adam Optim4RL VeLO (c) Ant 0 1 2 3 4 5 6 Step 1e8 0 50 100 150 200 250 300 Return RMSProp Adam Optim4RL (d) Grasp \u221215.0 \u221212.5 \u221210.0 \u22127.5 \u22125.0 \u22122.5 0.0 2.5 log(|g| + 10\u221216) 0K 1000K 2000K 3000K 4000K Counts in the bin (e) agent-gradients in ur5e \u221215.0 \u221212.5 \u221210.0 \u22127.5 \u22125.0 \u22122.5 0.0 2.5 log(|g| + 10\u221216) 0K 2500K 5000K 7500K 10000K 12500K 15000K 17500K 20000K Counts in the bin (f) agent-gradients in Humanoid \u221215.0 \u221212.5 \u221210.0 \u22127.5 \u22125.0 \u22122.5 0.0 2.5 log(|g| + 10\u221216) 0K 10000K 20000K 30000K 40000K 50000K Counts in the bin (g) agent-gradients in Ant \u221215.0 \u221212.5 \u221210.0 \u22127.5 \u22125.0 \u22122.5 0.0 2.5 log(|g| + 10\u221216) 0K 500K 1000K 1500K 2000K 2500K Counts in the bin (h) agent-gradients in Grasp Figure 6. (a-d) The optimization performance of Optim4RL in four Brax tasks, averaged over 10 runs with the shaded areas representing standard errors. This Optim4RL only trained in big dense long . The performance of VeLO is estimated based on Figure 11(a) in Metz et al. (2022b), shown as a dashed line. (e-h) The agent-gradients collected during training PPO in each task, optimized by RMSProp. In general, a good optimizer should be well-functioned across the whole training stage. To achieve that, we propose pipeline training. To be speci\ufb01c, we train n agents at the same time, each with its own task and optimizer state. Together they form a training unit (agent, task, optimizer state), and we have n training units in total. Let m be a positive integer we call reset interval. For unit i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, we assign an integer ri to it, where r1, \u00b7 \u00b7 \u00b7 , rn are chosen so that they are evenly spaced over the interval [0, m \u2212 1]. At training iteration t = 0, 1, \u00b7 \u00b7 \u00b7 , we reset the agent-parameters, the task, and the optimizer state of training unit i, where t \u2261 ri (mod m). In Figure 4, we show a special case of pipeline training where m = n = 3. By resetting each training unit at regular intervals, pipeline training guarantees that at any iteration t (e.g., at t = 3, the dashed line in Figure 4), we can get access to training data across one training interval for meta-learning. In practice, we \ufb01nd that pipeline training greatly increases the training stability and ef\ufb01ciency of learned optimizers. We apply pipeline training to meta-learn optimizers in one task and then test the \ufb01xed learned optimizers in this speci\ufb01c task. We compare the optimization performance of different optimizers and plot the learning curves in Figure 5, averaging over 10 runs. In small dense short , Optim4RL and STAR perform quite well, on par with Adam and RMSProp. However, in big dense long , which is a harder task with a larger state-action space than small dense short , STAR fails to optimize effectively, and the return increases slowly as training proceeds. In both small dense short and big dense long , LinearOptim (see Equation 5) and L2LGD2 fail to optimize, proving the importance of the dual-RNN structure in Optim4RL. In Ant, Optim4RL performs similarly to Adam and RMSProp, signi\ufb01cantly outperforming VeLO. In Humanoid, though Optim4RL is no better than RMSProp and Adam, the learning curve has a good increasing trend. Note that due to memory constraint, we only train Optim4RL to optimize a much smaller PPO network than the standard one used in Brax (Freeman et al., 2021). This might be one of the reasons that Optim4RL does not perform well in Humanoid. 6.2. Investigating Generalization Ability of Optim4RL To test the generalization ability of Optim4RL, we apply a learned Optim4RL trained in big dense long from Section 6.1 in four Brax tasks, shown in Figure 6 (a-d). Although the learned Optim4RL is trained in a much simpler task (i.e., big dense long ), it still generalizes to two unseen complex Brax games \u2014 ur5e and Humanoid, but fails in Ant and Grasp. To investigate, we plot the agent-gradients during training PPO in each Brax game in Figure 6 (e-h). Note that all agents are optimized with RMSProp1. Comparing the agent-gradient distributions in Figure 1 and Figure 6 (eh), we \ufb01nd that there is a large distribution overlap between the collected agent-gradients in big dense long and the ones in ur5e. Same for Humanoid but not for Ant or Grasp. This aligns with a straightforward intuition: a learned optimizer is unlikely to perform well in unseen gradient distributions. In other words, a learned optimizer can only generalize to tasks with similar agent-gradient distributions. 1The agent-gradient distributions optimized by Adam are also similar to Figure 6 (e-h). Learning to Optimize for Reinforcement Learning 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Step 1e7 0 1 2 3 4 Return RMSProp Adam Optim4RL (a) ur5e 0 1 2 3 4 Step 1e7 0 2000 4000 6000 8000 10000 Return RMSProp Adam Optim4RL (b) Humanoid 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e8 1000 2000 3000 4000 5000 6000 7000 8000 Return RMSProp Adam Optim4RL VeLO (c) Ant 0 1 2 3 4 5 6 Step 1e8 0 50 100 150 200 250 300 Return RMSProp Adam Optim4RL (d) Grasp 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e8 0 1000 2000 3000 4000 5000 6000 7000 Return RMSProp Adam Optim4RL (e) HalfCheetah 0 1 2 3 4 5 Step 1e7 \u2212120 \u2212110 \u2212100 \u221290 \u221280 \u221270 \u221260 Return RMSProp Adam Optim4RL (f) Pusher 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e8 4 6 8 10 12 Return RMSProp Adam Optim4RL (g) Fetch 0.0 0.2 0.4 0.6 0.8 1.0 Step 1e8 \u2212600 \u2212500 \u2212400 \u2212300 \u2212200 \u2212100 0 Return RMSProp Adam Optim4RL (h) Reacher Figure 7. Optim4RL generalizes to 8 Brax tasks successfully. Note that the performance of VeLO is estimated based on Figure 11(a) in Metz et al. (2022b), shown as a dashed line. All other results are averaged over 10 runs with the shaded areas representing standard errors. Optim4RL is trained in 6 selected gridworlds, showing a strong generalization ability. 6.3. Toward a General-Purpose Learned Optimizer for Reinforcement Learning The insight from the previous section points out a promising approach to learning a general-purpose optimizer for RL: meta-train learned optimizers in multiple RL tasks with various agent-gradient distributions. The key challenge is to design tasks with various agent-gradient distributions. We \ufb01nd that, by changing the reward scale in a gridworld, the agent-gradient distribution in that gridworld would change accordingly. For example, by using a reward function that is 10 times the original reward function, the new agentgradients shift toward large values. So we choose 6 gridworlds and carefully tune reward scales for these tasks such that the union of all agent-gradients covers a large range. We list the selected gridworlds as well as their corresponding reward scales and agent learning rates in Table 1. The scales of meta-gridents generated in different gridworlds are quite different. Following Metz et al. (2022b), we normalize meta-gradients of each task to be unit-length before averaging them across different tasks. We save trained meta-parameters in different training stages and test them in 8 Brax tasks: ur5e, Humanoid, Ant, Grasp, HalfCheetah, Pusher, Fetch, and Reacher. In Figure 7, we show the optimization performance of the best-learned optimizer. All results are averaged over 10 runs, and the shaded areas represent standard errors. Our learned optimizer Optim4RL surpasses VeLO signi\ufb01cantly in Ant. Compared with Adam and RMSProp, Optim4RL outperforms them in Fetch; matches Adam and RMSProp in ur5e, Ant, Grasp, and HalfCheetah. And it performs slightly worse in Humanoid, Pusher, and Table 1. The selected gridworlds with their corresponding reward scales and learning rates (LR) for learning a general-purpose optimizer. Gridworld Reward Scale LR small dense long 1000 1e-3 small dense short 100 3e-3 big sparse short 100 3e-3 big dense short 10 3e-3 big sparse long 10 1e-3 big dense long 1 3e-3 Reacher. In general, Optim4RL is competitive compared with classical human-designed optimizers, even though it is entirely trained from scratch in toy tasks. 7. Conclusion and Future Work In this work, we presented the \ufb01rst general-purpose learned optimizer for RL tasks \u2014 Optim4RL. Optim4RL is designed to have strong inductive biases and a novel network structure, achieving higher training stability and better performance than learned optimizers in SL. We showed that by training Optim4RL entirely from scratch in toy tasks, it is able to generalize to complex RL tasks and matches the performance of classical hand-designed optimizers. Though achieving great success, the current result is still limited since Optim4RL is only trained in a small number of toy tasks. In addition, we only considered two on-policy RL algorithms, A2C and PPO. Similar to Metz et al. (2022b), by ",
    "References": "References Alt, B., \u02c7So\u02c7si\u00b4c, A., and Koeppl, H. Correlation priors for reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019. Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., Shillingford, B., and De Freitas, N. Learning to learn by gradient descent by gradient descent. Advances in Neural Information Processing Systems, 2016. Bengio, E., Pineau, J., and Precup, D. Correcting momentum in temporal difference learning. NeurIPS Workshop on Deep RL, 2020a. Bengio, E., Pineau, J., and Precup, D. Interference and generalization in temporal difference learning. In International Conference on Machine Learning, 2020b. Boull\u00b4e, N., Nakatsukasa, Y., and Townsend, A. Rational neural networks. Advances in Neural Information Processing Systems, 33:14243\u201314253, 2020. Cho, K., van Merri\u00a8enboer, B., Bahdanau, D., and Bengio, Y. On the properties of neural machine translation: Encoder\u2013 decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014. Deng, L. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 2012. Freeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O. Brax - a differentiable physics engine for large scale rigid body simulation, 2021. URL http: //github.com/google/brax. Harrison, J., Metz, L., and Sohl-Dickstein, J. A closer look at learned optimization: Stability, robustness, and inductive biases. In Advances in Neural Information Processing Systems, 2022. Henderson, P., Romoff, J., and Pineau, J. Where did my optimum go?: An empirical analysis of gradient descent optimization in policy gradient methods. In The 14th European Workshop on Reinforcement Learning, 2018. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 1997. Jacobs, R. A. Increased rates of convergence through learning rate adaptation. Neural Networks, 1988. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 2015. Li, K. and Malik, J. Learning to optimize. In International Conference on Learning Representations, 2017. Lu, L., Jin, P., Pang, G., Zhang, Z., and Karniadakis, G. E. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, 2021. Lucas, J., Sun, S., Zemel, R., and Grosse, R. Aggregated momentum: Stability through passive damping. In International Conference on Learning Representations, 2019. Lv, K., Jiang, S., and Li, J. Learning gradient descent: Better generalization and longer horizons. In International Conference on Machine Learning, 2017. Maheswaranathan, N., Sussillo, D., Metz, L., Sun, R., and Sohl-Dickstein, J. Reverse engineering learned optimizers reveals known and novel mechanisms. Advances in Neural Information Processing Systems, 2021. Mahmood, A. R., Sutton, R. S., Degris, T., and Pilarski, P. M. Tuning-free step-size adaptation. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012. Medsker, L. R. and Jain, L. Recurrent neural networks. Design and Applications, 2001. Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., and Sohl-Dickstein, J. Understanding and correcting pathologies in the training of learned optimizers. In International Conference on Machine Learning, 2019. Metz, L., Maheswaranathan, N., Freeman, C. D., Poole, B., and Sohl-Dickstein, J. Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves. arXiv preprint arXiv:2009.11243, 2020a. Metz, L., Maheswaranathan, N., Sun, R., Freeman, C. D., Poole, B., and Sohl-Dickstein, J. Using a thousand optimization tasks to learn hyperparameter search strategies. arXiv preprint arXiv:2002.11887, 2020b. Metz, L., Freeman, C. D., Harrison, J., Maheswaranathan, N., and Sohl-Dickstein, J. Practical tradeoffs between memory, compute, and performance in learned optimizers. In Conference on Lifelong Learning Agents, 2022a. Learning to Optimize for Reinforcement Learning Metz, L., Harrison, J., Freeman, C. D., Merchant, A., Beyer, L., Bradbury, J., Agrawal, N., Poole, B., Mordatch, I., Roberts, A., et al. VeLO: Training versatile learned optimizers by scaling up. arXiv preprint arXiv:2211.09760, 2022b. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, 2016. Oh, J., Hessel, M., Czarnecki, W. M., Xu, Z., van Hasselt, H. P., Singh, S., and Silver, D. Discovering reinforcement learning algorithms. Advances in Neural Information Processing Systems, 2020. Sarig\u00a8ul, M. and Avci, M. Performance comparison of different momentum techniques on deep reinforcement learning. Journal of Information and Telecommunication, 2018. Schraudolph, N. and Sejnowski, T. J. Tempering backpropagation networks: Not all weights are created equal. Advances in Neural Information Processing Systems, 1995. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, 2018. Sutton, R. S. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, 1992. Telgarsky, M. Neural networks and rational functions. In International Conference on Machine Learning, 2017. Tieleman, T. and Hinton, G. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA Neural Networks Neural Networks for Machine Learning, 2012. Vicol, P., Metz, L., and Sohl-Dickstein, J. Unbiased gradient estimation in unrolled computation graphs with persistent evolution strategies. In International Conference on Machine Learning, 2021. Wichrowska, O., Maheswaranathan, N., Hoffman, M. W., Colmenarejo, S. G., Denil, M., Freitas, N., and SohlDickstein, J. Learned optimizers that scale and generalize. In International Conference on Machine Learning, 2017. Xu, C., Qin, T., Wang, G., and Liu, T.-Y. Reinforcement learning for learning rate control. arXiv preprint arXiv:1705.11159, 2017. Xu, Z., van Hasselt, H. P., Hessel, M., Oh, J., Singh, S., and Silver, D. Meta-gradient reinforcement learning with an objective discovered online. Advances in Neural Information Processing Systems, 2020. Yarotsky, D. Error bounds for approximations with deep ReLU networks. Neural Networks, 2017. Learning to Optimize for Reinforcement Learning A. Gridworlds We follow Oh et al. (2020) and design 8 gridwolds. In each gridworld, there are N objects. Each object is described as [r, \u03f5term, \u03f5respawn]. Object locations are randomly determined at the beginning of each episode, and an object reappears at a random location after being collected, with a probability of \u03f5respawn for each time-step. The observation consists of a tensor {0, 1}N\u00d7H\u00d7W , where N is the number of objects, and H \u00d7 W is the size of the grid. An agent has 9 movement actions for adjacent positions, including staying in the same position. When the agent collects an object, it receives the corresponding reward r, and the episode terminates with a probability of \u03f5term associated with the object. In the following tables, we describe each gridworld in detail. Table 2. small sparse short Component Description Size (H \u00d7 W) 3 \u00d7 8 Objects [1.0, 0.0, 0.05], [\u22121.0, 0.5, 0.05] Horizon 50 Table 3. small sparse long Component Description Size (H \u00d7 W) 8 \u00d7 3 Objects [1.0, 0.0, 0.05], [\u22121.0, 0.5, 0.05] Horizon 500 Table 4. small dense short Component Description Size (H \u00d7 W) 4 \u00d7 6 Objects [1.0, 0.0, 0.5], [\u22121.0, 0.5, 0.5] Horizon 50 Learning to Optimize for Reinforcement Learning Table 5. small dense long Component Description Size (H \u00d7 W) 6 \u00d7 4 Objects [1.0, 0.0, 0.5], [\u22121.0, 0.5, 0.5] Horizon 500 Table 6. big sparse short Component Description Size (H \u00d7 W) 10 \u00d7 12 Objects 2 \u00d7 [1.0, 0.0, 0.05], 2 \u00d7 [\u22121.0, 0.5, 0.05] Horizon 50 Table 7. big sparse long Component Description Size (H \u00d7 W) 12 \u00d7 10 Objects 2 \u00d7 [1.0, 0.0, 0.05], 2 \u00d7 [\u22121.0, 0.5, 0.05] Horizon 500 Table 8. big dense short Component Description Size (H \u00d7 W) 9 \u00d7 13 Objects 2 \u00d7 [1.0, 0.0, 0.5], 2 \u00d7 [\u22121.0, 0.5, 0.5] Horizon 50 Table 9. big dense long Component Description Size (H \u00d7 W) 13 \u00d7 9 Objects 2 \u00d7 [1.0, 0.0, 0.5], 2 \u00d7 [\u22121.0, 0.5, 0.5] Horizon 500 ",
    "title": "",
    "paper_info": "Learning to Optimize for Reinforcement Learning\nm\nh1\nRNN1\nMLP1\nh\u2032 1\nv\nh2\nRNN2\nMLP2\nh\u2032 2\ng\n\u0394\u03b8 = \u2212 \u03b1\nm\nv + \u03f5\ng2\nFigure 3. The network structure of Optim4RL.\nspirit, Metz et al. (2022b) continued to perform large-scale\noptimizer training, leveraging more computation (4, 000\nTPU-months) and more diverse SL tasks. The learned opti-\nmizer, VeLO, requires no hyperparameter tuning and works\nwell on a wide range of SL tasks.\nVeLO is the precious outcome of long-time research in the\narea of learning to optimize, building on the wisdom and ef-\nfort of many generations and a large amount of computation.\nAlthough marking a milestone for the success of learned\noptimizers in SL tasks, VeLO still performs poorly in RL\ntasks, as shown in Section 4.4.4 in Metz et al. (2022b). The\nfailure of VeLO in RL tasks suggests that designing learned\noptimizers for RL tasks is still a challenging problem.\nNote that all previously mentioned works focus on training\nlearned optimizers for SL. Our work is inspired by them but\nfocuses on learning to optimize for RL. Compared to these\nworks, our method (i.e., Optim4RL) is simple, stable, and\neffective, without using complex neural network structures\nor incorporating numerous human-designed input features.\nAs far as we know, our work is the \ufb01rst to demonstrate the\nsuccess of learned optimizers in deep RL.\n4. Learning to Optimize in RL is Much\nHarder\nIn supervised learning (SL), a training dataset consists of\npairs {(xi, yi)} where xi is the input and yi is the label\nfor input xi. The goal is to train a model that maps the\ninput data to the correct labels. During training, a mini-\nbatch is sampled from the dataset and fed into the model\nto generate predictions \u02c6ys for input xs. A loss is computed\ngiven the predicted labels \u02c6ys and the true labels ys. A\nlower loss usually indicates better performance (e.g., higher\nclassi\ufb01cation accuracy). The parameters of this model are\nthen optimized given the loss function with gradient descent\nmethods. Note that in the training process, it is assumed\nthat the training set consists of i.i.d. samples. Moreover,\nthe true labels ys are usually noiseless and stationary (i.e.,\ntime-invariant). For example, the true label of a written digit\n2 in MNIST (Deng, 2012) is always y = 2 which does not\nchange during training.\nHowever, in reinforcement learning (RL), the input train-\ning data distribution is non-i.i.d., which makes the whole\ntraining process much more unstable and complex, espe-\ncially when learning to optimize is involved. Compared\nwith SL, one signi\ufb01cant difference in RL training is that\nthe agent-environment interactions change input data distri-\nbution, such as visited states and rewards. TD learning is\nwidely used in RL, and TD targets (see Equation 1) in RL\nplay a similar role as labels in SL. However, unlike labels\nin SL, TD targets are usually biased, non-stationary, and\nnoisy due to changing state-values, complex state transi-\ntions, and noisy reward signals. This leads to a changing\nloss landscape that evolves as the policy and the state-value\nfunction change. Unlike SL, a lower loss is not necessarily\na good indicator of better performance (i.e., higher return)\nin RL for this reason. The randomness from state transi-\ntions, reward signals, and agent-environment interactions,\ntogether with biased TD targets, makes the bias and variance\nof agent-gradients relatively high.\nLearned optimizers for SL are infamously hard to train,\nsuffering from high training instability (Wichrowska et al.,\n2017; Metz et al., 2019; 2020a; Harrison et al., 2022). Learn-\ning an optimizer for RL tasks is even harder, due to the high\nbias and variance of agent-gradients in RL. During learn-\ning to optimize in RL, meta-gradients are af\ufb02icted with\nlarge noise induced by the high bias and variance of agent-\ngradients. Since meta-gradients are noisy and inaccurate,\nthe improvement of the learned optimizer becomes unstable\nand slow. With a poorly performed optimizer, the agent\npolicy improvement is no longer guaranteed. Due to the\nagent-environment interactions, a poor agent policy is un-\nlikely to collect \u201chigh-quality\u201d data to boost the performance\nof both the agent and the learned optimizer. In the end,\nthis two-level optimization gets stuck in a vicious spiral:\na poor optimizer \u2192 a poor agent policy \u2192 collected data\nof low-quality \u2192 a poor optimizer \u2192 \u00b7 \u00b7 \u00b7 . Note that this\nphenomenon also occurs to learning to optimize in SL. It\nis the agent-gradients of high bias and variance in RL and\nthe agent-environment interactions that make learning to\noptimize in RL much harder.\n5. Optim4RL: A Learned Optimizer for RL\nTo overcome the hardness of learning an optimizer in RL,\nwe introduce a dual-RNN structure optimizer, named Op-\ntim4RL, that is more robust and sample-ef\ufb01cient than previ-\nous methods. To achieve this, we \ufb01rst employ a technique\n",
    "GPTsummary": "\n                    - (1): The article aims to address the problem of unstable and complex loss landscapes in reinforcement learning and the lack of optimization methods specifically designed for reinforcement learning.\n\n\n                    - (2): Past methods for optimization are classical hand-designed optimizers and learned optimizers, but they fail to generalize to reinforcement learning tasks. The authors propose to learn to optimize for reinforcement learning using meta-learning. They introduce a novel optimizer structure that significantly improves the training efficiency of learned optimizers, making it possible to learn an optimizer for reinforcement learning from scratch. \n\n\n                    - (3): The authors take a data-driven approach to learn to optimize for reinforcement learning using meta-learning. They employ a meta-network which is updated during meta-learning to achieve better optimization. They design a set of small gridworlds to train the first general-purpose optimizer for reinforcement learning.\n\n\n                    - (4): The proposed method is evaluated on various reinforcement learning tasks and achieves improved performance compared to hand-designed optimizers and baseline methods. The performance supports the authors\u2019 goals of addressing the problem of unstable and complex loss landscapes in reinforcement learning and the lack of optimization methods specifically designed for reinforcement learning.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to propose a novel meta-learning approach to learning to optimize for reinforcement learning tasks, which addresses the problem of unstable and complex loss landscapes and the lack of optimization methods specifically designed for reinforcement learning. The proposed method significantly improves the training efficiency of learned optimizers, making it possible to learn an optimizer for reinforcement learning from scratch.\n\n- (2): Innovation point: The article proposes a novel meta-learning approach to learning to optimize for reinforcement learning using a meta-network, which significantly improves the training efficiency of learned optimizers. Performance: The proposed method achieves improved performance compared to hand-designed optimizers and baseline methods on various reinforcement learning tasks. Workload: The article requires a significant workload for designing a set of small gridworlds to train the first general-purpose optimizer for reinforcement learning, but the proposed method shows promising results for future research in this area.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to propose a novel meta-learning approach to learning to optimize for reinforcement learning tasks, which addresses the problem of unstable and complex loss landscapes and the lack of optimization methods specifically designed for reinforcement learning. The proposed method significantly improves the training efficiency of learned optimizers, making it possible to learn an optimizer for reinforcement learning from scratch.\n\n- (2): Innovation point: The article proposes a novel meta-learning approach to learning to optimize for reinforcement learning using a meta-network, which significantly improves the training efficiency of learned optimizers. Performance: The proposed method achieves improved performance compared to hand-designed optimizers and baseline methods on various reinforcement learning tasks. Workload: The article requires a significant workload for designing a set of small gridworlds to train the first general-purpose optimizer for reinforcement learning, but the proposed method shows promising results for future research in this area.\n\n\n"
}