{
    "Abstract": "Abstract    A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,                           providing a general purpose learning machine. By reference to a node threshold three features are                             described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)                          The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of                            forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning                               algorithm, providing a robust and biologically inspired alternative to backpropagation. The scheme may be                           used for supervised as well as unsupervised training regimes.    1 Introduction    This paper proposes that a general purpose learning machine can be achieved by implementing Reinforcement                              Learning in an Artificial Neural Network (ANN), three interdependent methods which attempt to emulate the core                                mechanisms of that process are presented. Ultimately the biological plausibility of this scheme may be validated by                                  reference to natural organisms. However that does not preclude the possibility that there is more than one underlying                                   mechanism providing Reinforcement Learning in nature.     AI research has characteristically followed a bottom-up approach; focusing on subsystems that address distinct,                            specialized and unrelated problem domains. In contrast the work presented follows a distinctly top-down approach                              attempting to model intelligence as a whole system; a causal agent interacting with the environment [6]. The agent is                                  not designed to solve a particular problem, but is instead assigned a reward condition. The reward condition serves                                  as a goal, and in the path a variety of unknown challenges may be present. To solve these problems efficiently the                                        agent requires intelligence.    This top-down approach assumes that the core self organizing mechanisms of learning that exist in natural                                 organisms can be replicated in artificial autonomous agents. These can then be scaled up by endowing the agent with                                      more resources (sensors, neurons & motors). Given sufficient resources and learning opportunities an agent may                               provide an efficient solution to a problem provided one exists. Also given the generalization properties of ANN\u2019s                                 the agent can provide appropriate responses to novel stimuli.     A distinction is made between supervised, unsupervised and reinforcement training regimes. Supervised learning                          regimes use a (human) trainer to assign desired input-output pattern pairings. Unsupervised training regimes are                              typically used to cluster a data set into related groups. Reinforcement Learning (RL) may be considered a subtype of                                   unsupervised training; it is sometimes called learning with a critic rather than learning with a teacher as the feedback                                  is evaluative (right or wrong) rather than instructive (where a desired output action is prescribed). Significant RL                                 successes have been achieved with the use of Temporal Difference (TD) methods [5][7], notably Q-learning[2].     1  First a definition of intelligence is required:    The demonstration of beneficial behaviors acquired through learning.    A beneficial action/behavior being one that would result in a positive survival outcome (eg successful feeding,                                mating, self preservation) for the agent. For the most part our inherent internal reward systems encourage us to                                     perform beneficial behaviors, but this is not always the case (eg substance abuse may be rewarding but not                                     beneficial). The term \u2018desirable behavior\u2019 is avoided due to existing usage of the term \u2018desired output\u2019 in supervised                                 learning schemes.    Let\u2019s revise our definition, and expectation, of intelligence:    The demonstration of rewarding behaviors acquired through learning.    Rewarding behaviors/actions will be selected for reinforcement (ie learnt) over non rewarding ones. Rewarding                             behaviors are those that allow the agent to achieve the reward condition, thereby achieving its goal(s) in an                                     acceptably efficient manner (eg elapsed time, steps taken, energy expended). Rewarding behaviors may lead to                               pleasure, or at least a reduction in pain. Goals are attained by achieving the pre-established reward condition, and                                  thereby satiating active desire(s). Behaviors need not be active they may be passive; inaction may lead to reward and                                      therefore be reinforced.    From initial state \u200bst if action \u200bat results in an immediate reward in the subsequent state \u200bs \u200b t+1 \u200b , action \u200ba \u200b t will be                                          reinforced. If the same (or similar from generalization) input pattern is encountered the learnt action will be                                   performed. This process of learning is termed Primary Reinforcement. Primary Reinforcement reward conditions (eg                           hunger or thirst) typically drive some form of homeostatic, adaptive control function for the agent [5][8].      This is in contrast to Secondary/Conditioned Reinforcement where from initial state \u200bs \u200b t action \u200ba \u200b t does not result in                                  immediate reward in subsequent state \u200bst+1. If later action \u200bat+1 does result in a reward in state \u200bs \u200b t+2 \u200b , this will lead to                                        reinforcement of actions \u200bat+1 and \u200bat. The number of actions learnt from start to goal is arbitrary and depends on the                                        relative size of the reward relative to the cost (or pain) in attaining it.     A learning scheme is presented that enables an embodied neural network agent to autonomously determine and learn                                 desirable behaviors. The agent may be embodied in a real or artificial environment. Artificial environments may be                                modelled on physical environments or even abstract problem domains. The environment may be any set of input                                   patterns, however there must be a causal relationship between the output (behavior) of the agent and the subsequent                                   input pattern.     The common theme of this work is the application of thresholds to neural network activation functions, which                                  thereby inform the learning process. Whilst the use of thresholds is by no means novel, mainstream learning                                   methods are not heavily reliant upon them. By contrast the three presented features are all strictly dependent on the                                       presence of a threshold.     Thresholds for neuron activation are widely found in animal cells, where sufficient \u2018excitation\u2019 is required to result                                in an electrical action potential or \u200b\u2018spike\u2019 that can be signalled to other neurons [11]. Activations exceeding the                                     threshold represent the occurrence of one or more spikes, with stronger activations representing sustained \u200b\u2018spike                               trains\u2019 (fig 1).   2    fig1 Logistic Activation Function with threshold    This paper presents three features:    1. A Primary Reinforcement learning scheme is achieved by wrapping a (supervised) backpropagation                        network within an unsupervised framework.     Primary Reinforcement enables \u2018desirable\u2019 actions (behaviors) to be autonomously generated and learnt;                         this is of particular benefit when a human supervisor is not available / does not know what the desired                                   output should be, or when the environment, or agent itself,  is changeable.    2. The framework is then extended to provide Conditioned (Secondary) Reinforcement.    Conditioned (Secondary) Reinforcement enables an arbitrarily long sequence of chained behaviors to be                         autonomously learnt, in expectation of a primary reward; this provides long term strategy.    3. An algorithm is described, termed Threshold Assignment of Connections (TAC), that replaces                         backpropagation within the framework, conversely this algorithm can also be used in supervised training                            schemes.    The Threshold Assignment of Connections algorithm provides a biologically inspired alternative to                        backpropagation.      The examples that follow are focused mainly on target tracking, however this approach may be applied to a wide                                     range of real and abstract problem spaces. The tasks are small in scale and primarily serve as proof of concept.                                         While learning rate performance has been documented, the scheme has not been performance tuned. The intent of                                   this work is to establish a biologically plausible working model of intelligence that is simple, scalable and generic.       2 Primary Reinforcement    In this section a learning scheme is described that provides Primary Reinforcement learning in an Artificial Neural                                Network. Weight adjustments using backpropagation[3] are traditionally used in supervised learning schemes; that                          is desired activations (or training sets) are established prior to the learning phase. In this example backpropagation                                 will be used in an unsupervised learning scheme; desired actions will be generated on-the-fly. This approach,                                 termed \u200bThreshold Assignment of Patterns (TAP)\u200b, relies on a threshold in the output layer to determine the                                  desired output pattern.   3  Unlike mainstream methods, the presented scheme will create it\u2019s own actions \u200bde novo rather than relying on a                                    predefined set. This provides a neural explanation of action selection and enables neural adaption should the causal                                 relationship between the agent and environment alter (eg damaged motors, icy surfaces).  Although reward results in reinforcement the scheme is also stochastic; punishment results in new candidate                              behaviors being randomly generated. Sensing utilizes a sensory input pattern and behavior arises from a motor                               output pattern. Thinking (or processing) is implemented via layers of artificial neurons.  The learning scheme consists of the following components:    \u25cf Artificial Neural Network  \u25cf Learning Algorithm  \u25cf Framework   \u25cf Environment    The Artificial Neural Network architecture is that of a familiar multilayer perceptron (MLP)[3]. An input (sensor)                               pattern representing the environment, produces activations that feed forward and result in an output (motor) pattern                                representing a behavior.     The Learning Algorithm used to derive weight updates is the standard (supervised) back propagation learning                               algorithm[3][10].     The Framework sits between the network and environment. The framework acts as an interface between network                                 and environment, and is responsible for establishing a reward condition as Primary Reinforcer and determining                              (desired output) behavior based on that reward condition. The framework, network and learning algorithm                             constitute the agent.     The Environment may be real or simulated. Simulated environments consist of states, and physics rules which                                 define the relationship between states. The physics rules take the current (t) network output and determine which                                   input pattern is next (t+1) presented to the network.             fig2 State \u2018t\u2019      fig3  State \u2018t+1\u2019  Backpropagation requires a set of desirable output patterns to be established. Through learning the desired output                                patterns will inform the network what the required output node activations are for each input pattern. In this scheme                                       4  desired output patterns will be dynamically generated on-the-fly. But how can the agent determine potentially                               complex desired output patterns from a simple yes/no reward condition?    Desired activation values are set depending on whether a reward or punishment occurred after the output response. If                                  a reward condition occurred all actual activations above threshold (eg fig 4 node 1) will be reinforced by setting the                                       corresponding desired activation to 1.0 and all activations below threshold (eg fig 4 node 2) will be reinforced by                                      setting the corresponding desired activation to 0.0. If punishment (no reward) occurred all actual activations, above                               or below threshold, will be weakened by setting desired activations to near threshold values.    fig4   Desired activation : Punishment vs Reward    The desired activation of an output layer node can be determined by reference to the actual activation of the      du(t)                                reward node   in conjunction with the actual activation of the output layer node  (equation 1). ar(t+1) au(t)      In reward conditions (reward threshold is achieved), the output node activation will be strengthened; a          \u03b8r            au(t)        maximal desired activation will be set if the output node threshold was achieved and a minimal value if it was                     \u03b8u                not.     In punishment conditions (reward threshold is not achieved), the output node activation will be weakened by          \u03b8r                       assigning it a random moderate desired activation, regardless of whether the output node threshold was achieved.        The agent will then learn by trial and error according to the following process (Box 1):    5  Processing steps  1. Input pattern for state \u2018t\u2019 is presented and forward propagated through the network (Box 2).                              Node output activation may be in the range 0-1 (Box 3). A threshold value of 0.5 is assigned                                 to each node in output layer.  2. Agent\u2019s behavior, based on whether output layer activations have exceeded threshold values                         (fig 1), determines subsequent input pattern \u2018t+1\u2019.  3. Framework determines \u2018reward\u2019 value based on \u2018t+1\u2019 input pattern.  4. Weight changes are made:  a. Desired \u2018t\u2019 activations at the output layer are calculated:  i. If reward condition then the desired \u2018t\u2019 activation for that node is set to                            maximal value (1.0) for those that were above threshold, and set to                         minimal value (0.0) for those that were below threshold.  ii. If no reward then the desired \u2018t\u2019 activation for all nodes are random                           moderate values [0.45..0.55].  b. Weight changes for state \u2018t\u2019 are made according to error values which are back                             propagated through the network (Box 4).    Box 1 Threshold Assignment of Patterns processing steps    Input activation for unit u.  =  etinput n u \u2211   i eight w ui ai       Box 2  Input activation    Output activation for unit u.  =  au 1 1 + e\u2212netinputu       Box 3  Logistic Activation Function    1) Derive desired activation for output unit u.    6        2) Derive delta error value for an output layer node u, by finding difference between desired activation ( ) and                                 du     actual activation ( ). au   =( ) ) elta d u du au au 1 ( \u2212au     3) Derive weight change for connection between a hidden unit h and an output unit u, using learning rate.    =  weight \u0394 uh rate delta l u ah     4) Derive delta error value for a hidden unit h, using weighted sum of all units in output layer .    =  elta d h ah 1  a ) ( \u2212 h \u2211   u elta d u eight w uh     5) Derive weight change for connection between an input unit i and a hidden unit h, using learning rate.    =  weight \u0394 hi rate delta l h ai       Box 4  Backpropagation weight update  In this way randomized desired out patterns will generate a new candidate behavior on the next presentation of that                                     same (or similar) stimulus. In effect the response has been established before it is first manifested, and this                                    rewarding response will be reinforced on future presentations. Conversely non-rewarding behaviors will be                           destroyed in favour of a new candidate behavior. Akin to natural selection, only rewarding behaviors will survive.    A behaviour (ie selected action) is represented by a generated output pattern of activity. Since output should be                                   considered at motor rather than functional level, representations may be distributed rather than winner-take-all. The                              actual function of an action is determined by the causal relationship between the agent and environment. The output                                    pattern will be determined by simultaneous excitation and inhibition arising from an input stimulus. In this sense the                                    roles of \u200bSense-Think-Act are tightly coupled.    A mapping is formed from an input pattern stimuli to an output pattern motor response, dependent on the reward                                     that follows. No distinction is made between learning and testing phase; that is the agent in a continual process of                                     learning and evaluation. A behavior is deemed to have been learnt when all output node activations are mature (eg                                     above 0.9 or less than 0.1) for a given input pattern and results in a reward.  2.1 Example: Target tracking (part I)    2.1.1 Problem description  In this example the agent must autonomously learn how to track a target (fig 5).  7    fig5 Target tracking example start state    \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                          by moving it to the centre cell.  \u25cf The network is rewarded only if it moves the target to it\u2019s centremost cell. Once the target is moved to the                                     centremost cell it is moved to a new starting position on the grid.  \u25cf The agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target                                     until it does so.  \u25cf The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are                                      required.     2.1.2 Network configuration  A network was created (table 1)(fig 6):    Input layer  9 input nodes; the centre node is designated as a special \u2018reward\u2019                       node  Hidden layer  12 hidden nodes  Output layer  4 motor nodes  Notes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0    Table 1 Target tracking network configuration  8    fig6 Target tracking network topology    The nine input nodes are mapped to each cell in the grid (fig7):    fig7 Target tracking input mapping  The network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                   the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                              there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                     Therefore there is only a 1-in-16 chance (2\u200b4\u200b) the agent will move in the correct direction by chance.  The framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                        zero if it is to be punished. A reward occurs if the target is moved to the centre cell (fig 8):          9  fig8 Target tracking reward condition  The behavior, or output response of the network, will causally influence the subsequent input pattern. Reward                                 feedback will be given to the network, thereby informing whether the output was \u2018correct\u2019. The output patterns are                                    not predetermined. The network is presented a pattern and produces a response behavior. If the behavior was                                rewarding then a stronger version of the actual output is assigned as the desired output pattern. If the behavior was                                      not rewarding a random pattern is set as the desired output pattern. A threshold is required at the output layer to                                     make this decision.    2.1.3 Results    Initially the agent moves the target randomly. Activations tend to be weak (eg 0.4 - 0.6) across all output nodes.                                       When rewarding behaviors are discovered these are further reinforced and the output matures (eg < 0.1, > 0.9).    With sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one step                                 left/right/up/down towards the food reward from starting locations (cells 1,3,5,7) (table 2). If the target is placed                                 directly on the centre cell it will remain stationary.      Scheme  # pattern presentations  Threshold Assignment of Patterns  (Unsupervised backpropagation)  116816    Table 2 Target tracking results  However, the agent is unable to learn how to move the target when placed in corners (cells 0,2,6,8). The physics                                      rules prevent the agent from moving the target diagonally in one step, instead two steps are required. Whilst the                                       agent was able to solve this \u20181-step\u2019 solution in 116816 presentations, it spent much of it\u2019s time temporarily \u2018stuck\u2019                                   in corner cells. The agent can only reinforce behaviors where there is an immediate reward in the subsequent input                                     pattern. The network is unable to solve the \u2018temporal credit assignment\u2019 problem. In order to learn two or more                                      consecutive behaviors secondary (conditioned) reinforcement is required (fig 9).          fig9 Primary Reinforcement partial solution for target tracking task  Note; In this example the reward condition was facilitated by assigning an existing input layer node as the \u2019Reward                                      Node\u2019. However, the reward condition could be evaluated against a combination of existing input layer nodes, a                                 separate input layer node(s), or even no node at all (see XOR example below).     2.2 Example: XOR  2.2.1 Problem description  The agent will be required to solve the XOR problem (table 3) in order to test the network's ability to map non linear                                          10  transformations that require multiple layers of neurons. This test will will also provide a performance comparison                                between unsupervised reinforcement learning and the supervised regime.       (input)  A  (input)  B  (output)  XOR  0  0  0  0  1  1  1  0  1  1  1  0    Table 3 XOR task  Any input pattern can be considered an environment, and any output pattern a behavior. Thus any set of mappings                                     can be learnt as they would under a conventional supervised learning regime. In contrast to the previous experiment,                                  the network will now receive controlled exposure to all input patterns in turn. The behavior, or output response of                                     the network will not influence the subsequent input pattern. However, reward feedback will be given to the network                                    thereby informing whether the output was correct according to the desired pattern in the supervised training set. This                                   tightly controlled presentation of input patterns is termed \u2018guided\u2019.    For clarity, and to allow for a close comparison with the supervised backpropagation training regime, no explicit                                  reward node will be established in the network topology. The framework is still responsible for setting the reward                                    condition. To allow exposure to all the input patterns they will be cycled through in sequence. The framework will                                      evaluate the output and decide whether it should be reinforced or not.    2.2.2 Network configuration    A network was created (table 4)(fig 10):  Input layer  2 input nodes  Hidden layer  3 hidden nodes  Output layer  1 output node  Notes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0    Table 4 XOR network configuration    11      fig10 XOR network topology    2.2.3 Results      Scheme  # pattern presentations  Supervised backpropagation  2182  Threshold Assignment of Patterns   (unsupervised backpropagation)  7550    Table 5 XOR results  The Threshold Assignment of Patterns (unsupervised backpropagation) regime required significantly more pattern                         presentations to learn the XOR solution than the supervised regime (table 5). Both were using the same set of initial                                       weights and hyperparameters (learning rate etc). Both are using the same backpropagation algorithm to perform                               weight updates. The difference in performance can be attributed to the manner in which desired output patterns are                                  provided. In the supervised scheme the desired patterns are known \u200ba priori, in the unsupervised (guided) learning                                 regime these must be explored by the network through trial and error. This performance gap would be expected to                                      widen as the number of nodes in the output layer grows and with it the number of candidate combinations.    2.3 Summary   Unsupervised Primary Reinforcement can be achieved, with reference to a threshold, by dynamically generating                            desired output activations and feeding these into a supervised learning algorithm. These experiments demonstrate                            the network's ability to learn and map arbitrary sets of patterns by reward, thereby producing behaviors that allow it                                    to reach it\u2019s goal in an efficient manner. Primary Reinforcement may solve problems that are that are linearly                                     inseparable. It is capable of training weights deep within the network, thereby capable of forming complex abstract                                representations.  The input patterns can be presented in any order. However, in order for the network to identify which output                                       patterns (or behaviors) were correct, the network must be presented with subsequent reward feedback. The                               unsupervised Reinforcement Learning regime requires more iterations to learn compared to the supervised regime.                           The principal reason being that the unsupervised agent must explore the \u2018correct\u2019 solution through trial and error.                                   And even when agent does not receive reinforcement, the new random candidate behavior may be a repeat of a                                     prior incorrect one.  12  Primary Reinforcement may only learn a single behavior sequence, therefore it is not suitable for acquiring long                                term strategy that lead to distal rewards. Problems requiring long term strategy must use Secondary/Conditioned                               Reinforcement.    3 Secondary/Conditioned Reinforcement  Primary reinforcement provides a generic method of autonomously establishing beneficial output responses. But it                           has a significant limitation; it can only provide a one step mapping from start state(s) to goal. A more useful feature                                      is the ability to establish an efficient series of behavior steps leading from start state(s) to a goal. This is the benefit                                         provided by Conditioned Reinforcement. The term \u2018Conditioned\u2019 Reinforcement is preferred over that of                           \u2018Secondary\u2019 Reinforcement, since the latter may imply chained behaviors only 2 steps deep. In fact the number of                                  chained behaviors can be arbitrarily deep, depending on the strength of the reward and subsequent reduction                                 (discount factor) applied to it. This approach, termed \u200bThreshold Assignment of Rewards (TAR)\u200b, relies on                               building an association between a rewarding stimulus and an internal proxy reward.  In Primary Reinforcement a mapping is established between an input pattern stimuli to an output pattern motor                                 response. In Secondary Reinforcement a mapping is established between an input pattern stimuli to an output                               pattern motor response AND a reward node. With sufficient reinforcement the response activation on the reward                                node matures; the reward node is now available as a proxy reward condition (secondary/conditioned reinforcer) for                              a given input pattern. In turn this conditioned reinforcer can help to create further conditioned reinforcers, that are                                    activated only in response to a recognized input pattern stimuli.  The first conditioned response to be learnt will be closest to the primary reinforcer. Thereafter a chain of                                    conditioned reinforcers can be established; via backward induction a series of \u2018breadcrumbs\u2019 are laid out in reverse                                from the goal. Using this mechanism planned or goal oriented tasks can be solved \u2018model-free\u2019. The mapped                                   reward node provides a \u200bstate-value function for the agent. This mechanic resembles action-value mappings derived                              from \u200bsample-backups in SARSA and Q-learning methods[5][2]. However, in the present scheme state-action                           mappings are dealt with separately (TAP), and can be readily decoupled should an \u200bactor-critic architecture be                                 employed [8]. Also while TD methods achieve exploration via (\ud835\udf3a-greedy) probability, in the presented scheme                               exploration occurs when the agent fails to obtain a reward that satisfies a threshold value. If the agent is pursuing a                                      suboptimal policy (path) to the goal, the threshold can be raised until the optimal path is found. Risky exploration                                      can therefore be avoided unless required.  This approach is intended to overcome the temporal \u2018hill climbing\u2019 limitation outlined in the previous example. It                                 is based on the a similar architecture and learning rule as before but with one important addition: a special reward                                      node is added to the output layer. Unlike other nodes in the output layer the special reward node is not a motor                                         neuron. Once this becomes mature it behaves like an input reward node; deciding which behaviors should be learnt.                                    Behaviors leading to a distant reward can be chained together.    The prior desired activation of an output layer reward node can be determined by reference to the subsequent        du(t)                            actual activation of the output layer reward node  (equation 2). au(t+1)     In reward conditions (reward threshold is achieved), the output reward node activation will be assigned the          \u03b8u              au(t)         product of the subsequent actual activation of the output layer reward node  and discount factor . au(t+1) \u03b3      In punishment conditions (reward threshold is not achieved), the output reward node activation will be assigned a          \u03b8u                        minimal value.       13    Essentially mapping are now formed between input patterns and rewards, rather than just input patterns and motor                                   nodes (Box 5).    Processing steps    Differences to the Primary Reinforcement process described previously are highlighted in bold.    1. Input pattern for state \u2018t\u2019 is presented and forward propagated through the network (Box 2).                              Node output activation may be in the range 0-1 (Box 3). A threshold value of 0.5 is assigned                                 to each node in output layer.  2. Agent\u2019s behavior, based on whether output layer activations have exceeded threshold values                         (fig 1), determines subsequent input pattern \u2018t+1\u2019.  3. Framework determines \u2018reward\u2019 value based on \u2018t+1\u2019 input pattern \u200band on activation of                           special output reward node exceeding reward threshold  (eg > 0.8 )\u200b.  4. Weight changes are made:  a. Desired \u2018t\u2019 activations at the output layer are calculated:  i. If reward condition then the desired \u2018t\u2019 activation for that node is set to                            maximal value (1.0) for those that were above threshold, and set to                         minimal value (0.0) for those that were below threshold. \u200bSet the desired                         activation for the special output reward node to discount (eg 95%) of                         reward value.  ii. If no reward then the desired \u2018t\u2019 activation for all nodes are random                           moderate values [0.45..0.55]. \u200bSet the desired activation for the special                     output reward node to minimal value (0.0).  b. Weight changes for state \u2018t\u2019 are made according to error values which are back                             propagated through the network (Box 6).    Note: To achieve this both the current and previous activations must be stored. Weight changes are  derived using back propagation on previous activations.    Box 5 Threshold Assignment of Rewards processing steps      1) Derive desired activation for reward output unit u.  14      2) Derive delta error value for an output layer node u, by finding difference between desired activation ( ) and                                 du     actual activation ( ). au   =( ) ) elta d u du au au 1 ( \u2212au     3) Derive weight change for connection between a hidden unit h and an output unit u, using learning rate.    =  weight \u0394 uh rate delta l u ah     4) Derive delta error value for a hidden unit h, using weighted sum of all units in output layer .    =  elta d h ah 1  a ) ( \u2212 h \u2211   u elta d u eight w uh     5) Derive weight change for connection between an input unit i and a hidden unit h, using learning rate.    =  weight \u0394 hi rate delta l h ai     Box 6  Backpropagation weight update  3.1 Example: Target tracking (part II)  The same problem as described in section \u200b3.1.1 is revisited. In this example the agent is equipped with the                                       resources required to facilitate Secondary (Conditioned) Reinforcement.    3.1.1 Problem description  In this example the agent must autonomously learn how to track a target (fig 11).    fig11 Target tracking example start state    \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                          by moving it to the centre cell.  \u25cf The network is rewarded only if it moves the target to it\u2019s centremost cell. Once the target is moved to the                                     centremost cell it is moved to a new starting position on the grid.  \u25cf The agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target                                     until it does so.  \u25cf The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are                                      15  required.     3.1.2 Network configuration  A network was created (table 6)(fig 12):  Input layer  9 input nodes; the centre node is designated as a special \u2018reward\u2019                       node  Hidden layer  12 hidden nodes  Output layer  4 motor nodes + 1 reward node  Notes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0  Table 6 Target tracking network configuration        fig12     The nine input nodes are mapped to each cell in the grid (fig 13):  16    fig13 Target tracking input mapping  The network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                   the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                              there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                     Therefore there is only a 1-in-16 chance (2\u200b4\u200b) the agent will move in the correct direction by chance.  The framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                        zero if it is to be punished. A reward occurs if the target is moved to the centre cell (fig 14):      fig14 Target tracking reward condition  The network is also assigned an additional special reward node in the output layer. A reward also occurs if the  activation of this output reward node is above the reward threshold (eg 0.8), which indicates it has been previously  reinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the  output reward node and motor output nodes will be reinforced.    Note; In this example a discrete special reward node was added to the output layer. It is possible to to have no                                         dedicated reward node at output layer, instead it possible to test if activations of all motor neurons are high (and                                      have therefore matured). Consequently activations, connections and therefore the strength of memories will be                             proportional to the reward.    3.1.3 Results    Initially the agent moves the target randomly. Activations tend to be weak (eg 0.4 - 0.6) across all output nodes.                                       When rewarding behaviors are discovered these are further reinforced and the output matures (eg < 0.1, > 0.9).      Scheme  # pattern presentations  Threshold Assignment of Reward  (Unsupervised backpropagation)  110324    Table 7 Target tracking results  17          fig15 Conditioned Reinforcement full solution for target tracking task  The agent is able to learn how to move the target when placed in corners (cells 0,2,6,8) (table 7). The agent has                                           effectively established proxy rewards in intermediate locations (cells 1,3,5,7) allowing chained sequences of                           behaviors to be learned (fig 15). The chaining of sequences of behaviors enables long term strategy to be acquired,                                     this is demonstrated more substantially in the following maze navigation problem.  3.2 Example: Maze navigation  3.2.1 Problem description  In this example the agent must autonomously learn how to navigate a target through a maze (fig 16).      fig16 Maze task start state  \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                          by moving it the top centre cell.  \u25cf Once the target is moved to the target cell (cell 1) it is moved back to it\u2019s original starting position on the                                        grid (cell 3).  \u25cf The agent has no prior knowledge. It does not know that food will lead to a reward until it happens.  \u25cf The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are                                      required.   \u25cf To further increase task difficulty there is an invisible barrier between the start state and the goal. The agent                                      must learn to navigate around the barrier.    3.2.2 Network configuration    A network was created (table 8) (fig 17):  Input layer  9 input nodes; the centre node is designated as a special \u2018reward\u2019                       node  18  Hidden layer  12 hidden nodes  Output layer  4 motor nodes + 1 reward node  Notes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0    Table 8 Maze task network configuration    fig17 Maze task network topology      The nine input nodes are mapped to each cell in the grid (fig 18):    fig18 Maze task input mapping  The network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                   the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                             19  there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                     Therefore there is only a 1-in-16 chance (2\u200b4\u200b) the agent will move in the correct direction by chance.  The framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                       zero if it is to be punished. A reward occurs if the target is moved to the top centre cell (cell 1).    The network is also assigned an additional special reward node in the output layer. A reward also occurs if the  activation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously  reinforced. Connections to the output reward node will be reinforced in the same manner as motor output  nodes if  the subsequent input reward node or subsequent output reward node is activated.      3.2.3 Results    Initially the network moves randomly through the environment. In time the agent is able to learn how to move the  target around corners. The physics rules prevent the agent from moving the target diagonally in one step, instead two  steps are required. The agent can only reinforce behaviors where there is an immediate reward in the next input  pattern. The agent has effectively established proxy rewards, or sub-goals, in intermediate locations (cells 2,5,4)  allowing chained sequences of behaviors to be learned (fig 19-22).          fig19 Agent encounters the Primary  Reinforcer and learns first  behavior.    fig20 Agent establishes first  Conditioned Reinforcer.      fig21 Agent establishes a subsequent  Conditioned Reinforcer by  reference to the the initial  Conditioned Reinforcer.    fig22 Agent establishes a series of  Conditioned Reinforcers from  goal to starting position.        With sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one step                                 left/right/up/down towards the goal from the starting location (cell 3).     The chaining of sequences of behaviors enables long term strategy to be acquired (table 9).        Scheme  # pattern presentations  Threshold Assignment of Reward  (Unsupervised backpropagation)  80184    Table 9 Target tracking results    Learning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once learnt  the distance may be increased.    20  3.3 Summary  A considerable challenge facing Reinforcement Learning schemes is that rewards can be very temporally delayed.  Secondary/Conditioned Reinforcement provides an effective solution to this \u2018Temporal\u2019 Credit Assignment  Problem. The reward condition was facilitated by adding single reward node to the output layer in addition to  evaluating the reward at the input layer. The output reward desired activation experiences \u2018reduction\u2019 as it becomes  temporally distant (number of iterations) from the target input reward. It is necessary to apply a discount factor to  the output reward node in order to prevent the agent from becoming infinitely rewarded on a proxy reward that has  been established. If the agent becomes fixated on a proxy reward it will become \u2018stuck\u2019, repeating the previous  behavior.  The network learns appropriate mappings based on the temporal ordering of events; these mappings are contingent                                on subsequent reward conditions. Learning is based on cause and effect. It is nondeterministic; alternate policies                              may be used to attain a reward. The scheme does not rely on \u200beligibility traces [1], or require long-term retention of                                       prior network states (other than t-1 activations).   If the reward condition is removed the agent displays extinction; behaviours weaken and eventually become  random.  It is also possible to achieve this effect without an explicit motivator node in output layer. Since only strong                                       (output) behaviors are those which have been reinforced, a test can be made against the strength of the entire output                                        pattern rather than a specific node.   When using a single node representation for the (mapped) output layer reward node and applying a discount factor,                                   a diminishing desired activation is set for that node. The node is interpreted in an analogue (continuous) fashion.                                   An alternative representation would be to introduce a discrete output reward node for each time step away from the                                     input node reward.     4 Threshold Assignment of Connections  So far backpropagation, an algorithm intended for supervised learning, has been housed within an unsupervised                               framework; desired output patterns have been presented by a framework acting as a surrogate supervisor. Using the                                same framework as described in earlier sections, the backpropagation learning algorithm was replaced with an                               alternative algorithm, termed \u200bThreshold Assignment of Connections (TAC)\u200b.   A biological plausibility concern faces supervised learning schemes in general:  \u25cf How are desired output patterns selected and presented to the network ?  In previous examples, once backpropagation is placed in the Reinforcement Learning framework it is no longer a                               truly supervised learning scheme; desired output patterns are not known \u200ba priori. However backpropagation faces                              the additional biological plausibility concern:  \u25cf How is error back propagated ?  Thus far research has provided little neurobiological support for backpropagation [4][15]. TAC does not require                               backward connections or tenuous biochemical explanations. TAC could be explained by neuromodulators rather                           than at the neural computation level. This would provide a more robust solution with a simpler biological                                 explanation.    The desired activation of any node can be determined by reference to the actual activation of the reward node      du(t)                                 in conjunction with the actual activation of the postsynaptic node and presynaptic node (equation ar(t+1)                      au(t)      ah(t)   3).     If the presynaptic node did not fire connections from that node will not be strengthened or weakened; if the                                      presynaptic node threshold was not achieved the desired activation will remain as the actual activation . If      \u03b8h                        au(t)    21  the presynaptic node did fire connections from that node may be strengthened or weakened.    In reward conditions (reward threshold is achieved), the postsynaptic node activation will be strengthened; a          \u03b8r            au(t)        maximal desired activation will be set if the postsynaptic node threshold was achieved and a minimal value if it                     \u03b8u              was not.     In punishment conditions (reward threshold is not achieved), the postsynaptic node activation will be weakened          \u03b8r                     by assigning it a random moderate desired activation, regardless of whether the postsynaptic node threshold was                               achieved.       Direct \u2018on-node\u2019 delta values that can be used to calculate weight updates, rather than backpropagating them (Box                                   7).    Processing steps    Differences to the Conditioned Reinforcement process described previously are highlighted in bold.    1. Input pattern for state \u2018t\u2019 is presented and forward propagated through the network (Box 2).                              Node output activation may be in the range 0-1 (Box 3). A threshold value of 0.5 is assigned                                 to all nodes in every layer.  2. Agent\u2019s behavior, based on whether output layer activations have exceeded threshold values                         (fig 1), determines subsequent input pattern \u2018t+1\u2019.  3. Framework determines \u2018reward\u2019 value based on \u2018t+1\u2019 input pattern \u200band on activation of                           special output reward node exceeding reward threshold (eg > 0.8 )\u200b.  4. Weight changes are made:  a. Desired \u2018t\u2019 activations \u200bon every node\u200b are calculated:  i. If reward condition then the desired \u2018t\u2019 activation for that node is set to                            maximal value (1.0) for those that were above threshold, and set to                         minimal value (0.0) for those that were below threshold. \u200bSet the desired                         activation for the special output reward node to 95% of reward value.  ii. If no reward then the desired \u2018t\u2019 activation for all nodes are random                           moderate values [0.45..0.55]. \u200bSet the desired activation for the special                     output reward node to minimal value (0.0).  b. Weight changes for state \u2018t\u2019 are made for all connections to each node in situ                               22  according to error values (Box 8); these are not back propagated.    Note: To achieve this both the current and previous activations must be stored.     Box 7 Threshold Assignment of Connections processing steps    1) Calculate desired activation for postsynaptic unit u.        2) Derive delta error value for postsynaptic node u, by finding difference between desired activation ( ) and                              du     actual activation ( ). au   =( ) ) elta d u du au au 1 ( \u2212au     3) Derive weight change for connection between presynaptic unit h and postsynaptic unit u, using learning rate.    =  weight \u0394 uh rate delta l u ah       Box 8 Threshold Assignment of Connections weight change    4.1 Example: Maze navigation  The same problem as described in section \u200b3.2.2 is revisited. In this example the agent is equipped with the TAC                                      algorithm rather than backpropagation.    4.1.1 Problem description  In this example the agent must autonomously learn how to navigate a target through a maze (fig 23).    23    fig23 Maze task start state  \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                         by moving it the top centre cell.  \u25cf Once the target is moved to the target cell (cell 1) it is moved back to it\u2019s original starting position on the                                       grid (cell 3).  \u25cf The agent has no prior knowledge. It does not know that food will lead to a reward until it happens.  \u25cf The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps                                     are required.   \u25cf To further increase task difficulty there is an invisible barrier between the start state and goal. The agent                                    must learn to navigate around the barrier.    4.1.2 Network configuration  A network was created (table 10)(fig 23):  Input layer  9 input nodes; the centre node is designated as a special \u2018reward\u2019                       node  Hidden layer  12 hidden nodes  Output layer  4 motor nodes + 1 reward node  Notes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0    Table 10 Maze task network configuration  24    fig24 Maze task network topology    The nine input nodes are mapped to each cell in the grid (fig 25):    fig25 Maze task input mapping  The network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                   the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                             there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                     Therefore there is only a 1-in-16 chance (2\u200b4\u200b) the agent will move in the correct direction by chance.  The framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                       zero if it is to be punished. A reward occurs if the target is moved to the top centre cell (cell 1).    The network is also assigned an additional special reward node in the output layer. A reward also occurs if the                                      activation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously                                    reinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the                                output reward node and motor output nodes will be reinforced.  25    4.1.3 Results    Initially the network moves randomly through the environment. In time the agent is able to learn how to move                                      around corners to the goal. The physics rules prevent the agent from moving the target diagonally in one step,                                       instead two steps are required. The agent can only reinforce behaviors where there is an immediate reward in the                                     next input pattern. The agent has effectively established proxy rewards, or sub-goals, in intermediate locations                               (cells 2,5,4) allowing chained sequences of behaviors to be learned (fig 26-29).        fig26 Agent encounters the Primary  Reinforcer and learns first  behavior.    fig27 Agent establishes first  Conditioned Reinforcer.      fig28 Agent establishes a subsequent  Conditioned Reinforcer by  reference to the the initial  Conditioned Reinforcer.    fig29 Agent establishes a series of  Conditioned Reinforcers from  goal to starting position.        With sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one                               step left/right/up/down towards the food reward from the starting location (cell 3).    The chaining of sequences of behaviors enables long term strategy to be acquired (table 11).    Scheme  # pattern presentations  Threshold Assignment of Reward  (Unsupervised backpropagation)  80184  Threshold Assignment of Connections  (Unsupervised)  18212    Table 11 Maze task tracking results    Learning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once                                       learnt increasing the distance.    4.2 Example: XOR  The same problem as described in section \u200b3.1.2 is revisited. In this example the agent is equipped with the TAC                                      algorithm rather than backpropagation.  4.2.1 Problem description  The agent will be required to solve the XOR problem (table 12) in order to test the network's ability to map non                                           linear transformations that require multiple layers of neurons. This test will will also provide a performance                                comparison between unsupervised reinforcement learning and the supervised regime.       26  (input)  A  (input)  B  (output)  XOR  0  0  0  0  1  1  1  0  1  1  1  0    Table 12 XOR task    The behavior, or output response of the network will not influence the subsequent input pattern. However, reward                                   feedback will be given to the network thereby informing whether the output was correct. Once again the output                                     patterns are not predetermined. This tightly controlled presentation of input patterns is termed \u2018guided\u2019. The physics                                rules are modified in order to guide the agent.    4.2.2 Network configuration  A network was created (table 13) (fig 30):  Input layer  2 input nodes  Hidden layer  3 hidden nodes  Output layer  1 output node  Notes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0    Table 13 XOR Network configuration      fig30 XOR topology  27  4.2.3 Results    Scheme  # pattern presentations  Supervised backpropagation  2182  Threshold Assignment of Connections  (unsupervised)  311    Table 14 XOR results    The Threshold Assignment of Connections (unsupervised) scheme required significantly less pattern presentations  to learn the XOR solution than the supervised regime (table 14). Both were using the same set of initial weights and  learning parameters (learning rate etc). Despite the fact that in the supervised scheme the desired patterns are known  a priori, while in the unsupervised (guided) learning regime these must be discovered by the network through trial  and error. However this performance gap would be expected to lessen as the number of nodes in the output layer  grows, increasing dimensionality and with it the number of candidate combinations.    4.3 Summary  These experiments demonstrate TAC may solve linearly inseparable problems that are normally reserved for                             supervised training regimes. It is capable of training weights deep within the network, thereby capable of forming                                complex abstract representations. TAC can also solve problems that require long term strategy, when applied in                                 conjunction with Secondary (Conditioned) Reinforcement. Once TAC has replaced backpropagation, the                       framework is no longer required to act as a surrogate supervisor.  It should be noted that while the scheme suggests assigning random desired activations around the threshold value                                   [0.45..0.55] in punishment conditions, it was found to be at least as effective to set this in the range [0-1]. It should                                      be appreciated that (with moderate learning rates) this has the same effect of establishing an \u2018immature\u2019 response                                   around the threshold level (fig 1). These alternatives have implications as to what similar mechanisms we might                                  expect to find in natural organisms.     5 Discussion  The merits of the present scheme can be evaluated in terms of Machine Learning Applications as well as the broader                                     Cognitive Science implications (eg biological, psychological, philosophical).   5.1 Applications  5.1.1 Primary Reinforcement (TAP)  One of the key advantages of Primary Reinforcement over supervised learning schemes is that the agent is able to                                    autonomously explore solutions (behaviors) to problems, of use when a human may be unable to provide training                                 data. Another benefit of Primary Reinforcement is that learning is dynamic and continuous, there is no distinction                                between training and classification phases. This is useful when the agent encounters novel stimuli, or when a once                                   useful response is no longer so.  28    fig31 \u2018Rags the robot\u2019     \u2018Rags the robot\u2019 learns how to track objects with a  vision sensor. The robot has an onboard Arduino  microcontroller and is wirelessly controlled by desktop  computer.    The learning scheme can be readily applied to robotics. As proof of concept an agent was created to remotely control                                      \u2018Rags the robot\u2019; a mobile Arduino based device (fig 31). The task was to target an object, in this case a red frisbee.                                           For this purpose Rags was equipped with a vision sensor (PixyCam), and was capable of rotating via a pair of motor                                        controlled wheels. The agent begins each learning session with randomised network weights. Similar to the previous                                targeting tasks, the robot initially moves randomly, rotating passed the target. Eventually the agent is able to                                   co-ordinate its motors and align itself with the target. A key benefit of TAP is that action is dynamically generated                                      by the network itself, rather than being reliant on a predefined set of actions. If the causal relationship between agent                                      and environment changes (eg rotation direction of the wheel motors is inverted), Rags will compensate via neural                                  adaption.  The Primary Reinforcement solution presented is readily scalable; additional sensors, motors and hidden units can                              be easily added without reworking the underlying learning process. These additional resources will be utilized in                                problem solving. Alternate behaviors will be adopted in the face of unforeseen environmental hazards, or if the                                  agent suffers sensor/motor damage.  Although Primary Reinforcement enables autonomous agents to discover solutions autonomously, a potential issue                         is that the learned behavior may not be ideal but is sufficient to achieve a reward condition; this can be termed a                                         suboptimal action (behavior) \u200blimitation. For example a ball may have been successfully kicked into a goal, but the                                   kicking technique itself was poor. To avoid this a higher standard can be achieved by setting a more stringent reward                                       condition (eg lowering the reward value or raising the threshold), the tradeoff being more optimal behaviors are                                   potentially slower to be explored and learnt.  Another limitation of the present solution is that we are unable to set arbitrary desired output activations on output                                     nodes, a maximal or minimal value is set (ie 1.0 or 0.0). For example in a network with three output nodes we may                                           choose a desired output pattern vector of [0.6, 0.3, 0.7] for a supervised network, but in the present scheme the                                       desired output pattern vector would be limited to [1.0, 0.0, 1.0]. This shortcoming can be termed a \u200bbinary limitation.                                     To workaround this limitation alternate output representations may be required to achieve the same level of                                 granularity.  One of the challenges of scaling relates to the complexity of output responses; the more output nodes that are                                       required for a rewarding behavior to occur, the longer it will take for the agent to explore candidate behaviors and                                     discover the requisite combination. Also it should also be noted that not all node activations may be relevant to                                     producing a rewarding response. These may be included in the dynamically generated desired output pattern, and the                                 agent is unable to differentiate which nodes were responsible for achieving the reward. These redundant activations                                may be eliminated by conforming to what can be termed a \u200blaziness principle, where rewards are reduced in                                    proportion to output node activations.   Note; while Reinforcement Learning is essentially a trial and error approach to learning, it is possible to first part                                  29  train  in supervised mode, save the weights, and then switch to unsupervised mode.    5.1.2 Secondary reinforcement (TAR)  A potential issue facing the presented scheme may be termed a \u200bsuboptimal policy (path) limitation. At first glance                                    this is similar to the \u200bsuboptimal behavior \u200blimitation described in Primary Reinforcement, but on a temporal level.                                 There may be a tendency to \u200bexploit known rewards rather than \u200bexplore new ones. The problem being that the while                                       the acquired sequence of behaviors reliably leads to the goal, and the individual behaviors may have been optimal,                                    the route was not the most efficient one possible. Once learnt the agent has no pressure to alter its preferred policy, a                                           condition which afflicts natural organisms. An optimal policy can be found by decreasing the reward value or                                   increasing the threshold relative to reward, but since exploration involves potential risk it can be avoided unless                                  required. In addition behaviors may be \u200b\u2018shaped\u2019 to elicit the optimal policy [9].   5.1.3 Threshold Assignment of Connections (TAC)  TAC is significantly easier to implement than backpropagation. It also provides a fully neural explanation of                               Reinforcement Learning, with processing essentially only at the node level. This would greatly simplify any                               hardware implementations of the method. It has fewer error derivation dependencies than backpropagation, and                            hence should be more resilient to damage/information loss occurring during weight updates. It is also is not                                expected to suffer from the vanishing gradient problem. However, further scalability testing is required for TAC                                (eg increasing network depth & breadth).  In terms of performance, TAC was found to learn in fewer iterations than supervised backpropagation. However this                                 advantage is expected to diminish as the number of output nodes, and therefore potential candidate behaviors,                                 increase.    5.2 Limitations  5.2.1 Timing issues and Actor-Critic architecture  In the examples provided the action function (TAP) is coupled with that of the value function (TAR), since they                                      both share the same network architecture. Consequently the output for the two functions are derived                               simultaneously. This presents a timing issue, since activations are fed forward through the action component before                                it has been given the opportunity to learn from output of the value component.  This timing issue can be avoided by use of an actor-critic architecture [8][14], providing separate pathways for the                                     two functions. The value function (TAR) can thereby be derived and used to dictate learning for itself and also for                                        the action function (TAR) prior to new actions being issued.   This would have implications regarding how we would expect connection weights in these two pathways to                                 respond to output from the value function. In the value pathway we would expect weights to be based on prior (t-1)                                          activations (since new activations have passed through this pathway), but in the action pathway we would expect                                  weight updates to be based on existing (t) activations (since new activations have not yet passed through this                                     pathway).    5.3 Final words  With reference to a threshold an entirely neural based explanation of Reinforcement Learning has been presented.                                A threshold has been used to provide the roles of action generation and selection in Primary Reinforcement, the                                     roles of exploitation and exploration in Conditioned Reinforcement, and the role of credit assignment in                               30  multi-layered networks. These roles are interdependent and rely on the causal interaction between agent and                               environment.  The scheme presented provides a self organizing model of cognition, being fundamentally hedonistic with learning                             driven by reward. The scheme is based on a single continuum of pleasure and pain, with a single learning                                     mechanism for the two; pleasure effectively being the alleviation of pain. Whilst the examples provided are                                 configured with only a single drive, this should be extended to multiple drives working in concert/competition. As                                a rule of thumb the agent should conform to a \u2018laziness principle\u2019, that is they should be punished for exerting                                       unnecessary energy, thereby achieving their goal with greater efficiency. The examples provided entail the drive                               being always active, and therefore continually reinforcing or weakening behaviors. However, it should be taken                              that when not active (eg drives are satiated), behaviors will not be reinforced or weakened and the agent will not                                         engage in active learning. This does not preclude the scheme being augmented with other forms of learning (eg                                     Hebbian). Also while the examples presented are based on feedforward architectures the present scheme is not                                mutually exclusive with recurrency [13]. Indeed it is envisaged LSTM [12] and other methods of recurrency could                                be used to enhance the presented scheme, providing information of prior state.   The scheme\u2019s true potential lies in embodiment, whether in real or artificial environments. Agents may also be                                   placed in abstract environments that do not physically exist (eg a stock market). The scheme is generic and readily                                    scalable, and given sufficient time and resources may tackle a wide variety of tasks. Tasks can be assigned either by                                       setting of the reward condition goal, or by placing in the path to that goal. If the presented scheme of Reinforcement                                      Learning should be deemed biologically or psychologically implausible, the practical benefits remain of utility.   5.4 Appendix  5.4.1 Platform  Desktop PC (i3 3.7GHz, 16GB ram)  OS Linux Mint 17.3   Neural Network software written in C (GCC) : \u200bhttps://github.com/thward/neural_agent    6 References  [1] Charles W. Anderson. 1986. Learning and Problem Solving with Multilayer Connectionist Systems.                         Technical Report. University of Massachusetts, Amherst, MA, USA.  [2] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,                             M.(2013). Playing atari with deep reinforcement learning.arXiv preprint arXiv:1312.5602  [3] Rumelhart, David E., Hinton, Geoffrey E., Williams, Ronald J., \u201cLearning representations by                         back-propagating errors\u201d, Nature, 1986  [4] D. G. Stork, \"Is backpropagation biologically plausible?,\" \u200bNeural Networks, 1989. IJCNN., International                         Joint Conference on, Washington, DC, USA, 1989, pp. 241-246 vol.2.  [5] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.  [6] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines                            that learn and think like people. arXiv preprint arXiv:1604.00289, 2016.  [7] Gerald Tesauro, \u201dTemporal difference learning and TD-Gammon\u201d, Communications of the ACM CACM                        Homepage archive, Volume 38 Issue 3, March 1995, Pages 58-68  [8] Andrew G. Barto, Richard S. Sutton, Charles W. Anderson, \u201cNeuronlike adaptive elements that can solve                               difficult learning control problems\u201d, IEEE Transactions on Systems, Man, and Cybernetics                       (Volume:SMC-13 ,  Issue: 5) pages 834 - 846, 1983  31  [9] Skinner, B. F. (1953). Science and human behavior. New York: Macmillan, Pages. 92\u20133  [10]William Bechtel, Adele Abrahamsen, \u201cConnectionism and the mind\u201d, Wiley,1991, pages 70-97 Skinner,                         B. F. (1953). Science and human behavior. New York: Macmillan, Pages. 92\u20133  [11]Hodgkin, A. L., A. F. Huxley, and B. Katz. \u201cMeasurement of Current-Voltage Relations in the Membrane                               of the Giant Axon of Loligo.\u201d The Journal of Physiology 116.4 (1952): 424\u2013448.  [12]Sepp Hochreiter and J\u00fcrgen Schmidhuber (1997). \"Long short-term memory\" (PDF). Neural                       Computation. 9 (8): 1735\u20131780. \u200bhttp://dx.doi.org/10.1162/neco.1997.9.8.1735  [13]Elman, J. L. (1990), Finding Structure in Time. Cognitive Science, 14: 179\u2013211.                         doi:10.1207/s15516709cog1402_1  [14]Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D., and                                   Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. arXiv preprint                       arXiv:1602.01783.  [15]Bengio, Y., Lee, D.-H., Bornschein, J., and Lin, Z. (2015). Towards biologically plausible deep learning.                               arXiv preprint arXiv:1502.04156.    32  ",
    "title": "A Threshold-based Scheme for Reinforcement  Learning in Neural Networks ",
    "paper_info": "A Threshold-based Scheme for Reinforcement \nLearning in Neural Networks \n \n \nThomas H. Ward \n \n \n \n \n \nthomas.holland.ward@gmail.com \n \n \nAbstract \n \nA generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,\n \n \n \n \n \n \n \n \n \n \n \n \n \nproviding a general purpose learning machine. By reference to a node threshold three features are\n  \n \n \n \n \n \n \n  \n \n \n \n \n \ndescribed 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)\n \n  \n \n \n \n \n  \n \n \n \n  \nThe learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of\n \n \n \n \n \n \n  \n \n \n \n \n \n \nforming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning\n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \nalgorithm, providing a robust and biologically inspired alternative to backpropagation. The scheme may be\n \n  \n \n \n \n \n  \n \n \n \n \n \nused for supervised as well as unsupervised training regimes. \n \n1 Introduction \n \nThis paper proposes that a general purpose learning machine can be achieved by implementing Reinforcement\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nLearning in an Artificial Neural Network (ANN), three interdependent methods which attempt to emulate the core\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmechanisms of that process are presented. Ultimately the biological plausibility of this scheme may be validated by\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nreference to natural organisms. However that does not preclude the possibility that there is more than one underlying\n  \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nmechanism providing Reinforcement Learning in nature.  \n \nAI research has characteristically followed a bottom-up approach; focusing on subsystems that address distinct,\n \n \n \n \n  \n \n \n \n \n \n \n \n \nspecialized and unrelated problem domains. In contrast the work presented follows a distinctly top-down approach\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nattempting to model intelligence as a whole system; a causal agent interacting with the environment [6]. The agent is\n  \n \n   \n \n  \n \n \n \n \n \n \n \n \n  \nnot designed to solve a particular problem, but is instead assigned a reward condition. The reward condition serves\n \n \n \n  \n \n \n  \n \n  \n \n \n \n \n \n \nas a goal, and in the path a variety of unknown challenges may be present. To solve these problems efficiently the\n  \n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nagent requires intelligence. \n \nThis top-down approach assumes that the core self organizing mechanisms of learning that exist in natural\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \norganisms can be replicated in artificial autonomous agents. These can then be scaled up by endowing the agent with\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmore resources (sensors, neurons & motors). Given sufficient resources and learning opportunities an agent may\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nprovide an efficient solution to a problem provided one exists. Also given the generalization properties of ANN\u2019s\n \n \n \n   \n \n \n \n \n \n \n \n \n \n \n \nthe agent can provide appropriate responses to novel stimuli.  \n \nA distinction is made between supervised, unsupervised and reinforcement training regimes. Supervised learning\n \n  \n \n \n \n \n \n \n \n \n \n \nregimes use a (human) trainer to assign desired input-output pattern pairings. Unsupervised training regimes are\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \ntypically used to cluster a data set into related groups. Reinforcement Learning (RL) may be considered a subtype of\n \n  \n  \n \n \n \n \n \n \n \n \n \n \n  \n  \nunsupervised training; it is sometimes called learning with a critic rather than learning with a teacher as the feedback\n \n   \n \n \n \n  \n \n \n \n \n  \n  \n \n \nis evaluative (right or wrong) rather than instructive (where a desired output action is prescribed). Significant RL\n \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \nsuccesses have been achieved with the use of Temporal Difference (TD) methods [5][7], notably Q-learning[2].  \n \n1 \n",
    "GPTsummary": "- (1): The paper aims to provide a generic and scalable Reinforcement Learning scheme for Artificial Neural Networks to serve as a general-purpose learning machine.\n \n- (2): The paper mentions three interdependent methods in Reinforcement Learning in Artificial Neural Network (ANN), which are Primary Reinforcement, Conditioned Reinforcement, and deep learning. Previous approaches to Reinforcement Learning have been bottom-up and focused on subsystems that address distinct, specialized and unrelated problem domains. The approach proposed in this paper is top-down and attempts to model intelligence as a whole system. The paper claims that the approach is well-motivated because it assumes that the core self-organizing mechanisms of learning that exist in natural organisms can be replicated in artificial autonomous agents.\n \n- (3): The proposed methodology is a threshold-based scheme for Reinforcement Learning in Artificial Neural Networks. It provides an algorithm for solving linearly inseparable problems, forming long-term strategy, and providing a robust and biologically-inspired alternative to backpropagation. The threshold in the scheme is set to decide when a neuron is firing and its value is altered by synaptic strength learning rules, based on whether the neuron is on or off when an action, providing positive or negative feedback, is performed.\n  \n- (4): The paper does not provide experimental results on any task.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in providing a threshold-based scheme for Reinforcement Learning in Artificial Neural Networks, which can replicate the self-organizing mechanisms of learning in natural organisms and serve as a general-purpose learning machine.\n\n- (2): In terms of innovation point, the paper proposes a top-down approach to Reinforcement Learning that aims to model intelligence as a whole system, which is an innovative contribution to the field. In terms of performance, the paper claims that its proposed methodology can solve linearly inseparable problems and form long-term strategies in a successful and biologically-inspired manner. However, the paper lacks experimental results on any task, which is a weakness in terms of evaluating the practical value of the work. In terms of workload, the paper provides a clear and concise description of its proposed methodology, making it accessible to researchers who are interested in Reinforcement Learning in Artificial Neural Networks.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in providing a threshold-based scheme for Reinforcement Learning in Artificial Neural Networks, which can replicate the self-organizing mechanisms of learning in natural organisms and serve as a general-purpose learning machine.\n\n- (2): In terms of innovation point, the paper proposes a top-down approach to Reinforcement Learning that aims to model intelligence as a whole system, which is an innovative contribution to the field. In terms of performance, the paper claims that its proposed methodology can solve linearly inseparable problems and form long-term strategies in a successful and biologically-inspired manner. However, the paper lacks experimental results on any task, which is a weakness in terms of evaluating the practical value of the work. In terms of workload, the paper provides a clear and concise description of its proposed methodology, making it accessible to researchers who are interested in Reinforcement Learning in Artificial Neural Networks.\n\n\n"
}