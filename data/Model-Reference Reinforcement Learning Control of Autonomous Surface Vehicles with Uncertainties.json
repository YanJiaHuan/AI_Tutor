{
    "Abstract": "Abstract\u2014 This paper presents a novel model-reference reinforcement learning control method for uncertain autonomous surface vehicles. The proposed control combines a conventional control method with deep reinforcement learning. With the conventional control, we can ensure the learning-based control law provides closed-loop stability for the overall system, and potentially increase the sample ef\ufb01ciency of the deep reinforcement learning. With the reinforcement learning, we can directly learn a control law to compensate for modeling uncertainties. In the proposed control, a nominal system is employed for the design of a baseline control law using a conventional control approach. The nominal system also de\ufb01nes the desired performance for uncertain autonomous vehicles to follow. In comparison with traditional deep reinforcement learning methods, our proposed learning-based control can provide stability guarantees and better sample ef\ufb01ciency. We demonstrate the performance of the new algorithm via extensive simulation results. I. ",
    "Introduction": "INTRODUCTION Autonomous surface vehicles (ASVs) have been attracting more and more attention, due to their advantages in many applications, such as environmental monitoring [1], resource exploration [2], shipping [3], and many more. Successful launch of ASVs in real life requires accurate tracking control along a desired trajectory [4]\u2013[6]. However, accurate tracking control for ASVs is challenging, as ASVs are subject to uncertain nonlinear hydrodynamics and unknown environmental disturbances [7]. Hence, tracking control of highly uncertain ASVs has received extensive research attention [8]\u2013[12]. Control algorithms for uncertain systems including ASVs mainly lie in four categories: 1) robust control which is the \u201cworst-case\u201d design for bounded uncertainties and disturbances [9]; 2) adaptive control which adapts to system uncertainties with parameter estimations [4], [5]; 3) disturbance observerbased control which compensates uncertainties and disturbances in terms of the observation technique [11], [13]; and 4) reinforcement learning (RL) which learns a control law from data samples [12], [14]. The \ufb01rst three algorithms follow a model-based control approach, while the last one is data driven. Model-based control can ensure closed-loop stability, but a system model is indispensable. Uncertainties and disturbances of a system should also satisfy different conditions for different model-based methods. In robust control, uncertainties and disturbances are assumed to be bounded with known boundaries [15]. As a consequence, 1Department of Maritime and Transport Technology, Delft University of Technology, Delft, the Netherlands Qingrui.Zhang@tudelft.nl; V.Reppa@tudelft.nl 2Department of Cognitive Robotics, Delft University of Technology, Delft, the Netherlands Wei.Pan@tudelft.nl robust control will lead to conservative high-gain control laws which usually limits the control performance (i.e., overshoot, settling time, and stability margins) [16]. Adaptive control can handle varying uncertainties with unknown boundaries, but system uncertainties are assumed to be linearly parameterized with known structure and unknown constant parameters [17], [18]. A valid adaptive control design also requires a system to be persistently excited, resulting in the unpleasant high-frequency oscillation behaviours in control actions [19]. On the other hand, disturbance observer-based control can adapt to both uncertainties and disturbances with unknown structures and without assuming systems to be persistently excited [13], [20]. However, we need the frequency information of uncertainty and disturbance signals when choosing proper gains for the disturbance observerbased control, otherwise it is highly possible to end up with a high-gain control law [20]. In addition, the disturbance observer-based control can only address matched uncertainties and disturbances, which act on systems through the control channel [18], [21]. In general, comprehensive modeling and analysis of systems are essential for all model-based methods. In comparison with model-based methods, RL is capable of learning a control law from data samples using much less model information [22]. Hence, it is more promising in controlling systems subject to massive uncertainties and disturbances as ASVs [12], [14], [23], [24], given the suf\ufb01ciency and good quality of collected data. Nevertheless, it is challenging for model-free RL to ensure closed-loop stability, though some research attempts have been made [25]. It implies that the learned control law must be retrained, once some changes happen to the environment or the reference trajectory (i.e. in [14], the authors conducted two independent training procedures for two different reference trajectories.). Model-based RL is possible to learn a control law which ensures the closed-loop stability by introducing a Lyapunov constraint into the objective function of the policy improvement according to the latest research [26]. However, the model-based RL with stability guarantees requires an admissible control law \u2014 a control law which makes the original system asymptotically stable \u2014 for the initialization. Both the Lyapunov candidate function and complete system dynamics are assumed to be Lipschitz continuous with known Lipschitz constants for the construction of the Lyapunov constraint. It is challenging to \ufb01nd the Lipschitz constant of an uncertain system subject to unknown environmental disturbances. Therefore, the introduced Lyapunov constraint function is restrictive, as it is established based on the worstcase consideration [26]. arXiv:2003.13839v1  [eess.SY]  30 Mar 2020 ",
    "Problem Formulation": "PROBLEM FORMULATION The full dynamics of autonomous surface vehicles (ASVs) have six degrees of freedom (DOF), including three linear motions and three rotational motions [7]. In most scenarios, we are interested in controlling the horizontal dynamics of (ASVs) [29], [30]. We, therefore, ignore the vertical, rolling, and pitching motions of ASVs by default in this paper. Let x and y be the horizontal position coordinates of an ASV in the inertial frame and \u03c8 the heading angle as shown in Figure 1. In the body frame (c.f., Figure 1), we use u and v to represent the linear velocities in surge (x-axis) and sway Inertial frame Figure 9: Experimental Setting Moment of Inertia to acquire the periodic time of the vessel Tcom. For the calculation to work the weight of the object and its approximate location of the center of gravity must be known. Using the acquired periodic time we use formula 14 to compute the moment of inertia. Iz = Wcom \u21e4 T 2 com \u21e4 L1 4 \u21e4 \u21e12 \ufffd Wframe \u21e4 T 2 frame \u21e4 L2 4 \u21e4 \u21e12 \ufffd Wvessel \u21e4 L2 3 Gravity (14) With: Wcom = Weight of the vessel and frame combined Wframe = Weight of the frame Wvessel = Weight of the vessel Tcom = Oscillation time of vessel and frame Tframe = Oscillation time of frame L1 = Length from pivot axis to combined CoG L2 = Length from pivot axis to frame CoG L3 = Length from pivot axis to vessel CoG 2.1.6 Drag (shape) In order for the model to work the drag-tensor , \u2327drag, has to be found. of a vessel under a certain angle is the result of its geometric shape the drag can be found in many ways. For this research an experi that will provide a big set of data in order to compute an over for the Tito Neri. All the data is stored in Microsoft-Excel. I out this experiment a rigid hoop, three load cells, an acceler tank that enables water-\ufb02ow in one direction is needed, see 14 Body frame (x, y)   u v YI East XI North XB YB \u2318 = [x, y,  ]T \u232b = [u, v, r]T Fig. 1: Coordinate systems of an autonomous surface vehicle (y-axis), respectively. The heading angular rate is denoted by r. The general 3-DOF nonlinear dynamics of an ASV can be expressed as \ufffd \u02d9\u03b7 = R (\u03b7) \u03bd M \u02d9\u03bd + (C (\u03bd) + D (\u03bd)) \u03bd + G (\u03bd) = \u03c4 (1) where \u03b7 = [x, y, \u03c8]T \u2208 R3 is a generalized coordinate vector, \u03bd = [u, v, r]T \u2208 R3 is the speed vector, M is the inertia matrix, C (\u03bd) denotes the matrix of Coriolis and centripetal terms, D (\u03bd) is the damping matrix, \u03c4 \u2208 R3 represents the control forces and moments, G (\u03bd) = [g1 (\u03bd) , g2 (\u03bd) , g3 (\u03bd)]T \u2208 R3 denotes unmodeled dynamics due to gravitational and buoyancy forces and moments [7], and R is a rotation matrix given by R = \uf8ee \uf8f0 cos \u03c8 \u2212 sin \u03c8 0 sin \u03c8 cos \u03c8 0 0 0 1 \uf8f9 \uf8fb The inertia matrix M = M T > 0 is M = [Mij] = \uf8ee \uf8f0 M11 0 0 0 M22 M23 0 M32 M33 \uf8f9 \uf8fb (2) where M11 = m\u2212X \u02d9u, M22 = m\u2212Y \u02d9v, M33 = Iz \u2212N \u02d9r, and M32 = M23 = mxg \u2212 Y \u02d9r. The matrix C (\u03bd) = \u2212CT (\u03bd) is C = [Cij] = \uf8ee \uf8f0 0 0 C13 (\u03bd) 0 0 C23 (\u03bd) \u2212C13 (\u03bd) \u2212C23 (\u03bd) 0 \uf8f9 \uf8fb (3) where C13 (\u03bd) = \u2212M22v \u2212 M23r, C23 (\u03bd) = \u2212M11u. The damping matrix D (\u03bd) is D (\u03bd) = [Dij] = \uf8ee \uf8f0 D11 (\u03bd) 0 0 0 D22 (\u03bd) D23 (\u03bd) 0 D32 (\u03bd) D33 (\u03bd) \uf8f9 \uf8fb (4) where D11 (\u03bd) = \u2212Xu \u2212 X|u|u|u| \u2212 Xuuuu2, D22 (\u03bd) = \u2212Yv \u2212 Y|v|v|v| \u2212 Y|r|v|r|, D23 (\u03bd) = \u2212Yr \u2212 Y|v|r|v| \u2212 Y|r|r|r|, D32 (\u03bd) = \u2212Nv \u2212 N|v|v|v| \u2212 N|r|v|r|, D33 (\u03bd) = \u2212Nr \u2212 N|v|r|v| \u2212 N|r|r|r|, and X(\u00b7), Y(\u00b7), and N(\u00b7) are hydrodynamic coef\ufb01cients whose de\ufb01nitions can be found in [7]. Accurate numerical models of the nonlinear dynamics (1) are rarely available. Major uncertainty sources come from M, C (\u03bd), and D (\u03bd) due to hydrodynamics, and G (\u03bd) due to gravitational and buoyancy forces and moments. The objective of this work is to design a control scheme capable of handling these uncertainties. will not tilt and eventually come to a balance in an upright position, one can conclude the z-component of the CoG is above the point of the rod the vessel was lifted with. By moving the two steel rods up or down one can test the location by reaching the tilting point of the Tito Neri. If the ship will tilt, and therefore ends upside down, the CoG is beneath the steel rods. By performing this experiment with a slight change in height of the beams. An assumption can be made in terms of the CoG. If the rods are exactly placed on the z-value of the CoG, the ship can be tilted and will stay in equilibrium for every position. Figure 8: Experimental setting CoG for the z-coordinate 2.1.5 Moment of inertia For this experiment keep in mind the research limitation of just 3 DoF. It is su\ufffdcient to determine the moment of inertia about the z-axis (yaw). As is stated before, the e\u21b5ects of buoyancy and rolling will be neglected in this research. This experimental setting has been inspired by an experiment that engineers at the NASA\u2019s Armstrong Flight Research Center carried out the \ufb01nd the moment of inertia of a plane. (NASA, 2016) The experimental setting consists of two frames. One Large frame which is meant to keep the object in the air. The other frame is constructed in such a way that it will function as the rigid swing between the vessel and the larger frame. See section 2.1.3 for the construction of the frame. Note that it is important for the arm to be rigid otherwise the periodic time will be a\u21b5ected by the arms ability to \ufb02ex. The goal of this experiment is to acquire the swing time of the vessel in order to calculate its moment of inertia. In this experiment the time the object takes to make a known amount of oscillations will be measured. This will be done by counting the amount of complete swings in a span of time. The data will be computed 13 Autonomous Surface Vehicle Nominal System ub ul xr x xm RL control Baseline  control Baseline  control xm x x um \u02d9xm = \uf8ff 0 R(\u2318) 0 Am \ufffd xm + \uf8ff 0 Bm \ufffd um Fig. 2: Model-reference reinforcement learning control III. MODEL-REFERENCE REINFORCEMENT LEARNING CONTROL Let x = \ufffd \u03b7T , \u03bdT \ufffdT and u = \u03c4, so (1) can be rewritten as \u02d9x = \ufffd 0 R (\u03b7) 0 A (\u03bd) \ufffd x + \ufffd 0 B \ufffd u (5) where A (\u03bd) = M \u22121 (C (\u03bd) + D (\u03bd)), and B = M \u22121. Assume an accurate model (5) is not available, but it is possible to get a nominal model expressed as \u02d9xm = \ufffd 0 R (\u03b7) 0 Am \ufffd xm + \ufffd 0 Bm \ufffd um (6) where Am and Bm are the known system matrices. Assume that there exists a control law um allowing the states of the nominal system (6) to converge to a reference signal xr, i.e., \u2225xm \u2212 xr\u22252 \u2192 0 as t \u2192 \u221e. The objective is to design a control law allowing the state of (5) to track state trajectories of the nominal model (6). As shown in Figure 2, the overall control law for the ASV system (5) has the following expression. u = ub + ul (7) where ub is a baseline control designed based on (6), and ul is a control policy from the deep reinforcement learning module shown in Figure 2. The baseline control ub is employed to ensure some basic performance, (i.e., local stability), while ul is introduced to compensate for all system uncertainties. The baseline control ub in (7) can be designed based on any existing model-based method based on the nominal model (6). Hence, we ignore the design process of ub, and mainly focus on the development of ul based on reinforcement learning. A. Reinforcement learning In RL, system dynamics are characterized using a Markov decision process denoted by a tuple MDP := \ufffd S, U, P, R, \u03b3 \ufffd , where S is the state space, U speci\ufb01es the action/input space, P : S \u00d7 U \u00d7 S \u2192 R de\ufb01nes a transition probability, R : S \u00d7 U \u2192 R is a reward function, and \u03b3 \u2208 [0, 1] is a discount factor. A policy in RL, denoted by \u03c0 (ul|s), is the probability of choosing an action ul \u2208 U at a state s \u2208 S. Note that the state vector s contains all available signals affecting the reinforcement learning control ul. In this paper, such signals include x, xm, xr, and ub, where xm performs like a target state for system (5) and ub is a function of x and xr. Hence, we choose s = {xm, x, ub}. Reinforcement learning uses data samples, so it is assumed that we can sample input and state data from system (5) at discrete time steps. Without loss of generality, we de\ufb01ne xt, ub,t, and ul,t as the ASV state, the baseline control action, and the control action from the reinforcement learning at the time step t, respectively. The state signal s at the time step t is, therefore, denoted by st = {xm,t, xt, ub,t}. The sample time step is assumed to be \ufb01xed and denoted by \u03b4t. For each state st, we de\ufb01ne a value function V\u03c0 (st) as an expected accumulated return described as V\u03c0 = \u221e \ufffd t \ufffd ul,t \u03c0 (ul,t|st) \ufffd st+1 Pt+1|t \ufffd Rt + \u03b3V\u03c0(st+1) \ufffd (8) where Rt = R(st, ul,t) and Pt+1|t = P (st+1 |st, ul,t ). The action-value function (a.k.a., Q-function) is de\ufb01ned to be Q\u03c0 (st, ul,t) = Rt + \u03b3 \ufffd st+1 Pt+1|tV\u03c0(st+1) (9) In our design, we aim to allow system (5) to track the nominal system (6), so Rt is de\ufb01ned as Rt = \u2212 (xt \u2212 xm,t)T G (xt \u2212 xm,t) \u2212 uT l,tHul,t (10) where G \u2265 0 and H > 0 are positive de\ufb01nite matrices. The objective of the reinforcement learning is to \ufb01nd an optimal policy \u03c0\u2217 to maximize the state-value function V\u03c0(st) or the action-value function Q\u03c0 (st, ul,t), \u2200st \u2208 S, namely, \u03c0\u2217 = arg max \u03c0 Q\u03c0 (st, ul,t) = arg max \u03c0 \uf8eb \uf8edRt + \u03b3 \ufffd st+1 Pt+1|tV\u03c0(st+1) \uf8f6 \uf8f8 (11) IV. DEEP REINFORCEMENT LEARNING CONTROL DESIGN In this section, we will present a deep reinforcement learning algorithm for the design of ul in (7), where both the control law ul and the Q-function Q\u03c0 (st, ul,t) are approximated using deep neural networks. The deep reinforcement learning control in this paper is developed based on the soft actor-critic (SAC) algorithm which provides both sample ef\ufb01cient learning and convergence [31]. In SAC, an entropy term is added to the objective function in (11) to regulate the exploration performance at the training stage. The objective of (11) is thus rewritten as \u03c0\u2217 = arg max \u03c0 \ufffd Rt + \u03b3Est+1 [V\u03c0(st+1) +\u03b1H (\u03c0 (ul,t+1|st+1))] \ufffd (12) where Est+1 [\u00b7] = \ufffd st+1 Pt+1|t [\u00b7] is an expectation operator, H (\u03c0 (ul,t|st)) = \u2212 \ufffd ul,t \u03c0 (ul,t|st) ln (\u03c0 (ul,t|st)) = \u2212E\u03c0 [ln (\u03c0 (ul,t|st))] is the entropy of the policy, and \u03b1 is a temperature parameter. Training of SAC repeatedly executes policy evaluation and policy improvement. In the policy evaluation, a soft Q-value is computed by applying a Bellman operation Q\u03c0 (st, ul,t) = T \u03c0Q\u03c0 (st, ul,t) where T \u03c0Q\u03c0 (st, ul,t) = Rt + \u03b3Est+1 {E\u03c0 [Q\u03c0 (st+1, ul,t+1) \u2212\u03b1 ln (\u03c0 (ul,t+1|st+1))]} (13) Input layer Hidden layers Output layer Actor neural  network st \u03c0\u03c6 (ul,t|st) State State Input layer Hidden layers Output layer Critic neural  network Input st ul,t Q\u03b8 (st,ul,t) Fig. 3: Approximation of Q\u03b8 and \u03c0\u03c6 using MLP In the policy improvement, the policy is updated by \u03c0new = arg min \u03c0\u2032 DKL \ufffd \u03c0\u2032 (\u00b7|st) \ufffd\ufffd\ufffdZ\u03c0oldeQ\u03c0old(st,\u00b7)\ufffd (14) where \u03c0old denotes the policy from the last update, Q\u03c0old is the Q-value of \u03c0old. DKL denotes the Kullback-Leibler (KL) divergence, and Z\u03c0old is a normalization factor. Via mathematical manipulations, the objective for the policy improvement is transformed into \u03c0\u2217 = arg min \u03c0 E\u03c0 \ufffd \u03b1 ln (\u03c0 (ul,t|st)) \u2212 Q (st, ul,t) \ufffd (15) More details on how (15) is obtained can be found in [31], [32]. As shown in Figure 3, both the policy \u03c0 (ul,t|st) and value function Q\u03c0 (st, ul,t) will be parameterized using fully connected multiple layer perceptrons (MLP) with \u2019ReLU\u2019 nonlinearities as the activation functions. The \u2019ReLU\u2019 function is de\ufb01ned as relu (z) = max {z, 0} The \u201cReLU\u201d activation function outperforms other activation functions like sigmoid functions [33]. For a vector z = [z1, . . . , zn]T \u2208 Rn, there exists relu (z) = [relu (z1) , . . . , relu (zn)]T . Hence, a MLP with \u2019ReLU\u2019 as the activation functions and one hidden layer is expressed as MLP (z) = W1 \ufffd relu \ufffd W0 \ufffd zT , 1 \ufffd\ufffdT , 1 \ufffdT where \ufffd zT , 1 \ufffdT is a vector composed of z and 1, and W0 and W1 with appropriate dimensions are weight matrices to be trained. For the simplicity, we use W = {W0, W1} to represent the set of parameters to be trained. System Actor neural  network Critic neural  network Reward Run the system using latest learned control policy and collect data Update critic and actor neural networks using historical data Replay memory Randomly sample a  batch of data samples \u21e0 Exploration noise input state Fig. 4: Of\ufb02ine training process of deep reinforcement learning In this paper, the Q-function is parameterized using \u03b8 and denoted by Q\u03b8 (st, ul,t). The parameterized policy is denoted by \u03c0\u03c6 (ul,t|st), where \u03c6 is the parameter set to be trained. Note that both \u03b8 and \u03c6 are a set of parameters whose dimensions are determined by the deep neural network setup. For example, if Q\u03b8 is represented by a MLP with K hidden layers and L neurons for each hidden layers, the parameter set \u03b8 is \u03b8 = {\u03b80, \u03b81, . . . , \u03b8K} with \u03b80 \u2208 R(dims+dimu+1)\u00d7L, \u03b8K \u2208 R(L+1), and \u03b8i \u2208 R1\u00d7(L)\u00d7(L+1) for 1 \u2264 i \u2264 K \u2212 1, where dims denotes the dimension of the state s and dimu is the dimension of the input ul. The deep neural network for Q\u03b8 is called critic, while the one for \u03c0\u03c6 is called actor. A. Training setup The algorithm training process is illustrated in Figure 4. The whole training process will be of\ufb02ine. We repeatedly run the system (5) under a trajectory tracking task. At each time step t + 1, we collect data samples, such as an input from the last time step ul,t, a state from the last time step st, a reward Rt, and a current state st+1. Those historical data will be stored as a tuple (st, ul,t, Rt, st+1) at a replay memory D [34]. At each policy evaluation or improvement step, we randomly sample a batch of historical data, B, from the replay memory D for the training of the parameters \u03b8 and \u03c6. Starting the training, we apply the baseline control policy ub to an ASV system to collect the initial data D0 as shown in Algorithm 1. The initial data set D0 is used for the initial \ufb01tting of Q-value functions. When the initialization is over, we execute both ub and the latest updated reinforcement learning policy \u03c0\u03c6 (ul,t|st) to run the ASV system. At the policy evaluation step, the parameters \u03b8 are trained to minimize the following Bellman residual. JQ (\u03b8) = E(st,ul,t)\u223cD \ufffd1 2 (Q\u03b8 (st, ul,t) \u2212 Ytarget)2 \ufffd (16) where (st, ul,t) \u223c D implies that we randomly pick data samples (st, ul,t) from a replay memory D, and Ytarget = Rt + \u03b3Est+1 \ufffd E\u03c0 [Q\u00af\u03b8 (st+1, ul,t+1) \u2212 \u03b1 ln (\u03c0\u03c6)] \ufffd where \u00af\u03b8 is the target parameter which will be updated slowly. Applying a stochastic gradient descent technique (ADAM Algorithm 1 Reinforcement learning control 1: Initialize parameters \u03b81, \u03b82 for Q\u03b81 and Q\u03b82, respectively, and \u03c6 for the actor network (18). 2: Assign values to the the target parameters \u00af\u03b81 \u2190 \u03b81, \u00af\u03b82 \u2190 \u03b82, D \u2190 \u2205, D0 \u2190 \u2205, 3: Get data set D0 by running ub on (5) with ul = 0 4: Turn off the exploration and train initial critic parameters \u03b80 1, \u03b80 2 using D0 according to (16). 5: Initialize the replay memory D \u2190 D0 6: Assign initial values to critic parameters \u03b81 \u2190 \u03b80 1, \u03b82 \u2190 \u03b80 2 and their targets \u00af\u03b81 \u2190 \u03b80 1, \u00af\u03b82 \u2190 \u03b80 2 7: repeat 8: for each data collection step do 9: Choose an action ul,t according to \u03c0\u03c6 (ul,t|st) 10: Run both the nominal system (6) and the full system (5) & collect st+1 = {xt+1, xm,t+1, ub,t+1} 11: D \u2190 D \ufffd {st, ul,t, R (st, ul,t) , st+1} 12: end for 13: for each gradient update step do 14: Sample a batch of data B from D 15: \u03b8j \u2190 \u03b8j \u2212 \u03b9Q\u2207\u03b8JQ (\u03b8j), and j = 1, 2 16: \u03c6 \u2190 \u03c6 \u2212 \u03b9\u03c0\u2207\u03c6J\u03c0 (\u03c6), 17: \u03b1 \u2190 \u03b1 \u2212 \u03b9\u03b1\u2207\u03b1J\u03b1 (\u03b1) 18: \u00af\u03b8j \u2190 \u03ba\u03b8j + (1 \u2212 \u03ba) \u00af\u03b8j, and j = 1, 2 19: end for 20: until convergence (i.e. JQ (\u03b8) < a small threshold) [35] in this paper) to (16) on a data batch B with a \ufb01xed size, we obtain \u2207\u03b8JQ (\u03b8) = \ufffd \u2207\u03b8Q\u03b8 |B| \ufffd Q\u03b8 (st, ul,t) \u2212 Ytarget \ufffd where |B| is the batch size. At the policy improvement step, the objective function de\ufb01ned in (15) is represented using data samples from the replay memory D as given in (17). J\u03c0 (\u03c6) = E(st,ul,t)\u223cD \ufffd \u03b1 ln(\u03c0\u03c6) \u2212 Q\u03b8 (st, ul,t) \ufffd (17) Parameter \u03c6 is trained to minimize (17) using a stochastic gradient descent technique. At the training stage, the actor neural network is expressed as ul,\u03c6 = \u00aful,\u03c6 + \u03c3\u03c6 \u2299 \u03be (18) where \u00aful,\u03c6 represents the control law to be implemented in the end, \u03c3\u03c6 denotes the standard deviation of the exploration noise, \u03be \u223c N (0, I) is the exploration noise with N (0, I) denoting a Gaussian distribution, and \u201c\u2299\u201d is the Hadamard product. Note that the exploration noise \u03be is only applied to the training stage. Once the training is done, we only need \u00aful,\u03c6 in the implementation. Hence, at the training stage, ul in Figure 2 is equal to ul,\u03c6. Once the training is over, we have ul = \u00aful,\u03c6. Applying the policy gradient technique to (17), we can calculate the gradient of J\u03c0 (\u03c6) with respect to \u03c6 in terms Algorithm 2 Policy iteration technique 1: Start from an initial control policy u0 2: repeat 3: for Policy evaluation do 4: Under a \ufb01xed policy ul, apply the Bellman backup operator T \u03c0 to the Q value function, Q (st, ul,t) = T \u03c0Q (st, ul,t) (c.f., (13)) 5: end for 6: for Policy improvement do 7: Update policy \u03c0 according to (12) 8: end for 9: until convergence of the stochastic gradient method as in (19) \u2207\u03c6J\u03c0 = \ufffd \u03b1\u2207\u03c6 ln \u03c0\u03c6 + (\u03b1\u2207ul ln \u03c0\u03c6 \u2212 \u2207ulQ\u03b8) \u2207\u03c6ul,\u03c6 |B| (19) The temperature parameters \u03b1 are updated by minimizing the following objective function. J\u03b1 = E\u03c0 \ufffd \u2212\u03b1 ln \u03c0 (ul,t|st) \u2212 \u03b1 \u00afH \ufffd (20) where \u00afH is a target entropy. Following the same setting in [32], we choose \u00afH = \u22123 where \u201c3\u201d here represents the action dimension. In the \ufb01nal implementation, we use two critics which are parameterized by \u03b81 and \u03b82, respectively. The two critics are introduced to reduce the over-estimation issue in the training of critic neural networks [36]. Under the two-critic mechanism, the target value Ytarget is Ytarget = Rt + \u03b3 min \ufffd Q\u00af\u03b81 (st+1, ul,t+1) , Q\u00af\u03b82 (st+1, ul,t+1) \ufffd \u2212 \u03b3\u03b1 ln (\u03c0\u03c6) (21) The entire algorithm is summarized in Algorithm 1. In Algorithm 1, \u03b9Q, \u03b9\u03c0, and \u03b9\u03b1 are positive learning rates (scalars), and \u03ba > 0 is a constant scalar. V. PERFORMANCE ANALYSIS In this subsection, both the convergence and stability of the proposed learning-based control are analyzed. For the analysis, the soft actor-critic RL method in Algorithm 1 is recapped as a policy iteration (PI) technique which is summarized in Algorithm 2. We thereafter present the following two lemmas without proofs for the convergence analysis [31], [32]. Lemma 1 (Policy evaluation): Let T \u03c0 be the Bellman backup operator under a \ufb01xed policy \u03c0 and Qk+1 (s, ul) = T \u03c0Qk (s, ul). The sequence Qk+1 (s, ul) will converge to the soft Q-function Q\u03c0 of the policy \u03c0 as k \u2192 \u221e. Lemma 2 (Policy improvement): Let \u03c0old be an old policy and \u03c0new be a new policy obtained according to (14). There exists Q\u03c0new (s, ul) \u2265 Q\u03c0old (s, ul) \u2200s \u2208 S and \u2200u \u2208 U. In terms of (1) and (2), we are ready to present Theorem 1 to show the convergence of the SAC algorithm. Theorem 1 (Convergence): If one repeatedly applies the policy evaluation and policy improvement steps to any control policy \u03c0, the control policy \u03c0 will converge to an optimal policy \u03c0\u2217 such that Q\u03c0\u2217 (s, ul) \u2265 Q\u03c0 (s, ul) \u2200\u03c0 \u2208 \u03a0, \u2200s \u2208 S, and \u2200u \u2208 U, where \u03a0 denotes a policy set. Proof: Let \u03c0i be the policy obtained from the i-th policy improvement with i = 0, 1, . . ., \u221e. According to Lemma 2, one has Q\u03c0i (s, ul) \u2265 Q\u03c0i\u22121 (s, ul), so Q\u03c0i (s, ul) is monotonically non-decreasing with respect to the policy iteration step i. In addition, Q\u03c0i (s, ul) is upper bounded according to the de\ufb01nition of the reward given in (10), so Q\u03c0i (s, ul) will converge to an upper limit Q\u03c0\u2217 (s, ul) with Q\u03c0\u2217 (s, ul) \u2265 Q\u03c0 (s, ul) \u2200\u03c0 \u2208 \u03a0, \u2200s \u2208 S, and \u2200ul \u2208 U. Theorem 1 demonstrates that we can \ufb01nd an optimal policy by repeating the policy evaluation and improvement processes. Next, we will show the closed-loop stability of the overall control law (baseline control ub plus the learned control ul). The following assumption is made for the baseline control developed using the nominal system (6). Assumption 1: The baseline control law ub can ensure that the overall uncertain ASV system is stable \u2013 that is, there exists a Lyapunov function V (st) associate with ub such that V (st+1) \u2212 V (st) \u2264 0 \u2200st \u2208 S. Note that the baseline control ub is implicitly included in the state vector s, as s consists of x, xm, and ub in this paper as discussed in Section III. Hence, V (st) in Assumption 1 is the Lyapunov function for the closed-loop system of (5) with the baseline control ub. Assumption 1 is possible in real world. One could treat the nominal model (6) as a linearized model of the overall ASV system (5) around a certain equilibrium. Therefore, a control law, which ensures asymptotic stability for (6), can ensure at least local stability for (5) [37]. In the stability analysis, we will ignore the entropy term H (\u03c0), as it will converge to zero in the end and it is only introduced to regulate the exploration magnitude. Now, we present Theorem 2 to demonstrate the closed-loop stability of the ASV system (5) under the composite control law (7). Theorem 2 (Stability): Suppose Assumption 1 holds. The overall control law ui = ub + ui l can always stabilize the ASV system (5), where ui l represents the RL control law from i-th iteration, and i = 0, 1, 2, ... \u221e. Proof: In our proposed algorithm, we start the training/learning using the baseline control law ub. According to Lemma 1, we are able to obtain the corresponding Q value function for the baseline control law ub. Let the Q value function be Q0 (s, ul) where ul is a function of s. According to the de\ufb01nitions of the reward function in (10) and Q value function in (9), we can choose the Lyapunov function candidate as V0 (s) = \u2212Q0 (s, ul). If Assumption 1 holds, there exists V0 (st+1) \u2212 V0 (st) \u2264 0 \u2200st \u2208 S. In the policy improvement, the control law is updated by u1 = min \u03c0 \ufffd \u2212Rt + \u03b3V0 (st+1) \ufffd (22) where the expectation operator is ignored as the system is deterministic. For any nonlinear system st+1 = f (st) + g (st) ut, a necessary condition for the existence of (22) is u1 = \u22121 2H\u22121g (st)T \u2202V0 (st+1) \u2202st+1 (23) Substituting (23) back into (22 yields V0 (st+1) \u2212 V0 (st) = \u2212 (xt \u2212 xm,t)T G (xt \u2212 xm,t) \u22121 4 \ufffd\u2202V0 (st+1) \u2202st+1 \ufffdT g (st) \u00d7H\u22121g (st)T \u2202V0 (st+1) \u2202st+1 \u2264 0 Hence, u1 is a control law which can stabilize the same ASV system (5), if Assumption 1 holds. Applying Lemma 1 to u1, we can get a new Lyapunov function V1 (st). In terms of V1 (st), (22) and (23), we can show that u2 also stabilizes the ASV system (5). Repeating (22) and (23) for all i = 1, 2, . . ., we can prove that all ui can stabilize the ASV system (5), if Assumption 1 holds. VI. SIMULATION In this section, the proposed learning-based control algorithm is implemented to the trajectory tracking control of a supply ship model presented in [29], [30]. Model parameters are summarized in Table I. The unmodeled dynamics in the simulations are given by g1 = 0.279uv2 + 0.342v2r, g2 = 0.912u2v, and g3 = 0.156ur2+0.278urv3, respectively. The based-line control law ub is designed based on a nominal model with the following simpli\ufb01ed linear dynamics in terms of the backstepping control method [13], [37]. Mm \u02d9\u03bdm = \u03c4 \u2212 Dm\u03bdm (24) where Mm = diag {M11, M22, M33}. Dm = diag {\u2212Xv, \u2212Yv, \u2212Nr}. The reference signal is assumed to be produced by the following motion planner. \u02d9\u03b7r = R (\u03b7r) \u03bdr \u02d9\u03bdr = ar (25) where \u03b7r = [xr, yr, \u03c8r]T is the generalized reference position vector, \u03bdr = [ur, 0, rr]T is the generalized reference velocity vector, and ar = [ \u02d9ur, 0, \u02d9rr]T . In the simulation, the initial position vector \u03b7r (0) is chosen to be \u03b7r (0) = \ufffd 0, 0, \u03c0 4 \ufffdT , and we set ur (0) = 0.4 m/s and rr (0) = 0 rad/s. The reference acceleration \u02d9ur and angular rates are chosen to be \u02d9ur = \ufffd 0.005 m/s2 if t < 20 s 0 m/s2 otherwise (26) \u02d9rr = \ufffd \u03c0 600 rad/s2 if 25 s \u2264 t < 50 s 0 rad/s2 otherwise (27) TABLE I: Model parameters Parameters Values Parameters Values m 23.8 Y \u02d9r \u22120.0 Iz 1.76 Yr 0.1079 xg 0.046 Y|v|r \u22120.845 X \u02d9u \u22122.0 Y|r|r \u22123.45 Xu \u22120.7225 Nv \u22120.1052 X|u|u \u22121.3274 N|v|v 5.0437 Xuuu \u22121.8664 N|r|v \u22120.13 Y \u02d9v \u221210.0 N \u02d9r \u22121.0 Yv \u22120.8612 Nr \u22121.9 Y|v|v \u221236.2823 N|v|r 0.08 Y|r|v \u22120.805 N|r|r \u22120.75 ",
    "References": "REFERENCES [1] D. O.B.Jones, A. R.Gates, V. A.I.Huvenne, A. B.Phillips, and B. J.Bett, \u201cAutonomous marine environmental monitoring: Application in decommissioned oil \ufb01elds,\u201d Science of The Total Environment, vol. 668, no. 10, pp. 835\u2013 853, 2019. [2] J. Majohr and T. Buch, Advances in Unmanned Marine Vehicles. Institution of Engineering and Technology, 2006, ch. Modelling, simulation and control of an autonomous surface marine vehicle for surveying applications Measuring Dolphin MESSIN. [3] O. Levander, \u201cAutonomous ships on the high seas,\u201d IEEE Spectrum, vol. 54, no. 2, pp. 26 \u2013 31, 2017. [4] K. Do, Z. Jiang, and J. Pan, \u201cRobust adaptive path following of underactuated ships,\u201d Autonomous Agents and Multi-Agent Systems, vol. 40, no. 6, pp. 929 \u2013 944, Nov. 2004. [5] K. Do and J. Pan, \u201cGlobal robust adaptive path following of underactuated ships,\u201d Automatica, vol. 42, no. 10, pp. 1713 \u2013 1722, Oct. 2006. [6] C. R. Sonnenburg and C. A. Woolsey, \u201cIntegrated optimal formation control of multiple unmanned aerial vehicles,\u201d Journal of Field Robotics, vol. 3, no. 30, pp. 371 \u2013 398, May/Jun. 2013. (a) Model reference reinforcement learning control (b) Only deep reinforcement learning (c) Only baseline control Fig. 6: Trajectory tracking results of the three algorithms (The \ufb01rst evaluation) Fig. 7: Position tracking errors (ex) Fig. 8: Position tracking errors (ey) Fig. 9: Mean absolute distance errors ( \ufffd e2x + e2y) [7] T. I. Fossen, Handbook of Marine Craft Hydrodynamics and Motion Control. John Wiley & Sons, Inc., 2011. [8] R. A. Soltan, H. Ashra\ufb01uon, and K. R. Muske, \u201cState-dependent trajectory planning and tracking control of unmanned surface vessels,\u201d in Proceedings of 2009 American Control Conference. St. Louis, MO, USA: IEEE, Jun. 2009. [9] R. Yu, Q. Zhu, G. Xia, and Z. Liu, \u201cSliding mode tracking control of an underactuated surface vessel,\u201d IET Control Theory & Applications, vol. 6, no. 3, pp. 461 \u2013 466, 2012. [10] N. Wang, J.-C. Sun, M. J. Er, and Y.-C. Liu, \u201cA novel extreme learning control framework of unmanned surface vehicles,\u201d IEEE Transactions on Cybernetics, vol. 46, no. 5, pp. 1106 \u2013 1117, May 2016. [11] N. Wang, S. Lv, W. Zhang, Z. Liu, and M. J. Er, \u201cFinite-time observer based accurate tracking control of a marine vehicle with complex unknowns,\u201d arXiv preprint arXiv:1711.00832, vol. 145, no. 15, pp. 406 \u2013 415, 2017. [12] J. Woo, C. Yu, and N. Kim, \u201cDeep reinforcement learning-based controller for path following of an unmanned surface vehicle,\u201d Ocean Engineering, vol. 183, no. 1, pp. 155 \u2013 166, Dec. 2019. [13] Q. Zhang and H. H. Liu, \u201cUDE-based robust command \ufb01ltered backstepping control for close formation \ufb02ight,\u201d IEEE Transactions on Industrial Electronics, vol. 65, no. 11, pp. 8818\u20138827, Nov. 2018, early access online, March 12, 2018. [14] W. Shi, S. Song, C. Wu, and C. L. P. Chen, \u201cMulti pseudo q-learningbased deterministic policy gradient for tracking control of autonomous underwater vehicles,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 12, pp. 3534 \u2013 3546, Dec. 2019. [15] T. Shen and K. Tamura, \u201cRobust h\u221e control of uncertain nonlinear system via state feedback,\u201d IEEE Transactions on Automatic Control, vol. 40, no. 4, pp. 766 \u2013 768, Apr. 1995. [16] X. Liu, H. Su, B. Yao, and J. Chu, \u201cAdaptive robust control of a class of uncertain nonlinear systems with unknown sinusoidal disturbances,\u201d in Proceedings of 2008 47th IEEE Conference on Decision and Control. Cancun, Mexico, USA: IEEE, Dec. 2008. [17] W. M. Haddad and T. Hayakawa, \u201cDirect adaptive control for nonlinear uncertain systems with exogenous disturbances,\u201d International Journal of Adaptive Control and Signal Processing, vol. 16, no. 2, pp. 151 \u2013 172, Feb. 2002. [18] Q. Zhang and H. H. Liu, \u201cAerodynamic model-based robust adaptive control for close formation \ufb02ight,\u201d Aerospace Science and Technology, vol. 79, pp. 5 \u2013 16, 2018. [19] P. A. Ioannou and J. Sun, Robust Adaptive Control. Prentice-Hall, Inc., 1996. [20] B. Zhu, Q. Zhang, and H. H. Liu, \u201cDesign and experimental evaluation of robust motion synchronization control for multivehicle system without velocity measurements,\u201d International Journal of Robust and Nonlinear Control, vol. 28, no. 7, pp. 5437 \u2013 5463, 2018. [21] S. Mondal and hitralekha Mahanta, \u201cChattering free adaptive multivariable sliding mode controller for systems with matched and mismatched uncertainty,\u201d ISA Transactions, vol. 52, pp. 335 \u2013 341, 2013. (a) Model reference reinforcement learning control (b) Only deep reinforcement learning (c) Only baseline control Fig. 10: Trajectory tracking results of the three algorithms (The second evaluation) [22] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introductions, 2nd ed. The MIT Press, 2018. [23] E. Meyer, H. Robinson, A. Rasheed, and O. San, \u201cTaming an autonomous surface vehicle for path following and collision avoidance using deep reinforcement learning,\u201d arXiv preprint arXiv:1912.08578, 2019. [24] X. Zhou, P. Wu, H. Zhang, W. Guo, and Y. Liu, \u201cLearn to navigate: Cooperative path planning for unmanned surface vehicles using deep reinforcement learning,\u201d IEEE Access, vol. 7, pp. 165 262 \u2013 165 278, Nov. 2019. [25] M. Han, Y. Tian, L. Zhang, J. Wang, and W. Pan, \u201cH\u221e model-free reinforcement learning with robust stability guarantee,\u201d arXiv preprint arXiv:1911.02875, 2019. [26] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, \u201cSafe modelbased reinforcement learning with stability guarantees,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, Dec. 2017, p. 908919. [27] R. Sutton, A. Barto, and R. Williams, \u201cReinforcement learning is direct adaptive optimal control,\u201d IEEE Control Systems Magazine, vol. 12, no. 2, pp. 19 \u2013 22, Apr. 1992. [28] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, \u201cControl of a quadrotor with reinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 2, no. 4, pp. 2096 \u2013 2103, Oct. 2017. [29] R. Skjetne, T. I. Fossen, and P. V. Kokotovi\u00b4c, \u201cAdaptive maneuvering, with experiments, for a model ship in a marine control laboratory,\u201d Mathematics of Operations Research, vol. 41, pp. 289 \u2013 298, 2005. [30] Z. Peng, D. Wang, T. Li, and Z. Wu, \u201cLeaderless and leader-follower cooperative control of multiple marine surface vehicles with unknown dynamics,\u201d Nonlinear Dynamics, vol. 74, pp. 95 \u2013 106, 2013. [31] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \u201cSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,\u201d arXiv preprint arXiv:1801.01290, 2018. [32] T. Haarnoja, K. H. Aurick Zhou, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine, \u201cSoft actor-critic algorithms and applications,\u201d arXiv preprint arXiv:1812.05905, 2018. [33] G. E. Dahl, T. N. Sainath, and G. E. Hinton, \u201cImproving deep neural networks for lvcsr using recti\ufb01ed linear units and dropout,\u201d in Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, May 2013. [34] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, C. B. Stig Petersen, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, pp. 529\u2013533, Feb. 2015. [35] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.69801, 2014. [36] S. Fujimoto, H. van Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d arXiv preprint arXiv:1802.09477, 2018. [37] H. K. Khalil, Nonlinear Systems, 3rd ed. Prentice Hall, 2001. ",
    "title": "Model-Reference Reinforcement Learning Control of Autonomous",
    "paper_info": "Model-Reference Reinforcement Learning Control of Autonomous\nSurface Vehicles with Uncertainties\nQingrui Zhang1,2, Wei Pan2, and Vasso Reppa1\nAbstract\u2014 This paper presents a novel model-reference rein-\nforcement learning control method for uncertain autonomous\nsurface vehicles. The proposed control combines a conventional\ncontrol method with deep reinforcement learning. With the\nconventional control, we can ensure the learning-based con-\ntrol law provides closed-loop stability for the overall system,\nand potentially increase the sample ef\ufb01ciency of the deep\nreinforcement learning. With the reinforcement learning, we\ncan directly learn a control law to compensate for modeling\nuncertainties. In the proposed control, a nominal system is\nemployed for the design of a baseline control law using a\nconventional control approach. The nominal system also de\ufb01nes\nthe desired performance for uncertain autonomous vehicles\nto follow. In comparison with traditional deep reinforcement\nlearning methods, our proposed learning-based control can\nprovide stability guarantees and better sample ef\ufb01ciency. We\ndemonstrate the performance of the new algorithm via extensive\nsimulation results.\nI. INTRODUCTION\nAutonomous surface vehicles (ASVs) have been attracting\nmore and more attention, due to their advantages in many\napplications, such as environmental monitoring [1], resource\nexploration [2], shipping [3], and many more. Successful\nlaunch of ASVs in real life requires accurate tracking control\nalong a desired trajectory [4]\u2013[6]. However, accurate tracking\ncontrol for ASVs is challenging, as ASVs are subject to un-\ncertain nonlinear hydrodynamics and unknown environmental\ndisturbances [7]. Hence, tracking control of highly uncertain\nASVs has received extensive research attention [8]\u2013[12].\nControl algorithms for uncertain systems including ASVs\nmainly lie in four categories: 1) robust control which is the\n\u201cworst-case\u201d design for bounded uncertainties and disturbances\n[9]; 2) adaptive control which adapts to system uncertainties\nwith parameter estimations [4], [5]; 3) disturbance observer-\nbased control which compensates uncertainties and distur-\nbances in terms of the observation technique [11], [13];\nand 4) reinforcement learning (RL) which learns a control\nlaw from data samples [12], [14]. The \ufb01rst three algorithms\nfollow a model-based control approach, while the last one\nis data driven. Model-based control can ensure closed-loop\nstability, but a system model is indispensable. Uncertainties\nand disturbances of a system should also satisfy different\nconditions for different model-based methods. In robust\ncontrol, uncertainties and disturbances are assumed to be\nbounded with known boundaries [15]. As a consequence,\n1Department of Maritime and Transport Technology, Delft University of\nTechnology, Delft, the Netherlands Qingrui.Zhang@tudelft.nl;\nV.Reppa@tudelft.nl\n2Department of Cognitive Robotics, Delft University of Technology, Delft,\nthe Netherlands Wei.Pan@tudelft.nl\nrobust control will lead to conservative high-gain control\nlaws which usually limits the control performance (i.e.,\novershoot, settling time, and stability margins) [16]. Adaptive\ncontrol can handle varying uncertainties with unknown\nboundaries, but system uncertainties are assumed to be linearly\nparameterized with known structure and unknown constant\nparameters [17], [18]. A valid adaptive control design also\nrequires a system to be persistently excited, resulting in the\nunpleasant high-frequency oscillation behaviours in control\nactions [19]. On the other hand, disturbance observer-based\ncontrol can adapt to both uncertainties and disturbances\nwith unknown structures and without assuming systems to\nbe persistently excited [13], [20]. However, we need the\nfrequency information of uncertainty and disturbance signals\nwhen choosing proper gains for the disturbance observer-\nbased control, otherwise it is highly possible to end up with\na high-gain control law [20]. In addition, the disturbance\nobserver-based control can only address matched uncertainties\nand disturbances, which act on systems through the control\nchannel [18], [21]. In general, comprehensive modeling and\nanalysis of systems are essential for all model-based methods.\nIn comparison with model-based methods, RL is capable\nof learning a control law from data samples using much\nless model information [22]. Hence, it is more promising\nin controlling systems subject to massive uncertainties and\ndisturbances as ASVs [12], [14], [23], [24], given the\nsuf\ufb01ciency and good quality of collected data. Nevertheless,\nit is challenging for model-free RL to ensure closed-loop\nstability, though some research attempts have been made\n[25]. It implies that the learned control law must be re-\ntrained, once some changes happen to the environment or the\nreference trajectory (i.e. in [14], the authors conducted two\nindependent training procedures for two different reference\ntrajectories.). Model-based RL is possible to learn a control\nlaw which ensures the closed-loop stability by introducing a\nLyapunov constraint into the objective function of the policy\nimprovement according to the latest research [26]. However,\nthe model-based RL with stability guarantees requires an\nadmissible control law \u2014 a control law which makes the\noriginal system asymptotically stable \u2014 for the initialization.\nBoth the Lyapunov candidate function and complete system\ndynamics are assumed to be Lipschitz continuous with known\nLipschitz constants for the construction of the Lyapunov\nconstraint. It is challenging to \ufb01nd the Lipschitz constant\nof an uncertain system subject to unknown environmental\ndisturbances. Therefore, the introduced Lyapunov constraint\nfunction is restrictive, as it is established based on the worst-\ncase consideration [26].\narXiv:2003.13839v1  [eess.SY]  30 Mar 2020\n",
    "GPTsummary": "- (1): This paper focuses on the control of highly uncertain autonomous surface vehicles to achieve accurate tracking control to follow a desired trajectory.\n\n\n- (2): The four categories of control algorithms used for uncertain systems, including autonomous surface vehicles, are discussed, namely, robust control, adaptive control, disturbance observer-based control, and reinforcement learning. However, these conventional control algorithms have their own limitations in handling uncertainties and disturbances. The approach proposed in this paper is well motivated given the limitations of conventional control algorithms.\n\n\n- (3): The proposed control method combines a conventional control method with deep reinforcement learning. In this way, a control law is directly learned to compensate for modeling uncertainties. A nominal system is employed for the design of a baseline control approach, and the nominal system defines the desired performance for uncertain autonomous vehicles to follow. With the deep reinforcement learning, it can provide stability guarantees and better sample efficiency.\n\n\n- (4): The proposed algorithm's performance was demonstrated through extensive simulation results. The results show that the new algorithm can successfully control the uncertain autonomous surface vehicles to track the desired trajectory with high accuracy, even in the presence of modeling uncertainties and environmental disturbances. The achieved performance supports their goals.\n\n\n\n\n\n8. Conclusion: \n\n- (1):The significance of this work is to propose a novel approach to control highly uncertain autonomous surface vehicles, which combines conventional control methods with deep reinforcement learning. The proposed approach can achieve accurate tracking control and better sample efficiency, even in the presence of modeling uncertainties and environmental disturbances. This work has strong practical implications for the development of autonomous surface vehicles.\n\n- (2):Innovation point: The proposed approach combines conventional control methods with deep reinforcement learning to address the limitations of conventional control algorithms in handling uncertainties and disturbances. This approach can provide stability guarantees and better sample efficiency.\n\nPerformance: The extensive simulation results demonstrate that the proposed approach can successfully control the uncertain autonomous surface vehicles to track the desired trajectory with high accuracy, even in the presence of modeling uncertainties and environmental disturbances.\n\nWorkload: The paper provides a clear and detailed description of the proposed approach and its implementation, as well as the simulation setup and results. However, the paper lacks a comparison with other state-of-the-art approaches in terms of performance and workload.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1):The significance of this work is to propose a novel approach to control highly uncertain autonomous surface vehicles, which combines conventional control methods with deep reinforcement learning. The proposed approach can achieve accurate tracking control and better sample efficiency, even in the presence of modeling uncertainties and environmental disturbances. This work has strong practical implications for the development of autonomous surface vehicles.\n\n- (2):Innovation point: The proposed approach combines conventional control methods with deep reinforcement learning to address the limitations of conventional control algorithms in handling uncertainties and disturbances. This approach can provide stability guarantees and better sample efficiency.\n\nPerformance: The extensive simulation results demonstrate that the proposed approach can successfully control the uncertain autonomous surface vehicles to track the desired trajectory with high accuracy, even in the presence of modeling uncertainties and environmental disturbances.\n\nWorkload: The paper provides a clear and detailed description of the proposed approach and its implementation, as well as the simulation setup and results. However, the paper lacks a comparison with other state-of-the-art approaches in terms of performance and workload.\n\n\n"
}