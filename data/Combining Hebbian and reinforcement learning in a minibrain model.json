{
    "Abstract": "Abstract A toy model of a neural network in which both Hebbian learning and reinforcement learning occur is studied. The problem of \u2018path interference\u2019, which makes that the neural net quickly forgets previously learned input\u2013output relations is tackled by adding a Hebbian term (proportional to the learning rate \u03b7) to the reinforcement term (proportional to \u03c1) in the learning rule. It is shown that the number of learning steps is reduced considerably if 1/4 < \u03b7/\u03c1 < 1/2, i.e., if the Hebbian term is neither too small nor too large compared to the reinforcement term. Preprint submitted to Elsevier Science 5 October 2018 1 Introduction The central question which we address in this article is in what way a biological neural network, i.e., the brain, or, more generally, a part of the nervous system of an animal or human being, may learn to realize input\u2013output relations. By biological we here mean: realizable with the help of elements occurring in nature, e.g., neurons or chemical substances that may in\ufb02uence other neurons or the synaptic e\ufb03cacy. An example of an input\u2013output relation is a motor task, like catching a prey, in reaction to visual, auditive, or other input. Many attempts to explain the way input\u2013output relations of this kind might be realized by (arti\ufb01cial) neural nets are encountered in the literature, most of which are not satisfactory from a biological point of view as we will illustrate in subsection 1.1. It is the purpose of this article to combine ideas which do satisfy certain biological constraints and study a toy model, in particular with respect to its ability to learn and realize input\u2013output relations. The widely accepted idea of Hebbian learning [5] at the one hand will be combined with some rule that implements a feedback signal at the other hand, in a way that, in principle, might be biologically realizable. Without the addition of any feedback-signal, learning of prescribed input\u2013output relations \u2014whether in reality or in a model\u2014 is, of course, impossible. 1.1 Arti\ufb01cial learning rules If one wants a network to learn to realize input\u2013output relations, there are various well-known prescriptions, associated with names like perceptron learning rule, back-propagation or Boltzmann machines [12,7]. None of these, however, model the functioning of real brains, since the learning rules in question violate the existing biological limitations. In order to illustrate this statement, let us give an example. Consider a single layered feed-forward network, i.e., a network consisting of an input and an output layer only, in which signals are sent by neurons of the input layer to neurons of the output layer, and not the other way around. Let wij be the strengths or \u2018weights\u2019 of the connections in this simple net. In 1962, Rosenblatt [13] proved that such a network will realize desired input\u2013output relations if, a \ufb01nite number of times, the weights are adapted according to the rule wij \u2192 wij + \u2206wij (1) 2 with \u2206wij = \u03b5(xTi \u2212 xOi)xj (2) where xTi is the desired or target output of neuron i, and xOi is its actual output. Furthermore, xj is the state of the pre-synaptic input neuron j and \u03b5 is some function of the neuron states and properties of neurons i and j. This learning rule can not be realized by a biological neural net since neuron i, producing xOi, cannot know that it should produce xTi. If, e.g., an animal does not succeed in catching a prey, its neurons get no speci\ufb01c feedback individually, on what the right output xTi should have been. Hence, xTi \u2212 xOi cannot be determined by the biological system, and, therefore, neither can it adapt the weights according to (2). Consequently, the perceptron learning rule (2) is unsuitable for a realistic modeling of the way in which a biological neural net can learn and realize input\u2013output relations. Similar observations can be made for back-propagation or Boltzmann machines. 1.2 Biological learning rules; Hebbian learning and reinforcement learning Already in 1949, Hebb suggested [5] that, in biological systems, learning takes place through the adaptation of the strengths of the synaptic interactions between neurons, depending on the activities of the neurons involved. In a model using binary neurons, i.e., xi = 0 or xi = 1, the most general form of a learning rule based on this principle is a linear function in xi and xj since x2 i = xi and x2 j = xj. It therefore reads \u2206wij = aij + bijxi + cijxj + dijxixj (3) In a biological setting, the coe\ufb03cients aij, bij, cij and dij in this learning rule can only depend on locally available information, such as the values of the membrane potential hi = \ufffd j wijxj (4) and the threshold potential \u03b8i of neuron i. In this way, the system adapts its weights without making use of neuron-speci\ufb01c information, like, e.g., xTi, of which there can be no knowledge, locally, at the position of the synapse. In a recurrent neural net, a Hebbian learning rule su\ufb03ces to store patterns [6,12] if all neurons are clamped to the patterns which are to be learned during a certain period, the \u2018learning stage\u2019. In feed-forward networks, however, only 3 the neurons of the input layers are clamped, and some kind of feed-back signal, governing the direction of adaptation of the synapses during the learning procedure, is indispensable. Probably the simplest form of such a signal one can think of is a \u2018success\u2019 or \u2018failure\u2019 signal that is returned to the network after each attempt to associate the correct output to given input. On the basis of trial and error, a neural network can then indeed learn certain tasks, the principle on which \u2018reinforcement learning\u2019 is based [3,14]. This principle of reinforcement learning has a rather natural interpretation: satisfactory behavior is rewarded, or reinforced, causing this behavior to occur more frequently. The reinforcement signal is supplied by the subject\u2019s environment, or by its own judgment of the e\ufb00ect of its behavior. In a biological perspective, one could think of the synaptic change being in\ufb02uenced by some chemical substance, which is released depending on whether the evaluation by the subject of the e\ufb00ect of the output is positive or negative, i.e., whether it is happy or unhappy with the attempt it made. Note that, in learning by reinforcement, the search for the correct output is more di\ufb03cult, and, hence, slower, than for non-biologically realizable algorithms like the perceptron learning rule or back-propagation. This is not surprising, since the latter give the system locally speci\ufb01c information on how to adjust individual weights, while reinforcement rules only depend upon a global \u2018measure of correctness\u2019. The most general form of a reinforcement learning algorithm is given by the prescription \u2206wij = aij(r) + bij(r)xi + cij(r)xj + dij(r)xixj (5) Here the coe\ufb03cients aij, bij, cij, dij, besides their dependence on the local variables such as the membrane potential, will in principle also depend on a reinforcement signal, denoted by r. The value of r is usually a real number between 0 and 1, denoting the degree of success (r = 1 means success, r = 0 means failure). An important issue in the literature on reinforcement learning is the so called \u2018credit assignment problem\u2019 [14]. It refers to the question how a neural net knows which connections wij were responsible for a successful or unsuccessful trial, and, as a consequence, which connections should be \u2018rewarded\u2019, and which should be \u2018punished\u2019, respectively. In their article \u2018Learning from mistakes\u2019 (1999), Chialvo and Bak [4], proposed a class of networks, in which learning takes place on the basis of a \u2018deinforcement signal\u2019 only, i.e., the weights of active synapses are decreased if the output is wrong, they are \u2018punished\u2019, so to say, in case of wrong performance of the network. If the output is right nothing happens. This procedure 4 works as long as the average activity in the net is kept very low: when only a few neurons are active at an unsuccessful attempt, one can be sure that the connections between these active neurons were the ones which were responsible, and thus should be \u2018punished\u2019. In this way Chialvo and Bak obtained an elegant solution to the credit assignment problem. The absence of a reinforcement signal (nothing happens if r = 1) makes their learning rule relatively simple. It is a version of the general rule (5) with bij = 0 and cij = 0: \u2206wij = \u2212(1 \u2212 r)(\u03c1xixj \u2212 \u03d5) (6) where \u03c1 and \u03d5 are positive constants; in this article we will suppose \u03d5 << \u03c1. A biological mechanism that could implement the learning rule (6) is the following: if the output is correct, nothing happens, since the network obviously performs satisfactory. If not, a chemical substance is released, which has the e\ufb00ect that synapses between neurons that have just been active, and thereby are \u2018tagged\u2019 in some electro-chemical way, are depressed. 1.3 Purpose The success of the \u2018minibrain model\u2019 of Chialvo and Bak [4] (as Wakeling and Bak referred to it in [15]), is limited to feed-forward neural nets in which the number of input and output neurons (or, equivalently in this model, the number of patterns) is small compared to the number of neurons in the hidden layer. As the number of neurons in the hidden layer decreases, learning, at a certain moment, becomes impossible: \u2018path interference\u2019 is the phenomenon which causes this e\ufb00ect [16]. Essentially, it amounts to the following. If, in each layer of the feed-forward neural net, only one neuron is active at each time step, an input\u2013output relation corresponds to a path of activity along the strongest connections between the neurons. Basically, path interference comes down to the erasure of an existing path of activity, which was correct at a previous learning step, by a change due to a punishment of a connection while trying to learn a di\ufb00erent input\u2013output relation. If the probability for this path interference to occur becomes too large, learning times tend to in\ufb01nity. In this article we attempt to improve the performance of the minibrain model of Chialvo and Bak \u2014in the sense of decreasing the learning time\u2014 by making sure that, at the occurrence of path interference, the punishment of a correct activity path is no longer such that the memory is erased. We achieve this by adding to the deinforcement term in the learning rule (6), which is proportional to \u03c1, a Hebbian term proportional to \u03b7. The latter term has the e\ufb00ect that an active path is strengthened, mitigating in this way the punishment. 5 By choosing the ratio between the coe\ufb03cients \u03b7 and \u03c1 of both terms in the learning rule appropriately, we are able to reduce the number of learning steps signi\ufb01cantly, without making the model less realistic from a biological point of view. In fact, in the class of models we study, Hebbian learning is a most appropriate way to account for biological observations like LTP and LTD [8]. In section 4 we explain that if the quotient of the Hebbian learning rate and the coe\ufb03cient of the deinforcement term is in the range 1 4 < \u03b7 \u03c1 < 1 2 (7) learning times are reduced considerably. In their article [2], Chialvo and Bak proposed a di\ufb00erent way to solve the problem of path interference. They reduced the amount of punishment of the connections that once had been active in a correct response. In this model a neuron needs to memorize whether it previously was active in a successful trial. In our model such a neuron memory is not needed. Let us denote the deinforcement contribution to learning by \u2206w\u2032 ij and denote the Hebbian part by \u2206w\u2032\u2032 ij. We will study a learning rule of the form \u2206wij = \u2206w\u2032 ij + \u2206w\u2032\u2032 ij (8) From all possibilities for Hebbian learning summarized by equation (3), we choose for \u2206w\u2032\u2032 ij a rule in which the coe\ufb03cients aij and bij both are zero: \u2206w\u2032\u2032 ij = \u03b5(xi, xj)(2xi \u2212 1)xj (9) We choose this particular form since it can be argued that this form is a most plausible candidate from a biological point of view [6]. Our paper has been set up as follows. In section 2, we describe a feed-forward network with one hidden layer, which we will use to study the learning rule (8), with (6) and (9). In section 3, numerical studies of various situations are given and explained. It turns out, in general, that taking into account Hebbian learning, and viewing it as a process which is permanently active, irrespective of the occurrence of reinforcement learning, has a positive in\ufb02uence on the learning time of the neural net. This is a new result, which, to the best of our knowledge, has not been noticed before. 6 NH NO NI Fig. 1. An example of a fully connected feed forward network with NI input neurons, NH hidden neurons and NO output neurons. The \ufb01lled circles represent active neurons. 2 Description of the model: updating rules for neuron activities and connection weights In order to explore a simpli\ufb01ed model of the brain we consider a fully connected, feed-forward neural network with an input layer of NI neurons, one hidden layer of NH neurons, and an output layer of NO neurons, see \ufb01gure 1. The state xi of neuron i is 1 if neuron i \ufb01res and 0 if it does not. In general, a neuron \ufb01res if its potential hi is su\ufb03ciently high, where hi stands for the membrane potential Vex \u2212 Vin, the di\ufb00erence between the intra- and extra cellular potentials Vin and Vex. Following Chialvo and Bak [4], we model the dynamics of the neural net by simply stating that in the hidden and output layers a given number of neurons having the highest potentials hi \u2014in their respective layers\u2014 are those that will \ufb01re. In their terminology: we use extremal dynamics. For McCulloch and Pitts neurons a way to control the average activity might be realized via suitably chosen threshold potentials \u03d1i (see e.g. [1], [3]). In nature, the average activity will depend on the threshold potentials and may, moreover, be in\ufb02uenced by chemical substances or the network structure [8,10]. In our model we put the threshold potentials \u03b8i equal to zero: \u03b8i = 0 (10) The number of neurons in the input, hidden and output layers that we choose to be active, will be denoted by N(a) I , N(a) H and N(a) O , respectively. The input pattern, a speci\ufb01c set of states of neurons in the input layer, will be denoted by \u03beI = (\u03beI1, ..., \u03beINI). The network is to associate every input pattern with a desired, or target, output pattern, \u03beT = (\u03beT1, ..., \u03beTNO). The \u03beI and \u03beT are vectors with components equal to 0 or 1. Consequently, the number of active neurons of the input and output layer are given by 7 N(a) I = NI \ufffd j=1 \u03beIj (11) N(a) O = NO \ufffd j=1 \u03beTj (12) In our numerical experiments, these numbers will be taken to be equal. Moreover, the number of active neurons in the hidden layer, N(a) H = NH \ufffd j=1 xHj (13) where xHj is the neuron state of neuron j in the hidden layer, will also be equal to the number of active neurons in the other layers. Hence, we choose N(a) I = N(a) O = N(a) H . We thus have explicitated the network dynamics. We now come to the update rules for the synaptic weights wij. Updating of the network state will happen at discrete time steps. At every time step tn, all neuron states are updated in the order: input layer \u2013 hidden layer \u2013 output layer. This being done, the values of the weights are updated, according to wij(tn+1) = wij(tn) + \u2206wij(tn) (14) Substituting (9) and (6) into (8), we \ufb01nd \u2206wij = \u03b5(xi, xj)(2xi \u2212 1)xj \u2212 (1 \u2212 r)(\u03c1xixj \u2212 \u03d5) (15) For the pre-factor of the Hebbian term we take [6] \u03b5(xi, xj) = \u03b7(\u03ba \u2212 (2xi \u2212 1)(hi \u2212 \u03b8i)) (16) The constants \u03b7 and \u03ba are positive numbers, the so-called learning rate and margin parameter. Finally, combining the above ingredients and noting that we chose \u03b8i = 0, the learning rule reads: \u2206wij(tn) = \u03b7[\u03ba \u2212 hi(tn)(2xi(tn) \u2212 1)][2xi(tn) \u2212 1]xj(tn) +(1 \u2212 r)[\u2212\u03c1xi(tn)xj(tn) + \u03d5] (17) Note that xi(tn) and xj(tn) are the activities of neurons in adjacent layers, since in our model there are no lateral connections. The constant \u03d5 is chosen in such a way that the change in \ufffd i,j wij, where the sum is extended over i and j in adjacent layers, due to the \u03c1-term (not due to the Hebbian term), 8 is independent of \u03c1. This can easily be achieved by choosing \u03d5 = \u03c1/P, where P is the product of the numbers of neurons in two adjacent layers, i.e., \u03d5 is equal to either \u03c1/NINH or \u03c1/NHNO. 3 Numerical Simulations The network described in the previous section will now \ufb01rst be studied numerically. The numerical details are: \u2013 The initial weights wij(t0) are randomly chosen with values between \u22120.01 and +0.01. \u2013 The punishment rate \u03c1 will be kept constant at 0.02. Thus when we vary the \u03b7/\u03c1 ratio, we vary the learning rate \u03b7. \u2013 The margin parameter \u03ba, appearing in (16), will be kept at the value 1. \u2013 Whenever the number of neurons in the input, hidden or output layer is \ufb01xed, we choose NI = 8, NH = 512 and NO = 8. \u2013 All data are averaged over 512 samples. The network is confronted with p di\ufb00erent input patterns \u03be\u00b5 I , (\u00b5 = 1, . . . , p), to which correspond equally many target patterns \u03be\u00b5 T. Learning proceeds as follows. The input layer remains clamped to the \ufb01rst input pattern until the time step at which the target pattern has been found. As soon as this inputoutput relation \u00b5 = 1 has been learned, we switch to input pattern \u00b5 = 2. After the corresponding target pattern has been found we continue, up to the p-th input-output relation. Then, we have completed what we will refer to as one \u2018learning cycle\u2019. After this cycle we start the process again, up to the point where the network can recall all p input-target relations at once. When that is the case, learning stops. We count the number of learning steps needed to learn all input\u2013output relations. Chialvo and Bak consider the case of one active neuron in each layer. In section 3.1 we present a numerical experiment with a neural network for which the activities are larger than one, i.e., N(a) I > 1, N(a) H > 1 and N(a) O > 1. In particular we study the total number of learning steps as a function of the ratio \u03b7/\u03c1. In section 3.2 we vary the number of neurons in input and output layer and keep the hidden layer \ufb01xed, and vice versa. Finally, in section 4, we interpret our results. 9 3.1 E\ufb00ect of the Hebbian term In \u2018Learning from mistakes\u2019 Chialvo and Bak [4] studied the case of one active neuron in the input, the hidden and the output layers: N(a) I = N(a) H = N(a) O = 1. We here will study what happens when N(a) I , N(a) H and N(a) O all are larger than one. In our \ufb01rst numerical experiment we take a network with p = 8 input\u2013target relations for which, in each input or target pattern \u00b5, the number of active neurons is 2, i.e., N(a) I = N(a) H = N(a) O = 2. In \ufb01gure 2 the number of learning steps is plotted against the ratio \u03b7/\u03c1 of the two proportionality coe\ufb03cients related to the Hebbian and the deinforcement term respectively. \ufffd =\ufffd 1000 2000 3000 4000 5000 6000 7000 8000 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Num b er of learning steps 0 Fig. 2. The number of learning steps as a function of the quotient \u03b7/\u03c1. There are eight input\u2013target relations to be learned and two active neurons in each layer. The number of neurons in the input, hidden and output layers are NI = 8, NH = 512 and NO = 8. Initially, the number of learning steps increases as a result of the Hebbian learning term, but at \u03b7/\u03c1 = 0.1 the number of learning steps starts to decrease dramatically from 7500 to 250 at \u03b7/\u03c1 = 0.25. For \u03b7/\u03c1 > 0.50, learning is impossible. From \ufb01gure 2 we see that, when there is no Hebbian component in the learning rule (\u03b7 = 0), the net needs 2500 learning steps to learn an input-output task. When we add a slight Hebbian component (\u03b7 small) the number of learning steps increases, and, hence, the ability of the net to learn diminishes. However, when the Hebbian component becomes more and more important, the number of learning steps starts to decrease dramatically: for \u03b7/\u03c1 between 0.25 and 0.5 the number of learning steps is approximately 250. The Hebbian component, which has the tendency to engrave previously learned patterns, seems to help to not forget the old patterns. If \u03b7/\u03c1 exceeds the value 0.5, learning fails. Apparently, the \u2018progressive\u2019 \u03c1 term, the power of which is to help the network to quickly adapt itself to new input-output relations, cannot conquer the \u2018conservative\u2019 power of the \u03b7-dependent (i.e., the Hebbian) term. 10 We will come back to these points and consider the e\ufb00ects of the \u03b7 and \u03c1 terms in some detail in section 4. 3.2 Size dependences In this section we consider the network of \ufb01gure 1 for varying numbers NI = NO and NH. 3.2.1 E\ufb00ect of varying the sizes of the input and output layers In this subsection we test the performance of the network for various sizes of the input, output and hidden layers. In particular, we chose to study the size-dependence for three di\ufb00erent values of the learning parameter: \u03b7/\u03c1 = 0, \u03b7/\u03c1 = 0.10 and \u03b7/\u03c1 = 0.45, values which we selected on the basis of the results of the previous subsection. First, we take a network with the \ufb01xed number of 512 neurons in the hidden layer, and only one active neuron per layer. The input and output layers will consist of increasing, equal numbers of neurons, starting with NI = NO = 4. Moreover, we choose the number of input\u2013output relations p to be learned equal to the number of neurons in the input and output layers. The input and output layers will be enlarged in steps of 4 neurons, up to NI = NO = 28 neurons. In Figure 3 we give the number of learning steps per pattern for the above mentioned three values of \u03b7/\u03c1. The positive e\ufb00ect of the addition of a Hebbian term to the learning rule becomes more and more convincing with increasing number of input\u2013output relations to be learned. 3.2.2 E\ufb00ect of varying the size of the hidden layer Next we consider a network with 8 input neurons, 8 output neurons and 8 subsequent patterns. The number of active neurons is 2 for all input and target patterns. We vary the number of neurons in the hidden layer. In Figure 4 we have plotted the number of learning steps as a function of the number of neurons in the hidden layer for three values of the quotient \u03b7/\u03c1. Note that, in agreement with \ufb01gure 2 the number of learning steps is the largest for \u03b7/\u03c1 = 0.1 (the symbols \u25a1 in \ufb01gure 4). A suitably chosen value for the coe\ufb03cient \u03b7 of the Hebbian term makes it possible for the network to perform satisfactory with very small number of neurons in the hidden layer (the symbols \u20dd in the \ufb01gure). 11 4 8 16 32 64 128 256 512 1024 4 8 12 16 20 24 28 Number of learning steps per pattern Number of patterns to be learned Fig. 3. The number of learning steps per pattern as a function of the number of input\u2013output relations p, for \u03b7/\u03c1 = 0 (\u25a0), \u03b7/\u03c1 = 0.1 (\u25a1) and \u03b7/\u03c1 = 0.45 (\u20dd). Input and output patterns have only one active neuron. The number of neurons in input and output layers equals the number of patterns p. Note the logarithmic scale of the vertical axis. For a small number of input\u2013output patterns, the learning time is roughly equal for di\ufb00erent values of \u03b7/\u03c1. The advantageous e\ufb00ect of a Hebbian term in the learning rule for this learning task becomes more and more pronounced with increasing numbers of input\u2013target relations. 128 256 512 1024 2048 4096 8192 16384 32768 0 512 1024 1536 2048 2560 3072 3584 4096 Number of learning steps Number of neurons in the hidden layer Fig. 4. Dependence of the number of learning steps on the number of neurons in the hidden layer of the network. The symbols (\u25a0), (\u25a1) and (\u20dd) correspond to \u03b7/\u03c1 = 0, \u03b7/\u03c1 = 0.1 and \u03b7/\u03c1 = 0.45 respectively. All input patterns and output patterns have 2 active neurons. The number of input neurons, output neurons and patterns are \ufb01xed; NI = 8, NO = 8, p = 8. 12 w max h = w max I H O A B t p t q C D w \rom h = w \rom Fig. 5. Path interference At both the time steps tp and tq the neuron C of the hidden layer \ufb01res, and as a result the same neuron D of the output layer is activated. This unwanted e\ufb00ect is due to the fact that the connection of B and C happens to be the largest one. The paths ACD and BCD partially overlap. 4 Explanation of the e\ufb00ect of the Hebbian term The di\ufb00erent behavior for di\ufb00erent values of \u03b7/\u03c1 is mainly due to its consequences for the e\ufb00ect we call path interference, after Wakeling [16], who studied the critical behavior of the Chialvo & Bak minibrain. As an example, let us consider the case in which only one neuron is active in each layer. In this case, the \u2018path of activity\u2019 from the active input neuron to the corresponding output neuron runs along the outgoing connections with the largest weights. During the learning process, it is possible that an established path (connecting, e.g., the active neuron of pattern \u03be1 I with the active neuron, in the output layer, of \u03be1 O) is \u2018wiped out\u2019 by an attempt to learn one of the other input\u2013output relations. This is likely to happen if the same neuron in the hidden layer becomes active, and, consequently, the connection to the output neuron corresponding to the previously learned pattern is punished by an amount \u03c1. This phenomenon of path interference will occur once in a while, irrespective of the values of the parameters \u03b7 and \u03c1. However, the question whether the memory of the old pattern is wiped out (i.e., whether the connection to the output neuron under consideration is no longer the largest), does depend on the parameters \u03c1 and \u03b7. To \ufb01nd out how, we should consider the change of this connection compared to the change of the other connections from the same hidden neuron to the other output neurons. For the total relative change, two di\ufb00erent learning steps should be taken into account. Firstly, the one at tp, at which the right output was found, and the deinforcement term did not contribute, and, secondly, the learning step at tq, at which path interference occurred, and the deinforcement term did contribute. Let wwin be the largest outgoing weight from the active neuron in the hidden 13 layer to the output layer, and let wcom be a weight value which is representative for one of the other, competing weights connecting the same neuron in the hidden layer to a di\ufb00erent neuron in the output layer. The membrane potentials of each neurons i of the output layer are given, according to (4), by hi = wij, where j is a neuron of the hidden layer. From (17), with xj = 1, we \ufb01nd in case of success (r = 1) for the changes of the connections to the winning (xi) and the competing (xi = 0) neurons in the output layer: \u2206wwin(tp) = \u03b7[\u03ba \u2212 wwin(tp)] (18) \u2206wcom(tp) = \u2212\u03b7(\u03ba + wcom(tp)) (19) respectively. Similarly, in case of failure (r = 0) these changes are \u2206wwin(tq) = \u03b7(\u03ba \u2212 wwin(tq)) \u2212 \u03c1 + \u03d5 (20) \u2206wcom(tq) = \u2212\u03b7(\u03ba + wcom(tq)) + \u03d5 (21) respectively. Only if the increase of wcom is larger than the increase of wwin, the memory path can be wiped out, since then wcom may become the largest weight, i.e., if \u2206wcom(tp) + \u2206wcom(tq) > \u2206wwin(tp) + \u2206wwin(tq) (22) We now substitute (18)\u2013(21) into (22). In the resulting inequality we can ignore the values of wwin and wcom relative to \u03ba as long as the number of adaptations of wwin and wcom is small; recall that \u03ba = 1, \u03c1 = 0.02, and the initial values of the weights are in the range [\u22120.01, 0.01] in our numerical experiments. With these approximations, the inequality reduces to \u03c1 > 4\u03b7. In the opposite case, \u03c1 < 4\u03b7 (23) wwin will remain larger than wcom and, consequently, path interference will not wipe out learned input\u2013output relations, which explains the decrease of the number of learning steps for \u03b7 > 1 4\u03c1. For \u03b7 > 1 2 the network is incapable of learning input\u2013output relations. This can be seen as follows. Each time a winning connection is punished, i.e., the output is wrong, it changes approximately by an amount \u03b7 \u2212 \u03c1, whereas the competing connection changes by an amount of \u2212\u03b7. Hence, only when \u03b7 \u2212 \u03c1 < \u2212\u03b7, or, equivalently, when 2\u03b7 < \u03c1 (24) the winning connection decreases more than the competing connection. In 14 the opposite case, 2\u03b7 > \u03c1, the winning connection remains larger than its competitor, and, at the next learning step, the output will be wrong again. Combining the inequalities (23) and (24), we \ufb01nd the central result of this article (7), the parameter region for which a Hebbian term in the learning rule is advantageous. This observation is con\ufb01rmed by the numerical experiment of section 3.1, so, in particular, \ufb01gure 2. Note that the reasoning leading to the main results (23) and (24) was based on an assumption regarding the initial values. In particular, it was assumed that the weights were small compared to \u03ba (which was put equal to 1). In reference [6] it was shown that the pre-factor (16) of the Hebbian term tends to zero during the learning process: \u03ba \u2212 (2xi \u2212 1)(hi \u2212 \u03b8i) \u2192 0 (25) implying, that for a small number of active neurons the absolute values of the weights are of the order \u03ba, as follows with (4) and (10). Hence, the assumption that the weights are small compared to \u03ba (\u03ba = 1) is guaranteed to break down at a certain point in the learning process. 5 Summary We have shown, in a particular model, that a Hebbian component in a reinforcement rule improves the ability to learn input\u2013output relations by a neural net. References [1] Alstrom P and Stassinopoulos D 1995 Phys. Rev. E 51 5027 [2] Bak P and Chialvo D R 2001 Phys. Rev. E 63 031912 [3] Barto A G 1995 Reinforcement learning: The Handbook of Brain Theory and Neural Networks ed M A Arbib (Cambridge, Massachusetts: MIT Press) 804809 [4] Chialvo D R and Bak P 1999 Neuroscience 90 1137 [5] Hebb D O 1949 The Organization of Behaviour (New York: Wiley) 62 [6] Heerema M and van Leeuwen W A 1999 J. Phys. A: Math. Gen. 32 263 15 ",
    "Introduction": "",
    "References": "References [1] Alstrom P and Stassinopoulos D 1995 Phys. Rev. E 51 5027 [2] Bak P and Chialvo D R 2001 Phys. Rev. E 63 031912 [3] Barto A G 1995 Reinforcement learning: The Handbook of Brain Theory and Neural Networks ed M A Arbib (Cambridge, Massachusetts: MIT Press) 804809 [4] Chialvo D R and Bak P 1999 Neuroscience 90 1137 [5] Hebb D O 1949 The Organization of Behaviour (New York: Wiley) 62 [6] Heerema M and van Leeuwen W A 1999 J. Phys. A: Math. Gen. 32 263 15 [7] Hertz J, Krogh A and Palmer R G 1991 Introduction to the theory of neural computation (Redwood City, California: Addison-Wesley) [8] Kandel E R, Schwartz J H and Jessell T M 1991 Principles of Neural Science, Third Edition (Englewood-Cli\ufb00s, New Jersey: Prentice-Hall International Inc.) [9] Klemm K, Bornhodt S and Schuster H G 2000 Phys. Rev. Lett. 84 1813 [10] Kolb B and Whishaw I Q 1990 Human Neuropsychology, 3rd edition (New York: Freeman) [11] Minsky M L and Papert S A 1969 Perceptrons: An Introduction to Computational Geometry (Cambridge, Massachusetts: MIT Press) [12] M\u00a8uller B, Reinhardt J and Strickland M T 1990 Neural Networks, An Introduction (Berlin: Springer-Verlag) [13] Rosenblatt F 1962 Principles of Neurodynamics (New York: Spartan) [14] Sutton R S and Barto A G 1998 Reinforcement learning: An introduction (Cambridge, Massachusetts: MIT Press) [15] Wakeling J and Bak P 2001 arXiv:nlin.AO/0201046 [16] Wakeling J 2002 cond-mat/0204562 16 ",
    "title": "Combining Hebbian and reinforcement",
    "paper_info": "arXiv:cond-mat/0301627v1  [cond-mat.dis-nn]  31 Jan 2003\nCombining Hebbian and reinforcement\nlearning in a minibrain model\nR. J. C. Bosman, W. A. van Leeuwen and B. Wemmenhove\nInstitute for Theoretical Physics, University of Amsterdam, Valckenierstraat 65,\n1018 XE Amsterdam, The Netherlands\nAbstract\nA toy model of a neural network in which both Hebbian learning and reinforcement\nlearning occur is studied. The problem of \u2018path interference\u2019, which makes that the\nneural net quickly forgets previously learned input\u2013output relations is tackled by\nadding a Hebbian term (proportional to the learning rate \u03b7) to the reinforcement\nterm (proportional to \u03c1) in the learning rule. It is shown that the number of learning\nsteps is reduced considerably if 1/4 < \u03b7/\u03c1 < 1/2, i.e., if the Hebbian term is neither\ntoo small nor too large compared to the reinforcement term.\nPreprint submitted to Elsevier Science\n5 October 2018\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing a novel approach to the problem of path interference in neural networks by combining Hebbian and reinforcement learning. This provides new insights into how different learning mechanisms can be combined to enhance the capabilities of machine learning models.\n\n- (2): In terms of innovation point, this article offers a new approach to the problem of path interference in neural networks. By combining Hebbian and reinforcement learning, the authors show that the learning performance can be improved. However, the proposed methodology was only tested on a toy model of a neural network, and it is unclear how well it will perform on more complex models. In terms of performance, the proposed method achieved a considerably reduced number of learning steps compared to past methods. However, as mentioned earlier, the generalizability of the results are yet to be tested. In terms of workload, the article presents a clear methodology and experimental setup, which allows other researchers to replicate the results more easily. Overall, this article provides an interesting contribution to the field of reinforcement learning and neural networks.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing a novel approach to the problem of path interference in neural networks by combining Hebbian and reinforcement learning. This provides new insights into how different learning mechanisms can be combined to enhance the capabilities of machine learning models.\n\n- (2): In terms of innovation point, this article offers a new approach to the problem of path interference in neural networks. By combining Hebbian and reinforcement learning, the authors show that the learning performance can be improved. However, the proposed methodology was only tested on a toy model of a neural network, and it is unclear how well it will perform on more complex models. In terms of performance, the proposed method achieved a considerably reduced number of learning steps compared to past methods. However, as mentioned earlier, the generalizability of the results are yet to be tested. In terms of workload, the article presents a clear methodology and experimental setup, which allows other researchers to replicate the results more easily. Overall, this article provides an interesting contribution to the field of reinforcement learning and neural networks.\n\n\n"
}