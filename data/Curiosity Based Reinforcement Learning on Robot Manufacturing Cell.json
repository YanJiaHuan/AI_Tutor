{
    "Abstract": "Abstract\u2014This paper introduces a novel combination of scheduling control on a \ufb02exible robot manufacturing cell with curiosity based reinforcement learning. Reinforcement learning has proved to be highly successful in solving tasks like robotics and scheduling. But this requires hand tuning of rewards in problem domains like robotics and scheduling even where the solution is not obvious. To this end, we apply a curiosity based reinforcement learning, using intrinsic motivation as a form of reward, on a \ufb02exible robot manufacturing cell to alleviate this problem. Further, the learning agents are embedded into the transportation robots to enable a generalized learning solution that can be applied to a variety of environments. In the \ufb01rst approach, the curiosity based reinforcement learning is applied to a simple structured robot manufacturing cell. And in the second approach, the same algorithm is applied to a graph structured robot manufacturing cell. Results from the experiments show that the agents are able to solve both the environments with the ability to transfer the curiosity module directly from one environment to another. We conclude that curiosity based learning on scheduling tasks provide a viable alternative to the reward shaped reinforcement learning traditionally used. Index Terms\u2014reinforcement learning, manufacturing cell, curiosity based learning, planning robots I. ",
    "Introduction": "INTRODUCTION Reinforcement Learning (RL) is becoming a popular algorithm to solve complex games and tasks like Atari [1], physics simulation [2], Go [3] and Chess [4], Robotics [5], and optimization problems [6] etc. In these approaches, the RL agent is given a continuous supply of dense extrinsic reward by the environment for the action it takes. While providing a reward or de\ufb01ning a reward function is not a problem in common gaming tasks, it quickly becomes cumbersome in complex engineering tasks like the optimization problem [7]. And traditionally when the rewards are not directly available, they need to be shaped to guide the agent in the direction of an optimal solution. This approach creates three problems: one, this introduces a huge inductive bias in the reward scheme that it cripples the generalized learning of the RL agent from the start. So much so that it might be only able to solve in the known solution space; two, reward shaping is a notoriously dif\ufb01cult engineering problem, where un-optimized rewards could break the learning process or provide only sub-optimal solutions; and \ufb01nally, it creates solutions which cannot be transferred to new environments, since the rewards have been heavily tuned for a particular environment. In this paper, we propose to apply curiosity based learning to a \ufb02exible Robot Manufacturing Cell. In [8], a Robot Manufacturing Cell (RMC) was proposed as an optimization problem similar to the scheduling problems encountered in the production planning. And in [9], the same RMC was solved using central and distributed advantage actor critic (A2C), through hand engineered reward signals. Similar reward shaping methods are also followed in [10] [11] [12], where rewards for each of the scheduling problem is hand engineered to suit the environment they work in. This obviously creates a problem of non-generalized solutions, where the learning of an agent from one environment is not transferable to an another, even though they both solve similar optimization problems. To overcome this limitation of the reinforcement learning application in engineering problems, in this paper we apply curiosity [13] to the RMC environment. Curiosity uses the intrinsic motivation of the agent to create rewards based on the surprise factor of the agents familiarity with the environment states. Through the application of curiosity in reinforcement learning of the RMC we overcome all the problems stated above. The main contributions of this paper as follows: \u2022 We propose curiosity as an effective tool to eliminate the inductive bias induced by the reward shaping normally used in reinforcement learning of scheduling tasks. Further, curiosity eliminates complex reward shaping for engineering tasks like scheduling. \u2022 We show that a direct transfer of the reward function from one manufacturing environment to another is possible without any tuning requirements. \u2022 We combine curiosity with gradient monitoring, an effective approach to stabilize training. We apply the curiositybased RL to two multi-robot environments where we particularly show the improvements achieved by the proposed approach in both environments. The paper is organized as follows. Section II introduces the learning problem, RMC, in detail, describing the operations and setup. Section III introduces the RL algorithms used in this paper along with further optimizations that were used, while Section IV, Section IV-A, and Section IV-B explain arXiv:2011.08743v1  [cs.RO]  17 Nov 2020 the curiosity module along with other optimizations used. Section V describes the results that were achieved on the RMC through the application of the RL along with its optimizations, while Section VI provides the conclusion with future scope of work. II. ROBOT MANUFACTURING CELL In this paper, RL model is applied to a cooperative selflearning RMC [8]. The RMC consists of two industrial robots, an Adept Cobra i600 SCARA-robot and an ABB IRB1400 6DOF-robot, both of which share a common work platform as shown in Figure 1. The setup has six stations totally, two input stations (IB1, IB2), three processing or work stations (M1, M2, M3) and one output station (OB). There are two different types of work-piece in this system (WP-1, WP-2). The WP-1 goes through IB1, M1, M2, M3, and then OB, while WP-2 goes through IB2, M2, M1, M3 and then OB. Both the robots have their own proprietary control software. Hence a supervisory Siemens Programmable Logic Control (PLC) system is developed to control and coordinate the robot movement. Two experiments are conducted to demonstrate the applicability of the curiosity based approach on scheduling problems. The \ufb01rst experiment is conducted on a simple structured Robot Manufacturing Cell (sRMC) environment, similar to [9] and the second experiment is conducted on a graph-network based Robot Manufacturing Cell (gRMC) environment, which was developed in [14]. gRMC is a scalable variant of the sRMC environment, where the machines and the buffers are represented by the nodes of a graph while the connections between them are expressed as edges. A workpiece is transported between the nodes when the edge is activated. Figure 1. Schematic drawing of the robot manufacturing cell In sRMC no buffers are considered for the processing stations environment, i.e., there is no queue in front or after the processing stations. The RL agent\u2019s actions here are the movements of the work-pieces between the processing stations. While in gRMC three buffer-stations are considered, one after each of the processing stations (MB1, MB2, MB3) as shown in Figure 2. Here the agent\u2019s action is the activation of the edges connecting the nodes. If a node is activated then the work-piece is moved from the \u2019from\u2019-node to the \u2019to\u2019-node. Two different types of edges are de\ufb01ned in the system, to move the different work-pieces. Both the work-pieces are embedded with the RFID chip which gives the required information to the robots. Figure 2. In the gRMC environment, the nodes are indexed from 0-8 and are colored according to their function, while the different colored lines represent the two work-pieces to be processed and transported. III. REINFORCEMENT LEARNING RL is a \ufb01eld of machine learning where an agent is trained to take a sequence of decisions on an environment, modeled as a Markov Decision Process (MDP), to maximize the cumulative reward signal. The RL system has two major components namely, the agent and the environment. The agent interacts with the environment, takes an action at based on the agent\u2019s policy \u03c0(at|st) from its current state st and moves to a new state st+1. Based on the action taken by the agent, the environment sends a reward signal rt to the agent. The goal of the agent is to \ufb01nd an optimal policy that maximizes the cumulative reward [15]. Hence, the agent tries to maximize the expected return E[Rt|\u03c0] with the discounted reward given by R = \ufffd t \u03b3trt and discount factor 0 \u2264 \u03b3 \u2264 1. The MDP is de\ufb01ned by the tuple (S, A, P, R, p0), with the set of states S, the \ufb01nite set of actions A, a transition model P for a given state s and an action a, the reward function R : S \u00d7 A \u00d7 S \u2192 R, which provides a reward r for each state transition st \u2192 st+1 with an initialization probability p0. The policy is de\ufb01ned using either value based methods or policy gradient methods. Value based methods use either a state-value function, v(s) = E[Rt|St = s] or a stateaction-value function, q(s, a) = E[Rt|St = s, At = a]. The policy is then de\ufb01ned by an \u03f5\u2212greedy strategy where greedy actions are given by \u03c0(a|s) = argmax(q(s, A)), where the agent exploits, or a non-greedy action is given, where the agent explores. In policy gradient methods, a parameterized policy, \u03c0(a|s, \u03b8), is used to take action. The policy can still be improved using value functions as seen in [1]. The the agent\u2019s objective is to derive an optimal policy, \u03c0\u2217(a|s), that maximizes the reward collection. Correspondingly, the policy is updated in a way to maximize a cost function J(\u03b8t) given by, \u03b8t+1 = \u03b8t + \u03c1\u2207J(\u03b8t), (1) where \u03b8 is the policy parameters of \u03c0 and \u03c1 is the learning rate. A. Advantage Actor Critic (A2C): Advantage Actor Critic is a policy gradient algorithm which combines policy gradient method with value based approach like SARSA [15]. In A2C, the actor tries to \ufb01nd the optimal policy and the critic evaluates the actor\u2019s policy. Here, the actor refers to the policy \u03c0(a|s, \u03b81) and the critic refers to the value function \u03bd(s, \u03b82), where \u03b81 is the parameter of the actor and the \u03b82 is the parameter of the critic. The cost function equation of the A2C is given by Eqn. 2 and 3 respectively. J(\u03b8) = E\u223c\u03c0\u03b8 \ufffd \ufffd (st,at)\u03f5 log\u03c0\u03b8(at, st)A\u03c0\u03b8 \ufffd (2) A\u03c0\u03b8(st, at) = Q\u03c0\u03b8(st, at) \u2212 V\u03c0\u03b8(st) (3) IV. CURIOSITY DRIVEN LEARNING Children often learn through intrinsic motivation such as curiosity. Psychologists de\ufb01ne a behavior as intrinsically motivated when a human works for his own sake. Similarly, intrinsic motivation in RL keeps an agent engaged in the exploration of new knowledge and skills when the rewards are rare [16]. There are lots of approaches to develop the intrinsic reward of an agent like prediction error, prediction uncertainty, or development of the forward model trained along with the policy of an agent [17]. These approaches drive the agent to explore the environment in the absence of a dense extrinsic reward. In this paper, we use prediction error as the intrinsic reward, which is produced by the agent itself with a model called Intrinsic Curiosity Model (ICM). ICM is composed of two subsystems: an inverse dynamic model that produce an intrinsic reward signal and forward dynamic model that outputs a sequence of actions which guides the agent to maximize the reward signal [13]. Figure 3 represents the schematic architecture of the ICM. The ICM encodes the current state st and the next state st+1 into the feature of current state \u03c6(st) and feature of next state \u03c6(st + 1). The feature of current state \u03c6(st) and the feature of next state \u03c6(st + 1) are trained with inverse dynamic model that predicts action \u02c6at [13]. Mathematically, this transition can be represented as, \u02c6at = g(st, st+1; \u03b8I) (4) where g is a soft-max distribution function of all possible actions that could be taken and \u03b8I is the neural network parameter. The loss function of the inverse dynamic model can represent by the following, min \u03b8I = LI( \u02c6at, at) (5) where LI the loss function that calculates the deviation between the actual action at and the predicted action \u02c6at. The forward dynamic model takes \u03c6(st) and action at as the inputs and outputs the predicted feature representation \u02c6\u03c6(at+1) of next state st+1. The forward dynamic model can be represented by the following, \u02c6\u03c6(st+1) = f(\u03c6(st), at; \u03b8F ) (6) where f is the learned function and \u03b8F is the ICM neural network parameter. The loss function of the forward model is given below: LF (\u03c6(st), \u02c6\u03c6(st+1) = 1 2||\u03c6(st+1) \u2212 \u03c6(st)||2 2 (7) where LF is the loss function that calculates the variance between the predicted feature representation of next state \u02c6\u03c6(at+1) and the actual predicted feature of next state \u03c6(st+1). So, the intrinsic reward ri t can be calculated as: ri t = \u03b7 2||\u03c6(st+1) \u2212 \u03c6(st)||2 2 (8) where \u03b7 > 0 is the scaling factor of the intrinsic reward signal. The intrinsic reward is generated by the agent at time t. Besides intrinsic rewards, the agent may also get extrinsic reward re t from the environment. To generate the intrinsic reward signal and to optimize the ICM, the inverse dynamic model and forward dynamic model loss should be combined. So, the overall optimization problem of ICM that helps the agent to learn can be represented as: min \u03b8P ,\u03b8I,\u03b8F \ufffd \u2212\u03bbE\u03c0(st;\u03b8P ) \ufffd \ufffd t rt \ufffd + (1 \u2212 \u03b2)LI + \u03b2LF \ufffd (9) where 0\u2264\u03b2\u22641 is a scaling factor that weights the inverse model against the forward model, and \u03bb > 0 is a scaling factor of policy gradient loss against the intrinsic reward [13]. In this paper, to speed up the learning process of the RL agents two optimization techniques called Gradient Monitoring and Curriculum Learning are used. They are explained further below. A. Gradient Monitoring RL has a non stationary learning problem. This makes ensuring stability in learning a critical part of RL. In this paper an optimization method called Gradient Monitoring (GM) is used to stabilize the RL training of the RMC environments. GM provides for a faster convergence and better generalization performance of the trained RL agent, for a small computational overhead, by steering the gradients during the learning process towards the most important weights. The added bene\ufb01t of using GM is that it allows for a continuous adjustment of the neural network\u2019s utilization capacity, i.e., the size of the neural network need not be tuned with as much care since GM automatically derives the required size of the neural network required for the training process [18]. ",
    "Experimental Results": "EXPERIMENTAL ",
    "Results": "RESULTS In this paper, two experiments are conducted. First sRMC is solved using ICM and CL, since the \ufb01nal task of planning of 20 WP-1 and 20 WP-2 was too dif\ufb01cult for the agent. Then in the same experiment, GM is introduced which helps the agent to converge without the use of CL. Next in the second experiment, the ICM used in sRMC is used in the gRMC environment. Here, only CL is used since the overhead GM did not provide a corresponding gain. The results thence obtained are discussed below in two parts based on their environment, sRMC and gRMC. The parts output in each of the case is reported, along with the rewards collected by in setup. A. sRMC: In this section, \ufb01rst the results of curriculum learning without gradient monitoring approach in sRMC environment is discussed. The result in parts output, WP-1 and WP-2, with respect to iterations is shown in Figure 4. Here, the target is 20 work-pieces for each agent. Initially, without the additional optimizations the agents were able to solve only seven workpieces each as opposed to the target of 20 each. Hence, CL was introduced to divide the \ufb01nal target into smaller targets of increasing dif\ufb01culty, i.e. target of 5, 10, 15, and 20 workpieces of WP-1 and WP-2. Both the agents are trained for 1e5 Figure 4. Parts Output (sRMC without gradient monitoring) episodes and the convergence for 20 target is achieved after 0.78e5 episodes. To increase the convergence speed of the agents, GM was introduced into the agent learning the sRMC environment. The improvement posed by GM was good enough to remove the CL completely from the training regime of sRMC environment. This quickened the learning progress as shown in Figure 4. A target of 20 work-pieces for each agent was set directly rather than dividing the targets into smaller tasks. The agents solved the target directly in 5e3 steps compared to the 0.78e5 without GM. With this, the objective of training the agents on the sRMC environment was completed. The results graph shows an interesting trend. Figure 6 shows the combined, intrinsic and extrinsic, rewards collected by the agents. The rewards graph can be used to describe the agent\u2019s learning behavior. Normally, in extrinsic reward-based RL algorithm, the slope of the reward curves becomes zero after reaching a solution. But the intrinsic reward-based system works differently. When the agent remains curious about completing the target, it generates the intrinsic reward by itself. ",
    "Conclusion": "CONCLUSION RL on industrial scheduling problems are dif\ufb01cult to train because of the inherent bias related to hand tuned rewards. A novel combination of the \ufb02exible RMC and a curiosity based RL has been proposed in this study. We found that the application of the curiosity based RL provides good solutions even with very scarce external reward. Additionally, we found that the reward function architecture used in one environment could be easily transferred to another, which was is impossible in hand tuned rewards. In future work, we plan to improve the performance of the agent by using a hybrid reward function providing rewards with little inductive bias for a quicker ",
    "References": "REFERENCES [1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [2] E. Todorov, T. Erez, and Y. Tassa, \u201cMujoco: A physics engine for model-based control,\u201d in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 5026\u20135033. [3] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan, and D. Hassabis, \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, pp. 1140\u20131144, 2018. [4] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al., \u201cMastering atari, go, chess and shogi by planning with a learned model,\u201d arXiv preprint arXiv:1911.08265, 2019. [5] S. Gu, E. Holly, T. Lillicrap, and S. Levine, \u201cDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates,\u201d in 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3389\u20133396. [6] B. Cunha, A. M. Madureira, B. Fonseca, and D. Coelho, \u201cDeep reinforcement learning as a job shop scheduling solver: A literature review,\u201d in ser. Advances in intelligent systems and computing, 2194-5357, vol. 923, Springer, 2020, pp. 350\u2013359. [7] K. Xia, C. Sacco, M. Kirkpatrick, C. Saidy, L. Nguyen, A. Kircaliali, and R. Harik, \u201cA digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence,\u201d Journal of Manufacturing Systems, 2020. [8] D. Schwung, F. Csaplar, A. Schwung, and S. X. Ding, \u201cAn application of reinforcement learning algorithms to industrial multi-robot stations for cooperative handling operation,\u201d in 2017 IEEE 15th International Conference on Industrial Informatics (INDIN), 2017, pp. 194\u2013199. [9] A. Schwung, D. Schwung, and M. S. Abdul Hameed, \u201cCooperative robot control in \ufb02exible manufacturing cells: Centralized vs. distributed approaches,\u201d in 2019 IEEE 17th International Conference on Industrial Informatics (INDIN), 2019, pp. 233\u2013238. [10] I.-B. Park, J. Huh, J. Kim, and J. Park, \u201cA reinforcement learning approach to robust scheduling of semiconductor manufacturing facilities: Ieee transactions on automation science and engineering, 1-12,\u201d IEEE Transactions on Automation Science and Engineering, pp. 1\u201312, 2020. [11] T. Gabel and M. Riedmiller, \u201cAdaptive reactive jobshop scheduling with reinforcement learning agents,\u201d International Journal of Information Technology and Intelligent Computing, vol. 24, no. 4, 2008. [12] B. Waschneck, A. Reichstaller, L. Belzner, T. Altenm\u00a8uller, T. Bauernhansl, A. Knapp, and A. Kyek, \u201cOptimization of global production scheduling with deep reinforcement learning,\u201d Procedia CIRP, vol. 72, no. 1, pp. 1264\u20131269, 2018. [13] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, \u201cCuriosity-driven exploration by self-supervised prediction,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017. [14] M. S. A. Hameed and A. Schwung, Reinforcement learning on job shop scheduling problems using graph networks, 2020. arXiv: 2009.03836. [15] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [16] N. Chentanez, A. G. Barto, and S. P. Singh, \u201cIntrinsically motivated reinforcement learning,\u201d in Advances in neural information processing systems, 2005, pp. 1281\u2013 1288. [17] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros, \u201cLarge-scale study of curiosity-driven learning,\u201d in International Conference on Learning Representations, 2018. [18] M. S. A. Hameed, G. S. Chadha, A. Schwung, and S. X. Ding, Gradient monitored reinforcement learning, 2020. arXiv: 2005.12108. [19] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum learning,\u201d in Proceedings of the 26th annual international conference on machine learning, 2009, pp. 41\u201348. ",
    "title": "Curiosity Based Reinforcement Learning on Robot",
    "paper_info": "Curiosity Based Reinforcement Learning on Robot\nManufacturing Cell\nMohammed Sharafath Abdul Hameed, Md Muzahid Khan, Andreas Schwung\nDepartment of Automation Technology\nSouth Westphalia University of Applied Sciences\nSoest, Germany.\nsharafath.mohammed, khan.mdmuzahid, schwung.andreas@fh-swf.de\nAbstract\u2014This paper introduces a novel combination of\nscheduling control on a \ufb02exible robot manufacturing cell with\ncuriosity based reinforcement learning. Reinforcement learning\nhas proved to be highly successful in solving tasks like robotics\nand scheduling. But this requires hand tuning of rewards in\nproblem domains like robotics and scheduling even where the\nsolution is not obvious. To this end, we apply a curiosity based\nreinforcement learning, using intrinsic motivation as a form of\nreward, on a \ufb02exible robot manufacturing cell to alleviate this\nproblem. Further, the learning agents are embedded into the\ntransportation robots to enable a generalized learning solution\nthat can be applied to a variety of environments. In the \ufb01rst\napproach, the curiosity based reinforcement learning is applied to\na simple structured robot manufacturing cell. And in the second\napproach, the same algorithm is applied to a graph structured\nrobot manufacturing cell. Results from the experiments show\nthat the agents are able to solve both the environments with the\nability to transfer the curiosity module directly from one envi-\nronment to another. We conclude that curiosity based learning\non scheduling tasks provide a viable alternative to the reward\nshaped reinforcement learning traditionally used.\nIndex Terms\u2014reinforcement learning, manufacturing cell, cu-\nriosity based learning, planning robots\nI. INTRODUCTION\nReinforcement Learning (RL) is becoming a popular al-\ngorithm to solve complex games and tasks like Atari [1],\nphysics simulation [2], Go [3] and Chess [4], Robotics [5],\nand optimization problems [6] etc. In these approaches, the RL\nagent is given a continuous supply of dense extrinsic reward\nby the environment for the action it takes. While providing\na reward or de\ufb01ning a reward function is not a problem in\ncommon gaming tasks, it quickly becomes cumbersome in\ncomplex engineering tasks like the optimization problem [7].\nAnd traditionally when the rewards are not directly available,\nthey need to be shaped to guide the agent in the direction of an\noptimal solution. This approach creates three problems: one,\nthis introduces a huge inductive bias in the reward scheme that\nit cripples the generalized learning of the RL agent from the\nstart. So much so that it might be only able to solve in the\nknown solution space; two, reward shaping is a notoriously\ndif\ufb01cult engineering problem, where un-optimized rewards\ncould break the learning process or provide only sub-optimal\nsolutions; and \ufb01nally, it creates solutions which cannot be\ntransferred to new environments, since the rewards have been\nheavily tuned for a particular environment. In this paper, we\npropose to apply curiosity based learning to a \ufb02exible Robot\nManufacturing Cell.\nIn [8], a Robot Manufacturing Cell (RMC) was proposed\nas an optimization problem similar to the scheduling problems\nencountered in the production planning. And in [9], the same\nRMC was solved using central and distributed advantage actor\ncritic (A2C), through hand engineered reward signals. Similar\nreward shaping methods are also followed in [10] [11] [12],\nwhere rewards for each of the scheduling problem is hand en-\ngineered to suit the environment they work in. This obviously\ncreates a problem of non-generalized solutions, where the\nlearning of an agent from one environment is not transferable\nto an another, even though they both solve similar optimization\nproblems.\nTo overcome this limitation of the reinforcement learning\napplication in engineering problems, in this paper we apply\ncuriosity [13] to the RMC environment. Curiosity uses the\nintrinsic motivation of the agent to create rewards based on the\nsurprise factor of the agents familiarity with the environment\nstates. Through the application of curiosity in reinforcement\nlearning of the RMC we overcome all the problems stated\nabove. The main contributions of this paper as follows:\n\u2022 We propose curiosity as an effective tool to eliminate\nthe inductive bias induced by the reward shaping nor-\nmally used in reinforcement learning of scheduling tasks.\nFurther, curiosity eliminates complex reward shaping for\nengineering tasks like scheduling.\n\u2022 We show that a direct transfer of the reward function from\none manufacturing environment to another is possible\nwithout any tuning requirements.\n\u2022 We combine curiosity with gradient monitoring, an effec-\ntive approach to stabilize training. We apply the curiosity-\nbased RL to two multi-robot environments where we\nparticularly show the improvements achieved by the\nproposed approach in both environments.\nThe paper is organized as follows. Section II introduces the\nlearning problem, RMC, in detail, describing the operations\nand setup. Section III introduces the RL algorithms used in\nthis paper along with further optimizations that were used,\nwhile Section IV, Section IV-A, and Section IV-B explain\narXiv:2011.08743v1  [cs.RO]  17 Nov 2020\n",
    "GPTsummary": "                    - (1): The research background of this article is the application of reinforcement learning to solve complex engineering tasks like the optimization problem.\n\n                    - (2): Traditional methods of reinforcement learning requires hand tuning of rewards, which can create a problem of non-generalized solutions, where the learning of an agent from one environment is not transferable to another, even though they both solve similar optimization problems. This paper proposes to apply curiosity-based learning to a flexible robot manufacturing cell to address these problems.\n\n                    - (3): The research methodology proposed in this paper combines scheduling control of a flexible manufacturing cell with curiosity-based reinforcement learning. The learning agents are embedded into the transportation robots to enable a generalized learning solution that can be applied to a variety of environments. The curiosity-based reinforcement learning is applied to a simple structured robot manufacturing cell and a graph-structured robot manufacturing cell.\n\n                    - (4): The methods in this paper achieve good performance in solving scheduling tasks. Results from experiments show that the agents are able to solve both environments with the ability to transfer the curiosity module directly from one environment to another. The paper concludes that curiosity-based learning on scheduling tasks provides a viable alternative to the reward-shaped reinforcement learning traditionally used.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work is to propose a curiosity-based reinforcement learning approach to solve complex scheduling problems in a flexible robot manufacturing cell. This approach enables generalized learning solutions that can be transferred to different environments.\n\n- (2): Innovation point: The use of curiosity-based reinforcement learning in scheduling tasks for flexible robot manufacturing cells is a novel and innovative approach. Performance: The proposed approach achieved good performance in solving scheduling tasks with the ability to transfer the learned model from one environment to another. Workload: The workload required to implement the proposed approach was not explicitly discussed in this article.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to propose a curiosity-based reinforcement learning approach to solve complex scheduling problems in a flexible robot manufacturing cell. This approach enables generalized learning solutions that can be transferred to different environments.\n\n- (2): Innovation point: The use of curiosity-based reinforcement learning in scheduling tasks for flexible robot manufacturing cells is a novel and innovative approach. Performance: The proposed approach achieved good performance in solving scheduling tasks with the ability to transfer the learned model from one environment to another. Workload: The workload required to implement the proposed approach was not explicitly discussed in this article.\n\n\n"
}