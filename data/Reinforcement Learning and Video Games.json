{
    "Abstract": "Abstract Reinforcement learning has exceeded human-level performance in game playing AI with deep learning methods according to the experiments from DeepMind on Go and Atari games. Deep learning solves high dimension input problems which stop the development of reinforcement for many years. This study uses both two techniques to create several agents with di\ufb00erent algorithms that successfully learn to play T-rex Runner. Deep Q network algorithm and three types of improvements are implemented to train the agent. The results from some of them are far from satisfactory but others are better than human experts. Batch normalization is a method to solve internal covariate shift problems in deep neural network. The positive in\ufb02uence of this on reinforcement learning has also been proved in this study. ii Acknowledgement I would like to express many thanks to my supervisor, Prof. Dr. Eleni Vasilaki for assigning me this project and her guidance across this study. I would also like to acknowledge my dear friends for helping me to solve di\ufb00erent problems and giving me inspiration. This include Mwiza L Kunda, Wei Wei, Yuliang Li, Ziling Li, Zixuan Zhang. Finally, I would like to acknowledge my family as well as my girl friend Fangni Liu for their encouragement during my project. iii Contents 1 Introduction 1 1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Aim of the project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.3 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 Literature Survey 3 2.1 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.1.1 History of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.1.2 Deep Neural Network and Activation Function . . . . . . . . . . . . . 4 2.1.3 Backpropagation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.4 Convolutional Neural Network . . . . . . . . . . . . . . . . . . . . . . 7 2.1.5 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.1 History of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . 10 2.2.2 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.3 Bellman Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.4 Exploitation vs Exploration . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2.5 Temporal Di\ufb00erence Learning . . . . . . . . . . . . . . . . . . . . . . . 17 2.2.6 Deep Q Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3 Methodology 23 3.1 Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1.1 Software Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1.2 Hardware Requirement . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2 Game Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.3 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.4 Image Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.5 Convolutional Neural Network Architecture . . . . . . . . . . . . . . . . . . . 27 3.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.6.1 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.6.2 Comparison of di\ufb00erent Deep Q Network Algorithms . . . . . . . . . . 29 3.6.3 E\ufb00ect of Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . 29 3.7 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4 Results and Discussion 31 iv CONTENTS v 4.1 Hyper Parameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.1.1 Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.1.2 Batch Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.1.3 Epsilon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.1.4 Explore Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.1.5 Gamma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.2 Training Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.2.1 DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.2.2 Double DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.2.3 Dueling DQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.2.4 DQN with Prioritized Experience Replay . . . . . . . . . . . . . . . . 37 4.2.5 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.2.6 Further Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.3 Testing Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5 Conclusions and Future Works 42 List of Figures 2.1 Three type of activation functions. . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 A simple neural network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 A simple deep neural network with three hidden layers. . . . . . . . . . . . . 7 2.4 Operations in convolution layer. . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.5 Operations in pooling layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.6 A simple convolutional neural network. . . . . . . . . . . . . . . . . . . . . . . 9 2.7 Interaction between the agent and the environment. . . . . . . . . . . . . . . 11 2.8 Markov decision process in reinforcement learning. . . . . . . . . . . . . . . . 14 2.9 Example of cli\ufb00 walking from [43]. . . . . . . . . . . . . . . . . . . . . . . . . 19 2.10 Architecture comparison between Dueling DQN and DQN [52] . . . . . . . . 22 3.1 A screenshot of T-rex Runner. . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2 Three type of actions in T-rex Runner . . . . . . . . . . . . . . . . . . . . . . 25 3.3 Preprocessing steps for T-rex Runner. . . . . . . . . . . . . . . . . . . . . . . 27 3.4 Convolutional Neural Network architecture for Deep Q Network. . . . . . . . 28 3.5 Convolutional Neural Network architecture for Dueling Deep Q Network. . . 29 4.1 Hyper parameter tuning for learning rate. . . . . . . . . . . . . . . . . . . . . 32 4.2 Hyper parameter tuning for batch size. . . . . . . . . . . . . . . . . . . . . . . 32 4.3 Hyper parameter tuning for explore probability \u03f5. . . . . . . . . . . . . . . . . 33 4.4 Hyper parameter tuning for explore steps. . . . . . . . . . . . . . . . . . . . . 34 4.5 Hyper parameter tuning for discount factor \u03b3. . . . . . . . . . . . . . . . . . . 34 4.6 Training result for DQN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.7 Training result for Double DQN compared with DQN. . . . . . . . . . . . . . 36 4.8 Training result for Dueling DQN compared with DQN. . . . . . . . . . . . . . 36 4.9 Training result for DQN with prioritized experience replay compared with DQN. 37 4.10 Batch normalization on DQN, Double DQN, Dueling DQN and DQN with prioritized experience replay . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.11 Boxplot for test result with eight di\ufb00erent algorithms . . . . . . . . . . . . . . 40 vi List of Tables 2.1 Dimension description of the deep neural network . . . . . . . . . . . . . . . . 6 2.2 Property of convolutional layer and pooling layer . . . . . . . . . . . . . . . . 9 3.1 Software requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2 Hardware requirement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.1 Hyper parameters used in all experiments . . . . . . . . . . . . . . . . . . . . 31 4.2 Hyperparameters used in all experiments . . . . . . . . . . . . . . . . . . . . . 35 4.3 Step size di\ufb00erence between DQN and DQN with PER . . . . . . . . . . . . . 37 4.4 Training results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.5 Test results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 vii Chapter 1 Introduction 1.1 Background The applications of Arti\ufb01cial Intelligence are widely used in recent years. As one part of them, Reinforcement Learning has achieved incredible results in game playing. An intelligent agent will be created and trained with reinforcement learning algorithms to ful\ufb01ll this tasks. In the Future of Go Summit 2017, Alpha Go which is an AI player trained with deep reinforcement learning algorithms won three games against the world best human player in Go. The success of reinforcement learning in this area shock the world and many researches are launched such as driverless cars. Deep learning methods such as convolutional neural network contributes a lot to this because these techniques solves the problem of dealing with high dimension input data and feature extraction. T-rex Runner is a dinosaur game from Google Chrome o\ufb04ine mode. The aim of the player is to escape all obstacles and get higher score until reaching the limitation which is 99999. The moving speed of the obstacles will increase as time goes by which make it di\ufb03cult to get the highest score. The code of this project can be found in this link which is written in Python. 1.2 Aim of the project The aim of this project is to create an agent using di\ufb00erent algorithms to play T-rex Runner and compare the performance of them. Internal covariate shift is the change of distribution in each layer of during the training which may result in longer training time especially in deep neural network. To cope with this problem, batch normalization use linear transformation on each feature to normalize the data with the same mean and variance. The same problem may also occur in deep reinforcement learning because the decision is based on neural network. Beyond the comparison of di\ufb00erent reinforcement learning algorithms, this project will also investigate the e\ufb00ect of batch normalization. The overall objectives of this project are list below. 1 ",
    "Introduction": "INTRODUCTION 2 \u2022 Create an agent to play T-rex Runner \u2022 Compare the di\ufb00erence among di\ufb00erent reinforcement learning algorithms \u2022 Investigate the e\ufb00ect of batch normalization in reinforcement learning 1.3 Overview This study opens with a literature review on deep learning and reinforcement learning. Each section includes the history of the \ufb01eld and the techniques related to this study. Chapter 3 includes the description of the game and the choice of algorithms according the literature review. The entire processing step will be shown as well as the architecture of the model. The design of the experiments and the evaluation methods are presented in this chapter too. Chapter 4 shows the result of all the experiments and the discussion of each experiment. Chapter 5 presents the conclusion of this study and the proposed future works. Chapter 2 Literature Survey This chapter introduces the techniques used in developing an agent to play T-rex Runner. There are two main sections which are Deep Learning and Reinforcement Learning. Brief history and some milestones will be described and some important methods will be shown in detail. 2.1 Deep Learning Deep learning is a class of Machine Learning model based on Arti\ufb01cial Neural Network (ANN). There two kinds of deep learning model which is widely used in recent years. Recurrent Neural Network is one of them which shows its power in Natural Language Processing. The other one plays an important role in deep reinforcement learning called Convolutional Neural Network (CNN). It is one of the most e\ufb00ective models for computer vision problems such as object detection and image classi\ufb01cation. This section gives a brief introduction of deep learning and detailed information about convolutional neural network. 2.1.1 History of Deep Learning An arti\ufb01cial neural network is a computation system inspired by biological neural networks which were \ufb01rst proposed by McCulloch, a neurophysiologist [24]. In 1957, Perceptron was invented by Frank [31]. Three years later, his experiments show this algorithm can recognize some of alphabets [32]. However, Marvin proved that a single layer perceptron cannot deal with XOR problem [25]. This stopped the development of ANN until Rumelhart et al. show that some useful representations can be learned with multi-layer perceptron, which is also called neural network, and backpropagation algorithm [33] in 1988. One year later, LeCun et al. \ufb01rst used a \ufb01ve-layer neural network and backpropagation to solved digit classi\ufb01cation problem and achieved great results [21]. His innovative model is known as LeNet which is the beginning of the convolutional neural network. 3 CHAPTER 2. LITERATURE SURVEY 4 The origin of CNN was proposed by Fukushima named Neocognitron which was a selforganized neural network model with multiple layers [10]. This model achieved a good result in object detection tasks because it is not position-sensitive. As mentioned before, LeCun et al. invented LeNet and got less than 1% error rate in mnist handwritten digits dataset in 1998[21]. The model used convolutions and sub-sampling which is called convolution layer and pooling layer today to convert the original images into feature vectors and perform classi\ufb01cation with fully connected layers. At the same time, some neural network models show some acceptable results in face recognition [20], speech recognition [51] and object detection [49]. But the lack of reliable theory caused the research of CNN to stagnate for many years. In the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [9], Alex and his team got 16.4% error rate with an eight-layer deep neural network (AlexNet) [19]. This was a signi\ufb01cant result compared with the one from second rank participant which was 26.2%. Beyond LeNet, AlexNet used eight layers to train the classi\ufb01er with data augmentation, dropout, ReLU, which mitigated over\ufb01tting problem. Another signi\ufb01cant discovery was that parallel computing with multiple GPUs can largely decrease the training time. Two years later, Simonyan and Zisserman introduced a sixteen-layer neural network (VGGNet) and won the \ufb01rst prize in ILSVRC 2014 classi\ufb01cation and localization tasks [41]. This model got the state-of-the-art result with 7.3% error rate at that time. VGGNet also proved that using smaller \ufb01lter size and deeper network can improve the performance of CNN. The size of all \ufb01lters in the model was no greater than 3 \u00d7 3 while the \ufb01rst two layers in AlexNet were 11 \u00d7 11 and 5 \u00d7 5. In the same year, GoogLeNet [46], the best model of ILSVRC 2014 classi\ufb01cation and detection tasks, \ufb01rst used inception which was proposed by Lin [23] to solve vanishing gradient problem. Inception replaced one node with a network which was consisted of several convolutional layers and pooling layers then concatenate them before passing to the next layer. This change made the feature selection between two layers more \ufb02exible. In other words, it can be updated by the backpropagation algorithm. Another problem of the deep neural network was degradation resulting in high training error caused by optimization di\ufb03culty. To solve that problem, He et al. proposed a deep residual learning framework (ResNet) using a residual mapping instead of stacking layers directly [12]. This model won the championship in ILSVRC 2015 with only 3.57% error rate. His experiments show that this new framework can not only solve degradation problems but also can improve the computing e\ufb03ciency. ResNet There were many variants based on ResNet such as Inception-ResNet [45] and DenseNet [14]. The former one combined improved inception techniques into ResNet. Every two convolutional layers were connected in the later model. This change mitigated vanishing gradient problems and improved the propagation of features. 2.1.2 Deep Neural Network and Activation Function A neural network or multi-layer perceptron consists of three main components: the input layer, the hidden layer, and the output layer. Each unit in one layer called a neuron. The input data are fed into the input layer conducting linear transformation through weights in the hidden layer. Finally, the result will be given non-linear ability through activation CHAPTER 2. LITERATURE SURVEY 5 function and fed into the output layer. Activation function enables the network to learn more complicated relationships between inputs and outputs. There are three widely used activation functions shown in Figure 2.1: sigmoid, tanh and ReLU. ReLU is the most commonly used one in three because it has a low computational requirement and better performance in solving vanishing gradient problems compared with the other two. tanh(x) = ex \u2212 e\u2212x ex + e\u2212x (2.1) sigmoid(x) = 1 1 + e\u2212x (2.2) ReLU(x) = max(x, 0) (2.3) Figure 2.1: Three type of activation functions. To illustrate the entire process in neural network, here is an example in Figure 2.2. Given an input data T = {(x1, y1), (x2, y2), (x3, y3), (x4, y4) | xi \u2208 Rm} and a randomly generated weight [w1, w2, w3, w4]T in the hidden layer, the output of the neural network is \u02c6y and the activation of the hidden layer is f. Therefore, the estimated value \u02c6yi can be calculated by \u02c6yi = f(wixi + b) (2.4) where b is the bias of the hidden layer and m is the number of features. If the number of hidden layers in the neural network is greater than two, this is also called Deep Neural Network (DNN). Consider a simple DNN with three hidden layers shown in Figure 2.3. Given the same input X = [x1, x2, x3, x4]T in matrix form and W i is the weight between (i\u22121)-th and i-th layer, the output of i-th layer ai can be calculated by ai = fi(zi) (2.5) where fi is the activation function between (i \u2212 1)-th and i-th layer and zi = ai\u22121W i, especially a0 = X. The dimension of those variables are shown in Table 2.1 CHAPTER 2. LITERATURE SURVEY 6 Figure 2.2: A simple neural network. Parameter Description Dimension X Input data 4 \u00d7 m W 1 Weight between input layer and hidden layer 1 m \u00d7 8 W 2 Weight between hidden layer 1 and hidden layer 2 8 \u00d7 6 W 3 Weight between hidden layer 2 and hidden layer 3 6 \u00d7 8 W 4 Weight between hidden layer 3 and output layer 8 \u00d7 1 Table 2.1: Dimension description of the deep neural network 2.1.3 Backpropagation Algorithm Section 2.1.2 introduces the way to estimated label using deep neural network. In order to optimize this estimation, a cost function J is used to quantify the di\ufb00erence between the estimated value \u02c6y and the true value y. To give a simple example, Mean Square Error [1] which is often applied to regression problems is used in this section to illustrate how the backpropagation algorithm works. Equation 2.6 shows the form of mean square error. J(w, x) = 1 2 n \ufffd i=1 (yi \u2212 \u02c6yi)2 (2.6) where x is the input data and w represent all weights used in the model. Thus, the optimization problem can be described as following min w J(w, x) (2.7) Stochastic Gradient Descent (SGD) is an e\ufb00ective way to solve this optimization problem CHAPTER 2. LITERATURE SURVEY 7 Figure 2.3: A simple deep neural network with three hidden layers. if J(w, x) is convex. However, it still shows acceptable results in deep neural network even though there is no guarantee for global optimal point in non-convex optimization [11]. Instead of \ufb01nding the optimal point directly, SGD optimize the objective function 2.7 iteratively by following equation w \u2190 w \u2212 \u03b7\u2202J(w, x) \u2202w (2.8) where \u03b7 is the learning rate which can control the update speed of the weight. Using gradient methods such as SGD to optimize the cost function in neural network is called backpropagation algorithm. Considering the deep neural network shown in 2.3 and the techniques of matrix calculus [13], the gradient of J with respect to W 4 is \u2202J \u2202W 4 = aT 3 ((a4 \u2212 y) \u2299 \u2207f4(z4)) (2.9) where \u2299 is element-wise matrix multiplication and \u2207f4(z4) is the gradient with respect to z4. Results for other W i can be calculated in a similar way. With equation 2.8, W i can be updated during each iteration by W i \u2190 W i \u2212 \u03b7 \u2202J \u2202W i (2.10) 2.1.4 Convolutional Neural Network Compared with a common deep neural network, a convolutional neural network has two extra components which are convolutional layer and pooling layer. The convolutional layers CHAPTER 2. LITERATURE SURVEY 8 make use of several trainable \ufb01lters to select di\ufb00erent features. The pooling layer reduces the dimension of the data by subsampling. In the convolution layer, the output of the last layer is convolved by trainable \ufb01lters with element-wise matrix multiplication. The size and the number of each \ufb01lter are de\ufb01ned by the user and the initial value is randomly generated. The moving step of a \ufb01lter in each convolution layer is decided by stride. In order to keep the information of the border during the forward propagation, a series of zeros attached to the border of the image called padding. Figure 2.4 shows how the result of one neuron in a convolution layer comes from and how the \ufb01lter (x, y, z, w) will be updated in every iteration in backpropagation. Figure 2.4: Operations in convolution layer. The reason for using a pooling layer is not only for dimension reduction but also for detecting invariant features including translation, rotation, scale from the input [37]. There are two types of operations in pooling layer: max pooling and average pooling. In max pooling, only the maximum value in user-de\ufb01ned windows will be chosen while all values in the window will make contributions to the output in average pooling. The choice of operation is dependent on tasks and Boureau has made a theoretical comparison between those two [6]. Both max and average operation are shown in Figure 2.5. Figure 2.5: Operations in pooling layer. A complete convolutional neural network consists of several convolutional layers, pooling CHAPTER 2. LITERATURE SURVEY 9 layers, and fully connected layers. The fully connected layer is the same concept of DNN which used the \ufb02atten vector of the last output of the other two layers as input. Considering a classi\ufb01cation problem as shown in Figure 2.6, an image with a size of 64 \u00d7 64 is fed into CNN and output a scalar which represents its class. Table 2.2 lists the \ufb01lters information used in CNN. Figure 2.6: A simple convolutional neural network. Layer Numbers Size Stride Padding Output dimension Convolution 1 8 8\u00d78 2 0 8\u00d728\u00d728 Max Pooling 8 2\u00d72 2 / 8\u00d714\u00d714 Convolution 2 16 6\u00d76 2 0 16\u00d74\u00d74 Table 2.2: Property of convolutional layer and pooling layer The backpropagation algorithm in convolutional neural network is a little di\ufb00erent from described in section 2.1.3 because of two extra layer type. In average pooling layer, the error will be divided by t \u00d7 t which is the size of the \ufb01lter and propagate to the last layer. In max pooling layer, the position of the maximum value will be stored when forward propagating and the error will be directly passed through that position. In convolutional layer, backpropagation can be calculated through basic di\ufb00erentiation. Consider the convolution operation in Figure 2.4, if the error from the output layer is \u03b4, then we have \u2202\u03b4 \u2202x = \u2202\u03b4 \u2202O11 \u2202O11 \u2202x + \u2202\u03b4 \u2202O12 \u2202O12 \u2202x + \u2202\u03b4 \u2202O21 \u2202O21 \u2202x + \u2202\u03b4 \u2202O22 \u2202O22 \u2202x (2.11) where \ufffd O11 O12 O21 O22 \ufffd (2.12) is the output matrix in Figure 2.4. The di\ufb00erentiation of \u03b4 with respect to y, z, w can be computed in a similar way. CHAPTER 2. LITERATURE SURVEY 10 2.1.5 Batch Normalization With the increasing depth of the neural network, the training time becomes longer. One of the reason is the distribution of input in each layer changes when updating the weight which is called Internal Covariate Shift. In 2015, Io\ufb00e proposed Batch Normalization (BN) which make the distribution in each layer more stable and achieve shorter training time [15]. In each neuron, the input can be normalized by Equation 2.13 \ufffdx = x \u2212 E[x] \ufffd Var[x] + \u03f5 (2.13) where \u03f5 is used to avoid zero variance. Now data in each neuron follow the distribution with mean 0 and standard deviation 1. However, this changes the representation ability of the network which may lead to the loss of information in the earlier layer. Therefore, Io\ufb00e used another linear transformation to restore that representation \u02dcx = m\ufffdx + n (2.14) where m and n are learnable parameters, especially, the result is the same as original when m = \ufffd Var[x] and n = E[x]. The mean and variance during training will be stored and will be treated as the mean of the variance of test data. In their experiment, BN can not only deal with Internal Covariate Shift problems but also mitigate vanishing gradient problems. 2.2 Reinforcement Learning Reinforcement Learning (RL) is a class of machine learning aiming at maximum the reward signal when making decisions. The basic component of reinforcement learning is the agent and the environment. As shown in Figure 2.7, the agent will receive feedback including observation and reward from the environment after each action. To generate a better policy, it will keep interacting with the environment and improve its decision-making ability step by step until the policy converges. 2.2.1 History of Reinforcement Learning In recent years, reinforcement learning becomes popular because of Alpha Go, a program that can beat human expert in Go [40]. In the Future of Go Summit 2017, Alpha Go Master shocked the world by winning all three games against Ke Jie, the world best player in Go. But the research of reinforcement learning started very early. According to Sutton, the early history of RL can be divided into two main threads [43]. One of them was optimal control. To cope with arising optimal control problems which were called \u201dmulti-stage decision processed\u201d in 1954, the theory Dynamic Programming (DP) was introduced by Bellman [5]. In the theory, he proposed the concept of \u201dfunctional equation\u201d, CHAPTER 2. LITERATURE SURVEY 11 Figure 2.7: Interaction between the agent and the environment. which was often called the Bellman equation today. Although DP was one of the most e\ufb00ective approaches to solve optimal control problems at that time, the high computational requirements which is called \u201dthe curse of dimensionality\u201d by Bellman were not easy to solve [4]. Three years later, he built a model called Markov Decision Processes (MDPs) to describe a kind of discrete deterministic processes [3]. This deterministic system and the concept of value function which is described in the Bellman equation consists of the basic theory of modern reinforcement learning. In optimal control thread, solving problems required full knowledge of the environment and it was not a feasible way to deal with most problems in the real world. The trial-and-error thread focused more on the feedback rather than the environment itself. The \ufb01rst expression about the key idea of trail-and-error including \u201dselectional\u201d and \u201dassociative\u201d called \u201dLaw of E\ufb00ect\u201d was written in Edward Thorndike\u2019s book \u201dAnimal Intelligence\u201d [47]. Although supervised learning was not \u201dselectional\u201d, some researchers still mistook it for reinforcement learning and concentrated on pattern recognition [8, 55]. This led to rare researches in actual trial-and-error learning until Klopf recognized the di\ufb00erence between supervised learning and RL: the motivation to gain more rewards from the environment [16, 17]. However, there were still some remarkable works such as the reinforcement learning rule called \u201dselective bootstrap adaptation\u201d by Widrwo in 1973 [54]. Both of two threads came across in modern reinforcement learning. Temporal Di\ufb00erence (TD) learning was a method that predicts future values depend on the current signal which originated from animal learning psychology. This idea was \ufb01rst proposed and implemented by Samuel [35]. In 1972, Klopf developed the idea of \u201dgeneralized reinforcement\u201d and linked the trial-and-error learning with animal learning psychology [16]. In 1983, Sutton developed and implemented the actor-critic architecture in trial-and-error learning based on the idea from Klopf [2]. Five years later, he proposed TD(\u03bb) algorithms which used additional step information for update policy and made TD learning a general prediction method for deterministic problems [42]. One year later, Chris used optimal control methods to solve CHAPTER 2. LITERATURE SURVEY 12 temporal-di\ufb00erence problems and developed the Q-learning algorithm which estimated delayed reward by action value function [53]. In 1994, an online Q-learning was proposed by Rummery and Niranjan which was known as SARSA [34]. The di\ufb00erence between Q-learning and SARSA was that the agent used the same policy during the learning process in SARSA while it always chooses the best action based on value function in Q-learning. With the development of the deep neural network, DeepMind proposed Deep Q-learning Network (DQN) algorithm which used a convolutional neural network to solve high dimensionality of the state in reinforcement learning problems [27]. Two years later, they modi\ufb01ed DQN by adding a target policy to improve its stability [28]. The highlight of the DQN was not only the combination of deep learning and RL but also the experience replay mechanism. To solve dependency problems when optimizing CNN, Mnih et al. stored the experiences to memory in each step and randomly sampled a mini-batch to optimize the neural network based on the idea from Lin [22]. In 2015, this mechanism was improved by measuring the importance of experience with temporal di\ufb00erence error [36]. Meanwhile, Wang proposed Dueling DQN which used an advantage function learning how valuable a state was without estimating each action value for each state [52]. This new neural network architecture was helpful when there was no strong relationship between actions and the environment. In 2016, DeepMind proposed Double DQN which show the higher stability of the policy by reducing overestimated action values [50]. Although a series of algorithms based on DQN show human-level performance on Atari games, they still failed to deal with some speci\ufb01c games. DQN was a value-based method which meant the choice of action was depend on the action values. However, choosing action randomly may be the best policy in some games such as Rock\u2212paper\u2212scissors. To deal with this problem, Sutton proposed policy gradient which enabled the agent to optimize the policy directly [44]. Based on this, OpenAI proposed a new family of algorithms such as Proximal Policy Optimization (PPO) [38]. PPO used a statistical method called importance sampling which was used to estimate a distribution by sampling data from another distribution and this simple modi\ufb01cation show a better performance in RoboschoolHumanoidFlagrun. Since the basic policy gradient method sampled data from completed episodes, the variance of the estimation was high because of the high dimension action space. Similar to valuebased method, Actor-critic method was proposed to solve this problem [18]. Compared with the policy gradient, this method used a critic to evaluate the chosen action. This made the policy can be updated after each decision which not only reduced the variance but also accelerated the convergence. The famous improved actor-critic based algorithm is asynchronous advantage actor-critic (A3C) [26]. Similar to Dueling DQN, this method used advantage function to estimate value function and performed computing in parallel which can largely increase the learning speed. 2.2.2 Markov Decision Processes As mentioned in Section 2.2.1, the interaction between the agent and the environment can be modeled as a Markov Decision Process which is based on Markov property. Markov property describes a kind of stochastic processes that the probability of next event occurring only CHAPTER 2. LITERATURE SURVEY 13 depend on the current event. De\ufb01nition 1 (Markov property [39]) Given a state St+1 at time t+1 in a \ufb01nite sequence {S0, S1, S2, \u00b7 \u00b7 \u00b7 , SN}. This sequence has Markov property, if and only if P [St+1|St] = P [St+1|S1, . . . , St] (2.15) A Markov Decision Process (MDP) is a random process with Markove property, values and decisions. De\ufb01nition 2 (Markov Decision Process [39]) A Markov Decision Process can be described as a tuple \u27e8S, A, P, R, \u03b3\u27e9 \u2022 S is a \ufb01nite set of states \u2022 A is a \ufb01nite set of actions \u2022 P is a state transition probability matrix Pa ss\u2032 = P \ufffd St+1 = s\u2032|St = s, At = a \ufffd (2.16) \u2022 R is a reward function Ra s = E [Rt+1|St = s, At = a] (2.17) \u2022 \u03b3 is a discount factor \u03b3 \u2208 [0, 1] To describe how the decision is made, a policy \u03c0 is required to de\ufb01ne the behaviour of the agent. De\ufb01nition 3 (Policy [39]) A policy is a distribution over actions given states \u03c0(a|s) = P [At = a|St = s] (2.18) In MDP, the agent is expected to get as many rewards as it can from the environment. However, maximizing the reward at time-step t makes the agent short-sighted which means it only considers the reward from the next action rather the total reward of one episode. Therefore, return is de\ufb01ned as the concept \u201dreward\u201d which the agent is expected to maximize. De\ufb01nition 4 (Return [39]) The return Gt is the total discounted reward Rt from time-step t. Gt = Rt+1 + \u03b3Rt+2 + . . . = \u221e \ufffd k=0 \u03b3kRt+k+1 (2.19) where \u03b3 \u2208 [0, 1] is the discount factor. The value of the discount factor represents how far-sighted the agent will be. If this value is 1, the agent will treat every reward in the future as the same. But this will also make the agent confused about which decision is not appropriate. At this point, the behavior of the agent can be described as in Figure 2.8 CHAPTER 2. LITERATURE SURVEY 14 Figure 2.8: Markov decision process in reinforcement learning. Since the return is de\ufb01ned in a random process, similar to reward function, the expectation of it can be de\ufb01ned as following which is also called value function. De\ufb01nition 5 (State Value Function [39]) The state-value function v\u03c0(s) of an MDP is the expected return starting from state s, and then following policy \u03c0 v\u03c0(s) = E\u03c0 [Gt|St = s] (2.20) De\ufb01nition 6 (Action Value Function [39]) The action-value function q\u03c0(s, a) is the expected return starting from state s, taking action a, and then following policy \u03c0 q\u03c0(s, a) = E\u03c0 [Gt|St = s, At = a] (2.21) With the De\ufb01nition 5, 6 and the de\ufb01nition of the expectation, we can simply write v\u03c0(s) = \ufffd a\u2208A \u03c0(a|s)q\u03c0(s, a) (2.22) where A is a set of action the agent can choose. 2.2.3 Bellman Equation Since we de\ufb01ne the Markov decision process in Section 2.2.2, the behavior of the agent can be described mathematically. As mentioned in Section 2.2.1, this problem can be solved by the Bellman equation. Theorem 1 (Bellman Expectation Equation [39]) The state-value function can be decomposed into immediate reward plus discounted value of successor state v\u03c0(s) = E\u03c0 [Rt+1 + \u03b3v\u03c0 (St+1) |St = s] (2.23) CHAPTER 2. LITERATURE SURVEY 15 The action-value function can similarly be decomposed q\u03c0(s, a) = E\u03c0 [Rt+1 + \u03b3q\u03c0 (St+1, At+1) |St = s, At = a] (2.24) Here is a simple proof for Equation 2.24. According to the De\ufb01nition 4, the return at time t can be decomposed into two parts: the immediate reward and the discounted return at time t + 1 Gt = Rt+1 + \u03b3Gt+1 (2.25) Substitute Gt with Equation 2.25 in De\ufb01nition 6 q\u03c0(s, a) = E\u03c0 [Rt+1 + \u03b3Gt+1|St = s, At = a] (2.26) Due to the linearity of expectation, Gt+1 can be replaced by q\u03c0 (St+1, At+1) and then we obtain the Bellman equation for action-value function. The state-value function can be proved in the same way. With the de\ufb01nition of optimal value function De\ufb01nition 7 (Optimal Value Function [39]) The optimal state-value function v\u2217(s) is the maximum value function over all policies v\u2217(s) = max \u03c0 v\u03c0(s) (2.27) The optimal action-value function q\u2217(s, a) is the maximum action-value function over all policies q\u2217(s, a) = max \u03c0 q\u03c0(s, a) (2.28) Theorem 1 can be extended to Bellman optimality equation Theorem 2 (Bellman Optimality Equation [39]) The optimal state-value function can be decomposed into maximum immediate reward plus discounted optimal value of successor state v\u2217(s) = max a Ra s + \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032v\u2217 \ufffd s\u2032\ufffd (2.29) The optimal action-value function can similarly be decomposed q\u2217(s, a) = Ra s + \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032 max a\u2032 q\u2217 \ufffd s\u2032, a\u2032\ufffd (2.30) where s = St, a = At, s\u2032 = St+1, a\u2032 = At+1 Here is a simple proof for Equation 2.30. Due to the linearity of expectation, Equation 2.26 can be decomposed into the expectation of the immediate reward E\u03c0 [Rt+1|St = s, At = a] (2.31) and the expectation of the discounted return at time t + 1 CHAPTER 2. LITERATURE SURVEY 16 \u03b3E\u03c0 [Gt+1|St = s, At = a] (2.32) According to the de\ufb01nition of reward function in De\ufb01nition 2, Equation 2.31 is equal to Ra s. If next state is s\u2032, Equation 2.32 can be written as following with the transition probability matrix Pa ss\u2032 \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032E\u03c0 \ufffd Gt+1|St = s, At = a, St+1 = s\u2032\ufffd (2.33) With the Markov property, we know the expectation of the return in Equation 2.33 is not related to the current state s and action a and this is equal to the state-value function. Therefore, Equation 2.33 can be written as following \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032v(s\u2032) (2.34) Considering 2.31, 2.34 and 2.22, the action-value function can be written as following q\u03c0(s, a) = Ra s + \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032 \ufffd a\u2032\u2208A \u03c0 \ufffd a\u2032|s\u2032\ufffd q\u03c0 \ufffd s\u2032, a\u2032\ufffd (2.35) It is easy to prove that there is always an optimal policy for any Markov decision process and it can be found by maximizing action-value function. \u03c0\u2217(a|s) = \ufffd 1 if a = argmax a\u2208A q\u2217(s, a) 0 otherwise (2.36) Considering 2.36, Bellman optimality equation for action-value function can be obtained by replacing the policy in Equation 2.35 with optimal policy. There are many ways to solve this equation such as Sarsa and Q-learning. This will be discussed in Section 2.2.5. 2.2.4 Exploitation vs Exploration If the agent has complete knowledge of the environment, in the other word, the transition probability P a ss\u2032 can be calculated given state s and action a, Equation 2.30 can be solved by an iterative method with appropriate \u03b3. However, this method is unable to deal with an unknown environment because a large amount of information has to be collected to estimate P a ss\u2032 before the convergence of action value function. If the q function tends to be stable before the environment has been fully explored, the performance of the model would be far from satisfactory, especially in high action space situation. To deal with this problem, \u03f5 - greedy selection [43] is introduced to ensure the agent make enough exploration before the convergence of the action value function. Instead of choosing CHAPTER 2. LITERATURE SURVEY 17 the best action estimated by q function, there is a probability of \u03f5 to randomly select from all actions. The mathematical expression of this method is shown as following \u03c0(a|s) = \ufffd \u03f5/m + 1 \u2212 \u03f5 if a\u2217 = arg max a\u2208A q(s, a) \u03f5/m otherwise (2.37) where m is the number of actions. This method may have a bad e\ufb00ect on the performance of the agent at \ufb01rst several episodes during the training but it can widen the horizon of the agent in long term view. 2.2.5 Temporal Di\ufb00erence Learning As mentioned in Section 2.2.4, most environment in the real world is unknown. To solve this problem, a method called Monte Carlo (MC) is used to sample data for estimating value function. The agent can learn the environment from one episode experience and the value function can be approximated by the mean of the return instead of the expectation. The mathematical expression can be described as following v(St) = S(St) N(St) (2.38) where St is the state at time t, S(St) is the sum of return and N(St) is the counter to record the visit number of state St. There are two kinds of visit: \ufb01rst visit and every visit. The former one means the model only need to record the \ufb01rst visit of state St in one episode while all visit of St in one episode will be taken into consideration in every visit. Simplify equation 2.38, we can get the recurrence equation for v(s) v(s) \u2190 v(s) + \u03b7(Gt \u2212 v(s)) (2.39) where \u03b7 is the learning rate which can control the update speed of the value function and s is the state at time t. The problem of Monte Carlo method is all rewards in one episode have to be collected to get Gt. The value function can only be updated when reaching the end of the episode which may lead to low training e\ufb03ciency. To update value function with an incomplete episode, the return can be replaced by estimated value function using bootstrapping. With the Bellman equation 2.23 and 2.39, we can write v(s) \u2190 v(s) + \u03b7(Rt+1 + \u03b3v(s\u2032) \u2212 v(s)) (2.40) This idea is called Temporal Di\ufb00erence (TD) Learning. In TD learning, value function will be updated immediately after a new observation. Compared with MC methods, TD learning has lower variance because there are too many random actions {At+1, At+2, \u00b7 \u00b7 \u00b7 } in the Monte Carlo method which will lead to the high variance. Similarly, the recurrence equation for action value function can be written as following CHAPTER 2. LITERATURE SURVEY 18 q(s, a) \u2190 q(s, a) + \u03b1 \ufffd Rt+1 + \u03b3q(s\u2032, a\u2032) \u2212 q(s, a) \ufffd (2.41) where s and a is the state and action at time t, s\u2032 and a\u2032 is the state and action at time t+1. Equation 2.41 shows an iterative method to get the optimal action value function q\u2217(s, a). With this equation and \u03f5 - greedy policy, the RL problem can be solved by Sarsa [34]. Algorithm 1 Sarsa 1: set learning rate \u03b1, number of episodes N, explore rate \u03f5, discount factor \u03b3 2: set q(s, a) \u2190 0, \u2200s, a 3: for episode \u2190 1 to N do 4: initialize time t \u2190 0 5: get state s0 from the environment 6: choose action a0 following \u03f5 - greedy policy from q(s, a) 7: while episode is incomplete do 8: take action and get next state st+1, reward rt+1 from the environment 9: choose action at+1 following \u03f5 - greedy policy from q(s, a) 10: update q(st, at) \u2190 q(st, at) + \u03b1 \ufffd rt+1 + \u03b3q(st+1, at+1) \u2212 q(st, at) \ufffd 11: t \u2190 t + 1, st \u2190 st+1, at \u2190 at+1 12: end while 13: end for The name of Sarsa is from the sequence {S0, A0, R1, S1, A1, R2, \u00b7 \u00b7 \u00b7 }. Besides Sarsa, there is another similar algorithm called Q learning [53]. Algorithm 2 Q learning 1: set learning rate \u03b1, number of episodes N, explore rate \u03f5, discount factor \u03b3 2: set q(s, a) \u2190 0, \u2200s, a 3: for episode \u2190 1 to N do 4: initialize time t \u2190 0 5: get state s0 from the environment 6: while episode is incomplete do 7: choose action at following \u03f5 - greedy policy from q(s, a) 8: take action and get next state st+1, reward rt+1 from the environment 9: update q(st, at) \u2190 q(st, at) + \u03b1 \ufffd rt+1 + \u03b3 max a q(st+1, a) \u2212 q(st, at) \ufffd 10: t \u2190 t + 1, st \u2190 st+1 11: end while 12: end for In algorithm 2, there are two policies during the iteration. When choosing the action at+1 from q(s, a) given st+1, Sarsa uses \u03f5 - greedy policy while Q learning uses greedy policy. But both of them are choosing at with \u03f5 - greedy policy. Considering the example of Cli\ufb00 Walking shown in Figure 2.9 from Sutton\u2019s book [43], every transition in the environment will get \u22121 reward except next state is the cli\ufb00 which the agent will get \u2212100 reward, Sarsa is more likely CHAPTER 2. LITERATURE SURVEY 19 to choose the safe path while Q learning tends to choose the optimal path with \u03f5 - greedy policy. But both of them can reach the optimal policy if reducing the value of \u03f5. Figure 2.9: Example of cli\ufb00 walking from [43]. 2.2.6 Deep Q Network Q learning is a powerful algorithm to solve simple reinforcement problems. However, it is unable to deal with continuous states or continuous actions. To solve the former problem, deep learning method can be used to approximate action value function. Generally, states are image data observed by the agent and convolutional neural network is an e\ufb00ective way to extract features from this kind of data in convolution layers and feed them into the fully connected layer to approximate q function. Several consistent stationary images will be stacked into one input data to make the model understand that the agent is moving. But the input data is highly dependent, the performance of the model will be largely a\ufb00ected by the dependency. As mentioned in Section 2.2.1, DeepMind introduced experience replay pool which will store the experience into the memory and sample some of them to optimize the neural network model in 2013 [27]. Using Q learning, deep learning and experience replay pool, the improved algorithm named Deep Q Network (DQN) shows incredible performance on Atari games according to their paper. Two years later, they found the agent became more stable by using two network [28]. This algorithm can be described as below All states in Algorithm 3 have to be pre-processed before feeding into a neural network model. Based on Deep Q Network, there are three kinds of improved algorithms considering the stability of the training process, the importance of each experience and new neural network architecture. Double DQN [50] utilizes the advantage of two networks. Instead of \ufb01nding the optimal q value from target network q\u2032(s, a) directly, this method chooses the optimal action from the policy network and \ufb01nd the corresponding q value in the target network. Use the term in Algorithm 3, the change can be illustrated as following CHAPTER 2. LITERATURE SURVEY 20 Algorithm 3 Deep Q Network 1: initialize policy network q(s, a) with random weights 2: set learning rate \u03b1, number of episodes N, explore rate \u03f5, discount factor \u03b3 3: set batch size M, update step T 4: set target network q\u2032(s, a) = q(s, a) 5: for episode \u2190 1 to N do 6: initialize time t \u2190 0 7: get state s0 from the environment 8: while episode is incomplete do 9: choose action at following \u03f5 - greedy policy from policy network q(s, a) 10: take action and get next state st+1, reward rt+1 from the environment 11: store transition (st, at, st+1, rt+1) in experience replay pool 12: random sample M batch experience (sk, ak, sk+1, rk+1) from the pool 13: calculate corresponding q(sk, ak) from policy network q(s, a) 14: calculate yk using target network q\u2032(s, a) yk = \ufffd rk+1 if next state is completed rk+1 + \u03b3 max a q\u2032(sk+1, a) otherwise 15: optimize the policy model with gradient (yk \u2212 q(sk, ak))2 16: replace target network with policy network when reach the update step T 17: t \u2190 t + 1, st \u2190 st+1 18: end while 19: end for yk = \ufffd rk+1 if next state is completed rk+1 + \u03b3q\u2032(sk+1, arg max a q(sk+1, a)) otherwise (2.42) Prioritized Experience Replay (PER) introduced a way to e\ufb03ciently sample transitions from the experience replay pool [36]. Instead of uniform random sampling, there is a priority of each transition P(i) = p\u03b1 i \ufffd k p\u03b1 k (2.43) where pi > 0 is the priority of transition i and \u03b1 is the indicator of the priority, especially \u03b1 = 0 when using uniform random sampling. The priority can be measured by TD error \u03b4i, which is the following term \u03b4i = Ri + \u03b3 max a q(si, a) \u2212 q(si\u22121, ai\u22121) (2.44) Based on TD error, p(i) can be calculated in two way. The \ufb01rst is proportional prioritization which uses the absolute value of TD error CHAPTER 2. LITERATURE SURVEY 21 p(i) = |\u03b4| + \u03f5 (2.45) where \u03f5 is to avoid zero prioritization. The other one is rank-based p(i) = 1 rank(i) (2.46) where rank(i) is the rank of transition by sorting TD error \u03b4i. According to Schaul, both proportional based and rank based prioritization can speed-up the training but the later one is more robust which has better performance when meeting outliers. However, the random sampling is abandoned after adding priority mechanism which will result in high bias. In other words, those transitions with small TD error are unlikely to be sampled and the distribution is changed. Therefore, the \ufb01nal model may far from the optimal policy and performance of the agent even be lower than DQN. Important sampling (IS) [29] is an e\ufb00ective technique to estimate a distribution by sample data from a di\ufb00erent distribution. Given a probability density function p(x) over distribution D, with the de\ufb01nition of the expectation Ep [f(x)] = \ufffd D f(x)p(x)dx (2.47) where Ep [\u00b7] denotes the expectation for x \u223c p and f is the integrand. Given another probability density function q(x), the expectation can be written as following \ufffd D f(x)p(x)dx = \ufffd D f(x)p(x) q(x) q(x)dx = Eq \ufffdf(x)p(x) q(x) \ufffd (2.48) where Eq [\u00b7] denotes the expectation for x \u223c q. With Monte Carlo integration, the expectation Ep [f(x)] can be estimated by 1 N N \ufffd i=1 f(i)p(i) q(i) (2.49) where i is sampled from x. if p(i) is uniform distribution and q(i) refers to Equation 2.43, we have p(i) q(i) = 1 N \u00b7 1 P(i) (2.50) adding a tunable parameter \u03b2, we obtain the importance-sampling weights wi = (N \u00b7 P(i))\u2212\u03b2 max i wi (2.51) CHAPTER 2. LITERATURE SURVEY 22 where \u03b2 will decay from a user-de\ufb01ned initial value to 1 and the bias completely disappears when \u03b2 = 1. Term max i wi is used to normalize the weight to increase stability. Use the term in Algorithm 3, the update of q function can be modi\ufb01ed as following q(s, a) \u2190 q(s, a) + \u03b7 \u00b7 wk \u00b7 \u2207(yk \u2212 q(sk, ak))2 (2.52) where \u03b4k is TD error and \u03b7 is learning rate. Dueling DQN architecture used a new concept called advantage function which is the subtraction of the action value function and state value function [52]. A\u03c0(s, a) = q\u03c0(s, a) \u2212 v\u03c0(s) (2.53) As shown in Figure 2.10, dueling network architecture use summation of two steams which is advantage function and state value function to get the q function. The state values can be updated more accurately with this method. Figure 2.10: Architecture comparison between Dueling DQN and DQN [52] Chapter 3 Methodology This chapter gives the requirements of the project, introduces the design of reward function and shows the preprocessing steps of the input image. The choice of the model, as well as the architecture, will be discussed. Experiment design and evaluation methods will be illustrated in the last. 3.1 Requirements 3.1.1 Software Requirement Considering the readability of the code, widely used additional frameworks such as Torch, Python is a suitable choice for this project. OpenCV is used to preprocess the image getting from the environment. Numpy is a Python library which accelerates matrices operations with C. This enables the user to write e\ufb03cient scienti\ufb01c computing code with Python. There are plenty of deep learning frameworks like Tensor\ufb02ow which has many extensive API and is widely used in industrial products. However, it will take a relatively long time for the beginner to fully understand the usage of Tensor\ufb02ow. Pytorch is a recently developed framework which is described as \u201dNumpy with GPU\u201d. The simplicity of Pytorch makes more and more academic researchers using it to implement their new ideas in a much easier way. Because T-rex Runner is running on Chrome, the latest Chrome is used here. Gym is a game library developed by OpenAI [7]. This framework provides a built-in environment for some famous games such as Atari 2600 and it is easy for the user to customize their own environment. Table 3.1 shows all software requirement in this project. 3.1.2 Hardware Requirement As the game is running on Chrome, it is hard to use a Linux server to perform the experiments. Although headless Chrome is a plausible choice, there are some environmental issues during the investigation. Therefore, all experiments will be running on the laptop from the author. There will be some limitation such as 6GB GPU memory limits the size of experience replay 23 CHAPTER 3. METHODOLOGY 24 Software Description OS Windows 10 Programming language Python 3.7.4 Framework OpenCV, Pytorch, Numpy, Gym Browser Chrome 76 Table 3.1: Software requirement pool. Therefore, parameters related to hardware limitation will be suitably chosen without tuning in this project. Table 3.2 lists all hardware information used in this project. Hardware Description CPU Intel Core i5-8300H RAM 16G GPU Nvidia GTX 1060 6G Table 3.2: Hardware requirement 3.2 Game Description T-rex Runner is a dinosaur game from Google Chrome o\ufb04ine mode. Everyone can access this link on Chrome to play the game. The target for players is to control the dinosaur overcoming as many obstacles as possible. The current score of the game will increase by time if the dinosaur keeps alive as shown at the top right corner of Figure 3.1 as well as the highest score. As shown in Figure 3.2, the dinosaur has three actions to choose in every state: do nothing, jump or duck. Figure 3.1: A screenshot of T-rex Runner. Environment plays an important role in reinforcement learning because the agent will improve the policy based on the feedback from it. However, it is di\ufb03cult to quantify the rewards for each action as well as the return for an entire episode. In most research for RL algorithms, modifying reward will not be taken into consideration but it will signi\ufb01cantly impact the CHAPTER 3. METHODOLOGY 25 (a) Do nothing (b) Jump (c) Duck Figure 3.2: Three type of actions in T-rex Runner performance of the model because it decides the behavior of the agent. For example, shaping reward shows a better performance in Andrew\u2019s experiment[30]. It adds a new term F to modify the original reward based on the goal R\u2032 = R + F (3.1) The closer the agent towards the goal, the larger the F is. However, the aim of this project is to train the agent to play the game and compare the performance between di\ufb00erent algorithms. So the e\ufb00ect of reward function will not be taken into consideration and a \ufb01xed reward function will be used across all experiments. Since there is no previous study on T-rex Runner with reinforcement learning, the design of reward function is a hard part of this project. Intuitively, the best design is awarding the agent for jumping over the obstacles and penalizing it for hitting the obstacles. The jumping reward will gradually increase as time goes by. However, object detection in moving pictures is required to ful\ufb01ll this goal. As this task is out of the requirements of this project, we proposed a naive reward design as shown in Algorithm 4. Algorithm 4 Reward Design in T-rex Runner 1: if episode is completed then 2: return reward as \u22121 3: else 4: if agent choose jump then 5: return reward as 0 6: else 7: return reward as 0.1 8: end if 9: end if The basic idea of Algorithm 4 is giving a relatively small reward to the agent if it is alive and penalize it when hitting an obstacle. Zero reward for jumping is set to make the dinosaur only jumps if it is very close to obstacles. The unexpected jump will limit the movement in the next few states. CHAPTER 3. METHODOLOGY 26 Although there are three kinds of action in this game as introduced in Section 3.2, duck is optional because the agent can overcome the obstacle using jump under the same circumstances. Considering most obstacles in the game are cactus which can only be overcome by jumping, only two actions (do nothing and jump) will be used in this investigation. 3.3 Model Selection Since there are only two actions in T-rex Runner, according to the literature review on deep reinforcement learning in Section 2.2.1, value-based methods are proved to be powerful to handle this game. Although policy-based methods such as proximal policy gradient is a good choice too, only DQN, double DQN, DQN with prioritized experience replay and dueling DQN will be investigated in this project due to the time limitation. Deep Q network which is shown in Algorithm 3 is a basic reinforcement learning algorithm using deep learning. According to the result from DeepMind, it is expected to achieve at least human-level results with only DQN. Double DQN mitigates the q value overestimation problems utilizing two advantage of two networks as shown in Equation 2.42 but it is not expected to achieve a higher performance in this experiment because there is only two actions. The bad e\ufb00ect of overestimation problems is not obvious under this circumstance. Dueling DQN adds an advantage function which is the subtraction of action value function and state value function before the output layer in the convolutional neural network as shown in Equation 2.53. Since the evaluated game in [52] is a similar racing game overcoming obstacles compared with T-rex Runner, this algorithm is expected to have a better performance than DQN. Prioritized Experience replay improves training e\ufb03ciency by changing the distribution of the stored transitions. It assigns the weight for each experience by TD error. There are two ways to calculate prioritization which is proportional based method and rank-based method. According to the [36], the former one has a relatively better performance, only this method will be implemented in this investigation due to the time limitation. The performance is expected to be the same as DQN because there is no change in the algorithm but it may be faster to reach the same performance. 3.4 Image Preprocessing Following the preprocessing step in [27, 28], the raw observed image which is in RGB representation will be converted to gray-scale representation. To make the network easier to recognize dinosaur and obstacles, unnecessary objects such as clouds and scores will be removed. In this step, the color of the background and the object are reversed in order to perform erosion and dilation. These two basic morphological operations can help reduce small bright color which is often noisy data. Finally, the image is resized to 84 \u00d7 84 following the recipe from DeepMind. Since the movement should be recognized by the neural network, perform the CHAPTER 3. METHODOLOGY 27 same preprocessing step for last four frames in the history and stack those four as one data point which is also the input of CNN. The entire process is shown in Figure 3.3. Figure 3.3: Preprocessing steps for T-rex Runner. 3.5 Convolutional Neural Network Architecture There are two kinds of convolutional neural network used in this project. The basic DQN is proposed in [27, 28] which used three convolutional layers and two fully connected layers. The reason for not using pooling layer is to detect the movement of the agent. Both max pooling and average pooling may make the neural network ignore a very small change in the image. Therefore, there are only convolutional layers in this architecture. The architecture for training the agent using DQN is shown in Figure 3.4. Dueling architecture is proposed in [52] which divided the q network into two parts. One of them is only related to the state value function v(s), the other one is advantage function A(s, a) which is a\ufb00ected by both state and action. The \ufb01nal action value function is the summation of those two. q(s, a; \u03b8, \u03c91, \u03c92) = v(s; \u03b8, \u03c91) + A(s, a; \u03b8, \u03c92) (3.2) CHAPTER 3. METHODOLOGY 28 Figure 3.4: Convolutional Neural Network architecture for Deep Q Network. where \u03b8 is the shared parameter of CNN, \u03c91 is the value function only parameter and \u03c92 is the advantage function only parameter. Both DQN and Dueling DQN are using Algorithm 3, the only di\ufb00erence is the neural network architecture. RMSprop [48] which is an adaptive gradient method based on stochastic gradient descent will be used as the optimization algorithm in this project. This is the same optimization method used by DeepMind [27, 28]. Figure 3.5 shows the process of dueling DQN. 3.6 Experiments 3.6.1 Hyperparameter Tuning Before the comparison of algorithms, hyperparameter tuning is required to get high-performance models. As mentioned before, the memory size is \ufb01xed to 3 \u00d7 105 due to the hardware limitation. Because there is no previous study on this game, and the hyperparameters list in [28] have a bad result on this game. All other hyperparameters have to be set to a suitable value. Grid search is performed to \ufb01nd a workable combination of those parameters. Due to the time limitation, all parameters will only be slightly modi\ufb01ed and only one hyperparameter will vary during each tuning experiment. The choice of the parameter will consider both score and stability. Each parameter will be tuned with 800 episodes. CHAPTER 3. METHODOLOGY 29 Figure 3.5: Convolutional Neural Network architecture for Dueling Deep Q Network. 3.6.2 Comparison of di\ufb00erent Deep Q Network Algorithms There are three improved reinforcement algorithms based on DQN mentioned in Section 2.2.6. Double DQN makes the performance of the agent more stable by solving the overestimated q value problem. Prioritized experience replay improves the training e\ufb03ciency by sample more valuable transitions. Dueling DQN modi\ufb01es the neural network architecture to get a better estimation of state values. In this experiment, DQN will be \ufb01rst used to train the agent based on the hyperparameters tuned in Section 3.6.1 and this result will be treated as a baseline across all the experiments. Double DQN, DQN with prioritized experience replay and Dueling DQN will be applied to the agent separately. The performance of those three is expected to be better than DQN according to the related papers. Due to time limitation, no combination of those three algorithms will be performed in this project. This section only compares the performance of each algorithm. 3.6.3 E\ufb00ect of Batch Normalization As mentioned in Section 2.1.5, it is proved that batch normalization can reduce training time and mitigate the vanishing gradient problem in a convolutional neural network. However, there is no evidence that this method has the same e\ufb00ect on reinforcement learning. This section will perform experiments on this point. Based on the experiment in Section 3.6.2, adding batch normalization in each convolutional layer and compared with the results with the outcome in previous experiments. ",
    "Methodology": "METHODOLOGY 30 3.7 Evaluation To evaluate the performance of the agent, DeepMind used trained agent playing the game for 30 times for up to 5 min and \u03f5 - greedy policy with \u03f5 = 0.05 [28]. Considering only one game is investigated in this project, the average score will be used instead of average reward because the number of jumps in each episode will a\ufb00ect the total reward according to the designed reward function. The greedy policy will be used in the evaluation stage instead of \u03f5 - greedy policy because the later one will bring randomness to the decision which will a\ufb00ect the performance of the trained model. Therefore, the trained agent will play the game for 30 times without time limitation and using greedy policy. All outcomes will be compared with the results from a human expert. The average scores during the training stage will be shown graphically. This is a clear way to show the learning e\ufb03ciency of each algorithm. Both graphical and statistical results such as mean, variance and median will be analyzed. However, only statistical results will be analyzed in the testing stage because the trained model for each algorithm are the same and there is no increasing trend can be shown like in the training stage. These results will be visualized with a boxplot. Chapter 4 Results and Discussion 4.1 Hyper Parameter Tuning The value of hyperparameters may a\ufb00ect the performance of the model. However, there are so many parameters in reinforcement learning including optimization algorithm parameters such as learning rate. This may take a long time to \ufb01nd the optimal combination of these parameters using a grid search. Since there is no metric like accuracy in RL which can easily re\ufb02ect the performance of the model, we assume each parameter is independent of others. Therefore, each parameter can be tuned one after another. Because the objective of this project is to compare the performance between di\ufb00erent algorithms and the e\ufb00ect of batch normalization, those tuned parameters by DQN will be used across all the experiments. The start hyperparameters of DQN are shown in Table 4.1. Hyper parameter Value Description Memory Size 3 \u00d7 105 Size of experience replay pool Batch Size 128 Size of minibatch to optimize model Gamma 0.99 Discount factor Initial \u03f50 1 \u00d7 10\u22121 Explore probability at the start of the training Final \u03f5\u2032 1 \u00d7 10\u22123 End point of explore probability in \u03f5 decay Explore steps 1 \u00d7 105 Number of steps for \u03f5 decay from \u03f50 to \u03f5\u2032 Learning Rate 1 \u00d7 10\u22124 Learning speed of the model Table 4.1: Hyper parameters used in all experiments 4.1.1 Learning Rate Learning rate controls the learning speed of the model, too large value will result in divergence and too small value may double the training time. Figure 4.1 shows four di\ufb00erent values of learning rate. Obviously, 1 \u00d7 10\u22125 is too small and there is no increase trend during the entire process. Both 1 \u00d7 10\u22124 and 5 \u00d7 10\u22125 make the 31 CHAPTER 4. RESULTS AND DISCUSSION 32 Figure 4.1: Hyper parameter tuning for learning rate. score unstable after 50th epoch. Considering the stability and 200 epochs will be trained in formal experiment, 2 \u00d7 10\u22125 will be chosen as learning rate. 4.1.2 Batch Size Batch size de\ufb01nes how many transitions will be used to update the neural network which may a\ufb00ect the training speed. But as mentioned in 2.2.1, too big size will cause the dependency problems which may largely a\ufb00ect the performance of the model. Figure 4.2: Hyper parameter tuning for batch size. As shown in Figure 4.2, the average score of three curves at epoch 80 are all around 800. Among those three, the most stable one is batch size 128. CHAPTER 4. RESULTS AND DISCUSSION 33 4.1.3 Epsilon \u03f5 - greedy policy determines the probability of exploration. In some games, especially with high action spaces, this value can a\ufb00ect how good the model will converge. However, there are only two actions in T-rex Runner so it is unnecessary to random choose action at the begin. Instead of initializing \u03f5 to 1 as DeepMind did in their paper [28], the start value is set to 0.1 in this model. Figure 4.3: Hyper parameter tuning for explore probability \u03f5. All experiments achieve acceptable results in Figure 4.3 except the one with \ufb01xed \u03f5 = 0.1. In this case, we select \u03f5 from 0.1 to 0.0001 but either of those three can be chosen according to this graph. This experiment also demonstrates the positive e\ufb00ect of linear annealing for \u03f5. 4.1.4 Explore Step Explore step is the number of steps required to anneal \u03f5 from 0.1 to 0.0001. As mentioned that hyperparameters related to exploration will not a\ufb00ect too much in this game. The most stable one will be selected from Figure 4.4 which is 1 \u00d7 105. 4.1.5 Gamma Discount factor decides how far-sighted the agent will be. Too small value will make the agent consider more about the current reward and too big value will make the agent pay the same attention to rewards after this time point. This may confuse the agent about which action leads to a high or low return. Figure 4.5 shows the average score for four di\ufb00erent gamma. Obviously, \u03b3 = 0.9 make the agent short-sighted and there is no signi\ufb01cant change during 80 epochs. When \u03b3 \u2265 0.999, the average score \ufb02uctuates widely after 50th epoch. Since \u03b3 = 0.99 has a gradually increasing trend, this will be used as the \ufb01nal discount factor. CHAPTER 4. RESULTS AND DISCUSSION 34 Figure 4.4: Hyper parameter tuning for explore steps. Figure 4.5: Hyper parameter tuning for discount factor \u03b3. 4.2 Training Results The tuned hyperparameters from the previous experiment are listed in Table 4.2. Although these parameters are tuned by DQN algorithm, they are expected to \ufb01t other three improved algorithms which are Double DQN, Dueling DQN and DQN with prioritized experience replay because there is no big di\ufb00erence among them. All algorithms will be only trained with 200 epochs because of the time limitation. The total training time for each algorithm is shown in the last column of Table 4.4 CHAPTER 4. RESULTS AND DISCUSSION 35 Hyper parameter Value before tune Value after tune Memory Size 3 \u00d7 105 3 \u00d7 105 Batch Size 128 128 Gamma 0.99 0.99 Initial \u03f50 1 \u00d7 10\u22121 1 \u00d7 10\u22121 Final \u03f5\u2032 1 \u00d7 10\u22123 1 \u00d7 10\u22124 Explore steps 1 \u00d7 105 1 \u00d7 105 Learning Rate 1 \u00d7 10\u22124 2 \u00d7 10\u22125 Table 4.2: Hyperparameters used in all experiments 4.2.1 DQN Figure 4.6 shows the result of DQN algorithm for 200 epochs with tuned parameters. A gradually increased average score can be seen from this graph. This not only proves that the agent can play the game through DQN but also shows that the design of the reward function is relatively reasonable. This result will be treated as a baseline and will be used to compare with other algorithms. Figure 4.6: Training result for DQN. 4.2.2 Double DQN Double DQN has a similar performance in training compared with DQN. As mentioned before, the e\ufb00ect of q overestimation is not so signi\ufb01cant in T-rex Runner because there are only two actions. As shown in Figure 4.7, there are four data points with average scores below 200 while all average scores are above this value in DQN. CHAPTER 4. RESULTS AND DISCUSSION 36 Figure 4.7: Training result for Double DQN compared with DQN. 4.2.3 Dueling DQN Surprisingly, dueling DQN shows an incredible training performance after 150th epoch while the curve before that time seems similar. In Figure 4.8, the average score is above 5000 which is ten times higher than the maximum average score in DQN. However, these scores have a high variance which \ufb02uctuates widely between 1000 and 5000. From the graph, the training process of dueling DQN is stable before 150th epoch and end up with an increasing trend. Since we tuned all hyperparameters based on DQN, these values may not be the best for dueling network which results in the stable and relatively low average scores before 150th epoch. Figure 4.8: Training result for Dueling DQN compared with DQN. CHAPTER 4. RESULTS AND DISCUSSION 37 4.2.4 DQN with Prioritized Experience Replay Another important \ufb01nding in this section is the performance of prioritized experience replay. This is expected to have a shorter training time and a higher performance compared with DQN. But the result shown in Figure 4.9 suggests that the agent failed to learn to play the game with this method. There are two reasons for that. Figure 4.9: Training result for DQN with prioritized experience replay compared with DQN. One problem is from the algorithm. Compared with DQN, there are two extra steps have been applied to PER: weight calculation and prioritization update. Following the implementation in [36], sum tree which is a data structure with time complexity O(log N) for sampling and updating is used to store transitions instead of a linear list to accelerate memory related manipulation. The training time of PER is twice more than the one of DQN because of the batch size. Since we know that all sampled transitions will be traversed when updating the prioritization, the larger batch size is the longer time is required to perform this operation. Table 4.3 shows that this process is very time-consuming even using the batch size 32. These data are extracted from the training results choosing the same score of 43. The step size is the average value from ten records. Algorithm Score Batch Size Step Size DQN 43 128 180 DQN with PER 43 128 7 DQN with PER 43 32 22 Table 4.3: Step size di\ufb00erence between DQN and DQN with PER The other problem is from the game. Because this game is based on Chrome, it continues running when performing optimization while the game from o\ufb03cial OpenAI Gym is paused during this operation. Therefore, there is a delayed time before sending the action to Chrome. This in\ufb02uence is enlarged in prioritized experience replay since the time for update operation CHAPTER 4. RESULTS AND DISCUSSION 38 with batch size 128 takes approximately 10 times longer than normal DQN. Change the choice of hyperparameter can mitigate the \ufb01rst problem but the result is not as good as other algorithms. One thing we can expect is PER is unable to help the agent to get a higher score under this circumstance because the game speed will increase as time goes by. Since the time for updating the prioritization will not change too much, the time interval between two consistent decisions will be longer. This may limit the performance of the model. To eliminate the high computational e\ufb00ect from updating prioritization, the best way is to redevelop the game but due to the time limitation and the primary objective of this study, this result will be used as we can still compare the e\ufb00ect of batch normalization on this algorithm. 4.2.5 Batch Normalization Since the aim of this experiment is to \ufb01nd how batch normalization a\ufb00ects DQN algorithms, each result will be compared with the one without batch normalization which is shown in Figure 4.10. Figure 4.10: Batch normalization on DQN, Double DQN, Dueling DQN and DQN with prioritized experience replay From Figure 4.10, we can see that batch normalization can increase the mean of average scores in all experiments. But this also brings high variance which makes the average score CHAPTER 4. RESULTS AND DISCUSSION 39 diverge. According to the top-left graph, the \ufb01rst time for DQN agent to reach the average 1000 is approximately 150th epoch while the agent using DQN with batch normalization reach the same average score at 60th epoch and it is easy for it to get the higher score after that time. Double DQN curve has a similar trend but batch normalization in both of them also result in wide \ufb02uctuation. It is hard to say whether dueling network bene\ufb01ts from the batch normalization because there is a signi\ufb01cant increase trend on the bottom left graph. However, it is still can be seen that BN enable the agent to reach the same performance much earlier from 20th epoch to 90th epoch. For DQN with prioritized experience replay, even the performance is limited by the game itself, the one with batch normalization still can get a relatively higher score. 4.2.6 Further Discussion As graphical results and some explanation of them are shown above, this part will discuss numerical results from the experiments. Table 4.4 shows some statistical data fro training process. The maximum score is pointless in most games but considering T-rex Runner is a racing game, we still include this in the table. The last three columns are percentile data which are calculated by sorting in ascending order and \ufb01nding the x% observation. So 50% is the same as the median. The last column shows the training time for each algorithm. Algorithm Mean Std Max 25% 50% 75% Time (h) DQN 537.50 393.61 1915 195.75 481 820 25.87 Double DQN 443.31 394.01 2366 97.75 337 662.25 21.36 Dueling DQN 839.04 1521.40 25706 155 457 956.5 35.78 DQN with PER 43.50 2.791 71 43 43 43 3.31 DQN (BN) 777.54 917.26 8978 97.75 462.5 1139.25 32.59 Double DQN (BN) 696.43 758.81 5521 79 430.5 1104.25 29.40 Dueling DQN (BN) 1050.26 1477.00 14154 84 541.5 1520 40.12 DQN with PER (BN) 46.14 7.54 98 43 43 43 3.44 Table 4.4: Training results Ignoring the result from prioritized experience replay because of the inappropriate game environment, all algorithms achieve great results according to Table 4.4. Two algorithms with dueling network stand out from them. The one with batch normalization has the mean over 1000 which is 200 more than the one without BN. But the later one got the maximum score of 25706 which means the agent can keep running for around half an hour in one episode. However, both of them have high variance which exceed the mean. Double DQN both with BN and without BN perform worse than DQN. This indicates that double DQN may reduce the performance in low dimension action space. But batch normalization shortens the gap between those two algorithms which can be seen from the median and 75% percentile. Although most of statistical metrics are improved by batch normalization, the variance is much higher than before. As shown in the table, the variance from DQN with BN is twice CHAPTER 4. RESULTS AND DISCUSSION 40 more than the one without BN. Only the variance from dueling network is lower after BN. But it is reasonable because there is an incredible increase in the very later stage of the training shown in Figure 4.10. 4.3 Testing Results After training the agent for 2000 episodes, we use the latest model with greedy policy and play T-rex Runner for 30 times with each algorithm. Figure 4.11 shows the boxplot of those results as well as the collected data from the human expert. It is obvious that the agent trained by DQN with prioritized experience replay fail to learn to play the game because of the game environment issue discussed in the last section. It is surprising that the performance of double DQN is far from satisfactory even though it has similar training results compared with DQN. Table 4.5 shows that the mean of DQN results is three times higher than the one from double DQN. Dueling DQN algorithm achieves the highest score even though it still has the highest variance which is three times more than the variance from DQN. Figure 4.11: Boxplot for test result with eight di\ufb00erent algorithms According to Table 4.5, batch normalization improves the performance of the model regardless of algorithms and even the mean of DQN with PER is increased. However, it is not easy to say the e\ufb00ect of BN in dueling DQN is positive or not. From Figure 4.11, the one without BN has more outliers which results in high variance even though its mean is higher. Consider the median which is not sensitive with the outlier data, the one with BN is better and the minimum score is more than 200 which stands out from other algorithms. Since score 43 indicates the \ufb01rst time the agent meets the obstacle, it is easy to infer that all trained model fails to jump over the \ufb01rst cacti at least once except dueling DQN with BN. But dueling DQN is not fully trained which can be seen from the training result in Figure 4.8. That\u2019s also one ",
    "Discussion": "",
    "Results and Discussion": "",
    "Results": "",
    "Background": "Background The applications of Arti\ufb01cial Intelligence are widely used in recent years. As one part of them, Reinforcement Learning has achieved incredible results in game playing. An intelligent agent will be created and trained with reinforcement learning algorithms to ful\ufb01ll this tasks. In the Future of Go Summit 2017, Alpha Go which is an AI player trained with deep reinforcement learning algorithms won three games against the world best human player in Go. The success of reinforcement learning in this area shock the world and many researches are launched such as driverless cars. Deep learning methods such as convolutional neural network contributes a lot to this because these techniques solves the problem of dealing with high dimension input data and feature extraction. T-rex Runner is a dinosaur game from Google Chrome o\ufb04ine mode. The aim of the player is to escape all obstacles and get higher score until reaching the limitation which is 99999. The moving speed of the obstacles will increase as time goes by which make it di\ufb03cult to get the highest score. The code of this project can be found in this link which is written in Python. 1.2 Aim of the project The aim of this project is to create an agent using di\ufb00erent algorithms to play T-rex Runner and compare the performance of them. Internal covariate shift is the change of distribution in each layer of during the training which may result in longer training time especially in deep neural network. To cope with this problem, batch normalization use linear transformation on each feature to normalize the data with the same mean and variance. The same problem may also occur in deep reinforcement learning because the decision is based on neural network. Beyond the comparison of di\ufb00erent reinforcement learning algorithms, this project will also investigate the e\ufb00ect of batch normalization. The overall objectives of this project are list below. 1 CHAPTER 1. INTRODUCTION 2 \u2022 Create an agent to play T-rex Runner \u2022 Compare the di\ufb00erence among di\ufb00erent reinforcement learning algorithms \u2022 Investigate the e\ufb00ect of batch normalization in reinforcement learning 1.3 Overview This study opens with a literature review on deep learning and reinforcement learning. Each section includes the history of the \ufb01eld and the techniques related to this study. Chapter 3 includes the description of the game and the choice of algorithms according the literature review. The entire processing step will be shown as well as the architecture of the model. The design of the experiments and the evaluation methods are presented in this chapter too. Chapter 4 shows the result of all the experiments and the discussion of each experiment. Chapter 5 presents the conclusion of this study and the proposed future works. Chapter 2 Literature Survey This chapter introduces the techniques used in developing an agent to play T-rex Runner. There are two main sections which are Deep Learning and Reinforcement Learning. Brief history and some milestones will be described and some important methods will be shown in detail. 2.1 Deep Learning Deep learning is a class of Machine Learning model based on Arti\ufb01cial Neural Network (ANN). There two kinds of deep learning model which is widely used in recent years. Recurrent Neural Network is one of them which shows its power in Natural Language Processing. The other one plays an important role in deep reinforcement learning called Convolutional Neural Network (CNN). It is one of the most e\ufb00ective models for computer vision problems such as object detection and image classi\ufb01cation. This section gives a brief introduction of deep learning and detailed information about convolutional neural network. 2.1.1 History of Deep Learning An arti\ufb01cial neural network is a computation system inspired by biological neural networks which were \ufb01rst proposed by McCulloch, a neurophysiologist [24]. In 1957, Perceptron was invented by Frank [31]. Three years later, his experiments show this algorithm can recognize some of alphabets [32]. However, Marvin proved that a single layer perceptron cannot deal with XOR problem [25]. This stopped the development of ANN until Rumelhart et al. show that some useful representations can be learned with multi-layer perceptron, which is also called neural network, and backpropagation algorithm [33] in 1988. One year later, LeCun et al. \ufb01rst used a \ufb01ve-layer neural network and backpropagation to solved digit classi\ufb01cation problem and achieved great results [21]. His innovative model is known as LeNet which is the beginning of the convolutional neural network. 3 CHAPTER 2. LITERATURE SURVEY 4 The origin of CNN was proposed by Fukushima named Neocognitron which was a selforganized neural network model with multiple layers [10]. This model achieved a good result in object detection tasks because it is not position-sensitive. As mentioned before, LeCun et al. invented LeNet and got less than 1% error rate in mnist handwritten digits dataset in 1998[21]. The model used convolutions and sub-sampling which is called convolution layer and pooling layer today to convert the original images into feature vectors and perform classi\ufb01cation with fully connected layers. At the same time, some neural network models show some acceptable results in face recognition [20], speech recognition [51] and object detection [49]. But the lack of reliable theory caused the research of CNN to stagnate for many years. In the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [9], Alex and his team got 16.4% error rate with an eight-layer deep neural network (AlexNet) [19]. This was a signi\ufb01cant result compared with the one from second rank participant which was 26.2%. Beyond LeNet, AlexNet used eight layers to train the classi\ufb01er with data augmentation, dropout, ReLU, which mitigated over\ufb01tting problem. Another signi\ufb01cant discovery was that parallel computing with multiple GPUs can largely decrease the training time. Two years later, Simonyan and Zisserman introduced a sixteen-layer neural network (VGGNet) and won the \ufb01rst prize in ILSVRC 2014 classi\ufb01cation and localization tasks [41]. This model got the state-of-the-art result with 7.3% error rate at that time. VGGNet also proved that using smaller \ufb01lter size and deeper network can improve the performance of CNN. The size of all \ufb01lters in the model was no greater than 3 \u00d7 3 while the \ufb01rst two layers in AlexNet were 11 \u00d7 11 and 5 \u00d7 5. In the same year, GoogLeNet [46], the best model of ILSVRC 2014 classi\ufb01cation and detection tasks, \ufb01rst used inception which was proposed by Lin [23] to solve vanishing gradient problem. Inception replaced one node with a network which was consisted of several convolutional layers and pooling layers then concatenate them before passing to the next layer. This change made the feature selection between two layers more \ufb02exible. In other words, it can be updated by the backpropagation algorithm. Another problem of the deep neural network was degradation resulting in high training error caused by optimization di\ufb03culty. To solve that problem, He et al. proposed a deep residual learning framework (ResNet) using a residual mapping instead of stacking layers directly [12]. This model won the championship in ILSVRC 2015 with only 3.57% error rate. His experiments show that this new framework can not only solve degradation problems but also can improve the computing e\ufb03ciency. ResNet There were many variants based on ResNet such as Inception-ResNet [45] and DenseNet [14]. The former one combined improved inception techniques into ResNet. Every two convolutional layers were connected in the later model. This change mitigated vanishing gradient problems and improved the propagation of features. 2.1.2 Deep Neural Network and Activation Function A neural network or multi-layer perceptron consists of three main components: the input layer, the hidden layer, and the output layer. Each unit in one layer called a neuron. The input data are fed into the input layer conducting linear transformation through weights in the hidden layer. Finally, the result will be given non-linear ability through activation CHAPTER 2. LITERATURE SURVEY 5 function and fed into the output layer. Activation function enables the network to learn more complicated relationships between inputs and outputs. There are three widely used activation functions shown in Figure 2.1: sigmoid, tanh and ReLU. ReLU is the most commonly used one in three because it has a low computational requirement and better performance in solving vanishing gradient problems compared with the other two. tanh(x) = ex \u2212 e\u2212x ex + e\u2212x (2.1) sigmoid(x) = 1 1 + e\u2212x (2.2) ReLU(x) = max(x, 0) (2.3) Figure 2.1: Three type of activation functions. To illustrate the entire process in neural network, here is an example in Figure 2.2. Given an input data T = {(x1, y1), (x2, y2), (x3, y3), (x4, y4) | xi \u2208 Rm} and a randomly generated weight [w1, w2, w3, w4]T in the hidden layer, the output of the neural network is \u02c6y and the activation of the hidden layer is f. Therefore, the estimated value \u02c6yi can be calculated by \u02c6yi = f(wixi + b) (2.4) where b is the bias of the hidden layer and m is the number of features. If the number of hidden layers in the neural network is greater than two, this is also called Deep Neural Network (DNN). Consider a simple DNN with three hidden layers shown in Figure 2.3. Given the same input X = [x1, x2, x3, x4]T in matrix form and W i is the weight between (i\u22121)-th and i-th layer, the output of i-th layer ai can be calculated by ai = fi(zi) (2.5) where fi is the activation function between (i \u2212 1)-th and i-th layer and zi = ai\u22121W i, especially a0 = X. The dimension of those variables are shown in Table 2.1 CHAPTER 2. LITERATURE SURVEY 6 Figure 2.2: A simple neural network. Parameter Description Dimension X Input data 4 \u00d7 m W 1 Weight between input layer and hidden layer 1 m \u00d7 8 W 2 Weight between hidden layer 1 and hidden layer 2 8 \u00d7 6 W 3 Weight between hidden layer 2 and hidden layer 3 6 \u00d7 8 W 4 Weight between hidden layer 3 and output layer 8 \u00d7 1 Table 2.1: Dimension description of the deep neural network 2.1.3 Backpropagation Algorithm Section 2.1.2 introduces the way to estimated label using deep neural network. In order to optimize this estimation, a cost function J is used to quantify the di\ufb00erence between the estimated value \u02c6y and the true value y. To give a simple example, Mean Square Error [1] which is often applied to regression problems is used in this section to illustrate how the backpropagation algorithm works. Equation 2.6 shows the form of mean square error. J(w, x) = 1 2 n \ufffd i=1 (yi \u2212 \u02c6yi)2 (2.6) where x is the input data and w represent all weights used in the model. Thus, the optimization problem can be described as following min w J(w, x) (2.7) Stochastic Gradient Descent (SGD) is an e\ufb00ective way to solve this optimization problem CHAPTER 2. LITERATURE SURVEY 7 Figure 2.3: A simple deep neural network with three hidden layers. if J(w, x) is convex. However, it still shows acceptable results in deep neural network even though there is no guarantee for global optimal point in non-convex optimization [11]. Instead of \ufb01nding the optimal point directly, SGD optimize the objective function 2.7 iteratively by following equation w \u2190 w \u2212 \u03b7\u2202J(w, x) \u2202w (2.8) where \u03b7 is the learning rate which can control the update speed of the weight. Using gradient methods such as SGD to optimize the cost function in neural network is called backpropagation algorithm. Considering the deep neural network shown in 2.3 and the techniques of matrix calculus [13], the gradient of J with respect to W 4 is \u2202J \u2202W 4 = aT 3 ((a4 \u2212 y) \u2299 \u2207f4(z4)) (2.9) where \u2299 is element-wise matrix multiplication and \u2207f4(z4) is the gradient with respect to z4. Results for other W i can be calculated in a similar way. With equation 2.8, W i can be updated during each iteration by W i \u2190 W i \u2212 \u03b7 \u2202J \u2202W i (2.10) 2.1.4 Convolutional Neural Network Compared with a common deep neural network, a convolutional neural network has two extra components which are convolutional layer and pooling layer. The convolutional layers CHAPTER 2. LITERATURE SURVEY 8 make use of several trainable \ufb01lters to select di\ufb00erent features. The pooling layer reduces the dimension of the data by subsampling. In the convolution layer, the output of the last layer is convolved by trainable \ufb01lters with element-wise matrix multiplication. The size and the number of each \ufb01lter are de\ufb01ned by the user and the initial value is randomly generated. The moving step of a \ufb01lter in each convolution layer is decided by stride. In order to keep the information of the border during the forward propagation, a series of zeros attached to the border of the image called padding. Figure 2.4 shows how the result of one neuron in a convolution layer comes from and how the \ufb01lter (x, y, z, w) will be updated in every iteration in backpropagation. Figure 2.4: Operations in convolution layer. The reason for using a pooling layer is not only for dimension reduction but also for detecting invariant features including translation, rotation, scale from the input [37]. There are two types of operations in pooling layer: max pooling and average pooling. In max pooling, only the maximum value in user-de\ufb01ned windows will be chosen while all values in the window will make contributions to the output in average pooling. The choice of operation is dependent on tasks and Boureau has made a theoretical comparison between those two [6]. Both max and average operation are shown in Figure 2.5. Figure 2.5: Operations in pooling layer. A complete convolutional neural network consists of several convolutional layers, pooling CHAPTER 2. LITERATURE SURVEY 9 layers, and fully connected layers. The fully connected layer is the same concept of DNN which used the \ufb02atten vector of the last output of the other two layers as input. Considering a classi\ufb01cation problem as shown in Figure 2.6, an image with a size of 64 \u00d7 64 is fed into CNN and output a scalar which represents its class. Table 2.2 lists the \ufb01lters information used in CNN. Figure 2.6: A simple convolutional neural network. Layer Numbers Size Stride Padding Output dimension Convolution 1 8 8\u00d78 2 0 8\u00d728\u00d728 Max Pooling 8 2\u00d72 2 / 8\u00d714\u00d714 Convolution 2 16 6\u00d76 2 0 16\u00d74\u00d74 Table 2.2: Property of convolutional layer and pooling layer The backpropagation algorithm in convolutional neural network is a little di\ufb00erent from described in section 2.1.3 because of two extra layer type. In average pooling layer, the error will be divided by t \u00d7 t which is the size of the \ufb01lter and propagate to the last layer. In max pooling layer, the position of the maximum value will be stored when forward propagating and the error will be directly passed through that position. In convolutional layer, backpropagation can be calculated through basic di\ufb00erentiation. Consider the convolution operation in Figure 2.4, if the error from the output layer is \u03b4, then we have \u2202\u03b4 \u2202x = \u2202\u03b4 \u2202O11 \u2202O11 \u2202x + \u2202\u03b4 \u2202O12 \u2202O12 \u2202x + \u2202\u03b4 \u2202O21 \u2202O21 \u2202x + \u2202\u03b4 \u2202O22 \u2202O22 \u2202x (2.11) where \ufffd O11 O12 O21 O22 \ufffd (2.12) is the output matrix in Figure 2.4. The di\ufb00erentiation of \u03b4 with respect to y, z, w can be computed in a similar way. CHAPTER 2. LITERATURE SURVEY 10 2.1.5 Batch Normalization With the increasing depth of the neural network, the training time becomes longer. One of the reason is the distribution of input in each layer changes when updating the weight which is called Internal Covariate Shift. In 2015, Io\ufb00e proposed Batch Normalization (BN) which make the distribution in each layer more stable and achieve shorter training time [15]. In each neuron, the input can be normalized by Equation 2.13 \ufffdx = x \u2212 E[x] \ufffd Var[x] + \u03f5 (2.13) where \u03f5 is used to avoid zero variance. Now data in each neuron follow the distribution with mean 0 and standard deviation 1. However, this changes the representation ability of the network which may lead to the loss of information in the earlier layer. Therefore, Io\ufb00e used another linear transformation to restore that representation \u02dcx = m\ufffdx + n (2.14) where m and n are learnable parameters, especially, the result is the same as original when m = \ufffd Var[x] and n = E[x]. The mean and variance during training will be stored and will be treated as the mean of the variance of test data. In their experiment, BN can not only deal with Internal Covariate Shift problems but also mitigate vanishing gradient problems. 2.2 Reinforcement Learning Reinforcement Learning (RL) is a class of machine learning aiming at maximum the reward signal when making decisions. The basic component of reinforcement learning is the agent and the environment. As shown in Figure 2.7, the agent will receive feedback including observation and reward from the environment after each action. To generate a better policy, it will keep interacting with the environment and improve its decision-making ability step by step until the policy converges. 2.2.1 History of Reinforcement Learning In recent years, reinforcement learning becomes popular because of Alpha Go, a program that can beat human expert in Go [40]. In the Future of Go Summit 2017, Alpha Go Master shocked the world by winning all three games against Ke Jie, the world best player in Go. But the research of reinforcement learning started very early. According to Sutton, the early history of RL can be divided into two main threads [43]. One of them was optimal control. To cope with arising optimal control problems which were called \u201dmulti-stage decision processed\u201d in 1954, the theory Dynamic Programming (DP) was introduced by Bellman [5]. In the theory, he proposed the concept of \u201dfunctional equation\u201d, CHAPTER 2. LITERATURE SURVEY 11 Figure 2.7: Interaction between the agent and the environment. which was often called the Bellman equation today. Although DP was one of the most e\ufb00ective approaches to solve optimal control problems at that time, the high computational requirements which is called \u201dthe curse of dimensionality\u201d by Bellman were not easy to solve [4]. Three years later, he built a model called Markov Decision Processes (MDPs) to describe a kind of discrete deterministic processes [3]. This deterministic system and the concept of value function which is described in the Bellman equation consists of the basic theory of modern reinforcement learning. In optimal control thread, solving problems required full knowledge of the environment and it was not a feasible way to deal with most problems in the real world. The trial-and-error thread focused more on the feedback rather than the environment itself. The \ufb01rst expression about the key idea of trail-and-error including \u201dselectional\u201d and \u201dassociative\u201d called \u201dLaw of E\ufb00ect\u201d was written in Edward Thorndike\u2019s book \u201dAnimal Intelligence\u201d [47]. Although supervised learning was not \u201dselectional\u201d, some researchers still mistook it for reinforcement learning and concentrated on pattern recognition [8, 55]. This led to rare researches in actual trial-and-error learning until Klopf recognized the di\ufb00erence between supervised learning and RL: the motivation to gain more rewards from the environment [16, 17]. However, there were still some remarkable works such as the reinforcement learning rule called \u201dselective bootstrap adaptation\u201d by Widrwo in 1973 [54]. Both of two threads came across in modern reinforcement learning. Temporal Di\ufb00erence (TD) learning was a method that predicts future values depend on the current signal which originated from animal learning psychology. This idea was \ufb01rst proposed and implemented by Samuel [35]. In 1972, Klopf developed the idea of \u201dgeneralized reinforcement\u201d and linked the trial-and-error learning with animal learning psychology [16]. In 1983, Sutton developed and implemented the actor-critic architecture in trial-and-error learning based on the idea from Klopf [2]. Five years later, he proposed TD(\u03bb) algorithms which used additional step information for update policy and made TD learning a general prediction method for deterministic problems [42]. One year later, Chris used optimal control methods to solve CHAPTER 2. LITERATURE SURVEY 12 temporal-di\ufb00erence problems and developed the Q-learning algorithm which estimated delayed reward by action value function [53]. In 1994, an online Q-learning was proposed by Rummery and Niranjan which was known as SARSA [34]. The di\ufb00erence between Q-learning and SARSA was that the agent used the same policy during the learning process in SARSA while it always chooses the best action based on value function in Q-learning. With the development of the deep neural network, DeepMind proposed Deep Q-learning Network (DQN) algorithm which used a convolutional neural network to solve high dimensionality of the state in reinforcement learning problems [27]. Two years later, they modi\ufb01ed DQN by adding a target policy to improve its stability [28]. The highlight of the DQN was not only the combination of deep learning and RL but also the experience replay mechanism. To solve dependency problems when optimizing CNN, Mnih et al. stored the experiences to memory in each step and randomly sampled a mini-batch to optimize the neural network based on the idea from Lin [22]. In 2015, this mechanism was improved by measuring the importance of experience with temporal di\ufb00erence error [36]. Meanwhile, Wang proposed Dueling DQN which used an advantage function learning how valuable a state was without estimating each action value for each state [52]. This new neural network architecture was helpful when there was no strong relationship between actions and the environment. In 2016, DeepMind proposed Double DQN which show the higher stability of the policy by reducing overestimated action values [50]. Although a series of algorithms based on DQN show human-level performance on Atari games, they still failed to deal with some speci\ufb01c games. DQN was a value-based method which meant the choice of action was depend on the action values. However, choosing action randomly may be the best policy in some games such as Rock\u2212paper\u2212scissors. To deal with this problem, Sutton proposed policy gradient which enabled the agent to optimize the policy directly [44]. Based on this, OpenAI proposed a new family of algorithms such as Proximal Policy Optimization (PPO) [38]. PPO used a statistical method called importance sampling which was used to estimate a distribution by sampling data from another distribution and this simple modi\ufb01cation show a better performance in RoboschoolHumanoidFlagrun. Since the basic policy gradient method sampled data from completed episodes, the variance of the estimation was high because of the high dimension action space. Similar to valuebased method, Actor-critic method was proposed to solve this problem [18]. Compared with the policy gradient, this method used a critic to evaluate the chosen action. This made the policy can be updated after each decision which not only reduced the variance but also accelerated the convergence. The famous improved actor-critic based algorithm is asynchronous advantage actor-critic (A3C) [26]. Similar to Dueling DQN, this method used advantage function to estimate value function and performed computing in parallel which can largely increase the learning speed. 2.2.2 Markov Decision Processes As mentioned in Section 2.2.1, the interaction between the agent and the environment can be modeled as a Markov Decision Process which is based on Markov property. Markov property describes a kind of stochastic processes that the probability of next event occurring only CHAPTER 2. LITERATURE SURVEY 13 depend on the current event. De\ufb01nition 1 (Markov property [39]) Given a state St+1 at time t+1 in a \ufb01nite sequence {S0, S1, S2, \u00b7 \u00b7 \u00b7 , SN}. This sequence has Markov property, if and only if P [St+1|St] = P [St+1|S1, . . . , St] (2.15) A Markov Decision Process (MDP) is a random process with Markove property, values and decisions. De\ufb01nition 2 (Markov Decision Process [39]) A Markov Decision Process can be described as a tuple \u27e8S, A, P, R, \u03b3\u27e9 \u2022 S is a \ufb01nite set of states \u2022 A is a \ufb01nite set of actions \u2022 P is a state transition probability matrix Pa ss\u2032 = P \ufffd St+1 = s\u2032|St = s, At = a \ufffd (2.16) \u2022 R is a reward function Ra s = E [Rt+1|St = s, At = a] (2.17) \u2022 \u03b3 is a discount factor \u03b3 \u2208 [0, 1] To describe how the decision is made, a policy \u03c0 is required to de\ufb01ne the behaviour of the agent. De\ufb01nition 3 (Policy [39]) A policy is a distribution over actions given states \u03c0(a|s) = P [At = a|St = s] (2.18) In MDP, the agent is expected to get as many rewards as it can from the environment. However, maximizing the reward at time-step t makes the agent short-sighted which means it only considers the reward from the next action rather the total reward of one episode. Therefore, return is de\ufb01ned as the concept \u201dreward\u201d which the agent is expected to maximize. De\ufb01nition 4 (Return [39]) The return Gt is the total discounted reward Rt from time-step t. Gt = Rt+1 + \u03b3Rt+2 + . . . = \u221e \ufffd k=0 \u03b3kRt+k+1 (2.19) where \u03b3 \u2208 [0, 1] is the discount factor. The value of the discount factor represents how far-sighted the agent will be. If this value is 1, the agent will treat every reward in the future as the same. But this will also make the agent confused about which decision is not appropriate. At this point, the behavior of the agent can be described as in Figure 2.8 CHAPTER 2. LITERATURE SURVEY 14 Figure 2.8: Markov decision process in reinforcement learning. Since the return is de\ufb01ned in a random process, similar to reward function, the expectation of it can be de\ufb01ned as following which is also called value function. De\ufb01nition 5 (State Value Function [39]) The state-value function v\u03c0(s) of an MDP is the expected return starting from state s, and then following policy \u03c0 v\u03c0(s) = E\u03c0 [Gt|St = s] (2.20) De\ufb01nition 6 (Action Value Function [39]) The action-value function q\u03c0(s, a) is the expected return starting from state s, taking action a, and then following policy \u03c0 q\u03c0(s, a) = E\u03c0 [Gt|St = s, At = a] (2.21) With the De\ufb01nition 5, 6 and the de\ufb01nition of the expectation, we can simply write v\u03c0(s) = \ufffd a\u2208A \u03c0(a|s)q\u03c0(s, a) (2.22) where A is a set of action the agent can choose. 2.2.3 Bellman Equation Since we de\ufb01ne the Markov decision process in Section 2.2.2, the behavior of the agent can be described mathematically. As mentioned in Section 2.2.1, this problem can be solved by the Bellman equation. Theorem 1 (Bellman Expectation Equation [39]) The state-value function can be decomposed into immediate reward plus discounted value of successor state v\u03c0(s) = E\u03c0 [Rt+1 + \u03b3v\u03c0 (St+1) |St = s] (2.23) CHAPTER 2. LITERATURE SURVEY 15 The action-value function can similarly be decomposed q\u03c0(s, a) = E\u03c0 [Rt+1 + \u03b3q\u03c0 (St+1, At+1) |St = s, At = a] (2.24) Here is a simple proof for Equation 2.24. According to the De\ufb01nition 4, the return at time t can be decomposed into two parts: the immediate reward and the discounted return at time t + 1 Gt = Rt+1 + \u03b3Gt+1 (2.25) Substitute Gt with Equation 2.25 in De\ufb01nition 6 q\u03c0(s, a) = E\u03c0 [Rt+1 + \u03b3Gt+1|St = s, At = a] (2.26) Due to the linearity of expectation, Gt+1 can be replaced by q\u03c0 (St+1, At+1) and then we obtain the Bellman equation for action-value function. The state-value function can be proved in the same way. With the de\ufb01nition of optimal value function De\ufb01nition 7 (Optimal Value Function [39]) The optimal state-value function v\u2217(s) is the maximum value function over all policies v\u2217(s) = max \u03c0 v\u03c0(s) (2.27) The optimal action-value function q\u2217(s, a) is the maximum action-value function over all policies q\u2217(s, a) = max \u03c0 q\u03c0(s, a) (2.28) Theorem 1 can be extended to Bellman optimality equation Theorem 2 (Bellman Optimality Equation [39]) The optimal state-value function can be decomposed into maximum immediate reward plus discounted optimal value of successor state v\u2217(s) = max a Ra s + \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032v\u2217 \ufffd s\u2032\ufffd (2.29) The optimal action-value function can similarly be decomposed q\u2217(s, a) = Ra s + \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032 max a\u2032 q\u2217 \ufffd s\u2032, a\u2032\ufffd (2.30) where s = St, a = At, s\u2032 = St+1, a\u2032 = At+1 Here is a simple proof for Equation 2.30. Due to the linearity of expectation, Equation 2.26 can be decomposed into the expectation of the immediate reward E\u03c0 [Rt+1|St = s, At = a] (2.31) and the expectation of the discounted return at time t + 1 CHAPTER 2. LITERATURE SURVEY 16 \u03b3E\u03c0 [Gt+1|St = s, At = a] (2.32) According to the de\ufb01nition of reward function in De\ufb01nition 2, Equation 2.31 is equal to Ra s. If next state is s\u2032, Equation 2.32 can be written as following with the transition probability matrix Pa ss\u2032 \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032E\u03c0 \ufffd Gt+1|St = s, At = a, St+1 = s\u2032\ufffd (2.33) With the Markov property, we know the expectation of the return in Equation 2.33 is not related to the current state s and action a and this is equal to the state-value function. Therefore, Equation 2.33 can be written as following \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032v(s\u2032) (2.34) Considering 2.31, 2.34 and 2.22, the action-value function can be written as following q\u03c0(s, a) = Ra s + \u03b3 \ufffd s\u2032\u2208S Pa ss\u2032 \ufffd a\u2032\u2208A \u03c0 \ufffd a\u2032|s\u2032\ufffd q\u03c0 \ufffd s\u2032, a\u2032\ufffd (2.35) It is easy to prove that there is always an optimal policy for any Markov decision process and it can be found by maximizing action-value function. \u03c0\u2217(a|s) = \ufffd 1 if a = argmax a\u2208A q\u2217(s, a) 0 otherwise (2.36) Considering 2.36, Bellman optimality equation for action-value function can be obtained by replacing the policy in Equation 2.35 with optimal policy. There are many ways to solve this equation such as Sarsa and Q-learning. This will be discussed in Section 2.2.5. 2.2.4 Exploitation vs Exploration If the agent has complete knowledge of the environment, in the other word, the transition probability P a ss\u2032 can be calculated given state s and action a, Equation 2.30 can be solved by an iterative method with appropriate \u03b3. However, this method is unable to deal with an unknown environment because a large amount of information has to be collected to estimate P a ss\u2032 before the convergence of action value function. If the q function tends to be stable before the environment has been fully explored, the performance of the model would be far from satisfactory, especially in high action space situation. To deal with this problem, \u03f5 - greedy selection [43] is introduced to ensure the agent make enough exploration before the convergence of the action value function. Instead of choosing CHAPTER 2. LITERATURE SURVEY 17 the best action estimated by q function, there is a probability of \u03f5 to randomly select from all actions. The mathematical expression of this method is shown as following \u03c0(a|s) = \ufffd \u03f5/m + 1 \u2212 \u03f5 if a\u2217 = arg max a\u2208A q(s, a) \u03f5/m otherwise (2.37) where m is the number of actions. This method may have a bad e\ufb00ect on the performance of the agent at \ufb01rst several episodes during the training but it can widen the horizon of the agent in long term view. 2.2.5 Temporal Di\ufb00erence Learning As mentioned in Section 2.2.4, most environment in the real world is unknown. To solve this problem, a method called Monte Carlo (MC) is used to sample data for estimating value function. The agent can learn the environment from one episode experience and the value function can be approximated by the mean of the return instead of the expectation. The mathematical expression can be described as following v(St) = S(St) N(St) (2.38) where St is the state at time t, S(St) is the sum of return and N(St) is the counter to record the visit number of state St. There are two kinds of visit: \ufb01rst visit and every visit. The former one means the model only need to record the \ufb01rst visit of state St in one episode while all visit of St in one episode will be taken into consideration in every visit. Simplify equation 2.38, we can get the recurrence equation for v(s) v(s) \u2190 v(s) + \u03b7(Gt \u2212 v(s)) (2.39) where \u03b7 is the learning rate which can control the update speed of the value function and s is the state at time t. The problem of Monte Carlo method is all rewards in one episode have to be collected to get Gt. The value function can only be updated when reaching the end of the episode which may lead to low training e\ufb03ciency. To update value function with an incomplete episode, the return can be replaced by estimated value function using bootstrapping. With the Bellman equation 2.23 and 2.39, we can write v(s) \u2190 v(s) + \u03b7(Rt+1 + \u03b3v(s\u2032) \u2212 v(s)) (2.40) This idea is called Temporal Di\ufb00erence (TD) Learning. In TD learning, value function will be updated immediately after a new observation. Compared with MC methods, TD learning has lower variance because there are too many random actions {At+1, At+2, \u00b7 \u00b7 \u00b7 } in the Monte Carlo method which will lead to the high variance. Similarly, the recurrence equation for action value function can be written as following CHAPTER 2. LITERATURE SURVEY 18 q(s, a) \u2190 q(s, a) + \u03b1 \ufffd Rt+1 + \u03b3q(s\u2032, a\u2032) \u2212 q(s, a) \ufffd (2.41) where s and a is the state and action at time t, s\u2032 and a\u2032 is the state and action at time t+1. Equation 2.41 shows an iterative method to get the optimal action value function q\u2217(s, a). With this equation and \u03f5 - greedy policy, the RL problem can be solved by Sarsa [34]. Algorithm 1 Sarsa 1: set learning rate \u03b1, number of episodes N, explore rate \u03f5, discount factor \u03b3 2: set q(s, a) \u2190 0, \u2200s, a 3: for episode \u2190 1 to N do 4: initialize time t \u2190 0 5: get state s0 from the environment 6: choose action a0 following \u03f5 - greedy policy from q(s, a) 7: while episode is incomplete do 8: take action and get next state st+1, reward rt+1 from the environment 9: choose action at+1 following \u03f5 - greedy policy from q(s, a) 10: update q(st, at) \u2190 q(st, at) + \u03b1 \ufffd rt+1 + \u03b3q(st+1, at+1) \u2212 q(st, at) \ufffd 11: t \u2190 t + 1, st \u2190 st+1, at \u2190 at+1 12: end while 13: end for The name of Sarsa is from the sequence {S0, A0, R1, S1, A1, R2, \u00b7 \u00b7 \u00b7 }. Besides Sarsa, there is another similar algorithm called Q learning [53]. Algorithm 2 Q learning 1: set learning rate \u03b1, number of episodes N, explore rate \u03f5, discount factor \u03b3 2: set q(s, a) \u2190 0, \u2200s, a 3: for episode \u2190 1 to N do 4: initialize time t \u2190 0 5: get state s0 from the environment 6: while episode is incomplete do 7: choose action at following \u03f5 - greedy policy from q(s, a) 8: take action and get next state st+1, reward rt+1 from the environment 9: update q(st, at) \u2190 q(st, at) + \u03b1 \ufffd rt+1 + \u03b3 max a q(st+1, a) \u2212 q(st, at) \ufffd 10: t \u2190 t + 1, st \u2190 st+1 11: end while 12: end for In algorithm 2, there are two policies during the iteration. When choosing the action at+1 from q(s, a) given st+1, Sarsa uses \u03f5 - greedy policy while Q learning uses greedy policy. But both of them are choosing at with \u03f5 - greedy policy. Considering the example of Cli\ufb00 Walking shown in Figure 2.9 from Sutton\u2019s book [43], every transition in the environment will get \u22121 reward except next state is the cli\ufb00 which the agent will get \u2212100 reward, Sarsa is more likely CHAPTER 2. LITERATURE SURVEY 19 to choose the safe path while Q learning tends to choose the optimal path with \u03f5 - greedy policy. But both of them can reach the optimal policy if reducing the value of \u03f5. Figure 2.9: Example of cli\ufb00 walking from [43]. 2.2.6 Deep Q Network Q learning is a powerful algorithm to solve simple reinforcement problems. However, it is unable to deal with continuous states or continuous actions. To solve the former problem, deep learning method can be used to approximate action value function. Generally, states are image data observed by the agent and convolutional neural network is an e\ufb00ective way to extract features from this kind of data in convolution layers and feed them into the fully connected layer to approximate q function. Several consistent stationary images will be stacked into one input data to make the model understand that the agent is moving. But the input data is highly dependent, the performance of the model will be largely a\ufb00ected by the dependency. As mentioned in Section 2.2.1, DeepMind introduced experience replay pool which will store the experience into the memory and sample some of them to optimize the neural network model in 2013 [27]. Using Q learning, deep learning and experience replay pool, the improved algorithm named Deep Q Network (DQN) shows incredible performance on Atari games according to their paper. Two years later, they found the agent became more stable by using two network [28]. This algorithm can be described as below All states in Algorithm 3 have to be pre-processed before feeding into a neural network model. Based on Deep Q Network, there are three kinds of improved algorithms considering the stability of the training process, the importance of each experience and new neural network architecture. Double DQN [50] utilizes the advantage of two networks. Instead of \ufb01nding the optimal q value from target network q\u2032(s, a) directly, this method chooses the optimal action from the policy network and \ufb01nd the corresponding q value in the target network. Use the term in Algorithm 3, the change can be illustrated as following CHAPTER 2. LITERATURE SURVEY 20 Algorithm 3 Deep Q Network 1: initialize policy network q(s, a) with random weights 2: set learning rate \u03b1, number of episodes N, explore rate \u03f5, discount factor \u03b3 3: set batch size M, update step T 4: set target network q\u2032(s, a) = q(s, a) 5: for episode \u2190 1 to N do 6: initialize time t \u2190 0 7: get state s0 from the environment 8: while episode is incomplete do 9: choose action at following \u03f5 - greedy policy from policy network q(s, a) 10: take action and get next state st+1, reward rt+1 from the environment 11: store transition (st, at, st+1, rt+1) in experience replay pool 12: random sample M batch experience (sk, ak, sk+1, rk+1) from the pool 13: calculate corresponding q(sk, ak) from policy network q(s, a) 14: calculate yk using target network q\u2032(s, a) yk = \ufffd rk+1 if next state is completed rk+1 + \u03b3 max a q\u2032(sk+1, a) otherwise 15: optimize the policy model with gradient (yk \u2212 q(sk, ak))2 16: replace target network with policy network when reach the update step T 17: t \u2190 t + 1, st \u2190 st+1 18: end while 19: end for yk = \ufffd rk+1 if next state is completed rk+1 + \u03b3q\u2032(sk+1, arg max a q(sk+1, a)) otherwise (2.42) Prioritized Experience Replay (PER) introduced a way to e\ufb03ciently sample transitions from the experience replay pool [36]. Instead of uniform random sampling, there is a priority of each transition P(i) = p\u03b1 i \ufffd k p\u03b1 k (2.43) where pi > 0 is the priority of transition i and \u03b1 is the indicator of the priority, especially \u03b1 = 0 when using uniform random sampling. The priority can be measured by TD error \u03b4i, which is the following term \u03b4i = Ri + \u03b3 max a q(si, a) \u2212 q(si\u22121, ai\u22121) (2.44) Based on TD error, p(i) can be calculated in two way. The \ufb01rst is proportional prioritization which uses the absolute value of TD error CHAPTER 2. LITERATURE SURVEY 21 p(i) = |\u03b4| + \u03f5 (2.45) where \u03f5 is to avoid zero prioritization. The other one is rank-based p(i) = 1 rank(i) (2.46) where rank(i) is the rank of transition by sorting TD error \u03b4i. According to Schaul, both proportional based and rank based prioritization can speed-up the training but the later one is more robust which has better performance when meeting outliers. However, the random sampling is abandoned after adding priority mechanism which will result in high bias. In other words, those transitions with small TD error are unlikely to be sampled and the distribution is changed. Therefore, the \ufb01nal model may far from the optimal policy and performance of the agent even be lower than DQN. Important sampling (IS) [29] is an e\ufb00ective technique to estimate a distribution by sample data from a di\ufb00erent distribution. Given a probability density function p(x) over distribution D, with the de\ufb01nition of the expectation Ep [f(x)] = \ufffd D f(x)p(x)dx (2.47) where Ep [\u00b7] denotes the expectation for x \u223c p and f is the integrand. Given another probability density function q(x), the expectation can be written as following \ufffd D f(x)p(x)dx = \ufffd D f(x)p(x) q(x) q(x)dx = Eq \ufffdf(x)p(x) q(x) \ufffd (2.48) where Eq [\u00b7] denotes the expectation for x \u223c q. With Monte Carlo integration, the expectation Ep [f(x)] can be estimated by 1 N N \ufffd i=1 f(i)p(i) q(i) (2.49) where i is sampled from x. if p(i) is uniform distribution and q(i) refers to Equation 2.43, we have p(i) q(i) = 1 N \u00b7 1 P(i) (2.50) adding a tunable parameter \u03b2, we obtain the importance-sampling weights wi = (N \u00b7 P(i))\u2212\u03b2 max i wi (2.51) CHAPTER 2. LITERATURE SURVEY 22 where \u03b2 will decay from a user-de\ufb01ned initial value to 1 and the bias completely disappears when \u03b2 = 1. Term max i wi is used to normalize the weight to increase stability. Use the term in Algorithm 3, the update of q function can be modi\ufb01ed as following q(s, a) \u2190 q(s, a) + \u03b7 \u00b7 wk \u00b7 \u2207(yk \u2212 q(sk, ak))2 (2.52) where \u03b4k is TD error and \u03b7 is learning rate. Dueling DQN architecture used a new concept called advantage function which is the subtraction of the action value function and state value function [52]. A\u03c0(s, a) = q\u03c0(s, a) \u2212 v\u03c0(s) (2.53) As shown in Figure 2.10, dueling network architecture use summation of two steams which is advantage function and state value function to get the q function. The state values can be updated more accurately with this method. Figure 2.10: Architecture comparison between Dueling DQN and DQN [52] Chapter 3 Methodology This chapter gives the requirements of the project, introduces the design of reward function and shows the preprocessing steps of the input image. The choice of the model, as well as the architecture, will be discussed. Experiment design and evaluation methods will be illustrated in the last. 3.1 Requirements 3.1.1 Software Requirement Considering the readability of the code, widely used additional frameworks such as Torch, Python is a suitable choice for this project. OpenCV is used to preprocess the image getting from the environment. Numpy is a Python library which accelerates matrices operations with C. This enables the user to write e\ufb03cient scienti\ufb01c computing code with Python. There are plenty of deep learning frameworks like Tensor\ufb02ow which has many extensive API and is widely used in industrial products. However, it will take a relatively long time for the beginner to fully understand the usage of Tensor\ufb02ow. Pytorch is a recently developed framework which is described as \u201dNumpy with GPU\u201d. The simplicity of Pytorch makes more and more academic researchers using it to implement their new ideas in a much easier way. Because T-rex Runner is running on Chrome, the latest Chrome is used here. Gym is a game library developed by OpenAI [7]. This framework provides a built-in environment for some famous games such as Atari 2600 and it is easy for the user to customize their own environment. Table 3.1 shows all software requirement in this project. 3.1.2 Hardware Requirement As the game is running on Chrome, it is hard to use a Linux server to perform the experiments. Although headless Chrome is a plausible choice, there are some environmental issues during the investigation. Therefore, all experiments will be running on the laptop from the author. There will be some limitation such as 6GB GPU memory limits the size of experience replay 23 CHAPTER 3. METHODOLOGY 24 Software Description OS Windows 10 Programming language Python 3.7.4 Framework OpenCV, Pytorch, Numpy, Gym Browser Chrome 76 Table 3.1: Software requirement pool. Therefore, parameters related to hardware limitation will be suitably chosen without tuning in this project. Table 3.2 lists all hardware information used in this project. Hardware Description CPU Intel Core i5-8300H RAM 16G GPU Nvidia GTX 1060 6G Table 3.2: Hardware requirement 3.2 Game Description T-rex Runner is a dinosaur game from Google Chrome o\ufb04ine mode. Everyone can access this link on Chrome to play the game. The target for players is to control the dinosaur overcoming as many obstacles as possible. The current score of the game will increase by time if the dinosaur keeps alive as shown at the top right corner of Figure 3.1 as well as the highest score. As shown in Figure 3.2, the dinosaur has three actions to choose in every state: do nothing, jump or duck. Figure 3.1: A screenshot of T-rex Runner. Environment plays an important role in reinforcement learning because the agent will improve the policy based on the feedback from it. However, it is di\ufb03cult to quantify the rewards for each action as well as the return for an entire episode. In most research for RL algorithms, modifying reward will not be taken into consideration but it will signi\ufb01cantly impact the CHAPTER 3. METHODOLOGY 25 (a) Do nothing (b) Jump (c) Duck Figure 3.2: Three type of actions in T-rex Runner performance of the model because it decides the behavior of the agent. For example, shaping reward shows a better performance in Andrew\u2019s experiment[30]. It adds a new term F to modify the original reward based on the goal R\u2032 = R + F (3.1) The closer the agent towards the goal, the larger the F is. However, the aim of this project is to train the agent to play the game and compare the performance between di\ufb00erent algorithms. So the e\ufb00ect of reward function will not be taken into consideration and a \ufb01xed reward function will be used across all experiments. Since there is no previous study on T-rex Runner with reinforcement learning, the design of reward function is a hard part of this project. Intuitively, the best design is awarding the agent for jumping over the obstacles and penalizing it for hitting the obstacles. The jumping reward will gradually increase as time goes by. However, object detection in moving pictures is required to ful\ufb01ll this goal. As this task is out of the requirements of this project, we proposed a naive reward design as shown in Algorithm 4. Algorithm 4 Reward Design in T-rex Runner 1: if episode is completed then 2: return reward as \u22121 3: else 4: if agent choose jump then 5: return reward as 0 6: else 7: return reward as 0.1 8: end if 9: end if The basic idea of Algorithm 4 is giving a relatively small reward to the agent if it is alive and penalize it when hitting an obstacle. Zero reward for jumping is set to make the dinosaur only jumps if it is very close to obstacles. The unexpected jump will limit the movement in the next few states. CHAPTER 3. METHODOLOGY 26 Although there are three kinds of action in this game as introduced in Section 3.2, duck is optional because the agent can overcome the obstacle using jump under the same circumstances. Considering most obstacles in the game are cactus which can only be overcome by jumping, only two actions (do nothing and jump) will be used in this investigation. 3.3 Model Selection Since there are only two actions in T-rex Runner, according to the literature review on deep reinforcement learning in Section 2.2.1, value-based methods are proved to be powerful to handle this game. Although policy-based methods such as proximal policy gradient is a good choice too, only DQN, double DQN, DQN with prioritized experience replay and dueling DQN will be investigated in this project due to the time limitation. Deep Q network which is shown in Algorithm 3 is a basic reinforcement learning algorithm using deep learning. According to the result from DeepMind, it is expected to achieve at least human-level results with only DQN. Double DQN mitigates the q value overestimation problems utilizing two advantage of two networks as shown in Equation 2.42 but it is not expected to achieve a higher performance in this experiment because there is only two actions. The bad e\ufb00ect of overestimation problems is not obvious under this circumstance. Dueling DQN adds an advantage function which is the subtraction of action value function and state value function before the output layer in the convolutional neural network as shown in Equation 2.53. Since the evaluated game in [52] is a similar racing game overcoming obstacles compared with T-rex Runner, this algorithm is expected to have a better performance than DQN. Prioritized Experience replay improves training e\ufb03ciency by changing the distribution of the stored transitions. It assigns the weight for each experience by TD error. There are two ways to calculate prioritization which is proportional based method and rank-based method. According to the [36], the former one has a relatively better performance, only this method will be implemented in this investigation due to the time limitation. The performance is expected to be the same as DQN because there is no change in the algorithm but it may be faster to reach the same performance. 3.4 Image Preprocessing Following the preprocessing step in [27, 28], the raw observed image which is in RGB representation will be converted to gray-scale representation. To make the network easier to recognize dinosaur and obstacles, unnecessary objects such as clouds and scores will be removed. In this step, the color of the background and the object are reversed in order to perform erosion and dilation. These two basic morphological operations can help reduce small bright color which is often noisy data. Finally, the image is resized to 84 \u00d7 84 following the recipe from DeepMind. Since the movement should be recognized by the neural network, perform the CHAPTER 3. METHODOLOGY 27 same preprocessing step for last four frames in the history and stack those four as one data point which is also the input of CNN. The entire process is shown in Figure 3.3. Figure 3.3: Preprocessing steps for T-rex Runner. 3.5 Convolutional Neural Network Architecture There are two kinds of convolutional neural network used in this project. The basic DQN is proposed in [27, 28] which used three convolutional layers and two fully connected layers. The reason for not using pooling layer is to detect the movement of the agent. Both max pooling and average pooling may make the neural network ignore a very small change in the image. Therefore, there are only convolutional layers in this architecture. The architecture for training the agent using DQN is shown in Figure 3.4. Dueling architecture is proposed in [52] which divided the q network into two parts. One of them is only related to the state value function v(s), the other one is advantage function A(s, a) which is a\ufb00ected by both state and action. The \ufb01nal action value function is the summation of those two. q(s, a; \u03b8, \u03c91, \u03c92) = v(s; \u03b8, \u03c91) + A(s, a; \u03b8, \u03c92) (3.2) ",
    "Experiments": "Experiments 3.6.1 Hyperparameter Tuning Before the comparison of algorithms, hyperparameter tuning is required to get high-performance models. As mentioned before, the memory size is \ufb01xed to 3 \u00d7 105 due to the hardware limitation. Because there is no previous study on this game, and the hyperparameters list in [28] have a bad result on this game. All other hyperparameters have to be set to a suitable value. Grid search is performed to \ufb01nd a workable combination of those parameters. Due to the time limitation, all parameters will only be slightly modi\ufb01ed and only one hyperparameter will vary during each tuning experiment. The choice of the parameter will consider both score and stability. Each parameter will be tuned with 800 episodes. CHAPTER 3. METHODOLOGY 29 Figure 3.5: Convolutional Neural Network architecture for Dueling Deep Q Network. 3.6.2 Comparison of di\ufb00erent Deep Q Network Algorithms There are three improved reinforcement algorithms based on DQN mentioned in Section 2.2.6. Double DQN makes the performance of the agent more stable by solving the overestimated q value problem. Prioritized experience replay improves the training e\ufb03ciency by sample more valuable transitions. Dueling DQN modi\ufb01es the neural network architecture to get a better estimation of state values. In this experiment, DQN will be \ufb01rst used to train the agent based on the hyperparameters tuned in Section 3.6.1 and this result will be treated as a baseline across all the experiments. Double DQN, DQN with prioritized experience replay and Dueling DQN will be applied to the agent separately. The performance of those three is expected to be better than DQN according to the related papers. Due to time limitation, no combination of those three algorithms will be performed in this project. This section only compares the performance of each algorithm. 3.6.3 E\ufb00ect of Batch Normalization As mentioned in Section 2.1.5, it is proved that batch normalization can reduce training time and mitigate the vanishing gradient problem in a convolutional neural network. However, there is no evidence that this method has the same e\ufb00ect on reinforcement learning. This section will perform experiments on this point. Based on the experiment in Section 3.6.2, adding batch normalization in each convolutional layer and compared with the results with the outcome in previous experiments. ",
    "Evaluation": "Evaluation To evaluate the performance of the agent, DeepMind used trained agent playing the game for 30 times for up to 5 min and \u03f5 - greedy policy with \u03f5 = 0.05 [28]. Considering only one game is investigated in this project, the average score will be used instead of average reward because the number of jumps in each episode will a\ufb00ect the total reward according to the designed reward function. The greedy policy will be used in the evaluation stage instead of \u03f5 - greedy policy because the later one will bring randomness to the decision which will a\ufb00ect the performance of the trained model. Therefore, the trained agent will play the game for 30 times without time limitation and using greedy policy. All outcomes will be compared with the results from a human expert. The average scores during the training stage will be shown graphically. This is a clear way to show the learning e\ufb03ciency of each algorithm. Both graphical and statistical results such as mean, variance and median will be analyzed. However, only statistical results will be analyzed in the testing stage because the trained model for each algorithm are the same and there is no increasing trend can be shown like in the training stage. These results will be visualized with a boxplot. Chapter 4 Results and Discussion 4.1 Hyper Parameter Tuning The value of hyperparameters may a\ufb00ect the performance of the model. However, there are so many parameters in reinforcement learning including optimization algorithm parameters such as learning rate. This may take a long time to \ufb01nd the optimal combination of these parameters using a grid search. Since there is no metric like accuracy in RL which can easily re\ufb02ect the performance of the model, we assume each parameter is independent of others. Therefore, each parameter can be tuned one after another. Because the objective of this project is to compare the performance between di\ufb00erent algorithms and the e\ufb00ect of batch normalization, those tuned parameters by DQN will be used across all the experiments. The start hyperparameters of DQN are shown in Table 4.1. Hyper parameter Value Description Memory Size 3 \u00d7 105 Size of experience replay pool Batch Size 128 Size of minibatch to optimize model Gamma 0.99 Discount factor Initial \u03f50 1 \u00d7 10\u22121 Explore probability at the start of the training Final \u03f5\u2032 1 \u00d7 10\u22123 End point of explore probability in \u03f5 decay Explore steps 1 \u00d7 105 Number of steps for \u03f5 decay from \u03f50 to \u03f5\u2032 Learning Rate 1 \u00d7 10\u22124 Learning speed of the model Table 4.1: Hyper parameters used in all experiments 4.1.1 Learning Rate Learning rate controls the learning speed of the model, too large value will result in divergence and too small value may double the training time. Figure 4.1 shows four di\ufb00erent values of learning rate. Obviously, 1 \u00d7 10\u22125 is too small and there is no increase trend during the entire process. Both 1 \u00d7 10\u22124 and 5 \u00d7 10\u22125 make the 31 CHAPTER 4. RESULTS AND DISCUSSION 32 Figure 4.1: Hyper parameter tuning for learning rate. score unstable after 50th epoch. Considering the stability and 200 epochs will be trained in formal experiment, 2 \u00d7 10\u22125 will be chosen as learning rate. 4.1.2 Batch Size Batch size de\ufb01nes how many transitions will be used to update the neural network which may a\ufb00ect the training speed. But as mentioned in 2.2.1, too big size will cause the dependency problems which may largely a\ufb00ect the performance of the model. Figure 4.2: Hyper parameter tuning for batch size. As shown in Figure 4.2, the average score of three curves at epoch 80 are all around 800. Among those three, the most stable one is batch size 128. CHAPTER 4. RESULTS AND DISCUSSION 33 4.1.3 Epsilon \u03f5 - greedy policy determines the probability of exploration. In some games, especially with high action spaces, this value can a\ufb00ect how good the model will converge. However, there are only two actions in T-rex Runner so it is unnecessary to random choose action at the begin. Instead of initializing \u03f5 to 1 as DeepMind did in their paper [28], the start value is set to 0.1 in this model. Figure 4.3: Hyper parameter tuning for explore probability \u03f5. All experiments achieve acceptable results in Figure 4.3 except the one with \ufb01xed \u03f5 = 0.1. In this case, we select \u03f5 from 0.1 to 0.0001 but either of those three can be chosen according to this graph. This experiment also demonstrates the positive e\ufb00ect of linear annealing for \u03f5. 4.1.4 Explore Step Explore step is the number of steps required to anneal \u03f5 from 0.1 to 0.0001. As mentioned that hyperparameters related to exploration will not a\ufb00ect too much in this game. The most stable one will be selected from Figure 4.4 which is 1 \u00d7 105. 4.1.5 Gamma Discount factor decides how far-sighted the agent will be. Too small value will make the agent consider more about the current reward and too big value will make the agent pay the same attention to rewards after this time point. This may confuse the agent about which action leads to a high or low return. Figure 4.5 shows the average score for four di\ufb00erent gamma. Obviously, \u03b3 = 0.9 make the agent short-sighted and there is no signi\ufb01cant change during 80 epochs. When \u03b3 \u2265 0.999, the average score \ufb02uctuates widely after 50th epoch. Since \u03b3 = 0.99 has a gradually increasing trend, this will be used as the \ufb01nal discount factor. CHAPTER 4. RESULTS AND DISCUSSION 34 Figure 4.4: Hyper parameter tuning for explore steps. Figure 4.5: Hyper parameter tuning for discount factor \u03b3. 4.2 Training Results The tuned hyperparameters from the previous experiment are listed in Table 4.2. Although these parameters are tuned by DQN algorithm, they are expected to \ufb01t other three improved algorithms which are Double DQN, Dueling DQN and DQN with prioritized experience replay because there is no big di\ufb00erence among them. All algorithms will be only trained with 200 epochs because of the time limitation. The total training time for each algorithm is shown in the last column of Table 4.4 CHAPTER 4. RESULTS AND DISCUSSION 35 Hyper parameter Value before tune Value after tune Memory Size 3 \u00d7 105 3 \u00d7 105 Batch Size 128 128 Gamma 0.99 0.99 Initial \u03f50 1 \u00d7 10\u22121 1 \u00d7 10\u22121 Final \u03f5\u2032 1 \u00d7 10\u22123 1 \u00d7 10\u22124 Explore steps 1 \u00d7 105 1 \u00d7 105 Learning Rate 1 \u00d7 10\u22124 2 \u00d7 10\u22125 Table 4.2: Hyperparameters used in all experiments 4.2.1 DQN Figure 4.6 shows the result of DQN algorithm for 200 epochs with tuned parameters. A gradually increased average score can be seen from this graph. This not only proves that the agent can play the game through DQN but also shows that the design of the reward function is relatively reasonable. This result will be treated as a baseline and will be used to compare with other algorithms. Figure 4.6: Training result for DQN. 4.2.2 Double DQN Double DQN has a similar performance in training compared with DQN. As mentioned before, the e\ufb00ect of q overestimation is not so signi\ufb01cant in T-rex Runner because there are only two actions. As shown in Figure 4.7, there are four data points with average scores below 200 while all average scores are above this value in DQN. CHAPTER 4. RESULTS AND DISCUSSION 36 Figure 4.7: Training result for Double DQN compared with DQN. 4.2.3 Dueling DQN Surprisingly, dueling DQN shows an incredible training performance after 150th epoch while the curve before that time seems similar. In Figure 4.8, the average score is above 5000 which is ten times higher than the maximum average score in DQN. However, these scores have a high variance which \ufb02uctuates widely between 1000 and 5000. From the graph, the training process of dueling DQN is stable before 150th epoch and end up with an increasing trend. Since we tuned all hyperparameters based on DQN, these values may not be the best for dueling network which results in the stable and relatively low average scores before 150th epoch. Figure 4.8: Training result for Dueling DQN compared with DQN. CHAPTER 4. RESULTS AND DISCUSSION 37 4.2.4 DQN with Prioritized Experience Replay Another important \ufb01nding in this section is the performance of prioritized experience replay. This is expected to have a shorter training time and a higher performance compared with DQN. But the result shown in Figure 4.9 suggests that the agent failed to learn to play the game with this method. There are two reasons for that. Figure 4.9: Training result for DQN with prioritized experience replay compared with DQN. One problem is from the algorithm. Compared with DQN, there are two extra steps have been applied to PER: weight calculation and prioritization update. Following the implementation in [36], sum tree which is a data structure with time complexity O(log N) for sampling and updating is used to store transitions instead of a linear list to accelerate memory related manipulation. The training time of PER is twice more than the one of DQN because of the batch size. Since we know that all sampled transitions will be traversed when updating the prioritization, the larger batch size is the longer time is required to perform this operation. Table 4.3 shows that this process is very time-consuming even using the batch size 32. These data are extracted from the training results choosing the same score of 43. The step size is the average value from ten records. Algorithm Score Batch Size Step Size DQN 43 128 180 DQN with PER 43 128 7 DQN with PER 43 32 22 Table 4.3: Step size di\ufb00erence between DQN and DQN with PER The other problem is from the game. Because this game is based on Chrome, it continues running when performing optimization while the game from o\ufb03cial OpenAI Gym is paused during this operation. Therefore, there is a delayed time before sending the action to Chrome. This in\ufb02uence is enlarged in prioritized experience replay since the time for update operation CHAPTER 4. RESULTS AND DISCUSSION 38 with batch size 128 takes approximately 10 times longer than normal DQN. Change the choice of hyperparameter can mitigate the \ufb01rst problem but the result is not as good as other algorithms. One thing we can expect is PER is unable to help the agent to get a higher score under this circumstance because the game speed will increase as time goes by. Since the time for updating the prioritization will not change too much, the time interval between two consistent decisions will be longer. This may limit the performance of the model. To eliminate the high computational e\ufb00ect from updating prioritization, the best way is to redevelop the game but due to the time limitation and the primary objective of this study, this result will be used as we can still compare the e\ufb00ect of batch normalization on this algorithm. 4.2.5 Batch Normalization Since the aim of this experiment is to \ufb01nd how batch normalization a\ufb00ects DQN algorithms, each result will be compared with the one without batch normalization which is shown in Figure 4.10. Figure 4.10: Batch normalization on DQN, Double DQN, Dueling DQN and DQN with prioritized experience replay From Figure 4.10, we can see that batch normalization can increase the mean of average scores in all experiments. But this also brings high variance which makes the average score CHAPTER 4. RESULTS AND DISCUSSION 39 diverge. According to the top-left graph, the \ufb01rst time for DQN agent to reach the average 1000 is approximately 150th epoch while the agent using DQN with batch normalization reach the same average score at 60th epoch and it is easy for it to get the higher score after that time. Double DQN curve has a similar trend but batch normalization in both of them also result in wide \ufb02uctuation. It is hard to say whether dueling network bene\ufb01ts from the batch normalization because there is a signi\ufb01cant increase trend on the bottom left graph. However, it is still can be seen that BN enable the agent to reach the same performance much earlier from 20th epoch to 90th epoch. For DQN with prioritized experience replay, even the performance is limited by the game itself, the one with batch normalization still can get a relatively higher score. 4.2.6 Further Discussion As graphical results and some explanation of them are shown above, this part will discuss numerical results from the experiments. Table 4.4 shows some statistical data fro training process. The maximum score is pointless in most games but considering T-rex Runner is a racing game, we still include this in the table. The last three columns are percentile data which are calculated by sorting in ascending order and \ufb01nding the x% observation. So 50% is the same as the median. The last column shows the training time for each algorithm. Algorithm Mean Std Max 25% 50% 75% Time (h) DQN 537.50 393.61 1915 195.75 481 820 25.87 Double DQN 443.31 394.01 2366 97.75 337 662.25 21.36 Dueling DQN 839.04 1521.40 25706 155 457 956.5 35.78 DQN with PER 43.50 2.791 71 43 43 43 3.31 DQN (BN) 777.54 917.26 8978 97.75 462.5 1139.25 32.59 Double DQN (BN) 696.43 758.81 5521 79 430.5 1104.25 29.40 Dueling DQN (BN) 1050.26 1477.00 14154 84 541.5 1520 40.12 DQN with PER (BN) 46.14 7.54 98 43 43 43 3.44 Table 4.4: Training results Ignoring the result from prioritized experience replay because of the inappropriate game environment, all algorithms achieve great results according to Table 4.4. Two algorithms with dueling network stand out from them. The one with batch normalization has the mean over 1000 which is 200 more than the one without BN. But the later one got the maximum score of 25706 which means the agent can keep running for around half an hour in one episode. However, both of them have high variance which exceed the mean. Double DQN both with BN and without BN perform worse than DQN. This indicates that double DQN may reduce the performance in low dimension action space. But batch normalization shortens the gap between those two algorithms which can be seen from the median and 75% percentile. Although most of statistical metrics are improved by batch normalization, the variance is much higher than before. As shown in the table, the variance from DQN with BN is twice CHAPTER 4. RESULTS AND DISCUSSION 40 more than the one without BN. Only the variance from dueling network is lower after BN. But it is reasonable because there is an incredible increase in the very later stage of the training shown in Figure 4.10. 4.3 Testing Results After training the agent for 2000 episodes, we use the latest model with greedy policy and play T-rex Runner for 30 times with each algorithm. Figure 4.11 shows the boxplot of those results as well as the collected data from the human expert. It is obvious that the agent trained by DQN with prioritized experience replay fail to learn to play the game because of the game environment issue discussed in the last section. It is surprising that the performance of double DQN is far from satisfactory even though it has similar training results compared with DQN. Table 4.5 shows that the mean of DQN results is three times higher than the one from double DQN. Dueling DQN algorithm achieves the highest score even though it still has the highest variance which is three times more than the variance from DQN. Figure 4.11: Boxplot for test result with eight di\ufb00erent algorithms According to Table 4.5, batch normalization improves the performance of the model regardless of algorithms and even the mean of DQN with PER is increased. However, it is not easy to say the e\ufb00ect of BN in dueling DQN is positive or not. From Figure 4.11, the one without BN has more outliers which results in high variance even though its mean is higher. Consider the median which is not sensitive with the outlier data, the one with BN is better and the minimum score is more than 200 which stands out from other algorithms. Since score 43 indicates the \ufb01rst time the agent meets the obstacle, it is easy to infer that all trained model fails to jump over the \ufb01rst cacti at least once except dueling DQN with BN. But dueling DQN is not fully trained which can be seen from the training result in Figure 4.8. That\u2019s also one CHAPTER 4. RESULTS AND DISCUSSION 41 reason for high variance as we can see in the boxplot. The agent trained with dueling DQN achieved over 8000 at least three times. Algorithm Mean Std Min Max 25% 50% 75% Human 1121.9 499.91 268 2384 758 992.5 1508.5 DQN 1161.30 814.36 45 3142 321.5 1277 1729.5 Double DQN 340.93 251.40 43 942 178.75 259.5 400.75 Dueling DQN 2383.03 2703.64 44 8943 534.75 1499.5 2961 DQN with PER 43.30 1.64 43 52 43 43 43 DQN (BN) 2119.47 1595.49 44 5823 1218.75 1909.5 2979.75 Double DQN (BN) 382.17 188.74 43 738 283.75 356 525.5 Dueling DQN (BN) 2083.37 1441.50 213 5389 1142.5 1912.5 2659.75 DQN with PER (BN) 45.43 7.384 43 78 43 43 43 Table 4.5: Test results Chapter 5 Conclusions and Future Works The project aims to create an agent trained by four types of algorithms to play T-rex Runner and investigate the in\ufb02uence of batch normalization in reinforcement learning. The former aim is reached except the prioritized experience replay due to the game environment issue. However, all other algorithms are successfully implemented and achieve great results, especially DQN and dueling DQN. Both of them can achieve better results than human experts. Batch normalization has shown relatively positive e\ufb00ects for all DQN algorithms in this project despite the unstable average score in the training stage. In further studies, the game environment should be \ufb01rst redeveloped to add a pause function when the neural network is calculating q values or doing optimization. Prioritized experience replay can be tested after that. In this project, only the proportional based method has been implemented, so rank-based prioritization can also be investigated in the future. Further combination of algorithms can be developed such as dueling DQN with prioritized experience replay. Policy-based algorithms such as PPO can also be implemented to train the agent. There is one interesting idea which has not been implemented yet. Considering the moving speed of the obstacles are gradually increasing, we can divide the game into several stages. Each stage has a neural network which is initialized by the previous stage and will be trained independently. The intuition of this idea is that the consequence of jumping will change when the agent is running in di\ufb00erent stages. This may also be one of the reasons for a high variance because when the agent has learned how to get a better score in the later stage, it forgets the best policy in the early stage. 42 Bibliography [1] D. M. Allen. Mean square error of prediction as a criterion for selecting variables. Technometrics, 13(3):469\u2013475, 1971. [2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve di\ufb03cult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834\u2013846, 1983. [3] R. Bellman. A markov decision process. journal of mathematical mechanics. 1957. [4] R. Bellman. Combinatorial processes and dynamic programming. Technical report, RAND CORP SANTA MONICA CA, 1958. [5] R. Bellman et al. The theory of dynamic programming. Bulletin of the American Mathematical Society, 60(6):503\u2013515, 1954. [6] Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features for recognition. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 2559\u20132566. Citeseer, 2010. [7] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [8] W. A. Clark and B. G. Farley. Generalization of pattern recognition in a self-organizing system. In Proceedings of the March 1-3, 1955, western joint computer conference, pages 86\u201391. ACM, 1955. [9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. [10] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition una\ufb00ected by shift in position. Biological cybernetics, 36(4):193\u2013202, 1980. [11] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797\u2013842, 2015. [12] K. He, X. Zhang, S. Ren, and J. Sun. Resnet-deep residual learning for image recognition. ResNet: Deep Residual Learning for Image Recognition, 2015. 43 BIBLIOGRAPHY 44 [13] P. Hu. Matrix calculus: Derivation and simple application. Technical report, 2012. [14] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017. [15] S. Io\ufb00e and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. [16] A. H. Klopf. Brain function and adaptive systems: a heterostatic theory. Technical report, AIR FORCE CAMBRIDGE RESEARCH LABS HANSCOM AFB MA, 1972. [17] A. H. Klopf. The hedonistic neuron: a theory of memory, learning, and intelligence. Toxicology-Sci, 1982. [18] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pages 1008\u20131014, 2000. [19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. [20] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back. Face recognition: A convolutional neural-network approach. IEEE transactions on neural networks, 8(1):98\u2013113, 1997. [21] Y. LeCun, L. Bottou, Y. Bengio, P. Ha\ufb00ner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. [22] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293\u2013321, 1992. [23] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. [24] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115\u2013133, 1943. [25] M. Minsky and S. Papert. Perceptron: an introduction to computational geometry. The MIT Press, Cambridge, expanded edition, 19(88):2, 1969. [26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937, 2016. [27] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. BIBLIOGRAPHY 45 [29] R. M. Neal. Annealed importance sampling. Statistics and computing, 11(2):125\u2013139, 2001. [30] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pages 278\u2013287, 1999. [31] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957. [32] F. Rosenblatt. Perceptron simulation experiments. Proceedings of the IRE, 48(3):301\u2013 309, 1960. [33] D. E. Rumelhart, G. E. Hinton, R. J. Williams, et al. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988. [34] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, England, 1994. [35] A. J. Samuel. Aerosol dispensers and like pressurized packages, Sept. 15 1959. US Patent 2,904,229. [36] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [37] D. Scherer, A. M\u00a8uller, and S. Behnke. Evaluation of pooling operations in convolutional architectures for object recognition. In International conference on arti\ufb01cial neural networks, pages 92\u2013101. Springer, 2010. [38] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [39] D. Silver. University college london course on reinforcement learning, 2015. [40] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. [41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [42] R. S. Sutton. Learning to predict by the methods of temporal di\ufb00erences. Machine learning, 3(1):9\u201344, 1988. [43] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. [44] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000. BIBLIOGRAPHY 46 [45] C. Szegedy, S. Io\ufb00e, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on Arti\ufb01cial Intelligence, 2017. [46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015. [47] E. Thorndike. Animal intelligence; experimental studies, by edward l. thorndike, 1911. [48] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 2012. [49] R. Vaillant, C. Monrocq, and Y. Le Cun. Original approach for the localisation of objects in images. IEE Proceedings-Vision, Image and Signal Processing, 141(4):245\u2013250, 1994. [50] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double qlearning. In Thirtieth AAAI conference on arti\ufb01cial intelligence, 2016. [51] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. Phoneme recognition using time-delay neural networks. Backpropagation: Theory, Architectures and Applications, pages 35\u201361, 1995. [52] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015. [53] C. J. C. H. Watkins. Learning from delayed rewards. 1989. [54] B. Widrow, N. K. Gupta, and S. Maitra. Punish/reward: Learning with a critic in adaptive threshold systems. IEEE Transactions on Systems, Man, and Cybernetics, (5):455\u2013465, 1973. [55] B. Widrow and M. E. Ho\ufb00. Adaptive switching circuits. Technical report, Stanford Univ Ca Stanford Electronics Labs, 1960. ",
    "title": "Contents List of Figures List of Tables Introduction Literature Survey Methodology Results and Discussion Conclusions and Future Works Bibliography",
    "paper_info": "Bibliography\n[1] D. M. Allen.\nMean square error of prediction as a criterion for selecting variables.\nTechnometrics, 13(3):469\u2013475, 1971.\n[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that\ncan solve di\ufb03cult learning control problems. IEEE transactions on systems, man, and\ncybernetics, (5):834\u2013846, 1983.\n[3] R. Bellman. A markov decision process. journal of mathematical mechanics. 1957.\n[4] R. Bellman.\nCombinatorial processes and dynamic programming.\nTechnical report,\nRAND CORP SANTA MONICA CA, 1958.\n[5] R. Bellman et al.\nThe theory of dynamic programming.\nBulletin of the American\nMathematical Society, 60(6):503\u2013515, 1954.\n[6] Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features for recog-\nnition. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition, pages 2559\u20132566. Citeseer, 2010.\n[7] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and\nW. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[8] W. A. Clark and B. G. Farley. Generalization of pattern recognition in a self-organizing\nsystem. In Proceedings of the March 1-3, 1955, western joint computer conference, pages\n86\u201391. ACM, 1955.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n[10] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of\npattern recognition una\ufb00ected by shift in position. Biological cybernetics, 36(4):193\u2013202,\n1980.\n[11] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle pointsonline stochastic\ngradient for tensor decomposition. In Conference on Learning Theory, pages 797\u2013842,\n2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Resnet-deep residual learning for image recognition.\nResNet: Deep Residual Learning for Image Recognition, 2015.\n43\n",
    "GPTsummary": "- (1): The background of this article is the application of reinforcement learning and deep learning in game playing AI.\n\n- (2): Previous methods in reinforcement learning have been hindered by high dimension input problems, but deep learning solves these issues. The approach is well-motivated because it has been demonstrated to surpass human-level performance in game playing AI. \n\n- (3): The research methodology proposed is the implementation of the Deep Q network algorithm and three types of improvements to train an agent to play T-rex Runner, a popular online game. Batch normalization is also utilized to solve internal covariate shift problems in deep neural networks. \n\n- (4): The proposed methods achieved varying levels of success, with some surpassing human experts in playing the game. The results suggest that batch normalization can have a positive influence on reinforcement learning. The performance demonstrated supports the goal of improving game playing AI through the use of reinforcement learning and deep learning methods.\n7. Methods: \n\n- (1): The article aims to apply reinforcement learning and deep learning methods in game playing AI, specifically in T-rex Runner, a popular online game.\n\n- (2): To address the high dimension input problems in reinforcement learning, the article proposes the use of deep Q network algorithm and three types of improvements to train an agent to play the game. These improvements include Double DQN, Dueling DQN, and DQN with prioritized experience replay.\n\n- (3): The article also implements batch normalization to solve internal covariate shift problems in deep neural networks.\n\n- (4): The performance of the proposed methods is evaluated by using the trained agent to play T-rex Runner for 30 times without time limitation and using greedy policy. The results are compared with those of a human expert, and graphical and statistical analyses are conducted. Hyperparameters are tuned to achieve optimal performance, and the results are presented and discussed.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This article explores the application of reinforcement learning and deep learning in game playing AI, specifically in T-rex Runner. The significance of this work lies in its potential to improve game playing AI through the use of innovative methods that have been demonstrated to surpass human-level performance.\n\n- (2): In terms of innovation point, the proposed use of deep Q network algorithm and three types of improvements to overcome high dimension input problems in reinforcement learning and the implementation of batch normalization to solve internal covariate shift problems in deep neural networks are notable strengths of this article. In terms of performance, the proposed methods achieved varying levels of success, with some surpassing human experts in playing the game. However, the workload required to achieve these results is not clearly presented, which is a weakness of this article.\n\n\n",
    "GPTmethods": "- (1): The article aims to apply reinforcement learning and deep learning methods in game playing AI, specifically in T-rex Runner, a popular online game.\n\n- (2): To address the high dimension input problems in reinforcement learning, the article proposes the use of deep Q network algorithm and three types of improvements to train an agent to play the game. These improvements include Double DQN, Dueling DQN, and DQN with prioritized experience replay.\n\n- (3): The article also implements batch normalization to solve internal covariate shift problems in deep neural networks.\n\n- (4): The performance of the proposed methods is evaluated by using the trained agent to play T-rex Runner for 30 times without time limitation and using greedy policy. The results are compared with those of a human expert, and graphical and statistical analyses are conducted. Hyperparameters are tuned to achieve optimal performance, and the results are presented and discussed.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This article explores the application of reinforcement learning and deep learning in game playing AI, specifically in T-rex Runner. The significance of this work lies in its potential to improve game playing AI through the use of innovative methods that have been demonstrated to surpass human-level performance.\n\n- (2): In terms of innovation point, the proposed use of deep Q network algorithm and three types of improvements to overcome high dimension input problems in reinforcement learning and the implementation of batch normalization to solve internal covariate shift problems in deep neural networks are notable strengths of this article. In terms of performance, the proposed methods achieved varying levels of success, with some surpassing human experts in playing the game. However, the workload required to achieve these results is not clearly presented, which is a weakness of this article.\n\n\n",
    "GPTconclusion": "- (1): This article explores the application of reinforcement learning and deep learning in game playing AI, specifically in T-rex Runner. The significance of this work lies in its potential to improve game playing AI through the use of innovative methods that have been demonstrated to surpass human-level performance.\n\n- (2): In terms of innovation point, the proposed use of deep Q network algorithm and three types of improvements to overcome high dimension input problems in reinforcement learning and the implementation of batch normalization to solve internal covariate shift problems in deep neural networks are notable strengths of this article. In terms of performance, the proposed methods achieved varying levels of success, with some surpassing human experts in playing the game. However, the workload required to achieve these results is not clearly presented, which is a weakness of this article.\n\n\n"
}