[{"instruction": "Mark the title of the paper \n Follow the format of the output that follows: Title: xxx\n\n Make sure the statements as concise and academic as possible, do not have too much repetitive information, numerical values using the original numbers, be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \n line feed. ", "input": "You are a researcher in the field of [reinforcement learning] who is good at summarizing papers using concise statementsThis is the title, abstract, introduction and the first page info of an English document seperated by |: Title:\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer|Abstract:Abstract\u2014Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable ef\ufb01ciency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Ef\ufb01ciency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% ef\ufb01ciency and real rows in 100% of trialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained model on the real robot with no additional real-world \ufb01ne-tuning. Toourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhulcsr/good_robot. Index Terms\u2014Computer vision for other robotic applications, deep learning in grasping and manipulation, reinforcement learning. I. |Introduction:INTRODUCTION M ULTI-STEP robotic tasks in real-world settings are notoriously challenging to learn. They intertwine learning the immediate physical consequences of actions with the need to understand how these consequences affect progress towards the overall goal. Furthermore, in contrast to traditional motion planning, which assumes perfect information and known action models, learning only has access to the spatially and temporally limited information from sensing the environment. Manuscript received February 24, 2020; accepted July 20, 2020. Date of publication August 11, 2020; date of current version August 27, 2020. This letter was recommended for publication by Associate Editor J. Kober and Editor T. Asfour upon evaluation of the reviewers\u2019. comments. This work was supported by the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce of Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew Hundt.) Andrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, and Gregory D. Hager are with The Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu; ngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu). Chris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail: cpaxton@nvidia.com). This article has supplementary downloadable material available at https:// ieeexplore.ieee.org, provided by the authors. Digital Object Identi\ufb01er 10.1109/LRA.2020.3015448 Fig. 1. Robot-created stacks and rows of cubes with sim to real transfer. Our Schedule for Positive Task (SPOT) framework allows us to ef\ufb01ciently \ufb01nd policies which can complete multi-step tasks. Video overview: https://youtu.be/MbCuEZadkIw Our key observation is that reinforcement learning wastes signi\ufb01cant time exploring actions which are unproductive at best. For example, in a block stacking task (Fig. 1), the knowledge that grasping at empty air will never snag an object is \u201ccommon sense\u201d for humans, but may take some time for a vanilla algorithm to discover. To address this, we propose the Schedule for Positive Task (SPOT) framework, which incorporates common sense constraints in a way that signi\ufb01cantly accelerates both learning and \ufb01nal task ef\ufb01ciency. While these types of constraints are intuitive, incorporating them into Deep RL (DRL) in a manner that leads to reliable and ef\ufb01cient learning is nontrivial [1], [2]. Our methods (Section III) takeinspirationfromahumaneandeffectiveapproachtotraining pets sometimes called \u201cPositive Conditioning.\u201d Consider the goal of training a dog \u201cSpot\u201d to ignore an object or event she \ufb01nds particularly interesting on command. Spot is rewarded with treats whenever partial compliance with the desired end behavior is shown, and simply removed from regressive situations with zero treats (reward). One way to achieve this is to start with multiple treats in hand, place one treat in view of Spot, and, if she eagerly jumps at the treat (a negative action), the human snatches and hides the treat immediately for zero reward on that action. With repetition, Spot will eventually hesitate, and so she is immediately praised with \u201cGood Spot!\u201d and gets a treat 2377-3766 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  \u4ee3\u7801\u5f00\u6e90\u4e86\uff0c\u5f3a\u5316 \u7528\u7684\u662f\u57fa\u7840\u63a7\u5236\u7b56\u7565\uff08pick place push\uff09\uff0c\u4ee5\u53ca\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u3002 \u51e0\u5343\u6b21\u5c31\u53ef\u4ee5\u5b66\u4f1a\uff01 HUNDT et al.: \u201cGOOD ROBOT!\u201d: EFFICIENT REINFORCEMENT LEARNING FOR MULTI-STEP VISUAL TASKS WITH SIM TO REAL TRANSFER 6725 Fig. 2. Our model architecture. Images are pre-rotated to 16 orientations \u03b8 before being passed to the network. Every coordinate a = (\u03c6, x, y, \u03b8) in the output pixel-wise Q-Values corresponds to a \ufb01nal gripper position, orientation, and open loop action type, respectively. Purple circles highlight the highest likelihood action arg maxa(Q(s, M(a))) (8) with an arrow to the corresponding height map coordinate, showing how these values are transformed to a gripper pose. The rotated overhead views overlay the Q value at each pixel from dark blue values near 0 to red for high probabilities. If you take a moment to compare the Q values of a single object across all actions (green arrows identify the same object across two oriented views) you will see each object is scored in a way which leads to a successful stack in accordance with its surrounding context. For example, the grasp model learns to give a high score to the lone unstacked red block for grasp actions and a low score to the yellow top of the stack, while the place model does the reverse. Here the model chooses to grasp the red block and place on the yellow, blue, and green stack. Experiment details are in Sections IV and V. separate from the one she should ignore. This approach can be expanded to new situations and behaviors, and it encourages explorationandrapidimprovementonceaninitialpartialsuccess is achieved. As we describe in Section III, our reward functions and SPOT-Q Learning are likewise designed to provide neither reward nor punishment for actions that reverse progress. Instances of progress reversal are associated with varying complexity. On the one hand, failing to stack the \ufb01rst block on top of another leaves the robot in a similar situation, so recovery takes \u03a9(1) actions. However, once a stack of n blocks exists, even a successful grasp might knock the whole stack down, reversing the entire history of actions for a given trial ( Fig. 3), so recovery is \u03a9(n). The latter, more dramatic instance of progress reversal is a challenging problem for reinforcement learning of multi-step tasks in robotics; our work provides a method for ef\ufb01ciently solving such cases. In summary, our contributions in this article are: 1) The overall SPOT framework for reinforcement learning of multi-step tasks, which improves on state of the art in simulation and can train ef\ufb01ciently on real-world situations. 2) SPOT-Q Learning, a method for safe and ef\ufb01cient training in which a mask focuses exploration at runtime and generates extra on-the-\ufb02y training examples from past experience during replay. 3) State of the art zero-shot domain transfer from simulated stacking and row building tasks to their real world counterparts, as well as robustness with respect to a change in hardware and scene positions. Fig. 3. Red arrows show how individual successful actions can fail on the larger stacking task, forcing eventual progress reversal where a partial stack topples or the top must be removed. Ideally algorithms should ef\ufb01ciently learn to prevent this situation and succeed as indicated by the green arrows. Thus, temporal and workspace dependencies must be considered. Events at a current time ti \u2208 T, i \u2208 [1...n] can in\ufb02uence the likelihood of successful outcomes for past actions th|h < i and future actions tj|j > i. A successful choice of action atanygiventi willensurebothpastandfutureactionsareproductivecontributors to the larger task at hand. In our experiments a partial stack or row is itself a scene obstacle. The gray wall pictured here is for illustrative purposes only. 4) Anablationstudyshowingthat SituationRemoval dramatically decreases progress reversal; that a progress metric increases ef\ufb01ciency; and that trial rewards improve on discounting, but involve a trade-off between ef\ufb01ciency and support for sparse rewards. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  |The first page info:6724\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 5, NO. 4, OCTOBER 2020\n\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for\nMulti-Step Visual Tasks with Sim to Real Transfer\nAndrew Hundt\n, Benjamin Killeen\n, Nicholas Greene, Hongtao Wu\n, Heeyeon Kwon,\nChris Paxton\n, and Gregory D. Hager\nAbstract\u2014Current Reinforcement Learning (RL) algorithms\nstruggle with long-horizon tasks where time can be wasted explor-\ning dead ends and task progress may be easily reversed. We develop\nthe SPOT framework, which explores within action safety zones,\nlearns about unsafe regions without exploring them, and prioritizes\nexperiences that reverse earlier progress to learn with remarkable\nef\ufb01ciency. The SPOT framework successfully completes simulated\ntrials of a variety of tasks, improving a baseline trial success rate\nfrom 13% to 100% when stacking 4 cubes, from 13% to 99% when\ncreating rows of 4 cubes, and from 84% to 95% when clearing toys\narranged in adversarial patterns. Ef\ufb01ciency with respect to actions\nper trial typically improves by 30% or more, while training takes\njust 1-20 k actions, depending on the task. Furthermore, we demon-\nstrate direct sim to real transfer. We are able to create real stacks\nin 100% of trials with 61% ef\ufb01ciency and real rows in 100% of\ntrialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained\nmodel on the real robot with no additional real-world \ufb01ne-tuning.\nToourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning\nwith successful sim to real transfer applied to long term multi-step\ntasks such as block-stacking and row-making with consideration\nof progress reversal. Code is available at https://github.com/jhu-\nlcsr/good_robot.\nIndex Terms\u2014Computer vision for other robotic applications,\ndeep learning in grasping and manipulation, reinforcement\nlearning.\nI. INTRODUCTION\nM\nULTI-STEP robotic tasks in real-world settings are no-\ntoriously challenging to learn. They intertwine learning\nthe immediate physical consequences of actions with the need\nto understand how these consequences affect progress towards\nthe overall goal. Furthermore, in contrast to traditional motion\nplanning, which assumes perfect information and known action\nmodels, learning only has access to the spatially and temporally\nlimited information from sensing the environment.\nManuscript received February 24, 2020; accepted July 20, 2020. Date of\npublication August 11, 2020; date of current version August 27, 2020. This letter\nwas recommended for publication by Associate Editor J. Kober and Editor T.\nAsfour upon evaluation of the reviewers\u2019. comments. This work was supported\nby the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce\nof Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew\nHundt.)\nAndrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon\nKwon, and Gregory D. Hager are with The Johns Hopkins University,\nBaltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu;\nngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu).\nChris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail:\ncpaxton@nvidia.com).\nThis article has supplementary downloadable material available at https://\nieeexplore.ieee.org, provided by the authors.\nDigital Object Identi\ufb01er 10.1109/LRA.2020.3015448\nFig. 1.\nRo I need your help to read and summarize the following questions: ", "output": "Title: \"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer"}, {"instruction": "List all the authors' names \n Follow the format of the output that follows: Authors: xxx\n\n Make sure the statements as concise and academic as possible, do not have too much repetitive information, numerical values using the original numbers, be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \n line feed. ", "input": "You are a researcher in the field of [reinforcement learning] who is good at summarizing papers using concise statementsThis is the title, abstract, introduction and the first page info of an English document seperated by |: Title:\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer|Abstract:Abstract\u2014Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable ef\ufb01ciency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Ef\ufb01ciency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% ef\ufb01ciency and real rows in 100% of trialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained model on the real robot with no additional real-world \ufb01ne-tuning. Toourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhulcsr/good_robot. Index Terms\u2014Computer vision for other robotic applications, deep learning in grasping and manipulation, reinforcement learning. I. |Introduction:INTRODUCTION M ULTI-STEP robotic tasks in real-world settings are notoriously challenging to learn. They intertwine learning the immediate physical consequences of actions with the need to understand how these consequences affect progress towards the overall goal. Furthermore, in contrast to traditional motion planning, which assumes perfect information and known action models, learning only has access to the spatially and temporally limited information from sensing the environment. Manuscript received February 24, 2020; accepted July 20, 2020. Date of publication August 11, 2020; date of current version August 27, 2020. This letter was recommended for publication by Associate Editor J. Kober and Editor T. Asfour upon evaluation of the reviewers\u2019. comments. This work was supported by the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce of Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew Hundt.) Andrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, and Gregory D. Hager are with The Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu; ngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu). Chris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail: cpaxton@nvidia.com). This article has supplementary downloadable material available at https:// ieeexplore.ieee.org, provided by the authors. Digital Object Identi\ufb01er 10.1109/LRA.2020.3015448 Fig. 1. Robot-created stacks and rows of cubes with sim to real transfer. Our Schedule for Positive Task (SPOT) framework allows us to ef\ufb01ciently \ufb01nd policies which can complete multi-step tasks. Video overview: https://youtu.be/MbCuEZadkIw Our key observation is that reinforcement learning wastes signi\ufb01cant time exploring actions which are unproductive at best. For example, in a block stacking task (Fig. 1), the knowledge that grasping at empty air will never snag an object is \u201ccommon sense\u201d for humans, but may take some time for a vanilla algorithm to discover. To address this, we propose the Schedule for Positive Task (SPOT) framework, which incorporates common sense constraints in a way that signi\ufb01cantly accelerates both learning and \ufb01nal task ef\ufb01ciency. While these types of constraints are intuitive, incorporating them into Deep RL (DRL) in a manner that leads to reliable and ef\ufb01cient learning is nontrivial [1], [2]. Our methods (Section III) takeinspirationfromahumaneandeffectiveapproachtotraining pets sometimes called \u201cPositive Conditioning.\u201d Consider the goal of training a dog \u201cSpot\u201d to ignore an object or event she \ufb01nds particularly interesting on command. Spot is rewarded with treats whenever partial compliance with the desired end behavior is shown, and simply removed from regressive situations with zero treats (reward). One way to achieve this is to start with multiple treats in hand, place one treat in view of Spot, and, if she eagerly jumps at the treat (a negative action), the human snatches and hides the treat immediately for zero reward on that action. With repetition, Spot will eventually hesitate, and so she is immediately praised with \u201cGood Spot!\u201d and gets a treat 2377-3766 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  \u4ee3\u7801\u5f00\u6e90\u4e86\uff0c\u5f3a\u5316 \u7528\u7684\u662f\u57fa\u7840\u63a7\u5236\u7b56\u7565\uff08pick place push\uff09\uff0c\u4ee5\u53ca\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u3002 \u51e0\u5343\u6b21\u5c31\u53ef\u4ee5\u5b66\u4f1a\uff01 HUNDT et al.: \u201cGOOD ROBOT!\u201d: EFFICIENT REINFORCEMENT LEARNING FOR MULTI-STEP VISUAL TASKS WITH SIM TO REAL TRANSFER 6725 Fig. 2. Our model architecture. Images are pre-rotated to 16 orientations \u03b8 before being passed to the network. Every coordinate a = (\u03c6, x, y, \u03b8) in the output pixel-wise Q-Values corresponds to a \ufb01nal gripper position, orientation, and open loop action type, respectively. Purple circles highlight the highest likelihood action arg maxa(Q(s, M(a))) (8) with an arrow to the corresponding height map coordinate, showing how these values are transformed to a gripper pose. The rotated overhead views overlay the Q value at each pixel from dark blue values near 0 to red for high probabilities. If you take a moment to compare the Q values of a single object across all actions (green arrows identify the same object across two oriented views) you will see each object is scored in a way which leads to a successful stack in accordance with its surrounding context. For example, the grasp model learns to give a high score to the lone unstacked red block for grasp actions and a low score to the yellow top of the stack, while the place model does the reverse. Here the model chooses to grasp the red block and place on the yellow, blue, and green stack. Experiment details are in Sections IV and V. separate from the one she should ignore. This approach can be expanded to new situations and behaviors, and it encourages explorationandrapidimprovementonceaninitialpartialsuccess is achieved. As we describe in Section III, our reward functions and SPOT-Q Learning are likewise designed to provide neither reward nor punishment for actions that reverse progress. Instances of progress reversal are associated with varying complexity. On the one hand, failing to stack the \ufb01rst block on top of another leaves the robot in a similar situation, so recovery takes \u03a9(1) actions. However, once a stack of n blocks exists, even a successful grasp might knock the whole stack down, reversing the entire history of actions for a given trial ( Fig. 3), so recovery is \u03a9(n). The latter, more dramatic instance of progress reversal is a challenging problem for reinforcement learning of multi-step tasks in robotics; our work provides a method for ef\ufb01ciently solving such cases. In summary, our contributions in this article are: 1) The overall SPOT framework for reinforcement learning of multi-step tasks, which improves on state of the art in simulation and can train ef\ufb01ciently on real-world situations. 2) SPOT-Q Learning, a method for safe and ef\ufb01cient training in which a mask focuses exploration at runtime and generates extra on-the-\ufb02y training examples from past experience during replay. 3) State of the art zero-shot domain transfer from simulated stacking and row building tasks to their real world counterparts, as well as robustness with respect to a change in hardware and scene positions. Fig. 3. Red arrows show how individual successful actions can fail on the larger stacking task, forcing eventual progress reversal where a partial stack topples or the top must be removed. Ideally algorithms should ef\ufb01ciently learn to prevent this situation and succeed as indicated by the green arrows. Thus, temporal and workspace dependencies must be considered. Events at a current time ti \u2208 T, i \u2208 [1...n] can in\ufb02uence the likelihood of successful outcomes for past actions th|h < i and future actions tj|j > i. A successful choice of action atanygiventi willensurebothpastandfutureactionsareproductivecontributors to the larger task at hand. In our experiments a partial stack or row is itself a scene obstacle. The gray wall pictured here is for illustrative purposes only. 4) Anablationstudyshowingthat SituationRemoval dramatically decreases progress reversal; that a progress metric increases ef\ufb01ciency; and that trial rewards improve on discounting, but involve a trade-off between ef\ufb01ciency and support for sparse rewards. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  |The first page info:6724\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 5, NO. 4, OCTOBER 2020\n\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for\nMulti-Step Visual Tasks with Sim to Real Transfer\nAndrew Hundt\n, Benjamin Killeen\n, Nicholas Greene, Hongtao Wu\n, Heeyeon Kwon,\nChris Paxton\n, and Gregory D. Hager\nAbstract\u2014Current Reinforcement Learning (RL) algorithms\nstruggle with long-horizon tasks where time can be wasted explor-\ning dead ends and task progress may be easily reversed. We develop\nthe SPOT framework, which explores within action safety zones,\nlearns about unsafe regions without exploring them, and prioritizes\nexperiences that reverse earlier progress to learn with remarkable\nef\ufb01ciency. The SPOT framework successfully completes simulated\ntrials of a variety of tasks, improving a baseline trial success rate\nfrom 13% to 100% when stacking 4 cubes, from 13% to 99% when\ncreating rows of 4 cubes, and from 84% to 95% when clearing toys\narranged in adversarial patterns. Ef\ufb01ciency with respect to actions\nper trial typically improves by 30% or more, while training takes\njust 1-20 k actions, depending on the task. Furthermore, we demon-\nstrate direct sim to real transfer. We are able to create real stacks\nin 100% of trials with 61% ef\ufb01ciency and real rows in 100% of\ntrialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained\nmodel on the real robot with no additional real-world \ufb01ne-tuning.\nToourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning\nwith successful sim to real transfer applied to long term multi-step\ntasks such as block-stacking and row-making with consideration\nof progress reversal. Code is available at https://github.com/jhu-\nlcsr/good_robot.\nIndex Terms\u2014Computer vision for other robotic applications,\ndeep learning in grasping and manipulation, reinforcement\nlearning.\nI. INTRODUCTION\nM\nULTI-STEP robotic tasks in real-world settings are no-\ntoriously challenging to learn. They intertwine learning\nthe immediate physical consequences of actions with the need\nto understand how these consequences affect progress towards\nthe overall goal. Furthermore, in contrast to traditional motion\nplanning, which assumes perfect information and known action\nmodels, learning only has access to the spatially and temporally\nlimited information from sensing the environment.\nManuscript received February 24, 2020; accepted July 20, 2020. Date of\npublication August 11, 2020; date of current version August 27, 2020. This letter\nwas recommended for publication by Associate Editor J. Kober and Editor T.\nAsfour upon evaluation of the reviewers\u2019. comments. This work was supported\nby the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce\nof Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew\nHundt.)\nAndrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon\nKwon, and Gregory D. Hager are with The Johns Hopkins University,\nBaltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu;\nngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu).\nChris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail:\ncpaxton@nvidia.com).\nThis article has supplementary downloadable material available at https://\nieeexplore.ieee.org, provided by the authors.\nDigital Object Identi\ufb01er 10.1109/LRA.2020.3015448\nFig. 1.\nRo I need your help to read and summarize the following questions: ", "output": "Authors: Andrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, Chris Paxton, and Gregory D. Hager."}, {"instruction": "Mark the first author's affiliation \n Follow the format of the output that follows: Affiliation: xxx\n\n Make sure the statements as concise and academic as possible, do not have too much repetitive information, numerical values using the original numbers, be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \n line feed. ", "input": "You are a researcher in the field of [reinforcement learning] who is good at summarizing papers using concise statementsThis is the title, abstract, introduction and the first page info of an English document seperated by |: Title:\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer|Abstract:Abstract\u2014Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable ef\ufb01ciency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Ef\ufb01ciency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% ef\ufb01ciency and real rows in 100% of trialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained model on the real robot with no additional real-world \ufb01ne-tuning. Toourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhulcsr/good_robot. Index Terms\u2014Computer vision for other robotic applications, deep learning in grasping and manipulation, reinforcement learning. I. |Introduction:INTRODUCTION M ULTI-STEP robotic tasks in real-world settings are notoriously challenging to learn. They intertwine learning the immediate physical consequences of actions with the need to understand how these consequences affect progress towards the overall goal. Furthermore, in contrast to traditional motion planning, which assumes perfect information and known action models, learning only has access to the spatially and temporally limited information from sensing the environment. Manuscript received February 24, 2020; accepted July 20, 2020. Date of publication August 11, 2020; date of current version August 27, 2020. This letter was recommended for publication by Associate Editor J. Kober and Editor T. Asfour upon evaluation of the reviewers\u2019. comments. This work was supported by the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce of Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew Hundt.) Andrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, and Gregory D. Hager are with The Johns Hopkins University, Baltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu; ngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu). Chris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail: cpaxton@nvidia.com). This article has supplementary downloadable material available at https:// ieeexplore.ieee.org, provided by the authors. Digital Object Identi\ufb01er 10.1109/LRA.2020.3015448 Fig. 1. Robot-created stacks and rows of cubes with sim to real transfer. Our Schedule for Positive Task (SPOT) framework allows us to ef\ufb01ciently \ufb01nd policies which can complete multi-step tasks. Video overview: https://youtu.be/MbCuEZadkIw Our key observation is that reinforcement learning wastes signi\ufb01cant time exploring actions which are unproductive at best. For example, in a block stacking task (Fig. 1), the knowledge that grasping at empty air will never snag an object is \u201ccommon sense\u201d for humans, but may take some time for a vanilla algorithm to discover. To address this, we propose the Schedule for Positive Task (SPOT) framework, which incorporates common sense constraints in a way that signi\ufb01cantly accelerates both learning and \ufb01nal task ef\ufb01ciency. While these types of constraints are intuitive, incorporating them into Deep RL (DRL) in a manner that leads to reliable and ef\ufb01cient learning is nontrivial [1], [2]. Our methods (Section III) takeinspirationfromahumaneandeffectiveapproachtotraining pets sometimes called \u201cPositive Conditioning.\u201d Consider the goal of training a dog \u201cSpot\u201d to ignore an object or event she \ufb01nds particularly interesting on command. Spot is rewarded with treats whenever partial compliance with the desired end behavior is shown, and simply removed from regressive situations with zero treats (reward). One way to achieve this is to start with multiple treats in hand, place one treat in view of Spot, and, if she eagerly jumps at the treat (a negative action), the human snatches and hides the treat immediately for zero reward on that action. With repetition, Spot will eventually hesitate, and so she is immediately praised with \u201cGood Spot!\u201d and gets a treat 2377-3766 \u00a9 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  \u4ee3\u7801\u5f00\u6e90\u4e86\uff0c\u5f3a\u5316 \u7528\u7684\u662f\u57fa\u7840\u63a7\u5236\u7b56\u7565\uff08pick place push\uff09\uff0c\u4ee5\u53ca\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u3002 \u51e0\u5343\u6b21\u5c31\u53ef\u4ee5\u5b66\u4f1a\uff01 HUNDT et al.: \u201cGOOD ROBOT!\u201d: EFFICIENT REINFORCEMENT LEARNING FOR MULTI-STEP VISUAL TASKS WITH SIM TO REAL TRANSFER 6725 Fig. 2. Our model architecture. Images are pre-rotated to 16 orientations \u03b8 before being passed to the network. Every coordinate a = (\u03c6, x, y, \u03b8) in the output pixel-wise Q-Values corresponds to a \ufb01nal gripper position, orientation, and open loop action type, respectively. Purple circles highlight the highest likelihood action arg maxa(Q(s, M(a))) (8) with an arrow to the corresponding height map coordinate, showing how these values are transformed to a gripper pose. The rotated overhead views overlay the Q value at each pixel from dark blue values near 0 to red for high probabilities. If you take a moment to compare the Q values of a single object across all actions (green arrows identify the same object across two oriented views) you will see each object is scored in a way which leads to a successful stack in accordance with its surrounding context. For example, the grasp model learns to give a high score to the lone unstacked red block for grasp actions and a low score to the yellow top of the stack, while the place model does the reverse. Here the model chooses to grasp the red block and place on the yellow, blue, and green stack. Experiment details are in Sections IV and V. separate from the one she should ignore. This approach can be expanded to new situations and behaviors, and it encourages explorationandrapidimprovementonceaninitialpartialsuccess is achieved. As we describe in Section III, our reward functions and SPOT-Q Learning are likewise designed to provide neither reward nor punishment for actions that reverse progress. Instances of progress reversal are associated with varying complexity. On the one hand, failing to stack the \ufb01rst block on top of another leaves the robot in a similar situation, so recovery takes \u03a9(1) actions. However, once a stack of n blocks exists, even a successful grasp might knock the whole stack down, reversing the entire history of actions for a given trial ( Fig. 3), so recovery is \u03a9(n). The latter, more dramatic instance of progress reversal is a challenging problem for reinforcement learning of multi-step tasks in robotics; our work provides a method for ef\ufb01ciently solving such cases. In summary, our contributions in this article are: 1) The overall SPOT framework for reinforcement learning of multi-step tasks, which improves on state of the art in simulation and can train ef\ufb01ciently on real-world situations. 2) SPOT-Q Learning, a method for safe and ef\ufb01cient training in which a mask focuses exploration at runtime and generates extra on-the-\ufb02y training examples from past experience during replay. 3) State of the art zero-shot domain transfer from simulated stacking and row building tasks to their real world counterparts, as well as robustness with respect to a change in hardware and scene positions. Fig. 3. Red arrows show how individual successful actions can fail on the larger stacking task, forcing eventual progress reversal where a partial stack topples or the top must be removed. Ideally algorithms should ef\ufb01ciently learn to prevent this situation and succeed as indicated by the green arrows. Thus, temporal and workspace dependencies must be considered. Events at a current time ti \u2208 T, i \u2208 [1...n] can in\ufb02uence the likelihood of successful outcomes for past actions th|h < i and future actions tj|j > i. A successful choice of action atanygiventi willensurebothpastandfutureactionsareproductivecontributors to the larger task at hand. In our experiments a partial stack or row is itself a scene obstacle. The gray wall pictured here is for illustrative purposes only. 4) Anablationstudyshowingthat SituationRemoval dramatically decreases progress reversal; that a progress metric increases ef\ufb01ciency; and that trial rewards improve on discounting, but involve a trade-off between ef\ufb01ciency and support for sparse rewards. Authorized licensed use limited to: University of Science & Technology of China. Downloaded on July 10,2022 at 08:56:24 UTC from IEEE Xplore.  Restrictions apply.  |The first page info:6724\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 5, NO. 4, OCTOBER 2020\n\u201cGood Robot!\u201d: Ef\ufb01cient Reinforcement Learning for\nMulti-Step Visual Tasks with Sim to Real Transfer\nAndrew Hundt\n, Benjamin Killeen\n, Nicholas Greene, Hongtao Wu\n, Heeyeon Kwon,\nChris Paxton\n, and Gregory D. Hager\nAbstract\u2014Current Reinforcement Learning (RL) algorithms\nstruggle with long-horizon tasks where time can be wasted explor-\ning dead ends and task progress may be easily reversed. We develop\nthe SPOT framework, which explores within action safety zones,\nlearns about unsafe regions without exploring them, and prioritizes\nexperiences that reverse earlier progress to learn with remarkable\nef\ufb01ciency. The SPOT framework successfully completes simulated\ntrials of a variety of tasks, improving a baseline trial success rate\nfrom 13% to 100% when stacking 4 cubes, from 13% to 99% when\ncreating rows of 4 cubes, and from 84% to 95% when clearing toys\narranged in adversarial patterns. Ef\ufb01ciency with respect to actions\nper trial typically improves by 30% or more, while training takes\njust 1-20 k actions, depending on the task. Furthermore, we demon-\nstrate direct sim to real transfer. We are able to create real stacks\nin 100% of trials with 61% ef\ufb01ciency and real rows in 100% of\ntrialswith59%ef\ufb01ciencybydirectlyloadingthesimulation-trained\nmodel on the real robot with no additional real-world \ufb01ne-tuning.\nToourknowledge,thisisthe\ufb01rstinstanceofreinforcementlearning\nwith successful sim to real transfer applied to long term multi-step\ntasks such as block-stacking and row-making with consideration\nof progress reversal. Code is available at https://github.com/jhu-\nlcsr/good_robot.\nIndex Terms\u2014Computer vision for other robotic applications,\ndeep learning in grasping and manipulation, reinforcement\nlearning.\nI. INTRODUCTION\nM\nULTI-STEP robotic tasks in real-world settings are no-\ntoriously challenging to learn. They intertwine learning\nthe immediate physical consequences of actions with the need\nto understand how these consequences affect progress towards\nthe overall goal. Furthermore, in contrast to traditional motion\nplanning, which assumes perfect information and known action\nmodels, learning only has access to the spatially and temporally\nlimited information from sensing the environment.\nManuscript received February 24, 2020; accepted July 20, 2020. Date of\npublication August 11, 2020; date of current version August 27, 2020. This letter\nwas recommended for publication by Associate Editor J. Kober and Editor T.\nAsfour upon evaluation of the reviewers\u2019. comments. This work was supported\nby the NSF NRI Awards nos. 1637949 and 1763705, and in part by Of\ufb01ce\nof Naval Research Award N00014-17-1-2124. (Corresponding author: Andrew\nHundt.)\nAndrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon\nKwon, and Gregory D. Hager are with The Johns Hopkins University,\nBaltimore, MD 21218 USA (e-mail: ahundt@jhu.edu; killeen@jhu.edu;\nngreen29@jhu.edu; hwu67@jhu.edu; hkwon28@jhu.edu; hager@cs.jhu.edu).\nChris Paxton is with NVIDIA, Seattle, WA, 98105 USA (e-mail:\ncpaxton@nvidia.com).\nThis article has supplementary downloadable material available at https://\nieeexplore.ieee.org, provided by the authors.\nDigital Object Identi\ufb01er 10.1109/LRA.2020.3015448\nFig. 1.\nRo I need your help to read and summarize the following questions: ", "output": "Affiliation: The first author, Andrew Hundt, is affiliated with The Johns Hopkins University, Baltimore, MD 21218 USA."}]