{
    "Abstract": "Abstract\u2014With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inef\ufb01ciency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample ef\ufb01cient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modi\ufb01cations, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation. In this paper, we conclude the state of this exciting \ufb01eld, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve ef\ufb01cient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modi\ufb01cations of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning. Index Terms\u2014Deep reinforcement learning, distributed machine learning, self-play, population-play, toolbox. ! 1 ",
    "Introduction": "INTRODUCTION W ITH the breakthrough of AlphaGo [1], [2], an agent that wins plenty of professional Go players in human-computer gaming, deep reinforcement learning (DRL) comes to most researchers\u2019 attention, which becomes a recognized technique for solving sequential decision making problems. Plenty of algorithms are developed to solve challenging issues that lie between DRL and real world applications, such as exploration and exploitation dilemma, data inef\ufb01ciency, multi-agent cooperation and competition. Among all these challenges, data inef\ufb01ciency is the most criticized due to the trial and error learning mechanism of DRL, which requires a huge amount of interactive data. To alleviate the data inef\ufb01ciency problem, several research directions are developed [3]. For example, model based deep reinforcement learning constructs environment model for generating imaginary trajectories to help reduce times of interaction with the environment. Transfer reinforcement learning mines shared skills, roles, or patterns from source tasks, and then uses the learned knowledge to accelerate reinforcement learning in the target task. Inspired from distributed machine learning techniques, which has been successfully utilized in computer vision and natural language processing [4], distributed deep reinforcement \u2022 Qiyue Yin (qyyin@nlpr.ia.ac.cn), Tongtong Yu, Shengqi Shen, Meijing Zhao, Kaiqi Huang and Liang Wang are with Institute of Automation, Chinese Academy of Sciences, Beijing, China, 100190. \u2022 Jun Yang and Bin Liang are with the Department of Automation, Tsinghua University, Beijing, China, 100084. \u2022 Corresponding authors: kqhuang@nlpr.ia.ac.cn (Kaiqi Huang); yangjun603@tsinghua.edu.cn (Jun Yang) learning (DDRL) is developed, which has shown its potential to train very successful agents, e.g., Suphx [5], OpenAI Five [6], and AlphaStar [7]. Generally, training DRL agents consists of two main parts, i.e., pulling policy network parameters to generate data by interacting with the environment, and updating policy network parameters by consuming data. Such a structured pattern makes distributed modi\ufb01cations of DRL feasible, and plenty of DDRL algorithms have been developed. For example, the general reinforcement learning architecture [8], likely the \ufb01rst DDRL architecture, divides the training system into four components, i.e., parameter server, learners, actors and replay buffer, which inspires successive more data ef\ufb01cient DDRL architectures. The recently proposed SEED RL [9], an improved version of IMPALA [10], is claimed to be able to produce and consume millions of frames per second, based on which, AlphaStar is successfully trained within 44 days (192 v3 + 12 128 core TPUs, 1800 CPUs) for beating professional human players. To make distributed modi\ufb01cations of DRL be able to use multiple machines, several engineering problems should be solved such as machines communication and distributed storage. Fortunately, several useful toolboxes have been developed and released, and revising codes of DRL to a distributed version usually requires a small amount of code modi\ufb01cation, which largely promotes the development of DDRL. For example, Horovod [11], released by Uber, makes full use of ring allreduce technique, and can properly use multiple GPUs for training acceleration by just adding a few lines of codes compared with the single GPU version. Ray arXiv:2212.00253v1  [cs.LG]  1 Dec 2022 ",
    "Background": "BACKGROUND 2.1 Deep Reinforcement Learning Reinforcement learning is a typical kind of machine learning paradigm, the essence of which is learning via interaction. In a general reinforcement learning method, an agent interacts with an environment by posing actions to drive the environment dynamics, and receiving rewards to improve its policy for chasing long-term outcomes. To learn a good agent that can make sequential decisions, there are two typical kinds of algorithms, i.e., model-free methods that use no environment models, and model-based approaches that use the pre-given or learned environment models. Plenty of algorithms have been proposed, and readers can refer to [16], [17] for a more thorough review. In reality, applications naturally involve the participation of multiple agents, making multi-agent reinforcement learning a hot topic. Generally, multi-agent reinforcement learning is modeled as a stochastic game, and obeys similar learning paradigm with conventional reinforcement learning. Based on the game setting, agents can be fully cooperative, competitive and a mix of the two, requiring reinforcement learning agents to emerge abilities that can match the goal. Various key problems of multi-agent reinforcement learning have been raised, e.g., communication and credit assignment. Readers can refer to [18], [19] for a detailed introduction. With the breakthrough of deep learning, deep reinforcement learning becomes a strong learning paradigm by combining representation learning ability of deep learning and decision making ability of reinforcement learning, and several successful deep reinforcement learning agents have been proposed. For example, AlphaGo [1], [2], a Go agent that can beat professional human players, is based on single agent deep reinforcement learning. OpenAI Five, a dota2 agent that wins champion players in an e-sport for the \ufb01rst time, relies on multi-agent deep reinforcement learning. In the following, unless otherwise stated, we do not distinguish single agent or multiple agents deep reinforcement learning. 2.2 Distributed Learning The success of deep learning is inseparable from huge data and computing power, which leads to huge demand of distributed learning that can handle data intensive and compute intensive computing. Due to the structured computation pattern of deep learning algorithms, some successful distributed learning methods are proposed for parallelism in deep learning [20], [21]. An early popular distributed deep learning framework is DistBelief [22], designed by Google, where concepts of parameter server and A-SGD are proposed. Based on DistBelief, Google released the second generation of distributed deep learning framework, Tensor\ufb02ow [23], which becomes a widely used tool. Other typical distributed deep learning frameworks, such as PyTorch, MXNet, and Caffe2 are also developed and used by the research and industrial communities. Ben-Nun and Hoe\ufb02er [24] gave an in-depth concurrency analysis of parallel and distributed deep learning. In the survey, the authors gave different types of concurrency for deep neural networks, covering the bottom level operators, and key factors such as network inference and training. Finally, several important topics such as asynchronous stochastic optimization, distributed system architectures, communication schemes are discussed, providing clues for future directions of distributed deep learning. Nowadays, distributed learning is widely used in various \ufb01elds, such as wireless networks [25], AIoT service platform [26] and human-computer gaming [27]. In short, DDRL is a special type of distributed deep learning. Instead of focusing on data parallelism and model parallelism in conventional deep learning, DDRL aims at improving data throughput due to the characteristics of reinforcement learning. 2.3 Testing Environments With the huge success of AlphaGo [1], DDRL is widely used in games, especially human-computer gaming. Those games provide an ideal testbeds for development of DDRL algorithms or frameworks, from single player single agent DDRL to multiple players multiple agents DDRL. Atari is a popular reinforcement learning testbed because it has the similar high dimensional visual input compared to human [28]. Besides, several environments confront challenging issues such as long time horizon and JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3 sparse rewards [29]. Plenty of DDRL algorithms are compared in Atari games, showing training acceleration against DRL without parallelism. However, typical Atari games are designed for single player single agent problems. With the emerging of multi-agent reinforcement learning in multi-agent games, StarCraft Multi-Agent Challenge (SMAC) [30] becomes a recognized testbed for single player multi-agent reinforcement learning. Speci\ufb01cally, SMAC is a sub-task of StarCraft by focusing on micromanagement challenges, where a team of units is controlled to \ufb01ght against build-in opponents. Several typical multi-agent reinforcement learning algorithms are released along with SMAC, which support parallel data collection in reinforcement learning. Apart from the above single player single agent and single player multiple agents testing environments, there are a few multiple players environments for deep reinforcement learning algorithms [31]. Even though huge success has been made for games like Go, StarCraft, dota2 and honor of kings, those multiple players environments are used for a few researchers due to the huge game complexity. Overall, those multiple player single agent and multiple agents environments largely promote the development of DDRL. 3 TAXONOMY OF DISTRIBUTED DEEP REINFORCEMENT LEARNING 3.1 Taxonomic Basis Plenty of DDRL algorithms or frameworks are developed with representatives such as GORILA [8], A3C [32], APEX [33], IMPALA [10], Distributed PPO [34], R2D2 [35] and Seed RL [9], based on which, we can draw the key components of a DDRL, as shown in Fig. 1. We sometimes use the frameworks instead of algorithms or methods because these frameworks are not targeted to a speci\ufb01c reinforcement learning algorithm, and they are more like a distributed framework for various reinforcement learning methods. Generally, there are mainly three parts for a basic DDRL algorithm, which forms a single player single agent DDRL method: \u2022 Actors: produce data (trajectories or gradients) by interacting with the environment. \u2022 Learners: consume data (trajectories or gradients) to perform neural network parameters updating. \u2022 Coordinators: coordinate data (parameters or trajectories) to control the communication between learners and actors. Actors pull neural network parameters from the learners, receive states from the environments, and perform inference to obtain actions, which drive the dynamics of environments to the next states. By repeating the above process with more than one actor, data throughput can be increased and enough data can be collected. Learners pull data from actors, perform gradients calculation or postprocessing, and update the network parameters. More than one learner can alleviate the limited storage of a GPU by utilizing multiple GPUs with tools such as ring allreduce or parameter-server [11]. By repeating above process, the \ufb01nal reinforcement learning agent can be obtained. Learners Actors Data: used for actor to inference Data: used for learner to update Coordinator Coordinator Fig. 1. Basic framework of DDRL. Agents cooperation Learners Actors Coordinators Single Player Single  Agent DDRL Players evolution Single Player Multiple  Agents DDRL Multiple Player Multiple  Agents DDRL Multiple Players Single  Agent DDRL Fig. 2. Single player single agent DDRL to multiple players multiple agents DDRL. Coordinators are important for the DDRL algorithms, which control the communication between learners and actors. For example, when the coordinators are used to synchronize the parameters updating and pulling (by actors), the DDRL algorithm is synchronous. When the parameters updating and pulling (by actors) are not strictly coordinated, the DDRL algorithm is asynchronous. So a basic classi\ufb01cation of DDRL algorithms can be based on the coordinators types. \u2022 Synchronous: global policy parameters updating is synchronized, and pulling policy parameters (by actors) is synchronous, i.e., different actors share the same latest global policy. \u2022 Asynchronous: Updating the global policy parameters is asynchronous, or policy updating (by learners) and pulling (by actors) are asynchronous, i.e., actors and learners usually have different policy parameters. With the above basic framework, a single player single agent DDRL algorithm can be designed. However, when facing multiple agents or multiple players, the basic framework is unable to train usable RL agents. Based on current DDRL algorithms that support large system level AI such as AlphaStar [7], OpenAI Five [6] and JueWU [36], two key components are essential to build multiple players and multiple agents DDRL, i.e., agents cooperation and players evolution, as shown in Fig. 2. Module of agents cooperation is used to train multiple agents based on multi-agent reinforcement learning algorithms [18]. Generally, multi-agent reinforcement learning can be classi\ufb01ed into two categories, i.e., independent trainJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4 Single Player Single Agent DDRL \u203b Coordinators type \u2022 Asynchronous based \u2022 Synchronous based \u203b Players  evolution \u2022 Self-play based for all players \u2022 Population-play based for all players \u203b Agents  cooperation \u2022 Independent training for each agent \u2022 Joint training for all agents Single Player multiple Agents DDRL Multiple Players Single Agent DDRL Multiple Players Multiple Agents DDRL DDRL components Learners Actors Coordinators Fig. 3. The taxonomy of distributed deep reinforcement learning. ing and joint training, based on how to perform agents relationship modeling. \u2022 Independent training: train each agent independently by considering other learning agents as part of the environment. \u2022 Joint training: train all the agents as a whole, considering factors such as agents communication, reward assignment, and centralized training with distributed execution. Module of players evolution is designed for agents iteration for each player, where agents of other players are learning at the same time, leading to more than one generation of agents to be learned for each player like in AlphaStar, and OpenAI Five. Based on current mainstream players evolution techniques, players evolution can be divided into two types: \u2022 Self-play based: different players share the same policy networks, and the player updates the current generation of policy by confronting its past versions. \u2022 Population-play based: different players have different policy networks, or called populations, and a player updates its current generation of policy by confronting other players or/and its past versions. Finally, based on the above key components for DDRL, the taxonomy of DDRL is shown in Fig. 3. In the following, we will summarize and compare representative methods based on their main characteristics. 3.2 Coordinators Types Based on the coordinators types, DDRL algorithms can be divided into asynchronous based and synchronous based. For a asynchronous based DDRL method, there are two cases: the updating of global policy parameters is asynchronous; the global policy parameters updating (by learners) and pulling (by actors) are asynchronous. For a synchronous based DDRL method, global policy parameters updating is synchronized, and pulling policy parameters (by actors) is synchronous. Learners Parameter server Target Q network Actors Q Network-1 Environment-1 Environment-n Q network -n \u2026 \u2026 action state action state Trajectory Q network Pull model Gradient T T Fig. 4. Basic framework of Gorila. 3.2.1 Asynchronous based Nair et al. [8] proposed probably the \ufb01rst massively distributed architecture for deep reinforcement learning, Gorila, which builds the basis of the succeeding DDRL algorithms. As shown in Fig. 4, a distributed deep Q-Network (DQN) algorithm is implemented. Apart from the basic DQN algorithm that mains a Q network and a target Q network, the distribution lies in: parallel actors to generate trajectories and send them to the Q network and target Q network of the learners, and learners to calculate gradients for parameters updating based on a parameter server tool that can store a distributed neural network. The algorithm is asynchronous because neural network parameters updating of learners and trajectories collecting of actors are asynchronously performed without waiting. In their paper, the implemented distributed DQN reduces the wall-time required to achieve compared or super results by an order of magnitude on most 49 games in Atari compared to nondistributed DQN. Similar with [8], Horgan et al. [33] introduced distributed prioritized experience replay, i.e., APE-X, to enhance the Qlearning based distributed reinforcement learning. Specifically, prioritized experience replay is used to sample the most important trajectories, which are generated by all the actors. Accordingly, a shared experience replay memory should be introduced to store all the generated trajectories. In the experiments, a fraction of the wall-clock training time is achieved on the Arcade Learning Environment. To further JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5 Learners Actors Agent Global network Local network -1 Environment-1 Environment-n Local network -n \u2026 \u2026 action state action state Gradient Gradient Pull model Pull model T T Fig. 5. Basic framework of A3C. enhance [33], Kapturowski et al. [35] proposed recurrent experience replay in distributed reinforcement learning, i.e., R2D2, by introducing RNN-based reinforcement learning agents. The authors investigate the effects of parameter lag and recurrent state staleness problems on the performance, obtaining the \ufb01rst agent to exceed human-level performance in 52 of the 57 Atari games with the designed training strategy. Mnih et. al [32] proposed Asynchronous Advantage Actor-Critic (A3C) framework, which can make full use of the multi-core CPU instead of the GPU, leading to cheap distribution of reinforcement learning algorithm. As shown in Fig. 5, each actor calculates gradient of the samples (mainly states, actions and rewards used for regular reinforcement learning algorithms), send them to the learners, and then update the global policy. The updating is asynchronous without synchronization among gradients from different actors. Besides, parameters (maybe not the latest version) are pulled by each actor to generate data with environments. In their paper, four speci\ufb01c reinforcement learning algorithms are established, i.e., asynchronous one-step Qlearning, asynchronous one-step Sarsa, asynchronous n-step Q-learning and asynchronous advantage actor-critic. Experiments show that half the time on a single multi-core CPU instead of a GPU is obtained on the Atari domain. To make use of the GPU\u2019s computational power instead of just the multi-core CPU as in A3C, Babaeizadeh et al. [37] proposed asynchronous advantage actor-critic on a gpu, i.e., GA3C, which is a hybrid CPU/GPU version of the A3C. As shown in Fig. 6, the learner consists of three parts: predictor to dequeue prediction requests and obtain actions by the inference, trainer to dequeue batches of trajectories for the agent model, and the agent model to update the parameters with the trajectories. Noted that the threads of predictor and trainer are asynchronously executed. With the above multiprocess, multi-thread CPU for environments rollout and a GPU, GA3C achieves a signi\ufb01cant speed up compared to A3C. Placing gradient calculation in the actor side will limit the data throughput of the whole DDRL system, i.e., trajectories collected per time unit, so Espeholt et al. [10] proposed Importance Weighted Actor-Learner Architecture (IMPALA) to alleviate this problem. As shown in 7, parallel actors communicate with environments, collect trajectories, and send them to the learners for parameters updating. Since gradients calculation is put in the learners side, which can be accelerated with GPUs, the framework is claimed to scale to thousands of machines without sacri\ufb01cing data ef\ufb01ciency. Considering that the local policy used to generate Learners Actors \u2026 Model Global network Gradient Environment-1 Environment-n Trainers Predictors States, Rewards States State State Actions Action T T Fig. 6. Basic framework of GA3C. Learners Actors Agent Global network Local network -1 Environment-1 Environment-n Local network -n \u2026 \u2026 action state action state Trajectory Pull model Trajectory Gradient Pull model T T Fig. 7. Basic framework of impala. trajectories are behind the global policy in the learners due to the asynchrony between learner and actors, a V-trace off-policy actor-critic algorithm is introduced to correct the harmful discrepancy. Experiments on DMLab-30 and Atari57 show that IMPALA can achieve better performance with less data compared with previous agents. By using synchronized sampling strategy for actors instead of the independent sampling of IMPALA, Stooke and Abbeel [38] proposed a novel accelerated method, which consists of two main parts, i.e, synchronized sampling and synchronous/asynchronous multi-GPU optimization. As shown in 8, individual observations of each environment are gathered into a batch for inference, which largely reduce the inference times compared with approaches that generate trajectories for each environment independently. However, such synchronized sampling may suffer from slowdown when different environments in different processes have large execution differences, which is alleviated by tricks such as allocating available CPU cores used for environments evenly. As for the learners, they server as a parameter server, whose parameters are pushed by actors, and then updated asynchronously among other actors. The implemented asynchronous version of PPO, i.e., APPO, learn successful policies in Acari games in mere minutes. With the above synchronized sampling in [38], inference Learners Actors Agent Global network Local network -1 Local network -n \u2026 actions states action state Pull  model Environment-1 Environment-n \u2026 Environment-1 Environment-n \u2026 \u2026 Push  model Push  model Gradient Gradient T T Fig. 8. Basic framework of APPO. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6 Learners Actors Environment-1 Environment-n \u2026 Environment-1 Environment-n \u2026 \u2026 Model Global network Trajectory queue States Actions Actions Gradient Batches Cache T T Fig. 9. Basic framework of SEEDRL. Learners Actors \u2026 Model Global network Trajectories pool Environment-1 Environment-n Batch  states Distribute  actions T T+1 Gradient T Fig. 10. Basic framework of PAAC. times will be largely reduced, but the communication burden between learners and actors will be a big problem when the networks are huge. Espeholt et al. [9] proposed Scalable, Ef\ufb01cient, Deep-RL (SEEDRL), which features centralized inference and an optimized communication layer called gRPC. As shown in Fig. 9, the communication between learners and actors are mere states and actions, which will reduce latency with the proposed high performance RPC library gRPC. The authors implemented policy gradients and Qlearning based algorithms and tested them on the Atari-57, DeepMind Lab and Google Research Football environments, and a 40% to 80% cost reduction is obtained, showing great improvements. 3.2.2 Synchronous based As an alternative to asynchronous advantage actor-critic (A3C), Clemente et al. [39] found that a synchronous version, i.e., advantage actor-critic (A2C), can better use the GPU resources, which should perform well with more actors. In the implementation of A2C, i.e., PAAC, a coordinator is utilized to wait for all gradients of the actors before optimizing the global network. As shown in Fig. 10, learners update the policy parameters before all the trajectories are collected, i.e., the job of actors is done, and when the learners are updating the policy, the trajectory sampling is stopped. As a result, all actors are coordinated to obtain the same global network to interact with environments in the following steps. As an alternative of advantage actor-critic (A2C) algorithm in handling continuous action space, PPO algorithm [40] shows great potential due to its trust region constraint. Heess et al. [34] proposed large scale reinforcement learning with distributed PPO, i.e., DPPO, which has both synchronous and asynchronous versions, and shows better performance with the synchronous update. As shown in Fig. 11, implementation of DPPO is similar to A3C but with synchronization when updating the policy neural network. However, the synchronization will limit the throughout of Learners Actors Model Global network Local network -1 Environment-1 Environment-n Local network -n \u2026 \u2026 action state action state Gradient Gradient Parameter T T+1 T+1 \u00d7 Fig. 11. Basic framework of DPPO. Learner + Actor Learner + Actor Environment-1 Environment-n Model Global network \u2026 Model Global network Environment-1 Environment-n \u2026 Gradient Gradient action state state action state state Gradient T+1 T T Fig. 12. Basic framework of DDPPO. the whole system due to different rhythm of the actors. The authors use a threshold for the number of actors whose gradients must be available for the learners, which makes the algorithm scale to large number of actors. Different from DPPO algorithm that a server is applied for neural networks updating, Wijmans et al. [41] further proposed a decentralized DPPO framework, i.e., DDPPO, which exhibits near-liner scaling to the GPUs. As shown in Fig. 12, a learner and an actor are bundled together, as a unit, to perform trajectories collection and gradients calculation. Then gradients from all the units are gathered together through some reduce operations, e.g., ring allreduce, to update the neural networks, which make sure that parameters are the same for all the units. Noted that to alleviate the synchronization overhead when performing trajectories collection in parallel units, similar strategies like in DPPO is used to discard certain percentages of trajectories in several units. Experiments show a speedup of 107x on 128 GPUs over a serial implementation. 3.2.3 Discussion Single machine or multiple machines. In the beginning of developing DDRL algorithms, researchers make previous non-distributed DRL methods distributed using one machine. For example, the parallel of several typical actorcritic algorithms are designed to use the multi-process of CPUs, e.g., A3C [32], and PAAC [39]. Lately, researchers aim at improving data throughput of the whole DDRL system, e.g., IMPALA [10], and SEEDRL [9], which serves as a basic infrastructure for training complex games AI like AlphaStar and OpenAI Five. These systems usually can make use of multiple machines. However, early DDRL algorithms designed for a single machine can also be deployed in multiple machines when communications between machines are solved, which is relatively simple by using open soured tools (will be introduced in Section 4). Exchange trajectories or gradients. Learners and actors serve as basic components for DDRL algorithms, and the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7 data communicated between them can be trajectories or gradients based on whether to put the gradient calculation on the actor or learner side. For example, actors of A3C [32] are in charge of trajectories collection and gradients calculation, and the gradients are then sent to learners for policy update, which just make simple operations such as sum operation. Since gradients calculation is time-consuming, especially the policy model is large, the calculating load between the learners and actors will be unbalanced. Accordingly, more and more DDRL algorithms put gradients calculation in the learners side by using more suitable devices, i.e., GPUs. For example, in the higher data throughput DDRL frameworks like IMPALA [10], learners are in charge of policy updating and actors are in charge of trajectories collection. synchronized or independent inference. When actors are collecting trajectories by interacting with the environments, actions should be inferred. Basically, when performing a step on an environment, there should be one times inference. Previous DDRL methods usually maintain an environment for an actor, where action inference is performed independently from other actors and environments. With the increasing number of environments to collect trajectories, it is resource consuming especially only CPUs are used in the actor side. By putting the inference in the GPU side, the resources are also largely wasted because the batch size of the inference is one. To cope with above problems, plenty of DDRL frameworks use an actor to manage several environments, and perform actions inference synchronized. For example, APPO [38] and SEEDRL [9] introduce synchronization to collect states and distribute actions obtained by environments and actor policy, respectively. Asynchronous or synchronous DDRL. In regarding synchronous based and asynchronous based DDRL algorithms, different methods share advantages and disadvantages. For asynchronous DDRL algorithms, the global policy usually does not need to wait all the trajectories or gradients, and data collection conventionally does not need to wait the latest policy parameters. Accordingly, the data throughput of the whole DDRL system will be large. However, there exists a lag between the global policy and behavior policy, and such a lag is usually a trouble for on-policy based reinforcement learning algorithms. DDRL frameworks such as IMPALA [10] introduces V-trace, and GA3C [37] brings in small term \u03b5 to alleviate the problem, but those kinds of methods will be unstable when the lags are large. For synchronous DDRL algorithms, synchronization among trajectories or gradients is required before updating the policy. Accordingly, waiting time is wasted for actors or learners when one side is working. However, synchronization makes the training stable, and it is easier to be implemented such as DPPO [34] and DDPPO [41]. Others. Usually, multiple actors can be implemented with no data exchange, because their jobs, i.e., trajectory collection, can be independent. As for learners, most methods only maintain one learner, which will be enough due to limited model size and especially the limited trajectory batch size. However, large batch size is claimed to be important for complex games [6], and accordingly multiple learners become necessary. In the multiple learners case, usually a synchronization should be performed before updating the global policy network. Generally, a sum operation can handle the synchronization, but it is time consuming when the learners are distributed in multiple machines. An optimal choice is proposed in [41], where ring allreduce operation can nicely deal with the synchronization problem, and an implementation of [41] is easy by using toolbox such as Horovod [11]. On the other hand, when the model size is large and a GPU can not load the whole model, a parameterserver framework [8], [33] based learner can be a choice, which may be combined with the ring allreduce operation to handle the large model size and large batch size challenge. Brief summary. Finally, when a DDRL algorithm is required, how to select a proper or ef\ufb01cient method largely rely on the computing resources can be used, the policy model resource occupancy, and the environment resource occupancy. If there is only one machine with multiple CPU cores and GPUs, no extra communication is required except for the data exchange between CPU and GPUs. But, if there are multiple machines, the data exchange should be considered, which may be the bottleneck of the whole system. When the policy model is large, exchange of model between machines is time consuming, so methods such as SEEDRL [9] is proper due to only states and actions being exchanged. However, if the policy model is small, frequently exchange the trajectories will be time consuming, and methods such as DDPPO [41] will be a choice. When the environment resource occupancy is large, massive resources will be used to start-up environments, and limited GPUs maybe competent at the policy updating. Accordingly, DDRL methods such as IMPALA [10] will be suitable because a high data throughput can be obtained. Finally, there may be no best DDRL methods for any learning environments, and researchers can choose one that best suits their tasks. 3.3 Agents Cooperation Types When confronting single agent reinforcement learning, previous DDRL algorithms can be easily used. But, when there are multiple agents, distributed multi-agent reinforcement learning algorithms are required to train multiple agents simultaneously. Accordingly, previous DDRL algorithms may need to be revised to handle the multiple agents case. Based on current training paradigms for multi-agent reinforcement learning, agents cooperation types can be classi\ufb01ed into two categories, i.e., independent training and joint training, as shown in Fig. 13. Usually, an agents manager is added to control all the agents in a game. Independent training trains each agent by considering other learning agents as part of the environment, and joint training trains all the agents as a whole by using typical multi-agent reinforcement learning algorithms. 3.3.1 Independent training Independent training makes a n agents training as a training of n independent training, and accordingly previous DDRL algorithms can be used with a few modi\ufb01cations. The agents manager is mainly used to bring other agents\u2019 information, e.g., actions, into current DDRL training of an agent, because dynamic of an environment should be driven by all the agents. Considering the requirement of agents cooperation, independent training makes more contribution on how to promote cooperation among independent agents. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8 Single Player Single Agent DDRL Agents Manager Agent n Agent 1 \u2026 As a whole like a  single agent Joint  training Independent training n agents = n copy  of single agent Fig. 13. Basic framework of agents training. Jaderberg et al. [42] proposed FTW agents for Quake III Arena in Capture the Flag (CTF) mode, where several agents cooperate to \ufb01ght another camp. To train scalable agents that can cooperate with any other agents even for unseen agents, the authors train agents independently, where a population of independent agents are trained concurrently, with each participating thousands of parallel matches. To handle thousands of parallel environments, an IMPALA [10] based framework is used1. As for the cooperation problem, the authors design rewards based on several marks between the agents cooperated, so as to promote the emergence of cooperation. More speci\ufb01cally, All the agents share the same \ufb01nal global reward, i.e., win or lose. Besides, intermediate rewards are learned based on several events that considering teammates\u2019 action such as teammates capturing the \ufb02ag, and teammate picking up the \ufb02ag. Berner et al. [6] proposed OpenAI Five for Dota2, where \ufb01ve heroes cooperate together to \ufb01ght another cooperated \ufb01ve heroes. In their AI, each hero is modeled as an agent and trained independently. To deal with large parallel environments so as to generate a batch size of more than a million of time steps, a SEEDRL [9] framework is used. Unlike [42] utilizing different policy networks for different agents, OpenAI Five uses the same policy for different agents, which may promote the emergence of cooperation. The actions differences lie in the features designing, where different agents in Dota2 share almost the same features but with speci\ufb01c features such as hero ID. Finally, similar with [42] that designs rewards to promote cooperation, the authors use a weighted sum of individual and team rewards, which are given by following experience of human players, e.g., gaining resources, and killing enemies. Ye et al. [36] proposed JueWu2 for Honor of Kings, which is a similar game compared to Dota2 but played in mobile devices instead of computer devices. Like in [6], a SEEDRL [9] framework is adopted. Besides, the authors also use the same policy for different agents as in [6]. The policy network is kind of different, where \ufb01ve value heads are used due to a deeper consideration of the game characteristics. Key difference between [6] is the training paradigm used to scale to a large number of heroes, which is not the main scope of this paper, and researchers can refer to the original paper for more details. Zha et al. [43] proposed DouZero for DouDiZhu, where 1. Mainly based on their codes released. 2. A recognized name. a Landlord agent and two Peasant agents are confronting for a win. Three agents using three policy networks are trained independently like in [42]. A Gorila [8] based DDRL algorithm is used for the three agents learning in a single server. Cooperation between the Peasants agents emerges with the increasing of training epochs. Baker et al. [44] proposed multi-agent autocurricula for game hide-and-seek to study the emergent tool use. Like in [6], a SEEDRL [9] framework is used, and the same policy for different agents are used for training. Besides, the authors test using distinct policies for different agents, showing similar results but reduced sample ef\ufb01ciency. 3.3.2 Joint training Joint training trains all the agents as a whole using typical multi-agent reinforcement learning algorithms like a single agent. The difference is the trajectories collected, which have all the agents\u2019 data instead of just an agent. The agents manager can be designed to handle multi-agent issues, such as communication, and coordination, to further accelerate training. However, current multi-agent DDRL algorithms only consider a simple way, i.e., actor parallelization to collect enough trajectories. Accordingly, most previous DDRL algorithms can be easily implemented. The implementation of QMIX [45], a popular Q value factorisation based multi-agent reinforcement learning algorithm, is implemented using multi-processing to interact with the environments [46]. Another example is the RLlib [13], a part of the open source Ray project [12], which makes abstractions for DDRL and implements several jointly trained multi-agent reinforcement learning algorithms, e.g., QMIX and PPO with centralized critic. Generally speaking, joint training is similar with single agent training in the \ufb01eld of DDRL, but consideration of parallelized training for issues such as communication and coordination among agents, the training speed may further accelerated. 3.3.3 Discussion As for independent training, even though different agents are trained independently, different methods take into account problems such as the feature engineering, and reward reshaping, so as to promote cooperation. Since different agents are trained by making other agents as part of the environment, conventional DDRL algorithms can be used without many modi\ufb01cations. From the successful agents such as OpenAI Five and JueWu, we can see that SeedRL or its revised versions are a good choice. As for joint training, it is far from satisfactory, because there is a huge room to improve parallelism among agents by properly considering the multi-agent issues such as communication when designing actors and learners. 3.4 Player Evolution Types In most case, we have no opponents to drive the capacity growth for a player3. To handle such a problem, the player usually \ufb01ghts against itself to increase its ability, such as AlphaGo [1], which uses DDRL and self-play for superhuman AI learning. Based on current learning paradigms 3. Here player means the a side for a game, which may controls one agent like Go or multiple agents like Dota2 ",
    "Discussion": "Discussion Self-play has a long history in multi-agent settings, where early work explored it in genetic algorithms [49]. It becomes very popular since the success of AlphaGo series [1], [2] and then be used for AI systems such as Libratus [50], DeepStack [51] and OpenAI Five [6]. Combining DDRL, it can be used to solve very complex games. On the other side, populationplay can be seen as an advanced self-play, which maintains more players to achieve ability improvement. Current works use population-play to accelerate training, overcome gametheoretic challenges, or just handle the problem that requires distinct players. Compared with self-play, population-play is more \ufb02exible, and can handle diverse situations, whereas, self-play is easy to be implemented, and has proved its potential in complex games. So, there is no conclusion which one is better, and researchers can select self-play or population-play DDRL based on their request. 4 TYPICAL DISTRIBUTED DEEP REINFORCEMENT LEARNING TOOLBOXES DDRL is important for complex problems using reinforcement learning as solvers, and several useful toolboxes have been released to help researchers reduce development costs. In this section, we analysis several typical toolboxes, hoping to give a clue when researchers are making a selection among them. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10 4.1 Typical Toolboxes Ray [12] is a distributed framework consisting of two main parts, i.e., a system layer to implement tasks scheduling and data management, and an application layer to provide highlevel API for various applications. Using Ray, researchers can easily implement a DDRL method without considering the nodes/machines communications and how to schedule different calculations. The API is user-friendly, and by adding @ray.remote, users can obtain a remote function that can be executed in parallel. A RLLib [13] package is speci\ufb01cally introduced to handle reinforcement learning problems such as A3C, APEX and IMPALA. Furthermore, several built-in multi-agent DDRL algorithms are provided such as QMIX [45] and MADDPG [52]. Acme [53] is designed to enable distributed reinforcement learning to promote development of novel RL agents and their applications. It involves many separate (parallel) acting, learning, as well as diagnostic and helper processes, which are key building blocks for a DDRL system. One of the main contributions is the in-memory storage system, called Reverb, which is a high-throughput data system that are suitable for experience replay based reinforcement learning algorithms. With the aim of supporting agents at various scales of execution, plenty of mainstream DDRL algorithms are implemented, i.e., online reinforcement learning algorithms such as Deep Q-Networks [28], R2D2 [35] and IMPALA [10], of\ufb02ine reinforcement learning such as behavior cloning and TD3 [54], imitation learning such as adversarial imitation learning [55] and soft Q imitation learning [56]. Tianshou [57] is a highly modularized Python library that uses PyTorch for DDRL. Its main characteristic is the design of building blocks that support more than 20 classic reinforcement learning algorithms with distributed version through a uni\ufb01ed interface. Since Tianshou focuses on smallto medium-scale applications of DDRL with only parallel sampling, it is a lightweight platform that is researchfriendly. It is claimed that Tianshou is easy to install, and users can apply Pip or Conda to accomplish installation on platforms covering Windows, macOS and Linux. TorchBeast [58] is another DDRL toolbox that bases on Pytorch to support for fast, asynchronous and parallel training of reinforcement learning agents. The authors provide two versions, i.e., a pure-Python MonoBeast and a multimachine high-performance PolyBeast with several parts being implemented with C++. Users only require Python and Pytorch to implement DDRL algorithms. In the toolbox, IMPALA is supported and tested with the classic Atari suite. MALib [59] is a scalable and ef\ufb01cient computing framework for population-based multi-agent reinforcement learning algorithms. Using a centralized task dispatching model, it supports self-generated tasks and heterogeneous policy combinations. Besides, by abstracting DDRL algorithms using Actor-Evaluator-Learner, a higher parallelism for learning and sampling is achieved. The authors also claimed to have an ef\ufb01cient code reuse and \ufb02exible deployments due to the higher-level abstractions of multi-agent reinforcement learning. In the released code, several popular reinforcement learning environments such as Google Research Football and SMAC are supported and typical population based algorithms such as policy space response oracle (PSRO) [60] and Pipeline-PSRO [61] are implemented. SeedRL [9] is a scalable and ef\ufb01cient deep reinforcement learning toolbox, as described in Section 3.2.1. Generally, it is veri\ufb01ed on the tensor processing unit (TPU) device, which is a special chip customized by Google for machine learning. Typical DDRL algorithms are implemented, e.g., IMPALA [10], and R2D2 [35], which are tested on four classical environments, i.e., Arati, DeepMind lab, Google research football and Mujoco. Distributed training is supported using cloud machine learning engine of Google. 4.2 Discussions Before comparing different kinks of toolboxes, we want to claim that there are no best DDRL toolboxes for any requirements, but the most suitable one depending on speci\ufb01c goals. Tianshou and TorchBeast are lightweight platforms that support several typical DDRL algorithms. Users can easily use and revise the released codes for developing reinforcement learning algorithms with the PyTorch deep learning library. The user-friendly features make these toolboxes popular. However, even though those toolboxes are highly modularized, the scalability to large number of machines for performing large learner parallel and actor parallel are not tested, and bottleneck may appear with increasing number of machines. Ray, Acme and SeedRL are relatively large toolboxes that can theoretically support any DDRL algorithms with certain modi\ufb01cations. Using their open projects, users can utilize multiple machines to implement high data throughput DDRL algorithms. Moreover, multiple agents training, and multiple players evolution can be achieved, such as for AlphaStar. However, the modi\ufb01cations are not easy when revising the DDRL algorithms due to the code nesting, especially for complex functions such as self-play and population-play. MALib is similar with Ray, Acme and SeedRL, which is a specially designed DDRL toolbox for population-based multi-agent reinforcement learning. With their APIs, users may easily implement population based multi-agent reinforcement learning algorithms such as \ufb01ctitious self-play [62] and PSRO. Even though experiments for large number of machines are not tested, this toolbox is fully functional (APIs provided) for various requirements of DDRL algorithms from single player single agent DDRL to multiple players multiple agents DDRL. In summary, current DDRL toolboxes provide a good support for DDRL algorithms, and several typical testing environments are applied for performance validation. However, those DDRL toolboxes are either lightweight or heavy, and not tested for complex games. In the following, we will design a new toolbox, which focuses on multiple players and multiple agents DDRL training on complex games. 5 A MULTI-PLAYER MULTI-AGENT REINFORCEMENT LEARNING TOOLBOX In this section, we open a multi-player multi-agent reinforcement learning toolbox, M2RL, to support populations of players (with each may control several agents) for complex games, e.g., Wargame [63]. Noted that this project is on JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 Players Manager Player-1 Player-2 Player-N Player-1 Population Player-2 Population Player-N Population Experience  Buffer Learner Actor Env My agent Another  player\u2019s  agent Experience  Buffer Learner Actor Env My agent Another  player\u2019s  agent Experience  Buffer Learner Actor Env My agent Another  player\u2019s  agent \u2026 \u2026 \u2026 \u2026 Evaluation->Player-X vs (Player-1,\u2026\u2026,Player-N) Choosing Algorithm-> Player-X\u2019s opponent chosen probabilities My agent My agent My agent My agent My agent My agent Fig. 15. Basic framework of the proposed multi-player multi-agent reinforcement learning toolbox M2RL. Player1 Step Sampling Step Sampling Players Manager Evaluate Manager Payoff Data All Players Data Choosen Player Data Player test Opponent Choosing Actor Step Trajectories Actor Actor Experience Buffer Batch Sampling Learner Unfinished Buffer Finished Buffer Env Env Env Our Obs Opponent Obs Reward Engineering observation Parameters Model Train Batch Trajectories Parameter Server Parameters Pulling action Step Sampling Agent Agent Agent action observation Step Trajectories Parameters Parameter  Buffer Save/Load Player Player Data All Experience Win Experience Episode  Trajectories Player Data Mask Engineering Feature Engineering Model Inference Action Engineering Step Counter Probability/Value mask feature Opponent Actor Model Inference feature +mask Agent Agent Agent Probability /Value Player2 Save/Load Player Player Data PlayerN Save/Load Player (a) (b) Fig. 16. Speci\ufb01c details of the proposed multi-player multi-agent reinforcement learning toolbox M2RL. going, so the main purpose is a preliminary introduction, and we will continue to improve this project. Hyperlink of the project is m2rl-V1.0. 5.1 Overall Framework The overall framework is shown in Fig 15. Each player, consisting one or multiple agents, has three key components: learner, actor and experience buffer. The multiple concurrently executed actors produce data for learners, which use the current player and other players as opponents based on the choice of players manager. The experience buffer is used to store trajectories of the player to support asynchronous or synchronous training. The learner for each player is used to update parameters of the player and send parameters to the actors. Apart from the above basic factors, Players manager maintains self-play and population-play, which has two key parts: evaluating players and choosing opponent players for each player. More speci\ufb01c details of M2RL is shown in Fig 16. To make M2RL easy to be used for complex games, we design each parts in a relatively \ufb02exible manner. \u2022 The players manager evaluates all saved players (including their past versions) using their confrontation results, based on which, various opponents selection JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12 300 500 700 900 1100 1300 1500 1700 5.5 16.5 27.5 38.5 49.5 60.5 71.5 82.5 93.5 104.5 Elo Rating Training Hours Elo_Red M2RL Demo Knowledge_1 Knowledge_2 Knowledge_3 300 500 700 900 1100 1300 1500 1700 5.5 16.5 27.5 38.5 49.5 60.5 71.5 82.5 93.5 104.5 Elo Rating Training Hours Elo_Blue M2RL Demo Knowledge_1 Knowledge_2 Knowledge_3 Fig. 17. Elo results of the trained AI bots (red and blue players) based on M2RL. Knowledge 1, Knowledge 2 and Knowledge 3 are three professional level AI bots. Demo is an AI with a strategy to select the highest priority action when it is possible. methods can be implemented to promote players evolution, e.g., revised self-play in OpenAI Five [6], and prioritized \ufb01ctitious self-play in AlphaStar [7]. \u2022 Each player maintains its own learner, actor and experience buffer, making distinct players training possible, e.g., red and blue players in Wargame [63]. Considering that the game is complex with different observation and action spaces compared to OpenAI gym, feature engineering and mask engineering are used in the framework. Besides, experience buffer is revised to change un\ufb01nished buffer to \ufb01nished buffer, which is very useful for asynchronous multiagent cooperation [64]. \u2022 All the codes are based on the user-friendly framework Ray, which is easy to be deployed, revised and used. More speci\ufb01cally, we can make full use of computing resources by segmenting a GPU to several parts and assigning each part to different tasks, which is important for complex games under limited computing resources. 5.2 A Case Wargame, a complex game like Dota2 and StarCraft, is a popular testing environment for verifying arti\ufb01cial intelligence [27]. In a Wargame map4, the red player controls several \ufb01ghting units to confront the blue player who also controls several units. The game is asymmetric because players have distinct strategies space, and usually the blue player has more forces, while the red player has vision advantage. Please refer to [63] for more details of the Wargame. We can naturally model Wargame as a two players multiple agents problem, where each \ufb01ghting unit is regarded as an agent. To train two AI bots for the red and blue players, respectively, we use several widely adopted settings like in OpenAI Five [6], JueWu [36] and AlphaStar [7], e.g., shared PPO policy for each agent, dual-clip for the PPO, and prioritized \ufb01ctitious self-play. Each player trains its bot using about 200,000 games, and uses 9,500 games for the players manager to evaluate each generation of the player. The computing resources used here are: 2\u00d7Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz, 4\u00d7 NVIDIA GeForce 4. wargame.ia.ac.cn, ID=2010431153 RTX 2080 Ti, and 500GB memory. With above resources, the training lasts for \ufb01ve days, and we \ufb01nally obtain 20 generations for each player. To evaluate the performance of these bots, we use the build-in demo agent as baseline, and bring in three professional level AI bots designed by teams who have studied Wargame for several years, represented as Knowledge 1, Knowledge 2, and Knowledge 3, respectively. It should be noted that those professional AI bots do not participated in training. Similar with the evaluation for AlphaGo [1] and AlphaStar [7], we use Elo as metrics. The results are shown in Fig 17. From Fig 17, it can be seen that with the increasing of players evolution, the learned policy for each player is becoming stronger. Since Wargame is a complex game and previous toolboxes are not speci\ufb01cally designed for complex games, the comparison is not performed due to a hard transfer on these toolboxes. Overall, the results show the ability of the proposed M2RL to some extent. Since this project is an on going item, so the main purpose of this part is an introduction, and we will continue improve this project (m2rl-V1.0). 6 CHALLENGES AND OPPORTUNITIES Plenty of DDRL algorithms and toolboxes are proposed, which largely promote the study of reinforcement learning and its applications. We think current methods still suffer several challenges, which may be the future directions. Firstly, current methods rarely consider accelerating complex reinforcement learning algorithms, such as those studying exploration, communication and generalization problems. Secondly, current approaches mainly use ring allreduce or parameter and server for learners, which seldom handle large model size and batch size situations simultaneously. Thirdly, self-play or population-play are important methods for multiple players and multiple agents training, which are also \ufb02exible without strict restrictions, but deeper study is de\ufb01cient. Fourthly, several famous DDRL toolboxes are developed, but none of them is veri\ufb01ed with large scale training, e.g., tens of machines for complex games. DDRL with advanced reinforcement learning algorithms. The research and application of reinforcement learning show explosive growth since the success of AlphaGo. ",
    "Conclusion": "CONCLUSION In this paper, we have surveyed representative distributed deep reinforcement learning methods. By summarizing key components to form a distributed deep reinforcement learning system, single player single agent distributed deep reinforcement learning methods are compared based on different types of coordinators. Furthermore, by introducing agents cooperation and players evolution, multiple players multiple agents distributed deep reinforcement learning approaches are elaborated. To support easy codes implementation, some popular distributed deep reinforcement learning toolboxes are introduced and discussed, based on which, a new multiple players and multiple agents learning toolbox is developed, hoping to assist learning for complex games. Finally, we discuss the challenges and opportunities of this exciting \ufb01led. Through this paper, we hope it becomes a reference for researchers and engineers when they are exploring novel reinforcement learning algorithms and solving practical reinforcement learning problems. ",
    "References": "REFERENCES [1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, pp. 484\u2013489, 2016. [2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang et al., \u201cMastering the game of go without human knowledge,\u201d Nature, vol. 550, pp. 354\u2013359, 2017. [3] Y. Yu, \u201cTowards sample ef\ufb01cient reinforcement learning,\u201d in International Joint Conference on Arti\ufb01cial Intelligence, 2018, pp. 5739\u2013 5743. [4] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai et al., \u201cPre-trained models for natural language processing: A survey,\u201d Science China Technological Sciences, pp. 1\u201326, 2020. [5] J. Li, S. Koyamada, Q. Ye, G. Liu, C. Wang et al., \u201cSuphx: mastering mahjong with deep reinforcement learning,\u201d arXiv:2003.13590v2, 2020. [6] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak et al., \u201cDota 2 with large scale deep reinforcement learning,\u201d arXiv:1912.06680v1, 2019. [7] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik et al., \u201cGrandmaster level in starcraft ii using multiagent reinforcement learning,\u201d Nature, vol. 575, pp. 350\u2013354, 2019. [8] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon et al., \u201cMassively parallel methods for deep reinforcement learning,\u201d in Deep Learning Workshop, International Conference on Machine Learning, 2015. [9] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang et al., \u201cSeed rl: Scalable and ef\ufb01cient deep-rl with accelerated central inference,\u201d in International Conference on Learning Representations, 2020. [10] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih et al., \u201cImpala: scalable distributed deep-rl with importance weighted actor-learner architectures,\u201d arXiv:1802.01561, 2018. [11] A. Sergeev and M. Del Balso, \u201cHorovod: fast and easy distributed deep learning in tensor\ufb02ow,\u201d arXiv:1802.05799, 2018. [12] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw et al., \u201cRay: A distributed framework for emerging {AI} applications,\u201d in 13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18), 2018, pp. 561\u2013577. [13] E. Liang, R. Liaw, P. Moritz, R. Nishihara, R. Fox et al., \u201cRllib: Abstractions for distributed reinforcement learning,\u201d in International Conference on Machine Learning, 2018. [14] M. R. Samsami and H. Alimadad, \u201cDistributed deep reinforcement learning: An overview,\u201d in Reinforcement Learning Algorithms: Analysis and Applications, 2021. [15] J. Czech, \u201cDistributed methods for reinforcement learning survey,\u201d in Reinforcement Learning Algorithms: Analysis and Applications, 2021. [16] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, \u201cDeep reinforcement learning: A brief survey,\u201d IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 26\u201338, 2017. [17] T. M. Moerland, J. Broekens, and C. M. Jonker, \u201cModel-based reinforcement learning: A survey,\u201d arXiv:2006.16712, 2020. [18] S. Gronauer and K. Diepold, \u201cMulti-agent deep reinforcement learning: a survey,\u201d Arti\ufb01cial Intelligence Review, vol. 55, pp. 895\u2013 943, 2022. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14 [19] Y. Yang and J. Wang, \u201cMulti-agent deep reinforcement learning: a survey,\u201d arXiv:2011.00583v3, 2021. [20] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, \u201cDemystifying parallel and distributed deep learning: An in-depth concurrency analysis,\u201d Ben-Num, Tai and Torsten, Hoe\ufb02er, vol. 52, no. 4, pp. 1\u201343, 2020. [21] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang et al., \u201cTerngrad: Ternary gradients to reduce communication in distributed deep learning,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 1509\u20131519. [22] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin et al., \u201cLarge scale distributed deep networks,\u201d in Advances in Neural Information Processing Systems, 2012, pp. 1232\u20131240. [23] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen et al., \u201cTensor\ufb02ow: Large-scale machine learning on heterogeneous distributed systems,\u201d https://arxiv.org/abs/1603.04467, 2016. [24] T. Ben-Nun and T. Hoe\ufb02er, \u201cDemystifying parallel and distributed deep learning: An in-depth concurrency analysis,\u201d ACM Computing Surveys, vol. 52, no. 4, pp. 1\u201343, 2020. [25] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis et al., \u201cCommunication-ef\ufb01cient and distributed learning over wireless networks: principles and applications,\u201d Proceedings of the IEEE, vol. 109, no. 5, pp. 796\u2013819, 2021. [26] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng et al., \u201cSemisupervised distributed learning with non-iid data for aiot service platform,\u201d IEEE Internet of Things Journal, vol. 7, no. 10, pp. 9266\u20139277, 2020. [27] Q. Yin, J. Yang, K. Huang, M. Zhao, W. Ni et al., \u201cAi in humancomputer gaming: techniques, challenges and opportunities,\u201d arXiv:2111.07631v2, 2022. [28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, and J. Veness, \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, pp. 529\u2013533, 2015. [29] Y. Burda, H. Edwards, A. Storkey, and O. Klimov, \u201cExploration by random network distillation,\u201d https://doi.org/10.48550/arXiv.1810.12894, 2018. [30] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli et al., \u201cThe starcraft multi-agent challenge,\u201d https://doi.org/10.48550/arXiv.1902.04043, 2019. [31] M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay et al., \u201cOpenspiel: A framework for reinforcement learning in games,\u201d arXiv:1908.09453v6, 2020. [32] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap et al., \u201cAsynchronous methods for deep reinforcement learning,\u201d in International Conference on Machine Learning, 2016, pp. 1928\u20131937. [33] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel et al., \u201cDistributed prioritized experience replay,\u201d in International Conference on Learning Representations, 2018. [34] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel et al., \u201cAi in human-computer gaming: techniques, challenges and opportunities,\u201d arXiv:1707.02286, 2017. [35] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney, \u201cRecurrent experience replay in distributed reinforcement learning,\u201d in International Conference on Learning Representations, 2019. [36] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan et al., \u201cTowards playing full moba games with deep reinforcement learning,\u201d in Neural Information Processing Systems, 2020. [37] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, \u201cReinforcement learning through asynchronous advantage actor-critic on a gpu,\u201d in International Conference on Learning Representations, 2017. [38] A. Stooke and P. Abbeel, \u201cAccelerated methods for deep reinforcement learning,\u201d arXiv:1803.02811v2, 2019. [39] A. V. Clemente, H. N. Castej\u00b4on, and A. Chandra, \u201cEf\ufb01cient parallel methods for deep reinforcement learning,\u201d arXiv:1705.04862v2, 2017. [40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv:1707.06347, 2017. [42] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever et al., \u201cHuman-level performance in 3d multiplayer games with populationbased reinforcement learning,\u201d Science, vol. 364, pp. 859\u2013865, 2019. [41] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa et al., \u201cDdppo: Learning near-perfect pointgoal navigators from 2.5 billion frames,\u201d in International Conference on Learning Representations, 2020. [43] D. Zha, J. Xie, W. Ma, S. Zhang, X. Lian, X. Hu, and J. Liu, \u201cDouzero: Mastering doudizhu with self-play deep reinforcement learning,\u201d in International Conference on Machine Learning, 2021. [44] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell et al., \u201cEmergent tool use from multi-agent autocurricula,\u201d arXiv:1909.07528, 2019. [45] T. Rashid, M. Samvelyan, C. S. d. Witt, G. Farquhar, J. N. Foerster, and S. Whiteson, \u201cQmix: Monotonic value function factorisation for deep multi-agent reinforcement learning,\u201d in International Conference on Machine Learning, 2018, pp. 4292\u20134301. [46] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli et al., \u201cThe starcraft multi-agent challenge,\u201d arXiv:1902.04043v5, 2019. [47] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai et al., \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, pp. 1140\u20131144, 2018. [48] X. Wang, J. Song, P. Qi, P. Peng, Z. Tang et al., \u201cScc: an ef\ufb01cient deep reinforcement learning agent mastering the game of starcraft ii,\u201d in International Conference on Machine Learning, 2021. [49] J. Paredis, \u201cCoevolutionary computation,\u201d Arti\ufb01cial life, vol. 2, no. 4, pp. 355\u2013375, 1995. [50] N. Brown and T. Sandholm, \u201cSuperhuman ai for heads-up nolimit poker: Libratus beats top professionals,\u201d Science, vol. 359, pp. 418\u2013424, 2018. [51] M. Morav\u02c7c\u00b4\u0131k, M. Schmid, N. Burch, V. Lis\u00b4y, D. Morrill et al., \u201cDeepstack: Expert-level arti\ufb01cial intelligence in heads-up nolimit poker,\u201d Science, vol. 356, pp. 508\u2013513, 2017. [52] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, \u201cMulti-agent actor-critic for mixed cooperative-competitive environments,\u201d arXiv:1706.02275v4, 2017. [53] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, N. Momchev et al., \u201cAcme: A research framework for distributed reinforcement learning,\u201d arXiv:2006.00979v2, 2020. [54] S. Fujimoto, H. Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d in International Conference on Machine Learning, 2018, pp. 1587\u20131596. [55] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d in Advances in Neural Information Processing Systems, 2016. [56] S. Reddy, A. D. Dragan, and S. Levine, \u201cSqil: Imitation learning via reinforcement learning with sparse rewards,\u201d https://doi.org/10.48550/arXiv.1905.11108, 2019. [57] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq et al., \u201cTianshou: A highly modularized deep reinforcement learning library,\u201d Journal of Machine Learning Research, vol. 23, no. 267, pp. 1\u20136, 2022. [58] H. K\u00a8uttler, N. Nardelli, T. Lavril, M. Selvatici, V. Sivakumar et al., \u201cTorchbeast: A pytorch platform for distributed rl,\u201d arXiv:1910.03552v1, 2019. [59] M. Zhou, Z. Wan, H. Wang, M. Wen, R. Wu et al., \u201cMalib: A parallel framework for population-based multi-agent reinforcement learning,\u201d arXiv:2106.07551, 2021. [60] P. Muller, S. Omidsha\ufb01ei, M. Rowland, K. Tuyls, J. Perolat et al., \u201cA generalized training approach for multiagent learning,\u201d https://doi.org/10.48550/arXiv.1909.12823, 2019. [61] S. Mcaleer, J. Lanier, R. Fox, and P. Baldi, \u201cPipeline psro: A scalable approach for \ufb01nding approximate nash equilibria in large games,\u201d in Advances in Neural Information Processing Systems, 2020. [62] J. Heinrich, M. Lanctot, and D. Silver, \u201cFictitious self-play in extensive-form games,\u201d in International Conference on Machine Learning, 2015, pp. 805\u2013813. [63] Q. Yin, M. Zhao, W. Ni, J. Zhang, and K. Huang, \u201cIntelligent decision making technology and challenge of wargame,\u201d Acta Automatica Sinica, vol. 47, 2021. [64] H. Jia, Y. Hu, Y. Chen, C. Ren, T. Lv et al., \u201cFever basketball: A complex, \ufb02exible, and asynchronized sports game environment for multi-agent reinforcement learning,\u201d arXiv:2012.03204, 2020. ",
    "title": "Distributed Deep Reinforcement Learning: A",
    "paper_info": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nDistributed Deep Reinforcement Learning: A\nSurvey and A Multi-Player Multi-Agent Learning\nToolbox\nQiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang\nAbstract\u2014With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential\ndecision-making problems. Despite its reputation, data inef\ufb01ciency caused by its trial and error learning mechanism makes deep\nreinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample ef\ufb01cient deep\nreinforcement learning, such as environment modeling, experience transfer, and distributed modi\ufb01cations, amongst which, distributed\ndeep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent\ntransportation. In this paper, we conclude the state of this exciting \ufb01eld, by comparing the classical distributed deep reinforcement\nlearning methods, and studying important components to achieve ef\ufb01cient distributed learning, covering single player single agent\ndistributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning.\nFurthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many\nmodi\ufb01cations of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed\ndeep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment,\nshowing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under\ncomplex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for\nresearchers who are interested in distributed deep reinforcement learning.\nIndex Terms\u2014Deep reinforcement learning, distributed machine learning, self-play, population-play, toolbox.\n!\n1\nINTRODUCTION\nW\nITH\nthe\nbreakthrough\nof\nAlphaGo\n[1],\n[2],\nan\nagent that wins plenty of professional Go players\nin human-computer gaming, deep reinforcement learning\n(DRL) comes to most researchers\u2019 attention, which becomes\na recognized technique for solving sequential decision mak-\ning problems. Plenty of algorithms are developed to solve\nchallenging issues that lie between DRL and real world\napplications, such as exploration and exploitation dilemma,\ndata inef\ufb01ciency, multi-agent cooperation and competition.\nAmong all these challenges, data inef\ufb01ciency is the most\ncriticized due to the trial and error learning mechanism of\nDRL, which requires a huge amount of interactive data.\nTo alleviate the data inef\ufb01ciency problem, several re-\nsearch directions are developed [3]. For example, model\nbased deep reinforcement learning constructs environment\nmodel for generating imaginary trajectories to help reduce\ntimes of interaction with the environment. Transfer rein-\nforcement learning mines shared skills, roles, or patterns\nfrom source tasks, and then uses the learned knowledge to\naccelerate reinforcement learning in the target task. Inspired\nfrom distributed machine learning techniques, which has\nbeen successfully utilized in computer vision and natural\nlanguage processing [4], distributed deep reinforcement\n\u2022\nQiyue Yin (qyyin@nlpr.ia.ac.cn), Tongtong Yu, Shengqi Shen, Meijing\nZhao, Kaiqi Huang and Liang Wang are with Institute of Automation,\nChinese Academy of Sciences, Beijing, China, 100190.\n\u2022\nJun Yang and Bin Liang are with the Department of Automation, Ts-\ninghua University, Beijing, China, 100084.\n\u2022\nCorresponding\nauthors:\nkqhuang@nlpr.ia.ac.cn\n(Kaiqi\nHuang);\nyangjun603@tsinghua.edu.cn (Jun Yang)\nlearning (DDRL) is developed, which has shown its poten-\ntial to train very successful agents, e.g., Suphx [5], OpenAI\nFive [6], and AlphaStar [7].\nGenerally, training DRL agents consists of two main\nparts, i.e., pulling policy network parameters to generate\ndata by interacting with the environment, and updating\npolicy network parameters by consuming data. Such a\nstructured pattern makes distributed modi\ufb01cations of DRL\nfeasible, and plenty of DDRL algorithms have been de-\nveloped. For example, the general reinforcement learning\narchitecture [8], likely the \ufb01rst DDRL architecture, divides\nthe training system into four components, i.e., parameter\nserver, learners, actors and replay buffer, which inspires\nsuccessive more data ef\ufb01cient DDRL architectures. The\nrecently proposed SEED RL [9], an improved version of\nIMPALA [10], is claimed to be able to produce and consume\nmillions of frames per second, based on which, AlphaStar\nis successfully trained within 44 days (192 v3 + 12 128 core\nTPUs, 1800 CPUs) for beating professional human players.\nTo make distributed modi\ufb01cations of DRL be able to use\nmultiple machines, several engineering problems should be\nsolved such as machines communication and distributed\nstorage. Fortunately, several useful toolboxes have been\ndeveloped and released, and revising codes of DRL to a\ndistributed version usually requires a small amount of code\nmodi\ufb01cation, which largely promotes the development of\nDDRL. For example, Horovod [11], released by Uber, makes\nfull use of ring allreduce technique, and can properly use\nmultiple GPUs for training acceleration by just adding a few\nlines of codes compared with the single GPU version. Ray\narXiv:2212.00253v1  [cs.LG]  1 Dec 2022\n",
    "GPTsummary": "- (1): The research background of this article is Deep Reinforcement Learning (DRL) becoming a recognized technique for solving sequential decision-making problems, however, data inefficiency caused by its trial and error learning mechanism makes DRL hard to be practical in a wide range of areas.\n\n- (2): Past methods for sample-efficient DRL, such as environment modeling and experience transfer have been developed to alleviate the data inefficiency problem. Distributed Deep Reinforcement Learning (DDRL), which is inspired by distributed machine learning techniques, has shown potential in various applications. The approach is well motivated as it has been successfully utilized in complex game environments.\n\n- (3): The research methodology proposed in this paper is a comprehensive literature review of DDRL, comparing classical DDRL methods, and studying important components to achieve efficient distributed learning. Additionally, the authors review recently released toolboxes that help realize DDRL without many modifications of their non-distributed versions, and develop and validate a multi-player multi-agent DDRL toolbox.\n\n- (4): The methods in this paper are validated on Wargame, a complex environment, and show the usability of the proposed toolbox for multiple players and multiple agents DDRL under complex games. The performance supports their goal of achieving efficient distributed learning using DDRL.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant as it comprehensively reviews distributed deep reinforcement learning (DDRL) methods, compares classical DDRL methods, and studies important components to achieve efficient distributed learning. The authors also develop and validate a multi-player multi-agent DDRL toolbox and validate it on a complex environment, supporting the goal of efficient distributed learning.\n\n- (2): Innovation point: This article innovatively develops a multi-player multi-agent DDRL toolbox that helps realize DDRL without many modifications of their non-distributed versions. \n\nPerformance: The methods in this paper are validated on a complex environment, proving the usability of the proposed toolbox for multiple players and multiple agents DDRL under complex games. \n\nWorkload: The workload of this article is appropriate, as it provides a comprehensive literature review of DDRL, compares classical DDRL methods, and studies important components. The authors also review and summarize recently released toolboxes that assist in the realization of DDRL. However, the article could benefit from further comparison with state-of-the-art DDRL methods.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work is significant as it comprehensively reviews distributed deep reinforcement learning (DDRL) methods, compares classical DDRL methods, and studies important components to achieve efficient distributed learning. The authors also develop and validate a multi-player multi-agent DDRL toolbox and validate it on a complex environment, supporting the goal of efficient distributed learning.\n\n- (2): Innovation point: This article innovatively develops a multi-player multi-agent DDRL toolbox that helps realize DDRL without many modifications of their non-distributed versions. \n\nPerformance: The methods in this paper are validated on a complex environment, proving the usability of the proposed toolbox for multiple players and multiple agents DDRL under complex games. \n\nWorkload: The workload of this article is appropriate, as it provides a comprehensive literature review of DDRL, compares classical DDRL methods, and studies important components. The authors also review and summarize recently released toolboxes that assist in the realization of DDRL. However, the article could benefit from further comparison with state-of-the-art DDRL methods.\n\n\n"
}