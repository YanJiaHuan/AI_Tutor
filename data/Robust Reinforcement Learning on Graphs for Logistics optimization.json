{
    "Abstract": "Abstract Logistics optimization nowadays is becoming one of the hottest areas in the AI community. In the past year, signi\ufb01cant advancements in the domain were achieved by representing the problem in a form of graph. Another promising area of research was to apply reinforcement learning algorithms to the above task. In our work, we made advantage of using both approaches and apply reinforcement learning on a graph. To do that, we have analyzed the most recent results in both \ufb01elds and selected SOTA algorithms both from graph neural networks and reinforcement learning. Then, we combined selected models on the problem of AMOD systems optimization for the transportation network of New York city. Our team compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore the important nodes on a graph representation. Finally, we achieved SOTA results on AMOD systems optimization problem employing PTDNet with GNN and training them in reinforcement fashion. Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement Learning 1 ",
    "Introduction": "Introduction For a long time, the problem of logistics optimization has used classical brute force algorithms or MPC. These algorithms are inef\ufb01cient in time, but accurate. According to the research of McKinsey [8], logistics is one of the industrial areas in which machine learning is expected to make one of the greatest progresses and can save huge costs in the next decade. For this, machine learning algorithms shall become more accurate than classical algorithms, while being already more time-ef\ufb01cient, since they solve the problem in polynomial time, in contrast to exponential time of classical algorithms. One of the potential directions in this \ufb01eld is reinforcement learning. It has already shown striking results for strategic tasks such as chess, Go, Atari [6] [7]. Since logistics is also a strategic optimization problem, the potential for using reinforcement learning for it is quite promising. To do this, RL should show robust results on different settings, on data of different types (homogeneous and heterogeneous) and different sizes (city, state, global scale). We concentrate on the second problem, where it is necessary to develop an algorithm which can build a logistic plan of a high quality independently of a given scale. Since the problem can be perfectly represented in the form of a graph, this shall also allow using the same model on the data of different scale, we are aiming to use a combination of RL and GNN to verify the robustness of the model on logistics (AMOD) problem. For now, our team has already analyzed the existing literature on this topic, reimplemented SOTA models on different datasets and checked their robustness on different graph sizes of New York transport environment. We are planning to use such SOTA models as GAT, Pro-GNN and PTDNet to improve data size robustness of RL on AMOD problem. To the best of our knowledge, there was no research conducted before in this direction. arXiv:2205.12888v1  [cs.LG]  25 May 2022 ",
    "Background": "Background Reinforcement learning is one of the methods of Machine Learning in which the model does not have any information about the system but has an option to interact with it and learn the policy from this experience. To formalize the reinforcement learning process we shall refer to Markov Decision Process (MDP) M which can be described as a function of a space S of system\u2019s observed states s \u2208 S, actions a \u2208 A available to the agent in that space, conditional probability P(st+1|st, at) of being in state st+1 upon taking action at at the state st, initial distribution d0 (starting point), reward r to identify the goodness of the steps which agent takes, and a discount factor 0 < \u03b3 \u2264 1: M = (S, A, P, d0, r, \u03b3) The ultimate goal of reinforcement learning is to derive the distribution of optimal actions to take in each state, also called policy \u03c0(at|st). In order to do that, the agent interacts with MDP following initial policy which is to be adjusted by the cycle: observing states st, taking actions at at that state and being rewarded or punished by rt depending on the newly observed state st+1. Graph Neural Networks (GNNs)are special type of neural network which directly works with a graph structure. Graph is a data structure G which consists of a set of vertices (nodes) V and edges E connecting them: G = (V, E) It is often very convenient to represent one\u2019s data as a graph structure when performing an image analysis task. This conveniences is directly related to graph\u2019s property of permutation invariance, meaning that an order of vertices is not affecting the output when performing calculations on them. Permutation invariance dramatically decreases the number of training examples required to generalize the model. However for traditional machine learning techniques one \ufb01rst need to represent graphstructured data into numerical vectors or other types of data structures, and such representation may lead to partial loss of data. GNN allows to work with graph data directly avoiding the above mentioned inconvenience. The core idea of GNN is based on a propagation of information on a graph. The latter is processed by a set of modules which are interconnected in accordance with the graph\u2019s edges and also linked with the vertices. While training, these modules update their states and exchange information until they reach an equilibrium. The mechanism to propagate information is limited to make sure the state of equilibrium exists. The output of GNN is calculated based on the state of the module at each vertex. On of the types of GNN is Graph Convolutional Network (GCNs) used for image classi\ufb01cation. GCNs use the following function to propagate information on a graph: X\u2032 = f(X, A) = \u03c3( \u02c6D\u2212 1 2 \u02c6A \u02c6D\u2212 1 2 XW) , \u02c6A = A + I Where I is an identity matrix, \u02c6D is a diagonal node degree matrix of \u02c6A, \u03c3 is a non-linear activation function and W is a matrix of weights. 3 Literature Review In the \ufb01eld of logistics optimization, the reinforcement learning approach is steadily gaining popularity among other machine learning techniques for its promising results. In 2017 Jian Wen et al. in [9] tackled rebalancing needs of AMOD systems. The team employed RL to be able to manage on large systems where data might be not fully available. As the result, the computational speed was 2.5 times faster versus classical models used in logistics while the performance of the model demonstrated near-optimal solution behavior. As the result, RL applications was considered a success for the task providing bene\ufb01ts for both users and operators. In [1] Daniele Gammelli et al. presented the \ufb01rst work that combined reinforcement learning and graph neural network on AMOD systems. They represented the transportation network as graph with areas of the city as vertices and connectivity between those areas as edges. RL algorithm was applied on a graph to manage the rebalancing of AMOD systems. The team showed that GNN provides the 2 ",
    "Methodology": "Methodology For sparse graphs A2C-GNN shows SOTA results for Autonomous Mobility-on-Demand problem. Deviation from MPC solution tend to reach zero value for small grid dimensions [1]. This means that for a graph with small number of connections for each node, A2C-GNN reaches the ideal NP-hard solution faster than other algorithms. However, the same study shows that as grid dimensions increase, the difference with the ideal solution grows. That is, the less sparse the input graph, the worse the results of the A2C-GNN algorithm we have. This trend will not allow the algorithm to be used in the industry for large scale problems. It is important to make an algorithm robust to increase in graph complexity. The ideal solution to the AMOD problem is a set of vehicles and assigned routes which is NP-hard task that can be ideally solved by MPC-tri-level algorithm. For each vehicle-node pair we choose only one node at a time as a partial route, while other nodes are meaningless at this iteration. However, when we use GNN algorithm we average information from all connected nodes. And the more connected nodes we have, the less meaningful becomes an information from every separate node, which makes it more dif\ufb01cult to \ufb01nd an ideal solution. We assume this is the reason why we are moving away from the ideal solution when we have more complex input graph. And to solve the problem, we should focus only on the most important nodes and consider only information coming from them, making averaged embeddings more meaningful for solving an NP-hard problem. To do this, we will consider three methods that allow us to emphasize the importance of certain nodes and zero out the information of others. The \ufb01rst algorithm is well-known Graph attention network [2], which \ufb01nds contextual weights for each node, thus changing the contribution of each node to the calculation of the new embedding. The second algorithm is Pro-GNN [3] which optimizes the adjacency matrix making it more sparse and low rank for more robust GNN computations. The third algorithm is PTDNet [4] which uses two neural networks, one to select the sparser subgraph and the other one, GNN, to process the subgraph making the \ufb01nal prediction. 4.1 GAT GNN recomputes feature vector of each node h given the formula: 3 \u20d7h\u2032 i = \u03c3 \uf8eb \uf8ed 1 K K \ufffd k=1 \ufffd j\u2208Ni \u03b1k ijWk\u20d7hj \uf8f6 \uf8f8 Where a is an attention value which measures \u2018importance\u2019 of each adjacent node for a new feature vector, and can be expressed using the following three formulas: eij = a \ufffd W\u20d7hi, W\u20d7hj \ufffd \u03b1ij = softmaxj (eij) = exp (eij) \ufffd k\u2208Ni exp (eik) \u03b1ij = exp \ufffd LeakyReLU \ufffd\u2212\u2192a T \ufffd W\u20d7hi\u2225W\u20d7hj \ufffd\ufffd\ufffd \ufffd k\u2208Ni exp \ufffd LeakyReLU \ufffd\u2212\u2192a T \ufffd W\u20d7hi\u2225W\u20d7hk \ufffd\ufffd\ufffd Here W is a weight matrix to be trained and both i and j represent indices of two different nodes. Attention weight a is in range [0, 1] so the higher it is, the more important corresponding adjacent node becomes. 4.2 Pro-GNN min \u03b8 LGNN (\u03b8, A, X, yL) = \ufffd vi\u2208VL \u2113 (f\u03b8(X, A)i, yi) By default, we optimize the above function, where function f is an output of GNN model with adjacency matrix A, feature input X and weight matrix W to be trained. f\u03b8(X, A) = softmax \ufffd \u02c6A\u03c3 \ufffd \u02c6AXW1 \ufffd W2 \ufffd From now on we assume that A is over complicated, it is having in\ufb02ated rank for ef\ufb01cient GNN use, and we shall replace it with another matrix S which we get from: arg min S\u2208S L0 = \u2225A \u2212 S\u22252 F + \u03b1\u2225S\u22251 + \u03b2\u2225S\u2225\u2217, s.t., S = S\u22a4 Here we initialize S = A, then we optimize S by making it sparser and downgrade the rank by minimizing its l1 and nuclear norms, while preserving it being symmetric and keeping it close to initial A as much as possible. 4.3 PTDNet Finding output Y , given graph G, can be formulated in terms of several subgraphs g: P(Y | G) \u2248 \ufffd g\u2208SG P(Y | g)P(g | G) Then, we can approximate second part using two different neural networks: \ufffd g\u2208SG P(Y | g)P(g | G) \u2248 \ufffd g\u2208SG Q\u03b8(Y | g)Q\u03c6(g | G) First network aims to select certain edges from initial adjacency matrix A forming subgraph g. Second network, given subgraph g, predicts output Y . Two networks can be trained together, given G and Y ; the only thing to consider is to make \ufb01rst part differentiable. For this purpose we will use 4 ",
    "Results": "Results The environment used for both training and testing is the Manhattan district represented in k x k grids map, where each grid is one of the New York City stations. Firstly, we create a graph of these grids and connect adjacent stations. Secondly, we run a simulation in which a daily demand for commute is synthesized. The model has a number of vehicles at its disposal which it has to assign to meet a demand and assign routes in order to maximize daily income. We can represent Manhattan in a different number of stations as a k x k grid. The larger k we have, the more complex graph we will build. On the input, the model receives a graph in which nodes are represented as stations and edges are connections between those stations. Each edge stores the information about the cost of moving from one node to another and the number of passengers who want to move this path. Each node stores information about the number of vehicles available in that station. The matrix of edges A and matrix of features X are fed to the Graph neural network that has one layer of graph convolution with ReLU activation function followed by 3 fully-connected layers with 32 neurons each. Actor and Critic networks have the same architecture with the only difference in output layer. Actor produces actions represented as a percentage distribution of vehicles to be rebalanced for each node at a given time step, that is predicting value in the range [0, 1] for each station. Critic aggregates information from all nodes using sum-pool function and predicts one value for the whole graph, where reward is represented as a pro\ufb01t at each time step. For GAT network we replaced GCN layer with Graph Attention layer, for Pro-GNN network we added optimization of adjacency matrix described in Section 4.2. Finally, for PTDNet we built additional sampling network that consists of several MLP layers and for every edge predicts probability of leaving it in A, changing A that will be used then in GCN. To train the models we employed PyTorch library in Python and built RL model. The model was trained using the Adam optimizer with 0.003 learning rate and 0.97 discount factor, for 16,000 episodes. To evaluate performance, we used demand, expressing it in total amount of executed trips, cost, expressing it in the expenses for all movements, and deviation from MPC-tri-level, which is an ideal solution for this problem, but unusable for a large real-life values of k. Table 2: Performance on New York 4 \u00d7 4 Network Model Reward Demand Rebalancing (%Dev. MPC-standard) Served Cost ($) A2C-GNN 33,384 (-3%) 8,699 5,036 A2C-GAT 33,280 (-3,3%) 8,569 5,049 A2C-Pro-GNN 33,246 (-3,4%) 8,664 5,018 A2C-PTDNet 33,418 (-2,9%) 8,709 5,031 The tables 1, 2 show the performance of all models for 4x4 and 20x20 grids\u2019 environments. In the \ufb01rst case, the differences between the models are insigni\ufb01cant, all performance metrices vary around the same values. However, in the second case, the difference rose sharply, with all three new models showing a better result than the baseline and the best result belongs to A2C-PTDNet with the smallest deviation of 30.1%, the largest demand and the smallest cost of 42,291 and 27,019 values respectively. The larger the value of k we choose, the higher the difference between the models\u2019 performance we have. It can be seen in Figure 1. 5 ",
    "Discussion": "Discussion The results show an improvement in model\u2019s performance. However, the reason for this may be that the model, in all three cases, has become more complex in comparison with the base model as a whole and not more robust to the size of the setting in particular. In addition to the above, the negative relationship between the quality of prediction and the size of the setting still remained quite high. For the \ufb01nal solution of this problem, this dependence should be close to zero. However, we have shown that focusing on \u2018most important\u2019 nodes during calculations can improve the performance. Moreover, discrete selection of nodes works better than their weighting or adjacency matrix continuous transformation. Thus, indeed, an increase in the average number of connected nodes for large graphs negatively affects learning, due to the greater blurring of information received from each node. In the future research, it can be effective to focus on a discrete selection of nodes for network performance improvement, as well as to test other techniques of node selection and nodes combinations, and to test the models on other logistics tasks besides AMOD. 7 ",
    "Conclusion": "Conclusion We got new SOTA results on the New York dataset of AMOD problem using PTDNet with GNN architecture trained in reinforcement fashion. However, it is necessary to further improve the result in order to obtain a robust result no matter of setting size and the closest possible solution to MPCtri-level (ideal). In this case, the model will be bene\ufb01cial for use in the industry, and can become in demand and save large costs for logistics tasks. In general, this direction is potentially bene\ufb01cial and interesting from both theoretical and practical points of view. 6 ",
    "References": "References [1] Gammelli, D., Yang, K., Harrison, J., Rodrigues, F., Pereira, F. C., & Pavone, M. (2021). Graph Neural Network Reinforcement Learning for Autonomous Mobility-on-Demand Systems. arXiv preprint arXiv:2104.11434. [2] Veli\u02c7ckovi\u00b4c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903. [3] Jin, W., Ma, Y., Liu, X., Tang, X., Wang, S., & Tang, J. (2020, August). Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 66-74). [4] Luo, D., Cheng, W., Yu, W., Zong, B., Ni, J., Chen, H., & Zhang, X. (2021, March). Learning to drop: Robust graph neural network via topological denoising. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (pp. 779-787). [5] Jang, E., Gu, S., & Poole, B. (2016). Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144. [6] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602. [7] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... & Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140-1144. [8] Baer, T., & Kamalnath, V. (2017). Controlling machine-learning algorithms and their biases. McKinsey Insights. [9] Wen, J., Zhao, J., & Jaillet, P. (2017, October). Rebalancing shared mobility-on-demand systems: A reinforcement learning approach. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) (pp. 220-225). Ieee. [10] Yan, Y., Chow, A. H., Ho, C. P., Kuo, Y. H., Wu, Q., & Ying, C. (2021). Reinforcement Learning for Logistics and Supply Chain Management: Methodologies, State of the Art, and Future Opportunities. State of the Art, and Future Opportunities (October 4, 2021). 7 ",
    "title": "Robust Reinforcement Learning on Graphs for",
    "paper_info": "Robust Reinforcement Learning on Graphs for\nLogistics optimization\nZangir Iklassov\nMBZUAI\nPhD in Machine Learning\n20020082@mbzuai.ac.ae\nDmitrii Medvedev\nMBZUAI\nMSc in Machine Learning\n20020035@mbzuai.ac.ae\nAbstract\nLogistics optimization nowadays is becoming one of the hottest areas in the AI\ncommunity. In the past year, signi\ufb01cant advancements in the domain were achieved\nby representing the problem in a form of graph. Another promising area of research\nwas to apply reinforcement learning algorithms to the above task. In our work,\nwe made advantage of using both approaches and apply reinforcement learning\non a graph. To do that, we have analyzed the most recent results in both \ufb01elds\nand selected SOTA algorithms both from graph neural networks and reinforcement\nlearning. Then, we combined selected models on the problem of AMOD systems\noptimization for the transportation network of New York city. Our team compared\nthree algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore the important\nnodes on a graph representation. Finally, we achieved SOTA results on AMOD\nsystems optimization problem employing PTDNet with GNN and training them in\nreinforcement fashion.\nKeywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement\nLearning\n1\nIntroduction\nFor a long time, the problem of logistics optimization has used classical brute force algorithms or\nMPC. These algorithms are inef\ufb01cient in time, but accurate. According to the research of McKinsey\n[8], logistics is one of the industrial areas in which machine learning is expected to make one of the\ngreatest progresses and can save huge costs in the next decade. For this, machine learning algorithms\nshall become more accurate than classical algorithms, while being already more time-ef\ufb01cient, since\nthey solve the problem in polynomial time, in contrast to exponential time of classical algorithms.\nOne of the potential directions in this \ufb01eld is reinforcement learning. It has already shown striking\nresults for strategic tasks such as chess, Go, Atari [6] [7].\nSince logistics is also a strategic optimization problem, the potential for using reinforcement learning\nfor it is quite promising. To do this, RL should show robust results on different settings, on data of\ndifferent types (homogeneous and heterogeneous) and different sizes (city, state, global scale). We\nconcentrate on the second problem, where it is necessary to develop an algorithm which can build\na logistic plan of a high quality independently of a given scale. Since the problem can be perfectly\nrepresented in the form of a graph, this shall also allow using the same model on the data of different\nscale, we are aiming to use a combination of RL and GNN to verify the robustness of the model on\nlogistics (AMOD) problem. For now, our team has already analyzed the existing literature on this\ntopic, reimplemented SOTA models on different datasets and checked their robustness on different\ngraph sizes of New York transport environment.\nWe are planning to use such SOTA models as GAT, Pro-GNN and PTDNet to improve data size\nrobustness of RL on AMOD problem. To the best of our knowledge, there was no research conducted\nbefore in this direction.\narXiv:2205.12888v1  [cs.LG]  25 May 2022\n",
    "GPTsummary": "                    - (1): The research background of this paper is on logistics optimization, which has traditionally used brute force algorithms or MPC. Machine learning algorithms have shown huge potential in the field of logistics optimization, with reinforcement learning being a promising direction for improving accuracy and time efficiency.\n\n                    - (2): Past methods such as brute force algorithms or MPC are time inefficient, while classical algorithms are accurate but require exponential time complexity. Reinforcement learning has shown great promise for solving strategic tasks, including logistics optimization. However, reinforcement learning needs to be robust on different data types and sizes. The authors propose using reinforcement learning on graphs for optimizing logistics and suggest that this approach will allow for the same model to be applied to data of different scales. This is a well-motivated approach given the advantages of using graph neural networks and reinforcement learning in the field of logistics optimization.\n\n                    - (3): The research methodology proposed in this paper involves selecting state-of-the-art algorithms from graph neural networks and reinforcement learning, combining them, and applying them to the problem of AMOD systems optimization for the transportation network of New York City. The authors compare three algorithms - GAT, Pro-CNN, and PTDNet - to highlight the important nodes on a graph representation.\n\n                    - (4): The performance achieved in this paper is SOTA results on AMOD systems optimization problem using PTDNet with GNN and training them in reinforcement fashion. The performance supports the authors' goals of improving the accuracy and time efficiency of classical algorithms for logistics optimization using reinforcement learning on graphs.\n7. Methods:\n\n- (1): The methodological idea of this article is to propose a reinforcement learning approach on graphs to optimize logistics. The authors combine the state-of-the-art algorithms from graph neural networks and reinforcement learning to solve the Autonomous Mobility-on-Demand (AMOD) problem for transportation networks in New York City. The focus is on making the reinforcement learning algorithm robust on different data types and sizes and showing that it can improve the accuracy and time efficiency of classical algorithms.\n\n- (2): To achieve this, the authors compare three algorithms for emphasizing important nodes on a graph representation: Graph Attention Network (GAT), Pro-GNN, and PTDNet. GAT finds contextual weights for each node, Pro-GNN optimizes the adjacency matrix to make it more sparse and low rank, while PTDNet uses two neural networks to select a sparser subgraph and then process it with GNN for making the final prediction.\n\n- (3): The authors highlight the challenge of increasing graph complexity and moving away from the ideal solution of the NP-hard AMOD problem. They propose focusing only on the most important nodes and considering only the information coming from them to make averaged embeddings more meaningful. The authors show that the PTDNet algorithm with GNN and training in reinforcement fashion achieves SOTA results on the AMOD systems optimization problem.\n\n\n\n\n\n8. Conclusion: \n\n- (1) This article proposes an innovative approach by combining reinforcement learning and graph neural networks to optimize logistics in the transportation network of New York City. This approach shows promise in improving the accuracy and time efficiency of classical algorithms in logistics optimization.\n\n- (2) Innovation point: The combination of reinforcement learning and graph neural networks for logistics optimization on complex graphs. Performance: The proposed approach achieves SOTA results on the Autonomous Mobility-on-Demand (AMOD) problem for transportation networks in New York City. Workload: The research methodology involves selecting state-of-the-art algorithms and training them in reinforcement fashion on large and complex graphs, which may be time-consuming and computationally expensive. Overall, the article provides valuable insights and contributions to the field of logistics optimization using reinforcement learning on graphs.\n\n\n",
    "GPTmethods": "- (1): The methodological idea of this article is to propose a reinforcement learning approach on graphs to optimize logistics. The authors combine the state-of-the-art algorithms from graph neural networks and reinforcement learning to solve the Autonomous Mobility-on-Demand (AMOD) problem for transportation networks in New York City. The focus is on making the reinforcement learning algorithm robust on different data types and sizes and showing that it can improve the accuracy and time efficiency of classical algorithms.\n\n- (2): To achieve this, the authors compare three algorithms for emphasizing important nodes on a graph representation: Graph Attention Network (GAT), Pro-GNN, and PTDNet. GAT finds contextual weights for each node, Pro-GNN optimizes the adjacency matrix to make it more sparse and low rank, while PTDNet uses two neural networks to select a sparser subgraph and then process it with GNN for making the final prediction.\n\n- (3): The authors highlight the challenge of increasing graph complexity and moving away from the ideal solution of the NP-hard AMOD problem. They propose focusing only on the most important nodes and considering only the information coming from them to make averaged embeddings more meaningful. The authors show that the PTDNet algorithm with GNN and training in reinforcement fashion achieves SOTA results on the AMOD systems optimization problem.\n\n\n\n\n\n8. Conclusion: \n\n- (1) This article proposes an innovative approach by combining reinforcement learning and graph neural networks to optimize logistics in the transportation network of New York City. This approach shows promise in improving the accuracy and time efficiency of classical algorithms in logistics optimization.\n\n- (2) Innovation point: The combination of reinforcement learning and graph neural networks for logistics optimization on complex graphs. Performance: The proposed approach achieves SOTA results on the Autonomous Mobility-on-Demand (AMOD) problem for transportation networks in New York City. Workload: The research methodology involves selecting state-of-the-art algorithms and training them in reinforcement fashion on large and complex graphs, which may be time-consuming and computationally expensive. Overall, the article provides valuable insights and contributions to the field of logistics optimization using reinforcement learning on graphs.\n\n\n",
    "GPTconclusion": "- (1) This article proposes an innovative approach by combining reinforcement learning and graph neural networks to optimize logistics in the transportation network of New York City. This approach shows promise in improving the accuracy and time efficiency of classical algorithms in logistics optimization.\n\n- (2) Innovation point: The combination of reinforcement learning and graph neural networks for logistics optimization on complex graphs. Performance: The proposed approach achieves SOTA results on the Autonomous Mobility-on-Demand (AMOD) problem for transportation networks in New York City. Workload: The research methodology involves selecting state-of-the-art algorithms and training them in reinforcement fashion on large and complex graphs, which may be time-consuming and computationally expensive. Overall, the article provides valuable insights and contributions to the field of logistics optimization using reinforcement learning on graphs.\n\n\n"
}