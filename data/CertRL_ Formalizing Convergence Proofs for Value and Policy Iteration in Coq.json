{
    "Introduction": "INTRODUCTION Reinforcement learning (RL) algorithms solve sequential decision making problems in which the goal is to choose actions that maximize a quantitative utility function [Bel54, How60, Put94, SB98]. Recent high-profile applications of reinforcement learning include beating the world\u2019s best players at Go [SHM+16], competing against top professionals in Dota [Ope18], improving protein structure prediction [SEJ+20], and automatically controlling complex robots [GHLL16]. These successes motivate the use of reinforcement learning in safety-critical and correctness-critical settings. Reinforcement learning algorithms produce, at a minimum, a policy that specifies which action(s) should be taken in a given state. The primary correctness property for reinforcement learning algorithms is convergence: in the limit, a reinforcement learning algorithm should converge to a policy that optimizes for the expected future-discounted value of the reward signal. This paper contributes CertRL, a formal proof of convergence for value iteration and policy iteration two canonical reinforcement learning algorithms [Bel54, How60, Put94]. They are often taught as the first reinforcement learning methods in machine learning courses because the algorithms are relatively simple but their convergence proofs contain the main ingredients of a typical convergence argument for a reinforcement learning algorithm. There is a cornucopia of presentations of these iterative algorithms and an equally diverse variety of proof techniques for establishing convergence. Many presentations state but do not prove the fact that the optimal policy of an infinite-horizon Markov decision process with \ud835\udefe-discounted reward is a stationary policy; i.e., the optimal decision in a given state does not depend on the time step at which the state is encountered. Following this convention, this paper contributes the first formal proof that policy and value iteration converge in the limit to the optimal policy in the space of stationary policies for infinite-horizon Markov decision processes. In addition to establishing convergence results for the classical iterative algorithms under classical infinitary and arXiv:2009.11403v2  [cs.AI]  15 Dec 2020 Vajjha et al. stationarity assumptions, we also formalize an optimality result about \ud835\udc5b-step iterations of value iteration without a stationarity assumption. The former formalization matches the standard theoretical treatment, while the latter is closer to real-world implementations. We shall refer to the former case \u2013 where the set of time steps is an infinite set \u2013 as infinite-horizon and the latter case as finite-horizon. In all cases, the convergence argument for policy/value iteration proceeds by proving that a contractive mapping converges to a fixed point and that this fixed point is an optimum. This is typical of convergence proofs for reinforcement learning algorithms. CertRL is intentionally designed for ongoing reinforcement learning formalization efforts. Formalizing the convergence proof directly would require complicated and tedious \ud835\udf16-hacking as well as long proofs involving large matrices. CertRL obviates these challenges using a combination of the Giry monad [Gir82, Jac18] and a proof technique called Metric coinduction [Koz07]. Metric coinduction was first identified by Kozen and Ruozzi as a way to streamline and simplify proofs of theorems about streams and stochastic processes [KR09]. Our convergence proofs use a specialized version of metric coinduction called contraction coinduction [FHM18] to reason about order statements concerning fixed points of contractive maps. Identifying a coinduction hypothesis allows us to automatically infer that a given (closed) property holds in the limit whenever it holds ab initio. The coinduction hypothesis guarantees that this property is a limiting invariant. This is significant because the low level \ud835\udf16 \u2212 \ud835\udeff arguments \u2013 typically needed to show that a given property holds of the limit \u2013 are now neatly subsumed by a single proof rule, allowing reasoning at a higher level of abstraction. The finitary Giry monad is a monad structure on the space of all finitely supported probability mass functions on a set. Function composition in the Kleisli category of this monad recovers the Chapman-Kolmogorov formula [Per19, Jac18]. Using this fact, our formalization recasts iteration of a stochastic matrix in a Markov decision process as iterated Kleisli composites of the Giry monad, starting at an initial state. Again, this makes the presentation cleaner since we identify and reason about the basic operations of bind and ret, thus bypassing the need to define matrices and matrix multiplication and substantially simplifying convergence proofs. This paper shows how these two basic building blocks \u2013 the finitary Giry monad and metric coinduction \u2013 provide a compelling foundation for formalizing reinforcement learning theory. CertRL develops the basic concepts in reinforcement learning theory and demonstrates the usefulness of this library by proving several results about value and policy iteration. CertRL contains a proof of the Bellman optimality principle, an inductive relation on the optimal value and policy over the horizon length of the Markov decision process. In practice, reinforcement learning algorithms almost always run in finite time by either fixing a run time cutoff (e.g., number training steps) or by stopping iteration after the value/policy changes become smaller than a fixed threshold. Therefore, our development also formalizes a proof that \ud835\udc5b-step value iteration satisfies a finite time analogue of our convergence results. To summarize, the CertRL library contains: (1) a formalization of Markov decision processes and their long-term values in terms of the finitary Giry monad, (2) a formalization of optimal value functions and the Bellman operator, (3) a formal proof of convergence for value iteration and a formalization of the policy improvement theorem in the case of stationary policies, and (4) a formal proof that the optimal value function for finitary sequences satisfies the finite time analogue of the Bellman equation. ",
    "Background": "BACKGROUND We provide a brief introduction to value/policy iteration and to the mathematical structures upon which our formalization is built: contractive metric spaces, metric coinduction, the Giry monad and Kleisli composition. 2.1 Reinforcement Learning This section gently introduces the basics of reinforcement learning with complete information about the stochastic reward and transition functions. In this simplified situation the focus of the algorithm is on optimal exploitation of reward. This framework is also known as the stochastic optimal control problem. We give an informal definition of Markov decision processes, trajectories, long-term values, and dynamic programming algorithms for solving Markov decision processes. Many of these concepts will be stated later in a more formal type-theoretic style; here, we focus on providing an intuitive introduction to the field. The basic mathematical object in reinforcement learning theory is the Markov decision process. A Markov decision process is a 4-tuple (\ud835\udc46,\ud835\udc34, \ud835\udc45,\ud835\udc47) where \ud835\udc46 is a set of states, \ud835\udc34 is a set of actions, \ud835\udc45 : \ud835\udc46 \u00d7 \ud835\udc34 \u00d7 \ud835\udc46 \u2192 R is a reward function, and \ud835\udc47 is a transition relation on states and actions mapping each (\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032) \u2208 \ud835\udc46 \u00d7 \ud835\udc34 \u00d7 \ud835\udc46 to the probability that taking action \ud835\udc4e in state \ud835\udc60 results in a transition to \ud835\udc60\u2032. Markov decision processes are so-called because they characterize a sequential decision-making process (each action is a decision) in which the transition structure on states and actions depends only on the current state. Example 1 (CeRtL the turtle \u273f). Consider a simple grid world environment in which a turtle can move in cardinal directions throughout a 2D grid. The turtle receives +1 point for collecting stars, -10 for visiting red squares, and +2 for arriving at the green square. The turtle chooses which direction to move, but with probability 1 4 will move in the opposite direction. For example, if the turtle takes action left then it will go left with probability 3 4 and right with probability 1 4. The game ends when the turtle arrives at the green square. This environment is formulated as a Markov decision process as follows: \u2022 The set of states \ud835\udc46 are the coordinates of each box \u273f: {(\ud835\udc65,\ud835\udc66) | 1 \u2264 \ud835\udc65 \u2264 5 and 1 \u2264 \ud835\udc66 \u2264 5} so that (1, 1) is the top-left corner and (5, 5) is the bottom-right corner. \u2022 The set of actions \ud835\udc34 are {up, down, left, right} \u273f. 1We recommend MacOS users view this document in Adobe, Firefox, or Chrome, as Preview and Safari parse the URLs linked to by \u273f\u2019s incorrectly. Vajjha et al. Fig. 1. An example grid-world environment \u273f. \u2022 The reward function is defined as \u273f: \ud835\udc45(1, 4) = 2 \ud835\udc45(4, 2) = 1 \ud835\udc45(3, 3) = 1 \ud835\udc45({1, 2, 3}, 2) = \u221210 \ud835\udc45(4, 3) = \u221210 \ud835\udc45(2, 4) = \u221210 \ud835\udc45(4, 5) = \u221210 \ud835\udc45(\u00b7, \u00b7) = 0 otherwise \u2022 The transition probabilities are as described \u273f; e.g., \ud835\udc47 ((3, 4), up, (3, 3)) = 3 4 \ud835\udc47 ((3, 4), up, (3, 5)) = 1 4 \ud835\udc47 ((3, 4), up, (\u00b7, \u00b7)) = 0 otherwise and so on. We implement this example in Coq as a proof-of-concept for CertRL. We first define a matrix whose indices are states (\ud835\udc65,\ud835\udc66) and whose entries are colors {red, green, star, empty}. We then define a reward function that maps from matrix entries to a reward depending on the color of the turtle\u2019s current state. We also define a transition function that comports with the description given above. At last, we prove that this combination of states, actions, transitions and rewards inhabits our MDP (standing for Markov decision process) type. Therefore, all of the theorems developed in this paper apply directly to our Coq implementation of the CertRL Turtle environment. CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq The goal of reinforcement learning is to find a policy (\ud835\udf0b : \ud835\udc46 \u2192 \ud835\udc34) specifying which action the algorithm should take in each state. This policy should maximize the amount of reward obtained by the agent. A policy is stationary if it is not a function of time; i.e., if the optimal action in some state \ud835\udc60 \u2208 \ud835\udc46 is always the same and, in particular, independent of the specific time step at which \ud835\udc60 is encountered. Reinforcement learning agents optimize for a discounted sum of rewards \u2013 placing more emphasis on reward obtained today and less emphasis on reward obtained tomorrow. A constant discount factor from the open unit interval, typically denoted by \ud835\udefe, quantitatively discounts future rewards and serves as a crucial hyperparameter to reinforcement learning algorithms. Value iteration, invented by Bellman [Bel54], is a dynamic programming algorithm that finds optimal policies to reinforcement learning algorithms by iterating a contractive mapping. Value iteration is defined in terms of a value function \ud835\udc49\ud835\udf0b : \ud835\udc46 \u2192 R, where \ud835\udc49\ud835\udf0b (\ud835\udc60) is the expected value of state \ud835\udc60 when following policy \ud835\udf0b from \ud835\udc60. Data: Markov decision process (\ud835\udc46,\ud835\udc34,\ud835\udc47, \ud835\udc45) Initial value function \ud835\udc490 = 0 Threshold \ud835\udf03 > 0 Discount factor 0 < \ud835\udefe < 1 Result: \ud835\udc49 \u2217, the value function for an optimal policy. for \ud835\udc5b from 0 to \u221e do for each \ud835\udc60 \u2208 \ud835\udc46 do \ud835\udc49\ud835\udc5b+1[\ud835\udc60] = max\ud835\udc4e \ufffd \ud835\udc60\u2032 \ud835\udc47 (\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032)(\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032) + \ud835\udefe\ud835\udc49\ud835\udc5b[\ud835\udc60\u2032]) end if \u2200\ud835\udc60|\ud835\udc49\ud835\udc5b+1[\ud835\udc60] \u2212 \ud835\udc49\ud835\udc5b| < \ud835\udf03 then return \ud835\udc49\ud835\udc5b+1 end end Algorithm 1: Pseudocode for Value Iteration. The optimal policy \ud835\udf0b\u2217 is then obtained by \ud835\udf0b\u2217(\ud835\udc4e) = argmax\ud835\udc4e\u2208\ud835\udc34 \u2211\ufe01 \ud835\udc60\u2032 \ud835\udc47 (\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032)(\ud835\udc45(\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032) + \ud835\udefe\ud835\udc49\ud835\udc5b+1[\ud835\udc60\u2032]). Policy iteration follows a similar iteration scheme, but with a policy estimation function \ud835\udc44\ud835\udf0b : \ud835\udc46 \u00d7 \ud835\udc34 \u2192 R where \ud835\udc44\ud835\udf0b (\ud835\udc60,\ud835\udc4e) estimates the value of taking action \ud835\udc4e in state \ud835\udc60 and then following the policy \ud835\udf0b. In Section 3.4 we will demonstrate a formalized proof that \ud835\udc49\ud835\udc5b is the optimal value function of a length \ud835\udc5b MDP; this algorithm implements the dynamic programming principle. 2.2 Metric and Contraction Coinduction Our formalization uses metric coinduction to establish convergence properties for infinite sequences. This section recalls the Banach fixed point theorem and explains how this theorem gives rise to a useful proof technique. A metric space (\ud835\udc4b,\ud835\udc51) is a set \ud835\udc4b equipped with a function \ud835\udc51 : \ud835\udc4b \u00d7 \ud835\udc4b \u2192 R satisfying certain axioms that ensure \ud835\udc51 behaves like a measurement of the distance between points in \ud835\udc4b. A metric space is complete if the limit of every Cauchy sequence of elements in \ud835\udc4b is also in \ud835\udc4b. Let (\ud835\udc4b,\ud835\udc51) denote a complete metric space with metric \ud835\udc51. Subsets of \ud835\udc4b are modeled by terms of the function type \ud835\udf19 : \ud835\udc4b \u2192 Prop. Another interpretation is that \ud835\udf19 denotes all those terms of \ud835\udc4b which satisfy a particular property. These subsets are also called Ensembles in the Coq standard library. Vajjha et al. A Lipschitz map \u273f is a mapping that is Lipschitz continuous; i.e., a mapping \ud835\udc39 from (\ud835\udc4b,\ud835\udc51\ud835\udc4b) into (\ud835\udc4c,\ud835\udc51\ud835\udc4c ) such that for all \ud835\udc651,\ud835\udc652 \u2208 \ud835\udc4b there is some \ud835\udc3e \u2265 0 such that \ud835\udc51\ud835\udc4c (\ud835\udc39 (\ud835\udc651), \ud835\udc39 (\ud835\udc652)) \u2264 \ud835\udc3e\ud835\udc51\ud835\udc4b (\ud835\udc651,\ud835\udc652). The constant \ud835\udc3e is called a Lipschitz constant. A map \ud835\udc39 : \ud835\udc4b \u2192 \ud835\udc4b is called a contractive map \u273f, or simply a contraction, if there exists a constant 0 \u2264 \ud835\udefe < 1 such that \ud835\udc51(\ud835\udc39 (\ud835\udc62), \ud835\udc39 (\ud835\udc63)) \u2264 \ud835\udefe\ud835\udc51(\ud835\udc62, \ud835\udc63) \u2200\ud835\udc62, \ud835\udc63 \u2208 \ud835\udc4b. Contractive maps are Lipschitz maps with Lipschitz constant \ud835\udefe < 1. The Banach fixed point theorem is a standard result of classical analysis which states that contractive maps on complete metric spaces have a unique fixed point. Theorem 2 (Banach fixed point theorem). If (\ud835\udc4b,\ud835\udc51) is a nonempty complete metric space and \ud835\udc39 : \ud835\udc4b \u2192 \ud835\udc4b is a contraction, then \ud835\udc39 has a unique fixed point; i.e., there exists a point \ud835\udc65\u2217 \u2208 \ud835\udc4b such that \ud835\udc39 (\ud835\udc65\u2217) = \ud835\udc65\u2217. This fixed point is \ud835\udc65\u2217 = lim\ud835\udc5b\u2192\u221e \ud835\udc39 (\ud835\udc5b) (\ud835\udc650) where \ud835\udc39 (\ud835\udc5b) stands for the \ud835\udc5b-th iterate of the function \ud835\udc39 and \ud835\udc650 is an arbitrary point in \ud835\udc4b. The Banach fixed point theorem generalizes to subsets of \ud835\udc4b. Theorem 3 (Banach fixed point theorem on subsets \u273f). Let (\ud835\udc4b,\ud835\udc51) be a complete metric space and \ud835\udf19 a closed nonempty subset of \ud835\udc4b. Let \ud835\udc39 : \ud835\udc4b \u2192 \ud835\udc4b be a contraction and assume that \ud835\udc39 preserves \ud835\udf19. In other words, \ud835\udf19(\ud835\udc62) \u2192 \ud835\udf19(\ud835\udc39 (\ud835\udc62)) Then \ud835\udc39 has a unique fixed point in \ud835\udf19; i.e., a point \ud835\udc65\u2217 \u2208 \ud835\udc4b such that \ud835\udf19(\ud835\udc65\u2217) and \ud835\udc39 (\ud835\udc65\u2217) = \ud835\udc65\u2217. The fixed point of \ud835\udc39 is given by \ud835\udc65\u2217 = lim\ud835\udc5b\u2192\u221e \ud835\udc39 (\ud835\udc5b) (\ud835\udc650) where \ud835\udc39 (\ud835\udc5b) stands for the \ud835\udc5b-th iterate of the function \ud835\udc39. Both the Banach fixed point theorem and the more general theorem on subsets were previously formalized in Coq by Boldo et al. [BCF+17]. This formalization includes definitions of Lipschitz maps and contractions. We make use of the fact that Boldo et al. prove the above theorem where \ud835\udc4b is either a CompleteSpace or a CompleteNormedModule. The fixed point of \ud835\udc39 in Theorem 3 is unique, but it depends on an initial point \ud835\udc650 \u2208 \ud835\udc4b, which \ud835\udc39 then iterates on. Uniqueness of the fixed point implies that different choices of the initial point still give the same fixed point \u273f. To emphasize how this theorem is used in our formalization, we restate it as an inductive proof rule: \ud835\udf19 closed \u2203\ud835\udc650,\ud835\udf19(\ud835\udc650) \ud835\udf19(\ud835\udc62) \u2192 \ud835\udf19(\ud835\udc39 (\ud835\udc62)) \ud835\udf19(fix \ud835\udc39 \ud835\udc650) \u273f (1) This proof rule states that in order to prove some closed \ud835\udf19 is a property of a fixed point of \ud835\udc39, it suffices to establish the standard inductive assumptions: that \ud835\udf19 holds for some initial \ud835\udc650, and that if \ud835\udf19 holds at \ud835\udc62 then it also holds after a single application of \ud835\udc39 to \ud835\udc62. In this form, the Banach fixed point theorem is called Metric coinduction. The rule (1) is coinductive because it is equivalent to the assertion that a certain coalgebra is final in a category of coalgebras. (Details are given in Section 2.3 of Kozen and Ruozzi [KR09]). The following snippet shows how we use the Banach Fixed Point theorem as proven in [BCF+17] as a proof rule. Theorem metric_coinduction {phi : X \u2192 Prop} ( nephi : phi init) ( Hcphi : closed phi) ( HFphi : forall x : X, phi x \u2192 phi (F x)): phi ( fixpt F init). Proof. assert (my_complete phi) CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq by ( now apply closed_my_complete). destruct (FixedPoint K F phi fphi ( ex_intro _ _ init_phi) H hF) as [? [Hin [? [? Hsub]]]]. specialize (Hsub init init_phi). rewrite \u2190 Hsub in Hin. apply Hin. Qed. Definition 4 (Ordered Metric Space). A metric space \ud835\udc4b is called an ordered metric space if the underlying set \ud835\udc4b is partially ordered and the sets {\ud835\udc67 \u2208 \ud835\udc4b |\ud835\udc67 \u2264 \ud835\udc66} and {\ud835\udc67 \u2208 \ud835\udc4b |\ud835\udc66 \u2264 \ud835\udc67} are closed sets in the metric topology for every \ud835\udc66 \u2208 \ud835\udc4b. For ordered metric spaces, metric coinduction specializes to [FHM18, Theorem 1], which we restate as Theorem 5 below. Theorem 5 (Contraction coinduction). Let \ud835\udc4b be a non-empty, complete ordered metric space. If \ud835\udc39 : \ud835\udc4b \u2192 \ud835\udc4b is a contraction and is order-preserving, then: \u2022 \u2200\ud835\udc65, \ud835\udc39 (\ud835\udc65) \u2264 \ud835\udc65 \u21d2 \ud835\udc65\u2217 \u2264 \ud835\udc65 \u273f and \u2022 \u2200\ud835\udc65,\ud835\udc65 \u2264 \ud835\udc39 (\ud835\udc65) \u21d2 \ud835\udc65 \u2264 \ud835\udc65\u2217 \u273f where \ud835\udc65\u2217 is the fixed point of \ud835\udc39. We will use the above result to reason about Markov decision processes. However, doing so requires first setting up an ordered metric space on the function space \ud835\udc34 \u2192 R where \ud835\udc34 is a finite set \u273f. 2.3 The function space \ud835\udc34 \u2192 R. Let \ud835\udc34 be a finite set \u273f. We endow the function space \ud835\udc34 \u2192 R with a natural vector space structure and with \ud835\udc3f\u221e norm \u273f: \u2225\ud835\udc53 \u2225\u221e = max \ud835\udc4e\u2208\ud835\udc34 |\ud835\udc53 (\ud835\udc4e)| (2) Our development establishes several important properties about this function space. The norm (2) is welldefined because \ud835\udc34 is finite and furthermore induces a metric that makes \ud835\udc34 \u2192 R a metric space \u273f. With this metric, the space of functions \ud835\udc34 \u2192 R is also complete \u273f. From R this metric inherits a pointwise order; viz., for functions \ud835\udc53 ,\ud835\udc54 : \ud835\udc34 \u2192 R, \ud835\udc53 \u2264 \ud835\udc54 \u21d0\u21d2 \u2200\ud835\udc4e \u2208 \ud835\udc34, \ud835\udc53 (\ud835\udc4e) \u2264 \ud835\udc54(\ud835\udc4e) \u273f \ud835\udc53 \u2265 \ud835\udc54 \u21d0\u21d2 \u2200\ud835\udc4e \u2208 \ud835\udc34, \ud835\udc53 (\ud835\udc4e) \u2265 \ud835\udc54(\ud835\udc4e) \u273f We also prove that the sets {\ud835\udc53 |\ud835\udc53 \u2264 \ud835\udc54} \u273f and {\ud835\udc53 |\ud835\udc53 \u2265 \ud835\udc54} \u273f are closed in the norm topology. Our formalization of the proof of closedness for these sets relies on classical reasoning. Additionally, we rely on functional extensionality to reason about equality between functions. We now have an ordered metric space structure on the function space \ud835\udc34 \u2192 R when \ud835\udc34 is finite. Constructing a contraction on this space will allow an application of Theorem 5. Once we set up a theory of Markov decision processes we will have natural examples of such a function space and contractions on it. Before doing so, we first introduce the Giry monad. Vajjha et al. 2.4 (Finitary) Giry Monad A monad structure on the category of all measurable spaces was first described by Lawvere in [Law62] and was explicitly defined by Giry in [Gir82]. This monad has since been called the Giry monad. While the construction is very general (applying to arbitrary measures on a space), for our purposes it suffices to consider finitely supported probability measures. The Giry monad for finitely supported probability measures is called the finitary Giry monad, although sometimes also goes by the more descriptive names distribution monad and convex combination monad. On a set \ud835\udc34, let \ud835\udc43(\ud835\udc34) denote the set of all finitely-supported probability measures on \ud835\udc34 \u273f. An element of \ud835\udc43(\ud835\udc34) is a list of elements of \ud835\udc34 together with probabilities. The probability assigned to an element \ud835\udc4e : \ud835\udc34 is denoted by \ud835\udc5d(\ud835\udc4e). In our development we state this as the record Record Pmf (A : Type) := mkPmf { outcomes :> list ( nonnegreal \u2217 A); sum1 : list_fst_sum outcomes = R1 }. where outcomes stores all the entries of the type A along with their atomic probabilities. The field sum1 ensures that the probabilities sum to 1. The Giry monad is defined in terms of two basic operations associated to this space: ret : \ud835\udc34 \u2192 \ud835\udc43(\ud835\udc34) \u273f \ud835\udc4e \u21a6\u2192 \ud835\udf06\ud835\udc65 : \ud835\udc34, \ud835\udeff\ud835\udc4e(\ud835\udc65) where \ud835\udeff\ud835\udc4e(\ud835\udc65) = 1 if \ud835\udc4e = \ud835\udc65 and 0 otherwise. The other basic operation is bind : \ud835\udc43(\ud835\udc34) \u2192 (\ud835\udc34 \u2192 \ud835\udc43(\ud835\udc35)) \u2192 \ud835\udc43(\ud835\udc35) \u273f bind \ud835\udc5d \ud835\udc53 = \ud835\udf06 \ud835\udc4f : \ud835\udc35, \u2211\ufe01 \ud835\udc4e\u2208\ud835\udc34 \ud835\udc53 (\ud835\udc4e)(\ud835\udc4f) \u2217 \ud835\udc5d(\ud835\udc4e) In both cases the resulting output is a probability measure. The above definition is well-defined because we only consider finitely-supported probability measures. A more general case is obtained by replacing sums with integrals. The definitions of bind and ret satisfy the following properties: bind (ret \ud835\udc65) \ud835\udc53 = ret(\ud835\udc53 (\ud835\udc65)) \u273f (3) bind \ud835\udc5d (\ud835\udf06\ud835\udc65, \ud835\udeff\ud835\udc65) = \ud835\udc5d \u273f (4) bind (bind \ud835\udc5d \ud835\udc53 ) \ud835\udc54 = bind \ud835\udc5d (\ud835\udf06\ud835\udc65, bind (\ud835\udc53 \ud835\udc65) \ud835\udc54) \u273f (5) These monad laws establish that the triple (\ud835\udc43, bind, ret) forms a monad. The Giry monad has been extensively studied and used by various authors because it has several attractive qualities that simplify (especially formal) proofs. First, the Giry monad naturally admits a denotational monadic semantics for certain probabilistic programs [RP02, JP89, \u015aGG15, APM09]. Second, it is useful for rigorously formalizing certain informal arguments in probability theory by providing a means to perform ad hoc notation overloading [TTV19]. Third, it can simplify certain constructions such as that of the product measure [EHN15]. CertRL uses the Giry monad as a substitute for the stochastic matrix associated to a Markov decision process. This is possible because the Kleisli composition of the Giry monad recovers the Chapman-Kolmogorov formula [Per18, Per19]. The Kleisli composition is the fish operator in Haskell parlance. CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq 2.4.1 Kleisli Composition. Reasoning about probabilistic processes requires composing probabilities. The ChapmanKolmogorov formula is a classical result in the theory of Markovian processes that states the probability of transition from one state to another through two steps can be obtained by summing up the probability of visiting each intermediate state. This application of the Chapman-Kolmogorov formula plays a fundamental role in the study of Markovian processes, but requires formalizing and reasoning about matrix operations. Kleisli composition provides an alternative and more elegant mechanism for reasoning about compositions of probabilistic choices. This section defines and provides an intuition for Kleisli composition. Think of \ud835\udc43(\ud835\udc34) as the random elements of \ud835\udc34 ([Per18, page 15]). In this paradigm, the set of maps \ud835\udc34 \u2192 \ud835\udc43(\ud835\udc35) are simply the set of maps with a random outcome. When \ud835\udc43 is a monad, such maps are called Kleisli arrows of \ud835\udc43. In terms of reinforcement learning, a map \ud835\udc53 : \ud835\udc34 \u2192 \ud835\udc43(\ud835\udc35) is a rule which takes a state \ud835\udc4e : \ud835\udc34 and gives the probability of transitioning to state \ud835\udc4f : \ud835\udc35. Suppose now that we have another such rule \ud835\udc54 : \ud835\udc35 \u2192 \ud835\udc43(\ud835\udc36). Kleisli composition puts \ud835\udc53 and \ud835\udc54 together to give a map (\ud835\udc53 \ud835\udc54) : \ud835\udc34 \u2192 \ud835\udc43(\ud835\udc36). It is defined as: \ud835\udc53 \ud835\udc54 := \ud835\udf06\ud835\udc65 : \ud835\udc34, bind (\ud835\udc53 \ud835\udc65) \ud835\udc54 (6) = \ud835\udf06\ud835\udc65 : \ud835\udc34, (\ud835\udf06\ud835\udc50 : \ud835\udc36, \u2211\ufe01 \ud835\udc4f:\ud835\udc35 \ud835\udc54(\ud835\udc4f)(\ud835\udc50) \u2217 \ud835\udc53 (\ud835\udc65)(\ud835\udc4f)) (7) = \ud835\udf06(\ud835\udc65 : \ud835\udc34) (\ud835\udc50 : \ud835\udc36), \u2211\ufe01 \ud835\udc4f:\ud835\udc35 \ud835\udc53 (\ud835\udc65)(\ud835\udc4f) \u2217 \ud835\udc54(\ud835\udc4f)(\ud835\udc50) (8) The motivation for (6)\u2013(8) is intuitive. In order to start at \ud835\udc65 : \ud835\udc34 and end up at \ud835\udc50 : \ud835\udc36 by following the rules \ud835\udc53 and \ud835\udc54, one must first pass through an intermediate state \ud835\udc4f : \ud835\udc35 in the codomain of \ud835\udc53 and the domain of \ud835\udc54. The probability of that point being any particular \ud835\udc4f : \ud835\udc35 is \ud835\udc53 (\ud835\udc65)(\ud835\udc4f) \u2217 \ud835\udc54(\ud835\udc4f)(\ud835\udc50). So, to obtain the total probability of transitioning from \ud835\udc65 to \ud835\udc50, simply sum over all intermediate states \ud835\udc4f : \ud835\udc35. This is exactly (8). We thus recover the classical Chapman-Kolmogorov formula, but as a Kleisli composition of the Giry monad. This obviates the need for reasoning about operators on linear vector spaces, thereby substantially simplifying the formalization effort. Indeed, if we did not use Kleisli composition, we would have to associate a stochastic transition matrix to our Markov process and manually prove various properties about stochastic matrices which can quickly get tedious. With Kleisli composition however, our proofs become more natural and we reason closer to the metal instead of adapting to a particular representation. 3 THE CERTRL LIBRARY CertRL contains a formalization of Markov decision processes, a definition of the Kleisli composition specialized to Markov decision processes, a definition of the long-term value of a Markov decision process, a definition of the Bellman operator, and a formalization of the operator\u2019s main properties. Building on top of its library of results about Markov decision processes, CertRL contains proofs of our main results: (1) the (infinite) sequence of value functions obtained by value iteration converges in the limit to a global optimum assuming stationary policies, (2) the (infinite) sequence of policies obtained by policy iteration converges in the limit to a global optimum assuming stationary policies, and (3) the optimal value function for Markov decision process of length \ud835\udc5b is computed inductively by application of Bellman operator, Section 3.4. The following sections describe the above results more carefully. Vajjha et al. 3.1 Markov Decision Processes We refer to [Put94] for detailed presentation of the theory of Markov decision processes. Our formalization considers the theory of infinite-horizon discounted Markov decision processes with deterministic stationary policies. We now elaborate on the above definitions and set up relevant notation. Our presentation will be type-theoretic in nature, to reflect the formal development. The exposition (and CertRL formalization) closely follows the work of Frank Feys, Helle Hvid Hansen, and Lawrence Moss [FHM18]. 3.1.1 Basic Definitions. Definition 6 (Markov Decision Process \u273f). A Markov decision process consists of the following data: \u2022 A nonempty finite type \ud835\udc46 called the set of states.2 \u2022 For each state \ud835\udc60 : \ud835\udc46, a nonempty finite type \ud835\udc34(\ud835\udc60) called the type of actions available at state \ud835\udc60. This is modelled as a dependent type. \u2022 A stochastic transition structure \ud835\udc47 : \ufffd \ud835\udc60:\ud835\udc46 (\ud835\udc34(\ud835\udc60) \u2192 \ud835\udc43(\ud835\udc46)). Here \ud835\udc43(\ud835\udc46) stands for the set of all probability measures on \ud835\udc46, as described in Section 2.4. \u2022 A reward function \ud835\udc5f : \ufffd \ud835\udc60:\ud835\udc46 (\ud835\udc34(\ud835\udc60) \u2192 \ud835\udc46 \u2192 R) where \ud835\udc5f (\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032) is the reward obtained on transition from state \ud835\udc60 to state \ud835\udc60\u2032 under action \ud835\udc4e. From these definitions it follows that the rewards are bounded in absolute value: since the state and action spaces are finite, there exists a constant \ud835\udc37 such that \u2200(\ud835\udc60 \ud835\udc60\u2032 : \ud835\udc46), (\ud835\udc4e : \ud835\udc34(\ud835\udc60)), |\ud835\udc5f (\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032)| \u2264 \ud835\udc37 \u273f (9) Definition 7 (Decision Rule / Policy). Given a Markov decision process with state space \ud835\udc46 and action space \ufffd \ud835\udc60:\ud835\udc46 \ud835\udc34(\ud835\udc60), \u2022 A function \ud835\udf0b : \ufffd \ud835\udc60:\ud835\udc46 \ud835\udc34(\ud835\udc60) is called a decision rule \u273f. The decision rule is deterministic 3. \u2022 A stationary policy is an infinite sequence of decision rules: (\ud835\udf0b, \ud835\udf0b, \ud835\udf0b, ...) \u273f. Stationary implies that the same decision rule is applies at each step. This policy \ud835\udf0b induces a stochastic dynamic process on \ud835\udc46 evolving in discrete time steps \ud835\udc58 \u2208 Z\u22650. In this section we consider only stationary policies, and therefore use the terms policy and decision rule interchangeably. 3.1.2 Kleisli Composites in a Markov Decision Process. Note that for a fixed decision rule \ud835\udf0b, we get a Kleisli arrow \ud835\udc47\ud835\udf0b : \ud835\udc46 \u2192 \ud835\udc43(\ud835\udc46) defined as \ud835\udc47\ud835\udf0b (\ud835\udc60) = \ud835\udc47 (\ud835\udc60)(\ud835\udf0b(\ud835\udc60)). Conventionally, \ud835\udc47\ud835\udf0b is represented as a row-stochastic matrix (\ud835\udc47\ud835\udf0b)\ud835\udc60\ud835\udc60\u2032 that acts on the probability co-vectors from the right, so that the row \ud835\udc60 of \ud835\udc47\ud835\udf0b corresponding to state \ud835\udc60 encodes the probability distribution of states \ud835\udc60\u2032 after a transition from the state \ud835\udc60. Let \ud835\udc5d\ud835\udc58 \u2208 \ud835\udc43(\ud835\udc46) for \ud835\udc58 \u2208 Z\u22650 denote a probability distribution on \ud835\udc46 evolving under the policy stochastic map \ud835\udc47\ud835\udf0b after \ud835\udc58 transition steps, so that \ud835\udc5d0 is the initial probability distribution on \ud835\udc46 (the initial distribution is usually taken to be ret \ud835\udc600 for a state \ud835\udc600). These are related by \ud835\udc5d\ud835\udc58 = \ud835\udc5d0\ud835\udc47 \ud835\udc58 \ud835\udf0b (10) In general (if \ud835\udc5d0 = ret \ud835\udc600) the number \ud835\udc5d\ud835\udc58 (\ud835\udc60) gives the probability that starting out at \ud835\udc600, one ends up at \ud835\udc60 after \ud835\udc58 stages. So, for example, if \ud835\udc58 = 1, we recover the stochastic transition structure at the end of the first step \u273f. 2There are various definitions of finite. Our mechanization uses surjective finiteness (the existence of a surjection from a bounded set of natural numbers)\u273f, and assumes that there is a decidable equality on \ud835\udc46. This pair of assumptions is equivalent to bijectve finitness. 3if the decision rule takes a state and returns a probability distribution on actions instead, it is called stochastic. CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq Instead of representing \ud835\udc47 \ud835\udc58 \ud835\udf0b as an iterated product of a stochastic matrix in our formalization, we recognize that (10) states that \ud835\udc5d\ud835\udc58 is the \ud835\udc58-fold iterated Kleisli composite of \ud835\udc47\ud835\udf0b applied to the initial distribution \ud835\udc5d0 \u273f. \ud835\udc5d\ud835\udc58 = (\ud835\udc5d0 \ud835\udc47\ud835\udf0b . . . \ud835\udc47\ud835\udf0b \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ud835\udc58 times ) (11) Thus, we bypass the need to define matrices and matrix multiplication entirely in the formalization. 3.1.3 Long-Term Value of a Markov Decision Process. Since the transition from one state to another by an action is governed by a probability distribution \ud835\udc47, there is a notion of expected reward with respect to that distribution. Definition 8 (Expected immediate reward). For a Markov decision process, \u2022 An expected immediate reward to be obtained in the transition under action \ud835\udc4e from state \ud835\udc60 to state \ud835\udc60\u2032 is a function \u00af\ud835\udc5f : \ud835\udc46 \u2192 \ud835\udc34 \u2192 R computed by averaging the reward function over the stochastic transition map to a new state \ud835\udc60\u2032 \u00af\ud835\udc5f (\ud835\udc60,\ud835\udc4e) := \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc5f (\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032)\ud835\udc47 (\ud835\udc60,\ud835\udc4e)(\ud835\udc60\u2032) (12) \u2022 An expected immediate reward under a decision rule \ud835\udf0b, denoted \u00af\ud835\udc5f\ud835\udf0b : \ud835\udc46 \u2192 R is defined to be: \u00af\ud835\udc5f\ud835\udf0b (\ud835\udc60) := \u00af\ud835\udc5f (\ud835\udc60, \ud835\udf0b(\ud835\udc60)) \u273f (13) That is, we replace the action argument in (12) by the action prescribed by the decision rule \ud835\udf0b. \u2022 The expected reward at time step \ud835\udc58 of a Markov decision process starting at initial state \ud835\udc60, following policy \ud835\udf0b is defined as the expected value of the reward with respect to the \ud835\udc58-th Kleisli iterate of \ud835\udc47\ud835\udf0b starting at state \ud835\udc60. \ud835\udc5f\ud835\udf0b \ud835\udc58 (\ud835\udc60) := E\ud835\udc47 \ud835\udc58\ud835\udf0b (\ud835\udc60) [\u00af\ud835\udc5f\ud835\udf0b] = \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ufffd \u00af\ud835\udc5f\ud835\udf0b (\ud835\udc60\u2032)\ud835\udc47 \ud835\udc58 \ud835\udf0b (\ud835\udc60)(\ud835\udc60\u2032) \ufffd \u273f The long-term value of a Markov decision process under a policy \ud835\udf0b is defined as follows: Definition 9 (Long-Term Value). Let \ud835\udefe \u2208 R, 0 \u2264 \ud835\udefe < 1 be a discount factor, and \ud835\udf0b = (\ud835\udf0b, \ud835\udf0b, . . . ) be a stationary policy. Then \ud835\udc49\ud835\udf0b : \ud835\udc46 \u2192 R is given by \ud835\udc49\ud835\udf0b (\ud835\udc60) = \u221e \u2211\ufe01 \ud835\udc58=0 \ud835\udefe\ud835\udc58\ud835\udc5f\ud835\udf0b \ud835\udc58 (\ud835\udc60) \u273f (14) The rewards being bounded in absolute value implies that the long-term value function \ud835\udc49\ud835\udf0b is well-defined for every initial state \u273f. It can be shown by manipulating the series in (14) that the long-term value satisfies the Bellman equation: \ud835\udc49\ud835\udf0b (\ud835\udc60) = \u00af\ud835\udc63(\ud835\udc60, \ud835\udf0b(\ud835\udc60)) + \ud835\udefe \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc49\ud835\udf0b (\ud835\udc60\u2032)\ud835\udc47\ud835\udf0b (\ud835\udc60)(\ud835\udc60\u2032) \u273f (15) = \u00af\ud835\udc5f\ud835\udf0b (\ud835\udc60) + \ud835\udefeE\ud835\udc47\ud835\udf0b (\ud835\udc60) [\ud835\udc49\ud835\udf0b] (16) Definition 10. Given a Markov decision process, we define the Bellman operator as B\ud835\udf0b :(\ud835\udc46 \u2192 R) \u2192 (\ud835\udc46 \u2192 R) (17) \ud835\udc4a \u21a6\u2192 \u00af\ud835\udc5f\ud835\udf0b (\ud835\udc60) + \ud835\udefeE\ud835\udc47\ud835\udf0b (\ud835\udc60)\ud835\udc4a (18) Theorem 11 (Properties of the Bellman Operator \u273f). The Bellman operator satisfies the following properties: \u2022 As is evident from (15), the long-term value \ud835\udc49\ud835\udf0b is the fixed point of the operator B\ud835\udf0b \u273f. \u2022 The operator B\ud835\udf0b (called the Bellman operator) is a contraction in the norm (2) \u273f. Vajjha et al. \u2022 The operator B\ud835\udf0b is a monotone operator. That is, \u2200\ud835\udc60,\ud835\udc4a1(\ud835\udc60) \u2264 \ud835\udc4a2(\ud835\udc60) \u21d2 \u2200\ud835\udc60, B\ud835\udf0b (\ud835\udc4a1)(\ud835\udc60) \u2264 B\ud835\udf0b (\ud835\udc4a2)(\ud835\udc60) The Banach fixed point theorem now says that \ud835\udc49\ud835\udf0b is the unique fixed point of this operator. Let \ud835\udc49\ud835\udf0b,\ud835\udc5b : \ud835\udc46 \u2192 R be the \ud835\udc5b-th iterate of the Bellman operator \ud835\udc35\ud835\udf0b. It can be computed by the recursion relation \ud835\udc49\ud835\udf0b,0(\ud835\udc600) = 0 \ud835\udc49\ud835\udf0b,\ud835\udc5b+1(\ud835\udc600) = \u00af\ud835\udc5f\ud835\udf0b (\ud835\udc600) + \ud835\udefeE\ud835\udc47\ud835\udf0b (\ud835\udc600)\ud835\udc49\ud835\udf0b,\ud835\udc5b \ud835\udc5b \u2208 Z\u22650 (19) where \ud835\udc600 is an arbitrary initial state. The first term in the reward function \ud835\udc49\ud835\udf0b,\ud835\udc5b+1 for the process of length \ud835\udc5b + 1 is the sum of the reward collected in the first step (immediate reward), and the remaining total reward obtained in the subsequent process of length \ud835\udc5b (discounted future reward). The \ud835\udc5b-th iterate is also seen to be equal to the \ud835\udc5b-th partial sum of the series (14) \u273f. The sequence of iterates {\ud835\udc49\ud835\udf0b,\ud835\udc5b}|\ud835\udc5b=0,1,2,... is convergent and equals \ud835\udc49\ud835\udf0b, by the Banach fixed point theorem. \ud835\udc49\ud835\udf0b = lim \ud835\udc5b\u2192\u221e\ud835\udc49\ud835\udf0b,\ud835\udc5b \u273f (20) 3.2 Convergence of Value Iteration In the previous subsection we defined the long-term value function \ud835\udc49\ud835\udf0b and showed that it is the fixed point of the Bellman operator. It is also the pointwise limit of the iterates \ud835\udc49\ud835\udf0b,\ud835\udc5b, which is the expected value of all length \ud835\udc5b realizations of the Markov decision process following a fixed stationary policy \ud835\udf0b. We note that the value function \ud835\udc49\ud835\udf0b induces a partial order on the space of all decision rules; with \ud835\udf0e \u2264 \ud835\udf0f if and only if \ud835\udc49\ud835\udf0e \u2264 \ud835\udc49\ud835\udf0f \u273f. The space of all decision rules is finite because the state and action spaces are finite \u273f. The above facts imply the existence of a decision rule (stationary policy) which maximizes the long-term reward. We call this stationary policy the optimal policy and its long-term value the optimal value function. \ud835\udc49\u2217(\ud835\udc60) = max \ud835\udf0b {\ud835\udc49\ud835\udf0b (\ud835\udc60)} \u273f (21) The aim of reinforcement learning, as we remarked in the introduction, is to have tractable algorithms to find the optimal policy and the optimal value function corresponding to the optimal policy. Bellman\u2019s value iteration algorithm is such an algorithm, which is known to converge asymptotically to the optimal value function. In this section we describe this algorithm and formally prove this convergence property. Definition 12. Given a Markov decision process we define the Bellman optimality operator as: \u02c6B :(\ud835\udc46 \u2192 R) \u2192 (\ud835\udc46 \u2192 R) \ud835\udc4a \u21a6\u2192 \ud835\udf06\ud835\udc60, max \ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) \ufffd\u00af\ud835\udc5f (\ud835\udc60,\ud835\udc4e) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udc4e) [\ud835\udc4a ]\ufffd \u273f Theorem 13. The Bellman optimality operator \u02c6B satisfies the following properties: \u2022 The operator \u02c6B is a contraction with respect to the \ud835\udc3f\u221e norm (2) \u273f. \u2022 The operator \u02c6B is a monotone operator. That is, \u2200\ud835\udc60,\ud835\udc4a1(\ud835\udc60) \u2264 \ud835\udc4a2(\ud835\udc60) \u21d2 \u2200\ud835\udc60, \u02c6B(\ud835\udc4a1)(\ud835\udc60) \u2264 \u02c6B(\ud835\udc4a2)(\ud835\udc60) \u273f Now we move on to proving the most important property of \u02c6B: the optimal value function \ud835\udc49\u2217 is a fixed point of \u02c6B. By Theorem 13 and the Banach fixed point theorem, we know that the fixed point of \u02c6B exists. Let us denote it \u02c6\ud835\udc49 . Then we have: CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq Theorem 14 (Lemma 1 of [FHM18] \u273f). For every decision rule \ud835\udf0e, we have \ud835\udc49\ud835\udf0e \u2264 \u02c6\ud835\udc49 . Proof. Fix a policy \ud835\udf0e. Note that for every \ud835\udc53 : \ud835\udc46 \u2192 R, we have B\ud835\udf0e (\ud835\udc53 ) \u2264 \u02c6B(\ud835\udc53 )\u273f. In particular, applying this to \ud835\udc53 = \ud835\udc49\ud835\udf0e and using Theorem 11, we get that \ud835\udc49\ud835\udf0e = B\ud835\udf0e (\ud835\udc49\ud835\udf0e) \u2264 \u02c6B(\ud835\udc49\ud835\udf0e). Now by contraction coinduction (Theorem 5 with \ud835\udc39 = \u02c6B along with Theorem 13) we get that \ud835\udc49\ud835\udf0e \u2264 \u02c6\ud835\udc49 . \u25a1 Theorem 14 immediately implies that \ud835\udc49\u2217 \u2264 \u02c6\ud835\udc49 . To go the other way, we introduce the following policy, called the greedy decision rule. \ud835\udf0e\u2217(\ud835\udc60) := argmax\ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) \ufffd \u00af\ud835\udc5f (\ud835\udc4e,\ud835\udc60) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udc4e) [ \u02c6\ud835\udc49 ] \ufffd \u273f (22) We now have the following theorem: Theorem 15 (Proposition 1 of [FHM18] \u273f). The greedy policy is the policy whose long-term value is the fixed point of \u02c6B: \ud835\udc49\ud835\udf0e\u2217 = \u02c6\ud835\udc49 Proof. We observe that B\ud835\udf0e\u2217( \u02c6\ud835\udc49 ) = \u02c6\ud835\udc49 \u273f. Thus, \u02c6\ud835\udc49 \u2264 B\ud835\udf0e\u2217( \u02c6\ud835\udc49 ). Note that we have \ud835\udc49\ud835\udf0e\u2217 is the fixed point of \ud835\udc35\ud835\udf0e\u2217 by Theorem 11. Now applying contraction coinduction with \ud835\udc39 = B\ud835\udf0e\u2217, we get \u02c6\ud835\udc49 \u2264 \ud835\udc49\ud835\udf0e\u2217. From Theorem 14 we get that \ud835\udc49\ud835\udf0e\u2217 \u2264 \u02c6\ud835\udc49 . \u25a1 Theorem 15 implies that \ud835\udc49\u2217 \u2265 \u02c6\ud835\udc49 and so we conclude that \ud835\udc49\u2217 = \u02c6\ud835\udc49 \u273f. Thus, the fixed point of the optimal Bellman operator \u02c6B exists and is equal to the optimal value function. Stated fully, value iteration proceeds by: (1) Initialize a value function \ud835\udc490 : \ud835\udc46 \u2192 R. (2) Define \ud835\udc49\ud835\udc5b+1 = \u02c6B\ud835\udc49\ud835\udc5b for \ud835\udc5b \u2265 0. At each stage, the following policy is computed \ud835\udf0b\ud835\udc5b(\ud835\udc60) \u2208 argmax\ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) \ufffd\u00af\ud835\udc5f (\ud835\udc60,\ud835\udc4e) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udc4e) [\ud835\udc49\ud835\udc5b]\ufffd By the Banach Fixed Point Theorem, the sequence {\ud835\udc49\ud835\udc5b} converges to the optimal value function \ud835\udc49\u2217 \u273f. In practice, one repeats this iteration as many times as needed until a fixed threshold is breached. In Section 3.4 we explain and provide a formalized proof of the dynamic programming principle: the value function\ud835\udc49\ud835\udc5b is equal to the optimal value function of a finite-horizon MDP of length\ud835\udc5b with a possibly non-stationary optimal policy. 3.3 Convergence of Policy Iteration The convergence of value iteration is asymptotic, which means the iteration is continued until a fixed threshold is breached. Policy iteration is a similar iterative algorithm that benefits from a more definite stopping condition. Define the \ud835\udc44 function to be: \ud835\udc44\ud835\udf0b (\ud835\udc60,\ud835\udc4e) := \u00af\ud835\udc5f (\ud835\udc60,\ud835\udc4e) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udc4e) [\ud835\udc49\ud835\udf0b]. The policy iteration algorithm proceeds in the following steps: (1) Initialize the policy to \ud835\udf0b0. (2) Policy evaluation: For \ud835\udc5b \u2265 0, given \ud835\udf0b\ud835\udc5b, compute \ud835\udc49\ud835\udf0b\ud835\udc5b. (3) Policy improvement: From \ud835\udc49\ud835\udf0b\ud835\udc5b, compute the greedy policy: \ud835\udf0b\ud835\udc5b+1(\ud835\udc60) \u2208 argmax\ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) \ufffd \ud835\udc44\ud835\udf0b\ud835\udc5b (\ud835\udc60,\ud835\udc4e) \ufffd (4) Check if \ud835\udc49\ud835\udf0b\ud835\udc5b = \ud835\udc49\ud835\udf0b\ud835\udc5b+1. If yes, stop. (5) If not, repeat (2) and (3). This algorithm depends on the following results for correctness. We follow the presentation from [FHM18]. Vajjha et al. Definition 16 (Improved policy \u273f). A policy \ud835\udf0f is called an improvement of a policy \ud835\udf0e if for all \ud835\udc60 \u2208 \ud835\udc46 it holds that \ud835\udf0f(\ud835\udc60) = argmax\ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) [\ud835\udc44\ud835\udf0e (\ud835\udc60,\ud835\udc4e)]] So, step (2) of the policy iteration algorithm simply constructs an improved policy from the previous policy at each stage. Theorem 17 (Policy Improvement Theorem). Let \ud835\udf0e and \ud835\udf0f be two policies. \u2022 If B\ud835\udf0f\ud835\udc49\ud835\udf0e \u2265 B\ud835\udf0e\ud835\udc49\ud835\udf0e then \ud835\udc49\ud835\udf0f \u2265 \ud835\udc49\ud835\udf0e \u273f. \u2022 If B\ud835\udf0f\ud835\udc49\ud835\udf0e \u2264 B\ud835\udf0e\ud835\udc49\ud835\udf0e then \ud835\udc49\ud835\udf0f \u2264 \ud835\udc49\ud835\udf0e \u273f. Using the above theorem, we have: Theorem 18 (Policy Improvement Improves Values \u273f). If \ud835\udf0e and \ud835\udf0f are two policies and if \ud835\udf0f is an improvement of \ud835\udf0e, then we have \ud835\udc49\ud835\udf0f \u2265 \ud835\udc49\ud835\udf0e. Proof. From Theorem 17, it is enough to show B\ud835\udf0f\ud835\udc49\ud835\udf0e \u2265 B\ud835\udf0e\ud835\udc49\ud835\udf0e. We have that \ud835\udf0f is an improvement of \ud835\udf0e. \ud835\udf0f(\ud835\udc60) = argmax\ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) [\ud835\udc44\ud835\udf0e (\ud835\udc60,\ud835\udc4e)] (23) = argmax\ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) \ufffd\u00af\ud835\udc5f (\ud835\udc60,\ud835\udc4e) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udc4e) [\ud835\udc49\ud835\udf0e] \ufffd (24) Note that B\ud835\udf0f\ud835\udc49\ud835\udf0e = \u00af\ud835\udc5f (\ud835\udc60,\ud835\udf0f(\ud835\udc60)) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udf0f (\ud835\udc60)) [\ud835\udc49\ud835\udf0e] = max \ud835\udc4e\u2208\ud835\udc34(\ud835\udc60) \ufffd\u00af\ud835\udc5f (\ud835\udc60,\ud835\udc4e) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udc4e) [\ud835\udc49\ud835\udf0e] \ufffd by (24) \u2265 \u00af\ud835\udc5f (\ud835\udc60, \ud835\udf0e(\ud835\udc60)) + \ud835\udefeE\ud835\udc47 (\ud835\udc60,\ud835\udf0e (\ud835\udc60)) [\ud835\udc49\ud835\udf0e] = B\ud835\udf0e\ud835\udc49\ud835\udf0e \u25a1 In other words, since \ud835\udf0b\ud835\udc5b+1 is an improvement of \ud835\udf0b\ud835\udc5b by construction, the above theorem implies that\ud835\udc49\ud835\udf0b\ud835\udc5b \u2264 \ud835\udc49\ud835\udf0b\ud835\udc5b+1. This means that \ud835\udf0b\ud835\udc5b \u2264 \ud835\udf0b\ud835\udc5b+1. Thus, the policy constructed in each stage in the policy iteration algorithm is an improvement of the policy in the previous stage. Since the set of policies is finite \u273f, this policy list must at some point stabilize. Thus, the algorithm is guaranteed to terminate. In Section 3.4 we will provide formalization of the statement that \ud835\udf0b\ud835\udc5b is actually the optimal policy to follow for an MDP process of any finite length at that timestep when \ud835\udc5b steps remain towards the end of the process. 3.4 Optimal policy for finite time horizon Markov decision processes All results up to this subsection were stated in terms of the convergences of infinite sequences of states and actions. Stating convergence results in terms of the limits of infinite sequences is not uncommon in texts on reinforcement learning; however, in practice, reinforcement learning algorithms are always run for a finite number of steps. In this section we consider decision processes of finite length and do not impose an assumption that the optimal policy is stationary. Let \ud835\udc49 \u00af\ud835\udf0b denote the value function of Markov decision process for a finite sequence of policies \u00af\ud835\udf0b = \ud835\udf0b0 :: \ud835\udf0b1 :: \ud835\udf0b2 :: . . . \ud835\udf0b\ud835\udc5b\u22121 of length \ud835\udc5b = len( \u00af\ud835\udf0b). Denote by \ud835\udc5d0 the probability distribution over the initial state at the start of the process. We define the probability measure at step \ud835\udc58 in terms of Kleisli iterates for each decision rule \ud835\udf0b\ud835\udc56 for \ud835\udc56 in 0 . . . (\ud835\udc58\u22121): \ud835\udc5d0\ud835\udc47 \u00af\ud835\udf0b [:\ud835\udc58] := (\ud835\udc5d0 \ud835\udc47\ud835\udf0b0 . . . \ud835\udc47\ud835\udf0b\ud835\udc58\u22121) \u273f (25) CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq Below, we will use the pairing notation (bra-ket notation) \u27e8\ud835\udc5d|\ud835\udc49 \u27e9 := E\ud835\udc5d [\ud835\udc49 ] (26) between a probability measure \ud835\udc5d on a finite set \ud835\udc46 and a function \ud835\udc49 : \ud835\udc46 \u2192 R, so that |\ud835\udc49 \u27e9 is an element of the vector space of real valued functions on \ud835\udc46, \u27e8\ud835\udc5d| is a linear form on this vector space associated to a probablity measure \ud835\udc5d on \ud835\udc46, and \u27e8\ud835\udc5d|\ud835\udc49 \u27e9 denotes evaluation of a linear form \u27e8\ud835\udc5d| on a vector |\ud835\udc49 \u27e9. Definition 19 (expectation value function of MDP of length \ud835\udc5b = len( \u00af\ud835\udf0b) over the initial probability distribution \ud835\udc5d0). \u27e8\ud835\udc5d0|\ud835\udc49 \u00af\ud835\udf0b\u27e9 = \ud835\udc5b\u22121 \u2211\ufe01 \ud835\udc58=0 \ud835\udefe\ud835\udc58\u27e8\ud835\udc5d0\ud835\udc47 \u00af\ud835\udf0b [:\ud835\udc58]|\u00af\ud835\udc5f\ud835\udf0b\ud835\udc58\u27e9 \u273f (27) Definition 19 implies the recursion relation \u27e8\ud835\udc5d0|\ud835\udc49\ud835\udf0b0::tail\u27e9 = \u27e8\ud835\udc5d0|\u00af\ud835\udc5f\ud835\udf0b0 + \ud835\udefe\ud835\udc47\ud835\udf0b0\ud835\udc49tail\u27e9 \u273f \ud835\udc5b \u2208 Z\u22650 (28) where \u00af\ud835\udf0b = \ud835\udf0b0 :: tail. Let \u02c6\ud835\udc49\u2217,\ud835\udc5b be the optimal value function of the Markov decision process of length \ud835\udc5b on the space of all policy sequences of length \ud835\udc5b: \u02c6\ud835\udc49\u2217,\ud835\udc5b := sup \u00af\ud835\udf0b |len(\ud835\udf0b)=\ud835\udc5b \ud835\udc49 \u00af\ud835\udf0b \u273f (29) Let \u02c6\ud835\udc49\ud835\udf0b0::\u2217,\ud835\udc5b+1 be the optimal value function of the Markov decision process of length \ud835\udc5b + 1 on the space of all policy sequences of length \ud835\udc5b + 1 whose initial term is \ud835\udf0b0. Using the relation (28) and that sup \ud835\udf0b0::tail \ud835\udc49\ud835\udf0b0::tail,\ud835\udc5b+1 = sup \ud835\udf0b0 sup tail \ud835\udc49\ud835\udf0b0::tail,\ud835\udc5b+1 \u273f (30) we find \u27e8\ud835\udc5d0| \u02c6\ud835\udc49\u2217,\ud835\udc5b+1\u27e9 = sup \ud835\udf0b0\u2208\ufffd \ud835\udc46 \ud835\udc34(\ud835\udc60) \u27e8\ud835\udc5d0|\u00af\ud835\udc5f\ud835\udf0b0 + \ud835\udefe\ud835\udc47\ud835\udf0b0 \u02c6\ud835\udc49\u2217,\ud835\udc5b\u27e9 \u273f \ud835\udc5b \u2208 Z>=0 (31) with the initial term of the sequence \ud835\udc49\u2217,0 = 0. The result (31) can be formulated as follows Theorem 20 (Bellman\u2019s finite-time optimal policy theorem). The optimal value function \u02c6\ud835\udc49\u2217,\ud835\udc5b+1 of a Markov decision process of length \ud835\udc5b + 1 relates to the optimal value function of the same Markov decision process of length \ud835\udc5b by the inductive relation \u02c6\ud835\udc49\u2217,\ud835\udc5b+1 = \u02c6B\ud835\udc49\u2217,\ud835\udc5b (32) where \u02c6B is Bellman optimality operator (Definition 12). The iterative computation of the sequence of optimal value functions { \u02c6\ud835\udc49\u2217,\ud835\udc5b}\ud835\udc5b\u2208Z\u22650 of Markov decision processes of length \ud835\udc5b = 0, 1, 2, . . . from the recursion \u02c6\ud835\udc49\u2217,\ud835\udc5b+1 = \u02c6B \u02c6\ud835\udc49\u2217,\ud835\udc5b is the same algorithm as value iteration. 3.5 Comments on Formalization CertRL contributes a formal library for reasoning about Markov decision processes. We demonstrate the effectiveness of this library\u2019s building blocks by proving the two most canonical results from reinforcement learning theory. In this section we reflect on the structure of CertRL\u2019s formalization, substantiating our claim that CertRL serves as a convenient foundations for a continuing line of work on formalization of reinforcement learning theory. Vajjha et al. 3.5.1 Characterizing Optimality. Most texts on Markov decision processses (for example [Put94, Section 2.1.6]) start out with a probability space on the space of all possible realizations of the Markov decision process. The long-term value for an infinite-horizon Markov decision process is then defined as the expected value over all possible realizations: \ud835\udc49\ud835\udf0b (\ud835\udc60) = E(\ud835\udc651,\ud835\udc652,... ) \ufffd \u221e \u2211\ufe01 \ud835\udc58=0 \ud835\udefe\ud835\udc58\ud835\udc63(\ud835\udc65\ud835\udc58, \ud835\udf0b(\ud835\udc65\ud835\udc58))|\ud835\udc650 = \ud835\udc60; \ud835\udf0b \ufffd (33) where each \ud835\udc65\ud835\udc58 is drawn from the distribution \ud835\udc47 (\ud835\udc65\ud835\udc58\u22121, \ud835\udf0b(\ud835\udc65\ud835\udc58\u22121)). This definition is hard to work with because, as [Put94] notes, it ignores the dynamics of the problem. Fortunately, it is also unnecessary since statements about the totality of all realizations are rarely made. In our setup, following [FHM18], we only consider the probability space over the finite set of states of the Markov decision process. By identifying the basic operation of Kleisli composition, we generate more realizations (and their expected rewards) on the fly as and when needed. Implementations of reinforcement learning algorithms often compute the long-term value using matrix operators for efficiency reasons. The observation that clean theoretical tools do not necessarily entail efficient implementations is not a new observation; both Puterman [Put94] and H\u00f6lzl [Hoe17a] make similar remarks. Fortunately, the design of our library provides a clean interface for future work on formalizing efficiency improvements. Extending CertRL with correctness theorems for algorithms that use matrix operations requires nothing more than a proof that the relevant matrix operations satisfy the definition of Kleisli composition. 3.5.2 Comparison of English and Coq Proofs. Comparing Theorem 14 and Theorem 15 with the the equivalent results from Puterman [Put94, Theorem 6.2.2] demonstrates that CertRL avoids reasoning about low-level \ud835\udf16 \u2212 \ud835\udeff details through strategic use of coinduction. The usefulness of contraction coinduction is reflected in the formalization, sometimes resulting in Coq proofs whose length is almost the same as the English text. We compare in Table 1 the Coq proof of Theorem 15 to an English proof of the same. The two proofs are roughly equivalent in length and, crucially, also make essentially the same argument at the same level of abstraction. Note that what we compare is not exactly the proof from Feys et al. [FHM18, Proposition 1], but is as close as possible to a restatement of their Proposition 1 and Lemma 1 with the proof of Lemma 1 inlined and the construction restated in terms of our development. The full proof from [FHM18], with Lemma 1 inlined, reads as follows: Proposition 1: The greedy policy is optimal. That is, \ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e\u2217 = \ud835\udc49 \u2217. (1) Observe that \u03a8\ud835\udf0e\u2217 \u2265 \ud835\udc49 \u2217 (in fact, equality holds). (2) By contraction coinduction, \ud835\udc49 \u2217 \u2264 \ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e\u2217. (3) Lemma 1: For all policies \ud835\udf0e, \ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e \u2264 \ud835\udc49 \u2217. (4) A straightforward calculation and monotonicity argument shows that for all \ud835\udc53 \u2208 \ud835\udc35(\ud835\udc46, R), \u03a8\ud835\udf0e (\ud835\udc53 ) \u2264 \u03a8\u2217(\ud835\udc53 ). (5) In particular, \ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e = \u03a8\ud835\udf0e (\ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e) \u2264 \u03a8\u2217(\ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e). (6) By contraction coinduction we conclude that \ud835\udc3f\ud835\udc47\ud835\udc49\ud835\udf0e \u2264 \ud835\udc49 \u2217. Table 1 compares two coinductive proofs \u2013 one in English and the other in Coq. Another important comparison is between a Coq coinductive proof and an English non-coinductive proof. The Coq proof of the policy improvement theorem provides one such point of comparison. Recall that theorem states that a particular closed property (the set {\ud835\udc65|\ud835\udc65 \u2264 \ud835\udc66}) holds of the fixed point of a particular contractive map (the Bellman operator). The most common argument \u2013 presented in the most common textbook on reinforcement learning \u2013 proves this theorem by expanding the infinite sum in multiple steps [SB98, Section 4.2]. We reproduce this below: CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq Theorem 15 (Proposition 1 of [FHM18] \u273f). The greedy policy is the policy whose long-term value is the fixed point of \u02c6B: \ud835\udc49\ud835\udf0e\u2217 = \u02c6\ud835\udc49 Proof. (1) \ud835\udc49\ud835\udf0e\u2217 \u2264 \u02c6\ud835\udc49 follows by Theorem 14. (2) Now we have to show \u02c6\ud835\udc49 \u2264 \ud835\udc49\ud835\udf0e\u2217. Note that we have \ud835\udc49\ud835\udf0e\u2217 is the fixed point of \ud835\udc35\ud835\udf0e\u2217 by Theorem 11. (3) We can now apply contraction coinduction with \ud835\udc39 = B\ud835\udf0e\u2217. (4) The hypotheses are satisfied since by Theorem 11, the B\ud835\udf0e\u2217 is a contraction and it is a monotone operator. (5) The only hypothesis left to show is \u02c6\ud835\udc49 \u2264 B\ud835\udf0e\u2217 \u02c6\ud835\udc49 . (6) But in fact, we have B\ud835\udf0e\u2217( \u02c6\ud835\udc49 ) = \u02c6\ud835\udc49 by the definition of \ud835\udf0e\u2217. \u25a1 (a) English proof adapted from [FHM18]. 1 Lemma exists_fixpt_policy : forall init, 2 let V' := fixpt (bellman_max_op) in 3 let pi' := greedy init in 4 ltv gamma pi' = V' init. 5 Proof. 6 intros init V' pi'; 7 eapply Rfct_le_antisym; split. 8 \u2212 eapply ltv_Rfct_le_fixpt. 9 \u2212 rewrite (ltv_bellman_op_fixpt _ init). 10 apply contraction_coinduction_Rfct_ge'. 11 + apply is_contraction_bellman_op. 12 + apply bellman_op_monotone_ge. 13 + unfold V', pi'. 14 now rewrite greedy_argmax_is_max. 15 Qed. (b) Coq proof \u273f Table 1. Comparison of English and Coq proofs of Theorem 15. Theorem 21 (Policy Improvement Theorem [SB98]). Let \ud835\udf0b, \ud835\udf0b \u2032 be a pair of deterministic policies such that, for all states \ud835\udc60, \ud835\udc44\ud835\udf0b (\ud835\udc60, \ud835\udf0b \u2032(\ud835\udc60)) \u2265 \ud835\udc49\ud835\udf0b (\ud835\udc60) (34) then \ud835\udc49\ud835\udf0b\u2032(\ud835\udc60) \u2265 \ud835\udc49\ud835\udf0b (\ud835\udc60). Proof. Starting with (34) we keep expanding the \ud835\udc44\ud835\udf0b side and reapplying (34) until we get \ud835\udc49\ud835\udf0b\u2032(\ud835\udc60). \ud835\udc49\ud835\udf0b (\ud835\udc60) \u2264 \ud835\udc44\ud835\udf0b (\ud835\udc60, \ud835\udf0b \u2032(\ud835\udc60)) = E\ud835\udf0b\u2032 {\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc49\ud835\udf0b (\ud835\udc60\ud835\udc61+1)|\ud835\udc60\ud835\udc61 = \ud835\udc60} \u2264 E\ud835\udf0b\u2032 {\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44\ud835\udf0b (\ud835\udc60\ud835\udc61+1, \ud835\udf0b \u2032(\ud835\udc60\ud835\udc61+1))|\ud835\udc60\ud835\udc61 = \ud835\udc60} = E\ud835\udf0b\u2032 {\ud835\udc5f\ud835\udc61+1 + \ud835\udefeE\ud835\udf0b\u2032 {\ud835\udc5f\ud835\udc61+2 + \ud835\udefe\ud835\udc49\ud835\udf0b (\ud835\udc60\ud835\udc61+2)} |\ud835\udc60\ud835\udc61 = \ud835\udc60} = E\ud835\udf0b\u2032 \ufffd \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5f\ud835\udc61+2 + \ud835\udefe2\ud835\udc49\ud835\udf0b (\ud835\udc60\ud835\udc61+2) | \ud835\udc60\ud835\udc61 = \ud835\udc60 \ufffd \u2264 E\ud835\udf0b\u2032 \ufffd \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5f\ud835\udc61+2 + \ud835\udefe2\ud835\udc5f\ud835\udc61+3 + \ud835\udefe3\ud835\udc49\ud835\udf0b (\ud835\udc60\ud835\udc61+3) | \ud835\udc60\ud835\udc61 = \ud835\udc60 \ufffd ... \u2264 E\ud835\udf0b\u2032 \ufffd \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5f\ud835\udc61+2 + \ud835\udefe2\ud835\udc5f\ud835\udc61+3 + \ud835\udefe3\ud835\udc5f\ud835\udc61+4 . . . | \ud835\udc60\ud835\udc61 = \ud835\udc60 \ufffd = \ud835\udc49\ud835\udf0b\u2032(\ud835\udc60) \u25a1 Vajjha et al. At a high level, this proof proceeds by showing that the closed property {\ud835\udc65|\ud835\udc65 \u2264 \ud835\udc66} holds of each partial sum of the infinite series \ud835\udc49\ud835\udf0b (\ud835\udc60). By completeness and using the fact that partial sums converge to the full series \ud835\udc49\ud835\udf0b, this property is also shown to hold of the fixed point \ud835\udc49\ud835\udf0b. However, the coinductive version of this proof (Theorem 17) is simpler because it exploits the fact that this construction has already been done once in the proof of the fixed point theorem: the iterates of the contraction operator were already proven to converge to the fixed point and so there is no reason to repeat the construction again. Thus, the proof is reduced to simply establishing the \u201cbase case\u201d of the (co)induction. The power of this method goes beyond simplifying proofs for Markov decision processes. See [KR09] for other examples. 4 RELATED AND FUTURE WORK To our knowledge, CertRL is the first formal proof of convergence for value iteration or policy iteration. Related work falls into three categories: (1) libraries that CertRL builds upon, (2) formalizations of results from probability and machine learning, and (3) work at the intersection of formal verification and reinforcement learning. Dependencies. CertRL builds on the Coquelicot [BLM14] library for real analysis. Our main results are statements about fixed points of contractive maps in complete normed modules. CertRL therefore builds on the formal development of the Lax-Milgram theorem and, in particular, Boldo et al.\u2019s formal proof of the Banach fixed point theorem [BCF+17]. CertRL also makes extensive use of some utilities from the Q*cert project [AHM+17]. CertRL includes a bespoke implementation of some basic results and constructions from probability theory and also an implementation of the Giry monad. Our use of the monad for reasoning about probabilistic processes, as well as the design of our library, is highly motivated by the design of the Polaris library [TH19]. Many of the thorough formalizations of probabilities in Coq \u2013 such as the Polaris [TH19], Infotheo [AH12], and Alea [APM09] \u2013 also contain these results. Refactoring CertRL to build on top of one or more of these formalizations might allow future work on certified reinforcement learning to leverage future improvements to these libraries. Building on these other foundations, CertRL demonstrates how existing work on formalization enables formalization of key results in reinforcement learning theory. Related Formalizations. There is a growing body of work on formalization of machine learning theory [TTV19, TTV+20, Hoe17b, SLD17, BS19, BBK19]. Johannes H\u00f6lzl\u2019s Isabelle/HOL development of Markov processes is most related to our own work [Hoe17b, Hoe17a]. H\u00f6lzl builds on the probability theory libraries of Isabelle/HOL to develop continuous-time Markov chains. Many of H\u00f6lzl\u2019s basic design choices are similar to ours; for example, he also uses the Giry monad to place a monadic structure on probability spaces and also uses coinductive methods. CertRL focuses instead on formalization of convergence proofs for dynamic programming algorithms that solve Markov decision processes. In the future, we plan to extend our formalization to include convergence proofs for model-free methods, in which a fixed Markov decision process is not known a priori. The CertiGrad formalization by Selsam et al. contains a Lean proof that the gradients sampled by a stochastic computation graph are unbiased estimators of the true mathematical function [SLD17]. This result, together with our development of a library for proving convergence of reinforcement learning algorithms, provides a path toward a formal proof of correctness for deep reinforcement learning. Formal Methods for RL. The likelihood that reinforcement learning algorithms will be deployed in safetycritical settings during the coming decades motivates a growing body of work on formal methods for safe reinforcement learning. This approach \u2013 variously called formally constrained reinforcement learning [HAK18], ",
    "References": "REFERENCES [ABE+18] Mohammed Alshiekh, Roderick Bloem, R\u00fcdiger Ehlers, Bettina K\u00f6nighofer, Scott Niekum, and Ufuk Topcu. Safe reinforcement learning via shielding. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018. [AH12] Reynald Affeldt and Manabu Hagiwara. Formalization of Shannon\u2019s theorems in SSReflect-Coq. In 3rd Conference on Interactive Theorem Proving (ITP 2012), Princeton, New Jersey, USA, August 13\u201315, 2012, volume 7406 of Lecture Notes in Computer Science, pages 233\u2013249. Springer, Aug 2012. [AHM+17] Joshua S. Auerbach, Martin Hirzel, Louis Mandel, Avraham Shinnar, and J\u00e9r\u00f4me Sim\u00e9on. Q*cert: A platform for implementing and verifying query compilers. In Semih Salihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, and Dan Suciu, editors, Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017, pages 1703\u20131706. ACM, 2017. [APM09] Philippe Audebaud and Christine Paulin-Mohring. Proofs of randomized algorithms in Coq. Science of Computer Programming, 74(8):568\u2013589, 2009. [BBK19] Alexander Bentkamp, Jasmin Christian Blanchette, and Dietrich Klakow. A formal proof of the expressiveness of deep learning. J. Autom. Reason., 63(2):347\u2013368, 2019. [BCF+17] Sylvie Boldo, Fran\u00e7ois Cl\u00e9ment, Florian Faissole, Vincent Martin, and Micaela Mayero. A Coq formal proof of the Lax\u2013Milgram theorem. In 6th ACM SIGPLAN Conference on Certified Programs and Proofs, Paris, France, January 2017. [Bel54] Richard Bellman. The theory of dynamic programming. Bull. Amer. Math. Soc., 60(6):503\u2013515, 11 1954. [BLM14] Sylvie Boldo, Catherine Lelay, and Guillaume Melquiond. Coquelicot: A user-friendly library of real analysis for Coq. Mathematics in Computer Science, 9, 03 2014. [BS19] Alexander Bagnall and Gordon Stewart. Certifying the true error: Machine learning in Coq with verified generalization guarantees. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, pages 2662\u20132669. AAAI Press, 2019. [BTM+18] Brandon Bohrer, Yong Kiam Tan, Stefan Mitsch, Magnus O. Myreen, and Andr\u00e9 Platzer. VeriPhy: Verified controller executables from verified cyber-physical system models. In Dan Grossman, editor, Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2018), pages 617\u2013630. ACM, 2018. [EHN15] Manuel Eberl, Johannes H\u00f6lzl, and Tobias Nipkow. A verified compiler for probability density functions. In Jan Vitek, editor, ESOP 2015, volume 9032 of LNCS, pages 80\u2013104. Springer, 2015. [FHM18] Frank MV Feys, Helle Hvid Hansen, and Lawrence S Moss. Long-term values in Markov decision processes, (co)algebraically. In International Workshop on Coalgebraic Methods in Computer Science, pages 78\u201399. Springer, 2018. Vajjha et al. [FP18] Nathan Fulton and Andr\u00e9 Platzer. Safe reinforcement learning via formal methods: Toward safe control through proof and learning. In Sheila McIlraith and Kilian Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018), pages 6485\u20136492. AAAI Press, 2018. [GHLL16] Shixiang Gu, Ethan Holly, Timothy P. Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation. CoRR, abs/1610.00633, 2016. [Gir82] Mich\u00e8le Giry. A categorical approach to probability theory. In B. Banaschewski, editor, Categorical Aspects of Topology and Analysis, pages 68\u201385, Berlin, Heidelberg, 1982. Springer Berlin Heidelberg. [HAK18] Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Logically-correct reinforcement learning. CoRR, abs/1801.08099, 2018. [HFM+20] Nathan Hunt, N. Fulton, Sara Magliacane, N. Ho\u00e0ng, Subhro Das, and Armando Solar-Lezama. Verifiably safe exploration for end-to-end reinforcement learning. ArXiv, abs/2007.01223, 2020. [Hoe17a] Johannes Hoelzl. Markov chains and Markov decision processes in Isabelle/HOL. Journal of Automated Reasoning, 2017. [Hoe17b] Johannes Hoelzl. Markov processes in Isabelle/HOL. In Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs and Proofs, CPP 2017, page 100\u2013111, New York, NY, USA, 2017. Association for Computing Machinery. [How60] R.A. Howard. Dynamic Programming and Markov Processes. Technology Press of Massachusetts Institute of Technology, 1960. [Jac18] Bart Jacobs. From probability monads to commutative effectuses. Journal of Logical and Algebraic Methods in Programming, 94:200 \u2013 237, 2018. [JP89] C. Jones and Gordon D. Plotkin. A probabilistic powerdomain of evaluations. In Proceedings of the Fourth Annual Symposium on Logic in Computer Science (LICS \u201989), Pacific Grove, California, USA, June 5-8, 1989, pages 186\u2013195. IEEE Computer Society, 1989. [Koz07] Dexter Kozen. Coinductive proof principles for stochastic processes. CoRR, abs/0711.0194, 2007. [KR09] Dexter Kozen and Nicholas Ruozzi. Applications of metric coinduction. Log. Methods Comput. Sci., 5(3), 2009. [Law62] F William Lawvere. The category of probabilistic mappings. preprint, 1962. [Ope18] OpenAI. OpenAI five. https://blog.openai.com/openai-five/, 2018. [Per18] Paolo Perrone. Categorical Probability and Stochastic Dominance in Metric Spaces. PhD thesis, University of Leipzig, 2018. [Per19] Paolo Perrone. Notes on category theory with examples from basic mathematics, 2019. [Put94] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley and Sons, Inc., USA, 1st edition, 1994. [RP02] Norman Ramsey and Avi Pfeffer. Stochastic lambda calculus and monads of probability distributions. In John Launchbury and John C. Mitchell, editors, Conference Record of POPL 2002: The 29th SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Portland, OR, USA, January 16-18, 2002, pages 154\u2013165. ACM, 2002. [SB98] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. [SEJ+20] Andrew Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \u017d\u00eddek, Alexander Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using potentials from deep learning. Nature, 577:1\u20135, 01 2020. [\u015aGG15] Adam \u015acibior, Zoubin Ghahramani, and Andrew D. Gordon. Practical probabilistic programming with monads. In Ben Lippmeier, editor, Proceedings of the 8th ACM SIGPLAN Symposium on Haskell, Haskell 2015, Vancouver, BC, Canada, September 3-4, 2015, pages 165\u2013176. ACM, 2015. [SHM+16] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, January 2016. [SLD17] Daniel Selsam, Percy Liang, and David L. Dill. Developing bug-free machine learning systems with formal mathematics. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3047\u20133056. PMLR, 2017. [Tea04] The Coq Development Team. The Coq Proof Assistant Reference Manual. LogiCal Project, 2004. Version 8.0. [TH19] Joseph Tassarotti and Robert Harper. A separation logic for concurrent randomized programs. Proceedings of the ACM on Programming Languages, 3(POPL):1\u201330, 2019. [TTV19] Joseph Tassarotti, Jean-Baptiste Tristan, and Koundinya Vajjha. A formal proof of PAC learnability for decision stumps. CoRR, abs/1911.00385, 2019. [TTV+20] Jean-Baptiste Tristan, Joseph Tassarotti, Koundinya Vajjha, Michael L. Wick, and Anindya Banerjee. Verification of ML systems via reparameterization. CoRR, abs/2007.06776, 2020. ",
    "title": "CertRL: Formalizing Convergence Proofs for Value and Policy",
    "paper_info": "CertRL: Formalizing Convergence Proofs for Value and Policy\nIteration in Coq\nKOUNDINYA VAJJHA, University of Pittsburgh , USA\nAVRAHAM SHINNAR, IBM Research , USA\nBARRY TRAGER, IBM Research , USA\nVASILY PESTUN, IBM Research , USA& IHES\nNATHAN FULTON, IBM Research , USA\nReinforcement learning algorithms solve sequential decision-making problems in probabilistic environments by optimizing\nfor long-term reward. The desire to use reinforcement learning in safety-critical settings inspires a recent line of work on\nformally constrained reinforcement learning; however, these methods place the implementation of the learning algorithm in\ntheir Trusted Computing Base. The crucial correctness property of these implementations is a guarantee that the learning\nalgorithm converges to an optimal policy.\nThis paper begins the work of closing this gap by developing a Coq formalization of two canonical reinforcement learning\nalgorithms: value and policy iteration for finite state Markov decision processes. The central results are a formalization of the\nBellman optimality principle and its proof, which uses a contraction property of Bellman optimality operator to establish that\na sequence converges in the infinite horizon limit. The CertRL development exemplifies how the Giry monad and mechanized\nmetric coinduction streamline optimality proofs for reinforcement learning algorithms. The CertRL library provides a general\nframework for proving properties about Markov decision processes and reinforcement learning algorithms, paving the way\nfor further work on formalization of reinforcement learning algorithms.\nAdditional Key Words and Phrases: Formal Verification, Policy Iteration, Value Iteration, Reinforcement Learning, Coinduction\n1\nINTRODUCTION\nReinforcement learning (RL) algorithms solve sequential decision making problems in which the goal is to\nchoose actions that maximize a quantitative utility function [Bel54, How60, Put94, SB98]. Recent high-profile\napplications of reinforcement learning include beating the world\u2019s best players at Go [SHM+16], competing against\ntop professionals in Dota [Ope18], improving protein structure prediction [SEJ+20], and automatically controlling\ncomplex robots [GHLL16]. These successes motivate the use of reinforcement learning in safety-critical and\ncorrectness-critical settings.\nReinforcement learning algorithms produce, at a minimum, a policy that specifies which action(s) should be\ntaken in a given state. The primary correctness property for reinforcement learning algorithms is convergence:\nin the limit, a reinforcement learning algorithm should converge to a policy that optimizes for the expected\nfuture-discounted value of the reward signal.\nThis paper contributes CertRL, a formal proof of convergence for value iteration and policy iteration two canon-\nical reinforcement learning algorithms [Bel54, How60, Put94]. They are often taught as the first reinforcement\nlearning methods in machine learning courses because the algorithms are relatively simple but their convergence\nproofs contain the main ingredients of a typical convergence argument for a reinforcement learning algorithm.\nThere is a cornucopia of presentations of these iterative algorithms and an equally diverse variety of proof\ntechniques for establishing convergence. Many presentations state but do not prove the fact that the optimal\npolicy of an infinite-horizon Markov decision process with \ud835\udefe-discounted reward is a stationary policy; i.e., the\noptimal decision in a given state does not depend on the time step at which the state is encountered. Following\nthis convention, this paper contributes the first formal proof that policy and value iteration converge in the\nlimit to the optimal policy in the space of stationary policies for infinite-horizon Markov decision processes. In\naddition to establishing convergence results for the classical iterative algorithms under classical infinitary and\narXiv:2009.11403v2  [cs.AI]  15 Dec 2020\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): This work is significant as it provides a Coq formalization of two reinforcement learning algorithms, value and policy iteration, which are commonly used in safety-critical settings. The formalization includes a framework for proving properties of Markov decision processes, which is exemplified by the formalization of Bellman's optimality principle and its proof. The formal proofs of convergence for value and policy iteration provide a formal guarantee of the optimality of the policies generated by these algorithms.\n\n- (2): Innovation point: The formalization of reinforcement learning algorithms in Coq is a novel contribution to the literature, especially in the context of safety-critical settings where formal verification is important. The use of the Giry monad and mechanized metric coinduction for streamlining optimality proofs is also innovative.\n\nPerformance: The methods presented in the article are evaluated by providing formal proof of convergence for value and policy iteration algorithms. Although the authors claim that the methods are scalable, the evaluation does not provide any evidence of scalability or efficiency of the methods.\n\nWorkload: The formalization and proof of convergence for reinforcement learning algorithms is a labor-intensive task, and the authors have put in significant effort to achieve this. However, the workload of utilizing the methods presented in this article in practical settings is not discussed in detail.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This work is significant as it provides a Coq formalization of two reinforcement learning algorithms, value and policy iteration, which are commonly used in safety-critical settings. The formalization includes a framework for proving properties of Markov decision processes, which is exemplified by the formalization of Bellman's optimality principle and its proof. The formal proofs of convergence for value and policy iteration provide a formal guarantee of the optimality of the policies generated by these algorithms.\n\n- (2): Innovation point: The formalization of reinforcement learning algorithms in Coq is a novel contribution to the literature, especially in the context of safety-critical settings where formal verification is important. The use of the Giry monad and mechanized metric coinduction for streamlining optimality proofs is also innovative.\n\nPerformance: The methods presented in the article are evaluated by providing formal proof of convergence for value and policy iteration algorithms. Although the authors claim that the methods are scalable, the evaluation does not provide any evidence of scalability or efficiency of the methods.\n\nWorkload: The formalization and proof of convergence for reinforcement learning algorithms is a labor-intensive task, and the authors have put in significant effort to achieve this. However, the workload of utilizing the methods presented in this article in practical settings is not discussed in detail.\n\n\n"
}