{
    "Abstract": "Abstract Long-term planning poses a major dif\ufb01culty to many reinforcement learning algorithms. This problem becomes even more pronounced in dynamic visual environments. In this work we propose Hierarchical Planning and Reinforcement Learning (HIP-RL), a method for merging the bene\ufb01ts and capabilities of Symbolic Planning with the learning abilities of Deep Reinforcement Learning. We apply HIPRL to the complex visual tasks of interactive question answering and visual semantic planning and achieve stateof-the-art results on three challenging datasets all while taking fewer steps at test time and training in fewer iterations. Sample results can be found at youtu.be/0TtWJ_ 0mPfI1 1. ",
    "Introduction": "Introduction An important goal in developing systems with visual understanding is to create agents that interact intelligently with the world. Teaching these agents about the world requires several steps. An agent must initially learn simple behaviors such as navigation and object affordances. Then, it can combine several actions together to accomplish longer term goals. As the task complexity increases, the agent must plan farther in the future. In recent years, researchers have predominantly trained interactive agents using either deep learning techniques on raw visual data or using planning algorithms on symbolic state representations. Deep learning has proven to be a very useful tool at learning to extract meaningful features from large sources of raw data. Deep Reinforcement Learning (Deep RL) has gained signi\ufb01cant traction in the vision community for simple tasks like playing video games [26, 27]. Yet as task complexity increases, and longer-term planning is required, these systems can no longer learn good reactive policies due to the exponentially branching state space. Conversely, many robotics systems still favor planning and search techniques such as RRTs and A* over the reactive Deep RL counterparts [9, 29]. The traditional planning algorithms offer better generalization when sensor data 1The full dataset and code will be open sourced soon. Figure 1: Sample Visual Semantic Planning task execution. The agent is asked to put a bowl in the microwave. At t=0, HIP-RL has not observed any locations where bowls can be, so it explores the room. At t=1, the Meta-Controller invokes the Planner which creates an ef\ufb01cient plan to check all the cabinets. It also sees the microwave and saves this information for later. At t=2 a bowl is found, and the Planner updates its plan to \ufb01nish the task. Since HIP-RL already saw the microwave, it saves time by not needing an additional search. At t=3 the Planner puts the bowl in the microwave, returning control to the Meta-Controller which \ufb01nishes the episode. In contrast, the Pure RL system spends much more time exploring the room, does not open any drawers, and ends the episode after many more steps, failing the task. is clean, and many provide optimality guarantees which are bene\ufb01cial for ensuring safety in potentially dangerous robotics environments. However most planning algorithms 1 arXiv:1901.01492v1  [cs.CV]  6 Jan 2019 ",
    "Related Work": "Related Work 2.1. Planning and Learning Although both learning and planning have signi\ufb01cant amounts of prior work, there have been relatively few attempts at merging the two. Many recent reinforcementlearning based algorithms fail when long-term planning is required; most algorithms trained on ATARI fail on the Montezuma\u2019s Revenge game due to its sparse rewards and long episodes [26]. Yet when planning and learning are combined, the results are often greater than either could do alone. One example of successfully merging planning and learning is the AlphaZero family of algorithms which combine Deep RL for board state evaluation and Monte Carlo Tree Search (MCTS) for planning and \ufb01nding high-value future board positions [28]. Rather than using MCTS to intelligently explore future states, which is not feasible in a partially observed visually dynamic environment, we use the Metric-FF Planner [16] to plan a single trajectory to the goal state. This chains actions together in order to shorten the number of hierarchical decisions and reduce the size of the action space. Planning in stochastic environments is often solved using Partially Observable Markov Decision Processes. Although they are frequently used to great success [1, 6], POMDPs often assume a known noise and transition model, which is not readily applicable for algorithms which use deep feature extraction. We avoid this issue by using both planning and learning; although our planner operates under the assumption of perfect and complete information, the Meta-Controller can divert control to the learning-based methods in the event of a planner failure or to gather more information. 2.2. Hierarchical Reinforcement Learning (HRL) HRL seeks to solve several problems with standard reinforcement learning such as handling very long episodes with sparse rewards. The design of these systems typically has one hierarchical meta-controller which invokes one of several low-level controllers. Each low-level controller is trained to accomplish a simpler task. In many cases [11, 12, 13, 22, 30] both the meta-controller and all low-level controllers are learned, and in some cases [22, 30] the tasks of the sub-controllers are also learned purely from interactions during training episodes rather than being human-engineered. This allows these systems to generalize well to unseen tasks with only a few training examples for the new tasks. In contrast, we use some learned low-level controllers, and some which use planning algorithms to directly solve subtasks. This allows our system to train qickly and still generalize well to new tasks with only moderate additional goal-state speci\ufb01cation. 2.3. Deep RL in Virtual Visual Environments In the past few years, many different virtual platforms have been created in order to facilitate better Deep RL. Virtual environments provide limitless labeled data and are easily parallelizable. Some of the most popular are OpenAI Gym [5] and VizDoom [19] which both build on existing video games, and MuJoCo [32] which implements more realistic contact physics. More recently, multiple environments have been created which offer near-photorealistic and physically accurate interaction such as AI2THOR [20], Gibson [34], and CHALET [35]. Other environments forgo photo-realism for increased rendering speed such as House3D [33], and DeepMind Lab [4]. Additionally, there have been many advancements in learning from interactions with a virtual visual environ",
    "Method": "Method In order to accomplish complex tasks, a system must be able to plan long action trajectories which satisfy the task goals. To operate in a visual world, a system must learn to understand a dynamic visual environment. Thus, to learn to plan in a visual environment, we combine Deep RL with Symbolic Planning, handing control of the agent back and forth between the two methods. In this section we outline the individual components of HIP-RL as well as how they work together to solve complex learning and planning tasks. 3.1. Hierarchical Planning and Reinforcement Learning (HIP-RL) HIP-RL consists of a hierarchical Meta-Controller, several direct (low-level) controllers, and a shared knowledge state (Figure 2). The knowledge state contains all perceptual and interactive information such as navigable locations, object positions, and which cabinets have previously been 2https://github.com/danielgordon10/thor-iqa-cvpr-2018 3https://github.com/facebookresearch/EmbodiedQA Meta-Controller Direct\tControllers (Planner,\tExplorer,\tStopper,\tEtc.) Environment Map/State\tKnowledge Goal/Task\t Embedding Figure 2: Diagram of the HIP-RL framework. Each direct controller interacts with the environment based off of commands given by the Meta-Controller. All controllers share a knowledge state which is updated by the various controllers during the episode based on perceptual and interactive observations. opened, as well as the goal representation. For example, in Figure 1 the knowledge state in image 1 contains the positions of the drawers, the microwave, the red plate, and the fact that none of the drawers have been checked. Each controller can read all of the state knowledge, and can update a portion of the knowledge based on its perception; the Navigator can update the world map, and the Object Detector can update the object locations but not vice versa. At the start of an episode, the Meta-Controller chooses which direct controller should be used to advance the current state towards the goal state, invoking that direct controller with a subtask. The direct controller attempts to accomplish this subtask and returns control back to the Meta-Controller upon completion. This process is repeated until the Meta-Controller decides the full task has been accomplished and calls the Stopper to end the episode. 3.2. Hierarchical Meta-Controller The Meta-Controller receives the goal and decides which of the direct controllers to invoke. It learns to trade off between the length of an episode and the reward/penalty received for successfully or unsuccessfully ending an episode. We train this behavior using the A3C algorithm [26] to reward successful episodes, penalize unsuccessful episodes, and add a time penalty for each hierarchical step. We visualize the Meta-Controller\u2019s architecture in the context of Interactive Question Answering in Figure 3. It takes as input a top-down spatial map of object locations, the question representation, and the current image from the enviDetected\tObject\tMap Current\tImage Question Spatial\tAttention\tMap * \ud835\udeba \ud835\udeba P(Explore) P(Scan) P(Plan) P(Answer) Value P(Answer0) P(Answer1) P(Answer2) \u2026 Tiled\tQuestion\tEmbedding Is\tthere\tan\tapple\tin\tthe\tfridge? Convolution Concatenation Fully\tConnected GRU Spatial Spatial Figure 3: Overview of the network architecture for the hierarchical Meta-Controller (and answerer) used for IQA tasks. The network takes as input the full detected object map, the question, and the current image. The question is embedded and spatially tiled. We concatenate the object map features with the question embedding, perform several convolutions, and spatially sum the output. Similarly, we concatenate the image features with the question embedding, and use an attention mechanism conditioned on the question to spatially sum the features. We do not use an attention mechanism on the detected object map as this makes counting questions dif\ufb01cult. The image features additionally use a GRU [10] to add temporal context. Since the detected object map is purely additive (the \ufb01nal map contains at least as much information as the previous ones) no temporal context is necessary. The map and image features are concatenated and fed into a fully-connected layer. The network outputs a probability distribution over the actions, a value estimate for the state, and a distribution over the answers for the question. ronment. The question embedding is concatenated with the spatial map and visual features to condition the features on the question. The network outputs a probability distribution over the action space of direct controllers as well as a value for the current state. The Meta-Controller architecture for Visual Semantic Planning is exactly the same except the question embedding is replaced with the semantic task embedding, and there is no answer branch in the output. 3.3. Planner The Planner is tasked with returning a sequence of actions which would accomplish the goal. It operates on logical primitives using the Planning Domain De\ufb01nition Language (PDDL) and is guaranteed to return a correct set of high-level planning actions given the observations are accurate. PDDL speci\ufb01es states using \u201c\ufb02uents\u201d which can be boolean values (cabinet 1 is open, an apple is in the fridge) or numeric (location A and B are 10 steps apart). Actions consist of templatized preconditions necessary for the action to be possible, and effects, which modify the values of the \ufb02uents caused by executing that action. For example, the Open action takes the preconditions that an object must be openable but closed and that the agent must be near the object, and has the effect of setting the object\u2019s closed \ufb02uent to False. Goals specify a set of criteria necessary for completing some task. Even complex tasks which take hundreds of steps like Put all the mugs into the sink. may be easily speci\ufb01ed. When using the Planner\u2019s output, HIP-RL sequentially takes the next Planner action, runs the Object Detector and updates the knowledge representation, and replans. This allows the Planner to easily recover from incorrect initial detections or false negatives which may be corrected over time. The Planner returns control either when the goal state has been reached, when it determines the goal is impossible, or after a \ufb01xed number of steps. An example Planner sequence for the question Is there a bowl in a drawer? is as follows. The Meta-Controller invokes the Planner with the goal All drawers have been checked or a bowl is in a drawer. The knowledge state contains three drawers and the microwave. The Planner outputs a plan to check each drawer. The \ufb01rst drawer is empty, so the plan continues. In the second drawer, the bowl is found, and the Planner returns control to the Meta-Controller. In a different example for the same task, the Planner checks each drawer and the bowl is in none of them. After all drawers are checked, the Planner returns control to the Meta-Controller. Grouping the multi-step output from the Planner into a single decision made by the hierarchical controller gives HIP-RL several advantages over pure RL solutions. Primarily, this signi\ufb01cantly reduces our action space and the number of high-level steps in a single episode. Furthermore, because the Planner guarantees that the goal will be reached (or ruled impossible), HIP-RL can be more thorough and ef\ufb01cient in its exploration. In the examples above, if the agent had only checked a single drawer and moved on, it would have missed the opportunity to know the answer was \u201cyes\u201d or be more con\ufb01dent that the answer was \u201cno.\u201d In this work we use the Metric-FF PDDL solver [16], one of the most popular planning algorithms for operating on PDDL instances. Metric-FF extends the origin FF Planning algorithm [17] to both boolean and numerical values. Metric-FF uses hill-climbing on relaxed plans (plans in which contradictions are ignored) as a heuristic to estimate the distance to the goal. For numeric values, the relaxation takes the form of ignoring any non-monotonicly increasing effects that an action may have. Finding the absolute shortest solution to PDDL problems is NP-Complete, but in practice, Metric-FF usually returns nearly optimal plans in around 100 milliseconds. We include a sample PDDL state and action domain and corresponding Planner output in the supplemental material. 3.4. Object Detector The Object Detector must detect objects from the current image, but it must also track what it has detected in the past. In this work, we assume perfect camera location knowledge which simpli\ufb01es this process. The Object Detector predicts object masks as well as pixelwise depths, and the objects are projected into a global coordinate frame and merged with prior detections. In our experiments we use Mask-RCNN [15] for the detection masks and the FRCN depth estimation network [23] which are both \ufb01netuned on the training environments. We merge detections by joining the bounding cube around two detections if their 3D intersection is above a certain threshold (in practice 0.3). A more sophisticated strategy with more frequent detections such as Fusion++[24] could further improve our method (however Fusion++ requires a depth camera). In Figure 3, the Detected Object Map represents the previously detected objects and their spatial locations. 3.5. Navigator We use the Navigator from [13] as it shows reliable performance for going to locations speci\ufb01ed in a relative coordinate frame by a hierarchical controller. The Navigator predicts edge weights for a small region in front of the agent and uses an Egocentric Spatial GRU to update the memory state. Then it chooses the next action based on A* search to the target location. For more details, see [13]. To improve the overall execution speed of our method, our algorithm only calls the Object Detector once the navigation has \ufb01nished rather than at every intermediate location. 3.6. Stopper The Stopper is tasked with \ufb01nishing an episode. For VSP, the Stopper simply terminates the episode. However for IQA, the Stopper must provide an answer to the posed question. For this, we train a network which takes the question, the entire memory state, and the current image features as input and outputs an answer. For questions from IQUAD V1, we use state information from the Object Detector to improve our accuracy. For example, for the question Is there bread in the room? since we track whether we have detected bread to be able to end planning upon detection, we can provide this information to the Stopper as well. For EQA V1, since the questions can be answered from a single image, we provided an additional image channel representing a detection mask of the question\u2019s subject. The Stopper is only trained based on the last state from a sequence via cross entropy over the possible answers. In practice, the Stopper shares a network with the Meta-Controller, as shown in Figure 3 which encourages the learned features to be semantically grounded. 3.7. Explorer To gather more information about an environment, the Explorer \ufb01nds a location which has not been visited and invokes the Navigator with that location. In this work, the Explorer is not learned; instead, it tracks where the agent has been and picks the location and orientation which maximizes new views while minimizing the distance from the current agent location. Note that the Explorer still operates on the Navigator\u2019s learned free-space map. 3.8. Scanner The Scanner issues a prede\ufb01ned sequence of commands to the environment to obtain a 360\u00b0 view of its surroundings. Speci\ufb01cally, it performs three full rotations at +30, 0, and -30 degrees with the horizon, stopping every 90 degrees to run the Object Detector. It is often useful to call the Scanner after calling the Explorer, but we leave this up to the hierarchical controller to learn. 4. Tasks and Environments We focus on two tasks (Interactive Question Answering and Visual Semantic Planning) in two virtual environments (AI2-THOR [20] and House3D [33] for IQA, and AI2-THOR for VSP). Both tasks require complex visual and spatial reasoning as well as multi-step planning. 4.1. Interactive Question Answering (IQA) We evaluate our agent on both IQUAD V1 [13] and EQA V1 [11]. IQUAD V1 provides 75,600 training questions in 25 training rooms, and 1920 test questions in 5 unseen rooms. Additionally, IQUAD V1 provides 2400 test questions in seen rooms which helps factor out errors due to ",
    "Experiments": "Experiments We compare HIP-RL across the datasets outlined in Section 4 using existing state-of-the-art methods as baselines as well as the unimodal baselines from [31] and pure planning baselines. On each dataset, we record the accuracy/success of our method as well as the episode length. Except in the generalization experiment, all tests are done on unseen environments. [2] proposes the Success weighted by (normalized inverse) Path Length (SPL) which combines accuracy and episode length into a single metric for evaluating embodied agents on pure navigation tasks. SPL is de\ufb01ned as SPL = 1 N N \ufffd i=1 Si \u2113i max(pi, \u2113i) (1) where Si is a success indicator for episode i, pi is the path length, and \u2113i is the shortest path length. SPL is not suf\ufb01cient for question answering as an agent which never moves could still be very successful depending on the dif\ufb01culty of the questions4. To address this issue, we propose the Shifted SPL (SSPL) metric which is de\ufb01ned as SSPL = \u00b5 \u2212 b 1 \u2212 b \u2217 SPL (2) where \u00b5 is the average accuracy of the method and b is the average accuracy of a baseline agent which is forced to end/answer immediately after beginning an episode. Note that SSPL directly accounts for dataset biases by subtracting the accuracy of a learned baseline rather than simply the most common answer or random chance accuracy. For the VSP experiments SPL is exactly equal to SSPL, as a baseline which cannot move will achieve 0% success. For the IQA experiments we use the \u201cLanguage Only\u201d baselines presented in [31] as b. 5.1. Baselines On all datasets we include (at least) one pure-learning and one pure-planning baseline. The \u201cPlanner Only\u201d baseline uses the same Plan/Act/Observe/Replan loop as HIPRL but does not include any hierarchical decision making. Additionally, if at the start of the episode the plan is empty (for example if the agent starts looking at a wall), we rotate the agent until the plan is not empty. We also use the \u201cLanguage Only\u201d baselines from [31] which attempt to answer the question without making any actions, effectively learning the language bias of the dataset. For VSP, we introduce a \u201cLearning Only\u201d baseline which removes the Planner from HIP-RL and adds reward shaping to encourage certain interactions like looking at and picking up the object of the task. Even after signi\ufb01cant training time, this method fails to learn a working policy. 4[31] shows signi\ufb01cant bias exists in EQA V1 [11] and in MatterPort3D [3] ",
    "Results": "Results We test our system for accuracy on IQUAD V1, EQA V1, and the new VSP dataset, achieving state-of-theart performance on all tasks. The results are shown in Figure 4. In IQUAD V1 and VSP the \u201cPlanner Only\u201d baseline outperforms the \u201cLearning Only\u201d baseline which coincides with the fact that the ground-truth trajectories are signi\ufb01cantly longer and contain more necessary interactions than in EQA V1. A fundamental issue with reinforcement learning is that it must \u201cluck into\u201d good solutions randomly before it can improve, which can be very rare in complex multi-step tasks. Planning simpli\ufb01es this by directly solving objectives rather than making guesses and observing rewards or penalties. Yet pure planning suffers from myopia in that (in our system) it assumes perfect and complete global information, leading it to ignore unobserved parts of the environment. This is most apparent in VSP where the planner assumes a task is impossible if it has not observed a location where the object can be. By combining both strategies, HIP-RL achieves the exploration tendencies of RL along with the goal-oriented direct problem solving of planning. 5.3. Ablation We further explore the effect that various sources of inaccuracy have on HIP-RL by substituting the Object Detector and the Navigator with Ground Truth (GT) information, shown in Table 1. Adding GT detections greatly improves our accuracy across the board. This is due to all the tasks being very object-centric, so if the object is misidenti\ufb01ed or not detected at all, the Answerer/Planner has no means of \ufb01xing the mistake. In contrast, using GT Navigation does not improve performance dramatically, but the path lengths do nearly halve. In practice we observe this is frequently not from the navigation agent wandering randomly but is instead usually from the beginning of the episodes where the map starts empty and the navigator unknowingly goes down dead ends or takes otherwise inef\ufb01cient paths. 5.4. Episode Ef\ufb01ciency Table 1 also lists episode lengths and SSPL scores for each method. Note that episode lengths include every interaction with the environment (turn left, open, move ahead each count as one action), not just hierarchical actions. While HIP-RL dramatically improves over [13], there is still a large gap between the shortest path estimate. Some inef\ufb01ciency due to exploration is unavoidable, but there are also cases where the agent explores even after it could an",
    "Conclusion": "Conclusion In this work, we presented Hierarchical Planning and Reinforcement Learning, a method for combining the bene\ufb01ts of Deep Reinforcement Learning and Symbolic Planning. We demonstrate its effectiveness at increasing accuracy while simultaneously decreasing episode length and training time. Though this exact implementation may not be applicable to many other tasks, we believe the high level idea of learning to invoke various direct controllers, some of which explicitly plan, could be applied to a broader array of tasks such as Task and Motion Planning. In general, we observe that using planning algorithms to assist in performing \u201cgood\u201d actions results in improved accuracy, test-time ef\ufb01ciency, and training speed. Still, HIP-RL could be improved to explore more ef\ufb01ciently using priors based on likely locations of an object. Additionally, we could learn the PDDL preconditions and effects directly so as to limit the need for human labels. We are excited about the potential impacts of visual agents and their ability to learn to interact more intelligently with the world around them. 7. Acknowledgements This work was funded in part by the National Science Foundation under contract number NSF-NRI-1637479, NSF-IIS-1338054, NSF-1652052, ONR N00014-13-10720, the Allen Distinguished Investigator Award, and the Allen Institute for Arti\ufb01cial Intelligence. We would like to ",
    "References": "References [1] C. Amato, G. Konidaris, A. Anders, G. Cruz, J. P. How, and L. P. Kaelbling. Policy search for multi-robot coordination under uncertainty. The International Journal of Robotics Research, 35(14):1760\u20131778, 2016. 2 [2] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 6 [3] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00a8underhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-Language Navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3, 6 [4] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. K\u00a8uttler, A. Lefrancq, S. Green, V. Vald\u00b4es, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. 2 [5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016. 2 [6] P. Cai, Y. Luo, D. Hsu, and W. S. Lee. Hyp-despot: A hybrid parallel algorithm for online planning under uncertainty. Robotics Science and Systems (RSS) 2018, 11, 2018. 2 [7] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. AAAI-18 Conference on Arti\ufb01cial Intelligence, 2018. 3 [8] R. Chitnis, D. Had\ufb01eld-Menell, A. Gupta, S. Srivastava, E. Groshev, C. Lin, and P. Abbeel. Guided search for task and motion plans using learned heuristics. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 447\u2013454. IEEE, 2016. 3 [9] S. Chitta, I. Sucan, and S. Cousins. Moveit![ros topics]. IEEE Robotics & Automation Magazine, 19(1):18\u201319, 2012. 1 [10] K. Cho, B. van Merri\u00a8enboer, C\u00b8 . G\u00a8ulc\u00b8ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. 4 [11] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 5, page 6, 2018. 2, 3, 5, 6 [12] A. Das, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Neural Modular Control for Embodied Question Answering. In Proceedings of the Conference on Robot Learning (CoRL), 2018. 2, 3, 6, 7 [13] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4089\u2013 4098, 2018. 2, 3, 5, 6, 7, 8 [14] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2616\u20132625, 2017. 3 [15] K. He, G. Gkioxari, P. Doll\u00b4ar, and R. Girshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 2980\u20132988. IEEE, 2017. 5, 7, 8 [16] J. Hoffmann. The metric-ff planning system: Translating\u201cignoring delete lists\u201dto numeric state variables. Journal of Arti\ufb01cial Intelligence Research, 20:291\u2013341, 2003. 2, 5 [17] J. Hoffmann and B. Nebel. The ff planning system: Fast plan generation through heuristic search. Journal of Arti\ufb01cial Intelligence Research, 14:253\u2013302, 2001. 5 [18] L. P. Kaelbling and T. Lozano-P\u00b4erez. Hierarchical task and motion planning in the now. In Robotics and Automation (ICRA), 2011 IEEE International Conference on, pages 1470\u20131477. IEEE, 2011. 3 [19] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Ja\u00b4skowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE Conference on, pages 1\u20138. IEEE, 2016. 2 [20] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017. 2, 3, 5, 6, 8 [21] G. Konidaris, L. P. Kaelbling, and T. Lozano-Perez. From skills to symbols: Learning symbolic representations for abstract high-level planning. Journal of Arti\ufb01cial Intelligence Research, 61:215\u2013289, 2018. 3 [22] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pages 3675\u20133683, 2016. 2 [23] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In 3D Vision (3DV), 2016 Fourth International Conference on, 2016. 5, 7, 8 [24] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger. Fusion++: Volumetric object-level slam. In 2018 International Conference on 3D Vision (3DV), pages 32\u201341. IEEE, 2018. 5 [25] P. W. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and R. Hadsell. Learning to navigate in complex environments. International Conference on Learning Representations (ICLR), 2017. 3 [26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016. 1, 2, 3 [27] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiositydriven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017. 1 [28] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017. 2 [29] I. A. Sucan, M. Moll, and L. E. Kavraki. The open motion planning library. IEEE Robotics & Automation Magazine, 19(4):72\u201382, 2012. 1 [30] C. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor. A deep hierarchical approach to lifelong learning in minecraft. In AAAI, volume 3, page 6, 2017. 2 [31] J. Thomason, D. Gordon, and Y. Bisk. Shifting the baseline: Single modality performance on visual navigation & qa. arXiv preprint arXiv:1811.00613, 2018. 6 [32] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012. 2 [33] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018. 2, 3, 5, 6 [34] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9068\u20139079, 2018. 2 [35] C. Yan, D. Misra, A. Bennnett, A. Walsman, Y. Bisk, and Y. Artzi. Chalet: Cornell house agent learning environment. arXiv preprint arXiv:1801.07357, 2018. 2 [36] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi, and A. Farhadi. Visual semantic planning using deep successor representations. In Proceedings of the IEEE International Conference on Computer Vision, pages 483\u2013 492, 2017. 6 [37] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. FeiFei, and A. Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3357\u20133364. IEEE, 2017. 3 Appendix A. PDDL Domain Below is the full PDDL Domain for question answering and visual semantic planning. ( d e f i n e ( domain q a v s p t a s k ) ( : r e q u i r e m e n t s : adl ) ( : types agent l o c a t i o n r e c e p t a c l e o b j e c t r t y p e otype ) ( : p r e d i c a t e s ( a t L o c a t i o n ? a \u2212 agent ? l \u2212 l o c a t i o n ) ( r e c e p t a c l e A t L o c a t i o n ? r \u2212 r e c e p t a c l e ? l \u2212 l o c a t i o n ) ( o b j e c t A t L o c a t i o n ?o \u2212 o b j e c t ? l \u2212 l o c a t i o n ) ( openable ? r \u2212 r e c e p t a c l e ) ( opened ? r \u2212 r e c e p t a c l e ) ( i n R e c e p t a c l e ?o \u2212 o b j e c t ? r \u2212 r e c e p t a c l e ) ( checked ? r \u2212 r e c e p t a c l e ) ( r e c e p t a c l e T y p e ? r \u2212 r e c e p t a c l e ? t \u2212 r t y p e ) ( objectType ?o \u2212 o b j e c t ? t \u2212 otype ) ( canContain ? t \u2212 r t y p e ?o \u2212 otype ) ( holds ? a \u2212 agent ?o \u2212 o b j e c t ) ( holdsAny ? a \u2212 agent ) ( f u l l ? r \u2212 r e c e p t a c l e ) ) ( : f u n c t i o n s ( d i s t a n c e ? from ? to ) ( t o t a l C o s t ) ) ; ; agent goes to r e c e p t a c l e ( : a c t i o n GotoLocation : parameters (? a \u2212 agent ? l S t a r t \u2212 l o c a t i o n ? lEnd \u2212 l o c a t i o n ) : p r e c o n d i t i o n ( a t L o c a t i o n ? a ? l S t a r t ) : e f f e c t ( and ( a t L o c a t i o n ? a ? lEnd ) ( not ( a t L o c a t i o n ? a ? l S t a r t ) ) ( f o r a l l (? r \u2212 r e c e p t a c l e ) ( when ( and ( r e c e p t a c l e A t L o c a t i o n ? r ? lEnd ) ( or ( not ( openable ? r ) ) ( opened ? r ) ) ) ( checked ? r ) ) ) ( i n c r e a s e ( t o t a l C o s t ) ( d i s t a n c e ? l S t a r t ? lEnd ) ) ) ) ; ; agent opens r e c e p t a c l e ( : a c t i o n OpenObject : parameters (? a \u2212 agent ? l \u2212 l o c a t i o n ? r \u2212 r e c e p t a c l e ) : p r e c o n d i t i o n ( and ( a t L o c a t i o n ? a ? l ) ( r e c e p t a c l e A t L o c a t i o n ? r ? l ) ( openable ? r ) ( f o r a l l (? re \u2212 r e c e p t a c l e ) ( not ( opened ? re ) ) ) ) : e f f e c t ( and ( opened ? r ) ( checked ? r ) ( i n c r e a s e ( t o t a l C o s t ) 1) ) ) ; ; agent c l o s e s r e c e p t a c l e ( : a c t i o n CloseObject : parameters (? a \u2212 agent ? l \u2212 l o c a t i o n ? r \u2212 r e c e p t a c l e ) : p r e c o n d i t i o n ( and ( a t L o c a t i o n ? a ? l ) ( r e c e p t a c l e A t L o c a t i o n ? r ? l ) ( openable ? r ) ( opened ? r ) ) : e f f e c t ( and ( not ( opened ? r ) ) ( i n c r e a s e ( t o t a l C o s t ) 1) ) ) ; ; agent picks up o b j e c t ( : a c t i o n PickupObject : parameters (? a \u2212 agent ? l \u2212 l o c a t i o n ?o \u2212 o b j e c t ? r \u2212 r e c e p t a c l e ) : p r e c o n d i t i o n ( and ( a t L o c a t i o n ? a ? l ) ( o b j e c t A t L o c a t i o n ?o ? l ) ( or ( not ( openable ? r ) ) ( opened ? r ) ) ( i n R e c e p t a c l e ?o ? r ) ( not ( holdsAny ? a ) ) ) : e f f e c t ( and ( not ( i n R e c e p t a c l e ?o ? r ) ) ( holds ? a ?o ) ( holdsAny ? a ) ( i n c r e a s e ( t o t a l C o s t ) 1) ) ) ; ; agent puts down o b j e c t ( : a c t i o n PutObject : parameters (? a \u2212 agent ? l \u2212 l o c a t i o n ? ot \u2212 otype ?o \u2212 o b j e c t ? r \u2212 r e c e p t a c l e ) : p r e c o n d i t i o n ( and ( a t L o c a t i o n ? a ? l ) ( r e c e p t a c l e A t L o c a t i o n ? r ? l ) ( or ( not ( openable ? r ) ) ( opened ? r ) ) ( not ( f u l l ? r ) ) ( objectType ?o ? ot ) ( holds ? a ?o ) ) : e f f e c t ( and ( i n R e c e p t a c l e ?o ? r ) ( f u l l ? r ) ( not ( holds ? a ?o ) ) ( not ( holdsAny ? a ) ) ( i n c r e a s e ( t o t a l C o s t ) 1) ) ) ) Appendix B. PDDL Goal Example Below is the goal speci\ufb01cation for the question Is there a mug in the room?. ( : goal ( or ( e x i s t s (? o \u2212 o b j e c t ) ( objectType ?o MugType ) ) ( and ( f o r a l l (? t \u2212 r t y p e ) ( f o r a l l (? r \u2212 r e c e p t a c l e ) ( or ( not ( and ( canContain ? t MugType ) ( r e c e p t a c l e T y p e ? r ? t ) ) ) ( checked ? r ) ) ) ) ( f o r a l l (? re \u2212 r e c e p t a c l e ) ( not ( opened ? re ) ) ) ) ) ) ",
    "title": "",
    "paper_info": "Detected\tObject\tMap\nCurrent\tImage\nQuestion\nSpatial\tAttention\tMap\n*\n\ud835\udeba\n\ud835\udeba\nP(Explore)\nP(Scan)\nP(Plan)\nP(Answer)\nValue\nP(Answer0)\nP(Answer1)\nP(Answer2)\n\u2026\nTiled\tQuestion\tEmbedding\nIs\tthere\tan\tapple\tin\tthe\tfridge?\nConvolution\nConcatenation\nFully\tConnected\nGRU\nSpatial\nSpatial\nFigure 3: Overview of the network architecture for the hierarchical Meta-Controller (and answerer) used for IQA tasks.\nThe network takes as input the full detected object map, the question, and the current image. The question is embedded\nand spatially tiled. We concatenate the object map features with the question embedding, perform several convolutions, and\nspatially sum the output. Similarly, we concatenate the image features with the question embedding, and use an attention\nmechanism conditioned on the question to spatially sum the features. We do not use an attention mechanism on the detected\nobject map as this makes counting questions dif\ufb01cult. The image features additionally use a GRU [10] to add temporal\ncontext. Since the detected object map is purely additive (the \ufb01nal map contains at least as much information as the previous\nones) no temporal context is necessary. The map and image features are concatenated and fed into a fully-connected layer.\nThe network outputs a probability distribution over the actions, a value estimate for the state, and a distribution over the\nanswers for the question.\nronment. The question embedding is concatenated with the\nspatial map and visual features to condition the features on\nthe question. The network outputs a probability distribu-\ntion over the action space of direct controllers as well as a\nvalue for the current state. The Meta-Controller architecture\nfor Visual Semantic Planning is exactly the same except the\nquestion embedding is replaced with the semantic task em-\nbedding, and there is no answer branch in the output.\n3.3. Planner\nThe Planner is tasked with returning a sequence of ac-\ntions which would accomplish the goal. It operates on log-\nical primitives using the Planning Domain De\ufb01nition Lan-\nguage (PDDL) and is guaranteed to return a correct set of\nhigh-level planning actions given the observations are ac-\ncurate. PDDL speci\ufb01es states using \u201c\ufb02uents\u201d which can be\nboolean values (cabinet 1 is open, an apple is in the fridge)\nor numeric (location A and B are 10 steps apart). Actions\nconsist of templatized preconditions necessary for the ac-\ntion to be possible, and effects, which modify the values of\nthe \ufb02uents caused by executing that action. For example, the\nOpen action takes the preconditions that an object must be\nopenable but closed and that the agent must be near the ob-\nject, and has the effect of setting the object\u2019s closed \ufb02uent\nto False. Goals specify a set of criteria necessary for com-\npleting some task. Even complex tasks which take hundreds\nof steps like Put all the mugs into the sink.\nmay be easily speci\ufb01ed. When using the Planner\u2019s output,\nHIP-RL sequentially takes the next Planner action, runs the\nObject Detector and updates the knowledge representation,\nand replans. This allows the Planner to easily recover from\nincorrect initial detections or false negatives which may\nbe corrected over time. The Planner returns control either\nwhen the goal state has been reached, when it determines\nthe goal is impossible, or after a \ufb01xed number of steps.\nAn example Planner sequence for the question Is\nthere a bowl in a drawer?\nis as follows.\nThe\nMeta-Controller invokes the Planner with the goal All\ndrawers have been checked or a bowl is\nin a drawer.\nThe knowledge state contains three\ndrawers and the microwave. The Planner outputs a plan to\ncheck each drawer. The \ufb01rst drawer is empty, so the plan\ncontinues. In the second drawer, the bowl is found, and the\nPlanner returns control to the Meta-Controller.\nIn a different example for the same task, the Planner\nchecks each drawer and the bowl is in none of them. Af-\nter all drawers are checked, the Planner returns control to\n",
    "GPTsummary": "- (1): The article addresses the difficulty of long-term planning faced by many reinforcement learning algorithms, particularly in dynamic visual environments.\n\n- (2): The past methods involve using either deep learning techniques on raw visual data or using planning algorithms on symbolic state representations. However, as task complexity increases and longer-term planning is required, these systems can no longer learn good reactive policies due to the exponentially branching state space. The approach proposed in this article, Hierarchical Planning and Reinforcement Learning (HIP-RL), merges the benefits and capabilities of Symbolic Planning with the learning abilities of Deep Reinforcement Learning.\n\n- (3): HIP-RL is applied to the complex visual tasks of interactive question answering and visual semantic planning, with the aim of achieving state-of-the-art results on three challenging datasets while taking fewer steps at test time and training in fewer iterations.\n\n- (4): The methods achieve state-of-the-art results on three challenging datasets of visual semantic planning and interactive question answering, while requiring fewer steps at test time and fewer iterations during training. Such performance supports the goals of using HIP-RL- to merge the benefits of symbolic planning with deep reinforcement learning.\n\n\n7. Methods:\n\n- (1): The article proposes Hierarchical Planning and Reinforcement Learning (HIP-RL) as a method to merge the benefits of Symbolic Planning with Deep Reinforcement Learning to achieve long-term planning in visual environments.\n\n- (2): HIP-RL consists of a hierarchical Meta-Controller, several direct controllers, and a shared knowledge state. The Meta-Controller chooses which direct controller to use to advance the state towards the goal state, invoking it with a subtask. The direct controller attempts to accomplish this subtask and returns control back to the Meta-Controller upon completion.\n\n- (3): The approaches of symbolic planning and deep reinforcement learning are brought together by the use of a Planner which operates on logical primitives using Planning Domain Definition Language (PDDL) and returns a sequence of actions to accomplish the goal. The Object Detector predicts object masks and pixelwise depths, and the objects are merged with prior detections. The Navigator predicts edge weights and uses A* search to the target location, while the Explorer picks the location and orientation which maximizes new views while minimizing the distance from the current agent location. The Stopper is responsible for finishing an episode and providing an answer to the posed question for Interactive Question Answering tasks. \n\n- (4): HIP-RL is applied to the complex visual tasks of interactive question answering and visual semantic planning, achieving state-of-the-art results on three challenging datasets while taking fewer steps at test time and training in fewer iterations.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work proposes a method called Hierarchical Planning and Reinforcement Learning (HIP-RL) that merges the benefits of Symbolic Planning with Deep Reinforcement Learning to achieve long-term planning in visual environments. The article presents the significance of using planning algorithms to assist in performing good actions, resulting in improved accuracy, test-time efficiency, and training speed.\n\n- (2): Innovation point: The article innovatively integrates the benefits and capabilities of Symbolic Planning and Deep Reinforcement Learning with HIP-RL to achieve long-term planning in visual environments. Performance: HIP-RL achieves state-of-the-art results on three challenging datasets of visual semantic planning and interactive question answering while requiring fewer steps at test time and fewer iterations during training. Workload: The article uses some human labels to learn the PDDL preconditions and effects directly, which could be improved in the future. Overall, HIP-RL shows promising potential for future research applications, such as Task and Motion Planning, and for improving visual agents' ability to learn to interact more intelligently with the world around them.\n\n\n",
    "GPTmethods": "- (1): The article proposes Hierarchical Planning and Reinforcement Learning (HIP-RL) as a method to merge the benefits of Symbolic Planning with Deep Reinforcement Learning to achieve long-term planning in visual environments.\n\n- (2): HIP-RL consists of a hierarchical Meta-Controller, several direct controllers, and a shared knowledge state. The Meta-Controller chooses which direct controller to use to advance the state towards the goal state, invoking it with a subtask. The direct controller attempts to accomplish this subtask and returns control back to the Meta-Controller upon completion.\n\n- (3): The approaches of symbolic planning and deep reinforcement learning are brought together by the use of a Planner which operates on logical primitives using Planning Domain Definition Language (PDDL) and returns a sequence of actions to accomplish the goal. The Object Detector predicts object masks and pixelwise depths, and the objects are merged with prior detections. The Navigator predicts edge weights and uses A* search to the target location, while the Explorer picks the location and orientation which maximizes new views while minimizing the distance from the current agent location. The Stopper is responsible for finishing an episode and providing an answer to the posed question for Interactive Question Answering tasks. \n\n- (4): HIP-RL is applied to the complex visual tasks of interactive question answering and visual semantic planning, achieving state-of-the-art results on three challenging datasets while taking fewer steps at test time and training in fewer iterations.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work proposes a method called Hierarchical Planning and Reinforcement Learning (HIP-RL) that merges the benefits of Symbolic Planning with Deep Reinforcement Learning to achieve long-term planning in visual environments. The article presents the significance of using planning algorithms to assist in performing good actions, resulting in improved accuracy, test-time efficiency, and training speed.\n\n- (2): Innovation point: The article innovatively integrates the benefits and capabilities of Symbolic Planning and Deep Reinforcement Learning with HIP-RL to achieve long-term planning in visual environments. Performance: HIP-RL achieves state-of-the-art results on three challenging datasets of visual semantic planning and interactive question answering while requiring fewer steps at test time and fewer iterations during training. Workload: The article uses some human labels to learn the PDDL preconditions and effects directly, which could be improved in the future. Overall, HIP-RL shows promising potential for future research applications, such as Task and Motion Planning, and for improving visual agents' ability to learn to interact more intelligently with the world around them.\n\n\n",
    "GPTconclusion": "- (1): This piece of work proposes a method called Hierarchical Planning and Reinforcement Learning (HIP-RL) that merges the benefits of Symbolic Planning with Deep Reinforcement Learning to achieve long-term planning in visual environments. The article presents the significance of using planning algorithms to assist in performing good actions, resulting in improved accuracy, test-time efficiency, and training speed.\n\n- (2): Innovation point: The article innovatively integrates the benefits and capabilities of Symbolic Planning and Deep Reinforcement Learning with HIP-RL to achieve long-term planning in visual environments. Performance: HIP-RL achieves state-of-the-art results on three challenging datasets of visual semantic planning and interactive question answering while requiring fewer steps at test time and fewer iterations during training. Workload: The article uses some human labels to learn the PDDL preconditions and effects directly, which could be improved in the future. Overall, HIP-RL shows promising potential for future research applications, such as Task and Motion Planning, and for improving visual agents' ability to learn to interact more intelligently with the world around them.\n\n\n"
}