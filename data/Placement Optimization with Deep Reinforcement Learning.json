{
    "Abstract": "ABSTRACT Placement Optimization is an important problem in systems and chip design, which consists of mapping the nodes of a graph onto a limited set of resources to optimize for an objective, subject to constraints. In this paper, we start by motivating reinforcement learning as a solution to the placement problem. We then give an overview of what deep reinforcement learning is. We next formulate the placement problem as a reinforcement learning problem, and show how this problem can be solved with policy gradient optimization. Finally, we describe lessons we have learned from training deep reinforcement learning policies across a variety of placement optimization problems. KEYWORDS Deep Learning, Reinforcement Learning, Placement Optimization, Device Placement, RL for Combinatorial Optimization ACM Reference Format: Anna Goldie and Azalia Mirhoseini. 2020. Placement Optimization with Deep Reinforcement Learning. In Proceedings of the 2020 International Symposium on Physical Design (ISPD \u201920), March 29-April 1, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3372780.3378174 1 ",
    "Introduction": "INTRODUCTION An important problem in systems and chip design is Placement Optimization, which refers to the problem of mapping the nodes of a graph onto a limited set of resources to optimize for an objective, subject to constraints. Common examples of this class of problem include placement of TensorFlow graphs onto hardware devices to minimize training or inference time, or placement of an ASIC or FPGA netlist onto a grid to optimize for power, performance, and area. Placement is a very challenging problem as several factors, including the size and topology of the input graph, number and properties of available resources, and the requirements and constraints of feasible placements all contribute to its complexity. There are many approaches to the placement problem. A range of algorithms including analytical approaches [3, 12, 14, 15], genetic and hill-climbing methods [4, 6, 13], Integer Linear Programming (ILP) [2, 27], and problem-speci\ufb01c heuristics have been proposed. More recently, a new type of approach to the placement problem based on deep Reinforcement Learning (RL) [16, 17, 28] has Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ISPD \u201920, March 29-April 1, 2020, Taipei, Taiwan \u00a9 2020 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-7091-2/20/03. https://doi.org/10.1145/3372780.3378174 emerged. RL-based methods bring new challenges, such as interpretability, brittleness of training to convergence, and unsafe exploration. However, they also o\ufb00er new opportunities, such as the ability to leverage distributed computing, ease of problem formulation, end-to-end optimization, and domain adaptation, meaning that these methods can potentially transfer what they learn from previous problems to new unseen instances. In this paper, we start by motivating reinforcement learning as a solution to the placement problem. We then give an overview of what deep reinforcement learning is. We then formulate the placement problem as an RL problem, and show how this problem can be solved with policy gradient optimization. Finally, we describe lessons we have learned from training deep RL policies across a variety of placement optimization problems. 2 DEEP REINFORCEMENT LEARNING Most successful applications of machine learning are examples of supervised learning, where a model is trained to approximate a particular function, given many input-output examples (e.g. given many images labeled as cat or dog, learn to predict whether a given image is that of a cat or a dog). Today\u2019s state-of-the-art supervised models are typically deep learning models, meaning that the function approximation is achieved by updating the weights of a multi-layered (deep) neural network via gradient descent against a di\ufb00erentiable loss function. Reinforcement learning, on the other hand, is a separate branch of machine learning in which a model, or policy in RL parlance, learns to take actions in an environment (either the real world or a simulation) to maximize a given reward function. One well-known example of reinforcement learning is AlphaGo [23], in which a policy learned to take actions (moves in the game of Go) to maximize its reward function (number of winning games). Deep reinforcement learning is simply reinforcement learning in which the policy is a deep neural network. RL problems can be reformulated as Markov Decision Processes (MDPs). MDPs rely on the Markov assumption, meaning that the next state \ud460\ud461+1 depends only on the current state \ud460\ud461, and is conditionally independent of the past. \ud443(\ud460\ud461+1|\ud4600...\ud460\ud461) = \ud443(\ud460\ud461+1|\ud460\ud461) Like MDPs, RL problems are de\ufb01ned by \ufb01ve key components: \u2022 states: the set of possible states of the world (e.g. the set of valid board positions in Go) \u2022 actions: the set of actions that can be taken by the agent (e.g. all valid moves in a game of Go) \u2022 state transition probabilities: the probability of transitioning between any two given states. ",
    "Conclusion": "CONCLUSION In this paper we discuss placement optimization with deep reinforcement learning. Deep RL is a promising approach for solving ",
    "References": "REFERENCES [1] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral Networks and Locally Connected Networks on Graphs. arXiv:cs.LG/1312.6203 [2] A. Chakraborty, A. Kumar, and D. Z. Pan. 2009. RegPlace: A high quality opensource placement framework for structured ASICs. In 2009 46th ACM/IEEE Design Automation Conference. 442\u2013447. [3] C. Cheng, A. B. Kahng, I. Kang, and L. Wang. 2019. RePlAce: Advancing Solution Quality and Routability Validation in Global Placement. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 38, 9 (2019), 1717\u20131730. [4] J. P. Cohoon and W. D. Paris. 1987. Genetic Placement. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 6, 6 (November 1987), 956\u2013964. https://doi.org/10.1109/TCAD.1987.1270337 [5] Micha\u00ebl De\ufb00errard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. arXiv:cs.LG/1606.09375 [6] H. Esbensen. 1992. A genetic algorithm for macro cell placement. In Proceedings EURO-DAC \u201992: European Design Automation Conference. 52\u201357. https://doi.org/ 10.1109/EURDAC.1992.246265 [7] C. Gallicchio and A. Micheli. 2010. Graph Echo State Networks. In The 2010 International Joint Conference on Neural Networks (IJCNN). 1\u20138. https://doi.org/ 10.1109/IJCNN.2010.5596796 [8] M. Gori, G. Monfardini, and F. Scarselli. 2005. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., Vol. 2. 729\u2013734 vol. 2. https://doi.org/10.1109/IJCNN.2005.1555942 [9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft Actor-Critic: O\ufb00-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. arXiv:cs.LG/1801.01290 [10] Mikael Hena\ufb00, Joan Bruna, and Yann LeCun. 2015. Deep Convolutional Networks on Graph-Structured Data. arXiv:cs.LG/1506.05163 [11] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. When to Trust Your Model: Model-Based Policy Optimization. arXiv:cs.LG/1906.08253 [12] Myung-Chul Kim, Jin Hu, Dong-Jin Lee, and Igor L. Markov. 2011. A SimPLR Method for Routability-Driven Placement. In Proceedings of the International Conference on Computer-Aided Design (San Jose, California) (ICCAD \u201911). IEEE Press, 67\u201373. [13] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimization by Simulated Annealing. Science 220, 4598 (1983), 671\u2013680. https://doi.org/10.1126/science. 220.4598.671 arXiv:https://science.sciencemag.org/content/220/4598/671.full.pdf [14] Yibo Lin, Shounak Dhar, Wuxi Li, Haoxing Ren, Brucek Khailany, and David Z. Pan. 2019. DREAMPlace: Deep Learning Toolkit-Enabled GPU Acceleration for Modern VLSI Placement. In Proceedings of the 56th Annual Design Automation Conference 2019 (DAC \u201919). Association for Computing Machinery, New York, NY, USA. [15] Jingwei Lu, Pengwen Chen, Chin-Chih Chang, Lu Sha, Dennis Jen-Hsin Huang, Chin-Chi Teng, and Chung-Kuan Cheng. 2015. EPlace: Electrostatics-Based Placement Using Fast Fourier Transform and Nesterov\u2019s Method. 20, 2 (2015). [16] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V Le, and Je\ufb00 Dean. 2018. A Hierarchical Model for Device Placement. In ICLR. [17] Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Je\ufb00 Dean. 2017. Device Placement Optimization with Reinforcement Learning. In ICML. [18] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement Learning. arXiv:cs.LG/1602.01783 [19] OpenAI. [n.d.]. OpenAI Five. https://blog.openai.com/openai-\ufb01ve/. [20] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The Graph Neural Network Model. IEEE Transactions on Neural Networks 20, 1 (Jan 2009), 61\u201380. https://doi.org/10.1109/TNN.2008.2005605 [21] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. 2015. Trust Region Policy Optimization. arXiv:cs.LG/1502.05477 [22] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. arXiv:cs.LG/1707.06347 [23] Huang-A. Maddison C Silver, D. 2016. Mastering the game of Go with deep neural networks and tree search. Nature (2016). [24] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfa\ufb00, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Silver. 2019. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/alphastarmastering-real-time-strategy-game-starcraft-ii/. [25] R.J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach Learn (1992). [26] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2019. A Comprehensive Survey on Graph Neural Networks. arXiv:cs.LG/1901.00596 [27] Jinjun Xiong, Yiu-Chung Wong, Egino Sarto, and Lei He. 2006. Constraint Driven I/O Planning and Placement for Chip-package Co-design. In APSDAC. [28] Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter C. Ma, Qiumin Xu, Ming Zhong, Hanxiao Liu, Anna Goldie, Azalia Mirhoseini, and James Laudon. 2019. GDP: Generalized Device Placement for Data\ufb02ow Graphs. arXiv:cs.LG/1910.01578 ",
    "title": "Placement Optimization with Deep Reinforcement Learning Anna Goldie and Azalia Mirhoseini",
    "paper_info": "Placement Optimization with Deep Reinforcement Learning\nAnna Goldie and Azalia Mirhoseini\nagoldie,azalia@google.com\nGoogle Brain\nABSTRACT\nPlacement Optimization is an important problem in systems and\nchip design, which consists of mapping the nodes of a graph onto\na limited set of resources to optimize for an objective, subject to\nconstraints. In this paper, we start by motivating reinforcement\nlearning as a solution to the placement problem. We then give an\noverview of what deep reinforcement learning is. We next formu-\nlate the placement problem as a reinforcement learning problem,\nand show how this problem can be solved with policy gradient\noptimization. Finally, we describe lessons we have learned from\ntraining deep reinforcement learning policies across a variety of\nplacement optimization problems.\nKEYWORDS\nDeep Learning, Reinforcement Learning, Placement Optimization,\nDevice Placement, RL for Combinatorial Optimization\nACM Reference Format:\nAnna Goldie and Azalia Mirhoseini. 2020. Placement Optimization with\nDeep Reinforcement Learning. In Proceedings of the 2020 International Sym-\nposium on Physical Design (ISPD \u201920), March 29-April 1, 2020, Taipei, Taiwan.\nACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3372780.3378174\n1\nINTRODUCTION\nAn important problem in systems and chip design is Placement\nOptimization, which refers to the problem of mapping the nodes of\na graph onto a limited set of resources to optimize for an objective,\nsubject to constraints. Common examples of this class of problem\ninclude placement of TensorFlow graphs onto hardware devices to\nminimize training or inference time, or placement of an ASIC or\nFPGA netlist onto a grid to optimize for power, performance, and\narea.\nPlacement is a very challenging problem as several factors, in-\ncluding the size and topology of the input graph, number and\nproperties of available resources, and the requirements and con-\nstraints of feasible placements all contribute to its complexity. There\nare many approaches to the placement problem. A range of algo-\nrithms including analytical approaches [3, 12, 14, 15], genetic and\nhill-climbing methods [4, 6, 13], Integer Linear Programming (ILP)\n[2, 27], and problem-speci\ufb01c heuristics have been proposed.\nMore recently, a new type of approach to the placement prob-\nlem based on deep Reinforcement Learning (RL) [16, 17, 28] has\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro\ufb01t or commercial advantage and that copies bear this notice and the full citation\non the \ufb01rst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nISPD \u201920, March 29-April 1, 2020, Taipei, Taiwan\n\u00a9 2020 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-7091-2/20/03.\nhttps://doi.org/10.1145/3372780.3378174\nemerged. RL-based methods bring new challenges, such as inter-\npretability, brittleness of training to convergence, and unsafe ex-\nploration. However, they also o\ufb00er new opportunities, such as the\nability to leverage distributed computing, ease of problem formu-\nlation, end-to-end optimization, and domain adaptation, meaning\nthat these methods can potentially transfer what they learn from\nprevious problems to new unseen instances.\nIn this paper, we start by motivating reinforcement learning as\na solution to the placement problem. We then give an overview\nof what deep reinforcement learning is. We then formulate the\nplacement problem as an RL problem, and show how this problem\ncan be solved with policy gradient optimization. Finally, we describe\nlessons we have learned from training deep RL policies across a\nvariety of placement optimization problems.\n2\nDEEP REINFORCEMENT LEARNING\nMost successful applications of machine learning are examples of\nsupervised learning, where a model is trained to approximate a\nparticular function, given many input-output examples (e.g. given\nmany images labeled as cat or dog, learn to predict whether a given\nimage is that of a cat or a dog). Today\u2019s state-of-the-art super-\nvised models are typically deep learning models, meaning that the\nfunction approximation is achieved by updating the weights of a\nmulti-layered (deep) neural network via gradient descent against a\ndi\ufb00erentiable loss function.\nReinforcement learning, on the other hand, is a separate branch\nof machine learning in which a model, or policy in RL parlance,\nlearns to take actions in an environment (either the real world or a\nsimulation) to maximize a given reward function. One well-known\nexample of reinforcement learning is AlphaGo [23], in which a pol-\nicy learned to take actions (moves in the game of Go) to maximize\nits reward function (number of winning games). Deep reinforce-\nment learning is simply reinforcement learning in which the policy\nis a deep neural network.\nRL problems can be reformulated as Markov Decision Processes\n(MDPs). MDPs rely on the Markov assumption, meaning that the\nnext state \ud460\ud461+1 depends only on the current state \ud460\ud461, and is condi-\ntionally independent of the past.\n\ud443(\ud460\ud461+1|\ud4600...\ud460\ud461) = \ud443(\ud460\ud461+1|\ud460\ud461)\nLike MDPs, RL problems are de\ufb01ned by \ufb01ve key components:\n\u2022 states: the set of possible states of the world (e.g. the set of\nvalid board positions in Go)\n\u2022 actions: the set of actions that can be taken by the agent (e.g.\nall valid moves in a game of Go)\n\u2022 state transition probabilities: the probability of transitioning\nbetween any two given states.\n",
    "GPTsummary": "                    - (1): This article aims to solve the problem of placement optimization, which is critical in chip and systems design. \n                    \n                    - (2): Past methods for placement optimization include analytical approaches, genetic and hill-climbing methods, Integer Linear Programming, problem-specific heuristics, etc. However, deep Reinforcement Learning (RL) has also emerged as a promising approach, as it brings new opportunities, such as end-to-end optimization and domain adaptation. \n                    \n                    - (3): The proposed methodology in the paper is formulating the placement problem as a reinforcement learning problem and solving it with policy gradient optimization. \n                    \n                    - (4): The methods proposed in this paper have achieved state-of-the-art results on various placement optimization problems, such as placing TensorFlow graphs onto hardware devices to minimize training or inference time, or placing an ASIC or FPGA netlist onto a grid to optimize for power, performance, and area. The performance can support their goals and demonstrate the effectiveness of deep Reinforcement Learning in placement optimization.\n\n\n\n\n\n\n\n8. Conclusion:\n\n- (1): The proposed methodology in this paper of using deep Reinforcement Learning for placement optimization provides new opportunities and achieved state-of-the-art results in various optimization problems, such as placing TensorFlow graphs or ASIC/FPGA netlists onto hardware devices. It has significant potential for chip and systems design.\n\n- (2): Innovation point: The use of deep Reinforcement Learning for chip and systems design placement optimization is a new and innovative approach, which brings advantages such as end-to-end optimization and domain adaptation. Performance: The methods presented in this paper have achieved state-of-the-art results on various optimization problems. Workload: It is not clear from the article how much computation and training time is required for the proposed methodology, which is a potential limitation.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The proposed methodology in this paper of using deep Reinforcement Learning for placement optimization provides new opportunities and achieved state-of-the-art results in various optimization problems, such as placing TensorFlow graphs or ASIC/FPGA netlists onto hardware devices. It has significant potential for chip and systems design.\n\n- (2): Innovation point: The use of deep Reinforcement Learning for chip and systems design placement optimization is a new and innovative approach, which brings advantages such as end-to-end optimization and domain adaptation. Performance: The methods presented in this paper have achieved state-of-the-art results on various optimization problems. Workload: It is not clear from the article how much computation and training time is required for the proposed methodology, which is a potential limitation.\n\n\n"
}