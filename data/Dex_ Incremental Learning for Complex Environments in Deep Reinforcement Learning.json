{
    "Abstract": "Abstract This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from \ufb01rst solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We \ufb01nally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention. 1 ",
    "Introduction": "Introduction Complex environments such as Go, Starcraft, and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved. They often require long, precise sequences of actions and domain knowledge in order to obtain reward, and have yet to be learned from random weight initialization. Solutions to these problems would mark a signi\ufb01cant breakthrough on the path to arti\ufb01cial general intelligence. Recent works in reinforcement learning have shown that environments such as Atari games [2] can be learned from pixel input to superhuman expertise [9]. The agents start with randomly initialized weights, and learn largely from trial and error, relying on a reward signal to indicate performance. Despite these successes, complex games, including those where rewards are sparse such as Montezuma\u2019s Revenge, have been notoriously dif\ufb01cult to learn. While methods such as intrinsic motivation [3] have been used to partially overcome these challenges, we suspect this becomes intractable as complexity increases. Additionally, as environments become more complex, they will become more expensive to simulate. This poses a signi\ufb01cant problem, since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms, representing days of training on a single machine. Thus, it appears likely that complex environments will become too costly to learn from randomly initialized weights, due both to the increased simulation cost as well as the inherent dif\ufb01culty of the task. Therefore, some form of prior information must be given to the agent. This can be seen with AlphaGo [18], where the agent never learned to play the game without \ufb01rst using supervised learning on human games. While supervised learning certainly has been shown to aid reinforcement learning, it is very costly to obtain suf\ufb01cient samples and requires the environment to be a task humans can play with reasonable skill, and is therefore impractical for a wide variety of important reinforcement learning problems. In this paper we introduce Dex, the \ufb01rst continual reinforcement learning toolkit for training and evaluating continual learning methods. We present and demonstrate a novel continual learning method arXiv:1706.05749v1  [stat.ML]  19 Jun 2017 we call incremental learning to solve complex environments. In incremental learning, environments are framed as a task to be learned by an agent. This task can be split into a series of subtasks that are solved simultaneously. Similar to how natural language processing and object detection are subtasks of neural image caption generation [23], reinforcement learning environments also have subtasks relevant to a given environment. These subtasks often include player detection, player control, obstacle detection, enemy detection, and player-object interaction, to name a few. These subtasks are common to many environments, but they are often suf\ufb01ciently different in function and representation that reinforcement learning algorithms fail to generalize them across environments, such as in Atari. These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments, and are a reason for humans superior data ef\ufb01ciency in learning complex tasks. In the case of deliberately similar environments, we can construct the subtasks such that they are similar in function and representation that an agent trained on the \ufb01rst environment can accelerate learning on the second environment due to its preconstructed subtask representations, thus partially avoiding the more complex environment\u2019s increased simulation cost and inherent learning dif\ufb01culty. 2 Related work Transfer learning [13] is the method of utilizing data from one domain to enhance learning of another domain. While sharing signi\ufb01cant similarities to continual learning, transfer learning is applicable across all machine learning domains, rather than being con\ufb01ned to reinforcement learning. For example, it has had signi\ufb01cant use with using networks trained on ImageNet [5] to accelerate or enhance learning and classi\ufb01cation accuracy by \ufb01netuning in a variety of vision tasks [12]. While the concept of continual learning, where an agent learns from a variety of experiences to enhance future learning, has been de\ufb01ned for some time [15], it has remained largely untapped by recent powerful algorithms that are best \ufb01t to bene\ufb01t from its effects. Recent work has been done with Progressive Neural Networks [17], where transfer learning was used to apply positive transfer in a variety of reinforcement learning domains. However, our method differs in that it does not add additional parameters for each environment or use lateral connections to features that result in increased memory space and training time. Recent work related to subtask utilization comes from Kirkpatrick et al. [7], which shows that expertise can be maintained on multiple environments that have not been experienced for a long time through elastic weight consolidation (EWC). Viable weights were found that simultaneously achieve expertise in a variety of Atari games. Incremental learning similarly trains on multiple environments, but with the goal of achieving enhanced expertise in a single environment, rather than expertise in all environments. We leave it to future work to overcome this limitation. 3 Dex Dex is a novel deep reinforcement learning toolkit for training and evaluating agents on continual learning methods. Dex acts as a wrapper to the game Open Hexagon [16], sending screen pixel information, reward information and performing actions via an OpenAI Gym like API [4]. Dex contains hundreds of levels, each acting as their own environment. These environments are collected into groups of similar environments for the task of continual learning. Dex environments vary greatly in dif\ufb01culty, ranging from very simple levels where agents achieve superhuman performance in less that 4 minutes, to levels we consider far more complex than any previously learned environments. Refer to videos available at github.com/innixma/dex of the Dex environments shown in Figure 1, as screenshots do not capture the environment complexity. Open Hexagon is a game involving navigating a triangle around a center \ufb01gure to avoid incoming randomly generated walls. Screenshots of various environments from the game are shown in Figure 1. The game progresses regardless of player action, and thus the player must react to the environment in real-time. If the player contacts a wall the game is over. At each point in the game, a player has only three choices for actions: move left, right, or stay put. It is a game of survival, with the score and thus total reward being the survival time in seconds. Open Hexagon contains hundreds of levels 2 drastically ranging in dif\ufb01culty, yet they each contain many similar core subtasks. This makes Open Hexagon an ideal platform for testing continual learning methods. (a) Distortion (b) Lanes (c) Reversal (d) System (e) Desync (f) Stutter (g) Arithmetic (h) Stretch (i) Open (j) Overcharge (k) Enneagon (l) Blink (m) Tunnel Town (n) Roulette (o) Echnoderm (p) Transmission (q) Triptych (r) Duality (s) World Battle (t) Universe (u) Apeirogon (v) Euclidean (w) Pi (x) Golden Ratio Figure 1: Dex environments. The small triangle is the player that must be rotated around the center to avoid incoming walls. Many of these environments incorporate various distortion effects that are not evident in screenshots. Reversal periodically \ufb02ips game controls, and some environments even add additional actions, such as in Arithmetic, which requires the agent to correctly solve various math equations during the level through the numpad. The Dex toolkit along with its source code is available at github.com/innixma/dex. 4 Incremental learning The novel continual learning method of incremental learning is de\ufb01ned as follows. In the formal case, an agent must learn from a series of n environments E, each with identical legal game actions A = {1, ..., K}. Note that any series of environments can be made to have the same action space by considering A to be the superset of all possible actions in each game, with new actions performing identically to no action, assuming no action is within the action space. Each environment Ei has a corresponding cost per step ci > 0 and a step count si \u2265 0, indicating the number of steps taken in that environment. Typically, more complex environments will have a higher cost per step. A resource maximum M is given, which indicates the total amount of data that can be gathered, shown in the following inequality: M \u2265 n \ufffd i=1 cisi The problem is to maximize the mean total reward R of the agent in the goal environment, En, while maintaining the above inequality. Steps can be taken in any order from the n environments, and there 3 is no assumed correlation between the environments beyond their data and action dimensions. While it may appear an optimal solution to only examine and gather data from En, as has been done for virtually all reinforcement learning algorithms in the past, this is not always the case. For example, if En\u22121 and En are highly correlated, and cn\u22121 << cn, then training with En\u22121 may be superior due to its lesser cost. Additionally, an environment En\u22121 may contain important aspects of En, while avoiding state spaces and rewards that are not useful for training, potentially allowing training on En\u22121 to be optimal, even if cn\u22121 > cn. By taking environments E to represent all real environments with their respective costs, the solution to this problem corresponds to the globally optimal sequence of training steps to achieve maximum performance with a \ufb01nite amount of computational resources for a given algorithm. It therefore necessarily contains the solution of achieving arti\ufb01cial general intelligence with minimal resources. Unfortunately, this has several drawbacks. Most importantly, the selection of the optimal environments is not obvious, and their order even less so. While this formal de\ufb01nition is useful to de\ufb01ne incremental learning, the following simpli\ufb01ed version will be what this paper focuses on. In the simpli\ufb01ed case, all variables are the same, except that steps must be taken in environments sequentially, without going back to previous environments. Thus, once a step has been taken in Ei, no steps may be taken in future environments Ej where j < i. Furthermore, it is assumed that the environments are correlated, where environment Ei\u22121 contains a subset of subtasks in environment Ei, with environment Ei being typically harder than environment Ei\u22121. The intuition behind this simpli\ufb01ed case is that simple environments are both cheap to simulate and easy to learn, and that the features and strategies learned from the simple environment could transfer to a more dif\ufb01cult correlated environment. This process can be done repeatedly, producing a compounding acceleration of learning as additional useful incremental environments are added. Thus, the general process of incremental environment selection is to use easier subsets of the goal environment. Furthermore, this means that every environment Ei in a simpli\ufb01ed incremental learning problem can be seen as the goal environment in an easier problem containing E1:i. 5 Baseline learning algorithm and model architecture To analyse the effectiveness of incremental learning, our agents learned environments from Dex. For training agents, we use Asynchronous Advantage Actor-Critic (A3C) framework introduced in [10], coupled with the network architecture described below. ConvNet implementation details. All Dex experiments were learned by a network with the following architecture. The network takes as input a series of 2 consecutive 42 \u00d7 42 grayscale images of a game state. It is then processed through a series of 3 convolutional layers, of stride 1 \u00d7 1 and size 5 \u00d7 5, 4 \u00d7 4, and 3 \u00d7 3 with \ufb01lter counts of 32, 64, and 64 respectively. Between each pair of convolutional layers is a 2 dimensional maxpooling layer, of size 2 \u00d7 2. The intermediate output is then \ufb02attened and processed by a dense layer of 512 nodes. The output layers are identical to those speci\ufb01ed for A3C. All convolutional layers and maxpooling layers are zero-padded. This results in an network with approximately 4,000,000 parameters. All intermediate layers are followed by recti\ufb01ed linear units (ReLU) [11], and Adam is used for the optimizer, with a learning rate of 10\u22123. Learning rate is not changed over the course of training as opposed to Mnih et al. [10], but instead stays constant. A gamma of 0.99 is used, along with n-step reward [14] with n = 4 , which is used to speed up value propagation in the network, at a cost of minor instability in the learning. Training is done with batches of 128 samples. The architecture was created using TensorFlow 1.1.0 [1], and Keras 2.0.3. Since Dex runs in real time, a slightly altered method of learning was utilized to avoid inconsistent time between steps. Our implemented A3C was modi\ufb01ed to work in an of\ufb02ine manner, with experience replay as done in Deep-Q Networks [8]. This is a naive approximation to the more complex ACER algorithm [22]. While this does destabilize the algorithm, which bases its computations on data being representative of the network in its current state, the agents are still able to learn the environments and signi\ufb01cantly outperform Double Deep-Q Networks [20], thus serving as a reasonable baseline. 4 ",
    "Experiments": "Experiments So far, we have performed experiments on ten different environments in Dex for incremental learning. These ten environments are split into two incremental learning sets of three and seven environments. The \ufb01rst set will be referred to as a, and deals with increasingly complex patterns. The second set will be referred to as b, and deals with increasingly complex task representation. The results show that incremental learning can have a signi\ufb01cant positive impact on learning speed and task performance, but can also lead to bad initializations when trained on overly simplistic environments. Code to reproduce the experiments in this paper will be released at a future date. 6.1 Setup In the following experiments, we naively assume that achieving near optimal performance in Ei with minimal resources requires as a prerequisite learning Ei\u22121 with minimal resources. This proceeds downward to the base case of E1, which can be considered a trivially solvable environment from random weight initialization. This assumption is used to simplify training. Furthermore, due to the exploratory nature of the baseline experiments, the costs per step are ignored, as we seek only to show that positive feature transfer is occurring, rather than optimizing the feature transfer itself, which we leave for future work. All environments are learned with identical architecture, algorithm and hyperparameters. We act and gather data on the environments 50 times per second. We use an \u03f5-greedy policy with \u03f5 = 0.05, and a replay memory size of 40,000. Replay memory is initialized by a random agent. For all experiments, agents are trained with 75 batches after each episode. Total reward is equal to the number of seconds survived in the environment each episode. Mean reward is calculated from an hour of testing weights without training. Each episode scales in dif\ufb01culty inde\ufb01nitely the longer the agent survives. 6.2 Models We evaluate four different types of models in this experiment. The \ufb01rst is a random agent for comparison. The second is the baseline, which is the standard reinforcement learning training method. To establish the baseline, each environment Ei is trained on from random initialization for one hour, equivalent to roughly 150,000 training steps. The weights which achieve the maximum total reward in a single episode are selected as the output weights from the training, called wi. A third model, which we shall call initial, is the initial weights to the incremental learning method before continued training. Thus, for environment Ei this model uses weights wi\u22121. This is used to measure the correlation between the two environments. We would expect uncorrelated environments to result in near random agent reward for initial. To establish the incremental learning agents, for each environment Ei we take the weights wi\u22121 outputted by the baseline, using them as the initial weights to training on Ei for an additional hour. The weights outputted by this method we call w\u2032 i. 6.3 Results and Observations The results can be found in Table 1 and Table 2. As can be seen in Table 1, incremental learning provided superior or roughly equivalent results on every environment in set b, with some environments such as b3 and b4 experiencing substantial increases in maximum reward, with the incrementally learned b4 achieving nearly triple the baseline in both maximum and mean reward. However, on the harder environments, there was not signi\ufb01cant improvement seen. This is likely because the earlier environments were not suf\ufb01ciently learned along with the fact that the tasks were generally too dif\ufb01cult to learn in one hour of training, indicated by the near random performance of b6 and b7 on both the baseline and incremental methods. The other set, results shown in Table 2 for set a, show that incremental learning was harmful in the case of a2. This is likely due to the difference in the wall patterns of a1 and a2. In a1, a single wall is on the screen at a time, requiring a simple avoidance. In a2, up to four separate walls occupy the screen at once, requiring a more complex method involving future planning and understanding of which walls are more important in a given state. We suspect that learning a1 leads the agent to 5 Table 1: Set b rewards Max E Random Initial Baseline Incremental b1 31.43 \u2014 425.92 \u2014 b2 8.23 120.77 259.80 280.59 b3 7.73 58.30 132.31 221.96 b4 8.27 35.10 35.66 105.48 b5 8.79 19.76 52.81 41.57 b6 9.33 11.05 10.79 13.11 b7 9.97 7.13 9.59 14.54 Mean E Random Initial Baseline Incremental b1 9.10 \u2014 169.54 \u2014 b2 2.23 36.05 92.24 85.52 b3 2.43 8.22 41.17 66.32 b4 2.17 5.30 8.11 26.75 b5 1.88 4.17 12.23 11.61 b6 2.17 2.89 2.15 2.25 b7 2.30 2.34 2.08 2.58 This table shows the max and mean reward for an agent on the given environment E with a given training method, as described in the experimental setup. Here we observe that incremental learning provided superior results to the baseline in nearly all environments, particularly b3 and b4. Table 2: Set a rewards Max E Random Initial Baseline Incremental a1 40.73 \u2014 771.67 \u2014 a2 19.65 53.37 445.85 86.57 a3 10.85 15.69 49.50 59.10 Mean E Random Initial Baseline Incremental a1 8.93 \u2014 717.93 \u2014 a2 7.46 18.34 87.06 18.31 a3 6.01 7.32 9.81 13.52 This table shows the max and mean reward for an agent on the given environment E with a given training method, as described in the experimental setup. Here we observe that incremental learning provided inferior results to the baseline in the a2 environment, likely due to over\ufb01tting. 6 overtrain on a variety of weights, hindering future learning. This indicates that certain environments may be too simple to include in incremental learning, as a1 can be learned to a reward of over 700 in less than four minutes. Additionally, the initial model shows that the environments are correlated, with generally far superior performance than random, despite never training on the environment explicitly. In the case of b4, it is nearly equal to the baseline in the maximum metric, indicating signi\ufb01cant correlation. This is likely a reason for the greatly enhanced performance of incremental learning over the baseline in b4. Note that these experimental results are strictly a simple baseline for both Dex and incremental learning, and suffer from instability due to the short timeline of training and lack of repeated experiments. This means that agents do not necessarily consistently improve throughout training, but rather may quickly improve to maximum performance followed by decreased performance for the remainder of training. We leave more comprehensive experiments to future work. 7 Visualization To qualitatively analyze the effects of incremental learning on a networks weights, we develop a saliency visualization method based on Simonyan et al. [19] for reinforcement learning. Heatmaps are generated with a given networks weights and an input image. The method for gathering the heatmaps is identical to Simonyan et al. [19], and thus equations and derivations shall not be repeated in this paper. The most likely action the network will take at a given frame is used for the action to minimize through the gradient. This is a difference from the supervised learning case, where the ground truth is used. In reinforcement learning, the ground truth is not known, and thus must be inferred, as done in Wang et al. [21]. Results of the visualization on the trained weights of environment sets a and b can be seen in Figure 2 and Figure 3. Figure 2: Saliency mappings of a state from environment a3, with weights from set a. The \ufb01rst row consists of the trained baseline weights w1 to w3. The second row consists of the incrementally learned weights w\u2032 2 to w\u2032 3. Figure 3: Saliency mappings of a state from environment b3, with weights from set b. The \ufb01rst row consists of the trained baseline weights w1 to w7. The second row consists of the incrementally learned weights w\u2032 2 to w\u2032 7. 7 ",
    "Conclusion": "Conclusion The ability to learn and transfer knowledge across domains is essential to the advancement of agents that can solve complex tasks. This paper introduced the continual learning toolkit Dex for training and evaluation of continual learning methods. We proposed the method of incremental learning for deep reinforcement learning, and demonstrated its ability to accelerate learning and produce drastically superior results to standard training methods in multiple Dex environments, supporting the notion of avoiding randomly initialized weights and instead using continual learning techniques to solve complex tasks. Source code for both the training methods used in this paper as well as the Dex toolkit source code can be found at github.com/innixma/dex. Acknowledgments We thank Vittorio Romeo for designing Open Hexagon. 8 ",
    "References": "References [1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. J\u00f3zefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man\u00e9, R. Monga, S. Moore, D. G. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker, V. Vanhoucke, V. Vasudevan, F. B. Vi\u00e9gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor\ufb02ow: Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467, 2016. [2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. CoRR, abs/1207.4708, 2012. [3] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. CoRR, abs/1606.01868, 2016. [4] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. [5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009. [6] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR, abs/1611.05397, 2016. [7] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017. doi: 10.1073/pnas.1611835114. [8] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. [9] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. [10] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. CoRR, abs/1602.01783, 2016. [11] V. Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted boltzmann machines. In J. F\u00fcrnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814. Omnipress, 2010. [12] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014. [13] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010. [14] J. Peng and R. J. Williams. Incremental multi-step q-learning. Machine Learning, 22(1): 283\u2013290, 1996. ISSN 1573-0565. doi: 10.1023/A:1018076709321. [15] M. B. Ring. Child: A \ufb01rst step towards continual learning. Machine Learning, 28(1):77\u2013104, 1997. [16] V. Romeo. Open hexagon, 2012. [17] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016. 9 [18] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587): 484\u2013489, jan 2016. ISSN 0028-0836. doi: 10.1038/nature16961. [19] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classi\ufb01cation models and saliency maps. CoRR, abs/1312.6034, 2013. [20] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. CoRR, abs/1509.06461, 2015. [21] Z. Wang, N. de Freitas, and M. Lanctot. Dueling network architectures for deep reinforcement learning. CoRR, abs/1511.06581, 2015. [22] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. Sample ef\ufb01cient actor-critic with experience replay. CoRR, abs/1611.01224, 2016. [23] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044, 2015. 10 ",
    "title": "",
    "paper_info": "Dex: Incremental Learning for Complex\nEnvironments in Deep Reinforcement Learning\nNick Erickson, Qi Zhao\nDepartment of Science and Engineering\nUniversity of Minnesota, Twin Cities\nMinneapolis, MN 55455\neric3068@umn.edu, qzhao@cs.umn.edu\nAbstract\nThis paper introduces Dex, a reinforcement learning environment toolkit special-\nized for training and evaluation of continual learning methods as well as general\nreinforcement learning problems. We also present the novel continual learning\nmethod of incremental learning, where a challenging environment is solved using\noptimal weight initialization learned from \ufb01rst solving a similar easier environment.\nWe show that incremental learning can produce vastly superior results than standard\nmethods by providing a strong baseline method across ten Dex environments. We\n\ufb01nally develop a saliency method for qualitative analysis of reinforcement learning,\nwhich shows the impact incremental learning has on network attention.\n1\nIntroduction\nComplex environments such as Go, Starcraft, and many modern video-games present profound\nchallenges in deep reinforcement learning that have yet to be solved. They often require long, precise\nsequences of actions and domain knowledge in order to obtain reward, and have yet to be learned from\nrandom weight initialization. Solutions to these problems would mark a signi\ufb01cant breakthrough on\nthe path to arti\ufb01cial general intelligence.\nRecent works in reinforcement learning have shown that environments such as Atari games [2]\ncan be learned from pixel input to superhuman expertise [9]. The agents start with randomly\ninitialized weights, and learn largely from trial and error, relying on a reward signal to indicate\nperformance. Despite these successes, complex games, including those where rewards are sparse such\nas Montezuma\u2019s Revenge, have been notoriously dif\ufb01cult to learn. While methods such as intrinsic\nmotivation [3] have been used to partially overcome these challenges, we suspect this becomes\nintractable as complexity increases. Additionally, as environments become more complex, they will\nbecome more expensive to simulate. This poses a signi\ufb01cant problem, since many Atari games\nalready require upwards of 100 million steps using state-of-the-art algorithms, representing days of\ntraining on a single machine.\nThus, it appears likely that complex environments will become too costly to learn from randomly\ninitialized weights, due both to the increased simulation cost as well as the inherent dif\ufb01culty of the\ntask. Therefore, some form of prior information must be given to the agent. This can be seen with\nAlphaGo [18], where the agent never learned to play the game without \ufb01rst using supervised learning\non human games. While supervised learning certainly has been shown to aid reinforcement learning,\nit is very costly to obtain suf\ufb01cient samples and requires the environment to be a task humans can\nplay with reasonable skill, and is therefore impractical for a wide variety of important reinforcement\nlearning problems.\nIn this paper we introduce Dex, the \ufb01rst continual reinforcement learning toolkit for training and\nevaluating continual learning methods. We present and demonstrate a novel continual learning method\narXiv:1706.05749v1  [stat.ML]  19 Jun 2017\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): This work proposes a solution to the problem of learning complex environments in deep reinforcement learning by utilizing prior information to accelerate learning. The method of incremental learning is used to split a challenging environment into a series of subtasks, which are solved simultaneously. By sharing information learned from solving easier environments, the method achieves state-of-the-art performance on several tested environments, supporting the notion of avoiding randomly initialized weights and instead using continual learning techniques to solve complex tasks.\n\n- (2): Innovation point: The proposed method of incremental learning for deep reinforcement learning is innovative in its approach to utilizing prior information to accelerate learning in complex environments. Performance: Experimental results show that the method achieves state-of-the-art performance on several tested environments and produces vastly superior results to standard training methods. Workload: The workload required for implementing this method is not extensively discussed in the paper, but the source code for both the training methods and Dex toolkit is available on Github.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This work proposes a solution to the problem of learning complex environments in deep reinforcement learning by utilizing prior information to accelerate learning. The method of incremental learning is used to split a challenging environment into a series of subtasks, which are solved simultaneously. By sharing information learned from solving easier environments, the method achieves state-of-the-art performance on several tested environments, supporting the notion of avoiding randomly initialized weights and instead using continual learning techniques to solve complex tasks.\n\n- (2): Innovation point: The proposed method of incremental learning for deep reinforcement learning is innovative in its approach to utilizing prior information to accelerate learning in complex environments. Performance: Experimental results show that the method achieves state-of-the-art performance on several tested environments and produces vastly superior results to standard training methods. Workload: The workload required for implementing this method is not extensively discussed in the paper, but the source code for both the training methods and Dex toolkit is available on Github.\n\n\n"
}