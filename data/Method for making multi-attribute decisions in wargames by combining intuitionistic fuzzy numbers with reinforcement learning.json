{
    "Abstract": "Abstract Researchers are increasingly focusing on intelligent games as a hot research area.The article proposes an algorithm that combines the multi-attribute management and reinforcement learning methods, and that combined their effect on wargaming, it solves the problem of the agent's low rate of winning against specific rules and its inability to quickly converge during intelligent wargame training.At the same time, this paper studied a multi-attribute decision making and reinforcement learning algorithm in a wargame simulation environment, and obtained data on red and blue conflict.Calculate the weight of each attribute based on the intuitionistic fuzzy number weight calculations. Then determine the threat posed by each opponent's chess pieces.Using the red side reinforcement learning reward function, the AC framework is trained on the reward function, and an algorithm combining multi-attribute decision-making with reinforcement learning is obtained. A simulation experiment confirms that the algorithm of multi-attribute decision-making combined with reinforcement learning presented in this paper is significantly more intelligent than the pure reinforcement learning algorithm.By resolving the shortcomings of the agent's neural network, coupled with sparse rewards in large-map combat games, this robust algorithm effectively reduces the difficulties of convergence. It is also the first time in this field that an algorithm design for intelligent wargaming combines multi-attribute decision making with reinforcement learning.Attempt interdisciplinary cross-innovation in the academic field, like designing intelligent wargames and improving reinforcement learning algorithms. Key words: Wargame, Reinforcement learning, Multiple attribute decision making, Intelligent game 1\u3001",
    "Introduction": "Introduction Artificial intelligence and machine learning are becoming more common in real-world applications, and games are increasingly fighting against humans through training agents. AlphaGo, an artificial intelligence that has achieved success in the field of Go, and Alphastar, an artificial intelligence that has achieved success in the man-machine conflict of \u2019StarCraft\u2019 are two typical examples. [1-2]. In RTS games, artificial intelligence methods are increasingly being integrated. In the King Glory Game, Ye D used his improved PPO algorithm to train the hero AI, with positive results. [3]. By using reinforcement learning algorithms, Silver D developed a training framework that requires no human knowledge other than the rules of the game, allowing AlphaGo to train itself, and achieving high levels of intelligence in the process [4]. Using deep reinforcement learning and supervised strategy learning, Barrigan improves the AI performance of RTS games, and defeats the built-in AI [5]. AI has become a hot research topic in recent years, showing a wide variety of applications such as deduction and analysis [6-7]. There are still insufficient effective solutions to the problem of convergence and convergence rate under a variety of conditions, especially when it comes to confrontation. Indexes measure the value of things or the parameter of an evaluation system. It is the scale of the effectiveness of things to the subject. As an attribute value, it provides the subjective consciousness or the objective facts expressed in numbers or words. It is important to select a scientifically valid target threat assessment (TA) index and evaluate that index scientifically[8]. Target threat assessment contributes to intelligence wargame decision-making as part of current intelligent wargames. It is mainly based on rules, decision trees, reinforcement learning, and other technologies in the current mainstream game intelligent decision-making field, but rarely incorporates multi-attribute decision-making theory and methods of management into the intelligent decision-making field. The actual wargame data obtained through wargame environments is presented in this paper, as well as the multi-attribute threat assessment indicators that are effectively transformed and presented as a unified expression. Using the three expression forms of real number, interval number, and intuitionistic fuzzy number, the multi-attribute decision-making theory and method are used to analyze the target threat degree, and then the reward function in reinforcement learning is established to train more effective intelligent decision-making algorithm.For the first time, this method combines the multi-attribute decision-making of management science and reinforcement learning of control science. This method appears to be effective based on a wargame experiment. To this end, this article conducts the following research: \uff081\uff09In this article we attempt to combine the multi-attribute decision-making method in management with reinforcement learning in cybernetics for the first time by extracting data from the environment in wargames and combining real numbers, interval numbers, and intuitionistic fuzzy numbers for data expression.Perform the multi-attribute decision analysis on this basis, analyze the threat of the opponent, and then feed back the reward function. The reward function is trained in an algorithm of reinforcement learning to obtain a more ideal result. \uff082\uff09 Increase the training convergence speed. To solve the sparsity problem involved in agent training, one that leads to the emergence of agent strategies that are non-convergent or slowly convergent. The main solution is to use multi-attribute decision making\uff08MADM\uff09to figure out the threat of the opponent in advance and determine it in real time in a certain step, then attack based on the threat in order to obtain additional rewards and add it to the training process. \uff083\uff09Increase the winning rate of chess piece agent training.The intelligence of the agent improves by predicting the opponent's most threatening chess piece and beating it as a result of practice, which is directly reflected in the agent's winning rate against a regular opponent. 2\u3001Related theories 2.1 The reinforcement learning Reinforcement learning is a large category of machine learning that is based on solving interaction problems using Bellman equations [9], helping to improve and ultimately achieve the goal.Ultimately, reinforcement learning causes the agent to form a strategy that maximizes the reward value in order to achieve the goal [10]. During the 1990s, Littman proposed multi-agent reinforcement learning based on the MDP as the framework and applied the ideas and algorithms of reinforcement learning to multi-agent systems, frequently considering both competition between agents and cooperation between them [11]. Reinforcement learning starts with a Markov Process (MDP), which describes the interaction process between the agent and the environment through state and action models. As a general rule, MDP is a quadruple of < S, A, R, T > consisting of 4 elements: (1) S is a finite State Space, which represents all the states of the Agent in the environment; (2) A is a limited Action Space, which contains the actions that the Agent is capable of taking in each state; (3) Rss' a means that the agent performs a action in the s state, and the agent is rewarded by the environment interaction; (4) The state transition function (STF) of an environment is Pss' a = \u2119 St+1 = s'\u2223St = s, At = a , which represents the probability of performing action a on state s and transitioning to state s'. In MDP, the agent interacts with the environment in the way shown in Figure 1. Figure 1 Schematic diagram of reinforcement learning and environment It interprets the current environment state st and selects at action from its action space A; subsequently, the environment sends the corresponding reward rt+1 to the agent, and transfers it to the new environment state st+1. Wait for the agent to make the next new decision [12].When an agent interacts with its environment, there are two uncertainties: one is what kind of action to choose in state S. The strategy \u03c0(a|s) is used to represent a certain strategy of the agent (i.e. the probability distribution from state to action). And the other is the probability Pss' a of a state transition generated by the environment. The goal of reinforcement learning is to find an optimal strategy \u03c0(a|s) so that it can obtain the maximum long-term cumulative reward in any state s and any time step t. * 0 argmax k t k t k r s s \uf070 \uf070 \uf070 \uf067 \uf0a5 \uf02b \uf03d \uf0ec \uf0fc \uf03d \uf03d \uf0ed \uf0fd \uf0ee \uf0fe \uf0e5 \u2223 E Among these, \ufffd\u03c0 represents the expected value given the strategy, \u03b3 \u2208 [0\uff0c1) the discount rate , k the next time period, and rk+t the instant reward the agent receives in the time period (t + k). In reinforcement learning, we mostly learn the optimal strategy \u03c0\u2217 by finding the optimal state value function V\u2217 s or the optimal state action value function Q\u2217(s\uff0ca) . Here are the formulas for V\u2217 s and Q\u2217(s\uff0ca). * 0 ( ) max k t k t k V s r s s \uf070 \uf070 \uf067 \uf0a5 \uf02b \uf03d \uf0ec \uf0fc \uf03d \uf03d \uf0ed \uf0fd \uf0ee \uf0fe \uf0e5 \u2223 E * 0 ( , ) max , k t k t t k Q s a r s s a a \uf070 \uf070 \uf067 \uf0a5 \uf02b \uf03d \uf0ec \uf0fc \uf03d \uf03d \uf03d \uf0ed \uf0fd \uf0ee \uf0fe \uf0e5 \u2223 E Reinforcement learning has attracted researchers' attention in the field of RTS games in the recent years.In this paper, we use reinforcement learning algorithms to make intelligent decision about wargame, establish an intelligent environment for playing wargames based on reinforcement learning algorithms, use reinforcement learning algorithms to select chess pieces, and create a strong AI for game intelligence. 2.2 Making multi-attribute decisions The field of decision-making has been a research hotspot in management, economics, and information science for a long time, and research on decision-making theory and methods has also been fruitful.Multiple attribute decision-making is a limited option selection problem based on multiple attributes that has a wide practical background [13][14][15]. For the multi-attribute utility theory, the value function is used to express the preference theory based on the concept of ordinal comparison and preference strength, and the utility function is used to express the preference theory based on the concept of risk choice [16]. Multi-attribute decision making (MADM) is commonly known as finite-scheme multi-objective decision-making, and it is an essential component of decision-making theory and method research [17]. The first problem of multi-attribute decision making is to determine the scheme set and the attribute set. Let A = {A1, A2,..., An} be the scheme set and G = {G1, G2, G3,..., GM} be the attribute set of multi-attribute decision making. The attribute value of scheme AI to attribute Gj is Yij (i = 1,2,..., n, j = 1,2,..., m). The decision matrix Y (nxm) is composed of Yij. The plan set is the object of decision making. The decision matrix provides information needed to build the decision plan. Different types of analysis use the decision matrix to make decisions. With a system with multiple targets, one target may be considered a decision-making plan, a plan set consisting of all the opponent's targets, such as tanks, helicopters, and infantry.The decision criterion is the threat level our opponent poses to our defending target. It includes a number of attributes that affect the extent of threat, such as target type, target distance, target speed, target attack ability, target defense ability, target environment, and visibility.Based on the actual wargame data obtained, apply reasonable methods to evaluate target threats. 2.3 The intuitionistic fuzzy number While the application of fuzzy number theory to information processing technology has gradually matured, its limitations have slowly emerged[18]. The fuzzy number A has a unique real number in [0,1] corresponding to each of its elements. However, fuzzy number theory cannot be applied to problems of either one or the other nature at the same time. Therefore, scholars have developed fuzzy number theory. In 1986, Atanassov devised intuitionistic fuzzy numbers and explored the nature and theorems of their operations. The intuitionistic fuzzy number definition: In the universe of \uf028 \uf029 0 1 A ix \uf070 \uf0a3 \uf0a3 , the intuitionistic fuzzy set A on X has the following form: \uf028 \uf029 \uf028 \uf029 \uf07b \uf07d , , i A i A i i A x x v x x X \uf06d \uf03d \uf0ce \u2223 In the formula, \uf028 \uf029 : [0,1] A ix X \uf06d \uf0ae and \uf028 \uf029 : [0,1] A i v x X \uf0ae are the membership function and non-membership function of A, respectively, and they hold for all \uf028 \uf029 \uf028 \uf029 ,0 1 i A i A i x X x v x \uf06d \uf0ce \uf0a3 \uf02b \uf0a3 on A. \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 A i A i A i x x v x \uf070 \uf06d \uf03d \uf02d \uf02d is the hesitation in A, and it is a measure of the hesitation of xi with A. Clearly, \uf028 \uf029 0 1 A ix \uf070 \uf0a3 \uf0a3 . As a convenience, call \uf028 \uf029 ,v \uf061 \uf061 \uf061 \uf06d \uf03d the Intuitionistic Fuzzy Number (IFN),There is [0,1] \uf061 \uf06d \uf0ce , [0,1] v\uf061 \uf0ce , and 1 v \uf061 \uf061 \uf06d \uf02b \uf0a3 . Definition 2: The algorithm of intuitionistic fuzzy numbers, set any intuitionistic fuzzy number to \uf028 \uf029 \uf028 \uf029 , , , v v \uf061 \uf061 \uf062 \uf062 \uf061 \uf06d \uf062 \uf06d \uf03d \uf03d , then 1\uff09 \uf028 \uf029 ,v v \uf061 \uf062 \uf061 \uf062 \uf061 \uf062 \uf061 \uf062 \uf06d \uf06d \uf06d \uf06d \uf0c5 \uf03d \uf02b \uf02d \uff1b 2\uff09 \uf028 \uf029 ,v v v v \uf061 \uf062 \uf061 \uf062 \uf061 \uf062 \uf061 \uf062 \uf06d \uf06d \uf0c4 \uf03d \uf02b \uf02d \uff1b 3\uff09 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 1 , , 0 v \uf06c \uf06c \uf061 \uf061 \uf06c\uf061 \uf06d \uf06c \uf03d \uf02d \uf02d \uf03e \uff1b 4\uff09 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 ( ) ,1 1 , 0 v \uf06c \uf06c \uf06c \uf061 \uf061 \uf061 \uf06d \uf06c \uf03d \uf02d \uf02d \uf03e \uff1b Definition 3: In this calculation we assume , , 1,2, , i i i a a a v i n \uf06d \uf03d \uf03d \uf04c is an IFN, \uf028 \uf029 1 2 , , , n w w w \uf03d W \uf04c is a weighted vector, 1 1, [0,1], 1,2, , n j j j w w j n \uf03d \uf03d \uf0ce \uf03d \uf0e5 \uf04c intuitionistic fuzzy weighted chess pieces satisfy IFWA, and it's a n \uf051 \uf0ae \uf051 mapping: \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 2 1 1 2 2 1 1 IFWA , , , 1 1 , j j j j n n w w n n n a a j j a a a w a w a w a v \uf06d \uf03d \uf03d \uf03d \uf0c5 \uf0c5 \uf0c5 \uf03d \uf02d \uf02d \uf0d5 \uf0d5 \uf04c \uf04c Definition 4: \uf07b \uf07d , , L U L U L U a a a x a x a a a \uf0e9 \uf0f9 \uf03d \uf03d \uf0a3 \uf0a3 \uf0ce \uf0eb \uf0fb R \uf025 \u2223 is called an interval number, L a is the lower limit of the interval, and U a is the upper limit. When L U a a \uf03d , a\uf025 degenerate into a real number, the interval number can be regarded as an extension of the real number. Here are the interval numbers , L U a a a \uf0e9 \uf0f9 \uf03d \uf0eb \uf0fb \uf025 and , L U b b b \uf0e9 \uf0f9 \uf03d \uf0eb \uf0fb \uf025 , and the real number 0 \uf06c \uf0b3 . The interval number arithmetic rules are as follows: 1\uff09Addition: , L L U U a b a b a b \uf0e9 \uf0f9 \uf02b \uf03d \uf02b \uf02b \uf0eb \uf0fb \uf025 \uf025 2\uff09Subtraction: , L U U L a b a b a b \uf0e9 \uf0f9 \uf02d \uf03d \uf02d \uf02d \uf0eb \uf0fb \uf025 \uf025 3\uff09Number multiplication: , L U a a a \uf06c \uf06c \uf06c \uf0e9 \uf0f9 \uf03d \uf0eb \uf0fb \uf025 \uff0cparticularly when 0 \uf06c \uf03d , 0 a \uf06c \uf03d \uf025 \uff1b 4\uff09Multiplication: \uf07b \uf07d \uf07b \uf07d min , , , max , , , L L L U U L U U L L L U U L U U a b a b a b a b a b a b a b a b a b \uf0e9 \uf0f9 \uf0d7 \uf03d \uf0eb \uf0fb \uf025 \uf025 ; 5\uff09Division: / , 1/ ,1/ , , 0 L U U L L U a b a a b b b b \uf0e9 \uf0f9 \uf0e9 \uf0f9 \uf03d \uf0d7 \uf0b9 \uf0eb \uf0fb \uf0eb \uf0fb \uf025 3\u3001Wargaming multiple attribute index threat quantification Obtaining scientific evaluation results requires a reasonable quantification of indicators. An important aspect of decision-making assistance in wargames is target threat assessment, and the evaluation result directly affects the effectiveness of wargame AI [19]. The aim of this section is to introduce threat quantification methods for different types of indicators. By combining the target type, this section divides the target into target distance threat, target attack threat, target speed threat, terrain visibility threat, environmental indicator threat, and target defense value. The acquired confrontation data are incorporated into different indicator types, and then the corresponding comprehensive threat value is calculated.In the table 1 are the attributes and meanings of specific indicators. Table 1 A list of indicator attributes and their meanings Indicator Attribute Meaning Target distance threat Cost type Distance between the two parties will influence the kill probability Target attack threat Benefit type Threat degrees should be determined by the opponent's type, range, and lethality of the weapon Target speed threat Benefit type The threat of speed from our opponents Terrain visibility threat Intervisibility > no intervisibility Whether or not the terrain is visible will directly impact the threat Environmental indicator threat Benefit type While the opponent's environment is conducive to concealment, mobility is more dangerous. Target defense value Cost type The stronger the opponent's armor, the harder it is to destroy it 3.1 Quantification of threat distance indicators Target distance is an important parameter to evaluate the threat degree of the target [20]. In the wargame environment, the distance between entities can be calculated using the ranged tool. To simplify the process, real numbers are used to determine the distance between entities.The traditional threat quantification method uses only the target distance, whereas this article considers the distance from the control point in the wargame environment. The player who reaches the control point first wins, regardless of who reaches it first. This consideration will also be a factor in the target distance indicator. For a red tank i and a blue tank j in wargame, calculate the target distance j threat to the red tank i.The coordinates of the control point are ( , ) O x y , and common \uf074 is the strength of the tank piece needed to pass over a grid of ordinary terrain, ( , ) x y \uf074 is the stamina consumed by the vehicle through special terrain, D( , ) J O is the distance from tank max D to the control point, E is the maximum distance from the imaginary boundary, and ij D is the grid distance between tanks i and j. Based on a comprehensive analysis of the terrain, it can be seen that for the red tank i, the closer the blue tank j is to the control point and the closer it is to the red tank, the greater the threat of the distance indicator. Using the following formula, we can calculate the target distance threat index of red tank i and blue tank j. The threat quantification of the target distance between the blue tank j and the red tank i is designated as ( , ) i x y \uf079 , the control threat value is designated as ( , ) j x y \uf06a , and the comprehensive target distance quantitative value of the blue tank j as compared to the red tank i is designated as ( , ) ij x y \uf066 , where i and j are the number of tanks on the red side and blue side respectively. common max D( , ) ( , ) 1 ( , ) j J O x y x y D \uf074 \uf06a \uf074 \uf0e6 \uf0f6 \uf03d \uf02d \uf0e7 \uf0f7 \uf0e8 \uf0f8 max ( , ) i ij x y D D \uf079 \uf03d \uf02d \uf028 \uf029 1 ( , ) + 2 ij j i x y \uf066 \uf06a \uf079 \uf03d 3.2 Quantification of threat indicators for target speeds Threat levels of the target speed are a function of target motion state. The faster the target moves, the faster its position and environment change, and the harder it is for us to hit it, so the greater the threat level [21]. Therefore, the target speed threat degree can be calculated based on the benefit index, that is, the greater the target speed, the greater the threat. As a reference for the target combat intention estimation, the speed direction information can be used. Here, only the scalar of the target speed is considered. As for different types of targets, their speed determines their quantified value of threat. For example, composite armored tanks, heavy tanks, and light tanks all have different speeds. For the composite armor target, Vcomposite-max, the heavy tank target Vheavy-max, the light tank target Vlight-max, and the relative speed between the opponent's tank target Tj, and our evaluation node Wi, Vij, quantitatively according to the target type, and the speed threat degree Tvij as 1 2 3 ,  C     ,  H   ,   ij composite max ij vij heavy max ij light max v omposite armored tanks V v T eavy tanks V v ight tanks V L \uf062 \uf062 \uf062 \uf02d \uf02d \uf02d \uf0ec \uf0ef \uf0ef \uf0ef\uf0ef \uf03d \uf0ed \uf0ef \uf0ef \uf0ef \uf0ef\uf0ee As part of this formula, 1 2 3 , , [0,1] \uf062 \uf062 \uf062 \uf0ce is the threat factor for composite armored tanks, heavy tanks, and light tanks, which represents the speed threat characteristics for different types of targets, respectively. The speed threats of general composite armored tanks and light tanks are greater than those of heavy tanks. The value of 1 2 3 , , \uf062 \uf062 \uf062 can be calculated in advance by professionals according to the game characteristics of a particular target, in order to effectively characterize the target's speed characteristics. 3.3 Quantification of threat indicators for target attacks In calculating the attack capability of a target tank, the attack capability threat function is mainly considered: \uf028 \uf029 \uf028 \uf029 1 2 1 2 3 4 ln ln 1 ln C B A A \uf065 \uf065 \uf065 \uf065 \uf0e9 \uf0f9 \uf03d \uf02b \uf02b \uf02b \uf0eb \uf0fb \uf0e5 \uf0e5 Assuming B is the maneuverability of the tank pawn; A1 is the weapon attack capability of the tank pawn; A2 is the detection capability of the tank pawn; By calculating the threat value of the target tank chess piece based on its attack capability threat function, 1\uf065 is the forward firing capability, 2 \uf065 is the bomb-carrying capability, 3\uf065 is the electronic countermeasure capability, and 4 \uf065 is the missile offensive capability of the tank pawn. 3.4 Quantification of terrain visibility threats In the assessment of threats, whether the targets can see each other is a significant factor. In particular, tanks, which are direct-pointing weapons, are important. To aim and track a target, one must see the target. A simplified view of the blue tank targets Tj and Wi is shown in the figure 2. (a) (b) Figure 2 The visibility of the red and blue tanks.(a) Both blue and red have the same terrain elevation, and both are visible. (b) Because the terrain between the red and blue is high and the elevation on both sides is low, the red and blue will not be visible Determine whether both parties' location information can be connected through the visibility interface of the wargame environment. A visibility interface uses the elevation between the direct connections between the two parties as a reference standard. If the direct elevation between the two parties is higher than the location of the two parties, inter-view is not possible; otherwise, it is possible. Based on the visibility interface and the locations of both parties, evaluate the terrain visibility threat fij of the target blue side Tj and red side Wi. \uf05b \uf05d \uf05b \uf05d \uf05b \uf05d \uf05b \uf05d 2 0, 0, 0, 0, 1 0, 0, 0, 1 , max ( ) 0 max ( ) 0 0, , max ( ) 0 max ( ) 0 0, , max ( ) 0 max ( ) 0 1,1 , max ( ) 0 , 0 ij ij ij ij ij ij ij ij j ij l L l L ij j ij l L l L ij ij j ij l L i i i l L ij j l L t H l H H l H H l H H l H f t H l H H l H H l H \uf0e9 \uf0f9 \uf0e9 \uf0f9 \uf0ce \uf0ce \uf0eb \uf0fb \uf0eb \uf0fb \uf0e9 \uf0f9 \uf0e9 \uf0f9 \uf0ce \uf0ce \uf0eb \uf0fb \uf0eb \uf0fb \uf0e9 \uf0f9 \uf0e9 \uf0f9 \uf0ce \uf0ce \uf0eb \uf0fb \uf0eb \uf0fb \uf0e9 \uf0f9 \uf0ce\uf0eb \uf0fb \uf02d \uf0a3 \uf02d \uf0a3 \uf02d \uf0b3 \uf02d \uf0b3 \uf03d \uf02d \uf0b3 \uf02d \uf0a3 \uf02d \uf0a3 \u4e14 \u4e14 \u4e14 \uf05b \uf05d 0, 1 2 max ( ) 0 , , ij i ij l L H l H t t other \uf0e9 \uf0f9 \uf0ce\uf0eb \uf0fb \uf0ec \uf0ef \uf0ef \uf0ef\uf0ef\uf0ed \uf0ef \uf02d \uf0b3 \uf0ef \uf0ef \uf0ef\uf0ee \u4e14 1 2 , (0,1) t t \uf0ce is a quantitative parameter, 1 2 t t \uf03c is expressed by the number of intervals, Lij is the linear distance between the blue tank target Tj and the red tank Wi, and l is the horizontal distance from the actual point to the target Tj. Hij (l) is the distance between red and blue targets.Tj represents the actual terrain height of l, Hi and Hj represent the actual terrain height of the positions of tank Wi and tank Tj. If the evaluation node and target are both fully visible and are above the mid-elevation, the terrain visibility threat degree is [t2,1]; when neither can communicate, the threat degree is [0,0];if the red-sided Wi is situated at a high point and higher than the middle elevation, and the blue-sided Tj is situated at a low point and lower than the middle elevation, then the threat degree is [0,t1];whenever the blue side of Tj is higher than the middle elevation, and the red side of Wi is lower than the middle elevation, the threat degree is [1,1]; As the shooting can be completed without a cross-view during indirect shooting, the degree of threat is uniformly set to [t1,t2]. By combining the target's position and elevation with the wargame environment, it is possible to evaluate the visibility of the target to the red tank Wi by combining the visibility of target with environment. 3.5 Quantification of environmental indicators The effects of the wargame confrontation environment are an important determinant of combat effectiveness. The better the environmental indicators, the better the equipment effectiveness of the chess pieces, and the worse the environmental indicators, the worse the combat effectiveness of the chess pieces[22-23]. Due to the fact that the simulation is carried out in a wargame environment, this article extracts the two major influencing factors for the wargame environment, urban residential areas and highways, for the quantification of environmental indicators.Tanks and infantry personnel can be concealed in urban residential areas so that the enemy cannot detect our targets, which is conducive to our defense. Highways can speed up the chess pieces' moving speed, and there are first-level and second-level highways in the wargame environment. The higher the level of the road, the better the chess pieces' speed.Therefore, environmental indicators can be quantified according to the environment where chess pieces are located, considering surrounding urban residential areas and road conditions. Determine whether there are first-class roads, second-class roads and residential areas in the two squares around the red tank Wi. Gaining threat degree Tei through judgment. 1 1 2 2 3 i Te w h w h w r \uf03d \uf02b \uf02b h1, h2, and r represent first-class roads, second-class roads, and urban residential areas, respectively, and w1, w2, and w3 represent weight vectors. If the above terrain environment is located around the chess piece, the associated value is assigned. 3.6 Quantification of target defense Generally, the defense quantification of chess pieces includes the armor protection attributes. The different types of armor include composite armor, heavy armor, medium armor, light armor, and unarmored. This article assigns different armors different defense values, based on their armor protection capabilities.The table 2 below shows. Table 2 Quantified value of target defense Armored attributes Quantified value composite armor 1 heavy armor 0.7 medium armor 0.5 light armor 0.3 unarmored 0 4 \u3001 Establishment of a multi-attribute quantitative threat model based on intuitionistic fuzzy numbers By using the interval number method, this article indicates whether visibility is possible, and different threats are generated. Nevertheless, the quantified values of other threat targets are real numbers. To unify the problem solving method, this paper converts all interval numbers and real numbers to intuitionistic fuzzy numbers, and calculates the size of the threat by calculating the intuitionistic fuzzy numbers. 4.1 Principles of conversion between different description forms The quantitative multi-attribute index set represented by different forms is X, and the quantitative index is x, x X \uf0ce ; the intuitionistic fuzzy set is Y, and the intuitionistic fuzzy number of the index is y, y Y \uf0ce . When converting indicators of different representations, considering the following principles should be followed in order to ensure that the form of the conversion is scientific and reasonable: 1) Range limitations In mapping x y \uf0ae , if x X \uf0ce is present, then y Y \uf0ce . 2\uff09Characteristics of boundary If x is the upper bound of its representation, then 1,0 y \uf03d \uf0e1 \uf0f1 . And if x is the lower bound of its representation, then 0,1 y \uf03d \uf0e1 \uf0f1 . 3\uff09Monotonic map When 1 2 x x \uf03c , then 1 2 y y \uf03c ; if 1 2 x x \uf03d , then 1 2 y y \uf03d , if 1 2 x x \uf03e , then 1 2 y y \uf03e . 4.2 The interval number is converted into an intuitionistic fuzzy number During the wargame confrontation, it is difficult to draw accurate conclusions about the level of visibility. This article, therefore, uses the interval number method to express this type of difficult-to-determine information. To uniformly express that all the numbers in this article are transformed into intuitionistic fuzzy numbers for representation, here is a conversion method between interval numbers and intuitionistic fuzzy numbers. The interval number , L U i i i a a a \uf0e9 \uf0f9 \uf03d \uf0eb \uf0fb \uf025 is transformed into intuitionistic fuzzy number , i i i f \uf06d \uf06e \uf03d using the following formula. \uf07b \uf07d \uf07b \uf07d 1,2, , 1,2, , max 1 max i i U i n i U i i U i n i a L a a a \uf06d \uf06e \uf03d \uf0bc \uf03d \uf0ec \uf03d \uf0ef \uf0ef\uf0ed \uf0ef \uf03d \uf02d \uf0ef\uf0ee \uf04c Proof: 1) Range restrictions If , L U i i i a a a R \uf0e9 \uf0f9 \uf03d \uf0ce \uf0eb \uf0fb \uf025 \uff0c then \uf07b \uf07d 1,2,, max L i i U i n i a a \uf06d \uf03d \uf03d \uff0c \uf07b \uf07d 1,2, , [0,1] max L i i U i n i a a \uf06d \uf03d \uf03d \uf0ce \uf04c \uff0c \uf07b \uf07d 1,2, , 1 [0,1] max U i i U i n i a a \uf06e \uf03d \uf03d \uf02d \uf0ce \uf04c so \uf07b \uf07d 0 1 1 max U L i i i i U i i a a a \uf06d \uf06e \uf02d \uf0a3 \uf02b \uf03d \uf02d \uf0a3 2) Boundary characteristics ia\uf025 takes the lower bound [0,0] , and \uf07b \uf07d 1,2, , max 0 U i n ia \uf03d \uf0b9 \uf04c . Apparently \uf07b \uf07d 1,2, , 0 max L i i U i n i a a \uf06d \uf03d \uf03d \uf03d \uf04c follows. 3) Mapping monotonic Set 1 1 1 , L U a a a \uf0e9 \uf0f9 \uf03d \uf0eb \uf0fb \uf025 and 2 2 2 , L U a a a \uf0e9 \uf0f9 \uf03d \uf0eb \uf0fb \uf025 intervals. The degree of uncertainty between these numbers is usually expressed as a probability relation. In order to judge the monotonicity of the mapping, it is important to use the determined relationship (possibility is 1).So, when determining the size relationship, this article is mainly concerned with three situations: 1a\uf025 is smaller than \uf028 \uf029 2 1 2 a a a \uf03c \uf025 \uf025 \uf025 by the full amount\uff0c 1a\uf025 equal \uf028 \uf029 2 1 2 a a a \uf03d \uf025 \uf025 \uf025 and 1a\uf025 is completely greater than \uf028 \uf029 2 1 2 a a a \uf03e \uf025 \uf025 \uf025 A\uff09If 1 2 a a \uf03c \uf025 \uf025 and 1 1 2 2 L U L U a a a a \uf0a3 \uf03c \uf0a3 , then \uf07b \uf07d \uf07b \uf07d 1 2 1 2 1,2, , 1,2, , 0 max max U U i n i i n i a L a L a a \uf06d \uf06d \uf03d \uf03d \uf02d \uf03d \uf02d \uf03c \uf04c \uf04c \uf07b \uf07d \uf07b \uf07d \uf07b \uf07d 1 2 2 1 1 2 1,2, , 1,2, , 1,2, , 1 1 0 max max max U U U U U U U i n i i n i i n i a a a a a a a \uf06e \uf06e \uf03d \uf03d \uf03d \uf0e6 \uf0f6 \uf02d \uf0e7 \uf0f7 \uf02d \uf03d \uf02d \uf02d \uf02d \uf03d \uf03e \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf04c \uf04c \uf04c So 1 2 f f \uf03c B\uff09If 1 2 a a \uf03d \uf025 \uf025 , obviously 1 2 \uf06d \uf06d \uf03d , 1 2 \uf06e \uf06e \uf03d , so 1 2 f f \uf03d C\uff09If 1 2 a a \uf03e \uf025 \uf025 \uff0cand 2 2 1 1 L U L U a a a a \uf0a3 \uf03c \uf0a3 \u3002Then \uf07b \uf07d \uf07b \uf07d 1 2 1 2 1,2, , 1,2, , 0 max max L U U i n i i n i a L a a a \uf06d \uf06d \uf03d \uf03d \uf02d \uf03d \uf02d \uf03e \uf04c \uf04c \uf07b \uf07d \uf07b \uf07d \uf07b \uf07d 1 2 2 1 1 2 1,2, , 1,2, , 1,2, , 1 1 0 max max max U U U U U U i n i i n i i n i aU a a a a a a \uf06e \uf06e \uf03d \uf03d \uf03d \uf0e6 \uf0f6 \uf02d \uf0e7 \uf0f7 \uf02d \uf03d \uf02d \uf02d \uf02d \uf03d \uf03c \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf04c \uf04c \uf04c So 1 2 f f \uf03e \u3002 4.3 Convert real numbers into intuitionistic fuzzy numbers To unify the index representation form, the real number type index representation must also be converted into intuitionistic fuzzy numbers. The following is the formula for calculating the membership degree and non-membership degree of the conversion of real numbers into IFN. Beneficial index \uf07b \uf07d \uf07b \uf07d 1,2, , max 1 max 1,2, , i i i n i i i i a a a a i n \uf06d \uf062 \uf06e \uf062 \uf03d \uf0ec \uf03d \uf0ef\uf0ef\uf0ed \uf0f6 \uf0e6 \uf0ef \uf03d \uf02d \uf0f7 \uf0e7 \uf0ef \uf03d \uf0e8 \uf0f8 \uf0ee \uf04c \uf04c The cost index \uf07b \uf07d \uf07b \uf07d 1,2, , min min 1,2, , 1 i n i i i i i i a a i n a a \uf06d \uf062 \uf06e \uf062 \uf03d \uf0ec \uf03d \uf0ef \uf0ef\uf0ed \uf03d \uf0e6 \uf0f6 \uf0ef \uf03d \uf02d \uf0e7 \uf0f7 \uf0ef \uf0e8 \uf0f8 \uf0ee \uf04c \uf04c In the formula, real numbers are considered special cases of interval numbers. The function of factor [0,1] \uf062 \uf0ce is to prevent the deterministic characteristics of real numbers from affecting other uncertainty indicators. The proof: can treat real numbers as special cases of interval numbers, so it can prove that real numbers are converted into intuitionistic fuzzy numbers via the same method used to prove interval numbers are converted into direct fuzzy numbers. 4.4 Quantitative calculation of fuzzy numbers intuitively based on multiple attributes \uff081\uff09This intuitionistic fuzzy entropy describes the degree of fuzzy judgment information provided by an intuitionistic fuzzy set. The larger the intuitionistic fuzzy entropy of an evaluation criterion, the smaller the weight should be given; otherwise, the larger needs to be. Based on formulas from the literature[24], we calculated the entropy weights for each intuitionistic fuzzy. Among them, ideal solution iS \uf02b is a conceived optimal solution (scheme), and its attribute values hit the best value among the alternatives; and the negative ideal solution iS \uf02d is the worst conceived solution (scheme), and its attribute values hit the worst value among the alternatives. ip is generated by comparing each alternative scheme with the ideal solution and negative ideal solution. If one of the solutions is closest to the ideal solution, but at the same time far from the negative ideal solution, then it is the best solution among the alternatives. \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 1 ln ln ln 1 ln 2 ln 2 m j ij ij ij ij ij ij ij ij ij ij i H v v v v v n \uf06d \uf06d \uf06d \uf06d \uf06d \uf03d \uf0f9 \uf0e9 \uf03d \uf02d \uf02b \uf02d \uf02b \uf02b \uf02d \uf02d \uf02d \uf0eb \uf0fb \uf0e5 If 0 ij \uf06d \uf03d \uff0c 0 ijv \uf03d \uff0c 0 ij ijv \uf06d \uf02b \uf03d \uff0cthen ln 0 ij ij \uf06d \uf06d \uf03d ln 0 ij ij v v \uf03d ,\uf028 \uf029 \uf028 \uf029 ln 0 ij ij ij ij v v \uf06d \uf06d \uf02b \uf02b \uf03d The entropy weight of the j attribute is defined as: 1 1 j j n j j H w n H \uf03d \uf02d \uf03d \uf02d\uf0e5 Among 1 0, 1,2, , , 1 n j j j w j n w \uf03d \uf0b3 \uf03d \uf03d \uf0e5 \uf04c \uff08 2 \uff09 Determine the optimal solution A+ and the worst solution A- using the following formula: \uf07b \uf07d \uf07b \uf07d 1 1 2 2 1 1 2 2 , , , , , , , , , , , , n n n n A A \uf06d \uf06e \uf06d \uf06e \uf06d \uf06e \uf06d \uf06e \uf06d \uf06e \uf06d \uf06e \uf02b \uf02b \uf02b \uf02b \uf02b \uf02b \uf02b \uf02d \uf02d \uf02d \uf02d \uf02d \uf02d \uf02d \uf0ec \uf03d \uf0ef\uf0ed \uf03d \uf0ef\uf0ee \uf04c \uf04c In which \uf07b \uf07d \uf07b \uf07d 12 1,2, , max , min i j m ij i j m ij \uf06d \uf06d \uf06e \uf06e \uf02b \uf02b \uf03d \uf0bc\uf0bc \uf03d \uf0bc \uf03d \uf03d \uf07b \uf07d \uf07b \uf07d 1,2, , 1,2, , min , max i j m ij i j m ij \uf06d \uf06d \uf06e \uf06e \uf02d \uf02d \uf03d \uf03d \uf03d \uf03d \uf04c \uf04c \uff083\uff09Calculate the similarity between the fuzzy intuitionistic A and B as follows: \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 2 1 2 1 2 1 2 1 2 1 2 1 1 2 2 2 2 , , , 1 1 3 2 3 2 s \uf06d \uf06d \uf06e \uf06e \uf06e \uf06e \uf06d \uf06d \uf070 \uf070 \uf070 \uf070 \uf06d \uf06e \uf06d \uf06e \uf02d \uf02d \uf02d \uf02d \uf02d \uf02d \uf02b \uf02b \uf0e6 \uf0f6 \uf0e6 \uf0f6 \uf03d \uf02d \uf0b4 \uf02d \uf02d \uf0b4 \uf0e7 \uf0f7 \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0e8 \uf0f8 In which\uff0c 1 1 1 2 2 2 1 , 1 \uf070 \uf06d \uf06e \uf070 \uf06d \uf06e \uf03d \uf02d \uf02d \uf03d \uf02d \uf02d \uff084\uff09Calculate the similarity iS \uf02b and iS \uf02d between each solution and the optimal solution and the worst solution based on the following formula: \uf028 \uf029 \uf028 \uf029 1 1 , , , , , , n i k k k ik ik k n i k k k ik ik k S w s S w s \uf06d \uf06e \uf06d \uf06e \uf06d \uf06e \uf06d \uf06e \uf02b \uf02b \uf02b \uf03d \uf02d \uf02d \uf02d \uf03d \uf0ec \uf03d \uf0d7 \uf0ef\uf0ef\uf0ed \uf0ef \uf03d \uf0d7 \uf0ef\uf0ee \uf0e5 \uf0e5 (5) Then calculate the relative closeness \uf028 \uf029 / i i i i p S S S \uf02d \uf02b \uf02d \uf03d \uf02b Comparing threat levels of opponents based on their closeness to the target depends on the level of threat assessment performed. 5\u3001Multi-attribute threat quantitative simulation experiment using intuitionistic fuzzy numbers The threat assessment problem is transformed into a multi-attribute decision-making problem, while the combat intention of the target is incorporated into the evaluation system to make the evaluation system more reasonable and the results more reliable. A simulation scene depicts ten tanks of the red and blue sides fighting each other, and ten opposite are found as chess pieces in the wargame. By using the war game deduction environment, each tank piece's attribute data will be derived in real time, including the opposite piece's position (hexagon number), elevation, type, distance, range, strike power and control point distance, as well as armor thickness. A table 3 is shown below. Table 3 Data about each piece in wargaming Time Number of target Posting Elevation Type Distance Range Taking Fire Control point's distance Thickness of armor t1 1 (15, 40) 110 Tank 34 16 5 18 Unarmored 2 (16, 41) 110 Tank 34 20 2 19 Unarmored 3 (17, 41) 110 Tank 34 17 3 20 Light armor 4 (18, 40) 110 Tank 33 17 5 19 Medium armor 5 (18, 41) 110 Tank 34 20 3 20 Composite armor 6 (17, 40) 110 Tank 33 20 4 19 Unarmored 7 (16, 40) 110 Tank 33 20 5 18 Unarmored 8 (15, 41) 110 Tank 35 16 4 19 Light armor 9 (18, 42) 110 Tank 35 16 2 21 Composite armor 10 (17, 42) 110 Tank 35 17 3 21 Heavy armor t2 1 (15, 39) 110 Tank 32 16 5 17 Unarmored 2 (16, 40) 110 Tank 32 20 2 18 Unarmored 3 (17, 41) 110 Tank 33 17 3 20 Light armor 4 (18, 40) 110 Tank 32 17 5 19 Medium armor 5 (18, 40) 110 Tank 32 20 3 19 Composite armor 6 (17, 40) 110 Tank 32 20 4 19 Unarmored 7 (16, 39) 110 Tank 31 20 5 17 Unarmored 8 (15, 41) 110 Tank 34 16 4 19 Light armor 9 (18, 41) 110 Tank 33 16 2 20 Composite armor 10 (17, 41) 110 Tank 33 17 3 20 Heavy armor t3 1 (15, 39) 110 Tank 31 16 5 17 Unarmored 2 (16, 40) 110 Tank 31 20 2 18 Unarmored 3 (17, 41) 110 Tank 32 17 3 20 Light armor 4 (18, 40) 110 Tank 31 17 5 19 Medium armor 5 (18, 40) 110 Tank 31 20 3 19 Composite armor 6 (17, 40) 110 Tank 31 20 4 19 Unarmored 7 (16, 39) 110 Tank 30 20 5 17 Unarmored 8 (15, 41) 110 Tank 33 16 4 19 Light armor 9 (18, 40) 110 Tank 31 16 2 19 Composite armor 10 (17, 41) 110 Tank 32 17 3 20 Heavy armor t4 1 (15, 39) 110 Tank 31 16 5 17 Unarmored 2 (16, 39) 110 Tank 30 20 2 17 Unarmored 3 (17, 41) 110 Tank 33 17 3 20 Light armor 4 (18, 39) 110 Tank 31 17 5 18 Medium armor 5 (18, 40) 110 Tank 32 20 3 19 Composite armor 6 (17, 39) 110 Tank 31 20 4 18 Unarmored 7 (16, 39) 110 Tank 30 20 5 17 Unarmored 8 (15, 41) 110 Tank 33 16 4 19 Light armor 9 (18, 40) 110 Tank 32 16 2 19 Composite armor 10 (17, 41) 110 Tank 33 17 3 20 Heavy armor t5 1 (15, 39) 110 Tank 31 16 5 17 Unarmored 2 (16, 38) 110 Tank 30 20 2 16 Unarmored 3 (17, 40) 110 Tank 33 17 3 19 Light armor 4 (18, 38) 110 Tank 31 17 5 17 Medium armor 5 (18, 40) 110 Tank 33 20 3 19 Composite armor 6 (17, 39) 110 Tank 32 20 4 18 Unarmored 7 (16, 39) 110 Tank 31 20 5 17 Unarmored 8 (15, 41) 110 Tank 33 16 4 19 Light armor 9 (18, 40) 110 Tank 33 16 2 19 Composite armor 10 (17, 41) 110 Tank 34 17 3 20 Heavy armor By integrating real numbers and interval numbers to form intuitionistic fuzzy number formulas, the target distance is quantified according to the distance threat quantification method in Section 3.1, and the distance threat degree is then turned into a fuzzy number using the conversion method of real numbers and fuzzy numbers in Section 4.The calculated result for the target distance threat degree is provided in the table 4. Consider the threat measurement at T1 as an example of a decision matrix. Table 4 Calculation of target distance threat Target Value of normal terrain stamina consumption Value of special terrain energy consumption Distance between the blue tank and the control point Consider the maximum distance between two borders Distance between tanks i and j Representati on of real numbers Representation of fuzzy numbers intuitively 1 3 6 18 50 34 8.16 [0.187378998, 0.012621002] 2 3 6 19 50 34 8.155 [0.18749387, 0.01250613] 3 3 6 20 50 34 8.15 [0.187608882, 0.012391118] 4 3 6 19 50 33 8.655 [0.176663586, 0.023336414] 5 3 6 20 50 34 8.15 [0.187608882, 0.012391118] 6 3 6 19 50 33 8.655 [0.176663586, 0.023336414] 7 3 6 18 50 33 8.66 [0.176561598, 0.023438402] 8 3 6 19 50 35 7.655 [0.199738767, 0.000261233] 9 3 6 21 50 35 7.645 [0.2, 0.0] 10 3 6 21 50 35 7.645 [0.2, 0.0] Based on the real number conversion intuitionistic fuzzy number formula, the target speed is quantified according to the 3.2 section speed threat quantification method, and the speed threat degree is converted into an intuitionistic fuzzy number according to the 4th section of the conversion method of real numbers and intuitionistic fuzzy numbers. Calculation of target velocity threat degree. Create a matrix showing the target velocity threat measurement at T1, for example, as shown in the table 5. Table 5 Calculation of target speed threat Targ et The target speed The relative speed between the opposite and the us The real number representation Intuitive fuzzy number representation 1 125 325 2.6 [0.153863899, 0.046136101] 2 150 350 2.333333333 [0.171440811, 0.028559189] 3 200 400 2 [0.2, 0.0] 4 200 400 2 [0.2, 0.0] 5 200 400 2 [0.2, 0.0] 6 200 400 2 [0.2, 0.0] 7 200 400 2 [0.2, 0.0] 8 150 350 2.333333333 [0.171440811, 0.028559189] 9 175 375 2.142857143 [0.186672886, 0.013327114] 10 150 350 2.333333333 [0.171440811, 0.028559189] The real number conversion intuitionistic fuzzy number formula is applied to determine the target's attack ability using the attack threat quantification method in Section 3.3, and then the attack threat degree is converted into an intuitionistic fuzzy number using the conversion method of real numbers and intuitionistic fuzzy numbers in Section 4. The calculated solution result of the target attack threat. Determine the decision matrix using the target attack threat measurement at T1 as an example, as shown in the table 6. Table 6 Calculation of target attack threat T ar ge t Mano euvra bility Weapon system attack capability Reconnaissan ce capability Capability of indirect shots The ammunition carrying capacity ECM capab ility Offensive missile capability The real number representati on Intuitive fuzzy number representation 1 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 2 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 3 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 4 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 5 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 6 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 7 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 8 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 9 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] 10 6 {'type1': 1, 'type2': 0.5, 'type3': 1.5} {'type1': 1, 'type2': 0.5, 'type3': 1.5} 1 3 1 1 10.8730228 [0.2, 0.0] By combining the interval number conversion intuitionistic fuzzy number formula with the terrain visibility threat quantification method in section 3.4, the target visibility is quantified, and then the terrain visibility threat degree is converted into intuitionistic fuzzy numbers. Solution result of the visibility threat degree of the target terrain. Create a decision matrix using the metric of the visibility threat of T1 as an example, as shown in the table 7. Table 7 Calculated threat level for terrain visibility Tar get The red square elevation The blue square elevation The highest altitude between red and blue The interval number representation Intuitive fuzzy number representation 1 130 110 150 [0, 0.2] [0.0, 0.0] 2 130 110 150 [0, 0.2] [0.0, 0.0] 3 130 110 150 [0, 0.2] [0.0, 0.0] 4 130 110 150 [0, 0.2] [0.0, 0.0] 5 130 110 150 [0, 0.2] [0.0, 0.0] 6 130 110 150 [0, 0.2] [0.0, 0.0] 7 130 110 150 [0, 0.2] [0.0, 0.0] 8 130 110 150 [0, 0.2] [0.0, 0.0] 9 130 110 150 [0, 0.2] [0.0, 0.0] 10 130 110 150 [0, 0.2] [0.0, 0.0] As described in section 3.5, environmental indicators are quantified and converted into intuitionistic fuzzy numbers using the real number conversion intuitionistic fuzzy number formula, then converted into real numbers using the conversion method of real numbers and intuitionistic fuzzy numbers in section 4. The calculation result of the target environment index. Construct a decision matrix using the environmental indicator threat measurement at T1 as an example, as shown in the table 8. Table 8 Calculation of the target environment index Tar get The first class road (weight W1) The secondary road (weight W2) The urban residential area (weight W3) The real number representation Intuitive fuzzy number representation 1 0(3) 0(2) 0(1) 0 [0.2, 0.0] 2 0(3) 0(2) 0(1) 0 [0.2, 0.0] 3 0(3) 0(2) 0(1) 0 [0.2, 0.0] 4 0(3) 0(2) 0(1) 0 [0.2, 0.0] 5 0(3) 0(2) 0(1) 0 [0.2, 0.0] 6 0(3) 0(2) 0(1) 0 [0.2, 0.0] 7 0(3) 0(2) 0(1) 0 [0.2, 0.0] 8 0(3) 0(2) 0(1) 0 [0.2, 0.0] 9 1(3) 0(2) 0(1) 3 [6.6644e-05, 0.199933356] 10 0(3) 0(2) 0(1) 0 [0.2, 0.0] Using the real number conversion intuitionistic fuzzy number formula, the target defense is quantified following the target defense quantification method in Section 3.6, and then the target defense quantified value is converted into intuitionistic fuzzy numbers following the conversion method of real numbers and intuitionistic fuzzy numbers in Section 4. Calculate the quantitative results of target defense in the table. Build a decision matrix by using the quantification of defense indicators at T1 as an example, as shown in the table 9. Table 9 Calculation of the target defense quantitatively Target Armored type The real number representation Intuitive fuzzy number representation 1 Unarmored 0 [0.2, 0.0] 2 Unarmored 0 [0.2, 0.0] 3 Light armor 0.3 [0.000664452, 0.199335548] 4 Medium armor 0.5 [0.000399202, 0.199600798] 5 Composite armor 1 [0.0001998, 0.1998002] 6 Unarmored 0 [0.2, 0.0] 7 Unarmored 0 [0.2, 0.0] 8 Light armor 0.3 [0.000664452, 0.199335548] 9 Composite armor 1 [0.0001998, 0.1998002] 10 Heavy armor 0.7 [0.000285307, 0.199714693] A unified intuitiveistic fuzzy number representation has been created for all multi-attribute indicators. An example of an intuitionistic fuzzy number representation of threat assessment indicators is illustrated in the following table 10. Table 10 Information decision table for threat target parameters (intuitionistic fuzzy number) Tank1 Tank2 Tank3 Tank4 Tank5 Tank6 Tank7 Tank8 Tank9 Tank10 Quantification of target distance threats [0.1873 78998, 0.01262 1002] [0.1874 9387, 0.01250 613] [0.1876 08882, 0.01239 1118] [0.1766 63586, 0.02333 6414] [0.1876 08882, 0.01239 1118] [0.1766 63586, 0.02333 6414] [0.1765 61598, 0.02343 8402] [0.1997 38767, 0.00026 1233] [0.2, 0.0] [0.2, 0.0] Quantification of target speed threats [0.1538 63899, 0.04613 6101] [0.1714 40811, 0.02855 9189] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.1714 40811, 0.02855 9189] [0.1866 72886, 0.01332 7114] [0.1714 40811, 0.02855 9189] Quantifying the threat from target attacks [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] Quantifying the threat posed by terrain visibility [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] [0.0, 0.0] Quantification of environmental indicators of threat [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [0.2, 0.0] [6.6644 e-05, 0.19993 3356] [0.2, 0.0] Quantification of target defense [0.2, 0.0] [0.2, 0.0] [0.0006 64452, 0.19933 5548] [0.0003 99202, 0.19960 0798] [0.0001 998, 0.19980 02] [0.2, 0.0] [0.2, 0.0] [0.0006 64452, 0.19933 5548] [0.0001 998, 0.19980 02] [0.0002 85307, 0.19971 4693] By obtaining data represented by the intuitionistic vagueness of the threat assessment indicators shown in the table, formulae in 4.4 may be used to obtain the intuitionistic vague target threat assessment based on multiple attribute decision-making approaches. As shown in the table 11, determine the target threat assessment results. Table 11 Threat assessment for target iS\uf02b [0.9900131572106283, 0.9930194457658972, 0.9713249517102417, 0.9694274902547305, 0.9712630240082707, 0.9960298049584839, 0.9960124538670997, 0.9685356920167532, 0.9447732710194203, 0.9685296037271114] iS\uf02d [0.9451975215527424, 0.9421912329974735, 0.963885727053129, 0.9657831885086402, 0.9639476547551001, 0.9391808738048868, 0.9391982248962711, 0.9666749867466174, 0.9904374077439504, 0.9666810750362593] iP [0.5115790069137391, 0.5131324752716081, 0.5019220710020746, 0.5009415775207532, 0.5018900705058751, 0.5146880470889931, 0.5146790810929003, 0.5004807500523212, 0.4882017660336315, 0.500477603991942] Ra nk in g T6>T7>T2>T1>T3>T5>T4>T8>T10>T9 In the table 12, the opposite target at T1 is shown as a threat. Table 12 Ranking of opposite targets at time T Type of piece Indicator comprehensive Ranking Tank 1 0.511579007 4 Tank 2 0.513132475 3 Tank 3 0.501922071 5 Tank 4 0.500941578 7 Tank 5 0.501890071 6 Tank 6 0.514688047 1 Tank 7 0.514679081 2 Tank 8 0.50048075 8 Tank 9 0.488201766 10 Tank 10 0.500477604 9 Figure 3 The threat value on the ordinate, and the threat of the opponent's ten tanks at time T represented by ten colors on the abscissa. Based on the evaluation results, it can be concluded that the blue T6 tank is the most harmful and the T7 tank is the second most harmful, this is shown in figure 3. This article does not limit itself to subjective analysis of experts, but also introduces reinforcement learning, associates the reinforcement learning intelligent algorithm through the reward function and analyzes the scientific nature of the method through an improvement of the actual intelligent AI algorithm's winning rate. 6\u3001Reinforcement learning and multi-attribute threat fusion model 6.1 Reinforcement learning algorithm and multi-attribute model formulation Prior work in this article has described the quantied value of multi-attribute decision-making threat based on the entropy weight method. The subsequent chapters deal with how to associate this value with reinforcement learning. This part of the work is also the goal of this article. Its essence is to establish a multi-attribute decision-making mechanism that is based on reinforcement learning, and then select the entity with the highest threat to establish the return value and threat degree. The higher the threat degree, the greater the return value, this is shown in figure 4. Figure 4 A fusion model of reinforcement learning and multi-attribute threat estimation based on AC framework.The module mainly consists of a reinforcement learning pre-training experience storage module that integrates multi-attribute decision-making, Critic evaluation network update module, and a new and old strategy network module A reinforcement learning algorithm is built using the AC framework to achieve intelligent decision-making. It includes a reinforcement learning pre-training module that integrates multi-attribute decision-making, Critic evaluation network update module and a new and old strategy network update module.In the intensive learning pre-training experience storage module, multi-attribute decision making mainly uses state data obtained from the wargame environment, such as elevation, distance, armor thickness, etc., to make multi-attribute decisions. By normalizing the data, calculating the threat of each piece of the opponent by using the entropy method, and then setting the reward function and storing it in the experience store, further actions in the environment will be taken to obtain the next state and action rewards. The Critic network calculates the value from the reward value determined during the last step of the action. combines the experience store data with the value calculated by the Critic network, slashes it from the reward value determined during the last action, then returns to update the Critic network parameters. As the advantage value guides the calculation of the actor network value, the network outputs the action value according to the old and new networks, and the distribution probability overall, and selects and outputs the action from the network. As a result, the advantage value is corrected, the actor loss is calculated, and the actor network is updated in the reverse direction. 6.2 Setting reward function value As a core challenge of deep reinforcement learning in solving practical tasks, the sparse reward problem relates to the fact that the training environment cannot supervise the updating of agent parameters in the process of reinforcement learning [25]. When supervised learning is used, the training process is supervised by humans, while in reinforcement learning, rewards are used to supervise the training process, and the agent optimizes strategies based on rewards [19]. In the experimental environment in this article, the pawn will only transmit a victory message if it reaches the control point or annihilates the opposite pawn, or if it reaches the control point or if we are all annihilated. Both cases have no rewards at every step of the training process, that is, the sparse reward problem can affect algorithm convergence. So, this article tries to use multi-attribute decision-making to judge the eopposite's threat at every step, and then set the reward function according to the threat to get a higher reward value by hitting the threatening pieces.Furthermore, an additional reward mechanism is introduced. The reward value is determined according to the state of the chess piece and the distance from the control point. Additionally, in order to prevent the agent from falling into an optimal local situation, one reward is added every time the agent wins. A table of specific additional rewards is shown in Table 13. Table 13 Additional award Situation Reward The state is now closer to the control point than the previous state Reward+0.5 This state is nearly as far from the control point as the previous state Reward-0.3 The map boundary has been reached Reward-1 Consumption per step (to avoid falling into local optimum) Reward-0.005 The opposite piece was hit Reward+\uff085*Risk of being hit by a piece\uff09 Hit by an opposite round Reward-\uff085*Risk of being hit by a piece\uff09 An opposite piece is annihilated Reward+10 Taking out one of the opposite's pieces will lead to victory Reward+20 Defeat an opposite piece leading to failure (other opposite pieces reach the control point) Reward-10 Get to the control point Reward+10 opposite wins Reward-10 When the above additional rewards are added to the training process, the convergence speed can be significantly accelerated, and the likelihood that the agent falls into the local optimum is significantly reduced. 7\u3001An experiment simulation of wargames 7.1 Experiment environment introduction It is the objective of this article to conduct experimental verification in the self-developed wargame environment. Figure 5 shows the situation that exists during the initial red and blue confrontation deployment. There are two tank pawns on each side, and the center is the point of contention.In a confrontation, both sides compete for control points, and the party that reaches the middle red flag first wins.At the same time, both red and blue parties can shoot at each other, while they can hide in urban residential areas. By concealing, it is difficult for our opponents to find our targets. Each hexagon has its own number and elevation. The higher the elevation, the darker the hexagon. On the highway, the tanks move faster than on the secondary roads. The red straight line represents the secondary road and the black straight line represents the primary road.As the cross symbol represents aiming and shooting, the destroyed target disappears from the map. Figure 5 Gaming environment display. The red and blue pawns fight separately, the red flag in the middle is the control point, and the first player to reach the control point wins.When all the chess pieces on one side are destroyed, the other side wins. 7.2 Results and analysis of the experiment In this article, the PPO algorithm and the PPO algorithm combined with multi-attribute decision-making are used to compare and analyze the winning rate. MADM-PPO and PPO are trained for 24 hours, and this article uses the MADM-PPO algorithm as the red side and the rule-based blue side algorithm to fight. At the same time, the second round uses the PPO algorithm as the red side, and the blue side fights according to rules. Next, this article observes the winning percentage of both algorithms in 100 games. Experiments have shown that the agents using the PPO reinforcement learning algorithm combined with the multi-attribute decision-making method performed better than the agents using the PPO algorithm based on the threat of the opponent.As can be seen in the figure 6 and figure 7, the threat-based multi-attribute decision-making method designed in this paper, combined with PPO algorithm of reinforcement learning, proves to effectively improve the effectiveness of intelligent wargame decision-making. A winning percentage chart is presented in the table 14, table 15. (a) (b) Figure 6. (a) Win rate: the red side is the AI of MADM-PPO intelligent algorithm and the blue side is rule-based AI; (b) Win times: the red side is the AI of MADM-PPO intelligent algorithm and the blue side is rule-based AI; The winning rate and the number of wins for the red and blue sides. The first round wins so one side starts from 1 and the other from 0. Table 14. Comparison of the number of winning matches between red and blue teams after swapping positions. Algorithm Victory Number Rounds MADM-PPO 78 100 Rule 22 100 (a) (b) Figure 7. (a) Win rate: the red side is the AI of PPO intelligent algorithm and the blue side is rule-based AI; (b) Win times: the red side is the AI of PPO intelligent algorithm and the blue side is rule-based AI; The winning rate and the number of wins for the red and blue sides. The first round wins so one side starts from 1 and the other from 0. Table 15. Comparison of winning matches between red and blue. Algorithm Victory Number Rounds PPO 62 100 Rule 38 100 The experimental results show that the MADM-PPO model can reduce the number of times to explore during training, and improve the problem that the PPO algorithm takes too long to train. It shows that the introduction of prior knowledge improves the performance of the PPO algorithm, and has a certain theoretical significance for improving the efficiency of the algorithm, the detail score is shown in Figure 8. (a) (b) (c) (d) ",
    "Conclusion": "Conclusion Using the wargaming environment, the paper proposes an entity that combines multi-attribute decision-making and reinforcement learning to solve the problem that the reinforcement learning algorithm cannot quickly converge in war game training and the agent has a low winning rate against the algorithm.As part of this study, this paper conducts experiments on the multi-attribute decision-making and reinforcement learning algorithms in a wargame simulation environment, and obtains red and blue confrontation data from the wargame environment. Calculate the weight of each attribute based on the intuitionistic fuzzy number weight calculations. Then determine the threat posed by each opponent's chess pieces. On the basis of the degree of threat, the red side reinforcement learning reward function is constructed and the AC framework is trained with the reward function, and the algorithm combines multi-attribute decision-making with reinforcement learning. A study demonstrated that the algorithm can gradually increase the reward value of the agent when exploring an environment over a short training period, while the final victory rate of the agent against specific rules and strategies reached 78%, which is significantly higher than that of a pure reinforcement learning algorithm, which is 62%. Solved the convergence difficulties of the state-space wargame's sparse rewards caused by the randomization of an agent's neural network. For the algorithm design of intelligent wargaming, this is the first attempt in this field to combine the multi-attribute decision-making method in management with the reinforcement learning algorithm in cybernetics. An interdisciplinary approach to cross-innovation in academia could lead to improvements in the design of intelligent wargames and even improvements in reinforcement learning algorithms. Reference [1] Pang Z J, Liu R Z, Meng Z Y, et al. On reinforcement learning for full-length game of starcraft[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33(01): 4691-4698. [2] Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J]. nature, 2016, 529(7587): 484-489. [3] Ye D, Liu Z, Sun M, et al. Mastering complex control in moba games with deep reinforcement learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(04): 6672-6679. [4] Silver D, Schrittwieser J, Simonyan K, et al. Mastering the game of go without human knowledge[J]. nature, 2017, 550(7676): 354-359. [5] Barriga N A, Stanescu M, Besoain F, et al. Improving rts game ai by supervised policy learning, tactical search, and deep reinforcement learning[J]. IEEE Computational Intelligence Magazine, 2019, 14(3): 8-18. [6] Schrittwieser J, Antonoglou I, Hubert T, et al. Mastering atari, go, chess and shogi by planning with a learned model[J]. Nature, 2020, 588(7839): 604-609. [7] Barriga N, Stanescu M, Buro M. Combining strategic learning with tactical search in real-time strategy games[C]//Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. 2017, 13(1). [8] Kong depeng, Chang Tianqing, Hao Na, Zhang Lei, Guo Libin. Multi attribute index processing method of ground combat target threat assessment [J]. Acta automatica Sinica, 2021,47 (01): 161-172 [9] Zhong S, Tan J, Dong H, et al. Modeling-Learning-Based Actor-Critic Algorithm with Gaussian Process Approximator[J]. Journal of Grid Computing, 2020, 18(2): 181-195. [10] Littman M L. Reinforcement learning improves behaviour from evaluative feedback[J]. Nature, 2015, 521(7553): 445-451. [11] Littman M L. Markov games as a framework for multi-agent reinforcement learning[M]//Machine learning proceedings 1994. Morgan Kaufmann, 1994: 157-163. [12] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018. [13] Hussain A, Chun J, Khan M. A novel multicriteria decision making (MCDM) approach for precise decision making under a fuzzy environment[J]. Soft Computing, 2021, 25(7): 5645-5661. [14] Opricovic S, Tzeng G H. Compromise solution by MCDM methods: A comparative analysis of VIKOR and TOPSIS[J]. European journal of operational research, 2004, 156(2): 445-455. [15] Kou G, Lu Y, Peng Y, et al. Evaluation of classification algorithms using MCDM and rank correlation[J]. International Journal of Information Technology & Decision Making, 2012, 11(01): 197-225. [16] Von Winterfeldt D, Fischer G W. Multi-attribute utility theory: models and assessment procedures[J]. Utility, probability, and human decision making, 1975: 47-85. [17] Tzeng G H, Huang J J. Multiple attribute decision making: methods and applications[M]. CRC press, 2011. [18] Zhu Y, Tian D, Yan F. Effectiveness of entropy weight method in decision-making[J]. Mathematical Problems in Engineering, 2020, 2020. [19] Zhang Zhen, Huang Yanyan, Zhang Yongliang, Chen Tiande. Game confrontation algorithm of combat entity based on near end strategy optimization [J]. Journal of Nanjing University of technology, 2021,45 (01): 77-83 [20] Li Chen, Huang Yanyan, Zhang Yongliang, Chen Tiande. Multi agent decision making method based on actor critical framework and its application in wargame [J]. Systems engineering and electronic technology, 2021,43 (03): 755-762 [21] Liu man, Zhang Hongjun, Hao Wenning, Cheng Kai, Wang Jiayin. Intelligent decision making method of tactical wargame entity combat action [J]. Control and decision, 2020,35 (12): 2977-2985 [22] Dorton S L, Maryeski L A R, Ogren L, et al. A wargame-augmented knowledge elicitation method for the agile development of novel systems[J]. Systems, 2020, 8(3): 27. [23] Zhang J, Xue Q, Chen Q, et al. Intelligent Battlefield Situation Comprehension Method Based On Deep Learning in Wargame[C]//2019 IEEE 1st International Conference on Civil Aviation Safety and Information Technology (ICCASIT). IEEE, 2019: 363-368. [24] Vlachos I K, Sergiadis G D. Intuitionistic fuzzy information\u2013applications to pattern recognition[J]. Pattern Recognition Letters, 2007, 28(2): 197-206. [25] Kaelbling L P, Littman M L, Moore A W. Reinforcement learning: A survey[J]. Journal of artificial intelligence research, 1996, 4: 237-285. ",
    "title": "",
    "paper_info": "\uf07b\n\uf07d\n\uf07b\n\uf07d\n1\n1\n2\n2\n1\n1\n2\n2\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\nn\nn\nn\nn\nA\nA\n\uf06d \uf06e\n\uf06d \uf06e\n\uf06d \uf06e\n\uf06d \uf06e\n\uf06d \uf06e\n\uf06d \uf06e\n\uf02b\n\uf02b\n\uf02b\n\uf02b\n\uf02b\n\uf02b\n\uf02b\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf0ec\n\uf03d\n\uf0ef\uf0ed\n\uf03d\n\uf0ef\uf0ee\n\uf04c\n\uf04c\nIn which\n\uf07b\n\uf07d\n\uf07b \uf07d\n12\n1,2,\n,\nmax\n,\nmin\ni\nj\nm\nij\ni\nj\nm\nij\n\uf06d\n\uf06d\n\uf06e\n\uf06e\n\uf02b\n\uf02b\n\uf03d\n\uf0bc\uf0bc\n\uf03d\n\uf0bc\n\uf03d\n\uf03d\n\uf07b\n\uf07d\n\uf07b \uf07d\n1,2,\n,\n1,2,\n,\nmin\n,\nmax\ni\nj\nm\nij\ni\nj\nm\nij\n\uf06d\n\uf06d\n\uf06e\n\uf06e\n\uf02d\n\uf02d\n\uf03d\n\uf03d\n\uf03d\n\uf03d\n\uf04c\n\uf04c\n\uff083\uff09Calculate the similarity between the fuzzy intuitionistic A and B as follows:\n\uf028\n\uf029\n\uf028\n\uf029 \uf028\n\uf029\n\uf028\n\uf029 \uf028\n\uf029\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n2\n2\n2\n2\n,\n,\n,\n1\n1\n3\n2\n3\n2\ns\n\uf06d\n\uf06d\n\uf06e\n\uf06e\n\uf06e\n\uf06e\n\uf06d\n\uf06d\n\uf070\n\uf070\n\uf070\n\uf070\n\uf06d \uf06e\n\uf06d \uf06e\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02b\n\uf02b\n\uf0e6\n\uf0f6\n\uf0e6\n\uf0f6\n\uf03d \uf02d\n\uf0b4\n\uf02d\n\uf02d\n\uf0b4\n\uf0e7\n\uf0f7\n\uf0e7\n\uf0f7\n\uf0e8\n\uf0f8\n\uf0e8\n\uf0f8\nIn which\uff0c\n1\n1\n1\n2\n2\n2\n1\n,\n1\n\uf070\n\uf06d\n\uf06e \uf070\n\uf06d\n\uf06e\n\uf03d \uf02d\n\uf02d\n\uf03d \uf02d\n\uf02d\n\uff084\uff09Calculate the similarity\niS \uf02b\nand\niS \uf02d between each solution and the optimal solution\nand the worst solution based on the following formula:\n\uf028\n\uf029\n\uf028\n\uf029\n1\n1\n,\n,\n,\n,\n,\n,\nn\ni\nk\nk\nk\nik\nik\nk\nn\ni\nk\nk\nk\nik\nik\nk\nS\nw\ns\nS\nw\ns\n\uf06d \uf06e\n\uf06d \uf06e\n\uf06d \uf06e\n\uf06d \uf06e\n\uf02b\n\uf02b\n\uf02b\n\uf03d\n\uf02d\n\uf02d\n\uf02d\n\uf03d\n\uf0ec\n\uf03d\n\uf0d7\n\uf0ef\uf0ef\uf0ed\n\uf0ef\n\uf03d\n\uf0d7\n\uf0ef\uf0ee\n\uf0e5\n\uf0e5\n(5) Then calculate the relative closeness\n\uf028\n\uf029\n/\ni\ni\ni\ni\np\nS\nS\nS\n\uf02d\n\uf02b\n\uf02d\n\uf03d\n\uf02b\nComparing threat levels of opponents based on their closeness to the target depends on the\nlevel of threat assessment performed.\n5\u3001Multi-attribute threat quantitative simulation experiment using intuitionistic\nfuzzy numbers\nThe threat assessment problem is transformed into a multi-attribute decision-making problem,\nwhile the combat intention of the target is incorporated into the evaluation system to make the\nevaluation system more reasonable and the results more reliable. A simulation scene depicts ten\ntanks of the red and blue sides fighting each other, and ten opposite are found as chess pieces in\nthe wargame. By using the war game deduction environment, each tank piece's attribute data will\nbe derived in real time, including the opposite piece's position (hexagon number), elevation, type,\n",
    "GPTsummary": "- (1): The article discusses the increasing attention towards intelligent gaming in the field of artificial intelligence and machine learning, and highlights that agents in training often have low rates of winning against specific rules and slow convergence during intelligent wargame training. \n- (2): The paper explores the challenges of using single attribute decision-making methods and reinforcement learning in wargaming, and proposes a multi-attribute decision-making approach that uses intuitionistic fuzzy numbers to determine the threat posed by each opponent's chess pieces. This approach improves the intelligence of the agent by predicting the opponent's most threatening chess piece and beating it during practice, resulting in better performance against regular opponents. The approach is motivated by the limitations of the previous methods and aims to achieve faster convergence and a higher winning rate of chess piece agent training. \n- (3): The research methodology proposed in this paper combines the multi-attribute decision-making method in management science with reinforcement learning in cybernetics. The authors extract data from the environment in wargames and combine real numbers, interval numbers, and intuitionistic fuzzy numbers for data expression. They perform the multi-attribute decision analysis to analyze the threat of the opponent and then feed back the reward function. The reward function is trained in an algorithm of reinforcement learning to obtain a more ideal result. They also use multi-attribute decision making to figure out the threat of the opponent in advance and determine it in real-time in a certain step, then attack based on the threat to obtain additional rewards and add it to the training process. \n- (4): In the simulation experiment, the algorithm of multi-attribute decision-making combined with reinforcement learning presented in this paper is significantly more intelligent than the pure reinforcement learning algorithm. By resolving the shortcomings of the agent's neural network and coupled with sparse rewards in large-map combat games, this robust algorithm effectively reduces the difficulties of convergence. The experiment demonstrates that this method combining multi-attribute decision making with reinforcement learning is effective and can be a valuable interdisciplinary approach to designing intelligent wargames and improving reinforcement learning algorithms.\n\n\n\n\n\n8. Conclusion: \n- (1): The significance of this work lies in proposing a novel approach that combines multi-attribute decision-making and reinforcement learning to improve the performance of intelligent agents in wargames. This interdisciplinary approach has the potential to contribute to the development of intelligent wargaming and reinforcement learning algorithms.\n- (2): Innovation point: The article proposes a novel approach that combines multi-attribute decision-making and reinforcement learning in the domain of wargaming. Performance: The proposed approach achieves faster convergence and a higher winning rate for agents compared to pure reinforcement learning algorithms. Workload: The paper presents a thorough analysis of the proposed approach and provides experimental evidence to support its effectiveness, which may require significant effort during the development and testing phases.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "\n"
}