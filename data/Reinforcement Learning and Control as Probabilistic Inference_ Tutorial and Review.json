{
    "Abstract": "Abstract The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in \ufb02exible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research. 1 ",
    "Introduction": "Introduction Probabilistic graphical models (PGMs) offer a broadly applicable and useful toolbox for the machine learning researcher (Koller and Friedman, 2009): by couching the entirety of the learning problem in the parlance of probability theory, they provide a consistent and \ufb02exible framework to devise principled objectives, set up models that re\ufb02ect the causal structure in the world, and allow a common set of inference methods to be deployed against a broad range of problem domains. Indeed, if a particular learning problem can be set up as a probabilistic graphical model, this can often serve as the \ufb01rst and most important step to solving it. Crucially, in the framework of PGMs, it is suf\ufb01cient to write down the model and pose the question, and the objectives for learning and inference emerge automatically. Conventionally, decision making problems formalized as reinforcement learning or optimal control have been cast into a framework that aims to generalize probabilistic models by augmenting them with utilities or rewards, where the reward function is viewed as an extrinsic signal. In this view, determining an optimal course of action (a plan) or an optimal decision-making strategy (a policy) is a fundamentally distinct type of problem than probabilistic inference, although the underlying dynamical system might still be described by a probabilistic graphical model. In this article, we instead derive an alterate view of decision making, reinforcement learning, and optimal control, where the decision making problem is simply an inference problem in a particular type of graphical model. Formalizing decision making as inference in probabilistic graphical models can in principle allow us to to bring to bear a wide array of approximate inference tools, extend the model in \ufb02exible and powerful ways, and reason about compositionality and partial observability. Speci\ufb01cally, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. This observation is not a new one, and the connection between probabilistic inference and control has been explored in the literature under a variety of names, including the Kalman duality (Todorov, 2008), maximum entropy reinforcement learning (Ziebart, 2010), KLdivergence control (Kappen et al., 2012; Kappen, 2011), and stochastic optimal control (Toussaint, 2009). While the speci\ufb01c derivations the differ, the basic underlying framework and optimization objective are the same. All of these methods involve formulating control or reinforcement learning as a PGM, either explicitly or implicitly, and then deploying learning and inference methods from the PGM literature to solve the resulting inference and learning problems. Formulating reinforcement learning and decision making as inference provides a number of other appealing tools: a natural exploration strategy based on entropy maximization, effective tools for inverse reinforcement learning, and the ability to deploy powerful approximate inference algorithms to solve reinforcement learning problems. Furthermore, the connection between probabilistic inference and control provides an appealing probabilistic interpretation for the meaning of the reward function, and its effect on the optimal policy. The design of reward or cost functions in reinforcement learning is oftentimes as much art as science, and the choice of reward often blurs the line between algorithm and objective, with task-speci\ufb01c heuristics and task objectives combined into a single reward. In the control as inference framework, the reward induces a distribution over random variables, and the optimal policy aims to explicitly match a probability distribution de\ufb01ned by the reward and system dynamics, which may in future work suggest a way to systematize reward design. This article will present the probabilistic model that can be used to embed a maximum entropy generalization of control or reinforcement learning into the framework of PGMs, describe how to perform inference in this model \u2013 exactly in the case of deterministic dynamics, or via structured variational inference in the case of stochastic dynamics, \u2013 and discuss how approximate methods based on function approximation \ufb01t within this framework. Although the particular variational inference interpretation of control differs somewhat from the presentation in prior work, the goal of this article is not to propose a fundamentally novel way of viewing the connection between control and inference. Rather, it is to provide a uni\ufb01ed treatment of the topic in a self-contained and accessible tutorial format, and to connect this framework to recent research in reinforcement learning, including recently proposed deep reinforcement learning algorithms. In addition, this article presents a review of the recent reinforcement learning literature that relates to this view of control as probabilistic inference, and offers some perspectives on future research directions. The basic graphical model for control will be presented in Section 2, variational inference for stochastic dynamics will be discussed in Section 3, approximate methods based on function approximation, including deep reinforcement learning, will be discussed in Section 4, and a survey and review of recent literature will be presented in Section 5. Finally, we will discuss perspectives on future research directions in Section 6. 2 A Graphical Model for Control as Inference In this section, we will present the basic graphical model that allows us to embed control into the framework of PGMs, and discuss how this framework can be used to derive variants of several standard reinforcement learning and dynamic programming approaches. The PGM presented in this section corresponds to a generalization of the standard reinforcement learning problem, where the RL objective is augmented with an entropy term. The magnitude of the reward function trades off between reward maximization and entropy maximization, allowing the original RL problem to be recovered in the limit of in\ufb01nitely large rewards. We will begin by de\ufb01ning notation, then de\ufb01ning the graphical model, and then presenting several inference methods and describing how they relate to standard algorithms in reinforcement learning and dynamic programming. Finally, we will discuss a few limitations of this method and motivate the variational approach in Section 3. 2.1 The Decision Making Problem and Terminology First, we will introduce the notation we will use for the standard optimal control or reinforcement learning formulation. We will use s \u2208 S to denote states and a \u2208 A to denote actions, which may each be discrete or continuous. States evolve according to the stochastic dynamics p(st+1|st, at), which are in general unknown. We will follow a discrete-time \ufb01nite-horizon derivation, with horizon T , and omit discount factors for now. A discount \u03b3 can be readily incorporated into this framework simply by modifying the transition dynamics, such that any action produces a transition into an absorbing state with probability 1 \u2212 \u03b3, and all standard transition probabilities are multiplied by \u03b3. 2 a1 a2 a3 a4 s1 s2 s3 s4 (a) graphical model with states and actions a1 a2 a3 a4 s1 s2 s3 s4 O1 O2 O3 O4 (b) graphical model with optimality variables Figure 1: The graphical model for control as inference. We begin by laying out the states and actions, which form the backbone of the model (a). In order to embed a control problem into this model, we need to add nodes that depend on the reward (b). These \u201coptimality variables\u201d correspond to observations in a HMMstyle framework: we condition on the optimality variables being true, and then infer the most probable action sequence or most probable action distributions. A task in this framework can be de\ufb01ned by a reward function r(st, at). Solving a task typically involves recovering a policy p(at|st, \u03b8), which speci\ufb01es a distribution over actions conditioned on the state parameterized by some parameter vector \u03b8. A standard reinforcement learning policy search problem is then given by the following maximization: \u03b8\u22c6 = arg max \u03b8 T \ufffd t=1 E(st,at)\u223cp(st,at|\u03b8)[r(st, at)]. (1) This optimization problem aims to \ufb01nd a vector of policy parameters \u03b8 that maximize the total expected reward \ufffd t r(st, at) of the policy. The expectation is taken under the policy\u2019s trajectory distribution p(\u03c4), given by p(\u03c4) = p(s1, at, . . . , sT, aT |\u03b8) = p(s1) T \ufffd t=1 p(at|st, \u03b8)p(st+1|st, at). (2) For conciseness, it is common to denote the action conditional p(at|st, \u03b8) as \u03c0\u03b8(at|st), to emphasize that it is given by a parameterized policy with parameters \u03b8. These parameters might correspond, for example, to the weights in a neural network. However, we could just as well embed a standard planning problem in this formulation, by letting \u03b8 denote a sequence of actions in an open-loop plan. Having formulated the decision making problem in this way, the next question we have to ask to derive the control as inference framework is: how can we formulate a probabilistic graphical model such that the most probable trajectory corresponds to the trajectory from the optimal policy? Or, equivalently, how can we formulate a probabilistic graphical model such that inferring the posterior action conditional p(at|st, \u03b8) gives us the optimal policy? 2.2 The Graphical Model To embed the control problem into a graphical model, we can begin simply by modeling the relationship between states, actions, and next states. This relationship is simple, and corresponds to a graphical model with factors of the form p(st+1|st, at), as shown in Figure 1 (a). However, this graphical model is insuf\ufb01cient for solving control problems, because it has no notion of rewards or costs. We therefore have to introduce an additional variable into this model, which we will denote Ot. This additional variable is a binary random variable, where Ot = 1 denotes that time step t is optimal, and Ot = 0 denotes that it is not optimal. We will choose the distribution over this variable to be given by the following equation: p(Ot = 1|st, at) = exp(r(st, at)). (3) The graphical model with these additional variables is summarized in Figure 1 (b). While this might at \ufb01rst seem like a peculiar and arbitrary choice, it leads to a very natural posterior distribution over 3 actions when we condition on Ot = 1 for all t \u2208 {1, . . . , T }: p(\u03c4|o1:T ) \u221d p(\u03c4, o1:T ) = p(s1) T \ufffd t=1 p(Ot = 1|st, at)p(st+1|st, at) = p(s1) T \ufffd t=1 exp(r(st, at))p(st+1|st, at) = \ufffd p(s1) T \ufffd t=1 p(st+1|st, at) \ufffd exp \ufffd T \ufffd t=1 r(st, at) \ufffd . (4) That is, the probability of observing a given trajectory is given by the product between its probability to occur according to the dynamics (the term in square brackets on the last line), and the exponential of the total reward along that trajectory. It is most straightforward to understand this equation in systems with deterministic dynamics, where the \ufb01rst term is a constant for all trajectories that are dynamically feasible. In this case, the trajectory with the highest reward has the highest probability, and trajectories with lower reward have exponentially lower probability. If we would like to plan for an optimal action sequence starting from some initial state s1, we can condition on o1:T and choose p(s1) = \u03b4(s1), in which case maximum a posteriori inference corresponds to a kind of planning problem. It is easy to see that this exactly corresponds to standard planning or trajectory optimization in the case where the dynamics are deterministic, in which case Equation (4) reduces to p(\u03c4|o1:T ) \u221d 1[p(\u03c4) \u0338= 0] exp \ufffd T \ufffd t=1 r(st, at) \ufffd . (5) Here, the indicator function simply indicates that the trajectory \u03c4 is dynamically consistent (meaning that p(st+1|st, at) \u0338= 0) and the initial state is correct. The case of stochastic dynamics poses some challenges, and will be discussed in detail in Section 3. However, even under deterministic dynamics, we are often interested in recovering a policy rather than a plan. In this PGM, the optimal policy can be written as p(at|st, Ot:T = 1) (we will drop = 1 in the remainder of the derivation for conciseness). This distribution is somewhat analogous to p(at|st, \u03b8\u22c6) in the previous section, with two major differences: \ufb01rst, it is independent of the parameterization \u03b8, and second, we will see later that it optimizes an objective that is slightly different from the standard reinforcement learning objective in Equation (1). 2.3 Policy Search as Probabilistic Inference We can recover the optimal policy p(at|st, Ot:T ) using a standard sum-product inference algorithm, analogously to inference in HMM-style dynamic Bayesian networks. As we will see in this section, it is suf\ufb01cient to compute backward messages of the form \u03b2t(st, at) = p(Ot:T |st, at). These messages have a natural interpretation: they denote the probability that a trajectory can be optimal for time steps from t to T if it begins in state st with the action at.1 Slightly overloading the notation, we will also introduce the message \u03b2t(st) = p(Ot:T |st). These messages denote the probability that the trajectory from t to T is optimal if it begins in state st. We can recover the state-only message from the state-action message by integrating out the action: \u03b2t(st) = p(Ot:T |st) = \ufffd A p(Ot:T |st, at)p(at|st)dat = \ufffd A \u03b2t(st, at)p(at|st)dat. The factor p(at|st) is the action prior. Note that it is not conditioned on O1:T in any way: it does not denote the probability of an optimal action, but simply the prior probability of actions. The PGM in Figure 1 doesn\u2019t actually contain this factor, and we can assume that p(at|st) = 1 |A| for simplicity \u2013 that is, it is a constant corresponding to a uniform distribution over the set of actions. 1Note that \u03b2t(st, at) is not a probability density over st, at, but rather the probability of Ot:T = 1. 4 We will see later that this assumption does not actually introduce any loss of generality, because any non-uniform p(at|st) can be incorporated instead into p(Ot|st, at) via the reward function. The recursive message passing algorithm for computing \u03b2t(st, at) proceeds from the last time step t = T backward through time to t = 1. In the base case, we note that p(OT |sT , aT ) is simply proportional to exp(r(sT , aT )), since there is only one factor to consider. The recursive case is then given as following: \u03b2t(st, at) = p(Ot:T |st, at) = \ufffd S \u03b2t+1(st+1)p(st+1|st, at)p(Ot|st, at)dst+1. (6) From these backward messages, we can then derive the optimal policy p(at|st, O1:T ). First, note that O1:(t\u22121) is conditionally independent of at given st, which means that p(at|st, O1:T ) = p(at|st, Ot:T ), and we can disregard the past when considering the current action distribution. This makes intuitive sense: in a Markovian system, the optimal action does not depend on the past. From this, we can easily recover the optimal action distribution using the two backward messages: p(at|st, Ot:T ) = p(st, at|Ot:T ) p(st|Ot:T ) = p(Ot:T |st, at)p(at|st)p(st) p(Ot:T |st)p(st) \u221d p(Ot:T |st, at) p(Ot:T |st) = \u03b2t(st, at) \u03b2t(st) , where the order of conditioning in the third step is \ufb02ipped by using Bayes\u2019 rule, and cancelling the factor of p(Ot:T ) that appears in both the numerator and denominator. The term p(at|st) disappears, since we previously assumed it was a uniform distribution. This derivation provides us with a solution, but perhaps not as much of the intuition. The intuition can be recovered by considering what these equations are doing in log space. To that end, we will introduce the log-space messages as Q(st, at) = log \u03b2t(st, at) V (st) = log \u03b2t(st). The use of Q and V here is not accidental: the log-space messages correspond to \u201csoft\u201d variants of the state and state-action value functions. First, consider the marginalization over actions in log-space: V (st) = log \ufffd A exp(Q(st, at))dat. When the values of Q(st, at) are large, the above equation resembles a hard maximum over at. That is, for large Q(st, at), V (st) = log \ufffd A exp(Q(st, at))dat \u2248 max at Q(st, at). For smaller values of Q(st, at), the maximum is soft. Hence, we can refer to V and Q as soft value functions and Q-functions, respectively. We can also consider the backup in Equation (6) in log-space. In the case of deterministic dynamics, this backup is given by Q(st, at) = r(st, at) + V (st+1), which exactly corresponds to the Bellman backup. However, when the dynamics are stochastic, the backup is given by Q(st, at) = r(st, at) + log Est+1\u223cp(st+1|st,at)[exp(V (st+1))]. (7) This backup is peculiar, since it does not consider the expected value at the next state, but a \u201csoft max\u201d over the next expected value. Intuitively, this produces Q-functions that are optimistic: if among the possible outcomes for the next state there is one outcome with a very high value, it will dominate the backup, even when there are other possible states that might be likely and have extremely low value. This creates risk seeking behavior: if an agent behaves according to this Qfunction, it might take actions that have extremely high risk, so long as they have some non-zero probability of a high reward. Clearly, this behavior is not desirable in many cases, and the standard PGM described in this section is often not well suited to stochastic dynamics. In Section 3, we will describe a simple modi\ufb01cation that makes the backup correspond to the soft Bellman backup in the case of stochastic dynamics also, by using the framework of variational inference. 5 2.4 Which Objective does This Inference Procedure Optimize? In the previous section, we derived an inference procedure that can be used to obtain the distribution over actions conditioned on all of the optimality variables, p(at|st, O1:T ). But which objective does this policy actually optimize? Recall that the overall distribution is given by p(\u03c4) = \ufffd p(s1) T \ufffd t=1 p(st+1|st, at) \ufffd exp \ufffd T \ufffd t=1 r(st, at) \ufffd , (8) which we can simplify in the case of deterministic dynamics into Equation (5). In this case, the conditional distributions p(at|st, O1:T ) are simply obtained by marginalizing the full trajectory distribution and conditioning the policy at each time step on st. We can adopt an optimization-based approximate inference approach to this problem, in which case the goal is to \ufb01t an approximation \u03c0(at|st) such that the trajectory distribution \u02c6p(\u03c4) \u221d 1[p(\u03c4) \u0338= 0] T \ufffd t=1 \u03c0(at|st) matches the distribution in Equation (5). In the case of exact inference, as derived in the previous section, the match is exact, which means that DKL(\u02c6p(\u03c4)\u2225p(\u03c4)) = 0, where DKL is the KL-divergence. We can therefore view the inference process as minimizing DKL(\u02c6p(\u03c4)\u2225p(\u03c4)), which is given by DKL(\u02c6p(\u03c4)\u2225p(\u03c4)) = \u2212E\u03c4\u223c\u02c6p(\u03c4)[log p(\u03c4) \u2212 log \u02c6p(\u03c4)]. Negating both sides and substituting in the equations for p(\u03c4) and \u02c6p(\u03c4), we get \u2212DKL(\u02c6p(\u03c4)\u2225p(\u03c4)) = E\u03c4\u223c\u02c6p(\u03c4) \ufffd log p(s1) + T \ufffd t=1 (log p(st+1|st, at) + r(st, at)) \u2212 log p(s1) \u2212 T \ufffd t=1 (log p(st+1|st, at) + log \u03c0(at|st)) \ufffd = E\u03c4\u223c\u02c6p(\u03c4) \ufffd T \ufffd t=1 r(st, at) \u2212 log \u03c0(at|st) \ufffd = T \ufffd t=1 E(st,at)\u223c\u02c6p(st,at))[r(st, at) \u2212 log \u03c0(at|st)] = T \ufffd t=1 E(st,at)\u223c\u02c6p(st,at))[r(st, at)] + Est\u223c\u02c6p(st)[H(\u03c0(at|st))]. Therefore, minimizing the KL-divergence corresponds to maximizing the expected reward and the expected conditional entropy, in contrast to the standard control objective in Equation (1), which only maximizes reward. Hence, this type of control objective is sometimes referred to as maximum entropy reinforcement learning or maximum entropy control. However, that in the case of stochastic dynamics, the solution is not quite so simple. Under stochastic dynamics, the optimized distribution is given by \u02c6p(\u03c4) = p(s1|O1:T ) T \ufffd t=1 p(st+1|st, at, O1:T )p(at|st, O1:T ), (9) where the initial state distribution and the dynamics are also conditioned on optimality. As a result of this, the dynamics and initial state terms in the KL-divergence do not cancel, and the objective does not have the simple entropy maximizing form derived above.2 We can still fall back on the original KL-divergence minimization at the trajectory level, and write the objective as \u2212DKL(\u02c6p(\u03c4)\u2225p(\u03c4)) = E\u03c4\u223c\u02c6p(\u03c4) \ufffd log p(s1) + T \ufffd t=1 r(st, at) + log p(st+1|st, at) \ufffd + H(\u02c6p(\u03c4)). (10) 2In the deterministic case, we know that p(st+1|st, at, O1:T ) = p(st+1|st, at), since exactly one transition is ever possible. 6 However, because of the log p(st+1|st, at) terms, this objective is dif\ufb01cult to optimize in a modelfree setting. As discussed in the previous section, it also results in an optimistic policy that assumes a degree of control over the dynamics that is unrealistic in most control problems. In Section 3, we will derive a variational inference procedure that does reduce to the convenient objective in Equation (9) even in the case of stochastic dynamics, and in the process also addresses the risk-seeking behavior discussed in Section 2.3. 2.5 Alternative Model Formulations It\u2019s worth pointing out that the de\ufb01nition of p(Ot = 1|st, at) in Equation (3) requires an additional assumption, which is that the rewards r(st, at) are always negative.3 Otherwise, we end up with a negative probability for p(Ot = 0|st, at). However, this assumption is not actually required: it\u2019s quite possible to instead de\ufb01ne the graphical model with an undirected factor on (st, at, Ot), with an unnormalized potential given by \u03a6t(st, at, Ot) = 1Ot=1 exp(r(st, at)). The potential for Ot = 0 doesn\u2019t matter, since we always condition on Ot = 1. This leads to the same exact inference procedure as the one we described above, but without the negative reward assumption. Once we are content to working with undirected graphical models, we can even remove the variables Ot completely, and simply add an undirected factor on (st, at) with the potential \u03a6t(st, at) = exp(r(st, at)), which is mathematically equivalent. This is the conditional random \ufb01eld formulation described by Ziebart (Ziebart, 2010). The analysis and inference methods in this model are identical to the ones for the directed model with explicit optimality variables Ot, and the particular choice of model is simply a notational convenience. We will use the variables Ot in this article for clarity of derivation and stay within the directed graphical model framework, but all derivations are straightforward to reproduce in the conditional random \ufb01eld formulation. Another common modi\ufb01cation to this framework is to incorporate an explicit temperature \u03b1 into the CPD for Ot, such that p(Ot|st, bat) = exp( 1 \u03b1r(st, at)). The corresponding maximum entropy objective can then be written equivalently as the expectation of the (original) reward, with an additional multiplier of \u03b1 on the entropy term. This provides a natural mechanism to interpolate between entropy maximization and standard optimal control or RL: as \u03b1 \u2192 0, the optimal solution approaches the standard optimal control solution. Note that this does not actually increase the generality of the method, since the constant 1 \u03b1 can always be multiplied into the reward, but making this temperature constant explicit can help to illuminate the connection between standard and entropy maximizing optimal control. Finally, it is worth remarking again on the role of discount factors: it is very common in reinforcement learning to use a Bellman backup of the form Q(st, at) \u2190 r(st, at) + \u03b3Est+1\u223cp(st+1|st,at)[V (st+1)], where \u03b3 \u2208 (0, 1] is a discount factor. This allows for learning value functions in in\ufb01nite-horizon settings, where the backup would otherwise be non-convergent for \u03b3 = 1, and reduces variance for Monte Carlo advantage estimators in policy gradient algorithms (Schulman et al., 2016). The discount factor can be viewed a simple rede\ufb01nition of the system dynamics. If the initial dynamics are given by p(st+1|st, at), adding a discount factor is equivalent to undiscounted value \ufb01tting under the modi\ufb01ed dynamics \u00afp(st+1|st, at) = \u03b3p(st+1|st, at), where there is an additional transition with probability 1 \u2212 \u03b3, regardless of action, into an absorbing state with reward zero. We will omit \u03b3 from the derivations in this article, but it can be inserted trivially in all cases simply by modifying the (soft) Bellman backups in any place where the expectation over p(st+1|st, at) occurs, such as Equation (7) previously or Equation (15) in the next section. 3 Variational Inference and Stochastic Dynamics The problematic nature of the maximum entropy framework in the case of stochastic dynamics, discussed in Section 2.3 and Section 2.4, in essence amounts to an assumption that the agent is allowed to control both its actions and the dynamics of the system in order to produce optimal trajectories, but its authority over the dynamics is penalized based on deviation from the true dynamics. Hence, the log p(st+1|st, at) terms in Equation (10) can be factored out of the equations, producing additive 3This assumption is not actually very strong: if we assume the reward is bounded above, we can always construct an exactly equivalent reward simply by subtracting the maximum reward. 7 terms that corresponds to the cross-entropy between the posterior dynamics p(st+1|st, at, O1:T ) and the true dynamics p(st+1|st, at). This explains the risk-seeking nature of the method discussed in Section 2.3: if the agent is allowed to in\ufb02uence its dynamics, even a little bit, it would reasonably choose to remove unlikely but extremely bad outcomes of risky actions. Of course, in practical reinforcement learning and control problems, such manipulation of system dynamics is not possible, and the resulting policies can lead to disastrously bad outcomes. We can correct this issue by modifying the inference procedure. In this section, we will derive this correction by freezing the system dynamics, writing down the corresponding maximum entropy objective, and deriving a dynamic programming procedure for optimizing it. Then we will show that this procedure amounts to a direct application of structured variational inference. 3.1 Maximum Entropy Reinforcement Learning with Fixed Dynamics The issue discussed in Section 2.4 for stochastic dynamics can brie\ufb02y be summarized as following: since the posterior dynamics distribution p(st+1|st, at, O1:T ) does not necessarily match the true dynamics p(st+1|st, at), the agent assumes that it can in\ufb02uence the dynamics to a limited extent. A simple \ufb01x to this issue is to explicitly disallow this control, by forcing the posterior dynamics and initial state distributions to match p(st+1|st, at) and p(s1), respectively. Then, the optimized trajectory distribution is given simply by \u02c6p(\u03c4) = p(s1) T \ufffd t=1 p(st+1|st, at)\u03c0(at|st), and the same derivation as the one presented in Section 2.4 for the deterministic case results in the following objective: \u2212DKL(\u02c6p(\u03c4)\u2225p(\u03c4)) = T \ufffd t=1 E(st,at)\u223c\u02c6p(st,at))[r(st, at) + H(\u03c0(at|st))]. (11) That is, the objective is still to maximize reward and entropy, but now under stochastic transition dynamics. To optimize this objective, we can compute backward messages like we did in Section 2.3. However, since we are now starting from the maximization of the objective in Equation (11), we have to derive these backward messages from an optimization perspective as a dynamic programming algorithm. As before, we will begin with the base case of optimizing \u03c0(aT |sT ), which maximizes E(sT ,aT )\u223c\u02c6p(sT ,aT )[r(sT , aT ) \u2212 log \u03c0(aT |sT )] = EsT \u223c\u02c6p(sT ) \ufffd \u2212DKL \ufffd \u03c0(aT |sT )\u2225 1 exp(V (sT )) exp(r(sT , aT )) \ufffd + V (sT ) \ufffd , (12) where the equality holds from the de\ufb01nition of KL-divergence, and exp(V (sT )) is the normalizing constant for exp(r(sT , aT )) with respect to aT where V (sT ) = log \ufffd A exp(r(sT , aT ))daT , which is the same soft maximization as in Section 2.3. Since we know that the KL-divergence is minimized when the two arguments represent the same distribution, the optimal policy is given by \u03c0(aT |sT ) = exp (r(sT , aT ) \u2212 V (sT )) , (13) The recursive case can then computed as following: for a given time step t, \u03c0(at|st) must maximize two terms: E(st,at)\u223c\u02c6p(st,at)[r(st, at) \u2212 log \u03c0(at|st)] + E(st,at)\u223c\u02c6p(st,at)[Est+1\u223cp(st+1|st,at)[V (st+1)]]. (14) The \ufb01rst term follows directly from the objective in Equation (11), while the second term represents the contribution of \u03c0(at|st) to the expectations of all subsequent time steps. The second term deserves a more in-depth derivation. First, consider the base case: given the equation for \u03c0(aT |sT ) in Equation (13), we can evaluate the objective for the policy by directly substituting this equation into Equation (12). Since the KL-divergence then evaluates to zero, we are left only with the V (sT ) term. In the recursive case, we note that we can rewrite the objective in Equation (14) as E(st,at)\u223c\u02c6p(st,at)[r(st, at) \u2212 log \u03c0(at|st)] + E(st,at)\u223c\u02c6p(st,at)[Est+1\u223cp(st+1|st,at)[V (st+1)]] = Est\u223c\u02c6p(st) \ufffd \u2212DKL \ufffd \u03c0(at|st)\u2225 1 exp(V (st)) exp(Q(st, at)) \ufffd + V (st) \ufffd , 8 where we now de\ufb01ne Q(st, at) = r(st, at) + Est+1\u223cp(st+1|st,at)[V (st+1)] (15) V (st) = log \ufffd A exp(Q(st, at))dat, which corresponds to a standard Bellman backup with a soft maximization for the value function. Choosing \u03c0(at|st) = exp (Q(st, at) \u2212 V (st)) , (16) we again see that the KL-divergence evaluates to zero, leaving Est\u223c\u02c6p(st)[V (st)] as the only remaining term in the objective for time step t, just like in the base case of t = T . This means that, if we \ufb01x the dynamics and initial state distribution, and only allow the policy to change, we recover a Bellman backup operator that uses the expected value of the next state, rather than the optimistic estimate we saw in Section 2.3 (compare Equation (15) to Equation (7)). While this provides a solution to the practical problem of risk-seeking policies, it is perhaps a bit unsatisfying in its divergence from the convenient framework of probabilistic graphical models. In the next section, we will discuss how this procedure amounts to a direct application of structured variational inference. 3.2 Connection to Structured Variational Inference One way to interpret the optimization procedure in Section 3.1 is as a particular type of structured variational inference. In structured variational inference, our goal is to approximate some distribution p(y) with another, potentially simpler distribution q(y). Typically, q(y) is taken to be some tractable factorized distribution, such as a product of conditional distributions connected in a chain or tree, which lends itself to tractable exact inference. In our case, we aim to approximate p(\u03c4), given by p(\u03c4) = \ufffd p(s1) T \ufffd t=1 p(st+1|st, at) \ufffd exp \ufffd T \ufffd t=1 r(st, at) \ufffd , (17) via the distribution q(\u03c4) = q(s1) T \ufffd t=1 q(st+1|st, at)q(at|st). (18) If we \ufb01x q(s1) = p(s1) and q(st+1|st, at) = p(st+1|st, at), then q(\u03c4) is exactly the distribution \u02c6p(\u03c4) from Section 3.1, which we\u2019ve renamed here to q(\u03c4) to emphasize the connection to structured variational inference. Note that we\u2019ve also renamed \u03c0(at|st) to q(at|st) for the same reason. In structured variational inference, approximate inference is performed by optimizing the variational lower bound (also called the evidence lower bound). Recall that our evidence here is that Ot = 1 for all t \u2208 {1, . . ., T }, and the posterior is conditioned on the initial state s1. The variational lower bound is given by log p(O1:T ) = log \ufffd \ufffd p(O1:T , s1:T , a1:T )ds1:T da1:T = log \ufffd \ufffd p(O1:T , s1:T , a1:T )q(s1:T , a1:T ) q(s1:T , a1:T )ds1:T da1:T = log E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) \ufffdp(O1:T , s1:T , a1:T ) q(s1:T , a1:T ) \ufffd \u2265 E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) [log p(O1:T , s1:T , a1:T ) \u2212 log q(s1:T , a1:T )] , where the inequality on the last line is obtained via Jensen\u2019s inequality. Substituting the de\ufb01nitions of p(\u03c4) and q(\u03c4) from Equations (17) and (18), and noting the cancellation due to q(st+1|st, at) = p(st+1|st, at), the bound reduces to log p(O1:T ) \u2265 E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) \ufffd T \ufffd t=1 r(st, at) \u2212 log q(at|st) \ufffd , (19) up to an additive constant. Optimizing this objective with respect to the policy q(at|st) corresponds exactly to the objective in Equation (11). Intuitively, this means that this objective attempts to \ufb01nd 9 the closest match to the maximum entropy trajectory distribution, subject to the constraint that the agent is only allowed to modify the policy, and not the dynamics. Note that this framework can also easily accommodate any other structural constraints on the policy, including restriction to a particular distribution class (e.g., conditional Gaussian, or a categorical distribution parameterized by a neural network), or restriction to partial observability, where the entire state st is not available as an input, but rather the policy only has access to some non-invertible function of the state. 4 Approximate Inference with Function Approximation We saw in the discussion above that a dynamic programming backward algorithm with updates that resemble Bellman backups can recover \u201csoft\u201d analogues of the value function and Q-function in the maximum entropy reinforcement learning framework, and the stochastic optimal policy can be recovered from the Q-function and value function. In this section, we will discuss how practical algorithms for high-dimensional or continuous reinforcement learning problems can be derived from this theoretical framework, with the use of function approximation. This will give rise to several prototypical methods that mirror corresponding techniques in standard reinforcement learning: policy gradients, actor-critic algorithms, and Q-learning. 4.1 Maximum Entropy Policy Gradients One approach to performing structured variational inference is to directly optimize the evidence lower bound with respect to the variational distribution (Koller and Friedman, 2009). This approach can be directly applied to maximum entropy reinforcement learning. Note that the variational distribution consists of three terms: q(s1), q(st+1|st, at), and q(at|st). The \ufb01rst two terms are \ufb01xed to p(s1) and p(st+1|st, at), respectively, leaving only q(at|st) to vary. We can parameterize this distribution with any expressive conditional, with parameters \u03b8, and will therefore denote it as q\u03b8(at|st). The parameters could correspond, for example, to the weights in a deep neural network, which takes st as input and outputs the parameters of some distribution class. In the case of discrete actions, the network could directly output the parameters of a categorical distribution (e.g., via a soft max operator). In the case of continuous actions, the network could output the parameters of an exponential family distribution, such as a Gaussian. In all cases, we can directly optimize the objective in Equation (11) by estimating its gradient using samples. This gradient has a form that is nearly identical to the standard policy gradient (Williams, 1992), which we summarize here for completeness. First, let us restate the objective as following: J(\u03b8) = T \ufffd t=1 E(st,at)\u223cq(st,at) [r(st, at) \u2212 H(q\u03b8(at|st))] . The gradient is then given by \u2207\u03b8J(\u03b8) = T \ufffd t=1 \u2207\u03b8E(st,at)\u223cq(st,at) [r(st, at) + H(q\u03b8(at|st))] = T \ufffd t=1 E(st,at)\u223cq(st,at) \ufffd \u2207\u03b8 log q\u03b8(at|st) \ufffd T \ufffd t\u2032=t r(st\u2032, at\u2032) \u2212 log q\u03b8(at\u2032|st\u2032) \u2212 1 \ufffd\ufffd = T \ufffd t=1 E(st,at)\u223cq(st,at) \ufffd \u2207\u03b8 log q\u03b8(at|st) \ufffd T \ufffd t\u2032=t r(st\u2032, at\u2032) \u2212 log q\u03b8(at\u2032|st\u2032) \u2212 b(st\u2032) \ufffd\ufffd , where the second line follows from applying the likelihood ratio trick (Williams, 1992) and the de\ufb01nition of entropy to obtain the log q\u03b8(at\u2032|st\u2032) term. The \u22121 comes from the derivative of the entropy term. The last line follows by noting that the gradient estimator is invariant to additive statedependent constants, and replacing \u22121 with a state-dependent baseline b(st\u2032). The resulting policy gradient estimator exactly matches a standard policy gradient estimator, with the only modi\ufb01cation being the addition of the \u2212 log q\u03b8(at\u2032|st\u2032) term to the reward at each time step t\u2032. Intuitively, the reward of each action is modi\ufb01ed by subtracting the log-probability of that action under the current policy, which causes the policy to maximize entropy. This gradient estimator can be written more 10 compactly as \u2207\u03b8J(\u03b8) = T \ufffd t=1 E(st,at)\u223cq(st,at) \ufffd \u2207\u03b8 log q\u03b8(at|st) \u02c6A(st, at) \ufffd , where \u02c6A(st, at) is an advantage estimator. Any standard advantage estimator, such as the GAE estimator (Schulman et al., 2016), can be used in place of the standard baselined Monte Carlo return above. Again, the only necessary modi\ufb01cation is to add \u2212 log q\u03b8(at\u2032|st\u2032) to the reward at each time step t\u2032. As with standard policy gradients, a practical implementation of this method estimates the expectation by sampling trajectories from the current policy, and may be improved by following the natural gradient direction. 4.2 Maximum Entropy Actor-Critic Algorithms Instead of directly differentiating the variational lower bound, we can adopt a message passing approach which, as we will see later, can produce lower-variance gradient estimates. First, note that we can write down the following equation for the optimal target distribution for q(at|st): q\u22c6(at|st) = 1 Z exp \ufffd Eq(s(t+1):T ,a(t+1):T |st,at) \ufffd T \ufffd t\u2032=t log p(Ot\u2032|st\u2032, at\u2032) \u2212 T \ufffd t\u2032=t+1 log q(at\u2032|st\u2032) \ufffd\ufffd . This is because conditioning on st makes the action at completely independent of all past states, but the action still depends on all future states and actions. Note that the dynamics terms p(st+1|st, at) and q(st+1|st, at) do not appear in the above equation, since they perfectly cancel. We can simplify the expectation above as follows: Eq(s(t+1):T ,a(t+1):T |st,at)[log p(Ot:T |st:T , at:T )] = log p(Ot|st, at) + Eq(st+1|st,at) \ufffd E \ufffd T \ufffd t\u2032=t+1 log p(Ot\u2032|st\u2032, at\u2032) \u2212 log q(at\u2032|st\u2032) \ufffd\ufffd . In this case, note that the inner expectation does not contain st or at, and therefore makes for a natural representation for a message that can be sent from future states. We will denote this message V (st+1), since it will correspond to a soft value function: V (st) = E \ufffd T \ufffd t\u2032=t+1 log p(Ot\u2032|st\u2032, at\u2032) \u2212 log q(at\u2032|st\u2032) \ufffd = Eq(at|st)[log p(Ot|st, at) \u2212 log q(at|st) + Eq(st+1|st,at[V (st+1)]]. For convenience, we can also de\ufb01ne a Q-function as Q(st, at) = log p(Ot|st, at) + Eq(st+1|st,at)[V (st+1)], such that V (st) = Eq(at|st)[Q(st, at) \u2212 log q(at|st)], and the optimal policy is q\u22c6(at|st) = exp(Q(st, at)) log \ufffd A exp(Q(st, at))dat . (20) Note that, in this case, the value function and Q-function correspond to the values of the current policy q(at|st), rather than the optimal value function and Q-function, as in the case of dynamic programming. However, at convergence, when q(at|st) = q\u22c6(at|st) for each t, we have V (st) = Eq(at|st)[Q(st, at) \u2212 log q(at|st)] = Eq(at|st) \ufffd Q(st, at) \u2212 Q(st, at) + log \ufffd A exp(Q(st, at))dat \ufffd = log \ufffd A exp(Q(st, at))dat, (21) which is the familiar soft maximum from Section 2.3. We now see that the optimal variational distribution for q(at|st) can be computed by passing messages backward through time, and the messages are given by V (st) and Q(st, at). 11 So far, this derivation assumes that the policy and messages can be represented exactly. We can relax the \ufb01rst assumption in the same way as in the preceding section. We \ufb01rst write down the variational lower bound for a single factor q(at|st) as following: max q(at|st) Est\u223cq(st) \ufffd Eat\u223cq(at|st)[Q(st, at) \u2212 log q(at|st)] \ufffd . (22) It\u2019s straightforward to show that this objective is simply the full variational lower bound, which is given by Eq(\u03c4)[log p(\u03c4) \u2212 log q(\u03c4)], restricted to just the terms that include q(at|st). If we restrict the class of policies q(at|st) so that they cannot represent q\u22c6(at|st) exactly, we can still optimize the objective in Equation (22) by computing its gradient, which is given by Est\u223cq(st) \ufffd Eat\u223cq(at|st)[\u2207 log q(at|st)(Q(st, at) \u2212 log q(at|st) \u2212 b(st))] \ufffd , where b(st) is any state-dependent baseline. This gradient can computed using samples from q(\u03c4) and, like the policy gradient in the previous section, is directly analogous to a classic likelihood ratio policy gradient. The modi\ufb01cation lies in the use of the backward message Q(st, at) in place of the Monte Carlo advantage estimate. The algorithm therefore corresponds to an actor-critic algorithm, which generally provides lower variance gradient estimates. In order to turn this into a practical algorithm, we must also be able to approximate the backward message Q(st, at) and V (st). A simple and straightforward approach is to represent them with parameterized functions Q\u03c6(st, at) and V\u03c8(st), with parameters \u03c6 and \u03c8, and optimize the parameters to minimize a squared error objectives E(\u03c6) = E(st,at)\u223cq(st,at) \ufffd\ufffd r(st, at) + Eq(st+1|st,at)[V\u03c8(st+1)] \u2212 Q\u03c6(st, at) \ufffd2\ufffd (23) E(\u03c8) = Est\u223cq(st) \ufffd\ufffd Eat\u223cq(at|st)[Q\u03c6(st, at) \u2212 log q(at|st)] \u2212 V\u03c8(st, at) \ufffd2\ufffd . This interpretation gives rise to a few interesting possibilities for maximum entropy actor-critic and policy iteration algorithms. First, it suggests that it may be bene\ufb01cial to keep track of both V (st) and Q(st, at) networks. This is perfectly reasonable in a message passing framework, and in practice might have many of the same bene\ufb01ts as the use of a target network, where the updates to Q and V can be staggered or damped for stability. Second, it suggests that policy iteration or actor-critic methods might be preferred (over, for example, direct Q-learning), since they explicitly handle both approximate messages and approximate factors in the structured variational approximation. This is precisely the scheme employed by the soft actor-critic algorithm (Haarnoja et al., 2018b). 4.3 Soft Q-Learning We can derive an alternative form for a reinforcement learning algorithm without using an explicit policy parameterization, \ufb01tting only the messages Q\u03c6(st, at). In this case, we assume an implicit parameterization for both the value function V (st) and policy q(at|st), where V (st) = log \ufffd A exp(Q(st, at))dat, as in Equation (21), and q(at|st) = exp(Q(st, at) \u2212 V (st)), which corresponds directly to Equation (20). In this case, no further parameterization is needed beyond Q\u03c6(st, at), which can be learned by minimizing the error in Equation (23), substituting the implicit equation for V (st) in place of V\u03c8(st). We can write the resulting gradient update as \u03c6 \u2190 \u03c6 \u2212 \u03b1E \ufffddQ\u03c6 d\u03c6 (st, at) \ufffd Q\u03c6(st, at) \u2212 \ufffd r(st, at) + log \ufffd A exp(Q(st+1, at+1))dat+1 \ufffd\ufffd\ufffd . It is worth pointing out the similarity to the standard Q-learning update: \u03c6 \u2190 \u03c6 \u2212 \u03b1E \ufffddQ\u03c6 d\u03c6 (st, at) \ufffd Q\u03c6(st, at) \u2212 \ufffd r(st, at) + max at+1 Q\u03c6(st+1, at+1)) \ufffd\ufffd\ufffd . Where the standard Q-learning update has a max over at+1, the soft Q-learning update has a \u201csoft\u201d max. As the magnitude of the reward increases, the soft update comes to resemble the hard update. 12 In the case of discrete actions, this update is straightforward to implement, since the integral is replaced with a summation, and the policy can be extracted simply by normalizing the Q-function. In the case of continuous actions, a further level of approximation is needed to evaluate the integral using samples. Sampling from the implicit policy is also non-trivial, and requires an approximate inference procedure, as discussed by Haarnoja et al. (Haarnoja et al., 2017). We can further use this framework to illustrate an interesting connection between soft Q-learning and policy gradients. According to the de\ufb01nition of the policy in Equation (20), which is de\ufb01ned entirely in terms of Q\u03c6(st, at), we can derive an alternative gradient with respect to \u03c6 starting from the policy gradient. This derivation represents a connection between policy gradient and Q-learning that is not apparent in the standard framework, but becomes apparent in the maximum entropy framework. The full derivation is provided by Haarnoja et al. (Haarnoja et al., 2017) (Appendix B). The \ufb01nal gradient corresponds to \u2207\u03c6J(\u03c6) = T \ufffd t=1 E(st,at)\u223cq(st,at) \ufffd (\u2207\u03c6Q(st, at) \u2212 \u2207\u03c6V (st)) \u02c6A(st, at) \ufffd . The soft Q-learning gradient can equivalently be written as \u2207\u03c6J(\u03c6) = T \ufffd t=1 E(st,at)\u223cq(st,at) \ufffd \u2207\u03c6Q(st, at) \u02c6A(st, at) \ufffd , (24) where we substitute the target value r(st, at) + V (st+1) for \u02c6A(st, at), taking advantage of the fact that we can use any state-dependent baseline. Although these gradients are not exactly equal, the extra term \u2212\u2207\u03c6V (st) simply accounts for the fact that the policy gradient alone is insuf\ufb01cient to resolve one extra degree of freedom in Q(st, at): the addition or subtraction of an action-independent constant. We can eliminate this term if we add the policy gradient with respect to \u03c6 together with Bellman error minimization for V (st), which has the gradient \u2207\u03c6V (st)Eat\u223cq(at|st) \ufffd r(st, at) + Est+1\u223cq(st+1|st,at)[V (st+1)] \ufffd = \u2207\u03c6V (st)Eat\u223cq(at|st)[ \u02c6Q(st, at)]. Noting that \u02c6Q(st, at) is simply a (non-baselined) return estimate, we can show that the sum of the policy gradient and value gradient exactly matches Equation (24) for a particular choice of state-dependent baseline, since the term \u2207\u03c6V (st)Eat\u223cq(at|st)[ \u02c6Q(st, at)] cancels against the term \u2212\u2207\u03c6V (st) \u02c6A(st, at) in expectation over at when \u02c6A(st, at) = \u02c6Q(st, at) (that is, when we use a baseline of zero). This completes the proof of a general equivalence between soft Q-learning and policy gradients. 5 Review of Prior Work In this section, we will discuss a variety of prior works that have sought to explore the connection between inference and control, make use of this connection to devise more effective learning algorithms, and extend it into other applications, such as intent inference and human behavior forecasting. We will \ufb01rst discuss the variety of frameworks proposed in prior work that are either equivalent to the approach presented in this article, or special cases (or generalization) thereof (Section 5.1). We will then discuss alternative formulations that, though similar, differ in some critical way (Section 5.2). We will then discuss speci\ufb01c reinforcement learning algorithms that build on the maximum entropy framework (Section 5.3), and conclude with a discussion of applications of the maximum entropy framework in other areas, such as intent inference, human behavior modeling, and forecasting (Section 5.4). 5.1 Frameworks for Control as Inference Framing control, decision making, and reinforcement learning as a probabilistic inference and learning problem has a long history, going back to original work by Rudolf Kalman (Kalman, 1960), who described how the Kalman smoothing algorithm can also be used to solve control problems with linear dynamics and quadratic costs (the \u201clinear-quadratic regulator\u201d or LQR setting). It is worth noting that, in the case of linear-quadratic systems, the maximum entropy solution is a linearGaussian policy where the mean corresponds exactly to the optimal deterministic policy. This sometimes referred to as the Kalman duality (Todorov, 2008). Unfortunately, this elegant duality does 13 ",
    "Approaches": "Approaches All of the methods discussed in the previous section are either special cases or generalizations of the control as inference framework presented in this article. A number of other works have presented related approaches that also aim to unify control and inference, but do so in somewhat different ways. We survey some of these prior techniques in this section, and describe their technical and practical differences from the presented formulation. Boltzmann Exploration. The form of the optimal policy in the maximum entropy framework (e.g., Equation (20)) suggests a very natural exploration strategy: actions that have large Q-value should be taken more often, while actions that have low Q-value should be taken less often, and the stochastic exploration strategy has the form of a Boltzmann-like distribution, with the Q-function acting as the negative energy. A large number of prior methods (Sutton, 1990; Kaelbling et al., 1996) have proposed to use such a policy distribution as an exploration strategy, but in the context of a reinforcement learning algorithm where the Q-function is learned via the standard (\u201chard\u201d) max operator, corresponding to a temperature (see Section 2.5) of zero. Boltzmann exploration therefore does not optimize the maximum entropy objective, but rather serves as a heuristic modi\ufb01cation to enable improved exploration. A closely related idea is presented in the work on energy-based reinforcement learning (Sallans and Hinton, 2004; Heess et al., 2013), where the free energy of an energy-based model (in that case, a restricted Boltzmann machine) is adjusted based on a reinforcement learning update rule, such that the energy corresponds to the negative Q-function. Interestingly, energy-based reinforcement learning can optimize either the maximum entropy objective or the standard objective (with Boltzmann exploration), based on the type of update rule that is used. When used with an on-policy SARSA update rule, as proposed by Sallans and Hinton (2004), the method actually does optimize the maximum entropy objective, since the policy uses the Boltzmann distribution. However, when updated using an off-policy Q-learning objective with a hard max, the method reduces to Boltzmann exploration and optimizes the standard RL objective. 14 Entropy Regularization. In the context of policy gradient and actor-critic methods, a commonly used technique is to use \u201centropy regularization,\u201d where an entropy maximization term is added to the policy objective to prevent the policy from becoming too deterministic prematurely. This technique was proposed as early as the \ufb01rst work on the REINFORCE algorithm (Williams and Peng, 1991; Williams, 1992), and is often used in recent methods (see, e.g., discussion by O\u2019Donoghue et al. (2017)). While the particular technique for incorporating this entropy regularizer varies, typically the simplest way is to simply add the gradient of the policy entropy at each sampled state to a standard policy gradient estimate, which itself may use a critic. Note that this is not, in general, equivalent to the maximum entropy objective, which not only optimizes for a policy with maximum entropy, but also optimizes the policy itself to visit states where it has high entropy. Put another way, the maximum entropy objective optimizes the expectation of the entropy with respect to the policy\u2019s state distribution, while entropy regularization only optimizes the policy entropy at the states that are visited, without actually trying to modify the policy itself to visit highentropy states (see, e.g., Equation (2) in O\u2019Donoghue et al. (2017)). While this does correspond to a well-de\ufb01ned objective, that objective is rather involved to write out and generally not mentioned in work that uses entropy regularization. The technique is typically presented as a heuristic modi\ufb01cation to the policy gradient. Interestingly, it is actually easier to perform proper maximum entropy RL than entropy regularization: maximum entropy RL with a policy gradient or actor-critic method only requires subtracting log \u03c0(a|s) from the reward function, while heuristic entropy maximization typically uses an explicit entropy gradient. Variational Policy Search and Expectation Maximization. Another formulation of the reinforcement learning problem with strong connections to probabilistic inference is the formulation of policy search in an expectation-maximization style algorithm. One common way to accomplish this is to directly treat rewards as a probability density, and then use a \u201cpseudo-likelihood\u201d written as J(\u03b8) = \ufffd r(\u03c4)p(\u03c4|\u03b8)d\u03c4, where r(\u03c4) is the total reward along a trajectory, and p(\u03c4|\u03b8) is the probability of observing a trajectory \u03c4 given a policy parameter vector \u03b8. Assuming r(\u03c4) is positive and bounded and applying Jensen\u2019s inequality results in a variety of algorithms (Peters and Schaal, 2007; Hachiya et al., 2009; Neumann, 2011; Abdolmaleki et al., 2018), including reward-weighted regression (Peters and Schaal, 2007), that all follow the following general recipe: samples are weighted according to some function of their return (potentially with importance weights), and the policy is then updated by optimizing a regression objective to match the sample actions, weighted by these weights. The result is that samples with higher return are matched more closely, while those with low return are ignored. Variational policy search methods also fall into this category (Neumann, 2011; Levine and Koltun, 2013b), sometimes with the modi\ufb01cation of using explicit trajectory optimization rather than reweighting to construct the target actions (Levine and Koltun, 2013b), and sometimes using an exponential transformation on r(\u03c4) to ensure positivity. Unlike the approach discussed in this article, the use of the reward as a \u201cpseudo-likelihood\u201d does not correspond directly to a well-de\ufb01ned probabilistic model, though the application of Jensen\u2019s inequality can still be motivated simply from the standpoint of deriving a bound for the RL optimization problem. A more serious disadvantage of this class of methods is that, by regressing onto the reweighted samples, the method loses the ability to properly handle risk for stochastic dynamics and policies. Consider, for example, a setting where we aim to \ufb01t a unimodal policy for a stateless problem with a 1D action. If we have a high reward for the action \u22121 and +1, and a low reward for the action 0, the optimal \ufb01t will still place all of the probability mass in the middle, at the action 0, which is the worst possible option. Mathematically, this problem steps from the fact that supervised learning matches a target distribution by minimizing a KL-divergence of the form DKL(ptgt\u2225p\u03b8), where ptgt is the target distribution (e.g., the reward or exponentiated reward). RL instead minimizes a KL-divergence of the form DKL(p\u03b8\u2225ptgt), which prioritizes \ufb01nding a mode of the target distribution rather than matching its moments. This issue is discussed in more detail in Section 5.3.5 of (Levine, 2014). In general, the issue manifests itself as risk-seeking behavior, though distinct in nature from the riskseeking behavior discussed in Section 2.4. Note that Toussaint and Storkey (2006) also propose an expectation-maximization based algorithm for control as inference, but in a framework that does in fact yield maximum expected reward solutions, with a similar formulation to the one in this article. 15 KL-Divergence Constraints for Policy Search. Policy search methods frequently employ a constraint between the new policy and the old policy at each iteration, in order to bound the change in the policy distribution and thereby ensure smooth, stable convergence. Since policies are distributions, a natural choice for the form of this constraint is a bound on the KL-divergence between the new policy and the old one (Bagnell and Schneider, 2003; Peters et al., 2010; Levine and Abbeel, 2014; Schulman et al., 2015; Abdolmaleki et al., 2018). When we write out the Lagrangian of the resulting optimization problem, we typically end up with a maximum entropy optimization problem similar to the one in Equation (11), where instead of taking the KL-divergence between the new policy and exponentiated reward, we instead have a KL-divergence between the new policy and the old one. This corresponds to a maximum entropy optimization where the reward is r(s, a) + \u03bb log \u00af\u03c0(a|s), where \u03bb is the Lagrange multiplier and \u00af\u03c0 is the old policy, and the entropy term has a weight of \u03bb. This is equivalent to a maximum entropy optimization where the entropy has a weight of one, and the reward is scaled by 1 \u03bb. Thus, although none of these methods actually aim to optimize the maximum entropy objective in the end, each step of the policy update involves solving a maximum entropy problem. A similar approach is proposed by Rawlik et al. (2013), where a sequence of maximum entropy problems is solved in a Q-learning style framework to eventually arrive at the standard maximum reward solution. 5.3 Reinforcement Learning Algorithms Maximum entropy reinforcement learning algorithms have been proposed in a range of frameworks and with a wide variety of assumptions. The path integral framework has been used to derive algorithms for both optimal control and planning (Kappen, 2011) and policy search via reinforcement learning (Theodorou et al., 2010). The framework of linearly solvable MDPs has been used to derive policy search algorithms (Todorov, 2010), value function based algorithms (Todorov, 2006), and inverse reinforcement learning algorithms (Dvijotham and Todorov, 2010). More recently, entropy maximization has been used as a component in algorithms based on model-free policy search with importance sampling and its variants (Levine and Koltun, 2013a; Nachum et al., 2017a,b), model-based algorithms based on the guided policy search framework (Levine and Abbeel, 2014; Levine and Koltun, 2014; Levine et al., 2016), and a variety of methods based on soft Q-learning (Haarnoja et al., 2017; Schulman et al., 2017) and soft actor-critic algorithms (Haarnoja et al., 2018b; Hausman et al., 2018). The particular reasons for the use of the control as inference framework differ between each of these algorithms. The motivation for linearly solvable MDPs is typically based on computationally tractable exact solutions for tabular settings (Todorov, 2010), which are enabled essentially by dispensing with the non-linear maximization operator in the standard RL framework. Although the maximum entropy dynamic programming equations are still not linear in terms of value functions and Q-functions, they are linear under an exponential transformation. The reason for this is quite natural: since these methods implement sum-product message passing, the only operations in the original probability space are summations and multiplications. However, for larger problems where tabular representations are impractical, these bene\ufb01ts are not apparent. In the case of path consistency methods (Nachum et al., 2017a,b), the maximum entropy framework offers an appealing mechanism for off-policy learning. In the case of guided policy search, it provides a natural method for matching distributions between model-based local policies and a global policy that uni\ufb01es the local policy into a single globally coherent strategy (Levine and Koltun, 2014; Levine and Abbeel, 2014; Levine et al., 2016). For the more recent model-free maximum entropy algorithms, such as soft Q-learning (Haarnoja et al., 2017) and soft-actor critic (Haarnoja et al., 2018b), as well the work of Hausman et al. (2018), the bene\ufb01ts are improved stability and modelfree RL performance, improved exploration, and the ability to pre-train policies for diverse and under-speci\ufb01ed goals. For example, Haarnoja et al. (2017) present a quadrupedal robot locomotion task where the reward depends only on the speed of the robot\u2019s motion, regardless of direction. In a standard RL framework, this results in a policy that runs in an arbitrary direction. Under the maximum entropy framework, the optimal policy runs in all directions with equal probability. This makes it well-suited for pretraining general-purpose policies that can then be \ufb01netuned for more narrowly tailored tasks. More recently, Haarnoja et al. also showed that maximum entropy policies can be composed simply by adding their Q-functions, resulting in a Q-function with bounded difference against the optimal Q-function for the corresponding composed reward (Haarnjoa et al., 2018). 16 Recently, a number of papers have explored how the control as inference or maximum entropy reinforcement learning framework can be extended to add additional latent variables to the model, such that the policy is given by \u03c0(a|s, z), where z is a latent variable. In one class of methods (Hausman et al., 2018; Gupta et al., 2018), these variables are held constant over the duration of the episode, providing for a time-correlated exploration signal that can enable a single policy to capture multiple skills and rapidly explore plausible behaviors for new tasks by searching in the space of values for z. In another class of methods (Haarnoja et al., 2017, 2018a), the latent variable z is selected independently at each time step, and the policy \u03c0(a|s, z) has some simple unimodal form (e.g., a Gaussian distribution) conditioned on z, but a complex multimodal form when z is integrated out. This enables the policy to represent very complex mulitmodal distributions, which can be useful, for example, for capturing the true maximum entropy distribution for an underspeci\ufb01ed reward function (e.g., run in all possible directions). It also makes it possible to learn a higher-level policy that uses z as its action space (Haarnoja et al., 2018a), effectively driving the lower level policy and using it as a distribution over skills. This leads to a natural probabilistic hierarchical reinforcement learning formulation. 5.4 Modeling, Intent Inference, and Forecasting Aside from devising more effective reinforcement learning and optimal control algorithms, maximum entropy reinforcement learning has also been used extensively in the inverse reinforcement learning setting, where the goal is to infer intent, acquire reward functions from data, and predict the behavior of agents (e.g., humans) in the world from observation. Indeed, the use of the term \u201cmaximum entropy reinforcement learning\u201d in this article is based on the work of Ziebart and colleagues, who proposed the maximum entropy inverse reinforcement learning algorithm (Ziebart et al., 2008) for inferring reward functions and modeling human behavior. While maximum entropy reinforcement learning corresponds to inference in the graphical model over the variables st, at, and Ot, inverse reinforcement learning corresponds to a learning problem, where the goal is to learn the CPD p(Ot|st, at), given example sequences {s1:T,i, a1:T,i, O1:T,i}, where Ot is always true, indicating that the data consists of demonstrations of optimal trajectories. As with all graphical model learning problems, inference takes place in the inner loop of an iterative learning procedure. Exact inference via dynamic programming results in an algorithm where, at each iteration, we solve for the optimal soft value function, compute the corresponding policy, and then use this policy to compute the gradient of the likelihood of the data with respect to the parameters of the CPD p(Ot|st, at). For example, if we use a linear reward representation, such that p(Ot|st, at) = exp(\u03c6T f(st, at)), the learning problem can be expressed as \u03c6\u22c6 = arg max \u03c6 \ufffd i \ufffd t log p(at,i|st,i, O1:T , \u03c6), where computing log p(at,i|st,i, O1:T , \u03c6) and its gradient requires solving for the optimal policy under the current reward parameters \u03c6. The same optimism issue discussed in Section 2 occurs in the inverse reinforcement learning setting, where exact inference in the graphical model produces an \u201coptimistic\u201d policy that assumes some degree of control over the system dynamics. For this reason, Ziebart and colleagues proposed the maximum causal entropy framework for inverse reinforcement learning under stochastic dynamics (Ziebart et al., 2010). Although this framework is derived starting from a causal reformulation of the maximum entropy principle, the resulting algorithm is exactly identical to the variational inference algorithm presented in Section 3, and the corresponding learning procedure corresponds to optimizing the variational lower bound with respect to the reward parameters \u03c6. Subsequent work in inverse reinforcement learning has studied settings where the reward function has a more complex, non-linear representation (Levine et al., 2011; Wulfmeier et al., 2015), and extensions to approximate inference via the Laplace approximation (under known dynamics) (Levine and Koltun, 2012; Dragan et al., 2013) and approximate reinforcement learning (under unknown dynamics) (Finn et al., 2016b; Fu et al., 2018). Aside from inferring reward functions from demonstrations for the purpose of imitation learning, prior work has also sought to leverage the framework of maximum entropy inverse reinforcement learning for inferring the intent of humans, for applications such as robotic assistance (Dragan et al., 2013), brain-computer interfaces (Javdani et al., 2015), and forecasting of human behavior (Huang and Kitani, 2014; Huang et al., 2015). 17 Recent work has also drawn connections between generative adversarial networks (GANs) (Goodfellow et al., 2014) and maximum entropy inverse reinforcement learning (Finn et al., 2016b,a; Fu et al., 2018; Ho and Ermon, 2016). This connection is quite natural since, just like generative adversarial networks, the graphical model in the maximum entropy reinforcement learning framework is a generative model, in this case of trajectories. GANs avoid the need for explicit estimation of the partition function by noting that, given a model \u02c6p(x) for some true distribution p(x), the optimal classi\ufb01er for discriminating whether a sample x came from the model or from the data corresponds to the odds ratio D(x) = p(x) p(x) + \u02c6p(x). Although \u02c6p(x) is unknown, \ufb01tting this \u201cdiscriminator\u201d and using its gradients with respect to x to modify p(x) allows for effective training of the generative model. In the inverse reinforcement learning setting, the discriminator takes the form of the reward function. The reward function is learned so as to maximize the reward of the demonstration data and minimize the reward of samples from the current policy, while the policy is updated via the maximum entropy objective to maximize the expectation of the reward and maximize entropy. As discussed by Finn et al., this process corresponds to a generative adversarial network over trajectories, and also corresponds exactly to maximum entropy inverse reinforcement learning (Finn et al., 2016a). A recent extension of this framework also provides for an effective inverse reinforcement learning algorithm in a model-free deep RL context, as well as a mechanism for recovering robust and transferable rewards in ambiguous settings (Fu et al., 2018). A simpli\ufb01cation on this setup known as generative adversarial imitation learning (GAIL) (Ho and Ermon, 2016) dispenses with the goal of recovering reward functions, and simply aims to clone the demonstrated policy. In this setup, the algorithm learns the advantage function directly, rather than the reward, which corresponds roughly to an adversarial version of the OptV algorithm (Dvijotham and Todorov, 2010). A number of prior works have also sought to incorporate probabilistic inference into a model of biological decision making and control (Solway and Botvinick, 2012; Botvinick and An, 2009; Botvinick and Toussaint, 2012; Friston, 2009). The particular frameworks employed in these approaches differ: the formulation proposed by Friston (2009) is similar to the maximum entropy approach outlined in this survey, and also employs the formalism of approximate variational inference. The formulation described by Botvinick and Toussaint (2012) does not use the exponential reward transformation, and corresponds more closely to the \u201cpseudo-likelihood\u201d formulation outlined in Section 5.2. 6 Perspectives and Future Directions In this article, we discussed how the maximization of a reward function in Markov decision process can be formulated as an inference problem in a particular graphical model, and how a set of update equations similar to the well-known value function dynamic programming solution can be recovered as the direct consequence of applying structured variational inference to this graphical model. The classical maximum expected reward formulation emerges as a limiting case of this framework, while the general case corresponds to a maximum entropy variant of reinforcement learning or optimal control, where the optimal policy not only aims to maximize the expected reward, but also aims to maintain high entropy. The framework of maximum entropy reinforcement learning has already been employed in a range of contexts, as discussed in the previous section, from devising more effective and powerful forward reinforcement learning algorithms, to developing probabilistic algorithms for modeling and reasoning about observed goal-driven behavior. A particularly exciting recent development is the intersection of maximum entropy reinforcement learning and latent variable models, where the graphical model for control as inference is augmented with additional variables for modeling timecorrelated stochasticity for exploration (Hausman et al., 2018; Gupta et al., 2018) or higher-level control through learned latent action spaces (Haarnoja et al., 2017, 2018a). The extensibility and compositionality of graphical models can likely be leveraged to produce more sophisticated reinforcement learning methods, and the framework of probabilistic inference can offer a powerful toolkit for deriving effective and convergent learning algorithms for the corresponding models. 18 ",
    "References": "References Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. (2018). Maximum a posteriori policy optimisation. In International Conference on Learning Representations (ICLR). Attias, H. (2003). Planning by probabilistic inference. In Proceedings of the 9th International Workshop on Arti\ufb01cial Intelligence and Statistics. Bagnell, J. A. and Schneider, J. (2003). Covariant policy search. In International Joint Conference on Arti\ufb01cal Intelligence (IJCAI). Botvinick, M. and An, J. (2009). Goal-directed decision making in prefrontal cortex: a computational framework. In Advances in Neural Information Processing Systems (NIPS). Botvinick, M. and Toussaint, M. (2012). Planning as inference. Trends in Cognitive Sciences, 16(10):485\u2013488. Dragan, A. D., Lee, K. C. T., and Srinivasa, S. S. (2013). Legibility and predictability of robot motion. In International Conference on Human-Robot Interaction (HRI). Dvijotham, K. and Todorov, E. (2010). Inverse optimal control with linearly-solvable mdps. In International Conference on International Conference on Machine Learning (ICML). Finn, C., Christiano, P., Abbeel, P., and Levine, S. (2016a). A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. CoRR, abs/1611.03852. Finn, C., Levine, S., and Abbeel, P. (2016b). Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning (ICML). 19 Friston, K. (2009). The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences, 13(7):293\u2013301. Fu, J., Luo, K., and Levine, S. (2018). Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations (ICLR). Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Neural Information Processing Systems (NIPS). Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. (2018). Meta-reinforcement learning of structured exploration strategies. CoRR, abs/1802.07245. Haarnjoa, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S. (2018). Composable deep reinforcement learning for robotic manipulation. In International Conference on Robotics and Automation (ICRA). Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a). Latent space policies for hierarchical reinforcement learning. CoRR, abs/1804.02808. Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energybased policies. In International Conference on Machine Learning (ICML). Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018b). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In arXiv. Hachiya, H., Peters, J., and Sugiyama, M. (2009). Ef\ufb01cient sample reuse in em-based policy search. In European Conference on Machine Learning (ECML). Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. (2018). Learning an embedding space for transferable robot skills. In International Conference on Learning Representations (ICLR). Heess, N., Silver, D., and Teh, Y. W. (2013). Actor-critic reinforcement learning with energy-based policies. In European Workshop on Reinforcement Learning (EWRL). Ho, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Neural Information Processing Systems (NIPS). Huang, D., Farahmand, A., Kitani, K. M., and Bagnell, J. A. (2015). Approximate MaxEnt inverse optimal control and its application for mental simulation of human interactions. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI). Huang, D. and Kitani, K. M. (2014). Action-reaction: Forecasting the dynamics of human interaction. In European Conference on Computer Vision (ECCV). Javdani, S., Srinivasa, S., and Bagnell, J. A. (2015). Shared autonomy via hindsight optimization. In Robotics: Science and Systems (RSS). Kaelbling, L. P., Littman, M. L., and Moore, A. P. (1996). Reinforcement learning: A survey. Journal of Arti\ufb01cial Intelligence Research, 4:237\u2013285. Kalman, R. (1960). A new approach to linear \ufb01ltering and prediction problems. ASME Transactions journal of basic engineering, 82(1):35\u201345. Kappen, H. J. (2011). Optimal control theory and the linear bellman equation. Inference and Learning in Dynamic Models, pages 363\u2013387. Kappen, H. J., G\u00f3mez, V., and Opper, M. (2012). Optimal control as a graphical model inference problem. Machine Learning, 87(2):159\u2013182. Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. The MIT Press. 20 Levine, S. (2014). Motor skill learning with local trajectory methods. PhD thesis, Stanford University. Levine, S. and Abbeel, P. (2014). Learning neural network policies with guided policy search under unknown dynamics. In Neural Information Processing Systems (NIPS). Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(1). Levine, S. and Koltun, V. (2012). Continuous inverse optimal control with locally optimal examples. In International Conference on Machine Learning (ICML). Levine, S. and Koltun, V. (2013a). Guided policy search. In International Conference on International Conference on Machine Learning (ICML). Levine, S. and Koltun, V. (2013b). Variational policy search via trajectory optimization. In Advances in Neural Information Processing Systems (NIPS). Levine, S. and Koltun, V. (2014). Learning complex neural network policies with trajectory optimization. In International Conference on Machine Learning (ICML). Levine, S., Popovi\u00b4c, Z., and Koltun, V. (2011). Nonlinear inverse reinforcement learning with gaussian processes. In Neural Information Processing Systems (NIPS). Minka, T. P. (2001). Expectation propagation for approximate bayesian inference. In Uncertainty in Arti\ufb01cial Intelligence (UAI). Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017a). Bridging the gap between value and policy based reinforcement learning. In arXiv. Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017b). Trust-pcl: An off-policy trust region method for continuous control. CoRR, abs/1707.01891. Neumann, G. (2011). Variational inference for policy search in changing situations. In International Conference on Machine Learning (ICML). O\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2017). Pgq: Combining policy gradient and q-learning. In International Conference on Learning Representations (ICLR). Peters, J., M\u00fclling, K., and Alt\u00fcn, Y. (2010). Relative entropy policy search. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI). Peters, J. and Schaal, S. (2007). Reinforcement learning by reward-weighted regression for operational space control. In International Conference on Machine Learning (ICML). Rawlik, K., Toussaint, M., and Vijayakumar, S. (2013). On stochastic optimal control and reinforcement learning by approximate inference. In Robotics: Science and Systems (RSS). Sallans, B. and Hinton, G. E. (2004). Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5. Schulman, J., Chen, X., and Abbeel, P. (2017). Equivalence between policy gradients and soft q-learning. In arXiv. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy optimization. In International Conference on Machine Learning (ICML). Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations (ICLR). Solway, A. and Botvinick, M. (2012). Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates. Psychol Rev., 119(1):120\u2013154. Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In International Conference on Machine Learning (ICML). 21 Theodorou, E. A., Buchli, J., and Schaal, S. (2010). Learning policy improvements with path integrals. In International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS 2010). Todorov, E. (2006). Linearly-solvable markov decision problems. In Advances in Neural Information Processing Systems (NIPS). Todorov, E. (2008). General duality between optimal control and estimation. In Conference on Decision and Control (CDC). Todorov, E. (2010). Policy gradients in linearly-solvable mdps. In Neural Information Processing Systems (NIPS). Toussaint, M. (2009). Robot trajectory optimization using approximate inference. In International Conference on Machine Learning (ICML). Toussaint, M. and Storkey, A. (2006). Probabilistic inference for solving discrete and continuous state markov decision processes. In International Conference on Machine Learning (ICML). Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229\u2013256. Williams, R. J. and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268. Wulfmeier, M., Ondruska, P., and Posner, I. (2015). Maximum entropy deep inverse reinforcement learning. In Neural Information Processing Systems Conference, Deep Reinforcement Learning Workshop. Ziebart, B. (2010). Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD thesis, Carnegie Mellon University. Ziebart, B. D., Bagnell, J. A., and Dey, A. K. (2010). Modeling interaction via the principle of maximum causal entropy. In International Conference on Machine Learning (ICML). Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum entropy inverse reinforcement learning. In International Conference on Arti\ufb01cial Intelligence (AAAI). 22 ",
    "title": "Reinforcement Learning and Control as Probabilistic",
    "paper_info": "arXiv:1805.00909v3  [cs.LG]  20 May 2018\nReinforcement Learning and Control as Probabilistic\nInference: Tutorial and Review\nSergey Levine\nUC Berkeley\nsvlevine@eecs.berkeley.edu\nAbstract\nThe framework of reinforcement learning or optimal control provides a mathe-\nmatical formalization of intelligent decision making that is powerful and broadly\napplicable. While the general form of the reinforcement learning problem enables\neffective reasoning about uncertainty, the connection between reinforcement learn-\ning and inference in probabilistic models is not immediately obvious. However,\nsuch a connection has considerable value when it comes to algorithm design: for-\nmalizing a problem as probabilistic inference in principle allows us to bring to\nbear a wide array of approximate inference tools, extend the model in \ufb02exible and\npowerful ways, and reason about compositionality and partial observability. In\nthis article, we will discuss how a generalization of the reinforcement learning\nor optimal control problem, which is sometimes termed maximum entropy rein-\nforcement learning, is equivalent to exact probabilistic inference in the case of\ndeterministic dynamics, and variational inference in the case of stochastic dynam-\nics. We will present a detailed derivation of this framework, overview prior work\nthat has drawn on this and related ideas to propose new reinforcement learning\nand control algorithms, and describe perspectives on future research.\n1\nIntroduction\nProbabilistic graphical models (PGMs) offer a broadly applicable and useful toolbox for the machine\nlearning researcher (Koller and Friedman, 2009): by couching the entirety of the learning problem\nin the parlance of probability theory, they provide a consistent and \ufb02exible framework to devise prin-\ncipled objectives, set up models that re\ufb02ect the causal structure in the world, and allow a common\nset of inference methods to be deployed against a broad range of problem domains. Indeed, if a\nparticular learning problem can be set up as a probabilistic graphical model, this can often serve as\nthe \ufb01rst and most important step to solving it. Crucially, in the framework of PGMs, it is suf\ufb01cient\nto write down the model and pose the question, and the objectives for learning and inference emerge\nautomatically.\nConventionally, decision making problems formalized as reinforcement learning or optimal control\nhave been cast into a framework that aims to generalize probabilistic models by augmenting them\nwith utilities or rewards, where the reward function is viewed as an extrinsic signal. In this view,\ndetermining an optimal course of action (a plan) or an optimal decision-making strategy (a policy)\nis a fundamentally distinct type of problem than probabilistic inference, although the underlying\ndynamical system might still be described by a probabilistic graphical model. In this article, we\ninstead derive an alterate view of decision making, reinforcement learning, and optimal control,\nwhere the decision making problem is simply an inference problem in a particular type of graphical\nmodel. Formalizing decision making as inference in probabilistic graphical models can in principle\nallow us to to bring to bear a wide array of approximate inference tools, extend the model in \ufb02exible\nand powerful ways, and reason about compositionality and partial observability.\nSpeci\ufb01cally, we will discuss how a generalization of the reinforcement learning or optimal control\nproblem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to ex-\nact probabilistic inference in the case of deterministic dynamics, and variational inference in the\ncase of stochastic dynamics. This observation is not a new one, and the connection between proba-\nbilistic inference and control has been explored in the literature under a variety of names, including\n",
    "GPTsummary": "                    - (1): The article discusses how to embed reinforcement learning and control into the framework of probabilistic graphical models using probabilistic inference. In this way, one can use a wide array of approximate inference tools, extend the model flexibly, and reason about compositionality and partial observability. \n\n                    - (2): The conventional way to cast reinforcement learning and control as decision-making problems is distinct from probabilistic inference, although the underlying dynamical system might still be described by a probabilistic graphical model. Prior research that connects probabilistic inference and control has been explored under several names, including the Kalman duality, maximum entropy reinforcement learning, KL-divergence control, and stochastic optimal control. The connection allows us to view reinforcement learning as a PGM and deploy tools such as an entropy-based exploration strategy, inverse reinforcement learning, and approximate inference. \n\n                    - (3): The proposed methodology involves deriving the framework for a maximum entropy generalization of control or reinforcement learning, embedding control into the PGM framework, and performing inference via exact probabilistic inference in the case of deterministic dynamics and structured variational inference in the case of stochastic dynamics. \n\n                    - (4): The paper surveys and reviews recent reinforcement learning literature that connects control and probabilistic inference and offers perspectives on future research directions. The proposed methodology is applicable to various dynamic control tasks and achieves good performance, making it a promising research direction.\n\n\n7. Methods:\n\n- (1): The methodological idea in this article is to embed reinforcement learning and control into the framework of probabilistic graphical models using probabilistic inference. By doing so, one can use a wide array of approximate inference tools, extend the model flexibly, and reason about compositionality and partial observability.\n\n- (2): The proposed approach involves deriving the framework for a maximum entropy generalization of control or reinforcement learning, embedding control into the PGM framework, and performing inference via exact probabilistic inference in the case of deterministic dynamics and structured variational inference in the case of stochastic dynamics.\n\n- (3): The article surveys and reviews recent reinforcement learning literature that connects control and probabilistic inference and offers perspectives on future research directions. The proposed methodology is applicable to various dynamic control tasks and achieves good performance, making it a promising research direction.\n\n- (4): The paper discusses various reinforcement learning algorithms that have been proposed in different frameworks and with different assumptions, all of which are either special cases or generalizations of the control as inference framework presented in the article. The article also discusses related approaches, including Boltzmann exploration, entropy regularization, variational policy search, and KL-Divergence constraints for policy search.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this article lies in its proposal to embed reinforcement learning and control into a probabilistic graphical model framework using probabilistic inference. This approach enables the use of approximate inference tools and flexible model extension, allowing for reasoning about compositionality and partial observability. The article also surveys relevant literature, highlighting future research directions.\n\n- (2): In terms of innovation, the article proposes a novel approach that connects reinforcement learning and control to probabilistic inference. It achieves good performance in various dynamic control tasks, making it a promising research direction. However, the workload required for learning the underlying model can be high, and the approach may not be suitable for all types of applications. Additionally, the article could have provided more detailed comparisons with other related approaches.\n\n\n",
    "GPTmethods": "- (1): The methodological idea in this article is to embed reinforcement learning and control into the framework of probabilistic graphical models using probabilistic inference. By doing so, one can use a wide array of approximate inference tools, extend the model flexibly, and reason about compositionality and partial observability.\n\n- (2): The proposed approach involves deriving the framework for a maximum entropy generalization of control or reinforcement learning, embedding control into the PGM framework, and performing inference via exact probabilistic inference in the case of deterministic dynamics and structured variational inference in the case of stochastic dynamics.\n\n- (3): The article surveys and reviews recent reinforcement learning literature that connects control and probabilistic inference and offers perspectives on future research directions. The proposed methodology is applicable to various dynamic control tasks and achieves good performance, making it a promising research direction.\n\n- (4): The paper discusses various reinforcement learning algorithms that have been proposed in different frameworks and with different assumptions, all of which are either special cases or generalizations of the control as inference framework presented in the article. The article also discusses related approaches, including Boltzmann exploration, entropy regularization, variational policy search, and KL-Divergence constraints for policy search.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this article lies in its proposal to embed reinforcement learning and control into a probabilistic graphical model framework using probabilistic inference. This approach enables the use of approximate inference tools and flexible model extension, allowing for reasoning about compositionality and partial observability. The article also surveys relevant literature, highlighting future research directions.\n\n- (2): In terms of innovation, the article proposes a novel approach that connects reinforcement learning and control to probabilistic inference. It achieves good performance in various dynamic control tasks, making it a promising research direction. However, the workload required for learning the underlying model can be high, and the approach may not be suitable for all types of applications. Additionally, the article could have provided more detailed comparisons with other related approaches.\n\n\n",
    "GPTconclusion": "- (1): The significance of this article lies in its proposal to embed reinforcement learning and control into a probabilistic graphical model framework using probabilistic inference. This approach enables the use of approximate inference tools and flexible model extension, allowing for reasoning about compositionality and partial observability. The article also surveys relevant literature, highlighting future research directions.\n\n- (2): In terms of innovation, the article proposes a novel approach that connects reinforcement learning and control to probabilistic inference. It achieves good performance in various dynamic control tasks, making it a promising research direction. However, the workload required for learning the underlying model can be high, and the approach may not be suitable for all types of applications. Additionally, the article could have provided more detailed comparisons with other related approaches.\n\n\n"
}