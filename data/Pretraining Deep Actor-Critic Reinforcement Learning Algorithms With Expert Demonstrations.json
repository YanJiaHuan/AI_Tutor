{
    "Abstract": "Abstract Pretraining with expert demonstrations have been found useful in speeding up the training process of deep reinforcement learning algorithms since less online simulation data is required. Some people use supervised learning to speed up the process of feature learning, others pretrain the policies by imitating expert demonstrations. However, these methods are unstable and not suitable for actor-critic reinforcement learning algorithms. Also, some existing methods rely on the global optimum assumption, which is not true in most scenarios. In this paper, we employ expert demonstrations in a actor-critic reinforcement learning framework, and meanwhile ensure that the performance is not affected by the fact that expert demonstrations are not global optimal. We theoretically derive a method for computing policy gradients and value estimators with only expert demonstrations. Our method is theoretically plausible for actor-critic reinforcement learning algorithms that pretrains both policy and value functions. We apply our method to two of the typical actor-critic reinforcement learning algorithms, DDPG and ACER, and demonstrate with experiments that our method not only outperforms the RL algorithms without pretraining process, but also is more simulation ef\ufb01cient. 1 ",
    "Introduction": "Introduction Deep reinforcement learning is a general method that have been successful in solving complex control problems. Mnih et al. in [Mnih et al., 2015] combined Q learning with deep neural networks and proved to be successful in image based Atari games. Policy gradient methods have been proved signi\ufb01cantly ef\ufb01cient in both continuous control problems ([Sutton et al., 1999], [Silver et al., 2014], [Heess et al., 2015]) and discrete control problems ([Silver et al., 2016], [Wang et al., 2016]). Among policy gradient methods, actor-critic algorithms are at the heart of many signi\ufb01cant advances in reinforcement learning ([Bhatnagar et al., 2009], [Degris et al., 2012], [Lillicrap et al., 2015], [Mnih et al., 2016]). These algorithms estimate state-action value functions independently, and proved to be ef\ufb01cient in policy optimization. However, an enormous number of online simulation data is required for deep reinforcement learning. Hence we attempt to learn from expert demonstrations and decrease the amount of online data required in deep reinforcement learning algorithms. One of the representative method of learning from expert demonstrations is inverse reinforcement learning. Ng et al. proposed the \ufb01rst inverse reinforcement learning algorithm [Ng and Russell, 2000], which recovers reward function based on the assumption that the expert policy is the global optimal policy. From recovered reward function, Abbeel et al. are able to propose apprenticeship learning ([Abbeel and Ng, 2004]) to train a policy with expert demonstrations and a simulation environment that does not output reward. Apprenticeship learning inspired many similar algorithms ([Syed and Schapire, 2008], [Syed et al., 2008], [Piot et al., 2014], [Ho et al., 2016]), Ho et al. [Ho and Ermon, 2016a] proposed a imitation learning method that merges inverse reinforcement learning and reinforcement learning, hence imitate the expert demonstrations with generative adversarial networks (GANs). These algorithms proved successful in solving MDP\\R ([Abbeel and Ng, 2004]). However, MDP\\R is different from original MDP since MDP\\R environments do not output task based reward data. And for this reason, inverse reinforcement based algorithms attempt to assume the expert demonstrations to be global optimal and imitate the expert demonstrations. In order to learn from expert demonstrations for MDP, alongside with state-of-the-art reinforcement learning algorithms, different frameworks are required. There are some prior work that attempt to make use of expert demonstrations for reinforcement learning algorithms. Lakshminarayanan et al. [Lakshminarayanan et al., 2016] proposed a training method for DQN based on the assumption that expert demonstrations are global optimal, thus pretrain the state-action value function estimators. Cruz Jr et al. [de la Cruz et al., 2017] focused on feature extracting for high dimensional, especially image based simulation environments, and proposed a framework for discrete control problems that pretrains the neural networks with classi\ufb01cation tasks using supervised learning. The purpose of this pretraining process is to speed up the training process by tryarXiv:1801.10459v2  [cs.AI]  9 Feb 2018 ing to extract features of high dimensional states. However, this work is only suitable for image based, discrete action environments, and ignored the fact that expert demonstrations perform better than current learned policies. The \ufb01rst published version of AlphaGo [Silver et al., 2016] is one of the most important work that pretrains the neural networks with human expert demonstrations. In this work, a policy network and a value network is used. The value network is trained with on-policy reinforcement learning, and the policy network is pretrained with expert demonstrations using supervised learning, then trained with policy gradient. This work and [de la Cruz et al., 2017] are quite similar, the role of expert demonstrations is to speed up the feature extraction, and to give policy a warm start. The fact that expert demonstrations perform better is not fully used, and the framework is not extensive enough for other problems and other reinforcement learning algorithms. In this paper, we propose an extensive framework that pretrains actor-critic reinforcement learning algorithms with expert demonstrations, and use expert demonstrations for both policy functions and value estimators. We theoretically derive a method for computing policy gradient and value estimators with only expert demonstrations. Experiments show that our method improves the performance of baseline algorithms on both continuous control environments and high-dimensionalstate discrete control environments. 2 Background and Preliminaries In this paper, we deal with an in\ufb01nite-horizon discounted Markov Decision Process (MDP), which is de\ufb01ned by the tuple {S, A, P, r, \u03c10, \u03b3}. In the tuple, S is a \ufb01nite set of states, A is a \ufb01nite set of actions, P : S \u00d7 A \u00d7 S \u2192 R is the transition probability distribution, r : S \u2192 R is the reward function, \u03c10 : S \u2192 R is the probability distribution of initial state S0, and \u03b3 \u2208 (0, 1) is the discount factor. A stochastic policy \u03c0s : S \u00d7 A \u2192 R returns the probability distribution of actions based on states, and a deterministic policy \u03c0d : S \u2192 A returns the action based on states. In this paper, we deal with both stochastic policies and deterministic policies, and a \u223c \u03c0(s) means a \u223c \u03c0s(a|s) or a = \u03c0d(s) respectively. Thus the state-action value function Q\u03c0is: Q\u03c0(st, at) = Est+1,at+1,... \ufffd \u221e \ufffd \u03c4=0 \u03b3\u03c4r(st+\u03c4) \ufffd The de\ufb01nitions of the value function V \u03c0 and the advantage function A\u03c0 are: V \u03c0(st) = Eat,st+1,... \ufffd \u221e \ufffd \u03c4=0 \u03b3\u03c4r(st+\u03c4) \ufffd A\u03c0(st, at) = Q\u03c0(st, at) \u2212 V \u03c0(st) And let \u03b7(\u03c0) denote the discounted reward of \u03c0: \u03b7(\u03c0) = Es0,a0,... \ufffd \u221e \ufffd t=0 \u03b3tr(st) \ufffd For future convenience, let d\u03c0(s) denote the limiting distribution of states: d\u03c0(s) = lim t\u2192\u221e Pr(st = s) where in all of the de\ufb01nitions above: s0 \u223c \u03c10(s0), at \u223c \u03c0(st), st+1 \u223c P(st+1|st, at) The goal of actor-critic reinforcement learning algorithms is to maximize the discounted reward, \u03b7(\u03c0), to obtain the optimal policy, where we use a parameterized policy \u03c0\u03b8. While estimating \u03b7(\u03c0) or \u2207\u03b8\u03b7(\u03c0\u03b8) based on simulated samples, many algorithms use a state-action value estimator Qw, to estimate the state-value function Q\u03c0 for policy function \u03c0\u03b8. One typical deterministic actor-critic algorithm DDPG (Deep Deterministic Policy Gradient) [Lillicrap et al., 2015] uses estimator Qw = \u02c6 Q\u03c0 to estimate the gradient of an off-policy deterministic discounted reward \u03b7\u03b2(\u03c0\u03b8) = \ufffd s\u2208S d\u03b2(s)V \u03c0(s) [Degris et al., 2012], where \u03b2 is the rollout policy: \u2207\u03b8\u03b7\u03b2(\u03c0) \u2248 Est\u223cd\u03b2(s) \ufffd \u2207\u03b8Qw(s, a)|s=st,a\u223c\u03c0\u03b8(st) \ufffd = Est\u223cd\u03b2(s) \ufffd \u2207aQw(s, a)|s=st,a\u223c\u03c0\u03b8(st)\u03c0\u2032 \u03b8 \ufffd Where Qw is updated with sampled data from \u03c0 using Bellman equation, \u03c0\u2032 \u03b8 = \u2207\u03b8\u03c0\u03b8(s)|s=st. Another off-policy algorithm that has Qw as an estimator of policy \u03c0\u03b8 is ACER (Actor-Critic with Experience Replay) [Wang et al., 2016] that optimizes stochastic policy. The algorithm maximizes off-policy deterministic discounted reward \u03b7\u03b2(\u03c0\u03b8) as well, and modi\ufb01es the off-policy policy gradient \u02c6gacer = \u2207\u03b8\u03b7\u03b2(\u03c0) to: \u02c6gacer = \u00af\u03c1t\u2207\u03b8 log \u03c0\u03b8(at|st) \ufffd Qret(st, at) \u2212 V w(st) \ufffd + Ea\u223c\u03c0\u03b8(st) \ufffd\ufffd\u03c1t(a) \u2212 c \u03c1t(a) \ufffd + \u2207\u03b8 log \u03c0\u03b8(a|st)Aw(st, a) \ufffd Where Aw(st, a) = Qw(st, a) \u2212 V w(st), \u00af\u03c1t = min \ufffd c, \u03c0(at,st) \u03b2(at,st) \ufffd ; [x]+ = x if x > 0 and is zero otherwise; V w(st) = Ea\u223c\u03c0\u03b8(st)(st, a); \u03c1t(a) = \u03c0(a,st) \u03b2(a,st); st \u223c d\u03b2(st) and at \u223c \u03b2(st); Qret is the Retrace estimator of Q\u03c0 [Munos et al., 2016], which can be expressed recursively as follows: Qret(st, at) = rt + \u03b3\u00af\u03c1t+1\u03b4Q(st+1, at+1) + \u03b3V w(s+1) where \u03b4Q(st+1, at+1) = Qret(st+1, at+1) \u2212 Qw(st+1, at+1) In ACER, state-action value function is updated using Qret as target, with gradient gQ: gQ = (Qret(st, at) \u2212 Qw(st, at))\u2207wQw(st, at) In this paper, we will apply our methods with expert demonstrations to DDPG and ACER. ",
    "Methods": "Methods Suppose there exists an expert policy \u03c0\u2217 that performs better than \u03c0. We de\ufb01ne perform better with the following straightforward constraint: \u03b7(\u03c0\u2217) \u2a7e \u03b7(\u03c0) (1) The de\ufb01nition of perform better above is based on the fact that the goal of actor-critic RL algorithms is to maximize \u03b7(\u03c0). Here the expert policy \u03c0\u2217 is different from that of IRL [Ng and Russell, 2000], imitation learning [Ho and Ermon, 2016b] or LfD [Hester et al., 2017], since \u03c0\u2217 here is not the optimum policy of the MDPs. Here we de\ufb01ne a demonstration of a policy \u03c0 as a sequence of (s, a) pairs, {(st, at)}t=0,1,2,..., sampled from \u03c0. Actor-critic RL algorithms tend to optimize \u03b7(\u03c0\u03b8) as the target. Thus pretraining procedures for these algorithms need to estimate \u03b7(\u03c0\u03b8) as the optimization target using expert demonstrations. Also, from de\ufb01nition (1), we need to estimate \u03b7(\u03c0\u2217) as well. However, With only demonstrations of expert policy \u03c0\u2217 and a black-box simulation environment, \u03b7(\u03c0\u2217) and \u03b7(\u03c0\u03b8) cannot be directly estimated. Hence we introduce Theorem 1 (see [Schulman et al., 2015] and [Kakade and Langford, 2002]). Theorem 1. For two policies \u03c0 and \u03c0\u2217: \u03b7(\u03c0\u2217) \u2212 \u03b7(\u03c0) = Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tA\u03c0(s\u2217 t , a\u2217 t ) \ufffd (2) Proof. (See also [Schulman et al., 2015] and [Kakade and Langford, 2002]) Note that A\u03c0(s, a) = Es\u2032\u223cP (s\u2032|s,a) [r(s) + \u03b3V \u03c0(s\u2032) \u2212 V \u03c0(s)] we have: Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tA\u03c0(s\u2217 t , a\u2217 t ) \ufffd =Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3t(r(st) + \u03b3V \u03c0(st+1) \u2212 V \u03c0(st)) \ufffd = \u2212 Es\u2217 0\u223c\u03c10 [V \u03c0(s0)] + Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tr(st) \ufffd = \u2212 \u03b7(\u03c0) + \u03b7(\u03c0\u2217) For many actor-critic RL algorithms like DDPG and ACER, policy optimization is based on accurate estimations of state-action value functions or value functions of the learned policy \u03c0\u03b8. Typically, those algorithms use data sampled from \u03c0\u03b8, {(st, at, rt)}t=0,1,2,..., to estimate Q\u03c0 and V \u03c0. The estimating processes usually need a large amount of simulations to be accurate enough. Combine Theorem 1 with constraint (1), we have: Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tA\u03c0(s\u2217 t , a\u2217 t ) \ufffd \u2a7e 0 (3) This result links state-action value functions with expert demonstrations, allowing us to apply constraint (1) while training state-action value functions. This constraint is for value estimators, like Qw and V w. When value estimators are not accurate enough, constraint (3) would not be satis\ufb01ed. Hence if an algorithm update value estimators under constraint (3), the estimators would be more accurate, and in result improve the policy optimizing process. Another pretraining process is policy optimization using expert demonstrations. Like most actor-critic algorithms, we suppose advantage function A\u03c0(s, a) is already known while conducting policy optimization. Then we can estimate the update step with expert demonstrations and estimations of value functions. Considering Theorem 1, we estimate he policy gradient as the following: \u2207\u03b8\u03b7(\u03c0\u03b8) =\u2207\u03b8(\u03b7(\u03c0\u03b8) \u2212 \u03b7(\u03c0\u2217)) = \u2212 \u2207\u03b8Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tA\u03c0(s\u2217 t , a\u2217 t ) \ufffd (4) Equation (4) provides an off-policy policy optimization procedure with data only from expert demonstrations. It turns out that perform better is not a must in this procedure for expert policy \u03c0\u2217. Recently, people like to propose sample ef\ufb01cient RL algorithms, like ACER and Q-Prop [Gu et al., 2017], since RL algorithms need a large amount of simulation time while training. With expert demonstrations, since there is no reward data, we cannot conduct sample ef\ufb01cient policy optimization processes. However, when we update policies with (4), no simulation time is needed. We call the situation simulation ef\ufb01cient, which means the algorithms may need a large amount of data, but need few simulation data while training. Note that sample ef\ufb01cient algorithms are all simulation ef\ufb01cient algorithms, all of these methods intend to decrease the simulation time. In this paper, we evaluate our method by how simulation ef\ufb01cient it is. In this section, we found two pretraining methods for actorcritic RL algorithms, namely (3) and (4). Both of them are based on Theorem 1. The theorem connects policy discounted reward \u03b7(\u03c0\u03b8) and expert demonstration data, requiring no reward data from expert trajectories. Equation (3) gives a constraint of value function estimators based on the de\ufb01nition of perform better, and equation (4) provides an offpolicy method to optimize policy function regardless of how expert demonstrations perform. 4 Algorithms with Expert Demonstrations Theorem 1 provides a way to satisfy constraint (1) and update policies \u03c0\u03b8 with demonstrations of expert policy \u03c0\u2217, and does not need reward data sampled from \u03c0\u2217. In this section, we organize the results in Section 3 in a more piratical way, then we apply the pretraining methods to two of the typical actorcritic RL algorithms, DDPG and ACER. These actor-critic RL algorithms use neural networks Qw(s, a) to estimate the state-action value functions of policy, Q\u03c0(s, a), where \u03c0 is the is the current learned policy while training, which is a parameterized function, \u03c0\u03b8, always in the form of arti\ufb01cial neural networks. For pretraining processes based on Theorem 1, we need an estimator of advantage function for policy \u03c0\u03b8, A\u03c0(s, a). Based on parameterized policy and state-action value function estimator Qw, we obtain the advantage function estimator Aw,\u03b8: Aw,\u03b8(s\u2217 t , a\u2217 t ) = Qw(s\u2217 t , a\u2217 t ) \u2212 V w,\u03b8(s\u2217 t ) (5) V w,\u03b8(s\u2217 t ) = Ea\u223c\u03c0\u03b8(s)Qw(s\u2217 t , a) (6) Considering the training processes of DDPG and ACER, at the beginning of the processes the policies are nearly random and estimators Qw(s, a) are not accurate, since there is little data from simulation. Therefore if there exist some expert demonstrations that perform better than initial policies, we can introduce the data using constraint (3), in order to obtain a more accurate estimator Qw(s, a). If constraint (3) is satis\ufb01ed, then Qw(s, a) is accurate enough for the fact that \u03c0\u2217 performs better. Hence we update the estimator with expert demonstrations with the following gradient, in which [x]+ = x if x > 0, otherwise is zero: g\u2217 Q = \u2207w \ufffd Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tAw,\u03b8(s\u2217 t , a\u2217 t ) \ufffd\ufffd + (7) From equation (4), we optimize policy with expert demonstrations. Since expert demonstrations do not contain reward data, we can update policy parameters with a simple policy gradient: g\u2217 \u03c0 = \u2212\u2207\u03b8Es\u2217 0,a\u2217 0,...\u223c\u03c0\u2217 \ufffd \u221e \ufffd t=0 \u03b3tAw,\u03b8(s\u2217 t , a\u2217 t ) \ufffd (8) For the reason that \u03c0\u2217 is not the optimal policy of the MDPs, we only train with expert demonstrations for a limited period of time at the beginning of the training process, to guarantee \u03c0\u2217 performs better than \u03c0\u03b8, hence we call the process pretraining. To pretrain actor-critic RL algorithms like DDPG and ACER, we add gradients g\u2217 Q and g\u2217 \u03c0 to the original gradients of the algorithms: gpre Q = gQ + \u03bbQg\u2217 Q (9) gpre \u03c0 = g\u03c0 + \u03bb\u03c0g\u2217 \u03c0 (10) Where gQ and g\u03c0 are original gradients of baseline actorcritic RL algorithms, andgpre Q and gpre \u03c0 are pretraining gradients for estimator Qw and parameterized policy function \u03c0\u03b8 respectively while pretraining. We introduce expert demonstrations to the base algorithms instead of replacing them, since the state-action value functions are estimated with the baseline algorithms and gradient g\u2217 Q only makes Qw satisfy constraint (1). Figure 1: Example screenshots of MuJoCo simulation environments that we attend to experiment on with DDPG as baseline. The tasks are: HalfCheetah (left), Hopper (middle), and Walker2d (right). 4.1 Pretraining DDPG DDPG is a representative off-policy actor-critic deterministic RL algorithm. The algorithm is for continuous action space MDPs, and optimizes the policy using off-policy policy gradient. Two neural networks are used in DDPG at the same time. One is named critic network, which is the state-action value function estimator Qw, and the other is named actor network, which is the parameterized policy \u03c0\u03b8. Since it is an algorithm for deterministic control, the input of the actor network is a state of MDPs, and the output is the corresponding action. Two neural networks are trained simultaneously, with gradients gQ and g\u03c0 respectively. gQ is based on Bellman equation, and g\u03c0 is the off-policy policy gradient. In order to introduce expert demonstrations for pretraining critic network and actor network, we apply (9) and (10) to pretrain the two neural networks. Note that for a deterministic policy \u03c0\u03b8, equation (6) becomes V w,\u03b8(s) = Qw(s, \u03c0\u03b8(s)). 4.2 Pretraining ACER ACER is an off-policy actor-critic stochastic RL algorithm, which modi\ufb01es the policy gradient to make the process sample ef\ufb01cient. ACER solves both discrete control problems and continuous control problems. For discrete control problems, a double-output convolutional neural work (CNN) is used in ACER. One output is a softmax policy \u03c0\u03b8, and the other is Qw values. Although \u03b8 and w share most of the parameters, they are updated separately with different gradients. For stochastic control problems, a new structure named Stochastic Dueling Networks (SDNs) is used for value function estimation. The network outputs a deterministic value estimation V w(s), and a stochastic state-action value estimation Qw,\u03b8(s, a) \u223c V w(s) + Aw(s, a) \u2212 1 n \ufffdn i=1 Aw(s, \u02d9a)|\u02d9a\u223c\u03c0\u03b8. Hence equation (5) becomes Aw,\u03b8(s\u2217 t , a\u2217 t ) = Qw,\u03b8(s\u2217 t , a\u2217 t ) \u2212 V w(s\u2217 t ). In ACER, gradient g\u03c0 is the modi\ufb01ed policy gradient, and gQ is based on Retrace. Both of the gradients are explained in Section 2. Policy gradient is estimated using trust region in ACER, but in this paper, we compute pretraining gradients g\u2217 Q and g\u2217 \u03c0 directly with expert demonstrations. ",
    "Experiments": "Experiments We test our algorithms based on DDPG and ACER on various environments, in order to investigate how simulation ef\ufb01cient the pretraining methods are. The baselines are DDPG and ACER without pretraining. Because of the existence of [x]+, g\u2217 Q de\ufb01ned in (7) could be in\ufb01nity sometimes. Hence we clip the gradient during pretraining. We set \u03bbQ and \u03bb\u03c0 = 1 in equations (9) and (10). The expert policies that generate expert demonstrations are policies trained with baseline algorithms, i.e. DDPG and ACER. With DDPG as baseline, we apply our algorithm to low dimensional simulation environments using the MuJoCo physics engine [Todorov et al., 2012], and test on tasks with action dimensionality are: HalfCheetah (6D), Hopper (3D), and Walker2d (6D). These tasks are illustrated in Figure 1. All the setups with DDPG as baseline share the same network architecture that compute policies and estimate value functions referring to [Lillicrap et al., 2015]. Adam [Kingma and Ba, 2014] is used for learning parameters and the learning rate of actor network and critic network are respectively 10\u22123 and 10\u22124. For critic network, L2 weight decay of 10\u22122 is used with \u03b3 = 0.99. Both actor network and critic network have 2 hidden layers with 400 and 300 units respectively. The results of our pretraining method based on DDPG are illustrated in Figure 2. In the \ufb01gures, the horizontal dashed brown lines represent the average episode reward of expert demonstrations. It is obvious that the expert demonstrations are not global optimal demonstrations, and in order to guarantee the expert policies perform better than learned policies, the pretraining process stops early with 30000 training steps and 60000 simulation steps. As shown in Figure 2, it is obvious that DDPG with our pretraining method outperforms initial DDPG. Results on HalfCheetah (Figure 2 left) is representative and clear, pretraining process gives training a warm start, and after pretraining stops, the performance drops because of the new learning gradient. However, after pretraining, DDPG learns faster than the baseline, hence it outperforms initial DDPG. Although the results of DDPG are unstable on Hopper (Figure 2 middle) and Walker2d (Figure 2 right), smoothed results Figure 3: Example screenshots of Atari simulation environments that we attend to experiment on with ACER as baseline. The tasks from left to right are: AirRaid, Breakout, Carnival, CrazyClimber and Gopher. indicate that DDPG with pretraining processes learns faster than DDPG. With ACER as baseline, we apply our algorithm to image based Atari games. We only tested on discrete control problems with ACER, and the environments we tested on are: AirRaid, Breakout, Carnival, CrazyClimber and Gopher. The environments are illustrated in Figure 3. The experiment settings are similar to [Wang et al., 2016], The double-output network consists of a convolutional layer with 32 8\u00d78 kernels with stride 4, a convolutional layer with 64 4 \u00d7 4 kernels with stride 2, a convolutional layer with 64 3 \u00d7 3 kernels with stride 1, followed by a fully connected layer with 512 units. The network outputs a softmax policy and state-action value Q for every action. Because of the limitation of memory, each thread of ACER only have a replay memory of 5000 frames, which is the only different setting from [Wang et al., 2016]. Entropy regularization with weight 0.001 is also adopted, and the discount factor \u03b3 = 0.99, importance weight truncation c = 10. Trust region updating is used as described in [Wang et al., 2016], and all the settings of trust region update remain the same. ACER without trust region update is not tested in this paper. The results of our pretraining method based on ACER with trust region update is illustrated in Figure 4. All of the environments are image based Atari games. All the lines have the same meaning as Figure 2, and it is obvious that ACER with pretraining process outperforms initial ACER. Unlike DDPG, the performance of learned policies does not fall after pretraining process ends. This is because for ",
    "Conclusion": "Conclusion In this work, we propose an extensive method that pretrains actor-critic reinforcement learning methods. Based on Theorem 1, we design a method that takes advantage of expert demonstrations. Our method does not rely on the global optimal assumption of expert demonstrations, which is one of the key differences between our method and IRL algorithms. Our method pretrains policy function and state-action value estimators simultaneously with gradients (9) and (10). With experiments based on DDPG and ACER, we demonstrate that our method outperforms the raw RL algorithms. One limitation of our framework is that it has to estimate the advantage function for expert demonstrations, and the framework is not suitable for algorithms like A3C [Mnih et al., 2016] and TRPO [Schulman et al., 2015] that only maintain a value estimator V w(s). On the other hand, the fact that expert demonstrations perform better is not considered during pretraining of policies (Equation (8)). We left these extensions in our future work. Acknowledgments This work was supported by National Key R&D Program of China (No. 2016YFB0100901), and National Natural Science Foundation of China (No. 61773231). ",
    "References": "References [Abbeel and Ng, 2004] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-\ufb01rst international conference on Machine learning. ACM, 2004. [Bhatnagar et al., 2009] Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actorcritic algorithms. Automatica, 45(11), 2009. [de la Cruz et al., 2017] Gabriel V. de la Cruz, Jr., Yunshu Du, and Matthew E. Taylor. Pre-training neural networks with human demonstrations for deep reinforcement learning. Technical report, September 2017. [Degris et al., 2012] Thomas Degris, Martha White, and Richard S Sutton. Off-Policy Actor-Critic.pdf. Icml, 2012. [Gu et al., 2017] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. QProp : Sample-Ef\ufb01cient Policy Gradient with An Off Policy Critic. ICLR, 2017. [Heess et al., 2015] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages 2944\u20132952, 2015. [Hester et al., 2017] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, et al. Learning from demonstrations for real world reinforcement learning. arXiv preprint arXiv:1704.03732, 2017. [Ho and Ermon, 2016a] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pages 4565\u2013 4573, 2016. [Ho and Ermon, 2016b] Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In Nips, pages 4565\u20134573, 2016. [Ho et al., 2016] Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy optimization. In International Conference on Machine Learning, pages 2760\u20132769, 2016. [Kakade and Langford, 2002] Sham Kakade and John Langford. Approximately Optimal Approximate Reinforcement Learning. Proceedings of the 19th International Conference on Machine Learning, pages 267\u2013274, 2002. [Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [Lakshminarayanan et al., 2016] Aravind S Lakshminarayanan, Sherjil Ozair, and Yoshua Bengio. Reinforcement Learning with Few Expert Demonstrations. Neural Information Processing Systems - Workshop on Deep Learning for Action and Interaction, 2016. [Lillicrap et al., 2015] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. [Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016. [Munos et al., 2016] R\u00b4emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Ef\ufb01cient Off-Policy Reinforcement Learning. arXiv, (Nips), 2016. [Ng and Russell, 2000] Andrew Ng and Stuart Russell. Algorithms for inverse reinforcement learning. Proceedings of the Seventeenth International Conference on Machine Learning, 0:663\u2013670, 2000. [Piot et al., 2014] Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted bellman residual minimization handling expert demonstrations. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 549\u2013564. Springer, 2014. [Schulman et al., 2015] John Schulman, Sergey Levine, Michael Jordan, and Pieter Abbeel. Trust Region Policy Optimization. Icml-2015, page 16, 2015. [Silver et al., 2014] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 387\u2013395, 2014. [Silver et al., 2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. [Sutton et al., 1999] Richard S. Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. Advances in Neural Information Processing Systems 12, pages 1057\u20131063, 1999. [Syed and Schapire, 2008] Umar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning. In Advances in neural information processing systems, pages 1449\u20131456, 2008. [Syed et al., 2008] Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th international conference on Machine learning, pages 1032\u20131039. ACM, 2008. [Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012. [Wang et al., 2016] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample ef\ufb01cient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016. ",
    "title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms",
    "paper_info": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms\nWith Expert Demonstrations\nXiaoqin Zhang, Huimin Ma\nDept. of EE, Tsinghua University\nxiaoqin-15@mails.tsinghua.edu.cn, mhmpub@tsinghua.edu.cn\nAbstract\nPretraining with expert demonstrations have been\nfound useful in speeding up the training process of\ndeep reinforcement learning algorithms since less\nonline simulation data is required. Some people use\nsupervised learning to speed up the process of fea-\nture learning, others pretrain the policies by imitat-\ning expert demonstrations. However, these meth-\nods are unstable and not suitable for actor-critic\nreinforcement learning algorithms.\nAlso, some\nexisting methods rely on the global optimum as-\nsumption, which is not true in most scenarios. In\nthis paper, we employ expert demonstrations in a\nactor-critic reinforcement learning framework, and\nmeanwhile ensure that the performance is not af-\nfected by the fact that expert demonstrations are not\nglobal optimal. We theoretically derive a method\nfor computing policy gradients and value estima-\ntors with only expert demonstrations. Our method\nis theoretically plausible for actor-critic reinforce-\nment learning algorithms that pretrains both policy\nand value functions. We apply our method to two\nof the typical actor-critic reinforcement learning al-\ngorithms, DDPG and ACER, and demonstrate with\nexperiments that our method not only outperforms\nthe RL algorithms without pretraining process, but\nalso is more simulation ef\ufb01cient.\n1\nIntroduction\nDeep reinforcement learning is a general method that have\nbeen successful in solving complex control problems. Mnih\net al. in [Mnih et al., 2015] combined Q learning with deep\nneural networks and proved to be successful in image based\nAtari games.\nPolicy gradient methods have been proved signi\ufb01cantly ef-\n\ufb01cient in both continuous control problems ([Sutton et al.,\n1999], [Silver et al., 2014], [Heess et al., 2015]) and discrete\ncontrol problems ([Silver et al., 2016], [Wang et al., 2016]).\nAmong policy gradient methods, actor-critic algorithms are at\nthe heart of many signi\ufb01cant advances in reinforcement learn-\ning ([Bhatnagar et al., 2009], [Degris et al., 2012], [Lillicrap\net al., 2015], [Mnih et al., 2016]). These algorithms estimate\nstate-action value functions independently, and proved to be\nef\ufb01cient in policy optimization.\nHowever, an enormous number of online simulation data is\nrequired for deep reinforcement learning. Hence we attempt\nto learn from expert demonstrations and decrease the amount\nof online data required in deep reinforcement learning algo-\nrithms.\nOne of the representative method of learning from ex-\npert demonstrations is inverse reinforcement learning.\nNg\net al. proposed the \ufb01rst inverse reinforcement learning algo-\nrithm [Ng and Russell, 2000], which recovers reward function\nbased on the assumption that the expert policy is the global\noptimal policy. From recovered reward function, Abbeel et\nal. are able to propose apprenticeship learning ([Abbeel and\nNg, 2004]) to train a policy with expert demonstrations and a\nsimulation environment that does not output reward. Appren-\nticeship learning inspired many similar algorithms ([Syed and\nSchapire, 2008], [Syed et al., 2008], [Piot et al., 2014], [Ho\net al., 2016]), Ho et al. [Ho and Ermon, 2016a] proposed\na imitation learning method that merges inverse reinforce-\nment learning and reinforcement learning, hence imitate the\nexpert demonstrations with generative adversarial networks\n(GANs).\nThese algorithms proved successful in solving MDP\\R\n([Abbeel and Ng, 2004]). However, MDP\\R is different from\noriginal MDP since MDP\\R environments do not output task\nbased reward data. And for this reason, inverse reinforce-\nment based algorithms attempt to assume the expert demon-\nstrations to be global optimal and imitate the expert demon-\nstrations. In order to learn from expert demonstrations for\nMDP, alongside with state-of-the-art reinforcement learning\nalgorithms, different frameworks are required.\nThere are some prior work that attempt to make use of ex-\npert demonstrations for reinforcement learning algorithms.\nLakshminarayanan et al. [Lakshminarayanan et al., 2016]\nproposed a training method for DQN based on the assump-\ntion that expert demonstrations are global optimal, thus pre-\ntrain the state-action value function estimators.\nCruz Jr et al. [de la Cruz et al., 2017] focused on feature\nextracting for high dimensional, especially image based sim-\nulation environments, and proposed a framework for discrete\ncontrol problems that pretrains the neural networks with clas-\nsi\ufb01cation tasks using supervised learning. The purpose of this\npretraining process is to speed up the training process by try-\narXiv:1801.10459v2  [cs.AI]  9 Feb 2018\n",
    "GPTsummary": "- (1): This paper focuses on solving the issue of an enormous amount of online simulation data requirement in deep reinforcement learning algorithms by learning from expert demonstrations and pretraining the algorithms.\n \n- (2): The past methods of using expert demonstrations, such as supervised learning and inverse reinforcement learning, have been unstable and unsuitable for actor-critic reinforcement learning algorithms, and some methods rely on global optimum assumption which is not true in most scenarios. Hence, the proposed approach of employing expert demonstrations in a actor-critic reinforcement learning framework while ensuring performance is not affected by the fact that expert demonstrations are not global optimal, is well motivated.\n \n- (3): The proposed approach derives a method for computing policy gradients and value estimators with only expert demonstrations and pretrains both policy and value functions. The method is applied to two of the typical actor-critic reinforcement learning algorithms, DDPG and ACER.\n  \n- (4): The experiments demonstrate that the proposed approach not only outperforms the reinforcement learning algorithms without pretraining but also more simulation efficient. The performance supports the goals of the research methodology.\n \n7. Methods: \n\n- (1): The paper proposes a method for pretraining deep actor-critic reinforcement learning algorithms with expert demonstrations to reduce the amount of online simulation data required in the algorithms.\n \n- (2): The approach derives a way to compute policy gradients and value estimators with only expert demonstrations and pretrains both policy and value functions. This is achieved by using Theorem 1, which links state-action value functions with expert demonstrations, allowing the training of value estimators under the constraint of the de\ufb01nition of perform better.\n \n- (3): Two typical actor-critic reinforcement learning algorithms, DDPG and ACER, are used as examples to apply the proposed approach. Gradients for pretraining critic and actor networks of DDPG and Retrace and modified policy gradient of ACER are computed directly with expert demonstrations. \n \n- (4): The proposed approach is evaluated based on its simulation efficiency compared with reinforcement learning algorithms without pretraining. The experiments demonstrate that the pretraining approach not only outperforms the algorithms without pretraining but is also more simulation efficient, achieving the goals of the research methodology.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work is to propose a method that pretrains deep actor-critic reinforcement learning algorithms with expert demonstrations to reduce the amount of online simulation data required in algorithms. This approach has the potential to improve the efficiency of reinforcement learning in practical applications with expert demonstrations.\n\n- (2): Innovation point: The proposed approach links state-action value functions with expert demonstrations, allowing for the training of value estimators under the constraint of performing better. The approach does not rely on the global optimal assumption of expert demonstrations as traditional methods have, making it suitable for actor-critic reinforcement learning algorithms.\n\nPerformance: The proposed approach has demonstrated better performance than reinforcement learning algorithms without pretraining and is more simulation efficient.\n\nWorkload: The method presented in this article requires the estimation of an advantage function for expert demonstrations and is not suitable for algorithms like A3C and TRPO that only maintain a value estimator V_w(s). However, the proposed approach has the potential to reduce the amount of online simulation data required for reinforcement learning, improving its efficiency in practical applications.\n\n\n",
    "GPTmethods": "- (1): The paper proposes a method for pretraining deep actor-critic reinforcement learning algorithms with expert demonstrations to reduce the amount of online simulation data required in the algorithms.\n \n- (2): The approach derives a way to compute policy gradients and value estimators with only expert demonstrations and pretrains both policy and value functions. This is achieved by using Theorem 1, which links state-action value functions with expert demonstrations, allowing the training of value estimators under the constraint of the de\ufb01nition of perform better.\n \n- (3): Two typical actor-critic reinforcement learning algorithms, DDPG and ACER, are used as examples to apply the proposed approach. Gradients for pretraining critic and actor networks of DDPG and Retrace and modified policy gradient of ACER are computed directly with expert demonstrations. \n \n- (4): The proposed approach is evaluated based on its simulation efficiency compared with reinforcement learning algorithms without pretraining. The experiments demonstrate that the pretraining approach not only outperforms the algorithms without pretraining but is also more simulation efficient, achieving the goals of the research methodology.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work is to propose a method that pretrains deep actor-critic reinforcement learning algorithms with expert demonstrations to reduce the amount of online simulation data required in algorithms. This approach has the potential to improve the efficiency of reinforcement learning in practical applications with expert demonstrations.\n\n- (2): Innovation point: The proposed approach links state-action value functions with expert demonstrations, allowing for the training of value estimators under the constraint of performing better. The approach does not rely on the global optimal assumption of expert demonstrations as traditional methods have, making it suitable for actor-critic reinforcement learning algorithms.\n\nPerformance: The proposed approach has demonstrated better performance than reinforcement learning algorithms without pretraining and is more simulation efficient.\n\nWorkload: The method presented in this article requires the estimation of an advantage function for expert demonstrations and is not suitable for algorithms like A3C and TRPO that only maintain a value estimator V_w(s). However, the proposed approach has the potential to reduce the amount of online simulation data required for reinforcement learning, improving its efficiency in practical applications.\n\n\n",
    "GPTconclusion": "- (1): The significance of this work is to propose a method that pretrains deep actor-critic reinforcement learning algorithms with expert demonstrations to reduce the amount of online simulation data required in algorithms. This approach has the potential to improve the efficiency of reinforcement learning in practical applications with expert demonstrations.\n\n- (2): Innovation point: The proposed approach links state-action value functions with expert demonstrations, allowing for the training of value estimators under the constraint of performing better. The approach does not rely on the global optimal assumption of expert demonstrations as traditional methods have, making it suitable for actor-critic reinforcement learning algorithms.\n\nPerformance: The proposed approach has demonstrated better performance than reinforcement learning algorithms without pretraining and is more simulation efficient.\n\nWorkload: The method presented in this article requires the estimation of an advantage function for expert demonstrations and is not suitable for algorithms like A3C and TRPO that only maintain a value estimator V_w(s). However, the proposed approach has the potential to reduce the amount of online simulation data required for reinforcement learning, improving its efficiency in practical applications.\n\n\n"
}