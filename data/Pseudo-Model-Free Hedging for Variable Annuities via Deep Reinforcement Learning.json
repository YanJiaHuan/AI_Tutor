{
    "Abstract": "Abstract This paper proposes a two-phase deep reinforcement learning approach, for hedging variable annuity contracts with both GMMB and GMDB riders, which can address model miscalibration in Black-Scholes \ufb01nancial and constant force of mortality actuarial market environments. In the training phase, an infant reinforcement learning agent interacts with a pre-designed training environment, collects sequential anchor-hedging reward signals, and gradually learns how to hedge the contracts. As expected, after a su\ufb03cient number of training steps, the trained reinforcement learning agent hedges, in the training environment, equally well as the correct Delta while outperforms misspeci\ufb01ed Deltas. In the online learning phase, the trained reinforcement learning agent interacts with the market environment in real time, collects single terminal reward signals, and self-revises its hedging strategy. The hedging performance of the further trained reinforcement learning agent is demonstrated via an illustrative example on a rolling basis to reveal the self-revision capability on the hedging strategy by online learning. Keywords: Two-phase deep reinforcement learning; Variable annuities hedging; Training phase; Sequential anchor-hedging reward signals; Online learning phase; Single terminal reward signals; Hedging strategy selfrevision. 1 ",
    "Introduction": "Introduction Variable annuities are long-term life products, in which policyholders participate in \ufb01nancial investments for pro\ufb01t sharing with insurers. Various guarantees are embedded in these contracts, such as guaranteed minimum maturity bene\ufb01t (GMMB), guaranteed minimum death bene\ufb01t (GMDB), guaranteed minimum accumulation bene\ufb01t (GMAB), guaranteed minimum income bene\ufb01t (GMIB), and guaranteed minimum withdrawal bene\ufb01t (GMWB). According to the Insurance Information Institute in 2020, the sales of variable annuity contracts in the United States have amounted to, on average, 100.7 billion annually, from 2016 to 2020. Due to their popularity in the market and their dual-risk bearing nature, valuation and risk management of variable annuities have been substantially studied in the literature. By the risk-neutral option pricing approach, to name a few, Milevsky and Posner (2001) studied the valuation of the GMDB rider; valuation and hedging of the GMMB rider under the Black-Scholes (BS) \ufb01nancial market model were covered in Hardy (2003); the GMWB rider was extensively investigated by Milevsky and Salisbury (2006), Dai et al. (2008), and Chen et al. (2008); valuation and hedging of the GMMB rider were studied in Cui et al. (2017) under the Heston \ufb01nancial market model; valuation of the GMMB rider, together with the feature that a contract can be surrendered before its maturity, was examined by Jeon and Kwak (2018), in which optimal surrender strategies were also provided. For a comprehensive review of this approach, see Feng (2018). Valuation and risk management of variable annuities have recently been advanced via various approaches as well. Trottier et al. (2018) studied the hedging of variable annuities in the presence of basis risk based on a \u2217This work was \ufb01rst initiated by the authors at the Illinois Risk Lab in January 2020. This work was presented at the 2020 Actuarial Research Conference in August 2020, the United As One: 24th International Congress on Insurance: Mathematics and Economics in July 2021, the 2021 Actuarial Research Conference in August 2021, Heriot-Watt University in November 2021, University of Amsterdam in June 2022, and the 2022 Insurance Data Science Conference in June 2022. The authors thank the participants for fruitful comments. This work utilizes resources supported by the National Science Foundation\u2019s Major Research Instrumentation program, grant #1725729, as well as the University of Illinois at Urbana-Champaign. The authors are grateful to anonymous reviewers for their careful reading and insightful comments. \u2020Corresponding author. 1 arXiv:2107.03340v3  [q-fin.RM]  1 Oct 2022 local optimization method. Chong (2019) revisited the pricing and hedging problem of equity-linked life insurance contracts utilizing the so-called principle of equivalent forward preferences. Feng and Yi (2019) compared the dynamic hedging approach to the stochastic reserving approach for the risk management of variable annuities. Moenig (2021a) investigated the valuation and hedging problem of a portfolio of variable annuities via a dynamic programming method. Moenig (2021b) explored the impact of market incompleteness on the policyholder\u2019s behavior. Wang and Zou (2021) solved the optimal fee structure for the GMDB and GMMB riders. Dang et al. (2020) and Dang et al. (2022) proposed and analyzed e\ufb03cient simulation methods for measuring the risk of variable annuities. Recently, state-of-the-art machine learning methods have been deployed to revisit the valuation and hedging problems of variable annuities at a portfolio level. Gan (2013) proposed a three-step technique, by (i) selecting representative contracts with clustering method, (ii) pricing these contracts with Monte Carlo (MC) simulation, and (iii) predicting the value of the whole portfolio based on the values of representative contracts with kriging method. To further boost the e\ufb03ciency and the e\ufb00ectiveness of selecting and pricing the representative contracts, as well as valuating the whole portfolio, various methods at each of these three steps have been proposed. For instance, Gan and Lin (2015) extended the ordinary kriging method to the universal kriging method; Hejazi and Jackson (2016) used a neural network as the predictive model to valuate the whole portfolio; Gan and Valdez (2018) implemented the generalized beta of the second kind method instead of the kriging method to capture the non-Gaussian behavior of the market price of variable annuities. See also, Gan (2018), Gan and Valdez (2020), Gweon et al. (2020), Liu and Tan (2020), Lin and Yang (2020), Feng et al. (2020), and Quan et al. (2021) for recent developments in this three-step technique. Similar idea has also been applied to the calculation of Greeks and risk measures of a portfolio of variable annuities; see Gan and Lin (2017), Gan and Valdez (2017), and Xu et al. (2018). All of the above literature applying the machine learning methods involve the supervised learning, which requires a pre-labelled dataset (in this case, it is the set of fair prices of the representative contracts) to train a predictive model. Other than valuating and hedging variable annuities, supervised learning methods have also been applied to di\ufb00erent actuarial contexts. W\u00a8uthrich (2018) used a neural network for the chain-ladder factors in the chain-ladder claim reserving model to include heterogeneous individual claim features. Gao and W\u00a8uthrich (2019) applied a convolutional neural network to classify drivers using their telematics data. Cheridito et al. (2020) estimated the risk measures of a portfolio of assets and liabilities with a feedforward neural network. Richman and W\u00a8uthrich (2021) and Perla et al. (2021) studied the mortality rate forecasting problem, where Richman and W\u00a8uthrich (2021) extended the traditional Lee-Carter model to multiple populations using a neural network, while Perla et al. (2021) applied deep learning techniques directly on a time-series data of mortality rate. Hu et al. (2022) modi\ufb01ed the loss function in tree-based models to improve the predictive performance when applying to imbalanced datasets which are common in the insurance practice. Meanwhile, a \ufb02ourishing sub-\ufb01eld in machine learning, called the reinforcement learning (RL), has been skyrocketing and has proved its powerfulness in various tasks; see Silver et al. (2017), and the references therein. Contrary to the supervised learning, the RL does not require a pre-labelled dataset for training. Instead, in the RL, an agent interacts with an environment, by sequentially observing states, taking, as well as revising, actions, and collecting rewards. Without possessing any prior knowledge of the environment, the agent needs to, explore the environment while exploit the collected reward signals, for learning. For a representative monograph of RL, see Sutton and Barto (2018); for its broad applications in economics, game theory, operations research, and \ufb01nance, see the recent survey paper by Charpentier et al. (2021). The mechanism of RL resembles how a hedging agent hedges any contingent claim dynamically. Indeed, the hedging agent could not know any speci\ufb01cs of the market environment, but could only observe states from the environment, take a hedging strategy, and learn from reward signals to progressively improve the hedging strategy. However, in the context of hedging, if an insurer builds a hedging agent based on a certain RL method, called RL agent hereafter, and allows this infant RL agent to interact and learn from the market environment right away, the insurer could bear enormous \ufb01nancial loss while the infant RL agent is still exploring the environment before it could e\ufb00ectively exploit the reward signals. Moreover, provided that the insurer could not know any speci\ufb01cs of the market environment as well, she could not supply any information derived from theoretical models to the infant RL agent, and thus the agent could only obtain the reward signals via the realized terminal pro\ufb01t and loss, based on the realized net liability and hedging portfolio value; these signals should not be e\ufb00ective for an infant RL agent to learn from the market environment. To resolve these two issues above, we propose a two-phase (deep) RL approach, which is composed of a training phase and an online learning phase. In the training phase, based on her best knowledge of the market, the insurer constructs a training environment. An infant RL agent is then designated to interact and learn from this training environment for a period of time. Comparing to putting the infant RL agent in the market environment right away, the infant RL agent could be supplied by more information derived from the constructed training environment, such as the net liabilities before any terminal times. In this paper, we propose that the RL agent collects anchor-hedging reward signals during the training phase. After the RL agent is experienced with the training environment, in the online learning phase, the insurer \ufb01nally designates the trained RL agent in the market environment. Again, since no theoretical model for the market environment is available to the insurer, the trained RL agent could only collect 2 single terminal reward signals in this phase. In this paper, an illustrative example is provided to demonstrate the hedging performance using this approach. All RL methods can be classi\ufb01ed into either MC or temporal-di\ufb00erence (TD) learning. As a TD method shall be employed in this paper, in both the training and online learning phases, the following RL literature review focuses on the latter method. Sutton (1984) and Sutton (1988) \ufb01rst introduced the TD method for prediction of value function. Based upon their works, Watkins (1989) and Watkins and Dayan (1992) proposed the well-known Q-learning for \ufb01nite state and action spaces. Since then, the Q-learning has been improved substantially, in Hasselt (2010) for the Double Q-learning, and in Mnih et al. (2013), as well as Mnih et al. (2015), for the deep Q-learning which allows in\ufb01nite state space. Any Q-learning approaches, or in general tabular solution methods and value function approximation methods, are only applicable to \ufb01nite action space. However, in the context of hedging, the action space is in\ufb01nite. Instead of discretizing the action space, proximal policy optimization (PPO) by Schulman et al. (2017), which is a policy gradient method, shall be applied in this paper; our Section 3.4 shall provide its self-contained review. To the best of our knowledge, this paper is the \ufb01rst work to implement the RL algorithms with online learning to hedge contingent claims, particularly variable annuities. Contrary to Xu (2020) and Carbonneau (2021), in which both adapted the state-of-the-art DH approach in B\u00a8uhler et al. (2019), this paper is in line with the recent works by Kolm and Ritter (2019) and Cao et al. (2021), while extends with actuarial components. We shall outline the di\ufb00erences between the RL and DH approaches throughout Sections 3 and 4, as well as Appendices A and B. Kolm and Ritter (2019) discretized the action space and implemented RL algorithms for \ufb01nitely many possible actions; however, as mentioned above, this paper does not discretize the action space but adapts the recently advanced policy gradient method, namely, the PPO. Comparing with Cao et al. (2021), in addition to the actuarial elements, this paper puts forward online learning to self-revise the hedging strategy. In the illustrative example, we assume that the market environment is the BS \ufb01nancial and constant force of mortality (CFM) actuarial markets, and the focus is on contracts with both GMMB and GMDB riders. Furthermore, we assume that the model of the market environment being presumed by the insurer, which shall be supplied as the training environment, is also the BS and the CFM, but with a di\ufb00erent set of parameters. That is, while the insurer constructs correct dynamic models of the market environment for the training environment, the parameters in the model of the market environment are not the same as those in the market environment. Section 2.4 shall set the stage of this illustrative example, and shall show that, if the insurer forwardly implements, in the market environment, the incorrect Delta hedging strategy based on her presumed model of the market environment, then its hedging performance for the variable annuities is worse than that by the correct Delta hedging strategy based on the market environment. In Sections 4 and 6, this illustrative example shall be revisited using the two-phase RL approach. As we shall see in Section 6, the hedging performance of the RL agent is even worse than that of the incorrect Delta, at the very beginning of hedging in real time. However, delicate analysis shows that, with a fair amount of future trajectories (which are di\ufb00erent from simulated scenarios, with more details in Section 6), the hedging performance of the RL agent becomes comparable with that of the correct Delta within a reasonable amount of time. Therefore, the illustrative example addresses model miscalibration issue in hedging variable annuity contracts with GMMB and GMDB riders in BS \ufb01nancial and CFM actuarial market environments, which is common in practice. This paper is organized as follows. Section 2 formulates the continuous hedging problem for variable annuities, reformulates it to the discrete and Markov setting, and motivates as well as outlines the two-phase RL approach. Section 3 discusses the RL approach in hedging variable annuities and provides a self-contained review of RL, particularly the PPO, which is a TD policy gradient method, while Section 5 presents the implementation details of the online learning phase. Sections 4 and 6 revisit the illustrative example in the training and online learning phases respectively. Section 7 collates the assumptions of utilizing the two-phase RL approach for hedging contingent claims, as well as their implications in practice. This paper \ufb01nally concludes and comments on future directions in Section 8. 2 Problem Formulation and Motivation 2.1 Classical Hedging Problem and Model-Based Approach We \ufb01rst review the classical hedging problem for variable annuities and its model-based solution to introduce some notations and to motivate the RL approach. 2.1.1 Actuarial and Financial Market Models Let (\u2126, F, P) be a rich enough complete probability space. Consider the current time t = 0 and \ufb01x T > 0 as a deterministic time in the future. Throughout this paper, all time units are in year. There are one risk-free asset and one risky asset in the \ufb01nancial market. Let Bt and St, for t \u2208 [0, T], be the 3 time-t values of the risk-free asset and the risky asset respectively. Let G(1) = \ufffd G(1) t \ufffd t\u2208[0,T ] be the \ufb01ltration which contains all \ufb01nancial market information; in particular, both processes B = {Bt}t\u2208[0,T ] and S = {St}t\u2208[0,T ] are G(1)-adapted. There are N policyholders in the actuarial market. For each policyholder i = 1, 2, . . . , N, denote T (i) xi as her random future lifetime, who is of age xi at the current time 0. De\ufb01ne, for each i = 1, 2, . . . , N, and for any t \u2265 0, J(i) t = 1\ufffd T (i) xi >t \ufffd, be the corresponding time-t jump value generated by the random future lifetime of the i-th policyholder; that is, if the i-th policyholder survives at some time t \u2208 [0, T], J(i) t = 1; otherwise, J(i) t = 0. Let G(2) = \ufffd G(2) t \ufffd t\u2208[0,T ] be the \ufb01ltration which contains all actuarial market information; in particular, all single-jump processes J(i) = \ufffd J(i) t \ufffd t\u2208[0,T ], for i = 1, 2, . . . , N, are G(2)-adapted. Let F = {Ft}t\u2208[0,T ] be the \ufb01ltration which contains all actuarial and \ufb01nancial market information; that is, F = G(1) \u2228 G(2). Therefore, the \ufb01ltered probability space is given by (\u2126, F, F, P). 2.1.2 Variable Annuities with Guaranteed Minimum Maturity Bene\ufb01t and Guaranteed Minimum Death Bene\ufb01t Riders At the current time 0, an insurer writes a variable annuity contract to each of these N policyholders. Each contract is embedded with both GMMB and GMDB riders. Assume that all these N contracts expire at the same \ufb01xed time T. In the following, \ufb01x a generic policyholder i = 1, 2, . . . , N. At the current time 0, the policyholder deposits F (i) 0 into her segregated account to purchase \u03c1(i) > 0 shares of the risky asset; that is, F (i) 0 = \u03c1(i)S0. Assume that the policyholder does not revise the number of shares \u03c1(i) throughout the e\ufb00ective time of the contract. For any t \u2208 \ufffd 0, T (i) xi \u2227 T \ufffd , the time-t segregated account value of the policyholder is given by F (i) t = \u03c1(i)Ste\u2212m(i)t, where m(i) \u2208 (0, 1) is the continuously compounded annualized rate at which the asset-value-based fees are deducted from the segregated account by the insurer. For any t \u2208 \ufffd T (i) xi \u2227 T, T \ufffd , the time-t segregated account value F (i) t must be 0; indeed, if the policyholder dies before the maturity, i.e. T (i) xi < T, then, due to the GMDB rider of a minimum guarantee G(i) D > 0, the bene\ufb01ciary inherits max \ufffd F (i) T (i) xi , G(i) D \ufffd , which can be decomposed into F (i) T (i) xi + \ufffd G(i) D \u2212 F (i) T (i) xi \ufffd + , at the policyholder\u2019s death time T (i) xi right away. Due to the GMMB rider of a minimum guarantee G(i) M > 0, if the policyholder survives beyond the maturity, i.e. T (i) xi > T, the policyholder acquires max \ufffd F (i) T , G(i) M \ufffd at the maturity, which can be decomposed into F (i) T + \ufffd G(i) M \u2212 F (i) T \ufffd +. 2.1.3 Net Liability of Insurer The liability of the insurer thus has two parts. The liability from the GMMB rider at the maturity for the i-th policyholder, where i = 1, 2, . . . , N, is given by \ufffd G(i) M \u2212 F (i) T \ufffd + if the i-th policyholder survives beyond the maturity, and is 0 otherwise. The liability from the GMDB rider at the death time T (i) xi for the i-th policyholder, where i = 1, 2, . . . , N, is given by \ufffd G(i) D \u2212 F (i) T (i) xi \ufffd + if the i-th policyholder dies before the maturity, and is 0 otherwise. Therefore, at any time t \u2208 [0, T], the future gross liability of the insurer accumulated to the maturity for these N contracts is given by N \ufffd i=1 \ufffd\ufffd G(i) M \u2212 F (i) T \ufffd + J(i) T + BT BT (i) xi \ufffd G(i) D \u2212 F (i) T (i) xi \ufffd + 1{T (i) xi <T }J(i) t \ufffd . Denote V GL t , for t \u2208 [0, T], as the time-t value of the discounted (via the risk-free asset B) future gross liability of the insurer; if the liability is 0, the value will be 0. From the asset-value-based fees collected by the insurer, a portion, known as the rider charge, is used to fund the liability due to the GMMB and GMDB riders; the remaining portion is used to cover overhead, commissions, and any other expenses. From the i-th policyholder, where i = 1, 2, . . . , N, the insurer collects m(i) e F (i) t J(i) t as the rider charge at any time t \u2208 [0, T], where m(i) e \u2208 \ufffd 0, m(i)\ufffd . Therefore, the cumulative future rider charge to be collected, from any time t \u2208 [0, T] onward, till the maturity, by the insurer from these N policyholders, is given by \ufffdN i=1 \ufffd T t m(i) e F (i) s J(i) s (BT /Bs) ds. Denote V RC t , for t \u2208 [0, T], as its time-t discounted (via the risk-free asset B) value; if the cumulative rider charge is 0, the value will be 0. 4 Hence, due to these N variable annuity contracts with both GMMB and GMDB riders, for any t \u2208 [0, T], the time-t net liability of the insurer for these N contracts is given by Lt = V GL t \u2212 V RC t , which is Ft-measurable. One of the many ways to set the rate m(i) \u2208 (0, 1) for the asset-value-based fees, and the rate m(i) e \u2208 \ufffd 0, m(i)\ufffd for the rider charge, for i = 1, 2, . . . , N, is based on the time-0 net liability of the insurer for the i-th policyholder. More precisely, m(i) and m(i) e are determined via L(i) 0 = V GL,(i) 0 \u2212 V RC,(i) 0 = 0, where V GL,(i) 0 and V RC,(i) 0 are the time-0 values of, respectively, the discounted future gross liability and the discounted cumulative future rider charge, of the insurer for the i-th policyholder. 2.1.4 Continuous Hedging and Hedging Objective The insurer aims to hedge this dual-risk bearing net liability via investing in the \ufb01nancial market. To this end, let \u02dcT be the death time of the last policyholder; that is, \u02dcT = maxi=1,2,...,N T (i) xi , which is random. While the net liability Lt is de\ufb01ned for any time t \u2208 [0, T], as the di\ufb00erence between the values of discounted future gross liability and discounted cumulative future rider charge, Lt = 0 for any t \u2208 \ufffd \u02dcT \u2227 T, T \ufffd . Indeed, if \u02dcT < T, then, for any t \u2208 \ufffd \u02dcT \u2227 T, T \ufffd , one has T (i) xi < t \u2264 T for all i = 1, 2, . . . , N, and hence, the future gross liability accumulated to the maturity, and the cumulative rider charge from time \u02dcT onward, are both 0, so are their values. Therefore, the insurer only hedges the net liability Lt, for any t \u2208 \ufffd 0, \u02dcT \u2227 T \ufffd . Let Ht be the hedging strategy, i.e. the number of shares of the risky asset being held by the insurer, at time t \u2208 [0, T). Hence, Ht = 0, for any t \u2208 \ufffd \u02dcT \u2227 T, T \ufffd . Let H be the admissible set of hedging strategies, which is de\ufb01ned by H = \ufffd H = {Ht}t\u2208[0,T ) : (i) H is F-adapted, (ii) H \u2208 R, P \u00d7 L-a.s., and (iii) for any t \u2208 \ufffd \u02dcT \u2227 T, T \ufffd , Ht = 0 \ufffd , where L is the Lebesgue measure on R. Let Pt be the time-t value, for t \u2208 [0, T], of the insurer\u2019s hedging portfolio. Then P0 = 0, and together with the rider charges collected from the N policyholders, as well as the withdrawal for paying the liabilities due to the bene\ufb01ciaries\u2019 inheritance from those policyholders who have already been dead, for any t \u2208 (0, T], Pt = \ufffd t 0 (Ps \u2212 HsSs) dBs Bs + \ufffd t 0 HsdSs + N \ufffd i=1 \ufffd t 0 m(i) e F (i) s J(i) s ds \u2212 N \ufffd i=1 \ufffd G(i) D \u2212 F (i) T (i) xi \ufffd + 1{T (i) xi \u2264t<T }, which obviously depends on {Hs}s\u2208[0,t). As in Bertsimas et al. (2000), the insurer\u2019s hedging objective function at the current time 0 should be given by the root-mean-square error (RMSE) of the terminal pro\ufb01t and loss (P&L), which is, for any H \u2208 H, \ufffd EP \ufffd (P \u02dcT \u2227T \u2212 L \u02dcT \u2227T )2\ufffd . If the insurer has full knowledge of the objective probability measure P, and hence the correct dynamics of the riskfree asset and the risky asset in the \ufb01nancial market, as well as the correct mortality model in the actuarial market, the optimal hedging strategy, being implemented forwardly, is given by minimizing the RMSE of the terminal P&L: H\u2217 = arg min H\u2208H \ufffd EP \ufffd (P \u02dcT \u2227T \u2212 L \u02dcT \u2227T )2\ufffd . 2.2 Pitfall of Model-Based Approach However, having correct model is usually not the case in practice. Indeed, the insurer, who is the hedging agent above, usually has little information regarding the objective probability measure P, and hence easily misspeci\ufb01es the \ufb01nancial market dynamics and the mortality model, which will in turn yield a poor performance from the supposedly optimal hedging strategy when it is implemented forwardly in the future. Section 2.4 outlines such an illustrative example which shall be discussed throughout the remaining of this paper. To rectify this, we propose a two-phase (deep) RL approach to solve an optimal hedging strategy. In this approach, an RL agent, which is not the insurer herself but is built by the insurer to hedge on her behalf, does not have any knowledge of the objective probability measure P, the \ufb01nancial market dynamics, and the mortality model; Section 2.5 shall explain this approach in details. Before that, in the following Section 2.3, the classical hedging problem shall \ufb01rst be reformulated with a Markov decision process (MDP) in a discrete-time setting so that RL methods can be implemented. The illustrative example outlined in Section 2.4 shall be revisited using the proposed two-phase RL approach in Sections 4 and 6. 5 In the remaining of this paper, unless otherwise speci\ufb01ed, all expectation operators shall be taken with respect to the objective probability measure P, and denoted simply as E [\u00b7]. 2.3 Discrete and Markov Hedging 2.3.1 Discrete Hedging and Hedging Objective Let t0, t1, . . . , tn\u22121 \u2208 [0, T), for some n \u2208 N, be the time when the hedging agent decides the hedging strategy, such that 0 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tn\u22121 < T. Denote also tn = T. Let t\u02dcn be the \ufb01rst time (right) after the last policyholder dies or all contracts expire, for some \u02dcn = 1, 2, . . . , n, which is random; that is, t\u02dcn = min \ufffd tk, k = 1, 2, . . . , n : tk \u2265 \u02dcT \ufffd , and when \u02dcT > T, by convention, min \u2205 = tn. Therefore, Ht = 0, for any t = t\u02dcn, t\u02dcn+1, . . . , tn\u22121. With a slight abuse of notation, the admissible set of hedging strategies in discrete time is H = \ufffd H = {Ht}t=t0,t1,...,tn\u22121 : (i) for any t = t0, t1, . . . , tn\u22121, Ht is Ft-measurable, (ii) for any t = t0, t1, . . . , tn\u22121, Ht \u2208 R, P-a.s., and (iii) for any t = t\u02dcn, t\u02dcn+1, . . . , tn\u22121, Ht = 0} . While the hedging agent decides the hedging strategy at the discrete time points, the actuarial and \ufb01nancial market models are continuous. Hence, the net liability Lt = V GL t \u2212 V RC t is still de\ufb01ned for any time t \u2208 [0, T] as before. Moreover, if t \u2208 [tk, tk+1), for some k = 0, 1, . . . , n \u2212 1, Ht = Htk; thus, P0 = 0, and, if t \u2208 (tk, tk+1], for some k = 0, 1, . . . , n \u2212 1, Pt = (Ptk \u2212 HtkStk) Bt Btk + HtkSt + N \ufffd i=1 \ufffd t tk m(i) e F (i) s J(i) s Bt Bs ds \u2212 N \ufffd i=1 Bt BT (i) xi \ufffd G(i) D \u2212 F (i) T (i) xi \ufffd + 1{tk<T (i) xi \u2264t<T }. (1) For any H \u2208 H, the hedging objective of the insurer at the current time 0 is \ufffd E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd . Hence, the optimal discrete hedging strategy, being implemented forwardly, is given by H\u2217 = arg min H\u2208H \ufffd E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd = arg min H\u2208H E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd . (2) 2.3.2 Markov Decision Process An MDP can be characterized by its state space, action space, Markov transition probability, and reward signal. In turn, these derive the value function and the optimal value function, which are equivalently known as, respectively, the objective function and the value function, in optimization as in the previous sections. In the remaining of this paper, we shall adapt the MDP language. \u2022 (State) Let X be the state space in Rp, where p \u2208 N. Each state in the state space represents a possible observation with p features in the actuarial and \ufb01nancial markets. Denote Xtk \u2208 X as the observed state at any time tk, where k = 0, 1, . . . , n; the state should minimally include an information related to the number of surviving policyholders \ufffdN i=1 J(i) tk , and the term to maturity T \u2212 tk, in order to terminate the hedging at time t\u02dcn, which is the \ufb01rst time when \ufffdN i=1 J(i) t\u02dcn = 0, or which is when T \u2212 t\u02dcn = 0. The states (space) shall be speci\ufb01ed in Sections 4 and 5. \u2022 (Action) Let A be the action space in R. Each action in the action space is a possible hedging strategy. Denote Htk (Xtk) \u2208 A as the action at any time tk, where k = 0, 1, . . . , n \u2212 1, which is assumed to be Markovian with respect to the observed state Xtk; that is, given the current state Xtk, the current action Htk (Xtk) is independent of the past states Xt0, Xt1, . . . , Xtk\u22121. In the sequel, for notational simplicity, we simply write Htk to represent Htk (Xtk), for k = 0, 1, . . . , n \u2212 1. If the feature of the number of surviving policyholders \ufffdN i=1 J(i) tk = 0, for k = 0, 1, . . . , n \u2212 1, in the state Xtk, then Htk = 0; in particular, for any tk, where k = \u02dcn, \u02dcn + 1, . . . , n \u2212 1, the hedging strategy Htk = 0. \u2022 (Markov property) At any time tk, where k = 0, 1, . . . , n \u2212 1, given the current state Xtk and the current hedging strategy Htk, the transition probability distribution of the next state Xtk+1 in the market is independent of the past states Xt0, Xt1, . . . , Xtk\u22121 and the past hedging strategies Ht0, Ht1, . . . , Htk\u22121; that is, for any Borel set B \u2208 B (X), P \ufffd Xtk+1 \u2208 B|Htk, Xtk, Htk\u22121, Xtk\u22121, . . . , Ht1, Xt1, Ht0, Xt0 \ufffd = P \ufffd Xtk+1 \u2208 B|Htk, Xtk \ufffd . (3) 6 \u2022 (Reward) At any time tk, where k = 0, 1, . . . , n \u2212 1, given the current state Xtk in the market and the current hedging strategy Htk, a reward signal Rtk+1 \ufffd Xtk, Htk, Xtk+1 \ufffd is received, by the hedging agent, as a result of transition to the next state Xtk+1. The reward signal shall be speci\ufb01ed after introducing the (optimal) value function below. In the sequel, occasionally, for notational simplicity, we simply write Rtk+1 to represent Rtk+1 \ufffd Xtk, Htk, Xtk+1 \ufffd , for k = 0, 1, . . . , n \u2212 1. \u2022 (State, action, and reward sequence) The states, actions, and reward signals form an episode, which is sequentially given by: \ufffd Xt0, Ht0, Xt1, Rt1, Ht1, Xt2, Rt2, Ht2, . . . , Xt\u02dcn\u22121, Rt\u02dcn\u22121, Ht\u02dcn\u22121, Xt\u02dcn, Rt\u02dcn \ufffd . \u2022 (Optimal value function) Based on the reward signals, the value function, at any time tk, where k = 0, 1, . . . , n\u2212 1, with the state x \u2208 X, is de\ufb01ned by, for any hedging strategies Htk, Htk+1, . . . , Htn\u22121, V \ufffd tk, x; Htk, Htk+1, . . . , Htn\u22121 \ufffd = E \ufffdn\u22121 \ufffd l=k \u03b3tl+1\u2212tkRtl+1 \ufffd\ufffd\ufffdXtk = x \ufffd , (4) where \u03b3 \u2208 [0, 1] is the discount rate; the value function, at the time tn = T with the state x \u2208 X, is de\ufb01ned by V (tn, x) = 0. Hence, the optimal discrete hedging strategy, being implemented forwardly, is given by H\u2217 = arg max H\u2208H E \ufffdn\u22121 \ufffd k=0 \u03b3tk+1Rtk+1 \ufffd\ufffd\ufffdX0 = x \ufffd . (5) In turn, the optimal value function, at any time tk, where k = 0, 1, . . . , n \u2212 1, with the state x \u2208 X, is V \u2217 (tk, x) = V \ufffd tk, x; H\u2217 tk, H\u2217 tk+1, . . . , H\u2217 tn\u22121 \ufffd , and V \u2217 (tn, x) = 0. (6) \u2022 (Reward engineering) To ensure the hedging problem being reformulated with the MDP, the value functions, given by that in (5), and the negative of that in (2), should coincide; that is, E \ufffdn\u22121 \ufffd k=0 \u03b3tk+1Rtk+1 \ufffd\ufffd\ufffdX0 = x \ufffd = \u2212E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd . (7) Hence, two possible constructions for the reward signals are proposed as follows; each choice of the reward signals shall be utilized in one of the two phases in the proposed RL approach. \u2013 (Single terminal reward) An obvious choice is to only have a reward signal from the negative squared terminal P&L; that is, for any time tk, Rtk+1 = \ufffd \u2212 (Pt\u02dcn \u2212 Lt\u02dcn)2 if k = \u02dcn \u2212 1, 0 otherwise. (8) Necessarily, the discount rate is given as \u03b3 = 1. \u2013 (Sequential anchor-hedging reward) A less obvious choice is via telescoping the RHS of Equation (7), that \u2212E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd = \u2212E \ufffd\u02dcn\u22121 \ufffd k=0 \ufffd\ufffd Ptk+1 \u2212 Ltk+1 \ufffd2 \u2212 (Ptk \u2212 Ltk)2\ufffd + (P0 \u2212 L0)2 \ufffd . Therefore, when L0 = P0, another possible construction for the reward signal is, for any time tk, Rtk+1 = \ufffd (Ptk \u2212 Ltk)2 \u2212 \ufffd Ptk+1 \u2212 Ltk+1 \ufffd2 if k = 0, 1, . . . , \u02dcn \u2212 1, 0 otherwise. (9) Again, the discount rate is necessarily given as \u03b3 = 1. The constructed reward in (9) outlines an anchorhedging scheme. First, note that, at the current time 0, when L0 = P0, there is no local hedging error. Then, at each future hedging time before the last policyholder dies and before the maturity, the hedging performance is measured by the local squared P&L, i.e. (Ptk \u2212 Ltk)2, which serves as an anchor. At the next hedging time, if the local squared P&L is smaller than the anchor, it will be rewarded, i.e. Rtk+1 > 0; however, if the local squared P&L becomes larger, it will be penalized, i.e. Rtk+1 < 0. 7 2.4 Illustrative Example The illustrative example below demonstrates the poor hedging performance by the Delta hedging strategy when the insurer miscalibrates the parameters in the market environment. We consider that the insurer hedges a variable annuity contract, with both GMMB and GMDB riders, of a single policyholder, i.e. N = 1, with the contract characteristics given in Table 1. Parameter Value Expiration date T 1 Minimum guarantee at maturity GM 100 Minimum guarantee at death GD 100 Table 1: Contract Characteristics The market environment follows the Black-Scholes (BS) in the \ufb01nancial part and the constant force of mortality (CFM) in the actuarial front. The risk-free asset earns a constant risk-free interest rate r > 0 that, for any t \u2208 [0, T], dBt = rBtdt, while the value of the risky asset evolves as a geometric Brownian motion that, for any t \u2208 [0, T], dSt = \u00b5Stdt + \u03c3StdWt, where \u00b5 is a constant drift, \u03c3 > 0 is a constant volatility, and W = {Wt}t\u2208[0,T ] is the standard Brownian motion. The random future lifetime of the policyholder Tx has a CFM \u03bd > 0; that is, for any 0 \u2264 t \u2264 s \u2264 T, the conditional survival probability P (Tx > s|Tx > t) = e\u2212\u03bd(s\u2212t). Moreover, the Brownian motion W in the \ufb01nancial market and the future lifetime Tx in the actuarial market are independent. Table 2 summarizes the parameters in the market environment. Note that the risk-free interest rate, the risky asset initial price, the initial age of the policyholder, and the investment strategy of the policyholder, are observable by the insurer. (a) Black-Scholes Financial Market Parameter Value Risk-free interest rate r 0.02 Risky asset initial price S0 100 Risky asset drift \u00b5 \u22120.2 Risky asset volatility \u03c3 0.4 (b) Constant Force of Mortality Actuarial Market Parameter Value Initial number of policyholders N 1 Initial age of policyholders x 20 Constant force of mortality \u03bd 0.03 Investment strategy of policyholders \u03c1 1.19 Table 2: Parameters setting of market environment Based on her best knowledge of the market, the insurer builds a model of the market environment. Suppose that the model happens to be the BS and the CFM as the market environment, but the insurer miscalibrates the parameters. Table 3 lists these parameters in the model of the market environment. In particular, the risky asset drift and volatility, as well as the force of mortality constant are di\ufb00erent from those in the market environment. For the observable parameters, they are the same as those in the market environment. (a) Black-Scholes Financial Market Parameter Value Risk-free interest rate r 0.02 Risky asset initial price S0 100 Risky asset drift \u00b5 0.08 Risky asset volatility \u03c3 0.2 (b) Constant Force of Mortality Actuarial Market Parameter Value Initial number of policyholders N 1 Initial age of policyholders x 20 Constant force of mortality \u03bd 0.02 Investment strategy of policyholders \u03c1 1.19 Table 3: Parameters setting of model of market environment, with bolded parameters being di\ufb00erent from those in market environment At any time t \u2208 [0, T], the value of the hedging portfolio of the insurer is given by (17), with N = 1, in which the values of the risky asset and the single-jump process follow the market environment with the parameters in Table 2. At any time t \u2208 [0, T], the value of the net liability of the insurer is given by (16), with N = 1, in both the market environment and its model; for its detailed derivations, we defer it to Section 4.1, as the model of the market environment, with multiple homogeneous policyholders for e\ufb00ective training, shall be supplied as the training environment. Since the parameters in the model of the market environment (see Table 3) are di\ufb00erent from those in the market environment (see Table 2), the net liability evaluated by the insurer using the model is di\ufb00erent from that of the market environment. There are two implications. Firstly, the Delta hedging strategy of the insurer using the parameters in Table 3 is incorrect, while the correct Delta hedging strategy should use the parameters in 8 Table 2. Secondly, the asset-value-based fee m and the rider charge me given in Table 4, which are determined by the insurer based on the time-0 value of her net liability by Table 3 via the method in Section 2.1.3, are mispriced. They would not lead to zero time-0 value of her net liability in the market environment which is based on Table 2. Parameter Value Rate for asset-value-based fee m 0.02 Rate for rider charge me 0.019 Table 4: Fee structures derived from model of market environment To evaluate the hedging performance of the incorrect Delta strategy by the insurer in the market environment for the variable annuity of contract characteristics in Table 1, 5000 market scenarios using the parameters in Table 2 are simulated to realize terminal P&Ls. For comparison, the terminal P&Ls by the correct Delta hedging strategy are also obtained. Figure 1 shows the empirical density and cumulative distribution functions of the 5000 realized terminal P&Ls by each Delta hedging strategy, while Table 5 outlines the summary statistics of the empirical distributions, in which \ufffd RMSE is the estimated RMSE of the terminal P&L similar to (2). In Figure 1a, the empirical density function of realized terminal P&Ls by the incorrect Delta hedging strategy is depicted to be more heavy-tailed on the left than that by the correct Delta strategy. In fact, the terminal P&L by the incorrect Delta hedging strategy is stochastically dominated by that by the correct Delta strategy in the \ufb01rst-order; see Figure 1b. Table 5 shows that the terminal P&L by the incorrect Delta hedging strategy has a mean and a median farther from zero, a higher standard deviation, larger left-tail risks in terms of Value-at-Risk and Tail Value-at-Risk, and a larger RMSE than that by the correct Delta strategy. These observations conclude that, even in a market environment as simple as the BS and the CFM, the incorrect Delta hedging strategy based on the miscalibrated parameters by the insurer does not perform well when it is being implemented forwardly. In general, the hedging performance of model-based approaches depends crucially on the calibration of parameters for the model of the market environment. (a) Empirical density (b) Empirical cumulative distribution Figure 1: Empirical density and cumulative distribution functions of realized terminal P&Ls by di\ufb00erent Delta strategies Terminal P&L of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 \ufffd RMSE Hedging Strategy Correct Delta \u22120.24 \u22120.14 2.96 \u22124.00 \u22125.59 \u22125.99 \u22127.22 2.97 Incorrect Delta \u22121.25 \u22120.22 3.41 \u22126.27 \u22128.80 \u22129.24 \u221211.05 3.63 Table 5: Summary statistics of empirical distributions of realized terminal P&Ls by di\ufb00erent Delta strategies 2.5 Two-Phase Reinforcement Learning Approach In an RL approach, at the current time 0, the insurer builds an RL agent to hedge on her behalf in the future. The agent interacts with a market environment, by sequentially observing states, taking, as well as revising, actions, which are the hedging strategies, and collecting rewards. Without possessing any prior knowledge of the market 9 environment, the agent needs to, explore the environment while exploit the collected reward signals, for e\ufb00ective learning. An intuitive proposition would be allowing an infant RL agent to learn directly from such market environment, like the one in Section 2.4, moving forward. However, recall that the insurer actually does not know any exact market dynamics in the environment and thus is not able to provide any theoretical model for the net liability to the RL agent. In turn, the RL agent could not receive any sequential anchor-hedging reward signal in (9) from the environment, but instead receives the single terminal reward signal in (8). Since the rewards, except the terminal one, are all zero, the infant RL agent would learn ine\ufb00ectively from such sparse rewards, i.e. the RL agent shall take a tremendous amount of time to \ufb01nally learn a nearly optimal hedging strategy in the environment. Most importantly, while the RL agent is exploring and learning from the environment, which is not a simulated one, the insurer could su\ufb00er from huge \ufb01nancial burden due to any sub-optimal hedging performances. In view of this, we propose that the insurer should \ufb01rst designate the infant RL agent to interact and learn from a training environment, which is constructed by the insurer based on her best knowledge of the market, for example, the model of the market environment in Section 2.4. Since the training environment is known to the insurer (but is unknown to the RL agent), the RL agent can be supplied by a net liability theoretical model, and consequently learn from the sequential anchor-hedging reward signal in (9) of the training environment. Therefore, the infant RL agent would be guided by the net liability to learn e\ufb00ectively from the local hedging errors. After interacting and learning from the training environment for a period of time, in order to gauge the e\ufb00ectiveness, the RL agent shall be tested for its hedging performance in simulated scenarios from the same training environment. This \ufb01rst phase is called the training phase. Training Phase: (i) The insurer constructs the MDP training environment. (ii) The insurer builds the infant RL agent which uses the PPO algorithm. (iii) The insurer assigns the RL agent in the MDP training environment to interact and learn for a period of time, during which the RL agent collects the anchor-hedging reward signal in (9). (iv) The insurer deploys the trained RL agent to hedge in simulated scenarios from the same training environment and documents the baseline hedging performance. If the hedging performance of the trained RL agent in the training environment is satisfactory, the insurer should then proceed to assign it to interact and learn from the market environment. Since the training and market environments are usually di\ufb00erent, such as having di\ufb00erent parameters as in Section 2.4, the initial hedging performance of the trained RL agent in the market environment is expected to diverge from the \ufb01ne baseline hedging performance in the training environment. However, di\ufb00erent from an infant RL agent, the trained RL agent is experienced so that the sparse reward signal in (8) should be su\ufb03cient for the agent to revise the hedging strategy, from the nearly optimal one in the training environment to that in the market environment, within a reasonable amount of time. This second phase is called the online learning phase. Online Learning Phase: (v) The insurer assigns the RL agent in the market environment to interact and learn in real time, during which the RL agent collects the single terminal reward signal in (8). These summarize the proposed two-phase RL approach. Figure 2 depicts the above sequence clearly. There are several assumptions underneath this two-phase RL approach in order to apply it e\ufb00ectively to a hedging problem of a contingent claim; as they involve speci\ufb01cs in later sections, we collate their discussions and elaborate their implications in practice in Section 7. In the following section, we shall brie\ufb02y review the training essentials of RL in order to introduce the PPO algorithm. For the details of online learning phase, we defer them until Section 5. 3 Review of Reinforcement Learning 3.1 Stochastic Action for Exploration One of the fundamental ideas in RL is that, at any time tk, where k = 0, 1, . . . , n \u2212 1, given the current state Xtk, the RL agent does not take a deterministic action Htk but extends it to a stochastic action, in order to explore the MDP environment and in turn learn from the reward signals. The stochastic action is sampled through a so-called policy, which is de\ufb01ned below. Let P (A) be a set of probability measures over the action space A; each probability measure \u00b5 (\u00b7) \u2208 P (A) maps a Borel set A \u2208 B (A) to \u00b5 \ufffd A \ufffd \u2208 [0, 1]. The policy \u03c0 (\u00b7) is a mapping from the state space X to the set of probability measures P (A); that is, for any state x \u2208 X, \u03c0 (x) = \u00b5 (\u00b7) \u2208 P (A). The value function and the optimal 10 Insurer RL Agent MDP Training Environment (i) construct (ii) build (iii) interact and learn RL Agent MDP Training Environment (iv) hedge and realize performance Training Phase (a) Training phase RL Agent Market Environment (v) interact and learn in real time (v) interact and learn in real time Online Learning Phase (b) Online learning phase Figure 2: The relationship among insurer, RL agent, MDP training environment, and market environment of the two-phase RL approach value function, at any time tk, where k = 0, 1, . . . , \u02dcn \u2212 1, with the state x \u2208 X, are then generalized as, for any policy \u03c0 (\u00b7), V (tk, x; \u03c0 (\u00b7)) = E \ufffd\u02dcn\u22121 \ufffd l=k Rtl+1 \ufffd\ufffd\ufffdXtk = x \ufffd , V \u2217 (tk, x) = sup \u03c0(\u00b7) V (tk, x; \u03c0 (\u00b7)) ; (10) at any time tk, where k = \u02dcn, \u02dcn+1, . . . , n\u22121, with the state x \u2208 X, for any policy \u03c0 (\u00b7), V (tk, x; \u03c0 (\u00b7)) = V \u2217 (tk, x) = 0. In particular, if P (A) contains only all Dirac measures over the action space A, which is the case in the DH approach of B\u00a8uhler et al. (2019) (see Appendix A for more details), the value function and the optimal value function reduce to (4) and (6). With this relaxed setting, solving the optimal hedging strategy H\u2217 boils down to \ufb01nding the optimal policy \u03c0\u2217 (\u00b7). 3.2 Policy Approximation and Parameterization As the hedging problem has the in\ufb01nite action space A, tabular solution methods for problems of \ufb01nite state space and \ufb01nite action space (such as Q-learning), or value function approximation methods for problems of in\ufb01nite state space and \ufb01nite action space (such as deep Q-learning) are not suitable. Instead, a policy gradient method is employed. To this end, the policy \u03c0 (\u00b7) is approximated and parametrized by the weights \u03b8p in an arti\ufb01cial neural network (ANN); in turn, denote the policy by \u03c0 (\u00b7; \u03b8p). The ANN Np (\u00b7; \u03b8p) (to be de\ufb01ned in (11) below) takes a state x \u2208 X as the input vector, and outputs parameters of a probability measure in P (A). In the sequel, the set P (A) contains all Gaussian measures (see, for example, Wang et al. (2020) and Wang and Zhou (2020)), in which each has a mean c and a variance d2, which depend on the state input x \u2208 X and the ANN weights \u03b8p. Therefore, for any state x \u2208 X, \u03c0 (x; \u03b8p) = \u00b5 (\u00b7; \u03b8p) \u223c Gaussian \ufffd c (x; \u03b8p) , d2 (x; \u03b8p) \ufffd , where \ufffd c (x; \u03b8p) , d2 (x; \u03b8p) \ufffd = Np (x; \u03b8p). With such approximation and parameterization, solving the optimal policy \u03c0\u2217 further boils down to \ufb01nding the 11 optimal ANN weights \u03b8\u2217 p. Hence, denote the value function and the optimal value function in (10) by V (tk, x; \u03b8p) and V \ufffd tk, x; \u03b8\u2217 p \ufffd , for any tk, where k = 0, 1, . . . , \u02dcn \u2212 1, with x \u2208 X. However, the (optimal) value function still depends on the objective probability measure P, the \ufb01nancial market dynamics, and the mortality model, which are unknown to the RL agent. Before formally introducing the policy gradient methods to tackle this issue, we shall \ufb01rst explicitly construct the ANNs for the approximated policy, as well as for an estimate of the value function (to prepare the algorithm of policy gradient method to be reviewed below). 3.3 Network Architecture As alluded above, in this paper, the ANN involves two parts, which are the policy network and the value function network. 3.3.1 Policy Network Let Np be the number of layers for the policy network. For l = 0, 1, . . . , Np, let d(l) p be the dimension of the l-th layer, where the 0-th layer is the input layer; the 1, 2, . . . , (Np \u2212 1)-th layers are hidden layers; the Np-th layer is the output layer. In particular, d(0) p = p, which is the number of features in the actuarial and \ufb01nancial parts, and d(Np) p = 2, which outputs the mean c and the variance d2 of the Gaussian measure. The policy network Np : Rp \u2192 R2 is de\ufb01ned as, for any x \u2208 Rp, Np (x) = \ufffd W (Np) p \u25e6 \u03c8 \u25e6 W (Np\u22121) p \u25e6 \u03c8 \u25e6 W (Np\u22122) p \u25e6 . . . \u25e6 \u03c8 \u25e6 W (1) p \ufffd (x) , (11) where, for l = 1, 2, . . . , Np, the mapping W (l) p : Rd(l\u22121) p \u2192 Rd(l) p is a\ufb03ne, and the mapping \u03c8 : Rd(l) p \u2192 Rd(l) p is a componentwise activation function. Let \u03b8p be the parameter vector of the policy network; in turn, denote the policy network in (11) by Np (x; \u03b8p), for any x \u2208 Rp. 3.3.2 Value Function Network The value function network is constructed similarly as in the policy network, except that all subscripts p (policy) are replaced by v (value). In particular, the value function network Nv : Rp \u2192 R is de\ufb01ned as, for any x \u2208 Rp, Nv (x) = \ufffd W (Nv) v \u25e6 \u03c8 \u25e6 W (Nv\u22121) v \u25e6 \u03c8 \u25e6 W (Nv\u22122) v \u25e6 . . . \u25e6 \u03c8 \u25e6 W (1) v \ufffd (x) , (12) which models an approximated value function \u02c6V (see Section 3.4 below). Let \u03b8v be the parameter vector of the value function network; in turn, denote the value function network in (12) by Nv (x; \u03b8v), for any x \u2208 Rp. 3.3.3 Shared Layers Structure Since the policy and value function networks should extract features from the input state vector in a similar manner, they are assumed to share the \ufb01rst few layers. More speci\ufb01cally, let Ns (< min {Np, Nv}) be the number of shared layers for the policy and value function networks; for l = 1, 2, . . . , Ns, W (l) p = W (l) v = W (l) s , and hence, for any x \u2208 Rp, Np (x; \u03b8p) = \ufffd W (Np) p \u25e6 \u03c8 \u25e6 W (Np\u22121) p \u25e6 . . . \u25e6 \u03c8 \u25e6 W (Ns+1) p \u25e6 \u03c8 \u25e6 W (Ns) s \u25e6 . . . \u25e6 \u03c8 \u25e6 W (1) s \ufffd (x) , Nv (x; \u03b8v) = \ufffd W (Nv) v \u25e6 \u03c8 \u25e6 W (Nv\u22121) v \u25e6 . . . \u25e6 \u03c8 \u25e6 W (Ns+1) v \u25e6 \u03c8 \u25e6 W (Ns) s \u25e6 . . . \u25e6 \u03c8 \u25e6 W (1) s \ufffd (x) . Let \u03b8 be the parameter vector of the policy and value function networks. Figure 3 depicts such a shared layers structure. 3.4 Proximal Policy Optimization: A Temporal-Di\ufb00erence Policy Gradient Method A policy gradient method entails that, starting from initial ANN weights \u03b8(0), and via interacting with the MDP environment to observe the states and collect the reward signals, the RL agent gradually updates the ANN weights, by the (stochastic) gradient ascent on a certain surrogate performance measure de\ufb01ned for the ANN weights. That is, at each update step u = 1, 2, . . . , \u03b8(u) = \u03b8(u\u22121) + \u03b1 \ufffd \u2207\u03b8J (u\u22121) \ufffd \u03b8(u\u22121)\ufffd , (13) where the hyperparameter \u03b1 \u2208 [0, 1] is the learning rate of the RL agent, and, based on the experienced episode(s), \ufffd \u2207\u03b8J (u\u22121) \ufffd \u03b8(u\u22121)\ufffd is the estimated gradient of the surrogate performance measure J (u\u22121) (\u00b7) evaluating at \u03b8 = 12 ... ... ... ... x1 x2 x4 \u02c6V (x) c (x) d2 (x) Input Layer Shared Layer Non-Shared Layer Ouput Layer Figure 3: An example of policy and value function arti\ufb01cial neural networks with a shared hidden layer and a non-shared hidden layer \u03b8(u\u22121). REINFORCE, which is pioneered by Williams (1992), is a Monte Carlo policy gradient method, which updates the ANN weights by each episode. As this paper applies a temporal-di\ufb00erence (TD) policy gradient method, we relegate the review of REINFORCE to Appendix B, where the Policy Gradient Theorem, the foundation of any policy gradient methods, is presented. PPO, which is pioneered by Schulman et al. (2017), is a TD policy gradient method, which updates the ANN weights by a batch of K \u2208 N realizations. At each update step u = 1, 2, . . . , based on the ANN weights \u03b8(u\u22121), and thus the policy \u03c0 \ufffd \u00b7; \u03b8(u\u22121) p \ufffd , the RL agent experiences E(u) \u2208 N realized episodes for the K realizations. \u2022 If E(u) = 1, the episode is given by \ufffd . . . , x(u\u22121) t K(u) s , h(u\u22121) t K(u) s , x(u\u22121) t K(u) s +1, r(u\u22121) t K(u) s +1, h(u\u22121) t K(u) s +1, . . . , x(u\u22121) t K(u) s +K\u22121, r(u\u22121) t K(u) s +K\u22121, h(u\u22121) t K(u) s +K\u22121, x(u\u22121) t K(u) s +K, r(u\u22121) t K(u) s +K, . . . \ufffd , where K(u) s = 0, 1, . . . , \u02dcn \u2212 1, such that the time tK(u) s is when the episode is initiated in this update, and h(u\u22121) tk , for k = 0, 1, . . . , \u02dcn \u2212 1, is the time-tk realized hedging strategy being sampled from the Gaussian distribution with the mean c \ufffd x(u\u22121) tk ; \u03b8(u\u22121) p \ufffd and the variance d2 \ufffd x(u\u22121) tk ; \u03b8(u\u22121) p \ufffd ; necessarily, \u02dcn\u2212K(u) s \u2265 K. \u2022 If E(u) = 2, 3, . . . , the episodes are given by \ufffd . . . , x(u\u22121,1) t K(u) s , h(u\u22121,1) t K(u) s , x(u\u22121,1) t K(u) s +1, r(u\u22121,1) t K(u) s +1, h(u\u22121,1) t K(u) s +1, . . . , x(u\u22121,1) t\u02dcn(1)\u22121 , r(u\u22121,1) t\u02dcn(1)\u22121 , h(u\u22121,1) t\u02dcn(1)\u22121 , x(u\u22121,1) t\u02dcn(1) , r(u\u22121,1) t\u02dcn(1) \ufffd , \ufffd x(u\u22121,2) t0 , h(u\u22121,2) t0 , x(u\u22121,2) t1 , r(u\u22121,2) t1 , h(u\u22121,2) t1 , . . . , x(u\u22121,2) t\u02dcn(2)\u22121 , r(u\u22121,2) t\u02dcn(2)\u22121 , h(u\u22121,2) t\u02dcn(2)\u22121 , x(u\u22121,2) t\u02dcn(2) , r(u\u22121,2) t\u02dcn(2) \ufffd , . . . , 13 \ufffd x(u\u22121,E(u)\u22121) t0 , h(u\u22121,E(u)\u22121) t0 , x(u\u22121,E(u)\u22121) t1 , r(u\u22121,E(u)\u22121) t1 , h(u\u22121,E(u)\u22121) t1 , . . . , x(u\u22121,E(u)\u22121) t \u02dcn(E(u)\u22121)\u22121 , r(u\u22121,E(u)\u22121) t \u02dcn(E(u)\u22121)\u22121 , h(u\u22121,E(u)\u22121) t \u02dcn(E(u)\u22121)\u22121 , x(u\u22121,E(u)\u22121) t \u02dcn(E(u)\u22121) , r(u\u22121,E(u)\u22121) t \u02dcn(E(u)\u22121) \ufffd , \ufffd x(u\u22121,E(u)) t0 , h(u\u22121,E(u)) t0 , x(u\u22121,E(u)) t1 , r(u\u22121,E(u)) t1 , h(u\u22121,E(u)) t1 , . . . , x(u\u22121,E(u)) t K(u) f \u22121 , r(u\u22121,E(u)) t K(u) f \u22121 , h(u\u22121,E(u)) t K(u) f \u22121 , x(u\u22121,E(u)) t K(u) f , r(u\u22121,E(u)) t K(u) f , . . . \ufffd , where K(u) f = 1, 2, . . . , \u02dcn(E(u)), such that the time tK(u) f is when the last episode is \ufb01nished (but not necessarily terminated) in this update; necessarily, \u02dcn(1) \u2212 K(u) s + \ufffdE(u)\u22121 e=2 \u02dcn(e) + K(u) f = K. The surrogate performance measure of PPO consists of three components. In the following, \ufb01x an update step u = 1, 2, . . . . Inspired by Schulman et al. (2015), in which the time-0 value function di\ufb00erence between two policies is shown to be equal to the expected advantage, together with importance sampling and KL divergence constraint reformulation, the \ufb01rst component in the surrogate performance measure of PPO is given by: \u2022 if E(u) = 1, L(u\u22121) CLIP (\u03b8p) = E \uf8ee \uf8f0 K(u) s +K\u22121 \ufffd k=K(u) s min \ufffd q(u\u22121) tk \u02c6A(u\u22121) \u03b8(u\u22121) p ,tk, clip \ufffd q(u\u22121) tk , 1 \u2212 \u03f5, 1 + \u03f5 \ufffd \u02c6A(u\u22121) \u03b8(u\u22121) p ,tk \ufffd\uf8f9 \uf8fb , where the importance sampling ratio q(u\u22121) tk = \u03c6 \ufffd H(u\u22121) tk ;X(u\u22121) tk ,\u03b8p \ufffd \u03c6 \ufffd H(u\u22121) tk ;X(u\u22121) tk ,\u03b8(u\u22121) p \ufffd, in which \u03c6 \ufffd \u00b7; X(u\u22121) tk , \u03b8p \ufffd is the Gaussian density function with mean c \ufffd X(u\u22121) tk ; \u03b8p \ufffd and variance d2 \ufffd X(u\u22121) tk ; \u03b8p \ufffd , the estimated advantage is evaluated at \u03b8p = \u03b8(u\u22121) p and bootstrapped through the approximated value function that \u02c6A(u\u22121) \u03b8(u\u22121) p ,tk = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \ufffdK(u) s +K\u22121 l=k R(u\u22121) tl+1 + \u02c6V \ufffd tK(u) s +K, X(u\u22121) t K(u) s +K; \u03b8(u\u22121) v \ufffd \u2212 \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd if K(u) s + K < \u02dcn, \ufffd\u02dcn\u22121 l=k R(u\u22121) tl+1 \u2212 \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd if K(u) s + K = \u02dcn, and the function clip \ufffd q(u\u22121) tk , 1 \u2212 \u03f5, 1 + \u03f5 \ufffd = min \ufffd max \ufffd q(u\u22121) tk , 1 \u2212 \u03f5 \ufffd , 1 + \u03f5 \ufffd . The approximated value function \u02c6V is given by the output of the value network, i.e. \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd = Nv \ufffd X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd as de\ufb01ned in (12) for k = 0, 1, . . . , \u02dcn \u2212 1. \u2022 if E(u) = 2, 3, . . . , L(u\u22121) CLIP (\u03b8p) = E \uf8ee \uf8f0 \u02dcn(1)\u22121 \ufffd k=K(u) s min \ufffd q(u\u22121,1) tk \u02c6A(u\u22121,1) \u03b8(u\u22121) p ,tk, clip \ufffd q(u\u22121,1) tk , 1 \u2212 \u03f5, 1 + \u03f5 \ufffd \u02c6A(u\u22121,1) \u03b8(u\u22121) p ,tk \ufffd + E(u)\u22121 \ufffd e=2 \u02dcn(e)\u22121 \ufffd k=0 min \ufffd q(u\u22121,e) tk \u02c6A(u\u22121,e) \u03b8(u\u22121) p ,tk, clip \ufffd q(u\u22121,e) tk , 1 \u2212 \u03f5, 1 + \u03f5 \ufffd \u02c6A(u\u22121,e) \u03b8(u\u22121) p ,tk \ufffd + K(u) f \u22121 \ufffd k=0 min \ufffd q(u\u22121,E(u)) tk \u02c6A(u\u22121,E(u)) \u03b8(u\u22121) p ,tk , clip \ufffd q(u\u22121,E(u)) tk , 1 \u2212 \u03f5, 1 + \u03f5 \ufffd \u02c6A(u\u22121,E(u)) \u03b8(u\u22121) p ,tk \ufffd \uf8f9 \uf8fa\uf8fb . Similar to REINFORCE in Appendix B, the second component in the surrogate performance measure of PPO minimizes the loss between the bootstrapped sum of reward signals and the approximated value function. To this end, de\ufb01ne: 14 \u2022 if E(u) = 1, L(u\u22121) VF (\u03b8v) = E \uf8ee \uf8f0 K(u) s +K\u22121 \ufffd k=K(u) s \ufffd \u02c6A(u\u22121) \u03b8(u\u22121) p ,tk + \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd \u2212 \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8v \ufffd\ufffd2 \uf8f9 \uf8fb ; \u2022 if E(u) = 2, 3, . . . , L(u\u22121) VF (\u03b8v) = E \uf8ee \uf8f0 \u02dcn(1)\u22121 \ufffd k=K(u) s \ufffd \u02c6A(u\u22121,1) \u03b8(u\u22121) p ,tk + \u02c6V \ufffd tk, X(u\u22121,1) tk ; \u03b8(u\u22121) v \ufffd \u2212 \u02c6V \ufffd tk, X(u\u22121,1) tk ; \u03b8v \ufffd\ufffd2 + E(u)\u22121 \ufffd e=2 \u02dcn(e)\u22121 \ufffd k=0 \ufffd \u02c6A(u\u22121,e) \u03b8(u\u22121) p ,tk + \u02c6V \ufffd tk, X(u\u22121,e) tk ; \u03b8(u\u22121) v \ufffd \u2212 \u02c6V \ufffd tk, X(u\u22121,e) tk ; \u03b8v \ufffd\ufffd2 + K(u) f \u22121 \ufffd k=0 \ufffd \u02c6A(u\u22121,E(u)) \u03b8(u\u22121) p ,tk + \u02c6V \ufffd tk, X(u\u22121,E(u)) tk ; \u03b8(u\u22121) v \ufffd \u2212 \u02c6V \ufffd tk, X(u\u22121,E(u)) tk ; \u03b8v \ufffd\ufffd2 \uf8f9 \uf8fa\uf8fb . Finally, to encourage the RL agent exploring the MDP environment, the third component in the surrogate performance measure of PPO is the entropy bonus. Based on the Gaussian density function, de\ufb01ne \u2022 if E(u) = 1, L(u\u22121) EN (\u03b8p) = E \uf8ee \uf8f0 K(u) s +K\u22121 \ufffd k=K(u) s ln d \ufffd X(u\u22121) tk ; \u03b8p \ufffd \uf8f9 \uf8fb ; \u2022 if E(u) = 2, 3, . . . , L(u\u22121) EN (\u03b8p) = E \uf8ee \uf8f0 \u02dcn(1)\u22121 \ufffd k=K(u) s ln d \ufffd X(u\u22121,1) tk ; \u03b8p \ufffd + E(u)\u22121 \ufffd e=2 \u02dcn(e)\u22121 \ufffd k=0 ln d \ufffd X(u\u22121,e) tk ; \u03b8p \ufffd + K(u) f \u22121 \ufffd k=0 ln d \ufffd X(u\u22121,E(u)) tk ; \u03b8p \ufffd \uf8f9 \uf8fa\uf8fb . Therefore, the surrogate performance measure of PPO is given by: J (u\u22121) (\u03b8) = L(u\u22121) CLIP (\u03b8p) \u2212 c1L(u\u22121) VF (\u03b8v) + c2L(u\u22121) EN (\u03b8p) , (14) where the hyperparameters c1, c2 \u2208 [0, 1] are the loss coe\ufb03cients of the RL agent. Its estimated gradient, based on the K realizations, is then computed via automatic di\ufb00erentiation; see, for example, Baydin et al. (2018). 4 Illustrative Example Revisited: Training Phase Recall that, in the training phase, the insurer constructs a model of the market environment for an MDP training environment, while the RL agent, which does not know any speci\ufb01cs of this MDP environment, observes states and receives the anchor-hedging reward signals in (9) from it, and hence gradually learns the hedging strategy by the PPO algorithm reviewed in the last section. This section revisits the illustrative example in Section 2.4 via the two-phase RL approach in the training phase. 4.1 Markov Decision Process Training Environment The model of the market environment is the BS and the CFM in the \ufb01nancial and the actuarial parts. However, unlike the model following the market environment to write a single contract to a single policyholder, for e\ufb00ective training, the insurer writes identical contracts to N homogeneous policyholders in the training environment. Because of the homogeneity of the contracts and the policyholders, for all i = 1, 2, . . . , N, xi = x, \u03c1(i) = \u03c1, m(i) = m, G(i) M = GM, G(i) D = GD, m(i) e = me, and F (i) t = Ft = \u03c1Ste\u2212mt, for t \u2208 [0, T]. 15 At any time t \u2208 [0, T], the future gross liability of the insurer accumulated to the maturity is thus (GM \u2212 FT )+ \ufffdN i=1 J(i) T + \ufffdN i=1 er(T \u2212T (i) x ) \ufffd GD \u2212 FT (i) x \ufffd + 1{T (i) x <T }J(i) t , and its time-t discounted value is V GL t = e\u2212r(T \u2212t)EQ \ufffd (GM \u2212 FT )+ N \ufffd i=1 J(i) T \ufffd\ufffd\ufffdFt \ufffd + EQ \ufffd N \ufffd i=1 e\u2212r(T (i) x \u2212t) \ufffd GD \u2212 FT (i) x \ufffd + 1{T (i) x <T }J(i) t \ufffd\ufffd\ufffdFt \ufffd = e\u2212r(T \u2212t)EQ \ufffd (GM \u2212 FT )+ |Ft \ufffd N \ufffd i=1 EQ \ufffd J(i) T |Ft \ufffd + N \ufffd i=1 J(i) t EQ \ufffd e\u2212r(T (i) x \u2212t) \ufffd GD \u2212 FT (i) x \ufffd + 1{T (i) x <T } \ufffd\ufffd\ufffdFt \ufffd , where the probability measure Q de\ufb01ned on (\u2126, F) is an equivalent martingale measure with respect to P. Herein, the probability measure Q is chosen to be the product measure of each individual equivalent martingale measure in the actuarial or \ufb01nancial part, which implies the independence among the Brownian motion W and the future lifetime T (1) x , T (2) x , . . . , T (N) x , clarifying the \ufb01rst term in the second equality above. The second term in that equality is due to the fact that, for i = 1, 2, . . . , N, the single-jump process J(i) is F-adapted. Under the probability measure Q, all future lifetime are identically distributed and have a CFM \u03bd > 0, which are the same as those under the probability measure P in Section 2.4. Therefore, for any i = 1, 2, . . . , N, and for any 0 \u2264 t \u2264 s \u2264 T, the conditional survival probability Q \ufffd T (i) x > s|T (i) x > t \ufffd = e\u2212\u03bd(s\u2212t) . For each policyholder i = 1, 2, . . . , N, by the independence and the Markov property, for any 0 \u2264 t \u2264 s \u2264 T, EQ \ufffd J(i) s |Ft \ufffd = EQ \ufffd J(i) s |J(i) t \ufffd = \uf8f1 \uf8f2 \uf8f3 Q \ufffd T (i) x > s|T (i) x \u2264 t \ufffd = 0 if T (i) x (\u03c9) \u2264 t Q \ufffd T (i) x > s|T (i) x > t \ufffd = e\u2212\u03bd(s\u2212t) if T (i) x (\u03c9) > t . (15) Moreover, under the probability measure Q, for any t \u2208 [0, T], dFt = (r \u2212 m) Ftdt + \u03c3FtdW Q t , where W Q = \ufffd W Q t \ufffd t\u2208[0,T ] is the standard Brownian motion under the probability measure Q. Hence, the time-t value of the discounted future gross liability, for t \u2208 [0, T], is given by V GL t = e\u2212\u03bd(T \u2212t) \ufffd GMe\u2212r(T \u2212t)\u03a6 (\u2212d2 (t, GM)) \u2212 Fte\u2212m(T \u2212t)\u03a6 (\u2212d1 (t, GM)) \ufffd N \ufffd i=1 J(i) t + \ufffd T t \ufffd GDe\u2212r(T \u2212s)\u03a6 (\u2212d2 (s, GD)) \u2212 Fte\u2212m(T \u2212s)\u03a6 (\u2212d1 (s, GD)) \ufffd \u03bde\u2212\u03bd(s\u2212t)ds N \ufffd i=1 J(i) t , where, for s \u2208 [0, T) and G > 0, d1 (s, G) = ln( Fs G )+ \ufffd r\u2212m+ \u03c32 2 (T \u2212s) \ufffd \u03c3\u221aT \u2212s , d2 (s, G) = d1 (s, G) \u2212 \u03c3 \u221a T \u2212 s, d1 (T, G) = lims\u2192T \u2212 d1 (s, G), d2 (T, G) = d1 (T, G), and \u03a6 (\u00b7) is the standard Gaussian distribution function. Note that \ufffdN i=1 J(i) t represents the number of surviving policyholders at time t \u2208 [0, T]. As for the cumulative future rider charge to be collected by the insurer from any time t \u2208 [0, T] onward, it is given by \ufffdN i=1 \ufffd T t meFsJ(i) s er(T \u2212s)ds, and its time-t discounted value is V RC t = e\u2212r(T \u2212t)EQ \ufffd N \ufffd i=1 \ufffd T t meFsJ(i) s er(T \u2212s)ds \ufffd\ufffd\ufffdFt \ufffd = N \ufffd i=1 \ufffd T t mee\u2212r(s\u2212t)EQ [Fs|Ft] EQ \ufffd J(i) s |J(i) t \ufffd ds, where the second equality is again due to the independence and the Markov property. Under the probability measure Q, EQ [Fs|Ft] = e(r\u2212m)(s\u2212t)Ft. Together with (15), V RC t = 1 \u2212 e\u2212(m+\u03bd)(T \u2212t) m + \u03bd meFt N \ufffd i=1 J(i) t . 16 Therefore, the time-t net liability of the insurer, for t \u2208 [0, T], is given by Lt = V GL t \u2212 V RC t = \ufffd e\u2212\u03bd(T \u2212t) \ufffd GMe\u2212r(T \u2212t)\u03a6 (\u2212d2 (t, GM)) \u2212 Fte\u2212m(T \u2212t)\u03a6 (\u2212d1 (t, GM)) \ufffd + \ufffd T t \ufffd GDe\u2212r(T \u2212s)\u03a6 (\u2212d2 (s, GD)) \u2212 Fte\u2212m(T \u2212s)\u03a6 (\u2212d1 (s, GD)) \ufffd \u03bde\u2212\u03bd(s\u2212t)ds \u2212 1 \u2212 e\u2212(m+\u03bd)(T \u2212t) m + \u03bd meFt \ufffd N \ufffd i=1 J(i) t , (16) which contributes parts of the reward signals in (9). The time-t value of the insurer\u2019s hedging portfolio, for t \u2208 [0, T], as in (1), is given by: P0 = 0, and if t \u2208 (tk, tk+1], for some k = 0, 1, . . . , n \u2212 1, Pt = (Ptk \u2212 HtkStk) er(t\u2212tk) + HtkSt + me \ufffd t tk Fser(t\u2212s) N \ufffd i=1 J(i) s ds \u2212 N \ufffd i=1 er(t\u2212T (i) x ) \ufffd GD \u2212 FT (i) x \ufffd + 1{tk<T (i) x \u2264t<T }, (17) which is also supplied to the reward signals in (9). At each time tk, where k = 0, 1, . . . , n, the RL agent is given to observe four features from this MDP environment; these four features are summarized in the state vector Xtk = \ufffd ln Ftk, Ptk N , \ufffdN i=1 J(i) tk N , T \u2212 tk \ufffd . (18) The \ufb01rst feature is the natural logarithm of the segregated account value of the policyholder. The second feature is the hedging portfolio value of the insurer, being normalized by the initial number of policyholders. The third feature is the ratio of the number of surviving policyholders with respect to the initial number of policyholders. These features are either log-transformed or normalized to prevent the RL agent from exploring and learning from features with high variability. The last feature is the term to maturity. In particular, when either the third or the last feature \ufb01rst hits zero, i.e. at time t\u02dcn, an episode is terminated. The state space X = R \u00d7 R \u00d7 [0, 1/N, 2/N, . . . , 1] \u00d7 {0, t1, t2, . . . , T}. Recall that, at each time tk, where k = 0, 1, . . . , \u02dcn \u2212 1, with the state vector (18) being the input, the output of the policy network in (11) is the mean c (Xtk; \u03b8p) and the variance d2 (Xtk; \u03b8p) of a Gaussian measure; herein, the Gaussian measure represents the distribution of the average number of shares of the risky asset being held by the insurer at the time tk for each surviving policyholder. Hence, for k = 0, 1, . . . , \u02dcn \u2212 1, the hedging strategy Htk in (17) is given by Htk = Htk \ufffdN i=1 J(i) tk , where Htk is sampled from the Gaussian measure. Since the hedging strategy is assumed to be Markovian with respect to the state vector, it can be shown, albeit tedious, that the state vector, in (18), and the hedging strategy together, satisfy the Markov property in (3). Also recall that the infant RL agent is trained in the MDP environment with multiple homogeneous policyholders. The RL agent should then e\ufb00ectively update the ANN weights \u03b8, and learn the hedging strategies, via a more direct inference on the force of mortality from the third feature in the state vector. The RL agent hedges daily, so that the di\ufb00erence between the consecutive discrete hedging time is \u03b4tk = tk+1 \u2212 tk = 1 252, for k = 0, 1, . . . , n \u2212 1. In this MDP training environment, the parameters of the model are given in Table 3, but with N = 500. 4.2 Building Reinforcement Learning Agent After constructing this MDP training environment, the insurer builds the RL agent which implements the PPO, which was reviewed in Section 3.4. Table 6a summarizes all hyperparameters of the implemented PPO, in which three of them are determined via grid search1, while the remaining two are \ufb01xed a priori since they alter the surrogate performance measure itself, and thus should not be based on grid search. Table 6b outlines the hyperparameters of the ANN architecture in Section 3.3, which are all pre-speci\ufb01ed, in which ReLU stands for Recti\ufb01ed Linear Unit; that is, the componentwise activation function is given by, for any z \u2208 R, \u03c8 (z) = max {z, 0}. 4.3 Training of Reinforcement Learning Agent With all these being set up, the insurer assigns the RL agent experiencing this MDP training environment, in order to observe the state, decide, as well as revise, the hedging strategy, and collect the anchor-hedging reward signal 1The grid search was performed using the Hardware-Accelerated Learning cluster in the National Center for Supercomputing Applications; see Kindratenko et al. (2020). 17 (a) Hyperparameters for Proximal Policy Optimization Grid-Searched Pre-Speci\ufb01ed Hyperparameter Value Hyperparameter Value Learning rate \u03b1 0.07 Coe\ufb03cient of value function 0.25 Batch size K 2048 approximation loss c1 Clip factor \u03f5 0.18 Coe\ufb03cient of entropy bonus c2 0.01 (b) Hyperparameters for Neural Network Hyperparameter Value(s) Number of layers in policy network Np 6 Number of layers in value function network Nv 6 Number of shared layers Ns 3 Dimension of hidden layers in policy network d(l) p [32, 64, 128, 64, 32] Dimension of hidden layers in value function network d(l) v [32, 64, 128, 64, 32] Activation function \u03c8 (\u00b7) ReLU Table 6: Hyperparameters setting of Proximal Policy Optimization and neural network based on (9), as much as possible. Let U \u2208 N be the number of update steps in the training environment on the ANN weights. Hence, the policy of the experienced RL agent is given by \u03c0 \ufffd \u00b7; \u03b8(U)\ufffd = \u03c0 \ufffd \u00b7; \u03b8(U) p \ufffd . Figure 4 depicts the training log of the RL agent in terms of bootstrapped sum of rewards and batch entropy. In particular, Figure 4a shows that the value function in (2) reduces to almost zero after around 108 training timesteps, which is equivalent to around 48828 update steps for the ANN weights; within the same number of training timesteps, Figure 4b illustrates a gradual depletion on the batch entropy, and hence the Gaussian measure gently becomes more concentrating around its mean, which implies that the RL agent progressively diminishes the degree of exploration on the MDP training environment, while increases the degree of exploitation on the learned ANN weights. (a) Bootstrapped sum of rewards (b) Batch entropy Figure 4: Training log in terms of bootstrapped sum of rewards and batch entropy 4.4 Baseline Hedging Performance In the \ufb01nal step of the training phase, the trained RL agent is assigned to hedge in simulated scenarios from the same MDP training environment, except that N = 1 which is in line with hedging in the market environment. The trained RL agent takes the deterministic action c \ufffd \u00b7; \u03b8(U) p \ufffd which is the mean of the Gaussian measure. The number of simulated scenarios is 5000. For each scenario, the insurer documents the realized terminal P&L, i.e. Pt\u02dcn \u2212Lt\u02dcn. After all scenarios are experienced by the trained RL agent, the insurer examines the baseline hedging performance via the empirical distribution and the summary statistics of the realized terminal P&Ls. The baseline hedging performance of the RL agent is also benchmarked with those by other methods, namely, the classical Deltas and the DH; see Appendix C for the implemented hyperparameters of the DH training. The following four classical 18 Deltas are implemented in the simulated scenarios from the training environment, in which the (in)correctness of the Deltas are with respect to the training environment: \u2022 (correct) Delta of the CFM actuarial and BS \ufb01nancial models with the model parameters as in Table 3; \u2022 (incorrect) Delta of the increasing force of mortality (IFM) actuarial and BS \ufb01nancial models, where, for any i = 1, 2, . . . , N, if T < b, the conditional survival probability Q \ufffd T (i) x > s|T (i) x > t \ufffd = b\u2212s b\u2212t , for any 0 \u2264 t \u2264 s \u2264 T < b, while if b \u2264 T, the conditional survival probability Q \ufffd T (i) x > s|T (i) x > t \ufffd = b\u2212s b\u2212t , for any 0 \u2264 t \u2264 s < b \u2264 T, and Q \ufffd T (i) x > s|T (i) x > t \ufffd = 0, for any 0 \u2264 t \u2264 b \u2264 s \u2264 T or 0 \u2264 b \u2264 t \u2264 s \u2264 T, with the model parameters as in Tables 3a and 7; \u2022 (incorrect) Delta in the CFM actuarial and Heston \ufb01nancial models, where, for any t \u2208 [0, T], dSt = \u00b5Stdt + \u221a\u03a3tStdW (1) t , d\u03a3t = \u03ba \ufffd \u03a3 \u2212 \u03a3t \ufffd dt + \u03b7\u221a\u03a3tdW (2) t , and \ufffd W (1), W (2)\ufffd t = \u03c6t, with the model parameters as in Tables 3b and 8; \u2022 (incorrect) Delta in the IFM actuarial and Heston \ufb01nancial models with the model parameters as in Tables 7 and 8. Parameter Value Initial number of policyholder N 1 Initial age of policyholder x 20 Lower bound of uniformly distributed lifetime b 0 Upper bound of uniformly distributed lifetime b 50 Investment strategy of policyholders \u03c1 1.19 Table 7: Parameters setting of increasing force of mortality actuarial model for Delta Parameter Value Risk-free interest rate r 0.02 Risky asset initial price S0 100 Risky asset drift \u00b5 0.08 Variance initial value \u03a30 0.04 Variance mean reversion rate \u03ba 0.2 Variance long-run average \u03a3 0.04 Variance volatility \u03b7 0.1 Brownian motions correlation \u03c6 \u22120.5 Table 8: Parameters setting of Heston \ufb01nancial model for Delta Figure 5 shows the empirical density and cumulative distribution functions via the 5000 realized terminal P&Ls by each hedging approach, while Table 9 outlines the summary statistics of these empirical distributions. To clearly illustrate the comparisons, Figure 6 depicts the empirical density functions via the 5000 pathwise di\ufb00erences of the realized terminal P&Ls between the RL agent and each of the other approaches, while Table 10 lists the summary statistics of the empirical distributions; for example, comparing with the DH approach, the pathwise di\ufb00erence of the realized terminal P&Ls for the e-th simulated scenario, for e = 1, 2, . . . , 5000, is calculated by \ufffd P RL t\u02dcn (\u03c9e) \u2212 LRL t\u02dcn (\u03c9e) \ufffd \u2212 \ufffd P DH t\u02dcn (\u03c9e) \u2212 LDH t\u02dcn (\u03c9e) \ufffd . 19 (a) Empirical density (b) Empirical cumulative distribution Figure 5: Empirical density and cumulative distribution functions of realized terminal P&Ls by the approaches of reinforcement learning, classical Deltas, and deep hedging Terminal P&L of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 \ufffd RMSE Hedging Approach Reinforcement Learning 0.02 \u22120.01 0.58 \u22120.54 \u22120.87 \u22121.05 \u22121.43 0.58 CFM & BS Delta \u22120.01 0.00 0.38 \u22120.44 \u22120.63 \u22120.70 \u22120.89 0.38 IFM & BS Delta \u22120.06 \u22120.06 0.45 \u22120.60 \u22120.77 \u22120.85 \u22121.02 0.45 CFM & Heston Delta \u22120.32 \u22120.33 0.87 \u22121.41 \u22121.73 \u22121.85 \u22122.17 0.93 IFM & Heston Delta \u22120.53 \u22120.53 1.20 \u22121.94 \u22122.48 \u22122.70 \u22123.23 1.31 Deep Hedging \u22120.01 \u22120.02 0.60 \u22120.52 \u22120.71 \u22121.04 \u22121.49 0.60 Table 9: Summary statistics of empirical distributions of realized terminal P&Ls by the approaches of reinforcement learning, classical Deltas, and deep hedging 20 (a) Reinforcement learning versus Delta in constant force of mortality and Black-Scholes models (b) Reinforcement learning versus Delta in increasing force of mortality and Black-Scholes models (c) Reinforcement learning versus Delta in constant force of mortality and Heston models (d) Reinforcement learning versus Delta in increasing force of mortality and Heston models (e) Reinforcement learning versus deep hedging Figure 6: Empirical density functions of realized pathwise di\ufb00erences of terminal P&Ls comparing with the approaches of classical Deltas and deep hedging Pathwise Di\ufb00erence of Mean Median Std. Dev. Probability of Terminal P&Ls Comparing With Non-Negativity CFM & BS Delta 0.02 0.01 0.62 50.6% IFM & BS Delta 0.08 0.07 0.66 54.7% CFM & Heston Delta 0.34 0.34 1.01 64.3% IFM & Heston Delta 0.54 0.58 1.29 70.0% Deep Hedging 0.02 0.01 0.75 51.3% Table 10: Summary statistics of empirical distributions of realized pathwise di\ufb00erences of terminal P&Ls comparing with the approaches of classical Deltas and deep hedging As expected, the baseline hedging performance of the trained RL agent in this training environment is comparable with those by, the correct CFM and BS Delta, as well as the DH approach. Moreover, the RL agent outperforms all the other three incorrect Deltas, which are based on either incorrect IFM actuarial or Heston \ufb01nancial model, or both. 21 5 Online Learning Phase Given the satisfactory baseline hedging performance of the experienced RL agent in the MDP training environment, the insurer \ufb01nally assigns the agent to interact and learn from the market environment. To distinguish them from the simulated time in the training environment, let \u02dctk, for k = 0, 1, 2, . . . , be the real time when the RL agent decides the hedging strategy in the market environment, such that 0 = \u02dct0 < \u02dct1 < \u02dct2 < \u00b7 \u00b7 \u00b7, and \u03b4\u02dctk = \u02dctk+1\u2212\u02dctk = 1 252. Note that the current time t = \u02dct0 = 0 and the RL agent shall hedge daily on behalf of the insurer. At the current time 0, the insurer writes a variable annuity contract with the GMMB and GMDB riders to the \ufb01rst policyholder. When this \ufb01rst contract terminates, due to either the death of the \ufb01rst policyholder or the expiration of the contract, the insurer shall write an identical contract, i.e. contract with the same characteristics, to the second policyholder. And so on. These contract re-establishments ensure that the insurer shall hold only one written variable annuity contract with the GMMB and GMDB riders at a time, and the RL agent shall solely hedge the contract being e\ufb00ective at that moment. To this end, iteratively, for the \u03b9-th policyholder, where \u03b9 \u2208 N, let \u02dct\u02dcn(\u03b9) be the \ufb01rst time (right) after the \u03b9-th policyholder dies or the contract expires, for some \u02dcn(\u03b9) = \u02dcn(\u03b9\u22121) + 1, \u02dcn(\u03b9\u22121) + 2, . . . , \u02dcn(\u03b9\u22121) + n; that is \u02dct\u02dcn(\u03b9) = min \ufffd \u02dctk, k = \u02dcn(\u03b9\u22121) + 1, \u02dcn(\u03b9\u22121) + 2, . . . , \u02dcn(\u03b9\u22121) + n : \u02dctk \u2212 \u02dct\u02dcn(\u03b9\u22121) \u2265 T (\u03b9) x\u03b9 \u2227 T \ufffd , where, by convention, \u02dcn(0) = 0. Therefore, the contract e\ufb00ective time for the \u03b9-th policyholder \u03c4 (\u03b9) k = \u02dct\u02dcn(\u03b9\u22121)+k, where \u03b9 \u2208 N and k = 0, 1, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121); in particular, \u03c4 (\u03b9) 0 = \u02dct\u02dcn(\u03b9\u22121) is the contract inception time for the \u03b9-th policyholder. Figure 7 depicts one of the possible realizations for clearly illustrating the real time and the contract e\ufb00ective time. t Real time Contract e\ufb00ective time \u02dct0 = 0 = \u03c4 (1) 0 \u02dct1 = \u03c4 (1) 1 \u02dct2 = \u03c4 (1) 2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u02dct\u02dcn(1)\u22121 = \u03c4 (1) \u02dcn(1)\u22121 \u02dct\u02dcn(1) = \u03c4 (2) 0 = \u03c4 (1) \u02dcn(1) \u02dct\u02dcn(1)+1 = \u03c4 (2) 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u02dct\u02dcn(2)\u22121 = \u03c4 (2) \u02dcn(2)\u2212\u02dcn(1)\u22121 \u02dct\u02dcn(2) = \u03c4 (3) 0 = \u03c4 (2) \u02dcn(2)\u2212\u02dcn(1) \u02dct\u02dcn(2)+1 = \u03c4 (3) 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 First policyholder Second policyholder \u00b7 \u00b7 \u00b7 Figure 7: An illustrative timeline with the real time and the contract e\ufb00ective time in the online learning phase In the online learning phase, the trained RL agent carries on with the PPO of policy gradient methods in the market environment. That is, as in Section 3.4, starting from the ANN weights \u03b8(U) at the current time 0, and via interacting with the market environment to observe the states and collect the reward signals, the RL agent further updates the ANN weights by a batch of \u02dcK \u2208 N realizations and the (stochastic) gradient ascent in (13) with the surrogate performance measure in (14), at each update step. However, there are subtle di\ufb00erences of applying the PPO in the market environment from that in the training environment. At each further update step v = 1, 2, . . . , based on the ANN weights \u03b8(U+v\u22121), and thus the policy \u03c0 \ufffd \u00b7; \u03b8(U+v\u22121) p \ufffd , the RL agent hedges each e\ufb00ective contract of \u02dcE(v) \u2208 N realized policyholders for the \u02dcK \u2208 N realizations. Indeed, the concept of episodes in the training environment, by the state re-initiation when one episode ends, should be replaced by sequential policyholders in the real-time market environment, via the contract re-establishment when one policyholder dies or contract expires. \u2022 If \u02dcE(v) = 1, which is when (v \u2212 1) \u02dcK, v \u02dcK \u2208 \ufffd \u02dcn(\u03b9\u22121), \u02dcn(\u03b9)\ufffd , for some \u03b9 \u2208 N, the batch of \u02dcK realizations is collected solely from the \u03b9-th policyholder. The realizations are given by \ufffd . . . , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s , h(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s +1 , r(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s +1 , h(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s +1 , . . . , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s + \u02dc K\u22121 , r(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s + \u02dc K\u22121 , h(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s + \u02dc K\u22121 , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s + \u02dc K , r(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s + \u02dc K , . . . \ufffd , where \u02dcK(v) s = 0, 1, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) \u2212 1, such that the time \u03c4 (\u03b9) \u02dc K(v) s is when the \ufb01rst state is observed for the \u03b9-th policyholder in this update; necessarily, \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) \u2212 \u02dcK(v) s \u2265 \u02dcK. \u2022 If \u02dcE(v) = 2, 3, . . . , which is when (v \u2212 1) \u02dcK \u2208 \ufffd \u02dcn(\u03b9\u22121), \u02dcn(\u03b9)\ufffd and v \u02dcK \u2208 \ufffd \u02dcn(j\u22121), \u02dcn(j)\ufffd , for some \u03b9, j \u2208 N such that \u03b9 < j, the batch of \u02dcK realizations is collected from the \u03b9-th, (\u03b9 + 1)-th, . . . , and j-th policyholders; that 22 is, \u02dcE(v) = j \u2212 \u03b9 + 1. The realizations are given by \ufffd . . . , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s , h(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s +1 , r(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s +1 , h(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dc K(v) s +1 , . . . , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dcn(\u03b9)\u2212\u02dcn(i\u22121)\u22121 , r(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dcn(\u03b9)\u2212\u02dcn(i\u22121)\u22121 , h(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dcn(\u03b9)\u2212\u02dcn(i\u22121)\u22121 , x(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dcn(\u03b9)\u2212\u02dcn(i\u22121) , r(v\u22121,\u03b9) \u03c4 (\u03b9) \u02dcn(\u03b9)\u2212\u02dcn(i\u22121) \ufffd , \ufffd x(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) 0 , h(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) 0 , x(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) 1 , r(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) 1 , h(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) 1 , . . . , x(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) \u02dcn(\u03b9+1)\u2212\u02dcn(\u03b9)\u22121 , r(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) \u02dcn(\u03b9+1)\u2212\u02dcn(\u03b9)\u22121 , h(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) \u02dcn(\u03b9+1)\u2212\u02dcn(\u03b9)\u22121 , x(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) \u02dcn(\u03b9+1)\u2212\u02dcn(\u03b9) , r(v\u22121,\u03b9+1) \u03c4 (\u03b9+1) \u02dcn(\u03b9+1)\u2212\u02dcn(\u03b9) \ufffd , . . . , \ufffd x(v\u22121,j\u22121) \u03c4 (j\u22121) 0 , h(v\u22121,j\u22121) \u03c4 (j\u22121) 0 , x(v\u22121,j\u22121) \u03c4 (j\u22121) 1 , r(v\u22121,j\u22121) \u03c4 (j\u22121) 1 , h(v\u22121,j\u22121) \u03c4 (j\u22121) 1 , . . . , x(v\u22121,j\u22121) \u03c4 (j\u22121) \u02dcn(j\u22121)\u2212\u02dcn(j\u22122)\u22121 , r(v\u22121,j\u22121) \u03c4 (j\u22121) \u02dcn(j\u22121)\u2212\u02dcn(j\u22122)\u22121 , h(v\u22121,j\u22121) \u03c4 (j\u22121) \u02dcn(j\u22121)\u2212\u02dcn(j\u22122)\u22121 , x(v\u22121,j\u22121) \u03c4 (j\u22121) \u02dcn(j\u22121)\u2212\u02dcn(j\u22122) , r(v\u22121,j\u22121) \u03c4 (j\u22121) \u02dcn(j\u22121)\u2212\u02dcn(j\u22122) \ufffd , \ufffd x(v\u22121,j) \u03c4 (j) 0 , h(v\u22121,j) \u03c4 (j) 0 , x(v\u22121,j) \u03c4 (j) 1 , r(v\u22121,j) \u03c4 (j) 1 , h(v\u22121,j) \u03c4 (j) 1 , . . . , x(v\u22121,j) \u03c4 (j) \u02dc K(v) f \u22121 , r(v\u22121,j) \u03c4 (j) \u02dc K(v) f \u22121 , h(v\u22121,j) \u03c4 (j) \u02dc K(v) f \u22121 , x(v\u22121,j) \u03c4 (j) \u02dc K(v) f , r(v\u22121,j) \u03c4 (j) \u02dc K(v) f , . . . \uf8fc \uf8fd \uf8fe , where \u02dcK(v) f = 1, 2, . . . , \u02dcn(j) \u2212 \u02dcn(j\u22121), such that the time \u03c4 (j) \u02dc K(v) f is when the last state is observed for the j-th policyholder in this update; necessarily, \u02dcn(j\u22121) \u2212 \u02dcn(i\u22121) + \u02dcK(v) f \u2212 \u02dcK(v) s = \u02dcK. Moreover, the \ufb01rst two features in the state vector (18) are based on the real-time risky asset price realization from the market, while all features depend on a particular e\ufb00ective policyholder. For \u03b9 \u2208 N and k = 0, 1, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121), X(v\u22121,\u03b9) \u03c4 (\u03b9) k = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \ufffd ln F (\u03b9) \u03c4 (\u03b9) k , P (\u03b9) \u03c4 (\u03b9) k , 1, T \u2212 \ufffd \u03c4 (\u03b9) k \u2212 \u03c4 (\u03b9) 0 \ufffd\ufffd if k = 0, 1, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) \u2212 1 \ufffd ln F (\u03b9) \u03c4 (\u03b9) k , P (\u03b9) \u03c4 (\u03b9) k , 0, T \u2212 \ufffd \u03c4 (\u03b9) k \u2212 \u03c4 (\u03b9) 0 \ufffd\ufffd if k = \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) and T (\u03b9) x\u03b9 \u2264 T \ufffd ln F (\u03b9) \u03c4 (\u03b9) k , P (\u03b9) \u03c4 (\u03b9) k , 1, 0 \ufffd if k = \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) and T (\u03b9) x\u03b9 > T , (19) where F (\u03b9) t = \u03c1(\u03b9)Ste \u2212m(\u03b9)\ufffd t\u2212\u03c4 (\u03b9) 0 \ufffd , if t \u2208 \ufffd \u03c4 (\u03b9) 0 , \u02dct\u02dcn(\u03b9) \ufffd , P (\u03b9) \u03c4 (\u03b9) 0 = 0, and P (\u03b9) \u03c4 (\u03b9) k = \ufffd P (\u03b9) \u03c4 (\u03b9) k\u22121 \u2212 H(\u03b9) \u03c4 (\u03b9) k\u22121S\u03c4 (\u03b9) k\u22121 \ufffd e r \ufffd \u03c4 (\u03b9) k \u2212\u03c4 (\u03b9) k\u22121 \ufffd + H(\u03b9) \u03c4 (\u03b9) k\u22121S\u03c4 (\u03b9) k + m(\u03b9) e \ufffd \u03c4 (\u03b9) k \u03c4 (\u03b9) k\u22121 F (\u03b9) s e r \ufffd \u03c4 (\u03b9) k \u2212s \ufffd J(\u03b9) s ds \u2212 \ufffd GD \u2212 F (\u03b9) T (\u03b9) x\u03b9 \ufffd + 1{\u03c4 (\u03b9) k\u22121<T (\u03b9) x\u03b9 \u2264\u03c4 (\u03b9) k }e r \ufffd \u03c4 (\u03b9) k \u2212T (\u03b9) x\u03b9 \ufffd , for k = 1, 2, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121). Recall also that the reward signals collecting from the market environment should be based on that in (8); that is, for \u03b9 \u2208 N and k = 0, 1, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121), R(v\u22121,\u03b9) \u03c4 (\u03b9) k = \uf8f1 \uf8f2 \uf8f3 0 if k = 0, 1, . . . , \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) \u2212 1 \u2212 \ufffd P (\u03b9) \u02dct\u02dcn(\u03b9) \u2212 L(\u03b9) \u02dct\u02dcn(\u03b9) \ufffd2 if k = \u02dcn(\u03b9) \u2212 \u02dcn(\u03b9\u22121) , in which L(\u03b9) \u02dct\u02dcn(\u03b9) = 0 if T (\u03b9) x\u03b9 \u2264 T, and L(\u03b9) \u02dct\u02dcn(\u03b9) = \ufffd GM \u2212 F (\u03b9) \u03c4 (\u03b9) 0 +T \ufffd + if T (\u03b9) x\u03b9 > T. Table 11 summarizes all hyperparameters of the implemented PPO in the market environment, while the hyperparameters of the ANN architecture are still given in Table 6b. In the online learning phase, the insurer should choose a smaller batch size \u02dcK comparing to that in the training phase; this yields a higher updating frequency by the PPO to ensure that the experienced RL agent could revise the hedging strategy within a reasonable amount of time. However, fewer realizations in the batch cause less credible updates; hence, the insurer should also tune down 23 the learning rate \u02dc\u03b1, from that in the training phase, to reduce the reliance on each further update step. Hyperparameter Value Hyperparameter Value Learning rate \u02dc\u03b1 0.001 Coe\ufb03cient of value function 0.25 Batch size \u02dcK 30 approximation loss c1 Clip factor \u03f5 0.18 Coe\ufb03cient of entropy bonus c2 0.01 Table 11: Hyperparameters setting of Proximal Policy Optimization for online learning with bolded hyperparameters being di\ufb00erent from those for training 6 Illustrative Example Revisited: Online Learning Phase This section revisits the illustrative example in Section 2.4 via the two-phase RL approach in the online learning phase. In the market environment, the policyholders being sequentially written of the contracts with both GMMB and GMDB riders are homogeneous. Due to contract re-establishments to these sequential homogeneous policyholders, the number and age of policyholders shall be reset to the values as in Table 3b at each contract inception time. Furthermore, via the approach discussed in Section 2.1.3, to determine the fee structures of each contract at its inception time, the insurer relies on the parameters of the model of the market environment in Table 3, except that now the risky asset initial price therein is replaced by the risky asset price observed at the contract inception time. Note that the fee structures of the \ufb01rst contract are still given as in Table 4, since the risky asset price observed at t = 0 is exactly the risky asset initial price. Let V \u2208 N be the number of further update steps in the market environment on the ANN weights. In order to showcase the result that, (RLw/OL) the further trained RL agent with the online learning phase, could gradually revise the hedging strategy, from the nearly optimal one in the training environment, to the one in the market environment, we evaluate the hedging performance of RLw/OL on a rolling-basis. That is, right after each further update step v = 1, 2, . . . , V, we \ufb01rst simulate \u02dc M = 500 market scenarios stemming from the real-time realized state vector x(v\u22121,j) \u03c4 (j) \u02dc K(v) f and by implementing the hedging strategy from the updated policy \u03c0 \ufffd \u00b7; \u03b8(U+v) p \ufffd , i.e. the further trained RL agent takes the deterministic action c \ufffd \u00b7; \u03b8(U+v) p \ufffd which is the mean of the Gaussian measure; we then document the realized terminal P&L, for each of the 500 simulated scenarios, i.e. P RLw/OL t (\u03c9e) \u2212 Lt (\u03c9e), for e = 1, 2, . . . , 500, where t = \u02dct\u02dcn(j) (\u03c9e) if \u03c4 (j) \u02dc K(v) f < \u02dct\u02dcn(j), and t = \u02dct\u02dcn(j+1) (\u03c9e) if \u03c4 (j) \u02dc K(v) f = \u02dct\u02dcn(j). Since the state vector x(v\u22121,j) \u03c4 (j) \u02dc K(v) f is realized in real time, the realized terminal P&L in fact depends on, not only the simulated scenarios after each update, but also the actual realization in the market environment. To this end, from the current time 0, we simulate M = 1000 future trajectories in the market environment; for each future trajectory f = 1, 2, . . . , 1000, the aforementioned realized terminal P&Ls are obtained as P RLw/OL t (\u03c9f, \u03c9e) \u2212 Lt (\u03c9f, \u03c9e), for e = 1, 2, . . . , 500, where t = \u02dct\u02dcn(j) (\u03c9f, \u03c9e) if \u03c4 (j) \u02dc K(v) f (\u03c9f) < \u02dct\u02dcn(j) (\u03c9f), and t = \u02dct\u02dcn(j+1) (\u03c9f, \u03c9e) if \u03c4 (j) \u02dc K(v) f (\u03c9f) = \u02dct\u02dcn(j) (\u03c9f). The rolling-basis hedging performance of RLw/OL is benchmarked with those by, (RLw/oOL) the trained RL agent without the online learning phase, (CD) the correct Delta based on the market environment, and (ID) the incorrect Delta based on the training environment. For the same set of future trajectories \u03c9f, for f = 1, 2, . . . , 1000, and the same sets of simulated scenarios \u03c9e, for e = 1, 2, . . . , 500, the realized terminal P&Ls are also obtained, by implementing each of these benchmark strategies starting from the current time 0, which does not need to be updated throughout; denote the realized terminal P&L as P S t (\u03c9f, \u03c9e) \u2212 Lt (\u03c9f, \u03c9e), where S = RLw/OL, RLw/oOL, CD, or ID. This example considers V = 25 further update steps of RLw/OL, for each future trajectory \u03c9f, where f = 1, 2, . . . , 1000; as the batch size in the online learning phase \u02dcK = 30, this is equivalent to 750 trading days, which is just less than 3 years (assuming that non-trading days are uniformly spread across a year). For each f = 1, 2, . . . , 1000, and v = 1, 2, . . . , 25, let \u00b5(v,j) S (\u03c9f) be the expected terminal P&L, right after the v-th further update step implementing the hedging strategy S for the future trajectory \u03c9f: \u00b5(v,j) S (\u03c9f) = E \uf8ee \uf8f0P S t (\u03c9f, \u00b7) \u2212 Lt (\u03c9f, \u00b7) \ufffd\ufffd\ufffdX(v\u22121,j) \u03c4 (j) \u02dc K(v) f = X(v\u22121,j) \u03c4 (j) \u02dc K(v) f (\u03c9f) \uf8f9 \uf8fb , which is a conditional expectation taking with respect to the scenarios from the time \u03c4 (j) \u02dc K(v) f forward; let \u02c6\u00b5(v,j) S (\u03c9f) 24 be the sample mean of the terminal P&L based on the simulated scenarios: \u02c6\u00b5(v,j) S (\u03c9f) = 1 500 500 \ufffd e=1 \ufffd P S t (\u03c9f, \u03c9e) \u2212 Lt (\u03c9f, \u03c9e) \ufffd . (20) Figure 8 plots the sample means of the terminal P&L in (20), right after each further update step and implementing each hedging strategy, in two future trajectories. Firstly, notice that, in both future trajectories, the average hedging performance of RLw/oOL is even worse than that of ID. Secondly, the average hedging performances of RLw/OL between the two future trajectories are substantially di\ufb00erent. In the best-case future trajectory, the RLw/OL is able to swiftly self-revise the hedging strategy, and hence quickly catch up the average hedging performance of ID by simply twelve further updates on the ANN weights, as well as that of CD in around two years; however, in the worst-case future trajectory, within 3 years, the RLw/OL is not able to improve the average hedging performance to even the level of ID, let alone to that of CD. Figure 8: Best-case and worst-case samples of future trajectories for rolling-basis evaluation of reinforcement learning agent with online learning phase, and comparisons with classical Deltas and reinforcement learning agent without online learning phase In view of the second observation above, the hedging performance of RLw/OL should not be concluded for each future trajectory alone; instead, it should be studied among the future trajectories. To this end, for each f = 1, 2, . . . , 1000, de\ufb01ne vCD (\u03c9f) = min \ufffd v = 1, 2, . . . , 25 : \u02c6\u00b5(v,j) RLw/OL (\u03c9f) > \u02c6\u00b5(v,j) CD (\u03c9f) \ufffd as the \ufb01rst further update step such that the sample mean of the terminal P&L by RLw/OL is strictly greater than that by CD, for the future trajectory \u03c9f; herein, let min \u2205 = 26, and also de\ufb01ne tCD (\u03c9f) = vCD (\u03c9f) \u00d7 \u02dc K 252 as the 25 corresponding number of years. Therefore, the estimated proportion of the future trajectories, where RLw/OL is able to exceed the average hedging performance of CD within 3 years, is given by 1 1000 1000 \ufffd f=1 1{tCD(\u03c9f )\u22643} = 95.4%. For each f = 1, 2, . . . , 1000, de\ufb01ne vID (\u03c9f) and tID (\u03c9f) similarly for comparing RLw/OL with ID. Figure 9 shows the empirical conditional density functions of tCD and tID, both subject to that RLw/OL exceeds the average hedging performance of CD within 3 years. Table 12 lists the summary statistics of the empirical conditional distributions. Figure 9: Empirical conditional density functions of \ufb01rst surpassing times conditioning on reinforcement learning agent with online learning phase exceeding correct Delta in terms of sample means of terminal P&L within 3 years Reinforcement Learning Agent Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 with Online Learning Phase First Surpassing Time to Correct Delta 1.84 1.79 0.32 2.38 2.50 2.66 2.73 Incorrect Delta 1.41 1.43 0.22 1.67 1.67 2.05 2.05 Table 12: Summary statistics of empirical conditional distributions of \ufb01rst surpassing times conditioning on reinforcement learning agent with online learning phase exceeding correct Delta in terms of sample means of terminal P&L within 3 years The above analysis obviously neglected the variance, due to the simulated scenarios, of hedging performance by each hedging strategy. In the following, for each future trajectory, we de\ufb01ne a re\ufb01ned \ufb01rst further update step such that the expected terminal P&L by RLw/OL is statistically signi\ufb01cant to be strictly greater than that by CD. To this end, for each f = 1, 2, . . . , 1000, and v = 1, 2, . . . , 25, consider the following null and alternative hypotheses: H(v,j) 0,S (\u03c9f) : \u00b5(v,j) RLw/OL (\u03c9f) \u2264 \u00b5(v,j) S (\u03c9f) versus H(v,j) 1,S (\u03c9f) : \u00b5(v,j) RLw/OL (\u03c9f) > \u00b5(v,j) S (\u03c9f) , where S = CD or ID; the analysis before supports this choice of the alternative hypothesis. De\ufb01ne respectively the test statistics and the p-value by T (v,j) S (\u03c9f) = \u02c6\u00b5(v,j) RLw/OL (\u03c9f) \u2212 \u02c6\u00b5(v,j) S (\u03c9f) \ufffd \u02c6\u03c3(v,j) RLw/OL(\u03c9f )2 500 + \u02c6\u03c3(v,j) S (\u03c9f )2 500 and p(v,j) S (\u03c9f) = P \ufffd TS (\u03c9f) > T (v,j) S (\u03c9f) \ufffd , 26 where the random variable TS (\u03c9f) follows a Student\u2019s t-distribution with the degree of freedom df(v,j) S (\u03c9f) = \ufffd \u02c6\u03c3(v,j) RLw/OL(\u03c9f )2 500 + \u02c6\u03c3(v,j) S (\u03c9f )2 500 \ufffd2 \ufffd \u02c6\u03c3(v,j) RLw/OL(\u03c9f )2/500 \ufffd2 500\u22121 + \ufffd \u02c6\u03c3(v,j) S (\u03c9f )2/500 \ufffd2 500\u22121 , and the sample variance \u02c6\u03c3(v,j) S (\u03c9f)2 of the terminal P&L based on the simulated scenarios is given by \u02c6\u03c3(v,j) S (\u03c9f)2 = 1 499 500 \ufffd e=1 \ufffd\ufffd P S t (\u03c9f, \u03c9e) \u2212 Lt (\u03c9f, \u03c9e) \ufffd \u2212 \u02c6\u00b5(v,j) S (\u03c9f) \ufffd2 . For a \ufb01xed level of signi\ufb01cance \u03b1\u2217 \u2208 (0, 1), if p(v,j) S (\u03c9f) < \u03b1\u2217, then the expected terminal P&L by RLw/OL is statistically signi\ufb01cant to be strictly greater than that by S = CD or ID. In turn, for each f = 1, 2, . . . , 1000, and for any \u03b1\u2217 \u2208 (0, 1), de\ufb01ne vp S (\u03c9f; \u03b1\u2217) = min \ufffd v = 1, 2, . . . , 25 : p(v,j) S (\u03c9f) < \u03b1\u2217\ufffd as the \ufb01rst further update step such that the expected terminal P&L by RLw/OL is statistically signi\ufb01cant to be strictly greater than that by S = CD or ID, for the future trajectory \u03c9f at the level of signi\ufb01cance \u03b1\u2217; again, herein, let min \u2205 = 26, and de\ufb01ne tp S (\u03c9f; \u03b1\u2217) = vp S (\u03c9f; \u03b1\u2217) \u00d7 \u02dc K 252 as the corresponding number of years. Table 13 lists the estimated proportion of the future trajectories, where RLw/OL is statistically signi\ufb01cant to be able to exceed the expected terminal P&L of S within 3 years, which is given by \ufffd1000 f=1 1{tp S(\u03c9f ;\u03b1\u2217)\u22643}/1000, with various levels of signi\ufb01cance. Estimated Proportion \u03b1\u2217 = 0.20 \u03b1\u2217 = 0.15 \u03b1\u2217 = 0.10 \u03b1\u2217 = 0.05 \u03b1\u2217 = 0.01 of Exceeding Correct Delta 55.7% 52.1% 47.6% 35.9% 21.8% Incorrect Delta 96.9% 95.1% 85.0% 70.6% 64.6% Table 13: Estimated proportions of future trajectories where reinforcement learning agent with online learning phase is statistically signi\ufb01cant to be exceeding correct Delta and incorrect Delta within 3 years with various levels of signi\ufb01cance When the level of signi\ufb01cance \u03b1\u2217 gradually decreases from 0.20 to 0.01, both estimated proportions, of the future trajectories for RLw/OL being statistically signi\ufb01cant to be exceeding CD or ID within 3 years, decline. This is because, for any \u03b1\u2217 1, \u03b1\u2217 2 \u2208 (0, 1) with \u03b1\u2217 1 \u2264 \u03b1\u2217 2, and for any \u03c9f, for f = 1, 2, . . . , 1000, tp S (\u03c9f; \u03b1\u2217 1) \u2264 3 implies that tp S (\u03c9f; \u03b1\u2217 2) \u2264 3, and thus 1{tp S(\u03c9f ;\u03b1\u2217 1)\u22643} \u2264 1{tp S(\u03c9f ;\u03b1\u2217 2)\u22643}, which leads to that \ufffd1000 f=1 1{tp S(\u03c9f ;\u03b1\u2217 1)\u22643}/1000 \u2264 \ufffd1000 f=1 1{tp S(\u03c9f ;\u03b1\u2217 2)\u22643}/1000; indeed, since tp S (\u03c9f; \u03b1\u2217 1) \u2264 3, or equivalently vp S (\u03c9f; \u03b1\u2217 1) \u2264 25, we have p(vp S(\u03c9f ;\u03b1\u2217 1),j) S (\u03c9f) < \u03b1\u2217 1 \u2264 \u03b1\u2217 2, and thus vp S (\u03c9f; \u03b1\u2217 2) = min \ufffd v = 1, 2, . . . , 25 : p(v,j) S (\u03c9f) < \u03b1\u2217 2 \ufffd \u2264 vp S (\u03c9f; \u03b1\u2217 1) \u2264 25, or equivalently tp S (\u03c9f; \u03b1\u2217 2) \u2264 tp S (\u03c9f; \u03b1\u2217 1) \u2264 3. However, notably, the declining rate of the estimated proportion for exceeding CD is greater than that for exceeding ID. Similar to Figure 9 and Table 12, one can depict the empirical conditional density functions and list the summary statistics of tp CD (\u00b7; \u03b1\u2217) and tp ID (\u00b7; \u03b1\u2217), for each level of signi\ufb01cance \u03b1\u2217, subject to that RLw/OL is statistically signi\ufb01cant to be exceeding CD within 3 years. For example, with \u03b1\u2217 = 0.1, Figure 10 and Table 14 illustrate that, comparing with Figure 9 and Table 12, the distributions are right-shifted as well as more spread, and the summary statistics are all increased. 27 Figure 10: Empirical conditional density functions of \ufb01rst statistically signi\ufb01cant surpassing times conditioning on reinforcement learning agent with online learning phase being statistically signi\ufb01cant to be exceeding correct Delta within 3 years for 0.1 level of signi\ufb01cance Reinforcement Learning Agent Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 with Online Learning Phase First Surpassing Time to Correct Delta 1.92 1.90 0.34 2.50 2.62 2.61 2.70 Incorrect Delta 1.70 1.55 0.24 2.02 2.14 2.07 2.20 Table 14: Summary statistics of empirical conditional distributions of \ufb01rst statistically signi\ufb01cant surpassing times conditioning on reinforcement learning agent with online learning phase being statistically signi\ufb01cant to be exceeding correct Delta within 3 years for 0.1 level of signi\ufb01cance Finally, to further examine the hedging performance of RLw/OL in terms of the sample mean of the terminal P&L in (20), as well as take the random future trajectories into account, Figure 11 shows the snapshots of the empirical density functions, among the future trajectories, of the sample mean by each hedging strategy over time at t = 0, 0.6, 1.2, 1.8, 2.4 and 3; Table 15 outlines their summary statistics. Note that, at the current time t = 0, since none of the future trajectories has been realized yet, the empirical density functions are given by Dirac delta at the corresponding sample mean by each hedging strategy, which only depends on the simulated scenarios. As the time progresses, one can observe that the empirical density function by RLw/OL is gradually shifting to the right, substantially passing the one by ID and almost catching up the one by CD at t = 1.8. This sheds light on the high probability that RLw/OL is able to self-revise the hedging strategy from a very sub-optimal one to a nearly optimal one close to the CD. 28 (a) t = 0 (b) t = 0.6 (c) t = 1.2 (d) t = 1.8 (e) t = 2.4 (f) t = 3 Figure 11: Snapshots of empirical density functions of sample mean of terminal P&L by reinforcement learning agent with online learning phase, reinforcement learning agent without online learning phase, correct Delta, and incorrect Delta at di\ufb00erent time points 29 Sample Mean of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 Terminal P&L by RL with OL \u22122.66 \u22122.66 0 \u22122.66 \u22122.66 \u22122.66 \u22122.66 RL without OL \u22122.66 \u22122.66 0 \u22122.66 \u22122.66 \u22122.66 \u22122.66 Correct Delta \u22120.26 \u22120.26 0 \u22120.26 \u22120.26 \u22120.26 \u22120.26 Incorrect Delta \u22121.01 \u22121.01 0 \u22121.01 \u22121.01 \u22121.01 \u22121.01 (a) t = 0 Sample Mean of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 Terminal P&L by RL with OL \u22122.27 \u22122.19 0.45 \u22122.86 \u22123.11 \u22123.17 \u22123.38 RL without OL \u22122.71 \u22122.69 0.42 \u22123.20 \u22123.39 \u22123.52 \u22123.76 Correct Delta \u22120.24 \u22120.26 0.15 \u22120.40 \u22120.45 \u22120.46 \u22120.49 Incorrect Delta \u22120.99 \u22120.99 0.16 \u22121.20 \u22121.26 \u22121.27 \u22121.31 (b) t = 0.6 Sample Mean of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 Terminal P&L by RL with OL \u22121.29 \u22121.14 0.55 \u22121.83 \u22122.75 \u22122.80 \u22123.10 RL without OL \u22122.71 \u22122.68 0.42 \u22123.22 \u22123.45 \u22123.54 \u22123.78 Correct Delta \u22120.24 \u22120.26 0.14 \u22120.41 \u22120.44 \u22120.45 \u22120.48 Incorrect Delta \u22120.99 \u22120.99 0.16 \u22121.19 \u22121.25 \u22121.27 \u22121.33 (c) t = 1.2 Sample Mean of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 Terminal P&L by RL with OL \u22120.63 \u22120.43 0.69 \u22121.14 \u22122.50 \u22122.52 \u22122.94 RL without OL \u22122.70 \u22122.67 0.43 \u22123.22 \u22123.42 \u22123.58 \u22123.86 Correct Delta \u22120.25 \u22120.27 0.15 \u22120.41 \u22120.45 \u22120.47 \u22120.50 Incorrect Delta \u22120.99 \u22120.99 0.15 \u22121.20 \u22121.25 \u22121.27 \u22121.31 (d) t = 1.8 Sample Mean of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 Terminal P&L by RL with OL \u22120.46 \u22120.33 0.71 \u22120.72 \u22122.24 \u22122.13 \u22123.24 RL without OL \u22122.69 \u22122.66 0.41 \u22123.18 \u22123.40 \u22123.48 \u22123.69 Correct Delta \u22120.24 \u22120.26 0.15 \u22120.40 \u22120.45 \u22120.46 \u22120.50 Incorrect Delta \u22120.98 \u22120.98 0.15 \u22121.18 \u22121.24 \u22121.26 \u22121.30 (e) t = 2.4 Sample Mean of Mean Median Std. Dev. VaR90 VaR95 TVaR90 TVaR95 Terminal P&L by RL with OL \u22120.45 \u22120.33 0.56 \u22120.66 \u22121.66 \u22121.75 \u22122.59 RL without OL \u22122.71 \u22122.68 0.41 \u22123.24 \u22123.38 \u22123.49 \u22123.68 Correct Delta \u22120.24 \u22120.26 0.15 \u22120.40 \u22120.44 \u22120.46 \u22120.49 Incorrect Delta \u22120.99 \u22120.99 0.15 \u22121.19 \u22121.24 \u22121.26 \u22121.31 (f) t = 3 Table 15: Summary statistics of empirical distributions of sample mean of terminal P&L by reinforcement learning agent with online learning phase, reinforcement learning agent without online learning phase, correct Delta, and incorrect Delta at di\ufb00erent time points 30 7 Methodological Assumptions and Implications in Practice To apply the proposed two-phase RL approach to a hedging problem of contingent claims, there are at least four assumptions to be satis\ufb01ed. This section discusses these assumptions and elaborates their implications in practice. 7.1 Observable, Su\ufb03cient, Relevant, and Transformed Features in State One of the crucial components in an MDP environment of the training phase or the online learning phase is the state, in which the features provide information from the environment to the RL agent. First, the features must be observable by the RL agent for learning. For instance, in our proposed state vectors (18) and (19), all the four features, namely, the segregated account value, the hedging portfolio value, the number of surviving policyholders, and the term to maturity, are observable. Any unobservable, albeit desirable, features cannot be included in the state, such as insider information which could provide a better inference on the future value of a risky asset, or exact health condition of a policyholder. Second, the observable features in the state should be su\ufb03cient for the RL agent to learn. For example, due to the dual-risk bearing nature of the contract in this paper, the proposed state vectors (18) and (19) incorporate both \ufb01nancial and actuarial features; also, the third and the fourth features in the state vectors (18) and (19) would inform the RL agent to halt its hedging at the terminal time. However, incorporating su\ufb03cient observable features in the state does not imply that every observable feature in the environment should be included; the observable features in the state need to be relevant for learning e\ufb03ciently. Since the segregated account value and the term to maturity have already been included in the state vectors (18) and (19) as features, the risky asset value and the hedging time are respective similar information from the environment, and thus are redundant features to be contained in the state. Finally, the features in the state which have high variance might be appropriately transformed for reducing the volatility due to exploration. For instance, the segregated account value in the state vectors (18) and (19) is log-transformed in both phases. 7.2 Reward Engineering Another crucial component in an MDP environment is the reward, which supplies signals to the RL agent to evaluate its actions, i.e. the hedging strategy, for learning. First, the reward signals, if available, should suggest the local hedging performance. For example, in this paper, the RL agent is provided by the sequential anchor-hedging reward, given in (9), in the training phase; through the net liability value in the MDP training environment, the RL agent often receives a positive (resp. negative) signal for encouragement (resp. punishment), which is more informative than collecting the zero reward. However, any informative reward signals need to be computable from an MDP environment. In this paper, since the insurer does not know the MDP market environment, the RL agent could not be supplied the sequential anchor-hedging reward signals, which consist of the net liability values, in the online learning phase, even though they are more informative; instead, the RL agent is given the less informative single terminal reward, given in (8), in the online learning phase which can be computed from the market environment. 7.3 Markov Property in State and Action In an MDP environment of the training phase or the online learning phase, the state and action pair needs to satisfy the Markov property as in (3). In the training phase, since the MDP training environment is constructed, the Markov property can be veri\ufb01ed theoretically for the state, with the included features in line with Section 7.1, and the action, which is the hedging strategy. For example, in this paper, with the model of the market environment being the BS and the CFM, the state vector in (18) and the Markovian hedging strategy satisfy the Markov property in the training phase. Since the illustrative example in this paper assumes that the market environment also follows the BS and the CFM, the state vector in (19) and the Markovian hedging strategy satisfy the Markov property in the online learning phase as well. However, in general, as the market environment is unknown, the Markov property for the state and action pair would need to be checked statistically in the online phase as follows. After the training phase and before an RL agent proceeding to the online learning phase, historical state and action sequences in a time frame are derived by hypothetically writing identical contingent claims and using the historical realizations from the market environment. For instance, historical values of risky assets are publicly available, or an insurer retrieves historical survival status of its policyholders with similar demographic information and medical history as the policyholder being actually written. These historical samples of the state and action pair are then used to conduct hypothesis testing on whether the Markov property in (3) holds for the pair in the market environment, by, for example, the test statistics proposed in Chen and Hong (2012). If the Markov property holds statistically, the RL agent could begin the online learning phase. Yet, if the property does not hold statistically, the state and action pair should be revised and then the training phase should be revisited; since the hedging strategy is the action in a hedging problem, only the state could be amended by including more features from the environment. Moreover, during the online learning phase, right after each further update step, new historical state and action sequences in a shifted time frame of the same duration are obtained together with the most recent 31 historical realizations from the market environment and using the action samples being drawn from the updated policy. These regularly new samples should be applied to statistically verify the Markov property on a rolling basis. If the property fails to hold at any time, the state needs to be revised and the RL agent must be re-trained before resuming the online learning. 7.4 Re-Establishment of Contingent Claims in Online Learning Phase Any contingent claims must have a \ufb01nite terminal time realization. On one hand, in the training phase, that would be the time when an episode ends and the state is re-initialized so that the RL agent can be trained in the training environment as long as possible. On the other hand, in the online learning phase, the market environment, and hence the state, could not be re-initialized; instead, at each terminal time realization, the seller re-establishes identical contingent claims of the same contract characteristics and writing on (more or less) the same assets so that the RL agent can be trained in the market environment successively. In this paper, the terms to maturity and the minimum guarantees of all variable annuity contracts in the online learning phase are the same. Moreover, all re-established contracts therein write on the same \ufb01nancial risky asset, though the initial values of the asset are given by the real-time realizations in the market environment. Finally, while a new policyholder is written at each contract inception time, these policyholders have similar, if not identical, distributions of their random future lifetimes via examining their demographic information and medical history. 8 Concluding Remarks and Future Directions This paper proposed the two-phase deep RL approach which can tackle practically common model miscalibration in hedging variable annuity contracts with both GMMB and GMDB riders in the BS \ufb01nancial and CFM actuarial market environments. The approach is composed of the training phase and the online learning phase. While the satisfactory hedging performance of the trained RL agent in the training environment was anticipated, the performance by the further trained RL agent in the market environment via the illustrative example should be highlighted. First, by comparing their sample means of terminal P&L from simulated scenarios, in most future trajectories, within a reasonable amount of time, the further trained RL agent was able to exceed the hedging performance by the correct Delta from the market environment and the incorrect Delta from the training environment. Second, through a more delicate hypothesis testing analysis, similar conclusions can be drawn in a fair amount of future trajectories. Finally, snapshots of empirical density functions, among the future trajectories, of the sample means of terminal P&L from simulated scenarios by each hedging strategy, shed light on the high probability that, the further trained RL agent is indeed able to self-revise the hedging strategy. There should be at least two future directions derived from this paper. (I) The market environment in the illustrative example of this paper was assumed to be the BS \ufb01nancial and CFM actuarial models, which turned out to be the same as designed by the insurer for the training environment, with di\ufb00erent parameters though. Moreover, the policyholders were assumed to be homogeneous that their survival probabilities and investment behaviors are all the same, with even identical contracts of the same minimum guarantee and maturity. In the market environment, the agent only had to hedge one contract at a time, instead of a portfolio of contracts. Obviously, if any of these is to be relaxed, the trained RL agent from the current training environment should not be able to produce satisfactory hedging performance in a market environment. Therefore, the training environment will certainly need to be substantially extended in terms of its sophistication, in order for the trained RL agent to be able to further learn and hedge well in any realistic market environments. (II) Beyond this, an even more ambitious question needs to be addressed is that how much similar do the training and market environments have to be, such that the online learning for self-revision on hedging strategy is possible, if not e\ufb03cient. This second future direction is related to the transfer learning being adapted to the variable annuities hedging problem, and shall be investigated carefully in the future. References Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2018). Automatic di\ufb00erentiation in machine learning: A survey. Journal of Machine Learning Research, 18(2):1\u201343. Bertsimas, D., Kogan, L., and Lo, A. W. (2000). When is time continuous? Journal of Financial Economics, 55(2):173\u2013204. B\u00a8uhler, H., Gonon, L., Teichmann, J., and Wood, B. (2019). Deep hedging. Quantitative Finance, 19(8):1271\u20131291. Cao, J., Chen, J., Hull, J., and Poulos, Z. (2021). Deep hedging of derivatives using reinforcement learning. Journal of Financial Data Science, 3(1):10\u201327. 32 Carbonneau, A. (2021). Deep hedging of long-term \ufb01nancial derivatives. Insurance: Mathematics and Economics, 99:327\u2013340. Charpentier, A., \u00b4Elie, R., and Remlinger, C. (2021). Reinforcement learning in economics and \ufb01nance. Computational Economics (2021). Chen, B., and Hong, Y. (2012). Testing for the Markov property in time series. Econometric Theory, 28:130\u2013178. Chen, Z., Vetzal, K., and Forsyth, P. (2008). The e\ufb00ect of modelling parameters on the value of GMWB guarantees. Insurance: Mathematics and Economics, 43(1):165\u2013173. Cheridito, P., Ery, J., and W\u00a8uthrich, M. V. (2020). Assessing asset-liability risk with neural networks. Risks, 8(1) article 16. Chong, W. F. (2019). Pricing and hedging equity-linked life insurance contracts beyond the classical paradigm: The principle of equivalent forward preferences. Insurance: Mathematics and Economics, 88:93\u2013107. Cui, Z., Feng, R., and MacKay, A. (2017). Variable annuities with VIX-linked fee structure under a Heston-type stochastic volatility model. North American Actuarial Journal, 21(3):458\u2013483. Dai, M., Kwok, Y. K., and Zong, J. (2008). Guaranteed minimum withdrawal bene\ufb01t in variable annuities. Mathematical Finance, 18(4):595\u2013611. Dang, O., Feng, M., and Hardy, M. R. (2020). E\ufb03cient nested simulation for conditional tail expectation of variable annuities. North American Actuarial Journal, 24(2):187\u2013210. Dang, O., Feng, M., and Hardy, M. R. (2022). Dynamic importance allocated nested simulation for variable annuity risk measurement. Annals of Actuarial Science, 16(2):319\u2013348. Feng, B. M., Tan, Z., and Zheng, J. (2020). E\ufb03cient simulation designs for valuation of large variable annuity portfolios. North American Actuarial Journal, 24(2):275\u2013289. Feng, R. (2018). An Introduction to Computational Risk Management of Equity-Linked Insurance. CRC Press. Feng, R. and Yi, B. (2019). Quantitative modeling of risk management strategies: Stochastic reserving and hedging of variable annuity guaranteed bene\ufb01ts. Insurance: Mathematics and Economics, 85:60\u201373. Gan, G. (2013). Application of data clustering and machine learning in variable annuity valuation. Insurance: Mathematics and Economics, 53(3):795\u2013801. Gan, G. (2018). Valuation of large variable annuity portfolios using linear models with interactions. Risks, 6(3):1\u201319. Gan, G. and Lin, X. S. (2015). Valuation of large variable annuity portfolios under nested simulation: A functional data approach. Insurance: Mathematics and Economics, 62:138\u2013150. Gan, G. and Lin, X. S. (2017). E\ufb03cient Greek calculation of variable annuity portfolios for dynamic hedging: A two-level metamodeling approach. North American Actuarial Journal, 21(2):161\u2013177. Gan, G. and Valdez, E. A. (2017). Modeling partial Greeks of variable annuities with dependence. Insurance: Mathematics and Economics, 76:118\u2013134. Gan, G. and Valdez, E. A. (2018). Regression modeling for the valuation of large variable annuity portfolios. North American Actuarial Journal, 22(1):40\u201354. Gan, G. and Valdez, E. A. (2020). Valuation of large variable annuity portfolios with rank order kriging. North American Actuarial Journal, 24(1):100\u2013117. Gao, G. and W\u00a8uthrich, M. V. (2019). Convolutional neural network classi\ufb01cation of telematics car driving data. Risks, 7(1) article 6. Gweon, H., Li, S., and Mamon, R. (2020). An e\ufb00ective bias-corrected bagging method for the valuation of large variable annuity portfolios. ASTIN Bulletin: The Journal of the International Actuarial Association, 50(3):853\u2013871. Hardy, M. (2003). Investment Guarantees: Modeling and Risk Management for Equity-Linked Life Insurance. John Wiley & Sons, Inc. Hasselt, H. (2010). Double Q-learning. In Advances in Neural Information Processing Systems, volume 23. Hejazi, S. A. and Jackson, K. R. (2016). A neural network approach to e\ufb03cient valuation of large portfolios of variable annuities. Insurance: Mathematics and Economics, 70:169\u2013181. 33 Hu, C., Quan, Z., and Chong, W. F. (2022). Imbalanced learning for insurance using modi\ufb01ed loss functions in tree-based models. Insurance: Mathematics and Economics, 106:13\u201332. Jeon, J. and Kwak, M. (2018). Optimal surrender strategies and valuations of path-dependent guarantees in variable annuities. Insurance: Mathematics and Economics, 83:93\u2013109. Kindratenko, V., Mu, D., Zhan, Y., Maloney, J., Hashemi, S. H., Rabe, B., Xu, K., Campbell, R., Peng, J., and Gropp, W. (2020). HAL: Computer system for scalable deep learning. pages 41\u201348. In Practice and Experience in Advanced Research Computing (PEARC \u201920). Kolm, P. N. and Ritter, G. (2019). Dynamic replication and hedging: A reinforcement learning approach. Journal of Financial Data Science, 1(1):159\u2013171. Lin, X. S. and Yang, S. (2020). Fast and e\ufb03cient nested simulation for large variable annuity portfolios: A surrogate modeling approach. Insurance: Mathematics and Economics, 91:85\u2013103. Liu, K. and Tan, K. S. (2020). Real-time valuation of large variable annuity portfolios: A green mesh approach. North American Actuarial Journal, 25(3):313-333. Milevsky, M. A. and Posner, S. E. (2001). The Titanic option: Valuation of the guaranteed minimum death bene\ufb01t in variable annuities and mutual funds. The Journal of Risk and Insurance, 68(1):93\u2013128. Milevsky, M. A. and Salisbury, T. S. (2006). Financial valuation of guaranteed minimum withdrawal bene\ufb01ts. Insurance: Mathematics and Economics, 38(1):21\u201338. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing Atari with deep reinforcement learning. arXiv: 1312.5602. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518:529\u2013533. Moenig, T. (2021a). E\ufb03cient valuation of variable annuity portfolios with dynamic programming. Journal of Risk and Insurance, 88(4):1023\u20131055. Moenig, T. (2021b). Variable annuities: Market incompleteness and policyholder behavior. Insurance: Mathematics and Economics, 99:63\u201378. Perla, F., Richman, R., Scognamiglio, S. and W\u00a8uthrich, M. V. (2021). Time-series forecasting of mortality rates using deep learning. Scandinavian Actuarial Journal, 7:572\u2013598. Quan, Z., Gan, G., and Valdez, E. (2021). Tree-based models for variable annuity valuation: Parameter tuning and empirical analysis. Annals of Actuarial Science, 16(1):95-118. Richman, R. and W\u00a8uthrich, M. V. (2021). A neural network extension of the Lee-Carter model to multiple populations. Annals of Actuarial Science, 15(2):346\u2013366. Schulman, J., Levine, S., Moritz, P., Jordan, M., and Abbeel, P. (2015). Trust region policy optimization. arXiv: 1502.05477. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv: 1707.06347. Trottier, D. A., Godin, F., and Hamel, E. (2018). Local hedging of variable annuities in the presence of basis risk. ASTIN Bulletin: The Journal of the International Actuarial Association, 48(2):611\u2013646. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., baker, L., Lai, M., bolton, A., chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and Hassabis, D. (2017). Mastering the game of Go without human knowledge. Nature, 550:354\u2013359. Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts. Sutton, R. S. (1988). Learning to predict by the methods of temporal di\ufb00erences. Machine Learning, 3:9\u201344. Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press. 34 Wang, G. and Zou, B. (2021). Optimal fee structure of variable annuities. Insurance: Mathematics and Economics, 101:587\u2013601. Wang, H., Zariphopoulou, T., and Zhou, X. (2020). Reinforcement learning in continuous time and space: A stochastic control approach. Journal of Machine Learning Research, 21:1\u201334. Wang, H. and Zhou, X. (2020). Continuous-time mean-variance portfolio selection: A reinforcement learning framework. Mathematical Finance, 30(4):1273\u20131308. Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, University of Cambridge. Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8:297\u2013292. Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229\u2013256. W\u00a8uthrich, M. V. (2018). Neural networks applied to chain-ladder reserving. European Actuarial Journal, 8:407\u2013436. Xu, W., Chen, Y., Coleman, C., and Coleman, T. F. (2018). Moment matching machine learning methods for risk management of large variable annuity portfolios. Journal of Economic Dynamics and Control, 87:1\u201320. Xu, X. (2020). Variable Annuity Guaranteed Bene\ufb01ts: An Integrated Study of Financial Modelling, Actuarial Valuation and Deep Learning. PhD thesis, UNSW Business School. 35 ",
    "Approach": "Approach In this section, we provide a brief review of the DH approach adapted from B\u00a8uhler et al. (2019). In particular, the hedging objective of the insurer is still given as \ufffd E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd , with Equation (2) being the optimal (discrete) hedging strategy. The hedging agent built by the insurer using the DH algorithm shall be called the DH agent hereafter. A.1 Deterministic Action Di\ufb00erent from Section 3.1, in which the RL agent takes a stochastic action which is sampled from the policy for the exploration in the MDP environment, the DH agent only deploys a deterministic action HDH : X \u2192 A, which is a direct mapping from the state space to the action space. Speci\ufb01cally, at each time tk, where k = 0, 1, . . . , n \u2212 1, given the current state Xtk \u2208 X, the DH agent takes an action HDH (Xtk) \u2208 A. In this case, the objective of the DH agent is to solve for the optimal hedging strategy HDH,\u2217 (\u00b7) that minimizes \ufffd E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd , or equivalently minimizes E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd . A.2 Action Approximation and Parameterization The deterministic action mapping HDH : X \u2192 A is then approximated and parameterized by an ANN with weights \u03c5a. The construction of such ANN Na (\u00b7; \u03c5a) is similar to that in Section 3.3.1, except that Na (x; \u03c5a) \u2208 R for any x \u2208 Rp; that is, Na (\u00b7; \u03c5a) takes a state vector x \u2208 Rp as the input, and directly outputs a deterministic action a (x; \u03c5a) \u2208 R, instead of the Gaussian mean-variance tuple \ufffd c (x; \u03c5a) , d2 (x; \u03c5a) \ufffd \u2208 R \u00d7 R+ in the RL approach, which then samples an action from the Gaussian measure. Hence, in the DH approach, solving the optimal hedging strategy HDH,\u2217 (\u00b7) boils down to \ufb01nding the optimal weights \u03c5\u2217 a. A.3 Deep Hedging Method The DH agent starts from initial ANN weights \u03c5(0) a , deploys the hedging strategy to collect terminal P&Ls, and gradually updates the ANN weights by stochastic gradient ascent as shown in Equation (13), with \u03b8 replaced by \u03c5. For the DH agent, at each update step u = 1, 2, . . . , the surrogate performance measure is given as J (u\u22121) \ufffd \u03c5(u\u22121) a \ufffd = \u2212E \ufffd\ufffd P (u\u22121) t\u02dcn \u2212 L(u\u22121) t\u02dcn \ufffd2\ufffd . Correspondingly, the gradient of the surrogate performance measure with respect to the ANN weights \u03c5a is \u2207\u03c5aJ (u\u22121) \ufffd \u03c5(u\u22121) a \ufffd = \u22122E \ufffd\ufffd P (u\u22121) t\u02dcn \u2212 L(u\u22121) t\u02dcn \ufffd \u2207\u03c5aP (u\u22121) t\u02dcn \ufffd . Therefore, based on the realized terminal P&L p(u\u22121) t\u02dcn and l(u\u22121) t\u02dcn , the estimated gradient is given as \ufffd \u2207\u03c5aJ (u\u22121) \ufffd \u03c5(u\u22121) a \ufffd = \u22122 \ufffd p(u\u22121) t\u02dcn \u2212 l(u\u22121) t\u02dcn \ufffd \u2207\u03c5ap(u\u22121) t\u02dcn . Algorithm 1 summarizes the DH method above. Algorithm 1 Pseudo-code for deep hedging method Input initial ANN model Na \ufffd \u00b7; \u03c5(0) a \ufffd , total number of updates \u02c6 M \u2208 N, learning rate \u02c6\u03b1 \u2208 [0, 1]. for u = 1, 2, \u00b7 \u00b7 \u00b7 , \u02c6 M do \u00b7 Initialize the MDP training environment and observe the initial state vector x(u\u22121) t0 . \u00b7 Follow the hedging strategy Na \ufffd \u00b7; \u03c5(u\u22121) a \ufffd to realize an episode and evaluate the terminal P&L p(u\u22121) t\u02dcn and l(u\u22121) t\u02dcn . \u00b7 Update \u03c5(u\u22121) a as \u03c5(u) a = \u03c5(u\u22121) a \u2212 2\u02c6\u03b1 \ufffd p(u\u22121) t\u02dcn \u2212 l(u\u22121) t\u02dcn \ufffd \u2207\u03c5ap(u\u22121) t\u02dcn . end Return the trained ANN model Na \ufffd \u00b7; \u03c5( \u02c6 M) a \ufffd . 36 ",
    "Method": "",
    "References": "References Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2018). Automatic di\ufb00erentiation in machine learning: A survey. Journal of Machine Learning Research, 18(2):1\u201343. Bertsimas, D., Kogan, L., and Lo, A. W. (2000). When is time continuous? Journal of Financial Economics, 55(2):173\u2013204. B\u00a8uhler, H., Gonon, L., Teichmann, J., and Wood, B. (2019). Deep hedging. Quantitative Finance, 19(8):1271\u20131291. Cao, J., Chen, J., Hull, J., and Poulos, Z. (2021). Deep hedging of derivatives using reinforcement learning. Journal of Financial Data Science, 3(1):10\u201327. 32 Carbonneau, A. (2021). Deep hedging of long-term \ufb01nancial derivatives. Insurance: Mathematics and Economics, 99:327\u2013340. Charpentier, A., \u00b4Elie, R., and Remlinger, C. (2021). Reinforcement learning in economics and \ufb01nance. Computational Economics (2021). Chen, B., and Hong, Y. (2012). Testing for the Markov property in time series. Econometric Theory, 28:130\u2013178. Chen, Z., Vetzal, K., and Forsyth, P. (2008). The e\ufb00ect of modelling parameters on the value of GMWB guarantees. Insurance: Mathematics and Economics, 43(1):165\u2013173. Cheridito, P., Ery, J., and W\u00a8uthrich, M. V. (2020). Assessing asset-liability risk with neural networks. Risks, 8(1) article 16. Chong, W. F. (2019). Pricing and hedging equity-linked life insurance contracts beyond the classical paradigm: The principle of equivalent forward preferences. Insurance: Mathematics and Economics, 88:93\u2013107. Cui, Z., Feng, R., and MacKay, A. (2017). Variable annuities with VIX-linked fee structure under a Heston-type stochastic volatility model. North American Actuarial Journal, 21(3):458\u2013483. Dai, M., Kwok, Y. K., and Zong, J. (2008). Guaranteed minimum withdrawal bene\ufb01t in variable annuities. Mathematical Finance, 18(4):595\u2013611. Dang, O., Feng, M., and Hardy, M. R. (2020). E\ufb03cient nested simulation for conditional tail expectation of variable annuities. North American Actuarial Journal, 24(2):187\u2013210. Dang, O., Feng, M., and Hardy, M. R. (2022). Dynamic importance allocated nested simulation for variable annuity risk measurement. Annals of Actuarial Science, 16(2):319\u2013348. Feng, B. M., Tan, Z., and Zheng, J. (2020). E\ufb03cient simulation designs for valuation of large variable annuity portfolios. North American Actuarial Journal, 24(2):275\u2013289. Feng, R. (2018). An Introduction to Computational Risk Management of Equity-Linked Insurance. CRC Press. Feng, R. and Yi, B. (2019). Quantitative modeling of risk management strategies: Stochastic reserving and hedging of variable annuity guaranteed bene\ufb01ts. Insurance: Mathematics and Economics, 85:60\u201373. Gan, G. (2013). Application of data clustering and machine learning in variable annuity valuation. Insurance: Mathematics and Economics, 53(3):795\u2013801. Gan, G. (2018). Valuation of large variable annuity portfolios using linear models with interactions. Risks, 6(3):1\u201319. Gan, G. and Lin, X. S. (2015). Valuation of large variable annuity portfolios under nested simulation: A functional data approach. Insurance: Mathematics and Economics, 62:138\u2013150. Gan, G. and Lin, X. S. (2017). E\ufb03cient Greek calculation of variable annuity portfolios for dynamic hedging: A two-level metamodeling approach. North American Actuarial Journal, 21(2):161\u2013177. Gan, G. and Valdez, E. A. (2017). Modeling partial Greeks of variable annuities with dependence. Insurance: Mathematics and Economics, 76:118\u2013134. Gan, G. and Valdez, E. A. (2018). Regression modeling for the valuation of large variable annuity portfolios. North American Actuarial Journal, 22(1):40\u201354. Gan, G. and Valdez, E. A. (2020). Valuation of large variable annuity portfolios with rank order kriging. North American Actuarial Journal, 24(1):100\u2013117. Gao, G. and W\u00a8uthrich, M. V. (2019). Convolutional neural network classi\ufb01cation of telematics car driving data. Risks, 7(1) article 6. Gweon, H., Li, S., and Mamon, R. (2020). An e\ufb00ective bias-corrected bagging method for the valuation of large variable annuity portfolios. ASTIN Bulletin: The Journal of the International Actuarial Association, 50(3):853\u2013871. Hardy, M. (2003). Investment Guarantees: Modeling and Risk Management for Equity-Linked Life Insurance. John Wiley & Sons, Inc. Hasselt, H. (2010). Double Q-learning. In Advances in Neural Information Processing Systems, volume 23. Hejazi, S. A. and Jackson, K. R. (2016). A neural network approach to e\ufb03cient valuation of large portfolios of variable annuities. Insurance: Mathematics and Economics, 70:169\u2013181. 33 Hu, C., Quan, Z., and Chong, W. F. (2022). Imbalanced learning for insurance using modi\ufb01ed loss functions in tree-based models. Insurance: Mathematics and Economics, 106:13\u201332. Jeon, J. and Kwak, M. (2018). Optimal surrender strategies and valuations of path-dependent guarantees in variable annuities. Insurance: Mathematics and Economics, 83:93\u2013109. Kindratenko, V., Mu, D., Zhan, Y., Maloney, J., Hashemi, S. H., Rabe, B., Xu, K., Campbell, R., Peng, J., and Gropp, W. (2020). HAL: Computer system for scalable deep learning. pages 41\u201348. In Practice and Experience in Advanced Research Computing (PEARC \u201920). Kolm, P. N. and Ritter, G. (2019). Dynamic replication and hedging: A reinforcement learning approach. Journal of Financial Data Science, 1(1):159\u2013171. Lin, X. S. and Yang, S. (2020). Fast and e\ufb03cient nested simulation for large variable annuity portfolios: A surrogate modeling approach. Insurance: Mathematics and Economics, 91:85\u2013103. Liu, K. and Tan, K. S. (2020). Real-time valuation of large variable annuity portfolios: A green mesh approach. North American Actuarial Journal, 25(3):313-333. Milevsky, M. A. and Posner, S. E. (2001). The Titanic option: Valuation of the guaranteed minimum death bene\ufb01t in variable annuities and mutual funds. The Journal of Risk and Insurance, 68(1):93\u2013128. Milevsky, M. A. and Salisbury, T. S. (2006). Financial valuation of guaranteed minimum withdrawal bene\ufb01ts. Insurance: Mathematics and Economics, 38(1):21\u201338. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing Atari with deep reinforcement learning. arXiv: 1312.5602. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518:529\u2013533. Moenig, T. (2021a). E\ufb03cient valuation of variable annuity portfolios with dynamic programming. Journal of Risk and Insurance, 88(4):1023\u20131055. Moenig, T. (2021b). Variable annuities: Market incompleteness and policyholder behavior. Insurance: Mathematics and Economics, 99:63\u201378. Perla, F., Richman, R., Scognamiglio, S. and W\u00a8uthrich, M. V. (2021). Time-series forecasting of mortality rates using deep learning. Scandinavian Actuarial Journal, 7:572\u2013598. Quan, Z., Gan, G., and Valdez, E. (2021). Tree-based models for variable annuity valuation: Parameter tuning and empirical analysis. Annals of Actuarial Science, 16(1):95-118. Richman, R. and W\u00a8uthrich, M. V. (2021). A neural network extension of the Lee-Carter model to multiple populations. Annals of Actuarial Science, 15(2):346\u2013366. Schulman, J., Levine, S., Moritz, P., Jordan, M., and Abbeel, P. (2015). Trust region policy optimization. arXiv: 1502.05477. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv: 1707.06347. Trottier, D. A., Godin, F., and Hamel, E. (2018). Local hedging of variable annuities in the presence of basis risk. ASTIN Bulletin: The Journal of the International Actuarial Association, 48(2):611\u2013646. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., baker, L., Lai, M., bolton, A., chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and Hassabis, D. (2017). Mastering the game of Go without human knowledge. Nature, 550:354\u2013359. Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts. Sutton, R. S. (1988). Learning to predict by the methods of temporal di\ufb00erences. Machine Learning, 3:9\u201344. Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press. 34 Wang, G. and Zou, B. (2021). Optimal fee structure of variable annuities. Insurance: Mathematics and Economics, 101:587\u2013601. Wang, H., Zariphopoulou, T., and Zhou, X. (2020). Reinforcement learning in continuous time and space: A stochastic control approach. Journal of Machine Learning Research, 21:1\u201334. Wang, H. and Zhou, X. (2020). Continuous-time mean-variance portfolio selection: A reinforcement learning framework. Mathematical Finance, 30(4):1273\u20131308. Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, University of Cambridge. Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8:297\u2013292. Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229\u2013256. W\u00a8uthrich, M. V. (2018). Neural networks applied to chain-ladder reserving. European Actuarial Journal, 8:407\u2013436. Xu, W., Chen, Y., Coleman, C., and Coleman, T. F. (2018). Moment matching machine learning methods for risk management of large variable annuity portfolios. Journal of Economic Dynamics and Control, 87:1\u201320. Xu, X. (2020). Variable Annuity Guaranteed Bene\ufb01ts: An Integrated Study of Financial Modelling, Actuarial Valuation and Deep Learning. PhD thesis, UNSW Business School. 35 Appendix A Deep Hedging Approach In this section, we provide a brief review of the DH approach adapted from B\u00a8uhler et al. (2019). In particular, the hedging objective of the insurer is still given as \ufffd E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd , with Equation (2) being the optimal (discrete) hedging strategy. The hedging agent built by the insurer using the DH algorithm shall be called the DH agent hereafter. A.1 Deterministic Action Di\ufb00erent from Section 3.1, in which the RL agent takes a stochastic action which is sampled from the policy for the exploration in the MDP environment, the DH agent only deploys a deterministic action HDH : X \u2192 A, which is a direct mapping from the state space to the action space. Speci\ufb01cally, at each time tk, where k = 0, 1, . . . , n \u2212 1, given the current state Xtk \u2208 X, the DH agent takes an action HDH (Xtk) \u2208 A. In this case, the objective of the DH agent is to solve for the optimal hedging strategy HDH,\u2217 (\u00b7) that minimizes \ufffd E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd , or equivalently minimizes E \ufffd (Pt\u02dcn \u2212 Lt\u02dcn)2\ufffd . A.2 Action Approximation and Parameterization The deterministic action mapping HDH : X \u2192 A is then approximated and parameterized by an ANN with weights \u03c5a. The construction of such ANN Na (\u00b7; \u03c5a) is similar to that in Section 3.3.1, except that Na (x; \u03c5a) \u2208 R for any x \u2208 Rp; that is, Na (\u00b7; \u03c5a) takes a state vector x \u2208 Rp as the input, and directly outputs a deterministic action a (x; \u03c5a) \u2208 R, instead of the Gaussian mean-variance tuple \ufffd c (x; \u03c5a) , d2 (x; \u03c5a) \ufffd \u2208 R \u00d7 R+ in the RL approach, which then samples an action from the Gaussian measure. Hence, in the DH approach, solving the optimal hedging strategy HDH,\u2217 (\u00b7) boils down to \ufb01nding the optimal weights \u03c5\u2217 a. A.3 Deep Hedging Method The DH agent starts from initial ANN weights \u03c5(0) a , deploys the hedging strategy to collect terminal P&Ls, and gradually updates the ANN weights by stochastic gradient ascent as shown in Equation (13), with \u03b8 replaced by \u03c5. For the DH agent, at each update step u = 1, 2, . . . , the surrogate performance measure is given as J (u\u22121) \ufffd \u03c5(u\u22121) a \ufffd = \u2212E \ufffd\ufffd P (u\u22121) t\u02dcn \u2212 L(u\u22121) t\u02dcn \ufffd2\ufffd . Correspondingly, the gradient of the surrogate performance measure with respect to the ANN weights \u03c5a is \u2207\u03c5aJ (u\u22121) \ufffd \u03c5(u\u22121) a \ufffd = \u22122E \ufffd\ufffd P (u\u22121) t\u02dcn \u2212 L(u\u22121) t\u02dcn \ufffd \u2207\u03c5aP (u\u22121) t\u02dcn \ufffd . Therefore, based on the realized terminal P&L p(u\u22121) t\u02dcn and l(u\u22121) t\u02dcn , the estimated gradient is given as \ufffd \u2207\u03c5aJ (u\u22121) \ufffd \u03c5(u\u22121) a \ufffd = \u22122 \ufffd p(u\u22121) t\u02dcn \u2212 l(u\u22121) t\u02dcn \ufffd \u2207\u03c5ap(u\u22121) t\u02dcn . Algorithm 1 summarizes the DH method above. Algorithm 1 Pseudo-code for deep hedging method Input initial ANN model Na \ufffd \u00b7; \u03c5(0) a \ufffd , total number of updates \u02c6 M \u2208 N, learning rate \u02c6\u03b1 \u2208 [0, 1]. for u = 1, 2, \u00b7 \u00b7 \u00b7 , \u02c6 M do \u00b7 Initialize the MDP training environment and observe the initial state vector x(u\u22121) t0 . \u00b7 Follow the hedging strategy Na \ufffd \u00b7; \u03c5(u\u22121) a \ufffd to realize an episode and evaluate the terminal P&L p(u\u22121) t\u02dcn and l(u\u22121) t\u02dcn . \u00b7 Update \u03c5(u\u22121) a as \u03c5(u) a = \u03c5(u\u22121) a \u2212 2\u02c6\u03b1 \ufffd p(u\u22121) t\u02dcn \u2212 l(u\u22121) t\u02dcn \ufffd \u2207\u03c5ap(u\u22121) t\u02dcn . end Return the trained ANN model Na \ufffd \u00b7; \u03c5( \u02c6 M) a \ufffd . 36 Compared with policy gradient methods introduced in Section 3.4, the DH method shows two key di\ufb00erences. First, it assumes that the hedging portfolio value P (u\u22121) t\u02dcn is di\ufb00erentiable with respect to \u03c5a at each update u = 1, 2, . . . . Second, the update of ANN weights does not depend on intermediate rewards collected during an episode; that is, to update the weights, the DH agent has to experience a complete episode to realize the terminal P&L. Therefore, the update frequency of the DH method is lower than that of the RL method with TD feature. Appendix B REINFORCE: A Monte Carlo Policy Gradient Method At each update step u = 1, 2, . . . , based on the ANN weights \u03b8(u\u22121), and thus the policy \u03c0 \ufffd \u00b7; \u03b8(u\u22121) p \ufffd , the RL agent experiences the realized episode: \ufffd x(u\u22121) t0 , h(u\u22121) t0 , x(u\u22121) t1 , r(u\u22121) t1 , h(u\u22121) t1 , . . . , x(u\u22121) t\u02dcn\u22121 , r(u\u22121) t\u02dcn\u22121 , h(u\u22121) t\u02dcn\u22121 , x(u\u22121) t\u02dcn , r(u\u22121) t\u02dcn \ufffd , where h(u\u22121) tk , for k = 0, 1, . . . , \u02dcn \u2212 1, is the time-tk realized hedging strategy being sampled from the Gaussian distribution with the mean c \ufffd x(u\u22121) tk ; \u03b8(u\u22121) p \ufffd and the variance d2 \ufffd x(u\u22121) tk ; \u03b8(u\u22121) p \ufffd . In the following, \ufb01x an update step u = 1, 2, . . . . REINFORCE takes directly the time-0 value function V (u\u22121) (0, x; \u03b8p), for any x \u2208 X, as a part of the surrogate performance measure: V (u\u22121) (0, x; \u03b8p) = E \ufffd\u02dcn\u22121 \ufffd k=0 R(u\u22121) tk+1 \ufffd\ufffd\ufffdX(u\u22121) 0 = x \ufffd . In Williams (1992), the Policy Gradient Theorem was proved, which states that \u2207\u03b8pV (u\u22121) (0, x; \u03b8p) = E \ufffd\u02dcn\u22121 \ufffd k=0 \ufffd\u02dcn\u22121 \ufffd l=k R(u\u22121) tl+1 \ufffd \u2207\u03b8p ln \u03c6 \ufffd H(u\u22121) tk ; X(u\u22121) tk , \u03b8p \ufffd \ufffd\ufffd\ufffdX(u\u22121) 0 = x \ufffd , where \u03c6 \ufffd \u00b7; X(u\u22121) tk , \u03b8p \ufffd is the Gaussian density function with mean c \ufffd X(u\u22121) tk ; \u03b8p \ufffd and variance d2 \ufffd X(u\u22121) tk ; \u03b8p \ufffd . Therefore, based on the realized episode, the estimated gradient of the time-0 value function is given by \ufffd \u2207\u03b8pV (u\u22121) \ufffd 0, x; \u03b8(u\u22121) p \ufffd = \u02dcn\u22121 \ufffd k=0 \ufffd\u02dcn\u22121 \ufffd l=k r(u\u22121) tl+1 \ufffd \u2207\u03b8p ln \u03c6 \ufffd h(u\u22121) tk ; x(u\u22121) tk , \u03b8(u\u22121) p \ufffd . Notice that, thanks to the Policy Gradient Theorem, the gradient of the surrogate performance measure does not depend on the gradient of the reward function, and hence the reward function could be discrete or non-di\ufb00erentiable while the estimated gradient of the surrogate performance measure only needs the numerical reward values. However, in the DH approach of B\u00a8uhler et al. (2019), the gradient of the surrogate performance measure therein does depend on the gradient of the terminal loss function, and thus that approach implicitly requires the di\ufb00erentiability of the hedging portfolio value while the estimated gradient of the surrogate performance requires its numerical gradient values. See Appendix A for more details. To reduce the variance of estimated gradient above, Williams (1992) suggested to introduce an unbiased baseline in this gradient, where a natural choice is the value function: \u2207\u03b8pV (u\u22121) (0, x; \u03b8p) = E \ufffd\u02dcn\u22121 \ufffd k=0 \ufffd\u02dcn\u22121 \ufffd l=k R(u\u22121) tl+1 \u2212 V \ufffd tk, X(u\u22121) tk ; \u03b8p \ufffd\ufffd \u2207\u03b8p ln \u03c6 \ufffd H(u\u22121) tk ; X(u\u22121) tk , \u03b8p \ufffd \ufffd\ufffd\ufffdX(u\u22121) 0 = x \ufffd ; see also Weaver and Tao (2001). Herein, at any time tk, for k = 0, 1, . . . , \u02dcn \u2212 1, A(u\u22121) tk = \ufffd\u02dcn\u22121 l=k R(u\u22121) tl+1 \u2212 V \ufffd tk, X(u\u22121) tk ; \u03b8p \ufffd is called an advantage. Since the true value function is unknown to the RL agent, it is approximated by \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd = Nv \ufffd X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd , de\ufb01ned in (12), and in which the ANN weights are evaluated at \u03b8v = \u03b8(u\u22121) v as the gradient of the time-0 value function is independent of the ANN weights \u03b8v; hence, the estimated advantage is given by \u02c6A(u\u22121) tk = \ufffd\u02dcn\u22121 l=k R(u\u22121) tl+1 \u2212 \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd . Due to the value function approximation in the baseline, REINFORCE includes a second component in the surrogate performance measure, which aims to minimize the loss between the sum of reward signals and the ap37 proximated value function by the ANN. Therefore, the surrogate performance measure is given by: J (u\u22121) (\u03b8) = V (u\u22121) (0, x; \u03b8p) \u2212 E \ufffd\u02dcn\u22121 \ufffd k=0 \ufffd \u02c6A(u\u22121) \u03b8(u\u22121) p ,tk + \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8(u\u22121) v \ufffd \u2212 \u02c6V \ufffd tk, X(u\u22121) tk ; \u03b8v \ufffd\ufffd2 \ufffd\ufffd\ufffdX(u\u22121) 0 = x \ufffd , where the estimated advantaged \u02c6A(u\u22121) \u03b8(u\u22121) p ,tk is evaluated at \u03b8p = \u03b8(u\u22121) p . Hence, at each update step u = 1, 2, . . . , based on the ANN weights \u03b8(u\u22121), and thus the policy \u03c0 \ufffd \u00b7; \u03b8(u\u22121) p \ufffd , the estimated gradient of the surrogate performance measure is given by \ufffd \u2207\u03b8J (u\u22121) \ufffd \u03b8(u\u22121)\ufffd = \u02dcn\u22121 \ufffd k=0 \ufffd\u02dcn\u22121 \ufffd l=k r(u\u22121) tl+1 \u2212 \u02c6V \ufffd tk, x(u\u22121) tk ; \u03b8(u\u22121) v \ufffd\ufffd \u2207\u03b8p ln \u03c6 \ufffd h(u\u22121) tk ; x(u\u22121) tk , \u03b8(u\u22121) p \ufffd + \u02dcn\u22121 \ufffd k=0 \ufffd\u02dcn\u22121 \ufffd l=k r(u\u22121) tl+1 \u2212 \u02c6V \ufffd tk, x(u\u22121) tk ; \u03b8(u\u22121) v \ufffd\ufffd \u2207\u03b8v \u02c6V \ufffd tk, x(u\u22121) tk ; \u03b8(u\u22121) v \ufffd = \u02dcn\u22121 \ufffd k=0 \u02c6a(u\u22121) tk \ufffd \u2207\u03b8p ln \u03c6 \ufffd h(u\u22121) tk ; x(u\u22121) tk , \u03b8(u\u22121) p \ufffd + \u2207\u03b8v \u02c6V \ufffd tk, x(u\u22121) tk ; \u03b8(u\u22121) v \ufffd\ufffd , where \u02c6a(u\u22121) tk = \ufffd\u02dcn\u22121 l=k r(u\u22121) tl+1 \u2212 \u02c6V \ufffd tk, x(u\u22121) tk ; \u03b8(u\u22121) v \ufffd , for k = 0, 1, . . . , \u02dcn \u2212 1, is the realized estimated advantage. Appendix C Deep Hedging Training The state vector observed by the DH agent is the same as that by the RL agent in Equation (18). Table 16a summarizes the hyperparameters of DH agent training, while Table 16b outlines the hyperparameters of the ANN architecture of DH agent; see Appendix A. (a) Hyperparameters of Deep Hedging Training Parameter Value Number of updates \u02c6 M 108 Learning rate \u02c6\u03b1 0.0001 Optimizer Adam (b) Hyperparameters for Neural Network Parameter Value(s) Number of layers 6 Dimension of hidden layers [32, 64, 128, 64, 32] Activation function ReLU Table 16: The hyperparameters of deep hedging training and the neural network 38 ",
    "title": "",
    "paper_info": "...\n...\n...\n...\nx1\nx2\nx4\n\u02c6V (x)\nc (x)\nd2 (x)\nInput\nLayer\nShared\nLayer\nNon-Shared\nLayer\nOuput\nLayer\nFigure 3: An example of policy and value function arti\ufb01cial neural networks with a shared hidden layer and a\nnon-shared hidden layer\n\u03b8(u\u22121).\nREINFORCE, which is pioneered by Williams (1992), is a Monte Carlo policy gradient method, which updates\nthe ANN weights by each episode. As this paper applies a temporal-di\ufb00erence (TD) policy gradient method, we\nrelegate the review of REINFORCE to Appendix B, where the Policy Gradient Theorem, the foundation of any\npolicy gradient methods, is presented.\nPPO, which is pioneered by Schulman et al. (2017), is a TD policy gradient method, which updates the ANN\nweights by a batch of K \u2208 N realizations. At each update step u = 1, 2, . . . , based on the ANN weights \u03b8(u\u22121), and\nthus the policy \u03c0\n\ufffd\n\u00b7; \u03b8(u\u22121)\np\n\ufffd\n, the RL agent experiences E(u) \u2208 N realized episodes for the K realizations.\n\u2022 If E(u) = 1, the episode is given by\n\ufffd\n. . . , x(u\u22121)\nt\nK(u)\ns\n, h(u\u22121)\nt\nK(u)\ns\n, x(u\u22121)\nt\nK(u)\ns\n+1, r(u\u22121)\nt\nK(u)\ns\n+1, h(u\u22121)\nt\nK(u)\ns\n+1,\n. . . , x(u\u22121)\nt\nK(u)\ns\n+K\u22121, r(u\u22121)\nt\nK(u)\ns\n+K\u22121, h(u\u22121)\nt\nK(u)\ns\n+K\u22121, x(u\u22121)\nt\nK(u)\ns\n+K, r(u\u22121)\nt\nK(u)\ns\n+K, . . .\n\ufffd\n,\nwhere K(u)\ns\n= 0, 1, . . . , \u02dcn \u2212 1, such that the time tK(u)\ns\nis when the episode is initiated in this update, and\nh(u\u22121)\ntk\n, for k = 0, 1, . . . , \u02dcn \u2212 1, is the time-tk realized hedging strategy being sampled from the Gaussian\ndistribution with the mean c\n\ufffd\nx(u\u22121)\ntk\n; \u03b8(u\u22121)\np\n\ufffd\nand the variance d2 \ufffd\nx(u\u22121)\ntk\n; \u03b8(u\u22121)\np\n\ufffd\n; necessarily, \u02dcn\u2212K(u)\ns\n\u2265 K.\n\u2022 If E(u) = 2, 3, . . . , the episodes are given by\n\ufffd\n. . . , x(u\u22121,1)\nt\nK(u)\ns\n, h(u\u22121,1)\nt\nK(u)\ns\n, x(u\u22121,1)\nt\nK(u)\ns\n+1, r(u\u22121,1)\nt\nK(u)\ns\n+1, h(u\u22121,1)\nt\nK(u)\ns\n+1,\n. . . , x(u\u22121,1)\nt\u02dcn(1)\u22121 , r(u\u22121,1)\nt\u02dcn(1)\u22121 , h(u\u22121,1)\nt\u02dcn(1)\u22121 , x(u\u22121,1)\nt\u02dcn(1)\n, r(u\u22121,1)\nt\u02dcn(1)\n\ufffd\n,\n\ufffd\nx(u\u22121,2)\nt0\n, h(u\u22121,2)\nt0\n, x(u\u22121,2)\nt1\n, r(u\u22121,2)\nt1\n, h(u\u22121,2)\nt1\n,\n. . . , x(u\u22121,2)\nt\u02dcn(2)\u22121 , r(u\u22121,2)\nt\u02dcn(2)\u22121 , h(u\u22121,2)\nt\u02dcn(2)\u22121 , x(u\u22121,2)\nt\u02dcn(2)\n, r(u\u22121,2)\nt\u02dcn(2)\n\ufffd\n,\n. . . ,\n13\n",
    "GPTsummary": "- (1): The paper proposes a methodology for hedging variable annuity contracts with both GMMB and GMDB riders. These contracts are dual-risk bearing and have various guarantees embedded in them, making risk management a challenging task. \n\n- (2): Previous techniques for hedging variable annuities using options pricing and stochastic programming models have limitations in its applicability to real-world situations. The paper lists the limitations and motivations for deploying deep reinforcement learning and proposes a two-phase approach involving online learning and sequential anchor-hedging reward signals. \n\n- (3): The methodology proposed in this paper is the two-phase deep reinforcement learning approach, which involves training an infant reinforcement learning agent with a pre-designed training environment and collecting sequential anchor-hedging reward signals. In the online learning phase, a trained reinforcement learning agent interacts with the market environment in real-time, self-revises its hedging strategy, and collects single terminal reward signals. \n\n- (4): The proposed methodology is evaluated on an illustrative example via a rolling basis to showcase the self-revision capability of the hedging strategy by leveraging online learning. The results show that the trained reinforcement learning agent outperforms misspecified deltas and performs equally well as the correct Delta. The performance of the proposed approach supports their goals of addressing model miscalibration in Black-Scholes financial and constant force of mortality actuarial market environments.\n7. Methods: \n\n- (1): The paper proposes a two-phase deep reinforcement learning approach for hedging variable annuities with GMMB and GMDB riders.\n\n- (2): The first phase involves training an infant reinforcement learning agent with a pre-designed training environment and collecting sequential anchor-hedging reward signals.\n\n- (3): The second phase is the online learning phase where a trained reinforcement learning agent interacts with the market environment in real-time, self-revises its hedging strategy, and collects single terminal reward signals.\n\n- (4): The hedging strategy is self-revised by leveraging online learning, and deep hedging method is used to update the weights of the hedging strategy.\n\n- (5): The proposed methodology is evaluated on an illustrative example via a rolling basis to showcase the self-revision capability of the hedging strategy by leveraging online learning. The results show that the trained reinforcement learning agent performs well and outperforms misspecified deltas.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This work proposes a novel methodology for hedging variable annuities with both GMMB and GMDB riders using deep reinforcement learning. The proposed two-phase approach involves training an agent with sequential anchor-hedging signals and leveraging online learning for self-revision of the hedging strategy.\n\n- (2): Innovation point: The proposed methodology leverages deep reinforcement learning, which is a new approach for hedging variable annuities contracts. The approach is innovative in its two-phase training and online learning process for self-revision of the hedging strategy. Performance: The proposed methodology is evaluated on an illustrative example, showing that it outperforms misspecified deltas and performs equally well as the correct Delta. The performance supports the potential practical applications of this methodology. Workload: The workload required for implementing the methodology is not explicitly discussed in the article. However, the authors note that the methodology is computationally efficient and can be applied in real-time situations.\n\nNote: Proper Nouns (capitalized words) are specific words that refer to an individual person, place, organization, or thing, and are not to be translated.\n\n\n",
    "GPTmethods": "- (1): The paper proposes a two-phase deep reinforcement learning approach for hedging variable annuities with GMMB and GMDB riders.\n\n- (2): The first phase involves training an infant reinforcement learning agent with a pre-designed training environment and collecting sequential anchor-hedging reward signals.\n\n- (3): The second phase is the online learning phase where a trained reinforcement learning agent interacts with the market environment in real-time, self-revises its hedging strategy, and collects single terminal reward signals.\n\n- (4): The hedging strategy is self-revised by leveraging online learning, and deep hedging method is used to update the weights of the hedging strategy.\n\n- (5): The proposed methodology is evaluated on an illustrative example via a rolling basis to showcase the self-revision capability of the hedging strategy by leveraging online learning. The results show that the trained reinforcement learning agent performs well and outperforms misspecified deltas.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This work proposes a novel methodology for hedging variable annuities with both GMMB and GMDB riders using deep reinforcement learning. The proposed two-phase approach involves training an agent with sequential anchor-hedging signals and leveraging online learning for self-revision of the hedging strategy.\n\n- (2): Innovation point: The proposed methodology leverages deep reinforcement learning, which is a new approach for hedging variable annuities contracts. The approach is innovative in its two-phase training and online learning process for self-revision of the hedging strategy. Performance: The proposed methodology is evaluated on an illustrative example, showing that it outperforms misspecified deltas and performs equally well as the correct Delta. The performance supports the potential practical applications of this methodology. Workload: The workload required for implementing the methodology is not explicitly discussed in the article. However, the authors note that the methodology is computationally efficient and can be applied in real-time situations.\n\nNote: Proper Nouns (capitalized words) are specific words that refer to an individual person, place, organization, or thing, and are not to be translated.\n\n\n",
    "GPTconclusion": "- (1): This work proposes a novel methodology for hedging variable annuities with both GMMB and GMDB riders using deep reinforcement learning. The proposed two-phase approach involves training an agent with sequential anchor-hedging signals and leveraging online learning for self-revision of the hedging strategy.\n\n- (2): Innovation point: The proposed methodology leverages deep reinforcement learning, which is a new approach for hedging variable annuities contracts. The approach is innovative in its two-phase training and online learning process for self-revision of the hedging strategy. Performance: The proposed methodology is evaluated on an illustrative example, showing that it outperforms misspecified deltas and performs equally well as the correct Delta. The performance supports the potential practical applications of this methodology. Workload: The workload required for implementing the methodology is not explicitly discussed in the article. However, the authors note that the methodology is computationally efficient and can be applied in real-time situations.\n\nNote: Proper Nouns (capitalized words) are specific words that refer to an individual person, place, organization, or thing, and are not to be translated.\n\n\n"
}