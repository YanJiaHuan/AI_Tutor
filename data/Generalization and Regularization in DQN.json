{
    "Abstract": "Abstract Deep reinforcement learning algorithms have shown an impressive ability to learn complex control policies in high-dimensional tasks. However, despite the ever-increasing performance on popular benchmarks, policies learned by deep reinforcement learning algorithms can struggle to generalize when evaluated in remarkably similar environments. In this paper we propose a protocol to evaluate generalization in reinforcement learning through di\ufb00erent modes of Atari 2600 games. With that protocol we assess the generalization capabilities of DQN, one of the most traditional deep reinforcement learning algorithms, and we provide evidence suggesting that DQN overspecializes to the training environment. We then comprehensively evaluate the impact of dropout and \u21132 regularization, as well as the impact of reusing learned representations to improve the generalization capabilities of DQN. Despite regularization being largely underutilized in deep reinforcement learning, we show that it can, in fact, help DQN learn more general features. These features can be reused and \ufb01ne-tuned on similar tasks, considerably improving DQN\u2019s sample e\ufb03ciency. 1 ",
    "Introduction": "Introduction Recently, reinforcement learning (RL) algorithms have proven very successful on complex high-dimensional problems, in large part due to the use of deep neural networks for function approximation (e.g., Mnih et al., 2015; Silver et al., 2016). Despite the generality of the proposed solutions, applying these algorithms to slightly di\ufb00erent environments often requires agents to learn the new task from scratch. The learned policies rarely generalize to other domains and the learned representations are seldom reusable. On the other hand, deep neural networks are lauded for their generalization capabilities (e.g., Lecun et al., 1998), with some communities heavily relying on reusing learned representations in di\ufb00erent problems. In light of the successes of supervised learning methods, the lack of generalization or reusable knowledge (i.e., policies, representation) acquired by current deep RL algorithms is somewhat surprising. \u2217Corresponding author. Contact: jfarebro@cs.ualberta.ca. 1 arXiv:1810.00123v3  [cs.LG]  17 Jan 2020 ",
    "Background": "Background We begin our exposition with an introduction of basic terms and concepts for supervised learning and reinforcement learning. We then discuss the related work, focusing on generalization in reinforcement learning. 2.1 Regularization in Supervised Learning In the supervised learning problem we are given a dataset of examples represented by a matrix X \u2208 Rm\u00d7n with m training examples of dimension n, and a vector y \u2208 R1\u00d7m denoting the output target yi for each training example Xi. We want 2 to learn a function which maps each training example Xi to its predicted output label \u02c6yi. The goal is to learn a robust model that accurately predicts yi from Xi while generalizing to unseen training examples. In this paper we focus on using a neural network parameterized by the weights \u03b8 to learn the function f such that f(Xi; \u03b8) = \u02c6yi. We typically train these models by minimizing min \u03b8 \u03bb 2 \u2225\u03b8\u22252 2 + 1 m m \ufffd i=1 L \ufffd yi, f(Xi; \u03b8) \ufffd , where L is a di\ufb00erentiable loss function which outputs a scalar determining the quality of the prediction (e.g., squared error loss). The \ufb01rst term is a form of regularization, that is, \u21132 regularization, which encourages generalization by imposing a penalty on large weight vectors. The hyperparameter \u03bb is the weighted importance of the regularization term. Another popular regularization technique is dropout (Srivastava et al., 2014). When using dropout, during forward propagation each neural unit is set to zero according to a Bernoulli distribution with probability p \u2208 [0, 1], referred to as the dropout rate. Dropout discourages the network from relying on a small number of neurons to make a prediction, making memorization of the dataset harder. Prior to training, the network parameters are usually initialized through a stochastic process such as Xavier initialization (Glorot and Bengio, 2010). We can also initialize the network using pre-trained weights from a di\ufb00erent task. If we reuse one or more pre-trained layers we say the weights encoded by those layers will be \ufb01ne-tuned during training (e.g., Razavian et al., 2014; Long et al., 2015), a topic we explore in Section 6. 2.2 Reinforcement Learning In the reinforcement learning (RL) problem an agent interacts with an environment with the goal of maximizing cumulative long term reward. RL problems are often modeled as a Markov decision process (MDP), de\ufb01ned by a 5-tuple \u27e8S, A, p, r, \u03b3\u27e9. At a discrete time step t, the agent observes the current state St \u2208 S and takes an action At \u2208 A to transition to the next state St+1 \u2208 S according to the transition dynamics function p(s\u2032 | s, a) .= P(St+1 = s\u2032 | St = s , At = a). The agent receives a reward signal Rt+1 according to the reward function r : S \u00d7 A \u2192 R. The agent\u2019s goal is to learn a policy \u03c0 : S \u00d7 A \u2192 [0, 1], written as \u03c0(a | s), which is de\ufb01ned as the conditional probability of taking action a in state s. The learning agent re\ufb01nes its policy with the objective of maximizing the expected return, that is, the cumulative discounted reward incurred from time t, de\ufb01ned by Gt .= \ufffd\u221e k=0 \u03b3kRt+k+1, where \u03b3 \u2208 [0, 1) is the discount factor. Q-learning (Watkins and Dayan, 1992) is a traditional approach to learning an optimal policy from samples obtained from interactions with the environment. For a given policy \u03c0, we de\ufb01ne the state-action value function as the expected return conditioned on a state and action q\u03c0(s, a) .= E\u03c0 \ufffd Gt|S0 = s, A0 = a \ufffd . The agent iteratively updates the state-action value function based on samples from 3 ",
    "Related Work": "Related Work In reinforcement learning, regularization is rarely applied to value-based methods. The few existing studies often focus on single-task settings with linear function approximation (e.g., Farahmand et al., 2008; Kolter and Ng, 2009). Here we look at the reusability, in di\ufb00erent tasks, of learned representations. The closest work to ours is Cobbe et al.\u2019s (2019), which also looks at regularization techniques applied to deep RL. However, di\ufb00erent from Cobbe et al., here we also evaluate the impact of regularization when \ufb01ne-tuning value functions. Moreover, in this paper we propose a di\ufb00erent platform for evaluating generalization in RL, which we discuss below. There are several recent papers that support our results with respect to the limited generalization capabilities of deep RL agents. Nevertheless, they often investigate generalization in light of di\ufb00erent aspects of an environment such as noise (e.g., Zhang et al., 2018a) and start state distribution (e.g., Rajeswaran et al., 2017; Zhang et al., 2018a,b). There are also some proposals for evaluating generalization in RL through procedurally generated or parametrized environments (e.g., Finn et al., 2017; Juliani et al., 2019; Justesen et al., 2018; Whiteson et al., 2011; Witty et al., 2018). These papers do not investigate generalization in deep RL the same way we do. Moreover, as aforementioned, here we also propose using a di\ufb00erent testbed, the modes and di\ufb03culties of Atari 2600 games. With respect to that, Witty et al.\u2019s (2018) work is directly related to ours, as they propose parameterizing a single Atari 2600 game, Amidar, as a way to evaluate generalization in RL. The use of modes and di\ufb03culties is much more comprehensive and it is free of experimenters\u2019 bias. In summary, our work adds to the growing literature on generalization in reinforcement learning. To the best of our knowledge, our paper is the \ufb01rst to 4 Freeway Hero Breakout Space Invaders Figure 1: Column show the variations between two \ufb02avours of each game. discuss over\ufb01tting in Atari 2600 games, to present results using the Atari 2600 modes as testbed, and to demonstrate the impact regularization can have in value function \ufb01ne-tuning in reinforcement learning. 3 The ALE as a Platform for Evaluating Generalization in Reinforcement Learning The Arcade Learning Environment (ALE) is a platform used to evaluate agents across dozens of Atari 2600 games (Bellemare et al., 2013). It is one of the standard evaluation platforms in the \ufb01eld and has led to several exciting algorithmic advances (e.g., Mnih et al., 2015). The ALE poses the problem of general competency by having agents use the same learning algorithm to perform well in as many games as possible, without using any game speci\ufb01c knowledge. Learning to play multiple games with the same agent, or learning to play a game by leveraging knowledge acquired in a di\ufb00erent game is harder, with fewer successes being known (Rusu et al., 2016; Kirkpatrick et al., 2016; Parisotto et al., 2016; Schwarz et al., 2018; Espeholt et al., 2018). Throughout this paper we evaluate the generalization capabilities of our agents using hold out test environments. We do so with di\ufb00erent modes and di\ufb03culties of Atari 2600 games, features the ALE recently started to support (Machado et al., 2018). Game modes, which were originally native to the Atari 2600 console, generally give us modi\ufb01cations of each Atari 2600 game by modifying sprites, velocities, and the observability of objects. These modes o\ufb00er an excellent framework for evaluating generalization in RL. They were designed several decades ago and remain free from experimenter\u2019s bias as they were not designed with the goal of being a testbed for AI agents, but with the goal of being varied1 and entertaining to humans. Figure 1 depicts some of the di\ufb00erent 1There are 48 Atari 2600 games with more than one \ufb02avour in the ALE. These games have 414 di\ufb00erent \ufb02avours (Machado et al., 2018). Notice that, on average, each game has less than 5 modes and di\ufb03culties available in the ALE. As Machado et al. (2018), hereinafter we call each mode/di\ufb03cult pair a \ufb02avour. Besides having the properties that made the ALE successful in the RL community, the di\ufb00erent game \ufb02avours allow us to look at the problem of generalization in RL from a di\ufb00erent perspective. Because of hardware limitations, the di\ufb00erent \ufb02avours of an Atari 2600 game could not be too di\ufb00erent from each other.2 Therefore, di\ufb00erent \ufb02avours can be seen as small variations of the default game, with few latent variables being changed. In this context, we pose the problem of generalization in RL as the ability to identify invariances across tasks with high-dimensional observation spaces. Such an objective is based on the assumption that the underlying dynamics of the world does not vary much. Instead of requiring an agent to play multiple games that are visually very di\ufb00erent or even non-analogous, the notion of generalization we propose requires agents to play games that are visually very similar and that can be played with policies that are conceptually similar, at least from a human perspective. In a sense, the notion of generalization we propose requires agents to be invariant to changes in the observation space. Introducing \ufb02avours to the ALE is not one of our contributions, this was done by Machado et al. (2018). Nevertheless, here we provide a \ufb01rst concrete suggestion on how to use these \ufb02avours in reinforcement learning. Our paper also provides the \ufb01rst baseline results for di\ufb00erent \ufb02avours of Atari 2600 games since Machado et al. (2018) incorporated them to the ALE but did not report any results on them. The baseline results for the traditional deep RL setting are available in Table 5 while the full baseline results for regularization are available in Table 6. Because these baseline results are quite broad, encompassing multiple games and \ufb02avours, and because we wanted to \ufb01rst discuss other experiments and analyses, Tables 5 and 6 are at the end of the paper. They follow Machado et al.\u2019s (2018) suggestions on how to report Atari 2600 games results. We believe our proposal is a more realistic and tractable way of de\ufb01ning generalization in decision-making problems. Instead of focusing on the samples (s, a, s\u2032, r), simply requiring them be drawn from the same distribution, we look at a more general notion of generalization where we consider multiple tasks, with the assumption that tasks are sampled from the same distribution, similar to the meta-RL setting. Nevertheless, we concretely constrain the distribution of tasks with the notion that only few latent variables describing the environment can vary. This also allows us to have a new perspective towards an agents\u2019 inability to succeed in slightly di\ufb00erent tasks from those they are trained on. At the same time, this is more challenging than using, for example, di\ufb00erent parametrizations of an environment, as often done when evaluating meta-RL algorithms. In fact, we could not obtain any positive results in these Atari 2600 games with traditional meta-RL algorithms (e.g., Finn et al., 2017; Nichol et al., 2018a) and to the best of our knowledge, there are no reports of meta-RL algorithms succeeding in Atari 2600 games. Because of that, we do not further 10 \ufb02avours though. This is another challenge since other settings often assume access to many more environment variations (e.g., via procedural content generation). 2The Atari 2600 console has only 2KB of RAM. 6 Freeway: a chicken must cross a road containing multiple lanes of moving tra\ufb03c within a prespeci\ufb01ed time limit. In all modes of Freeway the agent is rewarded for reaching the top of the screen and is subsequently teleported to the bottom of the screen. If the chicken collides with a vehicle in di\ufb03culty 0 it gets bumped down one lane of tra\ufb03c, alternatively, in di\ufb03culty 1 the chicken gets teleported to its starting position at the bottom of the screen. Mode 1 changes some vehicle sprites to include buses, adds more vehicles to some lanes, and increases the velocity of all vehicles. Mode 4 is almost identical to Mode 1; the only di\ufb00erence being vehicles can oscillate between two speeds. Mode 0, with di\ufb03culty 0, is the default one. Hero: you control a character who must navigate a maze in order to save a trapped miner within a cave system. The agent scores points for forward progression such as clearing an obstacle or killing an enemy. Once the miner is rescued, the level is terminated and you continue to the next level in a di\ufb00erent maze. Some levels have partially observable rooms, more enemies, and more di\ufb03cult obstacles to traverse. Past the default mode (m0d0), each subsequent mode starts o\ufb00 at increasingly harder levels denoted by a level number increasing by multiples of 5. The default mode starts you o\ufb00 at level 1, mode 1 starts at level 5, etc. Breakout: you control a paddle which can move horizontally along the bottom of the screen. At the beginning of the game, or on a loss of life, the ball is set into motion and can bounce o\ufb00 the paddle and collide with bricks at the top of the screen. The objective of the game is to break all the bricks without having the ball fall below your paddles horizontal plane. Subsequently, mode 12 of Breakout hides the bricks from the player until the ball collides with the bricks in which case the bricks \ufb02ash for a brief moment before disappearing again. Space Invaders: you control a spaceship which can move horizontally along the bottom of the screen. There is a grid of aliens above you and the objective of the game is to eliminate all the aliens. You are a\ufb00orded some protection from the alien bullets with three barriers just above your spaceship. Di\ufb03culty 1 of Space Invaders widens your spaceships sprite making it harder to dodge enemy bullets. Mode 1 of Space Invaders causes the shields above you to oscillate horizontally. Mode 9 of Space Invaders is similar to Mode 12 of Breakout where the aliens are partially observable until struck with the player\u2019s bullet. Mode 0, with di\ufb03culty 0, is the default one. Figure 2: Description of the game \ufb02avours used in the paper. discuss these approaches. In this paper we focus on a subset of Atari 2600 games with multiple \ufb02avours. Because we wanted to provide exhaustive results averaging over multiple trials, here we use 13 \ufb02avours obtained from 4 games: Freeway, HERO, Breakout, and Space Invaders. In Freeway, the di\ufb00erent modes vary the speed and number of vehicles, while di\ufb00erent di\ufb03culties change how the player is penalized for running into a vehicle. In HERO, subsequent modes start the player o\ufb00 at increasingly harder levels of the game. The mode we use in Breakout makes the bricks partially observable. Modes of Space Invaders allow for oscillating shield barriers, increasing the width of the player sprite, and partially observable aliens. Figure 1 depicts some of these \ufb02avours and Figure 2 further explains the di\ufb00erence between the ALE \ufb02avours we used.3 3Videos of the di\ufb00erent modes are available in the following link: https://goo.gl/pCvPiD. 7 Table 1: Direct policy evaluation. Each agent is initially trained in the default \ufb02avour for 50M frames then evaluated in each listed game \ufb02avour. Reported numbers are averaged over \ufb01ve runs. Std. dev. is reported between parentheses. Game Variant Evaluation Learn Scratch Freeway m1d0 0.2 (0.2) 4.8 (9.3) m1d1 0.1 (0.1) 0.0 (0.0) m4d0 15.8 (1.0) 29.9 (0.7) Hero m1d0 82.1 (89.3) 1425.2 (1755.1) m2d0 33.9 (38.7) 326.1 (130.4) Breakout m12d0 43.4 (11.1) 67.6 (32.4) Space Invaders m1d0 258.9 (88.3) 753.6 (31.6) m1d1 140.4 (61.4) 698.5 (31.3) m9d0 179.0 (75.1) 518.0 (16.7) 4 Generalization of the Policies Learned by DQN In order to test the generalization capabilities of DQN, we \ufb01rst evaluate whether a policy learned in one \ufb02avour can perform well in a di\ufb00erent \ufb02avour. As aforementioned, di\ufb00erent modes and di\ufb03culties of a single game look very similar. If the representation encodes a robust policy we might expect it to be able to generalize to slight variations of the underlying reward signal, game dynamics, or observations. Evaluating the learned policy in a similar but di\ufb00erent \ufb02avour can be seen as evaluating generalization in RL, similar to cross-validation in supervised learning. To evaluate DQN\u2019s ability to generalize across \ufb02avours, we evaluate the learned \u03f5-greedy policy on a new \ufb02avour after training for 50M frames in the default \ufb02avour, m0d0 (mode 0, di\ufb03culty 0). We measure the cumulative reward averaged over 100 episodes in the new \ufb02avour, adhering to the evaluation protocol suggested by Machado et al. (2018). The results are summarized in Table 1. Baseline results where the agent is trained from scratch for 50M frames in the target \ufb02avour used for evaluation are reported in the baseline column Learn Scratch. Theoretically, this baseline can be seen as an upper bound on the performance DQN can achieve in that \ufb02avour, as it represents the agent\u2019s performance when evaluated in the same \ufb02avour it was trained on. Full baseline results with the agent\u2019s performance after di\ufb00erent number of frames can be found in Tables 5 and 6. We can see in the results that the policies learned by DQN do not generalize well to di\ufb00erent \ufb02avours, even when the \ufb02avours are remarkably similar. For example, in Freeway, a high-level policy applicable to all \ufb02avours is to go up while avoiding cars. This does not seem to be what DQN learns. For example, the default \ufb02avour m0d0 and m4d0 comprise of exactly the same sprites, the only 8 10M 20M 30M 40M 50M Frames before evaluation 0 5 10 15 20 Cumulative Reward (log scale) Freeway Policy Evaluation m1d0 m1d1 m4d0 Figure 3: Performance of a trained agent in the default \ufb02avour of Freeway and evaluated every 500,000 frames in each target \ufb02avour. Error bars were omitted for clarity and the learning curves were smoothed using a moving average over two data points. Results were averaged over \ufb01ve seeds. di\ufb00erence is that in m4d0 some cars accelerate and decelerate over time. The close to optimal policy learned in m0d0 is only able to score 15.8 points when evaluated on m4d0, which is approximately half of what the policy learned from scratch in that \ufb02avour achieves (29.9 points). The learned policy when evaluated on \ufb02avours that di\ufb00er more from m0d0 perform even worse (for example, when a new sprite is introduced, or when there are more cars in each lane). As aforementioned, the di\ufb00erent modes of HERO can be seen as giving the agent a curriculum or a natural progression. Interestingly, the agent trained in the default mode for 50M frames can progress to at least level 3 and sometimes level 4. Mode 1 starts the agent o\ufb00 at level 5 and performance in this mode su\ufb00ers greatly during evaluation. There are very few game mechanics added to level 5, indicating that perhaps the agent is memorizing trajectories instead of learning a robust policy capable of solving each level. Results in some \ufb02avours suggest that the agent is over\ufb01tting to the \ufb02avour it is trained on. We tested this hypothesis by periodically evaluating the learned policy in each other \ufb02avour of that game. This process involved taking checkpoints of the network every 500,000 frames and evaluating the \u03f5-greedy policy in the prescribed \ufb02avour for 100 episodes, further averaged over \ufb01ve runs. The results obtained in Freeway, the most pronounced game in which we observe over\ufb01tting, are depicted in Figure 3. Learning curves for all \ufb02avours can be found in the Appendix. In Freeway, while we see the policy\u2019s performance \ufb02attening out in m4d0, we do see the traditional bell-shaped curve associated to over\ufb01tting in the other modes. At \ufb01rst, improvements in the original policy do correspond to improvements in the performance of that policy in other \ufb02avours. With time, it seems that the agent starts to re\ufb01ne its policy for the speci\ufb01c \ufb02avour it is 9 being trained on, over\ufb01tting to that \ufb02avour. With other game \ufb02avours being signi\ufb01cantly more complex in their dynamics and gameplay, we do not observe this prominent bell-shaped curve. In conclusion, when looking at Table 1, it seems that the policies learned by DQN struggle to generalize to even small variations encountered in game \ufb02avours. The results in Freeway even exhibit a troubling notion of over\ufb01tting. Nevertheless, being able to generalize across small variations of the task the agent was trained on is a desirable property for truly autonomous agents. Based on these results we evaluate whether deep RL can bene\ufb01t from established methods from supervised learning promoting generalization. 5 Regularization in DQN In order to evaluate the hypothesis that the observed lack of generalization is due to over\ufb01tting, we revisit some popular regularization methods from the supervised learning literature. We evaluate two forms of regularization: dropout and \u21132 regularization. First we want to understand the e\ufb00ect of regularization on deploying the learned policy in a di\ufb00erent \ufb02avour. We do so by applying dropout to the \ufb01rst four layers of the network during training, that is, the three convolutional layers and the \ufb01rst fully connected layer. We also evaluate the use of \u21132 regularization on all weights in the network during training. A grid search was performed on Freeway to \ufb01nd reasonable hyperparameters for the convolutional and fully connected dropout rate pconv, pfc \u2208 {(0.05, 0.1), (0.1, 0.2), (0.15, 0.3), (0.2, 0.4), (0.25, 0.5)} , and the \u21132 regularization parameter \u03bb \u2208 {10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126}. Each parameter was swept individually as well as exhausting the cartesian product of both sets of parameters for a total of \ufb01ve runs per con\ufb01guration. The in-depth ablation study, discussing the impact of di\ufb00erent values for each parameter, and their interaction, can be found in the Appendix. We ended up combining dropout and \u21132 regularization as this provided a good balance between training and evaluation performance. This con\ufb01rms Srivastava et al.\u2019s (2014) result that these methods provide bene\ufb01t in tandem. For all future experiments we use \u03bb = 10\u22124, and pconv, pfc = 0.05, 0.1. We follow the same evaluation scheme described when evaluating the nonregularized policy to di\ufb00erent \ufb02avours. We evaluate the policy learned after 50M frames of the default mode of each game. We contrast these results with the results presented in the previous section. This evaluation protocol allows us to directly evaluate the e\ufb00ect of regularization on the learned policy\u2019s ability to generalize. The results are presented in Table 2, on the next page, and the evaluation curves are available in the Appendix. When using regularization during training we sometimes observe a performance hit in the default \ufb02avour. Dropout generally requires increased training iterations to reach the same level of performance one would reach when not using dropout. However, maximal performance in one \ufb02avour is not our goal. We are interested in the setting where one may be willing to take lower performance on 10 Table 2: Policy evaluation using regularization. Each agent was initially trained in the default \ufb02avour for 50M frames with dropout and \u21132 regularization then evaluated on each listed \ufb02avour. Reported numbers are averaged over \ufb01ve runs. Standard deviation is reported between parentheses. Game Variant Eval. with Regularization Eval. without Regularization Freeway m1d0 5.8 (3.5) 0.2 (0.2) m1d1 4.4 (2.3) 0.1 (0.1) m4d0 20.6 (0.7) 15.8 (1.0) Hero m1d0 116.8 (76.0) 82.1 (89.3) m2d0 30.0 (36.7) 33.9 (38.7) Breakout m12d0 31.0 (8.6) 43.4 (11.1) Space Invaders m1d0 456.0 (221.4) 258.9 (88.3) m1d1 146.0 (84.5) 140.4 (61.4) m9d0 290.0 (257.8) 179.0 (75.1) one task in order to obtain higher performance, or adaptability, on future tasks. Full baseline results using regularization can also be found in Table 6. In most \ufb02avours, when looking at Table 2, we see that evaluating the policy trained with regularization does not negatively impact performance when compared to the performance of the policy trained without regularization. In some \ufb02avours we even see an increase in performance. When using regularization the agent\u2019s performance in Freeway improves for all \ufb02avours and the agent even learns a policy capable of outperforming the baseline learned from scratch in two of the three \ufb02avours. Moreover, in Freeway we now observe increasing performance during evaluation throughout most of the learning procedure as depicted in Figure 4, on the next page. These results seem to con\ufb01rm the notion of over\ufb01tting observed in Figure 3. Despite slight improvements from these techniques, regularization by itself does not seem su\ufb03cient to enable policies to generalize across \ufb02avours. Learning from scratch in these new \ufb02avours is still more bene\ufb01cial than re-using a policy learned with regularization. As shown in the next section, the real bene\ufb01t of regularization in deep RL seems to come from the ability to learn more general features. These features lead to a more adaptable representation which can be reused and subsequently \ufb01ne-tuned on other \ufb02avours. 11 10M 20M 30M 40M 50M Frames before evaluation 0 5 10 15 20 25 Cumulative Reward (log scale) Freeway Policy Evaluation w/ Regularization m1d0 m1d1 m4d0 m1d0 dropout+\u21132 m1d1 dropout+\u21132 m4d0 dropout+\u21132 Figure 4: Performance of an agent evaluated every 500, 000 frames after it was trained in the default \ufb02avour of Freeway with dropout and \u21132 regularization. Error bars were omitted for clarity and the learning curves were smoothed using a moving average (n = 2). Results were averaged over \ufb01ve seeds. Dotted lines depict the data presented in Figure 3. 6 Value function \ufb01ne-tuning We hypothesize that the bene\ufb01t of regularizing deep RL algorithms may not come from improvements during evaluation, but instead in having a good parameter initialization that can be adapted to new tasks that are similar. We evaluate this hypothesis using two common practices in machine learning. First, we use the weights trained with regularization as the initialization for the entire network. We subsequently \ufb01ne-tune all weights in the network. This is similar to what classi\ufb01cation methods do in computer vision problems (e.g., Razavian et al., 2014). Secondly, we evaluate reusing and \ufb01ne-tuning only early layers of the network. This has been shown to improve generalization in some settings (e.g., Yosinski et al., 2014), and is sometimes used in natural language processing problems (e.g., Mou et al., 2016; Howard and Ruder, 2018). 6.1 Fine-Tuning the Entire Neural Network In this setting we take the weights of the network trained in the default \ufb02avour for 50M frames and use them to initialize the network commencing training in the new \ufb02avour for 50M frames. We perform this set of experiments twice (for the weights trained with and without regularization, as described in the previous section). Each run is averaged over \ufb01ve seeds. For comparison, we provide a baseline trained from scratch for 50M and 100M frames in each \ufb02avour. Directly comparing the performance obtained after \ufb01ne-tuning to the performance after 50M frames (Scratch) shows the bene\ufb01t of re-using a representation learned in a di\ufb00erent task instead of randomly initializing the network. Comparing 12 the performance obtained after \ufb01ne-tuning to the performance of 100M frames (Scratch) lets us take into consideration the sample e\ufb03ciency of the whole learning process. The results are presented on the next page, in Table 3. Fine-tuning from a non-regularized representation yields con\ufb02icting conclusions. Although in Freeway we obtained positive \ufb01ne-tuning results, we note that rewards are so sparse in mode 1 that this initialization is likely to be acting as a form of optimistic initialization, biasing the agent to go up. The agent observes rewards more often, therefore, it learns quicker about the new \ufb02avour. However, the agent is still unable to reach the maximum score in these \ufb02avours. The results of \ufb01ne-tuning the regularized representation are more exciting. In Freeway we observe the highest scores on m1d0 and m1d1 throughout the whole paper. In HERO we vastly outperform \ufb01ne-tuning from a non-regularized representation. In Space Invaders we obtain higher scores across the board when comparing to the same amount of experience. These results suggest that reusing a regularized representation in deep RL might allow us to learn more general features which can be more successfully \ufb01ne-tuned. Initializing the network with a regularized representation also seems to be better than initializing the network randomly, that is, when learning from scratch. These results are impressive when we consider the potential regularization has in reducing the sample complexity of deep RL algorithms. Initializing the network with a regularized representation seems even better than learning from scratch when we take the total number of frames seen between two \ufb02avours into consideration. When we look at the rows Regularized Fine-tuning and Scratch in Table 3 we are comparing two algorithms that observed 100M frames. However, to generate the results in the column Scratch for two \ufb02avours we used 200M frames while we only used used 150M frames to generate the results in the column Regularized Fine-tuning (50M frames are used to learn in the default \ufb02avour and then 50M frames are used in each \ufb02avour you actually care about). Obviously, this distinction becomes larger as more tasks are taken into consideration. 6.2 Fine-Tuning Early Layers to Learn Co-Adaptations We also investigated which layers may encode general features able to be \ufb01netuned. We were inspired by other studies showing that neural networks can relearn co-adaptations when their \ufb01nal layers are randomly initialized, sometimes improving generalization (Yosinski et al., 2014). We conjectured DQN may bene\ufb01t from re-learning the co-adaptations between early layers comprising general features and the randomly initialized layers which ultimately assign state-action values. We hypothesized that it might be bene\ufb01cial to re-learn the \ufb01nal layers from scratch since state-action values are ultimately conditioned on the \ufb02avour at hand. Therefore, we also evaluated whether \ufb01ne-tuning only the convolutional layers, or the convolutional layers and the \ufb01rst fully connected layer, was more e\ufb00ective than \ufb01ne-tuning the whole network. This does not seem to be the case. The performance when we \ufb01ne-tune the whole network is consistently better than when we re-learn co-adaptations, as shown in Table 4. 13 Table 3: Experiments \ufb01ne-tuning the entire network with and without regularization (dropout + \u21132). An agent is trained with dropout + \u21132 regularization in the default \ufb02avour of each game for 50M frames, then DQN\u2019s parameters were used to initialize the \ufb01ne-tuning procedure on each new \ufb02avour for 50M frames. The baseline agent is trained from scratch up to 100M frames. Standard deviation is reported between parentheses. Fine-tuning Regularized Fine-tuning Scratch Game Variant 10M 50M 10M 50M 50M 100M Freeway m1d0 2.9 (3.7) 22.5 (7.5) 20.2 (1.9) 25.4 (0.2) 4.8 (9.3) 7.5 (11.5) m1d1 0.1 (0.2) 17.4 (11.4) 18.5 (2.8) 25.4 (0.4) 0.0 (0.0) 2.5 (7.3) m4d0 20.8 (1.1) 31.4 (0.5) 22.6 (0.7) 32.2 (0.5) 29.9 (0.7) 32.8 (0.2) Hero m1d0 220.7 (98.2) 496.7 (362.8) 322.5 (39.3) 4104.6 (2192.8) 1425.2 (1755.1) 5026.8 (2174.6) m2d0 74.4 (31.7) 92.5 (26.2) 84.8 (56.1) 211.0 (100.6) 326.1 (130.4) 323.5 (76.4) Breakout m12d0 11.5 (10.7) 69.1 (14.9) 48.2 (4.1) 96.1 (11.2) 67.6 (32.4) 55.2 (37.2) Space Invaders m1d0 617.8 (55.9) 926.1 (56.6) 701.8 (28.5) 1033.5 (89.7) 753.6 (31.6) 979.7 (39.8) m1d1 482.6 (63.4) 799.4 (52.5) 656.7 (25.5) 920.0 (83.5) 698.5 (31.3) 906.9 (56.5) m9d0 354.8 (59.4) 574.1 (37.0) 519.0 (31.1) 583.0 (17.5) 518.0 (16.7) 567.7 (40.1) 14 Table 4: Experiments \ufb01ne-tuning early layers of the network trained with regularization. An agent is trained with dropout + \u21132 regularization in the default \ufb02avour of each game for 50M frames, then DQN\u2019s parameters were used to initialize the corresponding layers to be further \ufb01ne-tuned on each new \ufb02avour. Remaining layers were randomly initialized. We also compare against \ufb01ne-tuning the entire network from Table 3. Standard deviation is reported between parentheses. Regularized Fine-Tuning 3Conv Regularized Fine-Tuning 3Conv + 1FC Regularized Fine-Tuning Entire Network Game Variant 10M 50M 10M 50M 10M 50M Freeway m1d0 0.0 (0.0) 0.7 (1.4) 0.1 (0.1) 4.9 (9.9) 20.2 (1.9) 25.4 (0.2) m1d1 0.0 (0.0) 0.0 (0.0) 0.1 (0.1) 10.0 (12.3) 18.5 (2.8) 25.4 (0.4) m4d0 7.3 (3.5) 30.4 (0.6) 4.9 (4.8) 30.7 (1.7) 22.6 (0.7) 32.2 (0.5) Hero m1d0 405.1 (82.0) 1949.1 (2076.4) 350.3 (52.1) 3085.3 (2055.6) 322.5 (39.3) 4104.6 (2192.8) m2d0 232.1 (30.1) 455.2 (170.4) 150.4 (38.5) 307.6 (64.8) 84.8 (56.1) 211.0 (100.6) Breakout m12d0 4.3 (1.7) 63.7 (26.6) 5.4 (0.8) 89.1 (16.7) 48.2 (4.1) 96.1 (11.2) Space Invaders m1d0 669.3 (29.1) 998.1 (78.8) 681.3 (17.2) 989.6 (39.4) 701.8 (28.5) 1033.5 (89.7) m1d1 609.8 (16.6) 836.3 (55.9) 638.7 (19.1) 883.4 (38.1) 656.7 (25.5) 920.0 (83.5) m9d0 436.1 (18.9) 581.0 (12.2) 439.9 (40.3) 586.7 (39.7) 519.0 (31.1) 583.0 (17.5) 15 7 Discussion and conclusion Many studies have tried to explain generalization of deep neural networks in supervised learning settings (e.g., Zhang et al., 2018b; Dinh et al., 2017). Analyzing generalization and over\ufb01tting in deep RL has its own issues on top of the challenges posed in the supervised learning case. Actually, generalization in RL can be seen in di\ufb00erent ways. We can talk about generalization in RL in terms of conditioned sub-goals within an environment (e.g., Andrychowicz et al., 2017; Sutton, 1995), learning multiple tasks at once (e.g., Teh et al., 2017; Parisotto et al., 2016), or sequential task learning as in a continual learning setting (e.g., Schwarz et al., 2018; Kirkpatrick et al., 2016). In this paper we evaluated generalization in terms of small variations of high-dimensional control tasks. This provides a candid evaluation method to study how well features and policies learned by deep neural networks in RL problems can generalize. The approach of studying generalization with respect to the representation learning problem intersects nicely with the aforementioned problems in RL where generalization is key. The results presented in this paper suggest that DQN generalizes poorly, even when tasks have very similar underlying dynamics. Given this lack of generality, we investigated whether dropout and \u21132 regularization can improve generalization in deep reinforcement learning. Other forms of regularization that have been explored in the past are sticky-actions, random initial states, entropy regularization (e.g., Zhang et al., 2018b), and procedural generation of environments (e.g., Justesen et al., 2018). More related to our work, regularization in the form of weight constraints has been applied in the continual learning setting in order to reduce the catastrophic forgetting exhibited by \ufb01ne-tuning on many sequential tasks (Kirkpatrick et al., 2016; Schwarz et al., 2018). Similar weight constraint methods were explored in multitask learning (Teh et al., 2017). Evaluation practices in RL often focus on training and evaluating agents on exactly the same task. Consequently, regularization has traditionally been underutilized in deep RL. With a renewed emphasis on generalization in RL, regularization applied to the representation learning problem can be a feasible method for improving generalization on closely related tasks. Our results suggest that dropout and \u21132 regularization seem to be able to learn more general purpose features which can be adapted to similar problems. Although other communities relying on deep neural networks have shown similar successes, this is of particular importance for the deep RL community which struggles with sample e\ufb03ciency (Henderson et al., 2018). This work is also related to recent meta-learning procedures like MAML (Finn et al., 2017) which aim to \ufb01nd a parameter initialization that can be quickly adapted to new tasks. As previously mentioned, techniques such as MAML (Finn et al., 2017) and REPTILE (Nichol et al., 2018b) did not succeed in the setting we used. Some of the results here can also be seen under the light of curriculum learning. The regularization techniques we have evaluated here seem to be e\ufb00ective in leveraging situations where an easier task is presented \ufb01rst, sometimes leading to unseen performance levels (e.g., Freeway). 16 Table 5: DQN baseline results for each tested game \ufb02avour. We report the average over \ufb01ve runs (std. deviations are reported between parentheses). Results were obtained with the default value of sticky actions (Machado et al., 2018). Game Variant 10M 50M 100M Best Action Freeway m0d0 3.0 (1.0) 31.4 (0.2) 32.1 (0.1) 23.0 (1.4) m1d0 0.0 (0.1) 4.8 (9.3) 7.5 (11.5) 5.0 (1.5) m1d1 0.0 (0.0) 0.0 (0.0) 2.5 (7.3) 4.2 (1.3) m4d0 4.4 (1.4) 29.9 (0.7) 32.8 (0.2) 7.5 (2.8) Hero m0d0 3187.8 (78.3) 9034.4 (1610.9) 13961.0 (181.9) 150.0 (0.0) m1d0 326.9 (40.3) 1425.2 (1755.1) 5026.8 (2174.6) 75.8 (7.5) m2d0 116.3 (11.0) 326.1 (130.4) 323.5 (76.4) 12.0 (27.5) Breakout m0d0 17.5 (2.0) 72.5 (7.7) 73.4 (13.5) 2.3 (1.3) m12d0 17.7 (1.3) 67.6 (32.4) 55.2 (37.2) 1.8 (1.1) Space Invaders m0d0 250.3 (16.2) 698.8 (32.2) 927.1 (85.3) 243.6 (95.9) m1d0 203.6 (24.3) 753.6 (31.6) 979.7 (39.8) 192.6 (65.7) m1d1 193.6 (11.0) 698.5 (31.3) 906.9 (56.5) 180.9 (101.9) m9d0 173.0 (17.8) 518.0 (16.7) 567.7 (40.1) 174.6 (65.9) Table 6: Baseline results in the default \ufb02avour with dropout and \u21132 regularization. We report the average over \ufb01ve runs (std. deviations are reported between parentheses). We used the default value of sticky actions (Machado et al., 2018). Game Variant 10M 50M 100M Best Action Freeway m0d0 4.6 (5.0) 25.9 (0.6) 29.0 (0.8) 23.0 (1.4) Hero m0d0 2466.5 (630.8) 6505.9 (1843.0) 12446.9 (397.4) 150.0 (0.0) Breakout m0d0 6.1 (2.7) 34.1 (1.8) 66.4 (3.6) 2.3 (1.3) Space Invaders m0d0 214.6 (13.8) 623.1 (16.3) 617.4 (29.6) 243.6 (95.9) Finally, it is obvious that we want algorithms that can generalize across tasks. Ultimately we want agents that can keep learning as they interact with the world in a continual learning fashion. We believe the \ufb02avours of Atari 2600 games can be a stepping stone towards this goal. Our results suggested that regularizing and \ufb01ne-tuning representations in deep RL might be a viable approach towards improving sample e\ufb03ciency and generalization on multiple tasks. It is particularly interesting that \ufb01ne-tuning a regularized network was the most successful approach because this might also be applicable in the continual learning settings where the environment changes without the agent being told so, and re-initializing layers of a network is obviously not an option. 17 Acknowledgments The authors would like to thank Matthew E. Taylor, Tom van de Wiele, and Marc G. Bellemare for useful discussions, as well as Vlad Mnih for feedback on a preliminary draft of the manuscript. This work was supported by funding from NSERC and Alberta Innovates Technology Futures through the Alberta Machine Intelligence Institute (Amii). Computing resources were provided by Compute Canada through CalculQu\u00b4ebec. Marlos C. Machado performed part of this work while at the University of Alberta. References Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight Experience Replay. In Advances in Neural Information Processing Systems (NeurIPS). 5048\u20135058. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Arti\ufb01cial Intelligence Research 47 (2013), 253\u2013279. Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. 2019. Quantifying Generalization in Reinforcement Learning. In Proceedings of the International Conference on Machine Learning (ICML). 1282\u20131289. Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. 2017. Sharp Minima Can Generalize For Deep Nets. In Proceedings of the International Conference on Machine Learning (ICML). 1019\u20131028. Lasse Espeholt, Hubert Soyer, R\u00b4emi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. 2018. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. In Proceedings of the International Conference on Machine Learning (ICML). 1406\u20131415. Amir Massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv\u00b4ari, and Shie Mannor. 2008. Regularized Policy Iteration. In Advances in Neural Information Processing Systems (NeurIPS). 441\u2013448. Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic MetaLearning for Fast Adaptation of Deep Networks. In Proceedings of the International Conference on Machine Learning (ICML). 1126\u20131135. Xavier Glorot and Yoshua Bengio. 2010. Understanding the Di\ufb03culty of Training Deep Feedforward Neural Networks. In Proceedings of the International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS). 249\u2013256. 18 Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep Reinforcement Learning That Matters. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence (AAAI). 3207\u2013 3214. Jeremy Howard and Sebastian Ruder. 2018. Fine-tuned Language Models for Text Classi\ufb01cation. CoRR abs/1801.06146 (2018). Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. 2019. Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning. In Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence (IJCAI). 2684\u20132691. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. 2018. Procedural Level Generation Improves Generality of Deep Reinforcement Learning. CoRR abs/1806.10729 (2018). James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2016. Overcoming Catastrophic Forgetting in Neural Networks. CoRR abs/1612.00796 (2016). J. Zico Kolter and Andrew Y. Ng. 2009. Regularization and Feature Selection in Least-Squares Temporal Di\ufb00erence Learning. In Proceedings of the International Conference on Machine Learning (ICML). 521\u2013528. Yann Lecun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Ha\ufb00ner. 1998. Gradientbased Learning Applied to Document Recognition. IEEE 86, 11 (1998), 2278\u20132324. Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional Networks for Semantic Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 3431\u20133440. Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling. 2018. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. Journal of Arti\ufb01cial Intelligence Research 61 (2018), 523\u2013562. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-Level Control through Deep Reinforcement Learning. Nature 518, 7540 (2015), 529\u2013533. 19 Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. How Transferable are Neural Networks in NLP Applications?. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). 479\u2013489. Alex Nichol, Joshua Achiam, and John Schulman. 2018a. On First-Order MetaLearning Algorithms. CoRR abs/1803.02999 (2018). Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. 2018b. Gotta Learn Fast: A New Benchmark for Generalization in RL. CoRR abs/1804.03720 (2018). Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. 2016. Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning. In Proceedings of the International Conference on Learning Representations (ICLR). Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham M. Kakade. 2017. Towards Generalization and Simplicity in Continuous Control. In Advances in Neural Information Processing Systems (NeurIPS). 6550\u20136561. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. 2014. CNN Features O\ufb00-the-Shelf: An Astounding Baseline for Recognition. In Workshops of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 512\u2013519. Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive Neural Networks. CoRR abs/1606.04671 (2016). Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka GrabskaBarwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. 2018. Progress & Compress: A Scalable Framework for Continual Learning. In Proceedings of the International Conference on Machine Learning (ICML). 4535\u20134544. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature 529, 7587 (2016), 484\u2013489. Nitish Srivastava, Geo\ufb00rey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a Simple Way to Prevent Neural Networks from Over\ufb01tting. Journal of Machine Learning Research 15, 1 (2014), 1929\u20131958. Richard S. Sutton. 1995. Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. In Advances in Neural Information Processing Systems (NeurIPS). 1038\u20131044. 20 Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. 2017. Distral: Robust Multitask Reinforcement Learning. In Advances in Neural Information Processing Systems (NeurIPS). 4496\u20134506. Christopher Watkins and Peter Dayan. 1992. Technical Note: Q-Learning. Machine Learning 8, 3-4 (1992). Shimon Whiteson, Brian Tanner, Matthew E. Taylor, and Peter Stone. 2011. Protecting Against Evaluation Over\ufb01tting in Empirical Reinforcement Learning. In IEEE Symposium on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL). 120\u2013127. Sam Witty, Jun Ki Lee, Emma Tosch, Akanksha Atrey, Michael L. Littman, and David D. Jensen. 2018. Measuring and Characterizing Generalization in Deep Reinforcement Learning. CoRR abs/1812.02868 (2018). Jason Yosinski, Je\ufb00 Clune, Yoshua Bengio, and Hod Lipson. 2014. How Transferable are Features in Deep Neural Networks?. In Advances in Neural Information Processing Systems (NeurIPS). 3320\u20133328. Amy Zhang, Nicolas Ballas, and Joelle Pineau. 2018a. A Dissection of Over\ufb01tting and Generalization in Continuous Reinforcement Learning. CoRR abs/1806.07937 (2018). Chiyuan Zhang, Oriol Vinyals, R\u00b4emi Munos, and Samy Bengio. 2018b. A Study on Over\ufb01tting in Deep Reinforcement Learning. CoRR abs/1804.06893 (2018). 21 Appendix Game Modes We provide a brief description of each game \ufb02avour used in the paper. Freeway Freeway m0d0 Freeway m1d0 Freeway m4d0 In Freeway a chicken must cross a road containing multiple lanes of moving tra\ufb03c within a prespeci\ufb01ed time limit. In all modes of Freeway the agent is rewarded for reaching the top of the screen and is subsequently teleported to the bottom of the screen. If the chicken collides with a vehicle in di\ufb03culty 0 it gets bumped down one lane of tra\ufb03c, alternatively, in di\ufb03culty 1 the chicken gets teleported to its starting position at the bottom of the screen. Mode 1 changes some vehicle sprites to include buses, adds more vehicles to some lanes, and increases the velocity of all vehicles. Mode 4 is almost identical to Mode 1; the only di\ufb00erence being vehicles can oscillate between two speeds. Hero Hero m0d0 Hero m1d0 Hero m2d0 In Hero you control a character who must navigate a maze in order to save a trapped miner within a cave system. The agent scores points for any forward progression such as clearing an obstacle or killing an enemy. Once the miner is rescued, the level is terminated and you continue to the next level with a di\ufb00erent maze. Some levels have partially observable rooms, more enemies, and Videos of the di\ufb00erent modes are available in the following link: https://goo.gl/pCvPiD. 22 more di\ufb03cult obstacles to traverse. Past the default mode, each subsequent mode starts o\ufb00 at increasingly harder levels denoted by a level number increasing by multiples of 5. The default mode starts you o\ufb00 at level 1, mode 1 starts at level 5, and so on. Breakout Breakout m0d0 Breakout m12d0 In Breakout you control a paddle which can move horizontally along the bottom of the screen. At the beginning of the game, or on a loss of life the ball is set into motion and can bounce o\ufb00 the paddle and collide with bricks at the top of the screen. The objective of the game is to break all the bricks without having the ball fall below your paddles horizontal plane. Subsequently, mode 12 of Breakout hides the bricks from the player until the ball collides with the bricks in which case the bricks \ufb02ash for a brief moment before disappearing again. Space Invaders Space Invaders m0d0 Space Invaders m1d1 Space Invaders m9d0 When playing Space Invaders you control a spaceship which can move horizontally along the bottom of the screen. There is a grid of aliens above you and the objective of the game is to eliminate all the aliens. You are a\ufb00orded some protection from the alien bullets with three barriers just above your spaceship. Di\ufb03culty 1 of Space Invaders widens your spaceships sprite making it harder to dodge enemy bullets. Mode 1 of Space Invaders causes the shields above you to oscillate horizontally. Mode 9 of Space Invaders is similar to Mode 12 of Breakout where the aliens are partially observable until struck with the player\u2019s bullet. 23 ",
    "Evaluation": "",
    "Results": "Results were obtained with the default value of sticky actions (Machado et al., 2018). Game Variant 10M 50M 100M Best Action Freeway m0d0 3.0 (1.0) 31.4 (0.2) 32.1 (0.1) 23.0 (1.4) m1d0 0.0 (0.1) 4.8 (9.3) 7.5 (11.5) 5.0 (1.5) m1d1 0.0 (0.0) 0.0 (0.0) 2.5 (7.3) 4.2 (1.3) m4d0 4.4 (1.4) 29.9 (0.7) 32.8 (0.2) 7.5 (2.8) Hero m0d0 3187.8 (78.3) 9034.4 (1610.9) 13961.0 (181.9) 150.0 (0.0) m1d0 326.9 (40.3) 1425.2 (1755.1) 5026.8 (2174.6) 75.8 (7.5) m2d0 116.3 (11.0) 326.1 (130.4) 323.5 (76.4) 12.0 (27.5) Breakout m0d0 17.5 (2.0) 72.5 (7.7) 73.4 (13.5) 2.3 (1.3) m12d0 17.7 (1.3) 67.6 (32.4) 55.2 (37.2) 1.8 (1.1) Space Invaders m0d0 250.3 (16.2) 698.8 (32.2) 927.1 (85.3) 243.6 (95.9) m1d0 203.6 (24.3) 753.6 (31.6) 979.7 (39.8) 192.6 (65.7) m1d1 193.6 (11.0) 698.5 (31.3) 906.9 (56.5) 180.9 (101.9) m9d0 173.0 (17.8) 518.0 (16.7) 567.7 (40.1) 174.6 (65.9) Table 6: Baseline results in the default \ufb02avour with dropout and \u21132 regularization. We report the average over \ufb01ve runs (std. deviations are reported between parentheses). We used the default value of sticky actions (Machado et al., 2018). Game Variant 10M 50M 100M Best Action Freeway m0d0 4.6 (5.0) 25.9 (0.6) 29.0 (0.8) 23.0 (1.4) Hero m0d0 2466.5 (630.8) 6505.9 (1843.0) 12446.9 (397.4) 150.0 (0.0) Breakout m0d0 6.1 (2.7) 34.1 (1.8) 66.4 (3.6) 2.3 (1.3) Space Invaders m0d0 214.6 (13.8) 623.1 (16.3) 617.4 (29.6) 243.6 (95.9) Finally, it is obvious that we want algorithms that can generalize across tasks. Ultimately we want agents that can keep learning as they interact with the world in a continual learning fashion. We believe the \ufb02avours of Atari 2600 games can be a stepping stone towards this goal. Our results suggested that regularizing and \ufb01ne-tuning representations in deep RL might be a viable approach towards improving sample e\ufb03ciency and generalization on multiple tasks. It is particularly interesting that \ufb01ne-tuning a regularized network was the most successful approach because this might also be applicable in the continual learning settings where the environment changes without the agent being told so, and re-initializing layers of a network is obviously not an option. 17 ",
    "References": "References Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight Experience Replay. In Advances in Neural Information Processing Systems (NeurIPS). 5048\u20135058. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Arti\ufb01cial Intelligence Research 47 (2013), 253\u2013279. Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. 2019. Quantifying Generalization in Reinforcement Learning. In Proceedings of the International Conference on Machine Learning (ICML). 1282\u20131289. Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. 2017. Sharp Minima Can Generalize For Deep Nets. In Proceedings of the International Conference on Machine Learning (ICML). 1019\u20131028. Lasse Espeholt, Hubert Soyer, R\u00b4emi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. 2018. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. In Proceedings of the International Conference on Machine Learning (ICML). 1406\u20131415. Amir Massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv\u00b4ari, and Shie Mannor. 2008. Regularized Policy Iteration. In Advances in Neural Information Processing Systems (NeurIPS). 441\u2013448. Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic MetaLearning for Fast Adaptation of Deep Networks. In Proceedings of the International Conference on Machine Learning (ICML). 1126\u20131135. Xavier Glorot and Yoshua Bengio. 2010. Understanding the Di\ufb03culty of Training Deep Feedforward Neural Networks. In Proceedings of the International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS). 249\u2013256. 18 Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep Reinforcement Learning That Matters. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence (AAAI). 3207\u2013 3214. Jeremy Howard and Sebastian Ruder. 2018. Fine-tuned Language Models for Text Classi\ufb01cation. CoRR abs/1801.06146 (2018). Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. 2019. Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning. In Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence (IJCAI). 2684\u20132691. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. 2018. Procedural Level Generation Improves Generality of Deep Reinforcement Learning. CoRR abs/1806.10729 (2018). James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2016. Overcoming Catastrophic Forgetting in Neural Networks. CoRR abs/1612.00796 (2016). J. Zico Kolter and Andrew Y. Ng. 2009. Regularization and Feature Selection in Least-Squares Temporal Di\ufb00erence Learning. In Proceedings of the International Conference on Machine Learning (ICML). 521\u2013528. Yann Lecun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Ha\ufb00ner. 1998. Gradientbased Learning Applied to Document Recognition. IEEE 86, 11 (1998), 2278\u20132324. Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional Networks for Semantic Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 3431\u20133440. Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling. 2018. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. Journal of Arti\ufb01cial Intelligence Research 61 (2018), 523\u2013562. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-Level Control through Deep Reinforcement Learning. Nature 518, 7540 (2015), 529\u2013533. 19 Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. How Transferable are Neural Networks in NLP Applications?. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). 479\u2013489. Alex Nichol, Joshua Achiam, and John Schulman. 2018a. On First-Order MetaLearning Algorithms. CoRR abs/1803.02999 (2018). Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, and John Schulman. 2018b. Gotta Learn Fast: A New Benchmark for Generalization in RL. CoRR abs/1804.03720 (2018). Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. 2016. Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning. In Proceedings of the International Conference on Learning Representations (ICLR). Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham M. Kakade. 2017. Towards Generalization and Simplicity in Continuous Control. In Advances in Neural Information Processing Systems (NeurIPS). 6550\u20136561. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. 2014. CNN Features O\ufb00-the-Shelf: An Astounding Baseline for Recognition. In Workshops of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 512\u2013519. Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive Neural Networks. CoRR abs/1606.04671 (2016). Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka GrabskaBarwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. 2018. Progress & Compress: A Scalable Framework for Continual Learning. In Proceedings of the International Conference on Machine Learning (ICML). 4535\u20134544. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature 529, 7587 (2016), 484\u2013489. Nitish Srivastava, Geo\ufb00rey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a Simple Way to Prevent Neural Networks from Over\ufb01tting. Journal of Machine Learning Research 15, 1 (2014), 1929\u20131958. Richard S. Sutton. 1995. Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. In Advances in Neural Information Processing Systems (NeurIPS). 1038\u20131044. 20 Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. 2017. Distral: Robust Multitask Reinforcement Learning. In Advances in Neural Information Processing Systems (NeurIPS). 4496\u20134506. Christopher Watkins and Peter Dayan. 1992. Technical Note: Q-Learning. Machine Learning 8, 3-4 (1992). Shimon Whiteson, Brian Tanner, Matthew E. Taylor, and Peter Stone. 2011. Protecting Against Evaluation Over\ufb01tting in Empirical Reinforcement Learning. In IEEE Symposium on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL). 120\u2013127. Sam Witty, Jun Ki Lee, Emma Tosch, Akanksha Atrey, Michael L. Littman, and David D. Jensen. 2018. Measuring and Characterizing Generalization in Deep Reinforcement Learning. CoRR abs/1812.02868 (2018). Jason Yosinski, Je\ufb00 Clune, Yoshua Bengio, and Hod Lipson. 2014. How Transferable are Features in Deep Neural Networks?. In Advances in Neural Information Processing Systems (NeurIPS). 3320\u20133328. Amy Zhang, Nicolas Ballas, and Joelle Pineau. 2018a. A Dissection of Over\ufb01tting and Generalization in Continuous Reinforcement Learning. CoRR abs/1806.07937 (2018). Chiyuan Zhang, Oriol Vinyals, R\u00b4emi Munos, and Samy Bengio. 2018b. A Study on Over\ufb01tting in Deep Reinforcement Learning. CoRR abs/1804.06893 (2018). 21 Appendix Game Modes We provide a brief description of each game \ufb02avour used in the paper. Freeway Freeway m0d0 Freeway m1d0 Freeway m4d0 In Freeway a chicken must cross a road containing multiple lanes of moving tra\ufb03c within a prespeci\ufb01ed time limit. In all modes of Freeway the agent is rewarded for reaching the top of the screen and is subsequently teleported to the bottom of the screen. If the chicken collides with a vehicle in di\ufb03culty 0 it gets bumped down one lane of tra\ufb03c, alternatively, in di\ufb03culty 1 the chicken gets teleported to its starting position at the bottom of the screen. Mode 1 changes some vehicle sprites to include buses, adds more vehicles to some lanes, and increases the velocity of all vehicles. Mode 4 is almost identical to Mode 1; the only di\ufb00erence being vehicles can oscillate between two speeds. Hero Hero m0d0 Hero m1d0 Hero m2d0 In Hero you control a character who must navigate a maze in order to save a trapped miner within a cave system. The agent scores points for any forward progression such as clearing an obstacle or killing an enemy. Once the miner is rescued, the level is terminated and you continue to the next level with a di\ufb00erent maze. Some levels have partially observable rooms, more enemies, and Videos of the di\ufb00erent modes are available in the following link: https://goo.gl/pCvPiD. 22 more di\ufb03cult obstacles to traverse. Past the default mode, each subsequent mode starts o\ufb00 at increasingly harder levels denoted by a level number increasing by multiples of 5. The default mode starts you o\ufb00 at level 1, mode 1 starts at level 5, and so on. Breakout Breakout m0d0 Breakout m12d0 In Breakout you control a paddle which can move horizontally along the bottom of the screen. At the beginning of the game, or on a loss of life the ball is set into motion and can bounce o\ufb00 the paddle and collide with bricks at the top of the screen. The objective of the game is to break all the bricks without having the ball fall below your paddles horizontal plane. Subsequently, mode 12 of Breakout hides the bricks from the player until the ball collides with the bricks in which case the bricks \ufb02ash for a brief moment before disappearing again. Space Invaders Space Invaders m0d0 Space Invaders m1d1 Space Invaders m9d0 When playing Space Invaders you control a spaceship which can move horizontally along the bottom of the screen. There is a grid of aliens above you and the objective of the game is to eliminate all the aliens. You are a\ufb00orded some protection from the alien bullets with three barriers just above your spaceship. Di\ufb03culty 1 of Space Invaders widens your spaceships sprite making it harder to dodge enemy bullets. Mode 1 of Space Invaders causes the shields above you to oscillate horizontally. Mode 9 of Space Invaders is similar to Mode 12 of Breakout where the aliens are partially observable until struck with the player\u2019s bullet. 23 Experimental Details Architecture and hyperparameters All experiments performed in this paper utilized the neural network architecture proposed by Mnih et al. (2015). That is, a convolutional neural network with three convolutional layers and two fully connected layers. A visualization of this network can be found in Figure 5. Unless otherwise speci\ufb01ed, hyperparametes are kept consistent with the ALE baselines discussed by Machado et al. (2018). A summary of the parameters, which were consistent across all experiments, can be found in in Table 7. 1024 18 ReLU ReLU ReLU ReLU fc fc Conv 32,8x8 stride 4   Conv 64,4x4 stride 2   Conv 64,3x3 stride 1   Q(St, \u00b7; \u2713) <latexit sha1_base64=\"cqv76kuSkKevFHNmxXRkOhkDf/U=\">ACAHicbZDLSsNAFIYn9VbrLerChZvBI lSQkoig4KboxmWL9gJNCJPJpB06uTBzIpTQja/ixoUibn0Md76N0zYLrf4w8PGfczhzfj8VXIFlfRmlpeWV1bXyemVjc2t7x9zd6gk5S1aSIS2fOJYoLHrA0cBOulkpHIF6zrj26m9e4Dk4on8T2MU+ZGZBDzkFMC2vLMg 1btzoNT7NAgStHAwZkBPrFp1ayb8F+wCqhQ0zM/nSChWcRioIo1betFNycSOBUsEnFyRLCR2RAetrjEnElJvPDpjgY+0EOEykfjHgmftzIieRUuPI150RgaFarE3N/2r9DMJLN+dxmgGL6XxRmAkMCZ6mgQMuGQUx1k Co5PqvmA6JBR0ZhUdgr148l/onNVtq263zquN6yKOMjpER6iGbHSBGugWNVEbUTRBT+gFvRqPxrPxZrzPW0tGMbOPfsn4+AZqxJUA</latexit> <latexit sha1_base64=\"cqv76kuSkKevFHNmxXRkOhkDf/U=\">ACAHicbZDLSsNAFIYn9VbrLerChZvBI lSQkoig4KboxmWL9gJNCJPJpB06uTBzIpTQja/ixoUibn0Md76N0zYLrf4w8PGfczhzfj8VXIFlfRmlpeWV1bXyemVjc2t7x9zd6gk5S1aSIS2fOJYoLHrA0cBOulkpHIF6zrj26m9e4Dk4on8T2MU+ZGZBDzkFMC2vLMg 1btzoNT7NAgStHAwZkBPrFp1ayb8F+wCqhQ0zM/nSChWcRioIo1betFNycSOBUsEnFyRLCR2RAetrjEnElJvPDpjgY+0EOEykfjHgmftzIieRUuPI150RgaFarE3N/2r9DMJLN+dxmgGL6XxRmAkMCZ6mgQMuGQUx1k Co5PqvmA6JBR0ZhUdgr148l/onNVtq263zquN6yKOMjpER6iGbHSBGugWNVEbUTRBT+gFvRqPxrPxZrzPW0tGMbOPfsn4+AZqxJUA</latexit> <latexit sha1_base64=\"cqv76kuSkKevFHNmxXRkOhkDf/U=\">ACAHicbZDLSsNAFIYn9VbrLerChZvBI lSQkoig4KboxmWL9gJNCJPJpB06uTBzIpTQja/ixoUibn0Md76N0zYLrf4w8PGfczhzfj8VXIFlfRmlpeWV1bXyemVjc2t7x9zd6gk5S1aSIS2fOJYoLHrA0cBOulkpHIF6zrj26m9e4Dk4on8T2MU+ZGZBDzkFMC2vLMg 1btzoNT7NAgStHAwZkBPrFp1ayb8F+wCqhQ0zM/nSChWcRioIo1betFNycSOBUsEnFyRLCR2RAetrjEnElJvPDpjgY+0EOEykfjHgmftzIieRUuPI150RgaFarE3N/2r9DMJLN+dxmgGL6XxRmAkMCZ6mgQMuGQUx1k Co5PqvmA6JBR0ZhUdgr148l/onNVtq263zquN6yKOMjpER6iGbHSBGugWNVEbUTRBT+gFvRqPxrPxZrzPW0tGMbOPfsn4+AZqxJUA</latexit> <latexit sha1_base64=\"cqv76kuSkKevFHNmxXRkOhkDf/U=\">ACAHicbZDLSsNAFIYn9VbrLerChZvBI lSQkoig4KboxmWL9gJNCJPJpB06uTBzIpTQja/ixoUibn0Md76N0zYLrf4w8PGfczhzfj8VXIFlfRmlpeWV1bXyemVjc2t7x9zd6gk5S1aSIS2fOJYoLHrA0cBOulkpHIF6zrj26m9e4Dk4on8T2MU+ZGZBDzkFMC2vLMg 1btzoNT7NAgStHAwZkBPrFp1ayb8F+wCqhQ0zM/nSChWcRioIo1betFNycSOBUsEnFyRLCR2RAetrjEnElJvPDpjgY+0EOEykfjHgmftzIieRUuPI150RgaFarE3N/2r9DMJLN+dxmgGL6XxRmAkMCZ6mgQMuGQUx1k Co5PqvmA6JBR0ZhUdgr148l/onNVtq263zquN6yKOMjpER6iGbHSBGugWNVEbUTRBT+gFvRqPxrPxZrzPW0tGMbOPfsn4+AZqxJUA</latexit> Figure 5: Network architecture used by DQN to predict state-action values. Table 7: Hyperparameters for baseline results. Learning rate \u03b1 0.00025 Minibatch size 32 Learning frequency 4 Frame skip 5 Sticky action prob. 0.25 Replay bu\ufb00er size 1, 000, 000 \u03f5 decay period 1M frames \u03f5 initial 1.0 \u03f5 \ufb01nal 0.01 Discount factor \u03b3 0.99 Evaluation We adhere to the evaluation methodologies set out by Machado et al. (2018). This includes the use of all 18 primitive actions in the ALE, not utilizing loss of life as episode termination, and the use of sticky actions to inject stochasticity. Each result outlined in this paper averages the agents performance over 100 episodes further averaged over \ufb01ve runs. We do not take the maximum over runs nor the maximum over the learning curve. When comparing results in this paper and with other evaluation methodologies it is worth noting the following terminology and time scales. We use a frame skip of 5 frames, i.e., following every action executed by the agent the simulator advances 5 frames into the future. The agent will take # frames/5 actions within the environment over the duration of each experiment. One step of stochastic gradient descent to update the network parameters is performed every 4 actions. The training routine will perform # frames/5\u00b74 gradient updates over the duration of each experiment. Therefore, when we discuss experiments with a duration of 50M frames this is in actuality 50M simulator frames, 10M agent steps, and 2.5M gradient updates. Code available at https://github.com/jessefarebro/dqn-ale. 24 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train \u21132 \u03bb = 10\u22126 \u03bb = 10\u22125 \u03bb = 10\u22124 \u03bb = 10\u22123 \u03bb = 10\u22122 (a) Performance during training in the default mode of Freeway with various values for \u03bb. 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 eval. \u21132 \u03bb = 10\u22126 \u03bb = 10\u22125 \u03bb = 10\u22124 \u03bb = 10\u22123 \u03bb = 10\u22122 (b) Performance in Freeway m1d0 from an agent trained with various values of \u03bb in Freeway m0d0. 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 eval. \u21132 \u03bb = 10\u22126 \u03bb = 10\u22125 \u03bb = 10\u22124 \u03bb = 10\u22123 \u03bb = 10\u22122 (c) Performance in Freeway m4d0 from an agent trained with various values of \u03bb in Freeway m0d0. Figure 6: Training and evaluation performance for DQN in Freeway using di\ufb00erent values of \u03bb. Regularization Ablation Study To gain better insight into the over\ufb01tting results presented in the paper, we performed an ablation study on the two main hyperparameters used to study generalization, \u21132 regularization and dropout (Srivastava et al., 2014). To perform this ablation study we trained an agent in the default \ufb02avour of Freeway (i.e., m0d0) for 50M frames and evaluated it in two di\ufb00erent \ufb02avours, Freeway m1d0, and Freeway m4d0. In the evaluation phase we took checkpoints every 500, 000 frames during training and subsequently recorded the mean performance over 100 episodes. All results presented in this section are averaged over 5 seeds. We tested the e\ufb00ects of \u21132 regularization, dropout, and the combination of these two methods. We varied the weighted importance \u03bb of our \u21132 term in the DQN loss function as well as studied the dropout rate for the three convolutional layers pconv, and the \ufb01rst fully connected layer pfc. We used the loss function L DQN = E \u03c4 \u223c U(\u00b7) \ufffd\ufffd Rt+1 + \u03b3 max a\u2032\u2208A Q(St+1, a\u2032; \u03b8\u2212) \u2212 Q(St, At; \u03b8) \ufffd2\ufffd + \u03bb \u2225\u03b8\u22252 2 , where \u03c4 = (St, At, Rt+1, St+1) are uniformly sampled from U(\u00b7), the experience replay bu\ufb00er \ufb01lled with experience collected by the agent. We considered the values \u03bb \u2208 {10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126} for \u21132 regularization, as well as the values pconv, pfc \u2208 {(0.05, 0.1), (0.1, 0.2), (0.15, 0.3), (0.2, 0.4), (0.25, 0.5)} for dropout. We conclude by analyzing the cartesian product of these two sets to study the e\ufb00ects of combining the two methods. \u21132 regularization We begin by analyzing the training performance for DQN in Freeway m0d0 for di\ufb00erent values of \u03bb. We also provide evaluation curves for m1d0, and m4d0 of Freeway. Both sets of experiments are presented in Figure 6. 25 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train dropout pconv, pfc = 0.05 0.1 pconv, pfc = 0.15 0.3 pconv, pfc = 0.1 0.2 pconv, pfc = 0.25 0.5 pconv, pfc = 0.2 0.4 (a) Performance during training in the default mode of Freeway with various values for pconv, pfc. 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 eval. dropout pconv, pfc = 0.05 0.1 pconv, pfc = 0.15 0.3 pconv, pfc = 0.1 0.2 pconv, pfc = 0.25 0.5 pconv, pfc = 0.2 0.4 (b) Performance in Freeway m1d0 from an agent trained with various values for pconv, pfc in m0d0. 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 eval. dropout pconv, pfc = 0.05 0.1 pconv, pfc = 0.15 0.3 pconv, pfc = 0.1 0.2 pconv, pfc = 0.25 0.5 pconv, pfc = 0.2 0.4 (c) Performance in Freeway m4d0 from an agent trained with various values for pconv, pfc in m0d0. Figure 7: Training and evaluation performance for DQN in Freeway using di\ufb00erent values pconv, pfc, the dropout rate for the convolutional layers and the \ufb01rst fully connected layer respectively. Large values of \u03bb seem to hurt training performance and smaller values are weak enough that the agent begins to over\ufb01t to m0d0. It is worth noting the performance during evaluation in m4d0 is similar to an agent trained without \u21132 regularization. The bene\ufb01ts of \u21132 do not seem to be apparent in m4d0 but provide improvement in m1d0. Dropout We provide results in Figure 7 depicting the training performance of the Freeway m0d0 agent with varying values of pconv, pfc. As with \u21132 regularization, we further evaluate each agent checkpoint for 100 episodes in the target \ufb02avour during training. Dropout seems to have a much bigger impact on the training performance when contrasting the results presented for \u21132 regularization in Figure 6. Curiously, larger values for the dropout rate can cause the agents\u2019 performance to \ufb02atline in both training and evaluation. The network may learn to bias a speci\ufb01c action, or sequence of actions independent of the state. However, reasonable dropout rates seem to improve the agents ability to generalize in both m1d0 and m4d0. Combining \u21132 regularization and dropout Commonly, we see dropout and \u21132 regularization combined in many supervised learning applications. We want to further explore the possibility that these two methods can provide bene\ufb01ts in tandem. We exhaust the cross product of the two sets of values examined above. We \ufb01rst analyze the impact these methods have on the training procedure in Freeway m0d0. Learning curves are presented in Figure 8. Interestingly, the combination of these methods can provide increased stability to the training procedure compared to the results in Figure 7. For example, 26 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train \u21132 + dropout (pconv, pfc = 0.05, 0.1) pconv, pfc = 0.05, 0.1; \u03bb = 10\u22126 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22125 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22124 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22123 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22122 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train \u21132 + dropout (pconv, pfc = 0.1, 0.2) pconv, pfc = 0.1, 0.2; \u03bb = 10\u22126 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22125 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22124 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22123 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22122 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train \u21132 + dropout (pconv, pfc = 0.15, 0.3) pconv, pfc = 0.15, 0.3; \u03bb = 10\u22126 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22125 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22124 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22123 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22122 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train \u21132 + dropout (pconv, pfc = 0.2, 0.4) pconv, pfc = 0.2, 0.4; \u03bb = 10\u22126 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22125 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22124 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22123 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22122 10M 20M 30M 40M 50M Number of Frames 5 10 15 20 25 30 Cumulative Reward Freeway m0d0 train \u21132 + dropout (pconv, pfc = 0.25, 0.5) pconv, pfc = 0.25, 0.5; \u03bb = 10\u22126 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22125 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22124 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22123 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22122 Figure 8: Performance during training on the default \ufb02avour of Freeway. For each plot pconv, pfc is held constant while varying the \u21132 regularization term \u03bb. Each parameter con\ufb01guration is averaged over \ufb01ve seeds. the con\ufb01guration pconv, pfc = 0.1, 0.2 scores less than 15 when solely utilizing dropout. When applying \u21132 regularization in tandem we can see the performance hover around 20 for moderate values of \u03bb. We continue observe the \ufb02atline behaviour for large values of pconv, pfc, regardless of \u21132 regularization. We now examine the evaluation performance for each parameter con\ufb01guration in both Freeway m1d0, and Freeway m4d0. These results are presented in Figure 9 for m1d0, and Figure 10 for m4d0. We observe that \u21132 regularization struggled to provide much bene\ufb01t in Freeway m4d0. Reasonable values of dropout seem to aid generalization performance in both modes tested. It does seem that balancing the two methods of regularization can provide some bene\ufb01ts, such as an increased training stability and more consistent zero-shot generalization performance. From the beginning we maintained a heuristic prescribing a balance between training performance and zero-shot generalization performance. In order to strike this balance we chose the parameters pconv, pfc = 0.05, 0.1 for the dropout rate, and \u03bb = 10\u22124 for the \u21132 regularization parameter. These seemed to strike the best balance in early testing and the results in the ablation study seem to con\ufb01rm our intuitions. 27 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 \u21132 + dropout (pconv, pfc = 0.05, 0.1) pconv, pfc = 0.05, 0.1; \u03bb = 10\u22126 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22125 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22124 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22123 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 \u21132 + dropout (pconv, pfc = 0.1, 0.2) pconv, pfc = 0.1, 0.2; \u03bb = 10\u22126 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22125 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22124 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22123 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 \u21132 + dropout (pconv, pfc = 0.15, 0.3) pconv, pfc = 0.15, 0.3; \u03bb = 10\u22126 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22125 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22124 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22123 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 \u21132 + dropout (pconv, pfc = 0.2, 0.4) pconv, pfc = 0.2, 0.4; \u03bb = 10\u22126 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22125 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22124 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22123 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 Cumulative Reward Freeway m1d0 \u21132 + dropout (pconv, pfc = 0.25, 0.5) pconv, pfc = 0.25, 0.5; \u03bb = 10\u22126 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22125 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22124 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22123 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22122 Figure 9: Evaluation performance for Freeway m1d0 post-training on Freeway m0d0 with dropout and \u21132. For each plot pconv, pfc is held constant while varying the \u21132 regularization term \u03bb. Each con\ufb01guration is averaged over \ufb01ve seeds. 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 \u21132 + dropout (pconv, pfc = 0.05, 0.1) pconv, pfc = 0.05, 0.1; \u03bb = 10\u22126 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22125 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22124 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22123 pconv, pfc = 0.05, 0.1; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 \u21132 + dropout (pconv, pfc = 0.1, 0.2) pconv, pfc = 0.1, 0.2; \u03bb = 10\u22126 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22125 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22124 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22123 pconv, pfc = 0.1, 0.2; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 \u21132 + dropout (pconv, pfc = 0.15, 0.3) pconv, pfc = 0.15, 0.3; \u03bb = 10\u22126 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22125 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22124 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22123 pconv, pfc = 0.15, 0.3; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 \u21132 + dropout (pconv, pfc = 0.2, 0.4) pconv, pfc = 0.2, 0.4; \u03bb = 10\u22126 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22125 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22124 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22123 pconv, pfc = 0.2, 0.4; \u03bb = 10\u22122 0M 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 25 Cumulative Reward Freeway m4d0 \u21132 + dropout (pconv, pfc = 0.25, 0.5) pconv, pfc = 0.25, 0.5; \u03bb = 10\u22126 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22125 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22124 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22123 pconv, pfc = 0.25, 0.5; \u03bb = 10\u22122 Figure 10: Evaluation performance for Freeway m4d0 post-training on Freeway m0d0 with dropout and \u21132. We used the same method described in Figure 9. 28 Policy Evaluation Learning Curves We provide learning curves for policy evaluation from a \ufb01xed representation in the default \ufb02avour of each game we analyzed. Each subplot results from evaluating a policy in the target \ufb02avour which was trained with and without regularization in the default \ufb02avour. We speci\ufb01cally took weight checkpoints during training every 500, 000 frames, up to 50M frames in total. Each checkpoint was then evaluated in the target \ufb02avour for 100 episodes averaged over \ufb01ve runs. The regularized representation was trained using a dropout rate of pconv, pfc = 0.05, 0.1, and \u03bb = 10\u22124 for \u21132 regularization. 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 7 8 Cumulative Reward Freeway m1d0 m1d0 m1d0 dropout + \u21132 10M 20M 30M 40M 50M Frames before evaluation 1 2 3 4 5 6 Cumulative Reward Freeway m1d1 m1d1 m1d1 dropout + \u21132 10M 20M 30M 40M 50M Frames before evaluation 5 10 15 20 Cumulative Reward Freeway m4d0 m4d0 m4d0 dropout + \u21132 0M 10M 20M 30M 40M 50M Frames before evaluation 25 50 75 100 125 150 175 Cumulative Reward Hero m1d0 m1d0 m1d0 dropout + \u21132 0M 10M 20M 30M 40M 50M Frames before evaluation 20 40 60 80 100 Cumulative Reward Hero m2d0 m2d0 m2d0 dropout + \u21132 0M 10M 20M 30M 40M 50M Frames before evaluation 10 20 30 40 50 Cumulative Reward Breakout m12d0 m12d0 m12d0 dropout + \u21132 10M 20M 30M 40M 50M Frames before evaluation 50 100 150 200 250 300 350 Cumulative Reward Space Invaders m1d0 m1d0 m1d0 dropout + \u21132 10M 20M 30M 40M 50M Frames before evaluation 50 100 150 200 250 Cumulative Reward Space Invaders m1d1 m1d1 m1d1 dropout + \u21132 10M 20M 30M 40M 50M Frames before evaluation 50 100 150 200 250 300 Cumulative Reward Space Invaders m9d0 m9d0 m9d0 dropout + \u21132 Figure 11: Performance curves for policy evaluation results. The x-axis is the number of frames before we evaluated the \u03f5-greedy policy from the default \ufb02avour on the target \ufb02avour. The y-axis is the cumulative reward the agent incurred. Green curves depict performance with regularization and red curves without. 29 ",
    "title": "Generalization and Regularization in DQN",
    "paper_info": "Generalization and Regularization in DQN\nJesse Farebrother\u22171, Marlos C. Machado2, and Michael Bowling1,3\n1University of Alberta, 2Google Research, 3DeepMind Alberta\nAbstract\nDeep reinforcement learning algorithms have shown an impressive\nability to learn complex control policies in high-dimensional tasks. However,\ndespite the ever-increasing performance on popular benchmarks, policies\nlearned by deep reinforcement learning algorithms can struggle to generalize\nwhen evaluated in remarkably similar environments. In this paper we\npropose a protocol to evaluate generalization in reinforcement learning\nthrough di\ufb00erent modes of Atari 2600 games. With that protocol we assess\nthe generalization capabilities of DQN, one of the most traditional deep\nreinforcement learning algorithms, and we provide evidence suggesting that\nDQN overspecializes to the training environment. We then comprehensively\nevaluate the impact of dropout and \u21132 regularization, as well as the\nimpact of reusing learned representations to improve the generalization\ncapabilities of DQN. Despite regularization being largely underutilized in\ndeep reinforcement learning, we show that it can, in fact, help DQN learn\nmore general features. These features can be reused and \ufb01ne-tuned on\nsimilar tasks, considerably improving DQN\u2019s sample e\ufb03ciency.\n1\nIntroduction\nRecently, reinforcement learning (RL) algorithms have proven very successful on\ncomplex high-dimensional problems, in large part due to the use of deep neural\nnetworks for function approximation (e.g., Mnih et al., 2015; Silver et al., 2016).\nDespite the generality of the proposed solutions, applying these algorithms to\nslightly di\ufb00erent environments often requires agents to learn the new task from\nscratch. The learned policies rarely generalize to other domains and the learned\nrepresentations are seldom reusable. On the other hand, deep neural networks\nare lauded for their generalization capabilities (e.g., Lecun et al., 1998), with\nsome communities heavily relying on reusing learned representations in di\ufb00erent\nproblems. In light of the successes of supervised learning methods, the lack of\ngeneralization or reusable knowledge (i.e., policies, representation) acquired by\ncurrent deep RL algorithms is somewhat surprising.\n\u2217Corresponding author. Contact: jfarebro@cs.ualberta.ca.\n1\narXiv:1810.00123v3  [cs.LG]  17 Jan 2020\n",
    "GPTsummary": "- (1): The author aims to address the issue of deep reinforcement learning algorithms struggling to generalize when evaluated in similar environments, despite being able to learn complex control policies in high-dimensional tasks.\n\n- (2): The past methods involve the lack of generalization or reusable knowledge acquired by current deep RL algorithms, despite the success of supervised learning methods in reusing learned representations for different problems. The approach is well motivated as the lack of generalization or policy transferability is a major weakness of current deep RL algorithms.\n\n- (3): The author proposes a protocol to evaluate generalization in reinforcement learning using different modes of Atari 2600 games, assesses the generalization capabilities of DQN, and evaluates the impact of dropout and \u21132 regularization on DQN's generalization capabilities.\n\n- (4): The performance of the methods is evaluated on different modes of Atari 2600 games, and the author provides evidence suggesting that DQN overspecializes to the training environment. The impact of regularization on DQN's ability to learn more general features and its sample efficiency is shown, indicating that regularization can help improve DQN's generalization capabilities. The results support the author's goals of improving the generalization and sample efficiency of deep reinforcement learning algorithms.\n\n\n\n\n\n8. Conclusion: \n- (1) The significance of this piece of work is to address the issue of deep reinforcement learning algorithms struggling to generalize when evaluated in similar environments. The proposed protocol evaluates generalization in reinforcement learning using different modes of Atari 2600 games, assesses the generalization capabilities of DQN, and evaluates the impact of dropout and \u21132 regularization on DQN's generalization capabilities.\n \n- (2) Innovation point: The article proposes a protocol to evaluate generalization in reinforcement learning by testing different modes of Atari 2600 games. Performance: The evaluation shows that DQN overspecializes to the training environment, and regularization can help improve DQN\u2019s generalization capabilities and sample efficiency. Workload: The article provides a substantial amount of evidence and analysis, which could require significant effort to replicate and reproduce.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "\n"
}