{
    "Introduction": "Introduction The integration of reinforcement learning (RL) and imitation learning (IL) is an important problem that has been studied for a long time in the \ufb01eld of intelligent robotics and machine learning [1\u201314]. RL and IL are both well-studied machine learning problems. The goal is to provide robots with capabilities to optimize policies automatically. RL is conventionally formalized as a reward maximization problem. An RL agent explores the optimal policy by maximizing the cumulative rewards. However, it requires many trials in most of the robotic control problems. Because they have high dimensional states-and-actions spaces, exploration and optimization becomes a di\ufb03cult problem. In contrast, IL can acquire behavior by mimicking the behaviors of experts. However, its performance depends on the expert, and there is no guarantee of acquiring an optimal policy. As a result, RL and IL have their own drawbacks. The research for integrating RL and IL has been conducted to overcome problems and make use of the learner\u2019s trial-and-error experience and expert\u2019s demonstrations. However, most variations are designed heuristically, i.e., few have a sophisticated theoretical basis [8]. This tends to lead to the heuristic parameter tuning and endless exploration of the variants of frameworks for integration. The main goal of this paper is to present the integration of reinforcement and imitation learning methods into a single theoretical framework, i.e., probabilistic graphical model and its inference. Such integration allows the learning agents to use expert demonstrations and agent trial and error in a synergistic manner, enabling them to perform complicated tasks, such as robotic control, more swiftly. For this purpose, we developed the probabilistic graphical model arXiv:1907.02140v2  [cs.LG]  16 Oct 2019 ",
    "Background": "Background 2.1 Reinforcement Learning as Probabilistic Inference Our proposal is based on the idea of the extension of PGM for RL as probabilistic inference (a.k.a. control as inference). Therefore, in this section, we brie\ufb02y introduce RL as probabilistic inference [22]. A Markov decision process (MDP) is de\ufb01ned as M = (S, A, P, r, T). S represents the state space. A represents the action space. P : S \u00d7 A \u00d7 S \u2192 R denotes the transition probability distribution when taking action a in state s and going to s\u2032. r : S \u00d7 A \u2192 R is the reward function after taking action a in state s. T is the time horizon of the task. We de\ufb01ne \u03c0 as a stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1], and \u03c0E as an expert policy. The expert demonstrations \u03c4E is a set of trajectories sampled by policy \u03c0E. Trajectory \u03c4 consists of a sequence of state and action pairs. RL aims to optimize the parameters of the policy that maximizes the expected total reward October 17, 2019 Advanced Robotics 0.main  3, 2019 Advanced Robotics output a1 a2 a3 a4 s1 s2 s3 s4 a1 a2 a3 a4 s1 s2 s3 s4 O1 O2 O3 O4 Figure 1. Left: Graphical model of MDP with states and actions as stochastic variables. Right: Graphical model for reinforcement learning as probabilistic inference. The agent observes a single optimality Ot each timestep and infers the most probable action assuming that the optimality variables are taking true, i.e., Ot = 1. In this paper, we refer to the graphical model as a probabilistic graphical model from Markov decision process (pMDP). based on the objective function \u03b8\u22c6 = arg max \u03b8 T \u2211 t=1 E(st,at)\u223cp(st,at|\u03b8)[r(st, at)]. (1) Recently, signi\ufb01cant progress has been made with regard to RL in continuous action decision problems, such as video games or robot control tasks, by integrating deep learning [3, 23, 24]. As summarized by Levine in [22], the maximization of a reward in MDP can be re-formulated as the probabilistic inference problem in a PGM. The study of RL as probabilistic inference (also known as \u2019control as inference\u2019) is not new but rather has a decade of history [15\u201320]. Similar ideas have been proposed independently. Attias proposed an approach to the problem of planning under uncertainty using hidden Markov-style models [15]. Todorov formulated a linear solvable Markov decision process that enables e\ufb03cient approximation and determines the optimal policy e\ufb03ciently [16]. Kappen reformulated Todorov\u2019s non-linear stochastic optimal control problem as a Kullback-Leibler (KL) minimization problem and e\ufb03ciently calculated it by applying an approximate inference to the calculation of optimal control [17]. In RL, as probabilistic inference, the graphical model is constructed with optimality variables indicating whether or not a state-action pair is optimal. Figure 1 illustrates the graphical model of MDP with an optimality variable. The optimality variable in RL is denoted as p(Ot = 1|st, at) = exp(r(st, at)). (2) In the general RL approach, the reward r(st, at), which is a scalar value, cannot be expressed on PGM; however, by introducing the optimality variable Ot as in Equation (2), RL can be formulated as probabilistic inference. In RL as probabilistic inference, the policy is optimized by solving the probabilistic inference, such that the policy distribution is close to the optimal trajectory distribution conditioned to Ot = 1. One way to optimize actions is as a particular type of structured variational inference. We aim to approximate the posterior distribution over actions when we condition Ot = 1 for all t \u2208 {1, . . . , T}: p (\u03c4|o1:T ) \u221d p (\u03c4, o1:T ) = p (s1) \u220f t=1 p (Ot = 1|st, at) p (st+1|st, at) = p (s1) T \u220f t=1 exp (r (st, at)) p (st+1|st, at) = [ p (s1) T \u220f t=1 p (st+1|st, at) ] exp ( T \u2211 t=1 r (st, at) ) (3) Figure 1. Left: Probabilistic graphical model of MDP with states and actions as stochastic variables. Right: Probabilistic graphical model for reinforcement learning as probabilistic inference. The agent observes a single optimality Ot each timestep and infers the most probable action assuming that the optimality variables are taking true, i.e., Ot = 1. In this paper, we refer to the graphical model as a probabilistic graphical model from Markov decision process (pMDP). based on the objective function \u03b8\u22c6 = arg max \u03b8 T \ufffd t=1 E(st,at)\u223cp(st,at|\u03b8)[r(st, at)]. (1) Recently, signi\ufb01cant progress has been made with regard to RL in continuous action decision problems, such as video games or robot control tasks, by integrating deep learning [3, 23, 24]. As summarized by Levine in [22], the maximization of a reward in MDP can be re-formulated as the probabilistic inference problem in a PGM. The study of RL as probabilistic inference (a.k.a. control as inference) is not new but rather has a decade of history [15\u201320]. Similar ideas have been proposed independently. Attias proposed an approach to the problem of planning under uncertainty using hidden Markov-style models [15]. Todorov formulated a linear solvable Markov decision process that enables e\ufb03cient approximation and determines the optimal policy e\ufb03ciently [16]. Kappen et al. reformulated Todorov\u2019s non-linear stochastic optimal control problem as a Kullback-Leibler (KL) minimization problem and e\ufb03ciently calculated it by applying an approximate inference to the calculation of optimal control [17]. In RL as probabilistic inference, the graphical model is constructed with optimality variables indicating whether or not a state-action pair is optimal. Figure 1 illustrates the graphical model of MDP with an optimality variable. The optimality variable in RL is denoted as p(Ot = 1|st, at) = exp(r(st, at)). (2) By introducing the optimality variable Ot as in Equation (2), RL can be formulated as probabilistic inference. In RL as probabilistic inference, the policy is optimized by solving the probabilistic inference, such that the policy distribution is close to the optimal trajectory distribution conditioned to Ot = 1. One way to optimize actions is as a particular type of structured variational inference. We aim to approximate the posterior distribution over actions when we condition Ot = 1 for all t \u2208 {1, . . . , T}: p (\u03c4|O1:T ) \u221d p (\u03c4, O1:T ) = p (s1) \ufffd t=1 p (Ot = 1|st, at) p (st+1|st, at) = p (s1) T \ufffd t=1 exp (r (st, at)) p (st+1|st, at) = \ufffd p (s1) T \ufffd t=1 p (st+1|st, at) \ufffd exp \ufffd T \ufffd t=1 r (st, at) \ufffd (3) October 17, 2019 Advanced Robotics 0.main with another distribution: q(\u03c4) = q (s1) T \ufffd t=1 q (st+1|st, at) q (at|st) . (4) By optimizing the variational lower bound, we can approximate the inference of an optimal trajectory distribution. Using Equation 3, 4, and Jensen\u2019s inequality, the variational lower bound of the log-likelihood is given by log p (O1:T ) = log \ufffd\ufffd p (O1:T , s1:T , a1:T ) ds1:T da1:T = log \ufffd\ufffd p (O1:T , s1:T , a1:T ) q (s1:T , a1:T ) q (s1:T , a1:T )ds1:T da1:T = log E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) \ufffdp (O1:T , s1:T , a1:T ) q (s1:T , a1:T ) \ufffd \u2265 E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) [log p (O1:T , s1:T , a1:T ) \u2212 log q (s1:T , a1:T )] = E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) \ufffd T \ufffd t=1 r (st, at) \u2212 log q (at|st) \ufffd . (5) From the above, in the context of RL as inference, reward maximization by RL in MDP is equivalent to the objective function of maximum entropy RL [22]. In recent studies, maximum a posteriori policy optimization, which is one of the RL methods serving as a probabilistic inference, shows good results by applying the EM algorithm to the problem of RL [25]. Fu et al. formulated a probabilistic inference that maximizes the probability of the occurrence of events in the framework of control as inference and proposed a generalization of inverse reinforcement learning (IRL) methods to cases in which full demonstrations are not needed, such as when only samples of desired goal states are available [26]. 2.2 Generative Adversarial Imitation Learning We consider integrating GAIL into RL via the framework of PGM for RL. Therefore, in this section, we brie\ufb02y summarize GAIL and IL. The goal of IL is to acquire the behavior that mimics expert behavior. IL can be learned without reward signal r when compared to RL, but it is necessary to prepare expert demonstrations in advance. IL can be classi\ufb01ed into two approaches: (1) Behavior cloning (BC) [27, 28], which learns policy over state-action pairs in supervised learning on expert demonstration \u03c4E. BC has been successfully applied to autonomous driving [29] and locomotions [30, 31]. The BC approach is di\ufb03cult to use in the real world because of the compounding error caused by a covariate shift. (2) Inverse reinforcement learning (IRL) [21, 32\u201335] recovers the reward function under the assumption that the expert policy is optimal and learns the policy on the recovered reward function. However, inverse reinforcement learning is too expensive to perform because it requires solving an RL in its learning process loop. Recently, Ho and Ermon developed GAIL [21], which is the IL method inspired by generative adversarial networks (GAN) [36]. GAIL is able to imitate the policy for complex high-dimensional control tasks. In the GAIL framework, the agent imitates the behavior of the expert policy by matching the generated state-action distribution with the distribution of experts. GAIL\u2019s generator tries to make the discriminator recognize that the state-action pairs generOctober 17, 2019 Advanced Robotics 0.main ated by the policy are generated from an expert. GAIL\u2019s discriminator distinguishes state-action pairs from those generated by the generator and expert. As learning progresses, the discriminator guides the policy to match the state-action pairs generated by the generator with the expert state-action pairs. When the Jensen-Shannon divergence between agent policy distributions and expert policy distribution is minimized, the agent policy is optimal under the condition that the expert policy is the optimal policy. GAIL\u2019s objective function is denoted as max \u03b8 min \u03c9 E\u03c0\u03b8 [log(D\u03c9(s, a))] + E\u03c0E [log(1 \u2212 D\u03c9(s, a))] (6) where \u03c0\u03b8 is the agent policy that is the role of the generator, \u03c0E is the expert policy, and D\u03c9 is a discriminator that tries to distinguish state-action pairs generated from \u03c0\u03b8 and \u03c0E. In other words, D\u03c9 outputs the probability that the state-action pair is optimal under the assumption that the demonstrator\u2019s behavior is optimal. Parameters \u03b8 and \u03c9 are the parameters of the generator and discriminator, respectively, which are represented as the deep neural network. The generator is trained by a policy gradient method, such as trust region policy optimization [37] and proximal policy optimization (PPO) [38]. The discriminator is optimized using ADAM [39]. Some variants of GAIL have been proposed recently as well [40\u201343]. 2.3 Integration of Reinforcement Learning and Imitation Learning via PGM RL and IL are both well-established approaches; however, each has its own drawbacks. On the one hand, RL has the following drawbacks. First, it is di\ufb03cult to manually design the appropriate reward function in complicated or high-dimensional tasks, such as robotic control. Second, the computational cost of learning is prohibitively expensive because of the exploration of the policy space for the reward. On the other hand, IL has the following drawbacks. First, its performance depends on the expert, with no guarantee of acquiring the optimal policy. Second, if the number of expert demonstrations is small, it is di\ufb03cult for agents to learn adequately. Third, Most IL methods cannot e\ufb00ectively use environmental feedbacks. Several conventional works have attempted to improve performance by combining RL with the learning from demonstrations [1, 3\u20135]. By adapting RL to the policy learned from demonstrations, it is possible to avoid searching for unnecessary action space. The combination of RL and learning from demonstration can evaluate whether the task is performed well, such that the learned policy is improved. In an early work, Lin used a successful demonstration to improve RL more e\ufb03ciently in a 2Ddynamics game [1]. The use of demonstrations becomes more e\ufb00ective as the task complexity increases [2]. The most famous approach of imitation and reinforcement learning is AlphaGo, which was used to learn the game Go and was proposed by Silver et al [3]. As a di\ufb00erent approach to the combination of RL and IL, Brys used demonstrations as a prior knowledge for the formation of reward functions [4]. This approach of using expert demonstrations to form a reward function is similar to that of inverse reinforcement learning. Levine and Koltun generated guide samples from human demonstrations and used them to explore high reward areas of the policy space [5]. In recent years, many approaches have been proposed to improve performance by combining deep reinforcement learning and IL[6\u201314] However, most of the integration was performed in a heuristic manner and was not formulated on a single probabilistic generative model. The main goal of this paper is to present a theoretical learning framework that can be used in complicated tasks, such as robotic control, by integrating these methods through pMDP-MO and making use of the GAIL discriminator as an optimality distribution. October 17, 2019 Advanced Robotics 0.main ly 3, 2019 Advanced Robotics output a1 a2 a3 a4 s1 s2 s3 s4 O1 1 O1 2 O1 3 O1 4 ... ... ... ... ON 1 ON 2 ON 3 ON 4 Figure 2. Graphical model framework for reinforcement learning as probabilistic inference with multiple optimal emissions. In this framework, the agent observes multiple optimalities O1 t , ..., ON t and infers the optimal behavior. 3. Probabilistic Graphical Model for Markov Decision Process with Multiple Optimality Emissions (pMDP-MO) In this section, we develop the new PGM framework for representing MDP with simultaneous observation of multiple optimality emissions, called pMDP-MO. This graphical model is obtained by extending the graphical model framework for control as inference, as shown in Figure 1. We assume a Markov decision process model in which multiple optimalities for the state-action pair at the time t are observed simultaneously. Assuming that the respective optimality variables are independent of each other, the probability that multiple optimalities are observed is described as follows: p(O1 t , O2 t , . . . , ON t |st, at) = N \u220f n=1 p(On|st, at) (7) where N is the number of optimality types observed simultaneously and On t is the n-th optimality. Figure 2 depicts a graphical model in this setting in which multiple types of optimality are observed simultaneously. Based on PGM, we can derive the optimal trajectory probabilistic distribution p(\u03c4|O1 1:T , . . . , ON 1:T ) by considering that the state-action pairs are optimal O1 t = 1, . . . , ON t = 1 at time to horizon T. p(\u03c4|O1 1:T , . . . , ON 1:T ) \u221d p(\u03c4, O1 1:T , . . . , ON 1:T ) = p(s1) T \u220f t=1 p(st+1|st, at)p(O1 t |st, at) . . . p(ON t |st, at) (8) Using Equation 3,8 and Jensen\u2019s inequality, the variational lower bound of log-likelihood is Figure 2. Graphical model framework for reinforcement learning as probabilistic inference with multiple optimal emissions. In this framework, the agent observes multiple optimalities O1 t , ..., ON t and infers the optimal behavior. 3. Probabilistic Graphical Model for Markov Decision Process with Multiple Optimality Emissions (pMDP-MO) In this section, we develop the new PGM framework for representing MDP with simultaneous observation of multiple optimality emissions, called pMDP-MO. This graphical model is obtained by extending the graphical model framework for control as inference, as shown in Figure 1. We assume a Markov decision process model in which multiple optimalities for the state-action pair at the time t are observed simultaneously. Assuming that the respective optimality variables are independent of each other, the probability distribution that multiple optimality emissions are observed is described as follows: p(O1 t , O2 t , . . . , ON t |st, at) = N \ufffd n=1 p(On|st, at) (7) where N is the number of optimality types observed simultaneously and On t is the n-th optimality. Figure 2 depicts a graphical model in this setting in which multiple types of optimality are observed simultaneously. Based on PGM, we can derive the optimal trajectory probabilistic distribution p(\u03c4|O1 1:T , . . . , ON 1:T ) by considering that the state-action pairs are optimal O1 t = 1, . . . , ON t = 1 at time to horizon T. p(\u03c4|O1 1:T , ..., ON 1:T ) \u221d p(\u03c4, o1 1:T , ..., oN 1:T ) = p(s1) T \ufffd t=1 \ufffd p(st+1|st, at) N \ufffd n=1 p(On|st, at) \ufffd (8) Using Equation 3, 8 and Jensen\u2019s inequality, the variational lower bound of log-likelihood is October 17, 2019 Advanced Robotics 0.main given by log p \ufffd O1 1:T , . . . , ON 1:T \ufffd = log \ufffd\ufffd p \ufffd O1 1:T , . . . , ON 1:T , s1:T , a1:T \ufffd ds1:T da1:T = log \ufffd\ufffd p \ufffd O1 1:T , . . . , ON 1:T , s1:T , a1:T \ufffd q (s1:T , a1:T ) q (s1:T , a1:T )ds1:T da1:T = log E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) \ufffd p \ufffd O1 1:T , . . . , ON 1:T , s1:T , a1:T \ufffd q (s1:T , a1:T ) \ufffd \u2265 E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) \ufffd log p \ufffd O1 1:T , . . . , ON 1:T , s1:T , a1:T \ufffd \u2212 log q (s1:T , a1:T ) \ufffd = E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) T \ufffd t=1 \ufffd N \ufffd n=1 log p(On t |st, at) \u2212 log q (st|at) \ufffd (9) This is derived by using the same procedure as Equation 5. Thus, in the PGM framework for simultaneously observing multiple optimality emissions, the probability that two optimalities are observed is denoted as Equation 9. If the distribution of optimality follows Equation 2, i.e., p(On t |st, at) = exp(rn(st, at)), the variational lower bound of log-likelihood is given by log p \ufffd O1 1:T , . . . , ON 1:T \ufffd \u2265 E(s1:T ,a1:T )\u223cq(s1:T ,a1:T ) T \ufffd t=1 \ufffd N \ufffd n=1 rn (st, at) \u2212 log q (st|at) \ufffd . (10) Equation 10 shows that assuming multiple types of optimalities is the same as providing several types of sub-rewards in an additive manner. 4. GAIL using Task-achievement Reward (TRGAIL) In this section, we show that the integrated learning of RL and IL could be formulated as the maximum entropy reinforcement learning for optimality for the reward in RL, i.e., taskachievement reward, and optimality for IL, representing whether the action is similar to that of the demonstrator. The probability for the latter is calculated by the GAIL discriminator. We de\ufb01ne two types of optimality, optimality for RL OR t and optimality for IL OI t . We de\ufb01ne the distribution over OR t as p(OR t = 1|st, at) = exp(r(st, at)). (11) By using this formulation, as with general RL, the agent learns a policy to maximize the accumulated expected designed rewards r in the PGM framework (see section 2.1). This de\ufb01nition of optimality represents the target of RL, i.e., maximizing cumulative rewards. In contrast, OI t represents the goal of IL. The goal of IL is to simulate expert behavior, which is assumed to be optimal. We adopt GAIL discriminator D\u03c9(st, at), which indicates the probability if the state and action pair is generated from experts, i.e., the optimal controller, to calculate p(OI t = 1|st, at) ie. this can be interpreted as the optimality for imitation. p(OI t = 1|st, at) = exp(log(D\u03c9(st, at))) = D\u03c9(st, at). (12) ",
    "Experiment": "Experiment To evaluate our algorithm, we used three physics-based control tasks\u2013Pusher, Striker, and Thrower\u2013which are simulated using the MuJoCo physics simulator [44]. Then, using a num",
    "Results": "Results Figure 5 depicts the performance of the learned policies of each task in the MuJoCo physics simulator. As shown in the \ufb01gure, in most tasks, the proposed method can learn faster than the conventional method can, and the episode score of the learned policy is also higher. In the Striker task, GAIL cannot e\ufb03ciently learn the optimal policy because the expert trajectories trained by PPO are sub-optimal. In contrast, TRGAIL learned a better score than that of the sub-optimal trajectories because of the task-achievement reward. In the Thrower task, considering that 15 expert trajectories were given, classical IL could su\ufb03ciently learn, and there was no signi\ufb01cant ",
    "Conclusion": "Conclusion In this study, we developed the new probabilistic graphical model framework for simultaneously observing multiple optimality emissions. Furthermore, we demonstrated that the integrated learning method of RL and IL can be formulated as a probabilistic inference maximizing October 17, 2019 Advanced Robotics 0.main 0 1 2 3 4 5 1e7 0 10 20 30 40 50 60 70 Pusher 0 1 2 3 4 5 1e7 0 10 20 30 40 50 60 70 Striker 0 1 2 3 4 5 1e7 0 20 40 60 80 Thrower GAIL(1) GAIL(5) GAIL(10) TRGAIL(1) TRGAIL(5) TRGAIL(10) Expert Training Steps Episode Scores Figure 6. Performance of the learned policy in the case of changing the number of expert trajectories given to the agent in the Pusher Task, Striker Task, and Thrower Task. The x-axis represents the training time steps. One episode consists of 100 time steps. The y-axis represents the episode score, which is the number of time steps that achieved the goal in one episode. Task Pusher Striker Thrower num of traj 1 5 10 15 1 5 10 15 1 5 10 15 Expert 72.0 70.0 69.7 69.0 43.0 41.2 46.2 38.7 87.0 87.0 87.0 87.0 BC 0.0 1.4 8.4 34.0 0.0 2.5 7.6 2.3 0.1 26.0 45.2 63.5 GAIL 61.0 58.4 55.3 61.1 15.0 22.3 40.1 30.9 2.5 57.2 82.6 86.1 BC+PPO 49.2 74.6 55.2 69.3 31.2 56.5 56.1 56.7 67.4 84.5 84.9 85.3 TRGAIL 65.7 72.2 71.8 72.4 20.3 72.6 72.4 67.8 76.0 86.6 86.0 86.9 Table 1. Experimental result of IL and combination approach of IL and RL in the case of changing the number of experts given in each task. Episode scores indicate the number of time steps that achieved the goal in one episode. The score value is the average of 100 episodes performed using the highest rated policy during the trial. multiple optimalities to PGM with multiple optimality emission. In experiments, our proposed method, which adapts generative adversarial imitation learning and task-achievement rewards to our framework, achieved signi\ufb01cantly better performance than that achieved by the agents trained with RL or IL alone. However, we sometimes observed that the \ufb01nal performance of TRGAIL decreased as the number of experts given to the agent increased. This is considered to be caused by the fact that IL inhibits the improvement of the score when a non-optimal expert is given. Basically, RL has a low learning e\ufb03ciency at the beginning of learning but can e\ufb03ciently learn at a later stage where it can stably obtain a reward. On the other hand, IL contributes signi\ufb01cantly at the beginning of learning when no reward signal can be obtained, but at a later stage, it su\ufb00ers a penalty for the distance from the expert. From the above, we assume that it is preferable to learn while changing the weight parameter of RL and IL according to the progress of learning. Speci\ufb01cally, approaches may be considered, such as setting a parameter of weight and decreasing this value as learning progresses. Our future study will focus on the formulation of this phenomenon in PGM. Examining the relevance of other GAIL extension methods and our proposed method is also important. The GAIL approach for obtaining desirable strategies by adding human-designed rewards has been previously proposed [41]. Comparing these conventional approaches with our proposed method, the feature of the proposed method is to reformulate this approach as probabilistic inference on PGM. By showing the relationship between such a problem and the probabilistic inference, various methods, such as existing probabilistic inference methods, can be applied. ",
    "References": "References [1] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, Vol. 8, No. 3-4, pp. 293\u2013321, 1992. [2] Long Ji Lin. Programming robots using reinforcement learning and teaching. In Proceedings of the association for the advancement of arti\ufb01cial intelligence conference, pp. 781\u2013786, 1991. [3] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, Vol. 529, No. 7587, pp. 484\u2013489, 2016. [4] Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia Chernova, Matthew E Taylor, and Ann Now\u00b4e. Reinforcement learning from demonstration through shaping. In Twenty-Fourth International Joint Conference on Arti\ufb01cial Intelligence, 2015. [5] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pp. 1\u20139, 2013. [6] Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvunakool, J\u00b4anos Kram\u00b4ar, Raia Hadsell, Nando de Freitas, et al. Reinforcement and imitation learning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564, 2018. [7] Josh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201, 2017. [8] Keita Hamahata, Tadahiro Taniguchi, Kazutoshi Sakakibara, Ikuko Nishikawa, Kazuma Tabuchi, and Tetsuo Sawaragi. E\ufb00ective integration of imitation learning and reinforcement learning by generating internal reward. In 2008 Eighth International Conference on Intelligent Systems Design and Applications, Vol. 3, pp. 121\u2013126, 2008. [9] Nan Jiang Alekh Agarwal Miroslav Dudk Yisong Yue Le, Hoang Minh and Hal Daum. Hierarchical imitation and reinforcement learning. International Conference on Machine Learning, 2018. [10] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 6292\u20136299, 2018. [11] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In ThirtySecond AAAI Conference on Arti\ufb01cial Intelligence, 2018. [12] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 166\u2013175, 2017. [13] Wen Sun, Arun Venkatraman, Geo\ufb00rey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply aggrevated: Di\ufb00erentiable imitation learning for sequential prediction. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3309\u20133318, 2017. [14] Nicholas Rhinehart, Rowan McAllister, and Sergey Levine. Deep imitative models for \ufb02exible inferOctober 17, 2019 Advanced Robotics 0.main ence, planning, and control. arXiv preprint arXiv:1810.06544, 2018. [15] Hagai Attias. Planning by probabilistic inference. In Proceedings of the 9th International Workshop on Arti\ufb01cial Intelligence and Statistics., 2003. [16] Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information processing systems, pp. 1369\u20131376, 2007. [17] Hilbert J Kappen, Vicen\u00b8c G\u00b4omez, and Manfred Opper. Optimal control as a graphical model inference problem. Machine learning, Vol. 87, No. 2, pp. 159\u2013182, 2012. [18] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th annual international conference on machine learning, pp. 1049\u20131056, 2009. [19] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by approximate inference. In Twenty-Third International Joint Conference on Arti\ufb01cial Intelligence, 2013. [20] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum causal entropy. 2010. [21] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016. [22] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. [23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In Deep Learning, Neural Information Processing Systems Workshop, 2013. [24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, Vol. 518, No. 7540, pp. 529\u2013533, 2015. [25] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. [26] Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward de\ufb01nition. In Advances in Neural Information Processing Systems, pp. 8538\u20138547, 2018. [27] Michael Bain and Claude Sommut. A framework for behavioural claning. Machine intelligence, Vol. 15, No. 15, p. 103, 1999. [28] Dean A Pomerleau. E\ufb03cient training of arti\ufb01cial neural networks for autonomous navigation. Neural Computation, Vol. 3, No. 1, pp. 88\u201397, 1991. [29] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. [30] Jun Nakanishi, Jun Morimoto, Gen Endo, Gordon Cheng, Stefan Schaal, and Mitsuo Kawato. Learning from demonstration and adaptation of biped locomotion. Robotics and autonomous systems, Vol. 47, No. 2-3, pp. 79\u201391, 2004. [31] Mrinal Kalakrishnan, Jonas Buchli, Peter Pastor, and Stefan Schaal. Learning locomotion over rough terrain using terrain templates. In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, pp. 167\u2013172, 2009. [32] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the association for the advancement of arti\ufb01cial intelligence conference, Vol. 8, pp. 1433\u20131438, 2008. [33] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learning. arXiv preprint arXiv:1507.04888, 2015. [34] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pp. 49\u201358, 2016. [35] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. InInternational Conference on Learning Representations (ICLR), 2018. [36] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014. [37] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1889\u20131897, 2015. October 17, 2019 Advanced Robotics 0.main [38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [39] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [40] Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end di\ufb00erentiable adversarial imitation learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 390\u2013399, 2017. [41] Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems, pp. 3815\u20133825, 2017. [42] Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph J Lim. Multimodal imitation learning from unstructured demonstrations using generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 1235\u20131245, 2017. [43] Jiahao Lin and Zongzhang Zhang. Acgail: Imitation learning about multiple intentions with auxiliary classi\ufb01er gans. In Paci\ufb01c Rim International Conference on Arti\ufb01cial Intelligence, pp. 321\u2013334, 2018. [44] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026\u2013 5033, 2012. [45] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. ",
    "title": "FULL PAPER Integration of Imitation Learning using GAIL and Reinforcement Learning",
    "paper_info": "October 17, 2019\nAdvanced Robotics\n0.main\nTo appear in Advanced Robotics\nVol. 00, No. 00, January 2013, 1\u201315\nFULL PAPER\nIntegration of Imitation Learning using GAIL and Reinforcement Learning\nusing Task-achievement Rewards via Probabilistic Graphical Model\nAkira Kinose and Tadahiro Taniguchi\n(Received 00 Month 201X; accepted 00 Month 201X)\nThe integration of reinforcement learning (RL) and imitation learning (IL) is an important problem\nthat has been studied for a long time in the \ufb01eld of intelligent robotics. RL optimizes policies to\nmaximize the cumulative reward, whereas IL attempts to extract general knowledge about the tra-\njectories demonstrated by experts, i.e., demonstrators. Because each of them has its own drawbacks,\nmethods combining them and compensating for each set of drawbacks have been explored thus far.\nHowever, many of the methods are heuristic and do not have a solid theoretical basis. In this paper,\nwe present a new theory for integrating RL and IL by extending the probabilistic graphical model\n(PGM) framework for RL, control as inference. We develop a new PGM for RL with multiple types of\nrewards called probabilistic graphical model for Markov decision processes with multiple optimality\nemissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning method of RL and\nIL can be formulated as a probabilistic inference of policies on pMDP-MO by considering the dis-\ncriminator in generative adversarial imitation learning (GAIL) as an additional optimality emission.\nWe adapt the GAIL and task-achievement reward to our proposed framework, achieving signi\ufb01cantly\nbetter performance than policies trained with baseline methods.\nKeywords: Imitation learning; Reinforcement learning; Probabilistic inference; Control as inference;\nGenerative adversarial imitation learning\n1.\nIntroduction\nThe integration of reinforcement learning (RL) and imitation learning (IL) is an important\nproblem that has been studied for a long time in the \ufb01eld of intelligent robotics and machine\nlearning [1\u201314]. RL and IL are both well-studied machine learning problems. The goal is to pro-\nvide robots with capabilities to optimize policies automatically. RL is conventionally formalized\nas a reward maximization problem. An RL agent explores the optimal policy by maximizing the\ncumulative rewards. However, it requires many trials in most of the robotic control problems.\nBecause they have high dimensional states-and-actions spaces, exploration and optimization be-\ncomes a di\ufb03cult problem. In contrast, IL can acquire behavior by mimicking the behaviors of\nexperts. However, its performance depends on the expert, and there is no guarantee of acquiring\nan optimal policy. As a result, RL and IL have their own drawbacks.\nThe research for integrating RL and IL has been conducted to overcome problems and make use\nof the learner\u2019s trial-and-error experience and expert\u2019s demonstrations. However, most variations\nare designed heuristically, i.e., few have a sophisticated theoretical basis [8]. This tends to lead\nto the heuristic parameter tuning and endless exploration of the variants of frameworks for\nintegration.\nThe main goal of this paper is to present the integration of reinforcement and imitation\nlearning methods into a single theoretical framework, i.e., probabilistic graphical model and its\ninference. Such integration allows the learning agents to use expert demonstrations and agent\ntrial and error in a synergistic manner, enabling them to perform complicated tasks, such as\nrobotic control, more swiftly. For this purpose, we developed the probabilistic graphical model\narXiv:1907.02140v2  [cs.LG]  16 Oct 2019\n",
    "GPTsummary": "\n\n\n\n8. Conclusion:\n\n- (1): This piece of work is significant for proposing a new theoretical basis for integrating reinforcement learning and imitation learning using a probabilistic graphical model (PGM) framework and probabilistic inference. The proposed PGM model, called the probabilistic graphical model for Markov decision processes with multiple optimality emissions (pMDP-MO), successfully integrates expert demonstrations and agent trial and error to enable robots to perform complicated tasks more swiftly.\n\n- (2): Innovation point: The work proposed a new theoretical basis for integrating reinforcement learning and imitation learning using a probabilistic graphical model framework and probabilistic inference. Performance: The proposed framework achieved significantly better performance than agents trained with reinforcement learning or imitation learning alone. Workload: The article is quite technically dense and requires a deep understanding of the field.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work is significant for proposing a new theoretical basis for integrating reinforcement learning and imitation learning using a probabilistic graphical model (PGM) framework and probabilistic inference. The proposed PGM model, called the probabilistic graphical model for Markov decision processes with multiple optimality emissions (pMDP-MO), successfully integrates expert demonstrations and agent trial and error to enable robots to perform complicated tasks more swiftly.\n\n- (2): Innovation point: The work proposed a new theoretical basis for integrating reinforcement learning and imitation learning using a probabilistic graphical model framework and probabilistic inference. Performance: The proposed framework achieved significantly better performance than agents trained with reinforcement learning or imitation learning alone. Workload: The article is quite technically dense and requires a deep understanding of the field.\n\n\n"
}