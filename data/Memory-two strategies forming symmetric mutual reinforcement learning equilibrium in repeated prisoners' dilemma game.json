{
    "Abstract": "Abstract We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners\u2019 dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-n strategies with n > 2. Keywords: Repeated prisoners\u2019 dilemma game; Reinforcement learning; Memory-two strategies 1. ",
    "Introduction": "Introduction The prisoners\u2019 dilemma is one of the simplest situations in which rational actions of individuals do not maximize social welfare [1]. Although the best action of each agent is defection, mutual cooperation improves the utility of both agents. On the other hand, if the prisoners\u2019 dilemma game is in\ufb01nitely repeated, the situation changes. In fact, mutual cooperation can be realized by rational behavior of each agent, and this result is known as folk theorem [2]. The folk theorem was also extended to a stronger version that any individually rational payo\ufb00s can be realized as subgame perfect equilibria [3]. At the same time, it has been pointed out by experiments that the realistic agents like human beings are not necessarily rational, and theories of bounded rationality have been needed [4]. One of the mainstream is modeling agents by \ufb01nite automata (agents with \ufb01nite complexity) [5, 6, 7, 8, 9, 10, 11]. Especially, Abreu and Rubinstein found that the equilibrium payo\ufb00s realized by Email address: m.ueda@yamaguchi-u.ac.jp (Masahiko Ueda) Preprint submitted to Elsevier December 29, 2022 \ufb01nite automaton selection games, where players choose \ufb01nite automata as their strategies in repeated games so as to maximize their payo\ufb00s and to minimize the number of states of the \ufb01nite automata lexicographically, are restricted to some small region in individually rational payo\ufb00s [8]. Kalai and Stanford proved that every subgame perfect equilibrium of repeated games can be approximated by a subgame perfect \u01eb-equilibrium of \ufb01nite complexity [7]. A slightly di\ufb00erent approach from \ufb01nite automata is modeling agents by ones with \ufb01nite memory, which recall only a \ufb01nite number past periods [12]. (Although there is distinction between memory and recall in computer science, we use these two words interchangeably.) Deterministic \ufb01nite-memory strategies are contained in a class of \ufb01nite automata. Sabourian and co-workers investigated how the folk theorem can be extended to \ufb01nite-memory strategies [13, 14, 15]. Another trend of studies of bounded rationality is modeling agents as adaptive ones which gradually acquire favorable strategies. One of the most successful approach is evolutionary game theory, where a population of individuals evolves by natural selection [16]. The concept of evolutionarily stable strategy, which is interpreted as stability against mutation, succeeded in strengthening the concept of Nash equilibrium. However, it was also shown that any strategy in the in\ufb01nitely repeated prisoners\u2019 dilemma game is not an evolutionarily stable strategy, and is not stable against neutral drift [17]. There are also studies of evolutionarily stable strategies with \ufb01nite complexity [18, 19, 20]. Particularly, Binmore and Samuelson proposed a modi\ufb01ed version of evolutionarily stable strategy and showed that such strategies must maximize the sum of payo\ufb00s of two players [19]. Furthermore, many evolutionary simulations on \ufb01nite-memory strategies have been done for various population sizes, mutation rates, and types of interaction [21, 22, 23, 24, 25, 26]. Stewart and Plotkin proposed the concept to evolutionary robust strategies, which is an extension of evolutionarily stable strategies to systems of \ufb01nite population size and cannot be selectively replaced by any mutant strategies [27]. Learning is another way of adaptation of human beings, and has also attracted much attention in theoretical economics [28, 29, 30], computer science [31], and complex systems theory [32, 33, 34, 35, 36, 37]. Many methods of learning have been proposed in game theory [38], and compared with experimental results [39, 40, 41]. One of the most popular learning methods is reinforcement learning [42]. In reinforcement learning, an agent gradually learns the optimal policy against a stationary environment. Mutual reinforcement learning in game theory is a more di\ufb03cult problem since the existence of multiple agents makes an environment nonstationary [43, 44, 45, 46, 47]. Several methods have been proposed for reinforcement learning with multiple agents [48]. Recently, memory-n strategies (n periods memory strategies) with n > 1 attract much attention in computational evolutionary game theory, because longer memory enables more complicated behavior [49, 50, 51, 52, 53, 54]. Especially, longer memory enables us to design robust strategies against implementation errors. Since agents in evolutionary biology are organisms, which are far from rational, it has been traditionally assumed that the length of memory of such agents is assumed to be short. This is in contrast to chronology of game theory 2 in economics, where behaviors of rational and forward-looking agents were \ufb01rst studied and then memory length becomes shorter in order to describe agents with bounded rationality. Because rationality of realistic agents is bounded, shorter-memory strategies will be preferred if complexity is also considered. Here, we investigate mutual reinforcement learning in the repeated prisoners\u2019 dilemma game [1]. More explicitly, we investigate properties of equilibria formed by learning agents when the two agents alternately learn their optimal strategies against the opponent. In the previous study [55], it was found that, among all deterministic memory-one strategies, only the Grim trigger strategy, the WinStay Lose-Shift strategy, and the All-D strategy can form symmetric equilibrium of mutual reinforcement learning. A natural question is \u201cHow does the set of such equilibria grow as the length of memory increases?\u201d. Such direction of research can be useful when we construct strong strategies based on memoryone strategies, as in computational evolutionary game theory. Furthermore, we want to understand mutual reinforcement learning equilibria in terms of strategies, not equilibrium payo\ufb00s. However, even whether the above equilibria formed by memory-one strategies are still equilibria in memory-n settings or not has not been known. In this paper, we extend the analysis of Ref. [55] to memory-two strategies. First, we provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. Then we provide three non-trivial examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. Furthermore, we also prove that mutual reinforcement learning equilibria formed by memory-n\u2032 strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-n strategies with n > n\u2032. This paper is organized as follows. In Section 2, we introduce the repeated prisoners\u2019 dilemma game with memory-n strategies, and players using reinforcement learning. In Section 3, we show that the structure of the optimal strategies is constrained by the Bellman optimality equation. In Section 4, we introduce the concepts of mutual reinforcement learning equilibrium and symmetric equilibrium. We then provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. In Section 5, we provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. In Section 6, we show that mutual reinforcement learning equilibria formed by memory-n\u2032 strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-n strategies with n > n\u2032. Section 7 is devoted to conclusion. 2. Model We introduce the repeated prisoners\u2019 dilemma game [43]. There are two players (1 and 2) in the game. Each player chooses cooperation (C) or defection (D) on every round. The action of player a is written as \u03c3a \u2208 {C, D}. We collectively write \u03c3 := (\u03c31, \u03c32), and call \u03c3 an action pro\ufb01le. We also write the space of all possible action pro\ufb01les as \u2126 := {C, D}2. The payo\ufb00 of player 3 a \u2208 {1, 2} when the action pro\ufb01le is \u03c3 is described as ra (\u03c3). The payo\ufb00s in the prisoners\u2019 dilemma game are given by (r1 (C, C) , r1 (C, D) , r1 (D, C) , r1 (D, D)) = (R, S, T, P) (1) (r2 (C, C) , r2 (C, D) , r2 (D, C) , r2 (D, D)) = (R, T, S, P) (2) with T > R > P > S and 2R > T + S. The (time-independent) memoryn strategy (n \u2265 1) of player a is described as the conditional probability Ta \ufffd \u03c3a| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd of taking action \u03c3a when the action pro\ufb01les in the previous n rounds are \ufffd \u03c3(\u2212m)\ufffdn m=1, together with an initial condition, where we have introduced the notation \ufffd \u03c3(\u2212m)\ufffdn m=1 := \ufffd \u03c3(\u22121), \u00b7 \u00b7 \u00b7 , \u03c3(\u2212n)\ufffd from newest to oldest [54]. (As a strategy of bounded rational players, we use \ufb01nite-memory strategies, not \ufb01nite automata, because the former allows strategies to be stochastic. Although stochastic strategies are allowed in our framework, we investigate only deterministic strategies in this paper.) We write the length of memory of player a as na and de\ufb01ne n := max {n1, n2}. In this paper, we assume that n is \ufb01nite. Assumption 1. Both players use time-independent \ufb01nite-memory strategies. Below we introduce the notation \u2212a := {1, 2}\\a. We consider the situation that both players learn their optimal strategies against the strategy of the opponent by reinforcement learning [42]. In reinforcement learning, each player learns mapping (called policy) from the action pro\ufb01les \ufffd \u03c3(\u2212m)\ufffdn m=1 in the previous n rounds to his/her action \u03c3 so as to maximize his/her expected future reward. We write the action of player a at round t as \u03c3a(t). In addition, we write ra(t) := ra (\u03c3(t)). We de\ufb01ne the action-value function of player a as Qa \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd := E \ufffd \u221e \ufffd k=0 \u03b3kra(t + k) \ufffd\ufffd\ufffd\ufffd\ufffd \u03c3a(t) = \u03c3a, [\u03c3(s)]t\u2212n s=t\u22121 = \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd , (3) where \u03b3 is a discounting factor satisfying 0 \u2264 \u03b3 < 1. The action-value function Qa \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd represents the expected future payo\ufb00s \ufffd\u221e k=0 \u03b3kra(t+k) of player a after round t by taking action \u03c3a when action pro\ufb01les in the previous n rounds are \ufffd \u03c3(\u2212m)\ufffdn m=1. Therefore, the action-value function suggests the best action in each action pro\ufb01le. It should be noted that the right-hand side does not depend on t. Due to the property of memory-n strategies, the action-value function Qa obeys the Bellman equation against a \ufb01xed strategy T\u2212a of the 4 opponent: Qa \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd +\u03b3 \ufffd \u03c3\u2032a \ufffd \u03c3\u2212a Ta \ufffd \u03c3\u2032 a| \u03c3, \ufffd \u03c3(\u2212m)\ufffdn\u22121 m=1 \ufffd T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd Qa \ufffd \u03c3\u2032 a, \u03c3, \ufffd \u03c3(\u2212m)\ufffdn\u22121 m=1 \ufffd . (4) See Appendix A for the derivation of Eq. (4). It has been known that the optimal policy T \u2217 a and the optimal action-value function Q\u2217 a obeys the following Bellman optimality equation: Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd +\u03b3 \ufffd \u03c3\u2212a T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd max \u02c6\u03c3 Q\u2217 a \ufffd \u02c6\u03c3, \u03c3, \ufffd \u03c3(\u2212m)\ufffdn\u22121 m=1 \ufffd , (5) with the support suppT \u2217 a \ufffd \u00b7| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = arg max \u03c3 Q\u2217 a \ufffd \u03c3, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd . (6) See Appendix B for the derivation of Eqs. (5) and (6). In other words, in the optimal policy against T\u2212a, player a takes the action \u03c3a which maximizes the value of Q\u2217 a \ufffd \u00b7, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd when the action pro\ufb01les at the previous n rounds are \ufffd \u03c3(\u2212m)\ufffdn m=1. In Q-learning, which is one of the simplest algorithms of reinforcement learning, it is known that values of action-value functions converge to the solutions of the Bellman optimality equation if all state-action pairs are visited an in\ufb01nite number of times [42]. We investigate the situation that players in\ufb01nitely repeat the in\ufb01nitelyrepeated games and players alternately learn their optimal strategies in each game, as in Ref. [55]. We write the optimal strategy and the corresponding optimal action-value function of player a at d-th game as T \u2217(d) a and Q\u2217(d) a , respectively. Given an initial strategy T \u2217(0) 2 of player 2, in the (2l \u2212 1)-th game (l \u2208 N), player 1 learns T \u2217(2l\u22121) 1 against T \u2217(2l\u22122) 2 by calculating Q\u2217(2l\u22121) 1 . In the 2l-th game, player 2 learns T \u2217(2l) 2 against T \u2217(2l\u22121) 1 by calculating Q\u2217(2l) 2 . We are interested in the \ufb01xed points of the dynamics, that is, T \u2217(\u221e) a and Q\u2217(\u221e) a . In this paper, we mainly investigate situations that the support (6) contains only one action, that is, strategies are deterministic. The number of deterministic memory-n strategies in the repeated prisoners\u2019 dilemma game is 222n, which increases rapidly as n increases. 5 3. Structure of optimal strategies Below we consider only the case n = 2. The Bellman optimality equation (5) for n = 2 is Q\u2217 a \ufffd \u03c3a, \u03c3(\u22121), \u03c3(\u22122)\ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \u03c3(\u22121), \u03c3(\u22122)\ufffd +\u03b3 \ufffd \u03c3\u2212a T\u2212a \ufffd \u03c3\u2212a| \u03c3(\u22121), \u03c3(\u22122)\ufffd max \u02c6\u03c3 Q\u2217 a \ufffd \u02c6\u03c3, \u03c3, \u03c3(\u22121)\ufffd (7) with suppT \u2217 a \ufffd \u00b7| \u03c3(\u22121), \u03c3(\u22122)\ufffd = arg max \u03c3 Q\u2217 a \ufffd \u03c3, \u03c3(\u22121), \u03c3(\u22122)\ufffd . (8) The number of memory-two deterministic strategies is 216, which is quite large, and therefore we cannot investigate all memory-two deterministic strategies as in the case of memory-one deterministic strategies [55]. Instead, we \ufb01rst investigate general properties of optimal strategies. We introduce the matrix representation of a strategy: Ta (\u03c3) := \uf8eb \uf8ec \uf8ec \uf8ed Ta (\u03c3| (C, C), (C, C)) Ta (\u03c3| (C, C), (C, D)) Ta (\u03c3| (C, C), (D, C)) Ta (\u03c3| (C, C), (D, D)) Ta (\u03c3| (C, D), (C, C)) Ta (\u03c3| (C, D), (C, D)) Ta (\u03c3| (C, D), (D, C)) Ta (\u03c3| (C, D), (D, D)) Ta (\u03c3| (D, C), (C, C)) Ta (\u03c3| (D, C), (C, D)) Ta (\u03c3| (D, C), (D, C)) Ta (\u03c3| (D, C), (D, D)) Ta (\u03c3| (D, D), (C, C)) Ta (\u03c3| (D, D), (C, D)) Ta (\u03c3| (D, D), (D, C)) Ta (\u03c3| (D, D), (D, D)) \uf8f6 \uf8f7 \uf8f7 \uf8f8 . (9) For deterministic strategies, each component in the matrix is 0 or 1. We now prove the following proposition: Proposition 1. For two di\ufb00erent action pro\ufb01les \u03c3(\u22122) and \u03c3(\u22122)\u2032, if T\u2212a \ufffd \u03c3| \u03c3(\u22121), \u03c3(\u22122)\ufffd = T\u2212a \ufffd \u03c3| \u03c3(\u22121), \u03c3(\u22122)\u2032\ufffd (\u2200\u03c3) (10) holds for some \u03c3(\u22121), then T \u2217 a \ufffd \u03c3| \u03c3(\u22121), \u03c3(\u22122)\ufffd = T \u2217 a \ufffd \u03c3| \u03c3(\u22121), \u03c3(\u22122)\u2032\ufffd (\u2200\u03c3) (11) also holds. 6 Proof. For such \u03c3(\u22121), because of Eq. (7), we \ufb01nd Q\u2217 a \ufffd \u03c3a, \u03c3(\u22121), \u03c3(\u22122)\ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \u03c3(\u22121), \u03c3(\u22122)\ufffd +\u03b3 \ufffd \u03c3\u2212a T\u2212a \ufffd \u03c3\u2212a| \u03c3(\u22121), \u03c3(\u22122)\ufffd max \u02c6\u03c3 Q\u2217 a \ufffd \u02c6\u03c3, \u03c3, \u03c3(\u22121)\ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \u03c3(\u22121), \u03c3(\u22122)\u2032\ufffd +\u03b3 \ufffd \u03c3\u2212a T\u2212a \ufffd \u03c3\u2212a| \u03c3(\u22121), \u03c3(\u22122)\u2032\ufffd max \u02c6\u03c3 Q\u2217 a \ufffd \u02c6\u03c3, \u03c3, \u03c3(\u22121)\ufffd = Q\u2217 a \ufffd \u03c3a, \u03c3(\u22121), \u03c3(\u22122)\u2032\ufffd (12) for all \u03c3a. Since T \u2217 a is determined by Eq. (8), we obtain Eq. (11). This proposition implies that the structure of the matrix T \u2217 a (\u03c3) is the same as that of T\u2212a(\u03c3). For deterministic strategies, in order to see this in more detail, we introduce the following sets for a \u2208 {1, 2} and \u03c3(\u22121) \u2208 \u2126: N (a) x \ufffd \u03c3(\u22121)\ufffd := \ufffd \u03c3(\u22122) \u2208 \u2126 \ufffd\ufffd\ufffd Ta \ufffd C| \u03c3(\u22121), \u03c3(\u22122)\ufffd = x \ufffd , (13) where x \u2208 {0, 1}. That is, N (a) 1 \ufffd \u03c3(\u22121)\ufffd describes the set of \u03c3(\u22122) such that player a using strategy Ta cooperates after the history \ufffd \u03c3(\u2212m)\ufffd2 m=1. Similarly, N (a) 0 \ufffd \u03c3(\u22121)\ufffd describes the set of \u03c3(\u22122) such that player a using strategy Ta defects after the history \ufffd \u03c3(\u2212m)\ufffd2 m=1. We remark that N (a) 0 \ufffd \u03c3(\u22121)\ufffd \u222a N (a) 1 \ufffd \u03c3(\u22121)\ufffd = \u2126 for all a and \u03c3(\u22121). Then, Proposition 1 leads the following corollary: Corollary 1. For a deterministic strategy T\u2212a of player \u2212a, if the optimal strategy T \u2217 a of player a against T\u2212a is also deterministic, then one of the following four relations holds for each \u03c3(\u22121) \u2208 \u2126: (a) N (a) x \ufffd \u03c3(\u22121)\ufffd = N (\u2212a) x \ufffd \u03c3(\u22121)\ufffd for all x (b) N (a) x \ufffd \u03c3(\u22121)\ufffd = N (\u2212a) 1\u2212x \ufffd \u03c3(\u22121)\ufffd for all x (c) N (a) 0 \ufffd \u03c3(\u22121)\ufffd = N (\u2212a) 0 \ufffd \u03c3(\u22121)\ufffd \u222a N (\u2212a) 1 \ufffd \u03c3(\u22121)\ufffd = \u2126 and N (a) 1 \ufffd \u03c3(\u22121)\ufffd = \u2205 (d) N (a) 1 \ufffd \u03c3(\u22121)\ufffd = N (\u2212a) 0 \ufffd \u03c3(\u22121)\ufffd \u222a N (\u2212a) 1 \ufffd \u03c3(\u22121)\ufffd = \u2126 and N (a) 0 \ufffd \u03c3(\u22121)\ufffd = \u2205. 4. Symmetric equilibrium In this section, we investigate symmetric equilibrium of mutual reinforcement learning. First, we introduce the notation C := D, D := C, and \u03c0 (\u03c31, \u03c32) := (\u03c32, \u03c31). We de\ufb01ne the word same strategy. 7 De\ufb01nition 1. A strategy Ta of player a is the same strategy as that of player \u2212a i\ufb00 Ta \ufffd \u03c3| \u03c3(\u22121), \u03c3(\u22122)\ufffd = T\u2212a \ufffd \u03c3| \u03c0 \ufffd \u03c3(\u22121)\ufffd , \u03c0 \ufffd \u03c3(\u22122)\ufffd\ufffd (14) for all \u03c3, \u03c3(\u22121) and, \u03c3(\u22122). Next, we introduce equilibria achieved by mutual reinforcement learning. De\ufb01nition 2. A pair of strategy T1 and T2 is a mutual reinforcement learning equilibrium i\ufb00 Ta is the optimal strategy against T\u2212a for a = 1, 2. We emphasize that such equilibria are de\ufb01ned only for a time-independent part of \ufb01nite-memory strategies Ta, although \ufb01nite-memory strategies of players are generally de\ufb01ned as a pair of a time-independent part Ta and an initial condition. This de\ufb01nition is in contrast to that of Nash equilibrium or subgame perfect equilibrium. When some appropriate initial condition is chosen, it becomes a subgame perfect equilibrium of all time-independent \ufb01nite-memory strategies. In addition, because the optimal policy is determined by comparing the action-value functions, which are functions of \ufb01nite-length histories including o\ufb00-equilibrium path, mutual reinforcement learning equilibrium is quite di\ufb00erent from Nash equilibrium. We also remark that a mutual reinforcement learning equilibrium can be achieved by Q-learning if all state-action pairs are visited an in\ufb01nite number of times as mentioned above, and if an initial strategy of player 2 is appropriate. Even if not all state-action pairs are visited an in\ufb01nite number of times, we can obtain the mutual reinforcement learning equilibrium by introducing in\ufb01nitesimal error probability to the opponent\u2019s strategy as in Ref. [55]. For deterministic mutual reinforcement learning equilibria, the following proposition is the direct consequence of Corollary 1. Proposition 2. For mutual reinforcement learning equilibria formed by deterministic strategies, one of the following two relations holds for each \u03c3(\u22121) \u2208 \u2126: (a) N (1) x \ufffd \u03c3(\u22121)\ufffd = N (2) x \ufffd \u03c3(\u22121)\ufffd for all x (b) N (1) x \ufffd \u03c3(\u22121)\ufffd = N (2) 1\u2212x \ufffd \u03c3(\u22121)\ufffd for all x. Proof. According to Corollary 1, one of the four situations (a)-(d) holds for the optimal strategy T1 against T2. However, because T2 is also the optimal strategy against T1, the cases (c) and (d) are excluded or integrated into the case (a) or (b). Furthermore, we introduce symmetric equilibria of mutual reinforcement learning. De\ufb01nition 3. A pair of strategy T1 and T2 is a symmetric mutual reinforcement learning equilibrium i\ufb00 Ta is the optimal strategy against T\u2212a and Ta is the same strategy as T\u2212a for a = 1, 2. 8 It should be noted that the deterministic optimal strategies can be written as T \u2217 a \ufffd \u03c3| \u03c3(\u22121), \u03c3(\u22122)\ufffd = I \ufffd Q\u2217 a \ufffd \u03c3, \u03c3(\u22121), \u03c3(\u22122)\ufffd > Q\u2217 a \ufffd \u03c3, \u03c3(\u22121), \u03c3(\u22122)\ufffd\ufffd , (15) where I(\u00b7 \u00b7 \u00b7 ) is the indicator function that returns 1 when \u00b7 \u00b7 \u00b7 holds and 0 otherwise. We also introduce the following sets for a \u2208 {1, 2} and \u03c3(\u22121) \u2208 \u2126: \u02dcN (a) x \ufffd \u03c3(\u22121)\ufffd := \ufffd \u03c3(\u22122) \u2208 \u2126 \ufffd\ufffd\ufffd Ta \ufffd C| \u03c3(\u22121), \u03c0 \ufffd \u03c3(\u22122)\ufffd\ufffd = x \ufffd , (16) where x \u2208 {0, 1}. We now prove the \ufb01rst main result of this paper. Theorem 1. For symmetric mutual reinforcement learning equilibria formed by deterministic strategies, the following relations must hold: (a) For \u03c3(\u22121) \u2208 {(C, C), (D, D)}, Ta \ufffd C| \u03c3(\u22121), (C, D) \ufffd = Ta \ufffd C| \u03c3(\u22121), (D, C) \ufffd (17) for all a. (b) For \u03c3(\u22121) \u2208 {(C, D), (D, C)}, N (a) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd = \u02dcN (a) x \ufffd \u03c3(\u22121)\ufffd (\u2200x) (18) or N (a) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd = \u02dcN (a) 1\u2212x \ufffd \u03c3(\u22121)\ufffd (\u2200x) (19) holds. Proof. For \u03c3(\u22121) \u2208 {(C, C), (D, D)}, \u03c0 \ufffd \u03c3(\u22121)\ufffd = \u03c3(\u22121) holds. Because T1 and T2 are the same strategies as each other, T1 \ufffd C| \u03c3(\u22121), \u03c3(\u22122)\ufffd = T2 \ufffd C| \u03c3(\u22121), \u03c3(\u22122)\ufffd \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (20) holds. This and Proposition 2 imply that N (1) x \ufffd \u03c3(\u22121)\ufffd = N (2) x \ufffd \u03c3(\u22121)\ufffd (\u2200x \u2208 {0, 1}) must holds. On the other hand, due to Eq. (14), T1 \ufffd C| \u03c3(\u22121), (C, D) \ufffd = T2 \ufffd C| \u03c3(\u22121), (D, C) \ufffd (21) T1 \ufffd C| \u03c3(\u22121), (D, C) \ufffd = T2 \ufffd C| \u03c3(\u22121), (C, D) \ufffd (22) also hold. This means that, if (C, D) \u2208 N (1) x \ufffd \u03c3(\u22121)\ufffd , then (D, C) \u2208 N (2) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd = N (2) x \ufffd \u03c3(\u22121)\ufffd = N (1) x \ufffd \u03c3(\u22121)\ufffd , leading to Eq. (17). 9 For \u03c3(\u22121) \u2208 {(C, D), (D, C)}, because T1 and T2 are the same strategies as each other, T2 \ufffd C| \u03c0 \ufffd \u03c3(\u22121)\ufffd , \u03c3(\u22122)\ufffd = T1 \ufffd C| \u03c3(\u22121), \u03c0 \ufffd \u03c3(\u22122)\ufffd\ufffd (23) holds for \u2200\u03c3(\u22122) \u2208 \u2126. This means that N (2) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd = \u02dcN (1) x \ufffd \u03c3(\u22121)\ufffd (\u2200x \u2208 {0, 1}) (24) holds. On the other hand, Proposition 2 implies that N (1) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd = N (2) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd (\u2200x \u2208 {0, 1}) (25) or N (1) x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd = N (2) 1\u2212x \ufffd \u03c0 \ufffd \u03c3(\u22121)\ufffd\ufffd (\u2200x \u2208 {0, 1}) (26) must hold. By combining Eq. (24) and Eq. (25) or (26), we obtain Eq. (18) or (19). Theorem 1 provides a necessary condition for a deterministic strategy to form a symmetric mutual reinforcement learning equilibrium. In particular, Eqs. (18) and (19) imply that the second row and the third row of Ta cannot be independent of each other. Explicitly, Ta must be one of the following 8 forms: \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 c1 c1 c1 d1 d1 d1 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 c1 c1 1 \u2212 c1 d1 d1 d1 1 \u2212 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 c1 1 \u2212 c1 c1 d1 1 \u2212 d1 d1 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 c1 1 \u2212 c1 1 \u2212 c1 d1 1 \u2212 d1 d1 1 \u2212 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 1 \u2212 c1 c1 c1 d1 d1 1 \u2212 d1 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 1 \u2212 c1 c1 1 \u2212 c1 d1 d1 1 \u2212 d1 1 \u2212 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 1 \u2212 c1 1 \u2212 c1 c1 d1 1 \u2212 d1 1 \u2212 d1 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , \uf8eb \uf8ec \uf8ec \uf8ed a1 b1 b1 a2 c1 1 \u2212 c1 1 \u2212 c1 1 \u2212 c1 d1 1 \u2212 d1 1 \u2212 d1 1 \u2212 d1 a3 b2 b2 a4 \uf8f6 \uf8f7 \uf8f7 \uf8f8 ,(27) where ai, bj, c1, d1 \u2208 {0, 1} (i = 1, 2, 3, 4), (j = 1, 2) independently. For example, the Tit-for-Tat-anti-Tit-for-Tat (TFT-ATFT) strategy [50] T1 (C) = \uf8eb \uf8ec \uf8ec \uf8ed 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 \uf8f6 \uf8f7 \uf8f7 \uf8f8 , (28) 10 does not satisfy the condition of Theorem 1, and therefore it cannot form a symmetric mutual reinforcement learning equilibrium. However, there are still many memory-two strategies which satisfy the necessary condition, and further re\ufb01nement will be needed. 5. Examples of deterministic strategies forming symmetric equilibrium In this section, we provide three examples of memory-two deterministic strategies forming symmetric mutual reinforcement learning equilibrium. For convenience, we de\ufb01ne the following sixteen quantities: q1 := R + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, C), (C, C)) (29) q2 := T + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, C), (C, C)) (30) q3 := S + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, D), (C, C)) (31) q4 := P + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, D), (C, C)) (32) q5 := R + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, C), (C, D)) (33) q6 := T + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, C), (C, D)) (34) q7 := S + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, D), (C, D)) (35) q8 := P + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, D), (C, D)) (36) q9 := R + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, C), (D, C)) (37) q10 := T + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, C), (D, C)) (38) q11 := S + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, D), (D, C)) (39) q12 := P + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, D), (D, C)) (40) q13 := R + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, C), (D, D)) (41) q14 := T + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, C), (D, D)) (42) q15 := S + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (C, D), (D, D)) (43) q16 := P + \u03b3 max \u03c3 Q\u2217 1 (\u03c3, (D, D), (D, D)) (44) 11 The Bellman optimality equation for symmetric equilibrium is Q\u2217 1 \ufffd \u03c31, \u03c3(\u22121), \u03c3(\u22122)\ufffd = \ufffd \u03c32 \ufffd r1 (\u03c3) + max \u02c6\u03c3 Q\u2217 1 \ufffd \u02c6\u03c3, \u03c3, \u03c3(\u22121)\ufffd\ufffd \u00d7I \ufffd Q\u2217 1 \ufffd \u03c32, \u03c0 \ufffd \u03c3(\u22121)\ufffd , \u03c0 \ufffd \u03c3(\u22122)\ufffd\ufffd > Q\u2217 1 \ufffd \u03c32, \u03c0 \ufffd \u03c3(\u22121)\ufffd , \u03c0 \ufffd \u03c3(\u22122)\ufffd\ufffd\ufffd . (45) We want to \ufb01nd solutions of this equation. 5.1. Delayed Grim trigger strategy The \ufb01rst candidate of the solution of Eq. (45) is T1 (C) = T2 (C) = \uf8eb \uf8ec \uf8ec \uf8ed 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 \uf8f6 \uf8f7 \uf8f7 \uf8f8 . (46) We can easily check that this strategy satis\ufb01es the necessary condition for symmetric equilibrium in Theorem 1. Because this strategy is a variant of the Grim trigger strategy [56] T1 (C) = \uf8eb \uf8ec \uf8ec \uf8ed 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \uf8f6 \uf8f7 \uf8f7 \uf8f8 (47) but uses only information at the second last action pro\ufb01le, the strategy (46) can be called as delayed Grim strategy. Theorem 2. A pair of the strategy (46) forms a symmetric mutual reinforcement learning equilibrium if \u03b3 > \ufffd T \u2212R T \u2212P . Proof. The Bellman optimality equation against the strategy (46) is Q\u2217 1 (C, (C, C), (C, C)) = q1 (48) Q\u2217 1 (D, (C, C), (C, C)) = q2 (49) Q\u2217 1 \ufffd C, (C, C), \u03c3(\u22122)\ufffd = q3 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (50) Q\u2217 1 \ufffd D, (C, C), \u03c3(\u22122)\ufffd = q4 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (51) Q\u2217 1 (C, (C, D), (C, C)) = q5 (52) Q\u2217 1 (D, (C, D), (C, C)) = q6 (53) Q\u2217 1 \ufffd C, (C, D), \u03c3(\u22122)\ufffd = q7 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (54) Q\u2217 1 \ufffd D, (C, D), \u03c3(\u22122)\ufffd = q8 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (55) 12 Q\u2217 1 (C, (D, C), (C, C)) = q9 (56) Q\u2217 1 (D, (D, C), (C, C)) = q10 (57) Q\u2217 1 \ufffd C, (D, C), \u03c3(\u22122)\ufffd = q11 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (58) Q\u2217 1 \ufffd D, (D, C), \u03c3(\u22122)\ufffd = q12 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (59) Q\u2217 1 (C, (D, D), (C, C)) = q13 (60) Q\u2217 1 (D, (D, D), (C, C)) = q14 (61) Q\u2217 1 \ufffd C, (D, D), \u03c3(\u22122)\ufffd = q15 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (62) Q\u2217 1 \ufffd D, (D, D), \u03c3(\u22122)\ufffd = q16 \ufffd \u03c3(\u22122) \u0338= (C, C) \ufffd (63) with the self-consistency condition q1 > q2 q3 < q4 q5 > q6 q7 < q8 q9 > q10 q11 < q12 q13 > q14 q15 < q16. (64) The solution is q1 = 1 1 \u2212 \u03b3 R (65) q2 = T + \u03b3 1 \u2212 \u03b32 R + \u03b32 1 \u2212 \u03b32 P (66) q3 = S + \u03b3 1 \u2212 \u03b32 R + \u03b32 1 \u2212 \u03b32 P (67) q4 = 1 1 \u2212 \u03b32 P + \u03b3 1 \u2212 \u03b32 R (68) q5 = q9 = q13 = 1 1 \u2212 \u03b32 R + \u03b3 1 \u2212 \u03b32 P (69) q6 = q10 = q14 = T + \u03b3 1 \u2212 \u03b3 P (70) q7 = q11 = q15 = S + \u03b3 1 \u2212 \u03b3 P (71) q8 = q12 = q16 = 1 1 \u2212 \u03b3 P. (72) 13 For these solution, the inequalities (64) are satis\ufb01ed if \u03b3 > \ufffd T \u2212 R T \u2212 P . (73) We remark that the condition (73) is more strict than the condition that Grim forms a symmetric equilibrium [55]: \u03b3 > T \u2212R T \u2212P . 5.2. Delayed Win-Stay Lose-Shift strategy The second candidate of the solution of Eq. (45) is T1 (C) = T2 (C) = \uf8eb \uf8ec \uf8ec \uf8ed 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 \uf8f6 \uf8f7 \uf8f7 \uf8f8 . (74) We can easily check that this strategy satis\ufb01es the necessary condition for symmetric equilibrium in Theorem 1. Because this strategy is a variant of the Win-Stay Lose-Shift (WSLS) strategy [22] T1 (C) = \uf8eb \uf8ec \uf8ec \uf8ed 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 \uf8f6 \uf8f7 \uf8f7 \uf8f8 (75) but uses only information at the second last action pro\ufb01le, the strategy (74) can be called as delayed WSLS strategy. Theorem 3. When 2R > T + P holds, a pair of the strategy (74) forms a symmetric mutual reinforcement learning equilibrium if \u03b3 > \ufffd T \u2212R R\u2212P . Proof. The Bellman optimality equation against the strategy (74) is Q\u2217 1 \ufffd C, (C, C), \u03c3(\u22122)\ufffd = q1 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (76) Q\u2217 1 \ufffd D, (C, C), \u03c3(\u22122)\ufffd = q2 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (77) Q\u2217 1 \ufffd C, (C, C), \u03c3(\u22122)\ufffd = q3 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (78) Q\u2217 1 \ufffd D, (C, C), \u03c3(\u22122)\ufffd = q4 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (79) Q\u2217 1 \ufffd C, (C, D), \u03c3(\u22122)\ufffd = q5 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (80) Q\u2217 1 \ufffd D, (C, D), \u03c3(\u22122)\ufffd = q6 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (81) Q\u2217 1 \ufffd C, (C, D), \u03c3(\u22122)\ufffd = q7 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (82) Q\u2217 1 \ufffd D, (C, D), \u03c3(\u22122)\ufffd = q8 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (83) 14 Q\u2217 1 \ufffd C, (D, C), \u03c3(\u22122)\ufffd = q9 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (84) Q\u2217 1 \ufffd D, (D, C), \u03c3(\u22122)\ufffd = q10 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (85) Q\u2217 1 \ufffd C, (D, C), \u03c3(\u22122)\ufffd = q11 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (86) Q\u2217 1 \ufffd D, (D, C), \u03c3(\u22122)\ufffd = q12 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (87) Q\u2217 1 \ufffd C, (D, D), \u03c3(\u22122)\ufffd = q13 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (88) Q\u2217 1 \ufffd D, (D, D), \u03c3(\u22122)\ufffd = q14 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (89) Q\u2217 1 \ufffd C, (D, D), \u03c3(\u22122)\ufffd = q15 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (90) Q\u2217 1 \ufffd D, (D, D), \u03c3(\u22122)\ufffd = q16 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (91) with the self-consistency condition q1 > q2 q3 < q4 q5 > q6 q7 < q8 q9 > q10 q11 < q12 q13 > q14 q15 < q16. (92) The solution is q1 = q13 = 1 1 \u2212 \u03b3 R (93) q2 = q14 = T + \u03b3R + \u03b32P + \u03b33 1 \u2212 \u03b3 R (94) q3 = q15 = S + \u03b3R + \u03b32P + \u03b33 1 \u2212 \u03b3 R (95) q4 = q16 = P + \u03b3 1 \u2212 \u03b3 R (96) q5 = q9 = R + \u03b3P + \u03b32 1 \u2212 \u03b3 R (97) q6 = q10 = T + \u03b3P + \u03b32P + \u03b33 1 \u2212 \u03b3 R (98) q7 = q11 = S + \u03b3P + \u03b32P + \u03b33 1 \u2212 \u03b3 R (99) q8 = q12 = P + \u03b3P + \u03b32 1 \u2212 \u03b3 R. (100) 15 For these solution, the inequalities (92) are satis\ufb01ed if \u03b3 > \ufffd T \u2212 R R \u2212 P . (101) It should be noted that such \u03b3 < 1 exists only if 2R > T + P. We remark that the condition (101) is more strict than the condition that WSLS forms a symmetric equilibrium [55]: \u03b3 > T \u2212R R\u2212P . 5.3. All-or-None strategy The third example of the solution of Eq. (45) is the All-or-None strategy AON2 [51]: T1 (C) = T2 (C) = \uf8eb \uf8ec \uf8ec \uf8ed 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 \uf8f6 \uf8f7 \uf8f7 \uf8f8 . (102) We can easily check that this strategy satis\ufb01es the necessary condition for symmetric equilibrium in Theorem 1. It has been known that AON2 forms subgame perfect equilibrium [51]. A similar strategy as AON2 was also observed in numerical simulation of evolution of cooperation [57]. Theorem 4. When 3R \u2212 2P \u2212 T > 0 and 2R \u2212 3P + S > 0 hold, a pair of the strategy (102) forms a symmetric mutual reinforcement learning equilibrium if \u03b3 > max \ufffd 1 2 \ufffd\ufffd 4T \u2212 3R \u2212 P R \u2212 P \u2212 1 \ufffd , 1 2 \ufffd\ufffd R + 3P \u2212 4S R \u2212 P \u2212 1 \ufffd\ufffd .(103) Proof. The Bellman optimality equation against the strategy (102) is Q\u2217 1 \ufffd C, (C, C), \u03c3(\u22122)\ufffd = q1 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (104) Q\u2217 1 \ufffd D, (C, C), \u03c3(\u22122)\ufffd = q2 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (105) Q\u2217 1 \ufffd C, (C, C), \u03c3(\u22122)\ufffd = q3 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (106) Q\u2217 1 \ufffd D, (C, C), \u03c3(\u22122)\ufffd = q4 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (107) Q\u2217 1 \ufffd C, (C, D), \u03c3(\u22122)\ufffd = q7 \ufffd \u03c3(\u22122) \u2208 \u2126 \ufffd (108) Q\u2217 1 \ufffd D, (C, D), \u03c3(\u22122)\ufffd = q8 \ufffd \u03c3(\u22122) \u2208 \u2126 \ufffd (109) Q\u2217 1 \ufffd C, (D, C), \u03c3(\u22122)\ufffd = q11 \ufffd \u03c3(\u22122) \u2208 \u2126 \ufffd (110) Q\u2217 1 \ufffd D, (D, C), \u03c3(\u22122)\ufffd = q12 \ufffd \u03c3(\u22122) \u2208 \u2126 \ufffd (111) 16 Q\u2217 1 \ufffd C, (D, D), \u03c3(\u22122)\ufffd = q13 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (112) Q\u2217 1 \ufffd D, (D, D), \u03c3(\u22122)\ufffd = q14 \ufffd \u03c3(\u22122) \u2208 {(C, C), (D, D)} \ufffd (113) Q\u2217 1 \ufffd C, (D, D), \u03c3(\u22122)\ufffd = q15 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (114) Q\u2217 1 \ufffd D, (D, D), \u03c3(\u22122)\ufffd = q16 \ufffd \u03c3(\u22122) \u2208 {(C, D), (D, C)} \ufffd (115) with the self-consistency condition q1 > q2 q3 < q4 q7 < q8 q11 < q12 q13 > q14 q15 < q16. (116) The solution is q1 = q13 = 1 1 \u2212 \u03b3 R (117) q2 = q14 = T + \u03b3P + \u03b32P + \u03b33 1 \u2212 \u03b3 R (118) q3 = q7 = q11 = q15 = S + \u03b3P + \u03b32P + \u03b33 1 \u2212 \u03b3 R (119) q4 = q16 = P + \u03b3 1 \u2212 \u03b3 R (120) q8 = q12 = P + \u03b3P + \u03b32 1 \u2212 \u03b3 R. (121) For these solution, the inequalities (116) are satis\ufb01ed if (R \u2212 P)\u03b32 + (R \u2212 P)\u03b3 \u2212 (T \u2212 R) > 0 (122) and (R \u2212 P)\u03b32 + (R \u2212 P)\u03b3 \u2212 (P \u2212 S) > 0, (123) which are equivalent to Eq. (103) for \u03b3 \u2265 0. It should be noted that such \u03b3 < 1 exists only if 3R \u2212 2P \u2212 T > 0 and 2R \u2212 3P + S > 0. 6. Optimality in longer memory In previous sections, we investigated symmetric equilibrium of mutual reinforcement learning when both players use memory-two strategies, and obtained three examples of deterministic strategies forming symmetric equilibrium. A natural question is \u201cDo these strategies forming symmetric equilibrium 17 in memory-two reinforcement learning also form symmetric equilibrium of mutual reinforcement learning of longer memory strategies?\u201d. In this section, we show that the answer to this question is \u201cyes\u201d. We \ufb01rst prove the following theorem. Theorem 5. Let T\u2212a be a memory-n\u2032 strategy of player \u2212a. Let T \u2217 a be the optimal strategy of player a against T\u2212a when player a use reinforcement learning of memory-n\u2032 strategies. When player a use reinforcement learning of memoryn strategies with n > n\u2032 to obtain the optimal strategy \u02c7T \u2217 a against T\u2212a, then \u02c7T \u2217 a = T \u2217 a . Proof. When player \u2212a use memory-n\u2032 strategy and player a use memory-n reinforcement learning with n > n\u2032, the Bellman optimality equation (5) becomes Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn\u2032 m=1 \ufffd +\u03b3 \ufffd \u03c3\u2212a T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn\u2032 m=1 \ufffd max \u02c6\u03c3 Q\u2217 a \ufffd \u02c6\u03c3, \u03c3, \ufffd \u03c3(\u2212m)\ufffdn\u22121 m=1 \ufffd . (124) Then, we \ufb01nd that the right-hand side does not depend on \u03c3(\u2212n), and therefore Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn\u22121 m=1 \ufffd (125) Then, the Bellman optimality equation becomes Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn\u22121 m=1 \ufffd = \ufffd \u03c3\u2212a ra (\u03c3) T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn\u2032 m=1 \ufffd +\u03b3 \ufffd \u03c3\u2212a T\u2212a \ufffd \u03c3\u2212a| \ufffd \u03c3(\u2212m)\ufffdn\u2032 m=1 \ufffd max \u02c6\u03c3 Q\u2217 a \ufffd \u02c6\u03c3, \u03c3, \ufffd \u03c3(\u2212m)\ufffdn\u22122 m=1 \ufffd . (126) By repeating the same argument until the length of memory decreases to n\u2032, we \ufb01nd that Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = Q\u2217 a \ufffd \u03c3a, \ufffd \u03c3(\u2212m)\ufffdn\u2032 m=1 \ufffd , (127) which implies that \u02c7T \u2217 a = T \u2217 a . 18 ",
    "Conclusion": "Conclusion In this paper, we investigated symmetric equilibrium of mutual reinforcement learning when both players use memory-two deterministic strategies in the repeated prisoners\u2019 dilemma game. First, we \ufb01nd that the structure of the optimal strategies is constrained by the Bellman optimality equation (Proposition 1). Then, we \ufb01nd a necessary condition for deterministic symmetric equilibrium of mutual reinforcement learning (Theorem 1). Furthermore, we provided three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibrium, some of which can be regarded as variants of the Grim strategy and the WSLS strategy (Theorem 2, Theorem 3 and Theorem 4). Finally, we proved that mutual reinforcement learning equilibria achieved by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-n strategies with n > 2 (Theorem 5). We want to investigate whether other symmetric mutual reinforcement learning equilibria of deterministic memory-two strategies exist or not in future. For such purpose, novel methods are needed, because the number of strategies is quite large. Furthermore, extension of our analysis to (i) asymmetric equilibrium and (ii) mixed strategies is also a subject of future work. Ultimately, if we would develop some method to \ufb01nd all equilibria in all length of memory n, analysis of mutual reinforcement learning equilibria is completed. 19 Acknowledgement We thank Genki Ichinose and Mashiho Mukaida for useful discussions. This study was supported by JSPS KAKENHI Grant Number JP20K19884 and Inamori Research Grants. Appendix A. Derivation of Eq. (4) First we introduce the notation T \ufffd \u03c3| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd := 2 \ufffd a=1 Ta \ufffd \u03c3a| \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd . (A.1) We remark that the joint probability distribution of the action pro\ufb01les \u03c3(\u00b5), \u00b7 \u00b7 \u00b7 , \u03c3(0) given \ufffd \u03c3(\u2212m)\ufffdn m=1 is described as P \ufffd \u03c3(\u00b5), \u00b7 \u00b7 \u00b7 , \u03c3(0) \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \u00b5 \ufffd s=0 T \ufffd \u03c3(s)\ufffd\ufffd\ufffd \ufffd \u03c3(s\u2212m)\ufffdn m=1 \ufffd . (A.2) 20 The action-value function (3) is rewritten as Qa \ufffd \u03c3(0) a , \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd [\u03c3(s)] \u221e s=1 \ufffd \u03c3(0) \u2212a \u221e \ufffd k=0 \u03b3kra \ufffd \u03c3(k)\ufffd \ufffd \u221e \ufffd s=1 T \ufffd \u03c3(s)\ufffd\ufffd\ufffd \ufffd \u03c3(s\u2212m)\ufffdn m=1 \ufffd\ufffd \u00d7T\u2212a \ufffd \u03c3(0) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd [\u03c3(s)] \u221e s=1 \ufffd \u03c3(0) \u2212a \ufffd ra \ufffd \u03c3(0)\ufffd + \u03b3 \u221e \ufffd k=0 \u03b3kra \ufffd \u03c3(k+1)\ufffd\ufffd \u00d7 \ufffd \u221e \ufffd s=1 T \ufffd \u03c3(s)\ufffd\ufffd\ufffd \ufffd \u03c3(s\u2212m)\ufffdn m=1 \ufffd\ufffd T\u2212a \ufffd \u03c3(0) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd \u03c3(0) \u2212a ra \ufffd \u03c3(0)\ufffd T\u2212a \ufffd \u03c3(0) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd +\u03b3 \ufffd [\u03c3(s)] \u221e s=1 \ufffd \u03c3(0) \u2212a \u221e \ufffd k=0 \u03b3kra \ufffd \u03c3(k+1)\ufffd \ufffd \u221e \ufffd s=2 T \ufffd \u03c3(s)\ufffd\ufffd\ufffd \ufffd \u03c3(s\u2212m)\ufffdn m=1 \ufffd\ufffd \u00d7T\u2212a \ufffd \u03c3(1) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(1\u2212m)\ufffdn m=1 \ufffd Ta \ufffd \u03c3(1) a \ufffd\ufffd\ufffd \ufffd \u03c3(1\u2212m)\ufffdn m=1 \ufffd T\u2212a \ufffd \u03c3(0) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd = \ufffd \u03c3(0) \u2212a ra \ufffd \u03c3(0)\ufffd T\u2212a \ufffd \u03c3(0) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd +\u03b3 \ufffd \u03c3(1) a \ufffd \u03c3(0) \u2212a Qa \ufffd \u03c3(1) a , \ufffd \u03c3(1\u2212m)\ufffdn m=1 \ufffd Ta \ufffd \u03c3(1) a \ufffd\ufffd\ufffd \ufffd \u03c3(1\u2212m)\ufffdn m=1 \ufffd T\u2212a \ufffd \u03c3(0) \u2212a \ufffd\ufffd\ufffd \ufffd \u03c3(\u2212m)\ufffdn m=1 \ufffd , (A.3) which is Eq. (4). 21 ",
    "References": "References References [1] A. Rapoport, A. M. Chammah, C. J. Orwant, Prisoner\u2019s dilemma: A study in con\ufb02ict and cooperation, Vol. 165, University of Michigan Press, 1965. [2] G. J. Mailath, L. Samuelson, Repeated games and reputations: long-run relationships, Oxford University Press, 2006. [3] D. Fudenberg, E. Maskin, The folk theorem in repeated games with discounting or with incomplete information, Econometrica: Journal of the Econometric Society 54 (3) (1986) 533\u2013554. [4] R. J. Aumann, Rationality and bounded rationality, Games and Economic Behavior 21 (1997) 2\u201314. [5] A. Neyman, Bounded complexity justi\ufb01es cooperation in the \ufb01nitely repeated prisoners\u2019 dilemma, Economics Letters 19 (3) (1985) 227\u2013229. 22 [6] A. Rubinstein, Finite automata play the repeated prisoner\u2019s dilemma, Journal of Economic Theory 39 (1) (1986) 83\u201396. [7] E. Kalai, W. Stanford, Finite rationality and interpersonal complexity in repeated games, Econometrica: Journal of the Econometric Society 56 (2) (1988) 397\u2013410. [8] D. Abreu, A. Rubinstein, The structure of nash equilibrium in repeated games with \ufb01nite automata, Econometrica: Journal of the Econometric Society 56 (6) (1988) 1259\u20131281. [9] J. S. Banks, R. K. Sundaram, Repeated games, \ufb01nite automata, and complexity, Games and Economic Behavior 2 (2) (1990) 97\u2013117. [10] E. Ben-Porath, Repeated games with \ufb01nite automata, Journal of Economic Theory 59 (1) (1993) 17\u201332. [11] A. Neyman, Finitely repeated games with \ufb01nite automata, Mathematics of Operations Research 23 (3) (1998) 513\u2013552. [12] E. Lehrer, Repeated games with stationary bounded recall strategies, Journal of Economic Theory 46 (1) (1988) 130\u2013144. [13] H. Sabourian, Repeated games with m-period bounded memory (pure strategies), Journal of Mathematical Economics 30 (1) (1998) 1\u201335. [14] M. Barlo, G. Carmona, H. Sabourian, Repeated games with one-memory, Journal of Economic Theory 144 (1) (2009) 312\u2013336. [15] M. Barlo, G. Carmona, H. Sabourian, Bounded memory folk theorem, Journal of Economic Theory 163 (2016) 728\u2013774. [16] J. M. Smith, G. R. Price, The logic of animal con\ufb02ict, Nature 246 (5427) (1973) 15. [17] R. Boyd, J. P. Lorberbaum, No pure strategy is evolutionarily stable in the repeated prisoner\u2019s dilemma game, Nature 327 (6117) (1987) 58\u201359. [18] D. Fudenberg, E. Maskin, Evolution and cooperation in noisy repeated games, The American Economic Review 80 (2) (1990) 274\u2013279. [19] K. G. Binmore, L. Samuelson, Evolutionary stability in repeated games played by \ufb01nite automata, Journal of Economic Theory 57 (2) (1992) 278\u2013 305. [20] M. A. Nowak, K. Sigmund, E. El-Sedy, Automata, repeated games and noise, Journal of Mathematical Biology 33 (7) (1995) 703\u2013722. [21] M. A. Nowak, K. Sigmund, Tit for tat in heterogeneous populations, Nature 355 (6357) (1992) 250\u2013253. 23 [22] M. Nowak, K. Sigmund, A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner\u2019s dilemma game, Nature 364 (6432) (1993) 56\u201358. [23] C. T. Bergstrom, M. Lachmann, The red king e\ufb00ect: when the slowest runner wins the coevolutionary race, Proceedings of the National Academy of Sciences 100 (2) (2003) 593\u2013598. [24] L. A. Imhof, D. Fudenberg, M. A. Nowak, Evolutionary cycles of cooperation and defection, Proceedings of the National Academy of Sciences 102 (31) (2005) 10797\u201310800. [25] A. Szolnoki, M. Perc, G. Szab\u00b4o, Phase diagrams for three-strategy evolutionary prisoner\u2019s dilemma games on regular graphs, Physical Review E 80 (5) (2009) 056104. [26] M. Perc, J. G\u00b4omez-Gardenes, A. Szolnoki, L. M. Flor\u00b4\u0131a, Y. Moreno, Evolutionary dynamics of group interactions on structured populations: a review, Journal of the Royal Society Interface 10 (80) (2013) 20120997. [27] A. J. Stewart, J. B. Plotkin, From extortion to generosity, evolution in the iterated prisoner\u2019s dilemma, Proceedings of the National Academy of Sciences 110 (38) (2013) 15348\u201315353. [28] E. Kalai, E. Lehrer, Rational learning leads to nash equilibrium, Econometrica: Journal of the Econometric Society (1993) 1019\u20131045. [29] D. Fudenberg, D. K. Levine, Steady state learning and nash equilibrium, Econometrica: Journal of the Econometric Society (1993) 547\u2013573. [30] S. Hart, A. Mas-Colell, A simple adaptive procedure leading to correlated equilibrium, Econometrica 68 (5) (2000) 1127\u20131150. [31] T. Roughgarden, Twenty lectures on algorithmic game theory, Cambridge University Press, 2016. [32] D. Kraines, V. Kraines, Pavlov and the prisoner\u2019s dilemma, Theory and Decision 26 (1) (1989) 47\u201379. [33] G. I. Bischi, A. Naimzada, Global analysis of a dynamic duopoly game with bounded rationality, in: Advances in dynamic games and applications, Springer, 2000, pp. 361\u2013385. [34] Y. Sato, E. Akiyama, J. D. Farmer, Chaos in learning a simple two-person game, Proceedings of the National Academy of Sciences 99 (7) (2002) 4748\u2013 4751. [35] M. W. Macy, A. Flache, Learning dynamics in social dilemmas, Proceedings of the National Academy of Sciences 99 (suppl 3) (2002) 7229\u20137236. 24 [36] N. Masuda, M. Nakamura, Numerical analysis of a reinforcement learning model with the dynamic aspiration level in the iterated prisoner\u2019s dilemma, Journal of Theoretical Biology 278 (1) (2011) 55\u201362. [37] T. Galla, J. D. Farmer, Complex dynamics in learning complicated games, Proceedings of the National Academy of Sciences 110 (4) (2013) 1232\u20131236. [38] D. Fudenberg, D. K. Levine, The theory of learning in games, Vol. 2, MIT Press, 1998. [39] I. Erev, A. E. Roth, Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria, American Economic Review 88 (4) (1998) 848\u2013881. [40] P. Dal B\u00b4o, Cooperation under the shadow of the future: experimental evidence from in\ufb01nitely repeated games, American Economic Review 95 (5) (2005) 1591\u20131604. [41] P. Dal B\u00b4o, G. R. Fr\u00b4echette, The evolution of cooperation in in\ufb01nitely repeated games: Experimental evidence, American Economic Review 101 (1) (2011) 411\u201329. [42] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT Press, 2018. [43] A. Rapoport, Optimal policies for the prisoner\u2019s dilemma., Psychological Review 74 (2) (1967) 136. [44] T. W. Sandholm, R. H. Crites, Multiagent reinforcement learning in the iterated prisoner\u2019s dilemma, Biosystems 37 (1-2) (1996) 147\u2013166. [45] J. Hu, M. P. Wellman, Nash q-learning for general-sum stochastic games, Journal of Machine Learning Research 4 (Nov) (2003) 1039\u20131069. [46] M. Harper, V. Knight, M. Jones, G. Koutsovoulos, N. E. Glynatsi, O. Campbell, Reinforcement learning produces dominant strategies for the iterated prisoner\u2019s dilemma, PloS One 12 (12) (2017) e0188046. [47] W. Barfuss, J. F. Donges, J. Kurths, Deterministic limit of temporal di\ufb00erence reinforcement learning for stochastic games, Physical Review E 99 (4) (2019) 043305. [48] L. Busoniu, R. Babuska, B. De Schutter, A comprehensive survey of multiagent reinforcement learning, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 38 (2) (2008) 156\u2013172. [49] J. Li, G. Kendall, The e\ufb00ect of memory size on the evolutionary stability of strategies in iterated prisoner\u2019s dilemma, IEEE Transactions on Evolutionary Computation 18 (6) (2013) 819\u2013826. 25 [50] S. D. Yi, S. K. Baek, J.-K. Choi, Combination with anti-tit-for-tat remedies problems of tit-for-tat, Journal of Theoretical Biology 412 (2017) 1\u20137. [51] C. Hilbe, L. A. Martinez-Vaquero, K. Chatterjee, M. A. Nowak, Memoryn strategies of direct reciprocity, Proceedings of the National Academy of Sciences 114 (18) (2017) 4715\u20134720. [52] Y. Murase, S. K. Baek, Seven rules to avoid the tragedy of the commons, Journal of Theoretical Biology 449 (2018) 94\u2013102. [53] Y. Murase, S. K. Baek, Five rules for friendly rivalry in direct reciprocity, Scienti\ufb01c Reports 10 (2020) 16904. [54] M. Ueda, Memory-two zero-determinant strategies in repeated games, Royal Society Open Science 8 (5) (2021) 202186. [55] Y. Usui, M. Ueda, Symmetric equilibrium of multi-agent reinforcement learning in repeated prisoner\u2019s dilemma, Applied Mathematics and Computation 409 (2021) 126370. [56] J. W. Friedman, A non-cooperative equilibrium for supergames, The Review of Economic Studies 38 (1) (1971) 1\u201312. [57] C. Hauert, H. G. Schuster, E\ufb00ects of increasing the number of players and memory size in the iterated prisoner\u2019s dilemma: a numerical approach, Proceedings of the Royal Society of London. Series B: Biological Sciences 264 (1381) (1997) 513\u2013519. 26 ",
    "title": "Memory-two strategies forming symmetric mutual",
    "paper_info": "arXiv:2108.03258v2  [physics.soc-ph]  28 Dec 2022\nMemory-two strategies forming symmetric mutual\nreinforcement learning equilibrium in repeated\nprisoners\u2019 dilemma game\nMasahiko Ueda\nGraduate School of Sciences and Technology for Innovation, Yamaguchi University,\nYamaguchi 753-8511, Japan\nAbstract\nWe investigate symmetric equilibria of mutual reinforcement learning when both\nplayers alternately learn the optimal memory-two strategies against the oppo-\nnent in the repeated prisoners\u2019 dilemma game. We provide a necessary condi-\ntion for memory-two deterministic strategies to form symmetric equilibria. We\nthen provide three examples of memory-two deterministic strategies which form\nsymmetric mutual reinforcement learning equilibria. We also prove that mu-\ntual reinforcement learning equilibria formed by memory-two strategies are also\nmutual reinforcement learning equilibria when both players use reinforcement\nlearning of memory-n strategies with n > 2.\nKeywords:\nRepeated prisoners\u2019 dilemma game; Reinforcement learning;\nMemory-two strategies\n1. Introduction\nThe prisoners\u2019 dilemma is one of the simplest situations in which rational\nactions of individuals do not maximize social welfare [1]. Although the best\naction of each agent is defection, mutual cooperation improves the utility of\nboth agents. On the other hand, if the prisoners\u2019 dilemma game is in\ufb01nitely\nrepeated, the situation changes. In fact, mutual cooperation can be realized by\nrational behavior of each agent, and this result is known as folk theorem [2].\nThe folk theorem was also extended to a stronger version that any individually\nrational payo\ufb00s can be realized as subgame perfect equilibria [3].\nAt the same time, it has been pointed out by experiments that the realistic\nagents like human beings are not necessarily rational, and theories of bounded\nrationality have been needed [4]. One of the mainstream is modeling agents\nby \ufb01nite automata (agents with \ufb01nite complexity) [5, 6, 7, 8, 9, 10, 11]. Es-\npecially, Abreu and Rubinstein found that the equilibrium payo\ufb00s realized by\nEmail address: m.ueda@yamaguchi-u.ac.jp (Masahiko Ueda)\nPreprint submitted to Elsevier\nDecember 29, 2022\n",
    "GPTsummary": "- (1): The article explores the symmetric equilibria of mutual reinforcement learning in the repeated prisoners' dilemma game.\n\n- (2): Past methods involved modeling agents as finite automata or adaptive agents, but they have limitations in describing the behavior of realistic agents. The proposed approach of using memory-two strategies with reinforcement learning is well motivated as it allows for more complicated behavior and is robust against implementation errors.\n\n- (3): The research methodology involves investigating necessary conditions for memory-two deterministic strategies to form symmetric equilibria and providing three examples of memory-two deterministic strategies that form symmetric mutual reinforcement learning equilibria. It is also proven that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-n strategies with n > 2.\n\n- (4): The performance of the proposed methods achieves symmetric mutual reinforcement learning equilibria in the repeated prisoners' dilemma game. The use of memory-two strategies with reinforcement learning allows for complicated behavior and is robust against implementation errors, supporting the goals of the research.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work is significant as it investigates the formation of symmetric mutual reinforcement learning equilibria in the repeated prisoners' dilemma game using memory-two strategies with reinforcement learning. Such an approach allows for more complicated behavior and is robust against implementation errors, supporting the goals of the research.\n\n- (2): Innovation point: The use of memory-two strategies with reinforcement learning in investigating symmetric mutual reinforcement learning equilibria is a novel approach that allows for more complex behavior and robustness against implementation errors.\n\nPerformance: The proposed methods achieve symmetric mutual reinforcement learning equilibria in the repeated prisoners' dilemma game, supporting the efficacy of the approach.\n\nWorkload: The number of strategies to investigate for all equilibria is quite large, which may be a limitation of the research. Additionally, future work is needed to extend the analysis to asymmetric equilibria and mixed strategies.\n\nOverall, this article provides a novel approach to investigate symmetric mutual reinforcement learning equilibria that has potential applications in various fields.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work is significant as it investigates the formation of symmetric mutual reinforcement learning equilibria in the repeated prisoners' dilemma game using memory-two strategies with reinforcement learning. Such an approach allows for more complicated behavior and is robust against implementation errors, supporting the goals of the research.\n\n- (2): Innovation point: The use of memory-two strategies with reinforcement learning in investigating symmetric mutual reinforcement learning equilibria is a novel approach that allows for more complex behavior and robustness against implementation errors.\n\nPerformance: The proposed methods achieve symmetric mutual reinforcement learning equilibria in the repeated prisoners' dilemma game, supporting the efficacy of the approach.\n\nWorkload: The number of strategies to investigate for all equilibria is quite large, which may be a limitation of the research. Additionally, future work is needed to extend the analysis to asymmetric equilibria and mixed strategies.\n\nOverall, this article provides a novel approach to investigate symmetric mutual reinforcement learning equilibria that has potential applications in various fields.\n\n\n"
}