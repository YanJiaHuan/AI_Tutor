{
    "Abstract": "Abstract This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can stabilize methods like LSPI by controlling the improvements\u2019 stochasticity. Additionally we show that a suitable representation of the value function also stabilizes the solution to some degree. The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning. Keywords: stability, approximate reinforcement learning, non-deterministic policy improvement, least-squares policy iteration, slow-feature-analysis representation 1. ",
    "Introduction": "Introduction This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can be used to achieve stability for large discount factors. The presented approach is simple and should also be easily transferable to more sophisticated algorithms. Recently deep reinforcement learning (deep RL) has been very successful in solving complex tasks in large, often continuous state spaces (e.g. playing Atari games and Go, Mnih et al., 2015; Silver et al., 2016). These approaches use gradient based Q-learning (Watkins and Dayan, 1992) or policy gradient methods (Williams, 1992). Gradients in neural networks must be based on i.i.d. distributed samples, though (see Riedmiller, 2005). Deep RL uses therefore mini-batches that are sampled i.i.d. from a \ufb01xed set of experiences, which has been collected before training (called experience replay, Mnih et al., 2013). In di\ufb00erence to online algorithms, which are often guaranteed to converge in the limit of an in\ufb01nite training sequence (e.g. Sutton et al., 2009), batch learning has long been known to be vulnerable to the choice of training sets (Tsitsiklis and Van Roy, 1997; Bertsekas, 2007). Depending on the batch of training samples at hand, an RL algorithm can either converge to an almost optimal or to an arbitrarily bad policy. In practice, this depends strongly (but not predictably) on the discount factor \u03b3. For example, in Figure 1 we demonstrate that 1 B\u00a8ohmer, Guo and Obermayer Figure 1: Navigation performance of policies, learned by LSPI in two environments (see sketched layouts), for varying discount factors \u03b3. Error bars indicate mean and standard deviation of the fraction of successful test-trajectories (starting at random positions) over 10 random-walk training sets with 50000 samples each. The agent can either move forward or rotate 45\u25e6 left or right (i.e. 3 actions). Reaching the goal area is rewarded (+1) and crashing into a wall is punished (-1 or -10). policies learned by least-squares policy iteration (LSPI, Lagoudakis and Parr, 2003) yield very di\ufb00erent performances when the discount factor \u03b3 is varied (experimental details can be found in Section 3). The left plot shows an unpredictable drop in performance for a simple navigation experiment with continuous states and discrete actions, and the right plot the failure of LSPI to learn a suitable policy for a more complicated environment. The young discipline of deep RL has not yet reported e\ufb00ects like these, but it is reasonable to assume that they happen in batch algorithms with more sophisticated architectures as well. Most authors attribute this instability to a lack of convergence guarantees in o\ufb00-policy batch value estimation (see Dann et al., 2014, for an overview). But the distribution of training samples in the batch may also have a profound impact on the policy improvement in approximate RL. For example, Perkins and Precup (2002) show for an algorithm similar to LSPI, that the greedy policy improvement can cause the instability shown in Figure 1. Although their analysis does not carry over to LSPI1, they show that a sequence of non-deterministic policies converge reliably when they are changed slowly enough. Conservative policy iteration (CPI, Kakade and Langford, 2002) follows a similar line of thought and slows down the policy improvement considerably to guarantee convergence2. Safe policy iteration (SPI, Pirotta et al., 2013) extends this concept by determining the speed of 1. Perkins and Precup (2002) use open-ended on-policy online value estimation. Training samples are drawn every time the policy is improved and errors on observed samples can thus average out over time. 2. In CPI, the next policy \u03c0i+1 is a combination of the previous policy \u03c0i and the greedy policy \u03c0\u2217 i+1, i.e., \u03c0i+1 = (1 \u2212 \u03b1)\u03c0i + \u03b1 \u03c0\u2217 i+1. CPI converges for small \u03b1 \u2208 [0, 1]. In SPI the update rate \u03b1 is determined by maximizing a lower bound on the policy improvement, which converges much faster than CPI. 2 Non-Deterministic Policy Improvement Stabilizes Approximated RL change through a lower bound on the policy improvement. The algorithm improves convergence speed signi\ufb01cantly, but is computationally expensive even in \ufb01nite state spaces. Other approaches suggest an actor-critic architecture to avoid oscillations (Wagner, 2011) or optimize a parameterizable softmax-policy directly (Azar et al., 2012). In this paper we evaluate the idea of Perkins and Precup (2002) empirically with LSPI in continuous navigation tasks. Surprisingly, we \ufb01nd that the stochasticity of the improved policy stabilizes the solution, rather than the slowness of policy change. This requires only a small modi\ufb01cation to the policy improvement scheme. Although our approach is a heuristic and theoretically not as well-grounded as the above algorithms, it is fast, simple to implement, and can be applied to most algorithms used in deep RL. 2. Non-Deterministic Policy Improvement In this paper we consider tasks with continuous state space X and discrete3 action space A. A non-deterministic policy \u03c0(a|x) \u2208 [0, 1] , \u2200a \u2208 A , \ufffd a\u2032\u2208A \u03c0(a\u2032|x) = 1 , \u2200x \u2208 X , can be evaluated by any algorithm to estimate the corresponding Q-value function q : X \u00d7A \u2192 IR. To converge to the optimal policy, the policy \u03c0 must also be improved, either during Q-value estimation or in an additional step. The improvement in a state x \u2208 X usually chooses the action a \u2208 A that maximizes the current Q-value estimate q(x, a). Instead of this greedy improvement, we propose to produce an improved non-deterministic policy. Examples are softmax \u03c0q \u03b2 or \u01eb-greedy \u03c0q \u01eb policies4, that is, \u2200a \u2208 A, \u2200x \u2208 X: \u03c0q \u03b2(a|x) = exp \ufffd \u03b2 q(x, a) \ufffd \ufffd a\u2032\u2208A exp \ufffd \u03b2 q(x, a\u2032) \ufffd or \u03c0q \u01eb(a|x) = \u01eb 1 |A| + \ufffd 1 \u2212 \u01eb , if a = arg max a\u2032\u2208A q(x, a\u2032) 0 , otherwise . Existing algorithms can be adapted by identifying the greedy policy improvement operator \u02c6\u0393\u2217 and replacing it with the non-deterministic \u02c6\u0393\u03b2, that is, for functions f, q : X \u00d7 A \u2192 IR: \u02c6\u0393\u2217[f|q](x) = f \ufffd x, arg max a\u2208A q(x, a) \ufffd \u21d2 \u02c6\u0393\u03b2[f|q](x) = \ufffd a\u2208A \u03c0q \u03b2(a|x) f(x, a) , \u2200x \u2208 X . Here \u03b2 \u2208 [0, \u221e) denotes the inverse stochasticity of the operator. For example, a nondeterministic version of the TD-error \u03b4t in Q-learning for the observation (xt, at, rt, xt+1) is \u03b4t = rt+\u03b3 \u02c6\u0393\u03b2[q|q](xt+1)\u2212q(xt, at), and the matrix A \u2208 IRm\u00d7m, which has to be inverted during non-deterministic least-squares temporal di\ufb00erence learning (LSTD, Bradtke and Barto, 1996, used by LSPI), would be computed from a training batch {xt, at, rt}n t=0 by Aij = 1 n n\u22121 \ufffd t=0 \u03c6i(xt, at) \ufffd \u03c6j(xt, at) \u2212 \u03b3\u02c6\u0393\u03b2[\u03c6j|q](xt+1) \ufffd , \u2200i, j \u2208 {1, . . . , m} . Softmax policies use more information than \u01eb-greedy and are in most situations the better choice. However, the stochasticity of the softmax depends strongly on the di\ufb00erences between Q-values. Far away from the reward, Q-values can become very similar and softmax 3. The extension to continuous action spaces is straight forward, but requires to compute an integral for each application of the policy improvement operator \u02c6\u0393\u03b2[f|q](x) = \ufffd \u03c0q \u03b2(a|x) f(x, a) da. 4. The softmax is also called the Boltzmann or the Gibbs policy. Note the similarities to the policies of Wagner (2011) and Azar et al. (2012), which both implement a softmax based on the optimized function. 3 ",
    "Experiments": "Experiments We evaluated the e\ufb00ects of non-deterministic policy improvement at the example of a simple navigation experiment in an U- and a S-shaped environment (see inlays of Figure 1). The three dimensional state space X consisted of the agent\u2019s two-dimensional position and its orientation. The action space A contained 3 actions: a forward movement and two 45\u25e6 rotations. Crashing into a wall stopped movement and it would take the agent between 20 and 25 unimpeded moves to traverse the environment in one spatial dimension. Reaching the goal area (gray circle in the inlays) yielded a reward of +1 and crashes incurred a punishment of -1 in the U-shaped and -10 in the S-shaped environment. To represent the Q-value function, we chose a Fourier basis (Konidaris et al., 2011) and constructed 1500 basis functions over the space of states and actions. The bases contained all combinations of: 10 cosine functions (including a constant) for each spatial dimension; a constant, 2 cosine and 2 sine functions for the orientation; and 3 discrete Kronecker-delta functions for the actions. Irrespective their policy improvement, policies were evaluated greedily to remain comparable. Performance was measured in fraction of successful trajectories, which we estimated by running the greedy policy from 200 random starting positions/orientations. Successful trajectories reach the goal within 100 actions without hitting a wall. 3.1 Non-Deterministic Policy Improvement We started out to test the idea of Perkins and Precup (2002) for LSPI by using nondeterministic policy improvement (soft-LSPI) with slowly growing inverse stochasticity \u03b2 (similar to simulated annealing, Haykin, 1998). However, we observed that the annealing process itself did not improve the learned policy. The performance was always comparable to soft-LSPI with the annealing\u2019s \ufb01nal stochasticity \u03b2 (not shown here). Figure 2 plots the performance of greedy-LSPI and soft-LSPI (with constant stochasticity \u03b2) for varying discount factors \u03b3. In the face of sparse rewards, \u03b3 determines how far that reward is propagated, before it is drowned in inevitable approximation errors. Low \u03b3 yields policies that are only correct close to the reward, and have therefore a bad performance. On the other hand, \u03b3 close to 1 can lead to nearly optimal policies everywhere, but performance is strongly a\ufb00ected by the instability investigated in this paper. Note that the large standard deviations in both plots stem from some training sets producing near optimal, while others producing nonsensical policies. For this reason we refer to these regimes as \u201cinstable\u201d. First, one can observe that increasing stochasticity (lower \u03b2) drastically stabilizes the soft-LSPI policies. Secondly, note that there seems to be a trade-o\ufb00 between inverse stochasticity \u03b2 and discount factor \u03b3. Low \u03b2 reduces performance while increasing stability, but in the left plot the performance with low \u03b2 becomes near optimal for larger \u03b3, 4 Non-Deterministic Policy Improvement Stabilizes Approximated RL Figure 2: LSPI with greedy and softmax policy improvement, compared in the navigation tasks of Figure 1. Large standard deviations are usually caused by a mixture of excellent and horrible policies. We therefore call these regimes \u201cinstable\u201d. Stochastic improvements (with small \u03b2, e.g. green triangles) decrease performance for small \u03b3, but stabilize convergence for large \u03b3 signi\ufb01cantly. Figure 3: LSPI policies based on di\ufb00erent representations in the navigation tasks of Figure 1. Better representations (here RSK-SFA) generally improve performance, but non-deterministic policy improvement is still needed to stabilize LSPI in complex tasks (e.g., right plot). Also note the pronounced trade-o\ufb00 between \u03b2 and \u03b3. 5 ",
    "Conclusion": "Conclusion We have shown that (at least) LSPI can become instable in some unpredictable regimes of the discount factor \u03b3. Here small di\ufb00erences in the training set can lead to large di\ufb00erences in policy performance. It is not exactly clear why solutions become instable, but we show that learned policies can be stabilized by using a non-deterministic policy improvement scheme. All presented experiments became signi\ufb01cantly more stable by increasing stochasticity 1 \u03b2 and discount factor \u03b3 at the same time. Future works may extend our approach by adjusting both parameters during policy iteration (like in SPI, Pirotta et al., 2013). Better representations of the state-action space have also improved stability to some extend. More sophisticated approaches (like deep RL) learn these representations implicitly in their lower layers and may therefore be more stable than LSPI. Nonetheless, instabilities will probably occur, and non-deterministic policy improvement can most likely be employed to stabilize the learned policy in deep RL, too. In conclusion, when success or failure of learned policies depends crucially on the training set (e.g. during cross-validation), one should consider a non-deterministic policy improvement scheme. The scheme presented in this paper is computationally cheap, easy to implement, and can be \ufb01ne-tuned with the inverse stochasticity \u03b2. 5. Strictly speaking, this holds only for values of the sampling policy of the training data. However, SFA features are reported to work well with LSPI for random-walk training sets (B\u00a8ohmer et al., 2013). 6 ",
    "References": "References Mohammad Gheshlaghi Azar, Vicen\u00b8c G\u00b4omez, and Hilbert J. Kappen. Dynamic policy programming. Journal of Machine Learning Research, 13:3207\u20133245, 2012. Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scienti\ufb01c, 3rd edition, 2007. Wendelin B\u00a8ohmer, Ste\ufb00en Gr\u00a8unew\u00a8alder, Hannes Nickisch, and Klaus Obermayer. Generating feature spaces for linear algorithms with regularized sparse kernel slow feature analysis. Machine Learning, 89(1-2):67\u201386, 2012. Wendelin B\u00a8ohmer, Ste\ufb00en Gr\u00a8unew\u00a8alder, Yun Shen, Marek Musial, and Klaus Obermayer. Construction of approximation spaces for reinforcement learning. Journal of Machine Learning Research, 14:2067\u20132118, July 2013. Steven J. Bradtke and Aandrew G. Barto. Linear least-squares algorithms for temporal di\ufb00erence learning. Machine Learning, 22(1/2/3):33\u201357, 1996. Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal di\ufb00erences: a survey and comparison. Journal of Machine Learning Research, 15:809\u2013 883, 2014. Simon Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, 2nd edition, 1998. ISBN 978-0132733502. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the 19th International Conference on Machine Learning, pages 267\u2013274, 2002. G. D. Konidaris, S. Osentoski, and P.S. Thomas. Value function approximation in reinforcement learning using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on Arti\ufb01cial Intelligence, 2011. Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107\u20131149, 2003. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig 7 B\u00a8ohmer, Guo and Obermayer Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, February 2015. Theodore J. Perkins and Doina Precup. A convergent form of approximate policy iteration. In Advances in Neural Information Processing Systems 15, pages 1595\u20131602. 2002. Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration. In Proceedings of the 30th International Conference on Machine Learning, pages 307\u2013315, 2013. Martin Riedmiller. Neural \ufb01tted Q-iteration - \ufb01rst experiences with a data e\ufb03cient neural reinforcement learning method. In 16th European Conference on Machine Learning, pages 317\u2013328. Springer, 2005. David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484\u2013503, January 2016. Richard S. Sutton, Csaba Szepesv\u00b4ari, and Hamid Reza Maei. A convergent o(n) algorithm for o\ufb00-policy temporal-di\ufb00erence learning with linear function approximation. In Advances in Neural Information Processing Systems, volume 21, pages 1609\u20131616. MIT Press, 2009. John N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-di\ufb00erence learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674\u2013690, 1997. Paul Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy iteration. In Advances in Neural Information Processing Systems 24, pages 2573\u20132581. 2011. Christopher Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279\u2013292, 1992. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, 1992. Laurenz Wiskott and Terrence Sejnowski. Slow feature analysis: unsupervised learning of invariances. Neural Computation, 14(4):715\u2013770, 2002. 8 ",
    "title": "Non-Deterministic Policy Improvement",
    "paper_info": "arXiv:1612.07548v1  [cs.AI]  22 Dec 2016\nThis paper has been presented at the 13th European Workshop on Reinforcement Learn-\ning (EWRL 2016) on the 3rd and 4th of December 2016 in Barcelona, Spain.\nNon-Deterministic Policy Improvement\nStabilizes Approximated Reinforcement Learning\nWendelin B\u00a8ohmer\u2217 and Rong Guo and Klaus Obermayer\nNeural Information Processing Group, Technische Universit\u00a8at Berlin,\nMarchstra\u00dfe 23, D-10587 Berlin, Germany.\n\u2217 corresponding author (email: wendelin@ni.tu-berlin.de)\nEditor: Gergely Neu, Vince\u00b8c G\u00b4omez and Csaba Szepesv\u00b4ari\nAbstract\nThis paper investigates a type of instability that is linked to the greedy policy improvement\nin approximated reinforcement learning. We show empirically that non-deterministic policy\nimprovement can stabilize methods like LSPI by controlling the improvements\u2019 stochastic-\nity. Additionally we show that a suitable representation of the value function also stabilizes\nthe solution to some degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\nKeywords:\nstability, approximate reinforcement learning, non-deterministic policy im-\nprovement, least-squares policy iteration, slow-feature-analysis representation\n1. Introduction\nThis paper investigates a type of instability that is linked to the greedy policy improvement\nin approximated reinforcement learning. We show empirically that non-deterministic policy\nimprovement can be used to achieve stability for large discount factors.\nThe presented\napproach is simple and should also be easily transferable to more sophisticated algorithms.\nRecently deep reinforcement learning (deep RL) has been very successful in solving\ncomplex tasks in large, often continuous state spaces (e.g. playing Atari games and Go,\nMnih et al., 2015; Silver et al., 2016).\nThese approaches use gradient based Q-learning\n(Watkins and Dayan, 1992) or policy gradient methods (Williams, 1992).\nGradients in\nneural networks must be based on i.i.d. distributed samples, though (see Riedmiller, 2005).\nDeep RL uses therefore mini-batches that are sampled i.i.d. from a \ufb01xed set of experiences,\nwhich has been collected before training (called experience replay, Mnih et al., 2013).\nIn di\ufb00erence to online algorithms, which are often guaranteed to converge in the limit of\nan in\ufb01nite training sequence (e.g. Sutton et al., 2009), batch learning has long been known to\nbe vulnerable to the choice of training sets (Tsitsiklis and Van Roy, 1997; Bertsekas, 2007).\nDepending on the batch of training samples at hand, an RL algorithm can either converge\nto an almost optimal or to an arbitrarily bad policy. In practice, this depends strongly (but\nnot predictably) on the discount factor \u03b3. For example, in Figure 1 we demonstrate that\n1\n",
    "GPTsummary": "\n\n\n\n8. Conclusion:\n\n- (1): This article proposes a methodology for stabilizing the approximated reinforcement learning, which involves choosing a sequence of non-deterministic polices instead of using a greedy policy improvement. This approach is found to be fast and effective in dealing with instability issues related to the greedy policy improvement in deep reinforcement learning.\n\n- (2): Innovation point: The idea of using non-deterministic policy improvement to stabilize approximated reinforcement learning is innovative, and the approach is simple and easily applicable to most algorithms used in deep reinforcement learning. Performance: The proposed approach is evaluated and found to be effective in stabilizing the learning process in continuous navigation tasks using the LSPI method. Workload: The article is well-written and adequately detailed to assist readers in understanding the proposed methodology and its implementation. However, future research could explore the proposed methodology's general applicability and expand on the results obtained in the empirical evaluation.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This article proposes a methodology for stabilizing the approximated reinforcement learning, which involves choosing a sequence of non-deterministic polices instead of using a greedy policy improvement. This approach is found to be fast and effective in dealing with instability issues related to the greedy policy improvement in deep reinforcement learning.\n\n- (2): Innovation point: The idea of using non-deterministic policy improvement to stabilize approximated reinforcement learning is innovative, and the approach is simple and easily applicable to most algorithms used in deep reinforcement learning. Performance: The proposed approach is evaluated and found to be effective in stabilizing the learning process in continuous navigation tasks using the LSPI method. Workload: The article is well-written and adequately detailed to assist readers in understanding the proposed methodology and its implementation. However, future research could explore the proposed methodology's general applicability and expand on the results obtained in the empirical evaluation.\n\n\n"
}