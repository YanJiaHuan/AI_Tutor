{
    "Abstract": "Abstract. Learning-based approaches for solving large sequential decision making problems have become popular in recent years. The resulting agents perform di\ufb00erently and their characteristics depend on those of the underlying learning approach. Here, we consider a benchmark planning problem from the reinforcement learning domain, the Racetrack, to investigate the properties of agents derived from di\ufb00erent deep (reinforcement) learning approaches. We compare the performance of deep supervised learning, in particular imitation learning, to reinforcement learning for the Racetrack model. We \ufb01nd that imitation learning yields agents that follow more risky paths. In contrast, the decisions of deep reinforcement learning are more foresighted, i.e., avoid states in which fatal decisions are more likely. Our evaluations show that for this sequential decision making problem, deep reinforcement learning performs best in many aspects even though for imitation learning optimal decisions are considered. Keywords: Deep Reinforcement Learning \u00b7 Imitation Learning \u00b7 Racetrack. 1 ",
    "Introduction": "Introduction In recent years, deep learning (DL) and especially deep reinforcement learning (DRL) have been applied with great successes to the task of learning nearoptimal policies for sequential decision making problems. DRL has been applied to various applications such as Atari games [11, 12], Go and Chess [16\u201318], or Rubic\u2019s cube [1]. It relies on a feedback loop between self-play and the improvement of the current strategy by reinforcing decisions that lead to good performance. Passive imitation learning (PIL) is another well-known approach to solve sequential decision making problems, where a policy is learned based on training data that is labeled by an expert [15]. An extension of this approach is active imitation learning (AIL), where after an initial phase of passive learning, additional data is iteratively generated by exploring the state space based on the current strategy and subsequent expert labeling [7, 14]. AIL has successfully arXiv:2008.00766v1  [cs.LG]  3 Aug 2020 2 T. Gros et al. been applied to common reinforcement learning benchmarks such as cart-pole or bicycle-balancing [7]. Sequential decision making problems are typically described by Markov decision processes (MDPs). During the simulation of an MDP, the set of those states that will be visited in the future depend on current decisions. In PIL, the agent, which represents a policy, is trained by iterating over the given expert data set, whose distribution does not generally resemble this dependence. AIL extends the data with sequentially generated experiences. Hence, the data is more biased towards sequentially taken decisions. In contrast, DRL does not rely on expert data at all, but simply alternates between exploitation of former experiences and exploration. It is a priori not obvious which method achieves the best result for a particular sequential decision making problem. Here we aim at an in-depth study of empirical learning agent behavior for a range of di\ufb00erent learning frameworks. Speci\ufb01cally we are interested in di\ufb00erences due to the sequential nature of action decisions, inherent in reinforcement learning and active imitation learning but not in passive imitation learning. To be able to study and understand algorithm behavior in detail, we conduct our investigation in a simple benchmark problem, namely Racetrack. Racetrack is originally a pen and paper game, adopted as a benchmark in AI sequential decision making for the evaluation of MDP solution algorithms [2, 3, 13, 19]. A map with obstacles is given, and a policy for reaching a goal region from an initial position has to be found. Decisions for two-dimensional accelerations are taken sequentially, which requires foresighted planning. Ignoring tra\ufb03c, changing weather conditions, fuel consumption, and technical details, Racetrack can be considered a simpli\ufb01ed model of autonomous driving control [4]. Racetrack is ideally suited for a comparison of di\ufb00erent learning approaches, because not only the performance of di\ufb00erent agents but also their \u201cdriving characteristics\u201d can be analyzed. Moreover, for small maps, expert data describing optimal policies can be obtained. We train di\ufb00erent agents for Racetrack using DRL, PIL, and AIL and study their characteristics. We \ufb01rst apply PIL and train agents represented by linear functions and arti\ufb01cial neural networks. As expert labeling, we apply the A\u2217 algorithm to \ufb01nd optimal actions for states in Racetrack. We suggest di\ufb00erent variants of data generation to obtained more appropriate sample distributions. For AIL, we use the DAGGER approach [14] to train agents represented by neural networks. We use the same network architecture when we apply deep reinforcement learning. More speci\ufb01cally, we train deep Q-networks [12] to solve the Racetrack benchmark. We compare the resulting agents considering three di\ufb00erent aspects: the success rate, the quality of the resulting action sequences, and the relative number of optimal and fatal decisions. Amongst other things, we \ufb01nd that, even though it is based on optimal training data, imitation learning leads to unsafe policies, much more risky than those found by RL. Upon closer inspection, it turns out that this apparent contradiction actually has an intuitive explanation in terms of the nature of the application and the di\ufb00erent learning methods: to minimize time to goal, optimal Tracking the Race Between Deep RL and Imitation Learning 3 decisions navigate very closely to dangerous states. This works well when taking optimal decisions throughout \u2013 but is brittle to (and thus fatal in the presence of) even small divergences as are to be expected from a learned policy. We believe that this characteristic might carry over to many other applications beyond Racetrack. The outline of our paper is the following: We \ufb01rst introduce the Racetrack domain (Section 2). Then we introduce the DAGGER framework and deep Qlearning (Section 3), before we describe our application to the Racetrack domain (Section 4). In Section 5, we present our experiments and \ufb01ndings. We \ufb01nally draw a conclusion and present future work in Section 6. This report is an extended version of the conference paper by Gros et al. [6]. 2 Racetrack Racetrack has been used as a benchmark in the context of planning [3, 13] and reinforcement learning [2, 19]. It can be played on di\ufb00erent maps. The example used throughout the paper is displayed in Figure 1. 2.1 The Racetrack Game At the beginning of the game, a car is placed randomly at one of the discrete positions on the start line (in purple) with zero velocity. In every step it can speed up, hold the velocity or slow down in x and/or y dimension. Then, the car moves in a straight line with the new velocity from the old position to a new one, where we discretize the maps into cells. The game is lost when the car crashes, which is the case when either (1) the new position itself is a wall position or outside the map, or (2) the straight line between the old and new position intersects with a wall, i.e. the car drives through a wall on its way to the new position. The game is won when the car either stops at or drives through the goal line (in green). Fig. 1. Example of a Racetrack map: goal line is green, start line is purple. 4 T. Gros et al. 2.2 Markov Decision Process Given a Racetrack map, the game can be modeled as a Markov decision process. States. The current state is uniquely de\ufb01ned by the position p = (x, y) and the velocity v = (vx, vy). Actions. Actions represent the acceleration a. As the car can be accelerated with values {\u22121, 0, 1} in the x and in the y dimension, there are exactly 32 = 9 di\ufb00erent actions available in every state. Transitions. We assume a wet road, so with a chance of 0.1, the acceleration cannot be applied, i.e. a = (0, 0). Otherwise, with probability 0.9, the acceleration is as selected by the action. The new velocity v\u2032 = (vx \u2032, vy \u2032) is given by the sum of the acceleration a = (ax, ay) and the current velocity. The new position p\u2032 = (x\u2032, y\u2032) is given by adding v\u2032 to the current position, i.e. vx \u2032 = vx + ax, x\u2032 = x + vx, vy \u2032 = vy + ay, y\u2032 = y + vy. To de\ufb01ne several properties, we use a discretization of transitions of the MDP similar to the one of Bonet & Ge\ufb00ner [3]. The corresponding driving trajectory is a sequence of visited positions T = \u27e8(x0, y0) , (x1, y1) , . . . , (xn\u22121, yn\u22121) , (xn, yn)\u27e9 , such that T = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u27e8(x, y)\u27e9 if vx \u2032 = 0 and vy \u2032 = 0 (1) \u27e8(x, y) , (x + \u03c3x, y) , (x + 2 \u00b7 \u03c3x, y) . . . , (x\u2032, y\u2032)\u27e9 if vx \u2032 \u0338= 0 and vy \u2032 = 0 (2) \u27e8(x, y) , (x, y + \u03c3y) , (x, y + 2 \u00b7 \u03c3y) . . . , (x\u2032, y\u2032)\u27e9 if vx \u2032 = 0 and vy \u2032 \u0338= 0 (3) \u27e8(x, y) , (x + \u03c3x, \u230ay + my\u2309) , (x + 2 \u00b7 \u03c3x, \u230ay + 2 \u00b7 my\u2309) . . . , (x\u2032, y\u2032)\u27e9 if vx \u2032 \u0338= 0 and vy \u2032 \u0338= 0 and |vx \u2032| \u2265 |vy \u2032| (4) \u27e8(x, y) , (\u230ax + mx\u2309 , y + \u03c3y) , (\u230ax + 2 \u00b7 mx\u2309 , y + 2 \u00b7 \u03c3y) . . . , (x\u2032, y\u2032)\u27e9 if vx \u2032 \u0338= 0 and vy \u2032 \u0338= 0 and |vx \u2032| < |vy \u2032| (5), where \u03c3x = sgn(vx \u2032), \u03c3y = sgn(vy \u2032) and mx = vx \u2032 |vy \u2032|, my = vy \u2032 |vx \u2032| [5]. If either the vertical or horizontal speed is 0, exactly all grid coordinates between p = (x, y) and p\u2032 = (x\u2032, y\u2032) are contained in the trajectory. Otherwise, we consider n equidistant points on the linear interpolation between the two positions and for each one round to the closest position on the map. While in the original discretization n = |vx \u2032| [3], in this model it is given by max (|vx| , |vy|). The former is problematic when having a velocity which moves less into the x than into the y direction, as then only few points will be contained in the trajectory and counterintuitive results may be produced. We consider a transition to be valid, if and only if it does not crash, i.e. no position p \u2208 T is either a wall or outside of the map. A transition is said ",
    "Approaches": "Approaches We consider two di\ufb00erent learning approaches that are based on di\ufb00erent principles. Imitation learning is based on labeled training data, while deep reinforcement learning is based on self-play without prior knowledge. 6 T. Gros et al. 3.1 Imitation Learning We consider both passive and active imitation learning. For passive imitation learning, we use (1) logistic regression (LR) and linear discriminant analysis (LDA) to train linear functions, and (2) stochastic gradient descent to train neural networks. To represent the class of active imitation learning algorithms, we consider DAGGER [14]. DAGGER. Dataset Aggregation (DAGGER) is a meta-algorithm for active imitation learning. The main idea of DAGGER is to mitigate the problem related to the disruption of the independently identical distributed (i.i.d.) assumption in passive imitation learning for sequential decision making. The trained agent is then used to iteratively sample more labeled data to train a new neural network. The algorithm starts with a pre-trained neural network using the following steps: (i) It follows the current action policy to explore the state space. (ii) For every visited state, it uses an expert to \ufb01nd the action that shall be imitated. (iii) It adds the pairs of state and action to the training set, and (iv) trains a new policy on the enlarged data set. Step (i) can be varied via a hyper-parameter \u03b2 \u2208 [0, 1] that sets the ratio of following the current policy or the expert for exploration. With \u03b2 = 0 it follows the current policy only. Step (iii) can be done with any thinkable expert and step (iv) can be done with any training procedure. 3.2 Deep Reinforcement Learning While there are many di\ufb00erent approaches of deep reinforcement learning, e.g. policy-based methods [10] or methods based on Monte Carlo tree search [16, 18], we here focus on the value-based approach of deep Q-learning [12]. Deep Q-learning. Given an MDP, we train an agent which represents a policy such that the expected cumulative reward of the MDP\u2019s episodes is maximized. As (potentially) a race can last forever, the task is a continuing one [19] and the accumulated future reward, the so-called return, of step t is therefore given by Gt = \ufffd\u221e i=t \u03b3i \u00b7 Ri+1, where \u03b3 is a discount factor with \u03b3 \u2208 [0, 1] and we assume that Ri+1 is the reward obtained during the transition from the state Si to state Si+1 for i \u2208 {0, 1, . . .} [19]. For a \ufb01xed state s, an action a, and a policy \u03c0, the action-value q\u03c0(s, a) gives the expected return that is achieved by taking action a in state s and following the policy \u03c0 afterwards, i.e. q\u03c0(s, a) = E\u03c0 [Gt | St = s, At = a] = E\u03c0 \ufffd\ufffd\u221e k=0 \u03b3kRt+k+1 \ufffd\ufffd St = s, At = a \ufffd . We write q\u2217(s, a) for the optimal action-value function that maximizes the expected return. The idea of value-based reinforcement learning methods is to Tracking the Race Between Deep RL and Imitation Learning 7 \ufb01nd an estimate Q(s, a) of the optimal action-value function. Arti\ufb01cial neural networks can express complex non-linear relationships and are able to generalize. Hence, they have become popular for function approximation. We estimate the Q-value function using a neural network with weights \u03b8, a so-called deep Qnetwork (DQN) [11]. We denote the DQN by Q(s, a; \u03b8) and optimize it w.r.t. the target y(s, a; \u03b8) = E [Rt+1 + \u03b3 \u00b7 maxa\u2032 Q(St+1, a\u2032; \u03b8) | St = s, At = a] . (1) Thus, in iteration i the corresponding loss function is L(\u03b8i) = E \ufffd (y(St, At; \u03b8\u2212) \u2212 Q(St, At; \u03b8i))2\ufffd , (2) where \u03b8\u2212 refers to the parameters from some previous iteration, with the socalled \ufb01xed target [12] y(St, At; \u03b8\u2212). We optimize the loss function by stochastic gradient descent using an approximation of \u2207L(\u03b8i) [12]. Furthermore, we apply the idea of experience replay [12]. Instead of directly learning from observations, we store all experience tuples in a data set and sample uniformly from that set. We generate our experience tuples by exploring the state space epsilongreedily, that is, with a chance of 1 \u2212 \u03f5 during the Monte Carlo simulation we follow the policy that is implied by the current network weights and otherwise uniformly choose a random action [12]. In the following, we will use the terms reinforcement learning (RL) and deep reinforcement learning (DRL) interchangeably. 4 Training Racetrack Agents In this section we describe the training process of agents based on active and passive imitation learning as well as deep reinforcement learning. State Encoding. Although a state in the Racetrack problem is uniquely given by the car\u2019s position and velocity, we provide several other features that can be used as state encoding to improve the learning procedure. Instead of giving a complete encoding of the grid to the agent, the following features will be provided. These features correspond well to the idea of Racetrack being a model of autonomous driving control. \u2013 d1, . . . , d8: linear distance to a wall in all directions. These eight distances are distributed equally around the car position and are given analogously to the acceleration, i.e. \u22121, 0 or 1 in both dimensions. \u2013 dgx, dgy: distance to the nearest goal \ufb01eld in x and y dimension, respectively. \u2013 dg: total goal distance, i.e. |dgx| + \ufffd\ufffddgy \ufffd\ufffd. Together with the position and the velocity, this gives us a total of 15 features per state. We use these features for all considered learning approaches. 8 T. Gros et al. Objective Function. The learning methods that we consider rely on two di\ufb00erent objective functions: DRL uses the reward function and imitation learning uses data sets that were optimized w.r.t. the number of steps until the goal is reached. As DRL makes use of discounting (see Section 3.2), the accumulated reward is higher if less steps are taken. Thus, both objective functions serve the same purpose, even though they are not completely equivalent. Note that a direct mapping from the costs used in the planning procedure to the reward structure was not possible. We tested di\ufb00erent reward structures for DRL and found that a negative reward for each single step combined with a positive reward for the goal and a negative reward for invalid states led to very poor convergence properties of the training procedure. No well-performing alternative was found to the reward structure de\ufb01ned in Section 2.2 up to scaling. 4.1 Imitation Learning We want to train agents for all simulation scenarios including those where the car starts at an arbitrary position on the map and visits future positions on the map with di\ufb00erent velocities. Usually, all learning methods are based on the assumption that the data is i.i.d.. Data that is generated via simulation of the Racetrack greatly disrupts this assumption. Thus, we propose di\ufb00erent approaches for data generation to encounter this problem. Data Sets. In the base case, we uniformly sample states and velocities for the simulation scenarios described in Section 2.3. The samples are then labeled by an expert. This expert basically is a Racetrack-tailored version of the A\u2217 algorithm to \ufb01nd an optimal action (there might be more than one), i.e. acceleration, from the current state. We further use additional options that can be set when sampling data to address the problem of decisions depending on each other: \u2013 Complete trajectory (T): If this option is set, all states on the way to the goal are added to the data set instead of only the current state. \u2013 Exhaustive (E): If the exhaustive option is set, all optimal solutions for the speci\ufb01ed state are added to the data set. \u2013 Unique (U): Only states having a unique optimal acceleration are added to the data set. Option E excludes option T due to runtime constraints as the number of optimal trajectories increases exponentially with the trajectory\u2019s length. This leads to a total of 6 di\ufb00erent combinations as displayed in Table 1. The \ufb01rst data set contains uniformly sampled (valid) positions and velocities and combines them with a single optimal action. This explores the whole state space equally. The data sets (2) and (3) di\ufb00er in their starting points. For (2), the car is positioned on the start line, for (3) it might be anywhere on the map. Both sets contain not only the optimal acceleration for this starting state, but for every one visited on the trajectory from there on to the goal. To do both, Tracking the Race Between Deep RL and Imitation Learning 9 Table 1. Racetrack con\ufb01gurations used to create our data sets. No ID Description RS RV T E U (1) RS-RV Uniform sample from all positions on map and all possible velocities. \u0013 \u0013 \u0017 \u0017 \u0017 (2) NS-ZV-T Uniform sample from all positions on the start line; combined with zero velocity. All states that were visited on the optimal trajectory to the goal line are included in the data set. \u0017 \u0017 \u0013 \u0017 \u0017 (3) RS-ZV-T Uniform sample from all positions on the map; combined with zero velocity. All states that were visited on the optimal trajectory to the goal line are included in the data set. \u0013 \u0017 \u0013 \u0017 \u0017 (4) RS-RV-T Uniform sample from all positions on the map and all possible velocities. All states visited on the optimal trajectory to the goal line are included in the data set. \u0013 \u0013 \u0013 \u0017 \u0017 (5) RS-RV-E Uniform sample from all positions on the map and all possible velocities. All optimal actions for that state are included in the data set. \u0013 \u0013 \u0017 \u0013 \u0017 (6) RS-RV-U Uniform sample from all positions on the map and all possible velocities. Only such states that have a unique optimal action are included in the data set. \u0013 \u0013 \u0017 \u0017 \u0013 uniformly sample through the state space and take into account the trajectories, (4) starts with a random position and a random velocity but still collects the whole trace. The data set (5) includes all optimal solutions instead of just one. Apart from that, (5) is similar to set (1). (6) only includes entries that have a unique optimal next action. For each learning method, we train several instances; at least one on each data set. Each data set consists of approximately 105 entries. Passive Imitation Learning Linear Predictors. While deep learning clearly is more powerful than linear learning, linear classi\ufb01ers have the advantage that their decisions are more transparent. We use the package sklearn to apply both Linear Discriminant Analysis (LDA) and Logistic Regression (LR). Together with the six combinations of data sets, this gives 12 di\ufb00erent agents. Neural Networks. We use the PyTorch package to train neural networks [8]. We repeatedly iterate over the labeled training data. We use the MSE as loss function. As neural networks tend to over\ufb01t when the training iterates over 10 T. Gros et al. the training data too often, we store the neural network after every iteration. We experimentally found that a maximum iteration number of 20 is more than su\ufb03cient. As we again use every 6 data sets, this gives us a total of 120 agents. As explained in Section 2, a state is represented by 15 features, which gives us the input size of the network. There are 9 possible actions. As we do not process the game via an image but through prede\ufb01ned features, we only use fully connected layers. More sophisticated network structures are only needed for complex inputs such as images. For a fair comparison, we use the same network size for all methods. We use two hidden layers of size 64, resulting in a network structure of 15 \u00d7 64 \u00d7 64 \u00d7 9. Active Imitation Learning DAGGER. In the case of active imitation learning, we applied DAGGER using \u03b2 = 0 for all iterations, i.e. after the pre-training we followed the trained agent without listening to the expert for exploration. To have a fair comparison, DAGGER has the same number of samples as PIL, i.e. 105. Still, the pre-training is important for sampling within the \ufb01rst iteration of the algorithm, but the main idea is to generate further entries that are more important for the training of the agent. Thus, we pre-trained the agent on each of our data sets and then additionally allowed DAGGER to add 105 samples. We split these 105 samples into 20 iterations. The neural network was trained by performing eight iterations over the data set. Our experiments with the networks showed that this is the best trade-o\ufb00 between over- and under-\ufb01tting. Again, we store the trained agents after every iteration, giving us a total of 120 agents for the DAGGER method. 4.2 Deep Reinforcement Learning Deep Q-learning. In contrast to imitation learning, reinforcement learning is not based on data sets and thus is not applied to any of the data sets given in Table 1. Training is done by self-play only; the Racetrack agent chooses its actions using a neural network and applies them to the environment. After every move, the next state (given by the features), a reward as de\ufb01ned in Section 2 that was achieved for the move, as well as the information whether the episode is terminated are returned to the agent. The agent then uses the loss, de\ufb01ned again by the MSE function, between the accumulated reward and the expected return to correct the weight of the network. All imitation learning agents were trained with 105 (new) samples using the same network structure. Therefore, we here restrict the agents to (1) 105 entries in the replay bu\ufb00er, i.e. the maximal number of entries an agent can learn from at the same time, and (2) 105 episodes that the agent can play at all. The neural network is not pre-trained but initiated randomly. To have a trade-o\ufb00 between exploration and exploitation, our agent acts \u03f5greedy, i.e. with a probability of \u03f5 it chooses a random acceleration instead of ",
    "Results": "Results For evaluation, we consider all possible combinations given by the simulation parameters as described in Section 2.3. In total, it results in 6 di\ufb00erent simulation settings on which we compare the trained agents. These settings are given in Table 3. The combinations with NS and RV are not considered, as they include more starting states where a crash is inevitable than solvable ones. In the sequel, for each learning method we present the best-performing parameter combination of all those that we tested. We investigate three aspects of 12 T. Gros et al. Fig. 2. Training progress of the RL agent. The left graph shows the RS-N mode, the right one displays NS-D. The right plot further displays a temporarily decrease of the return, which is not uncommon during training. the behavior of the resulting agents: the success rate, the quality of the resulting action sequences, and the relative number of optimal and fatal decisions. Table 3. Con\ufb01gurations on which we evaluate the agents. No ID Description RS RV D 1 NS-ZV-D Starting on a random position on the start line with zero velocity using the deterministic simulation \u0017 \u0017 \u0013 2 NS-ZV-N Starting on a random position on the start line with zero velocity using the noisy simulation \u0017 \u0017 \u0017 3 RS-ZV-D Starting on a random position on the map with zero velocity using the deterministic simulation \u0013 \u0017 \u0013 4 RS-ZV-N Starting on a random position on the map with zero velocity using the noisy simulation \u0013 \u0017 \u0017 5 RS-RV-D Starting on a random position on the map with a random velocity using the deterministic simulation \u0013 \u0013 \u0013 6 RS-RV-N Starting on a random position on the map with a random velocity using the noisy simulation \u0013 \u0013 \u0017 5.1 Success Rate We \ufb01rst compare how often the agents win a game, i.e. reach the goal, or loose, i.e. crash into a wall. We limit the game to 1000 steps. If an agent then neither Tracking the Race Between Deep RL and Imitation Learning 13 0 2000 4000 6000 8000 10000 # Number of Episodes RL DAGGER NN LDA LR 99.27% 64.24% 47.61% 40.16% 51.22% 0.73% 35.76% 52.39% 59.84% 48.78% 0.0% 0.0% 0.0% 0.0% 0.0% NS_ZV_N Win Loose Timeout 0 2000 4000 6000 8000 10000 # Number of Episodes RL DAGGER NN LDA LR 100.0% 92.37% 57.1% 24.32% 49.51% 0.0% 7.63% 42.9% 75.68% 50.49% 0.0% 0.0% 0.0% 0.0% 0.0% RS_ZV_D 0 2000 4000 6000 8000 10000 # Number of Episodes RL DAGGER NN LDA LR 99.28% 76.67% 47.79% 22.86% 46.04% 0.72% 23.33% 52.21% 77.14% 53.96% 0.0% 0.0% 0.0% 0.0% 0.0% RS_ZV_N 0 2000 4000 6000 8000 10000 # Number of Episodes RL DAGGER NN LDA LR 65.05% 60.04% 42.95% 16.02% 38.02% 34.95% 39.96% 57.05% 83.94% 61.98% 0.0% 0.0% 0.0% 0.04% 0.0% RS_RV_D 0 2000 4000 6000 8000 10000 # Number of Episodes RL DAGGER NN LDA LR 60.09% 44.96% 33.81% 15.62% 31.47% 39.91% 55.04% 66.19% 84.38% 68.53% 0.0% 0.0% 0.0% 0.0% 0.0% RS_RV_N Fig. 3. Success rate results for all classes of examined agents. 14 T. Gros et al. succeeded nor failed we count the episode as timed out. We compare the agents on 104 simulation runs. For each single run of the simulation, all agents start in the same initial state. The results can be found in Figure 3. We omitted the plot for NS-ZV-D, as all of the agents had 100% winning rate. The linear agents perform worst. Especially with random starting points and velocities, they fail to reach the goal. DAGGER outperforms the passive imitation learning agents. This is not surprising, as it has been designed to cope with sequential decision making. Throughout all settings, the DRL agents perform best. They clearly outperform DAGGER, reaching the goal more than 1.5 times more often in the NS-ZV-N setting. 5.2 Quality of Action Sequences We illustrate results for the quality of the chosen action sequences in Figure 4. The left plot gives the cumulative reward reached by the agents averaged over RS_ZV_N RS_RV_N RS_RV_D RS_ZV_D NS_ZV_D NS_ZV_N Simulation Setting 20 0 20 40 60 80 Return RS_ZV_N RS_RV_N RS_RV_D RS_ZV_D NS_ZV_D NS_ZV_N Simulation Setting 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 Steps RL LDA LR NN DAGGER Fig. 4. Average reward (left) and average number of needed steps (right) for all classes of agents. all runs (also over those that are not successful). DRL clearly achieves the highest cumulative reward. We remark that the optimal policies computed via A\u2217 give higher cumulative rewards as the goal is reached faster. However, imitation learning achieves lower results on average as it fails more often. The right of Figure 4 shows results for the number of steps needed. When a car crashes, we are not interested in the number of steps taken. Therefore \u2013 in this speci\ufb01c analysis \u2013 we only report on successful runs. They show that \u2013 while reinforcement learning has the most wins and is the best agent considering the Tracking the Race Between Deep RL and Imitation Learning 15 0 500 1000 1500 2000 # Number of Episodes RL DAGGER NN LDA LR 77.9% 92.6% 98.85% 95.15% 96.2% 3.55% 5.3% 0.1% 3.7% 1.65% 18.55% 2.1% 1.05% 1.15% 2.15% NS_ZV_D Optimal Secure Fatal 0 500 1000 1500 2000 # Number of Episodes RL DAGGER NN LDA LR 70.6% 84.9% 95.65% 88.7% 89.85% 1.35% 5.85% 0.65% 3.7% 4.65% 28.05% 9.25% 3.7% 7.6% 5.5% RS_ZV_D 0 500 1000 1500 2000 # Number of Episodes RL DAGGER NN LDA LR 67.85% 84.95% 95.3% 89.7% 91.0% 5.95% 8.1% 1.4% 4.7% 4.65% 26.2% 6.95% 3.3% 5.6% 4.35% RS_RV_D Fig. 5. Quality of selected actions. reward objective \u2013 it is consuming the highest number of steps when reaching the goal. It even takes more steps than linear classi\ufb01ers. 5.3 Quality of Single Action Choices Next we examine whether the agents choose the optimal acceleration, i.e. the acceleration that does not crash and leads to the goal with as few steps as possible, for di\ufb00erent positions and velocities. We distinguish between (1) optimal actions, (2) fatal actions that unavoidably lead to a crash, and (3) secure actions that are neither of the former. We use the same settings as before, except for the ones with noise, which does not make sense when considering optimal actions, i.e. NS-ZV, RS-ZV and RS-RV. ",
    "Discussion": "Discussion We found that passive imitation learning agents perform poorly (see Figure 3) even though they select optimal actions most often. One reason for this is that the data sets from which they learn contain samples that have not been generated by iteratively improving the current policy. Hence, it is not biased towards sequences of dependent decisions leading to good performance. We have observed that DAGGER and in particular DRL sometimes do not select optimal actions, but those with lower risk of hitting a wall. As a result, they need more steps than other approaches before reaching the goal, but the trajectories they use are more secure and they crash less often. This is an interesting insight, as all approaches (including PIL) try to optimize the same objective: reach the goal as soon as possible without hitting a wall. The fact that both, DAGGER and RL have a relatively high number of fatal actions, but not an increased number of losses, leads us to the assumption that these agents avoid states where they might make fatal decisions, even though these states could help reaching the goal faster. Figure 6 illustrates the paths taken by the di\ufb00erent agents for the easiest case (NS-ZV-D) where all policies reach their goal. DRL di\ufb00ers the most from the optimal (black) trajectory, which describes one of the shortest paths to the goal and obtains the maximum cumulative reward. For the harder setting where a starting point is chosen randomly (RS-ZV-D), only DAGGER and DRL make it to the goal, with DRL using signi\ufb01cantly more steps than the optimal agent. In summary, DRL performs surprisingly well. In some aspects, it performs even better than active imitation learning, which is not only considered a state of the art for sequential decision making [7], but \u2013 in contrast to DRL \u2013 even has the chance to bene\ufb01t from expert knowledge. 6 ",
    "Conclusion": "Conclusion We have presented an extensive comparison between di\ufb00erent learning approaches to solve the Racetrack benchmark. Even though we provided optimal decisions during imitation learning, the agents based on deep reinforcement learning outperform those of imitation learning in many aspects. We believe that our observations carry over to other applications, in particular to more complex autonomous vehicle control algorithms. We plan to consider extensions of the Racetrack problem, which include further real-world characteristics of autonomous driving. We believe that, to address the di\ufb03culties we ",
    "References": "References 1. Agostinelli, F., McAleer, S., Shmakov, A., Baldi, P.: Solving the Rubik\u2019s Cube with deep reinforcement learning and search. Nature Machine Intelligence 1(8), 356\u2013363 (2019) 2. Barto, A.G., Bradtke, S.J., Singh, S.P.: Learning to act using real-time dynamic programming. Arti\ufb01cial Intelligence 72(1-2), 81\u2013138 (1995) 18 T. Gros et al. 3. Bonet, B., Ge\ufb00ner, H.: GPT: A tool for planning with uncertainty and partial information. In: Proceedings of the IJCAI Workshop on Planning with Uncertainty and Incomplete Information. pp. 82\u201387 (2001) 4. Gros, T.P., Hermanns, H., Ho\ufb00mann, J., Klauck, M., Steinmetz, M.: Deep statistical model checking. In: Proceedings of the 40th International Conference on Formal Techniques for Distributed Objects, Components, and Systems (FORTE). pp. 96\u2013114. Springer (2020) 5. Gros, T.P., Hermanns, H., Ho\ufb00mann, J., Klauck, M., Steinmetz, M.: Models and Infrastructure used in \u201dDeep Statistical Model Checking\u201d. http://doi.org/10.5281/zenodo.3760098 (2020) 6. Gros, T.P., H\u00a8oller, D., Ho\ufb00mann, J., Wolf, V.: Tracking the race between deep reinforcement learning and imitation learning. In: Proceedings of the 17th International Conference on Quantitative Evaluation of SysTems (QEST). Springer (2020) 7. Judah, K., Fern, A.P., Dietterich, T.G., Tadepalli, P.: Active imitation learning: Formal and practical reductions to i.i.d. learning. Journal of Machine Learning Research 15(120), 4105\u20134143 (2014) 8. Ketkar, N.: Introduction to pytorch. In: Deep learning with Python, pp. 195\u2013208. Springer (2017) 9. Laud, A.D.: Theory and application of reward shaping in reinforcement learning. Tech. rep. (2004) 10. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. In: International conference on machine learning. pp. 1928\u20131937 (2016) 11. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.: Playing Atari with deep reinforcement learning. In: NIPS Deep Learning Workshop (2013) 12. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D.: Human-level control through deep reinforcement learning. Nature 518(7540), 529\u2013533 (2015) 13. Pineda, L.E., Zilberstein, S.: Planning under uncertainty using reduced models: Revisiting determinization. In: Proceedings of the 24th International Conference on Automated Planning and Scheduling (ICAPS). pp. 217\u2013225. AAAI Press (2014) 14. Ross, S., Gordon, G.J., Bagnell, D.: A reduction of imitation learning and structured prediction to no-regret online learning. In: Proceedings of the 14th International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS). JMLR Proceedings, vol. 15, pp. 627\u2013635. JMLR.org (2011) 15. Schaal, S.: Is imitation learning the route to humanoid robots? Trends in cognitive sciences 3(6), 233\u2013242 (1999) 16. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D.: Mastering the game of Go with deep neural networks and tree search. Nature 529, 484\u2013503 (2016) 17. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., Hassabis, D.: A general reinforcement learning algorithm that masters Chess, Shogi, and Go through self-play. Science 362(6419), 1140\u20131144 (2018) Tracking the Race Between Deep RL and Imitation Learning 19 18. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., Hassabis, D.: Mastering the game of Go without human knowledge. Nature 550, 354\u2013359 (2017) 19. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. Adaptive computation and machine learning, The MIT Press, second edn. (2018) ",
    "title": "Tracking the Race Between Deep Reinforcement",
    "paper_info": "Tracking the Race Between Deep Reinforcement\nLearning and Imitation Learning \u2013\nExtended Version\nTimo P. Gros, Daniel H\u00a8oller, J\u00a8org Ho\ufb00mann, and Verena Wolf\nSaarland University, Saarland Informatics Campus, 66123 Saarbr\u00a8ucken, Germany\n{timopgros, hoeller, hoffmann, wolf}@cs.uni-saarland.de\nhttps://mosi.uni-saarland.de, http://fai.cs.uni-saarland.de\nAbstract. Learning-based approaches for solving large sequential deci-\nsion making problems have become popular in recent years. The resulting\nagents perform di\ufb00erently and their characteristics depend on those of\nthe underlying learning approach.\nHere, we consider a benchmark planning problem from the reinforce-\nment learning domain, the Racetrack, to investigate the properties of\nagents derived from di\ufb00erent deep (reinforcement) learning approaches.\nWe compare the performance of deep supervised learning, in particular\nimitation learning, to reinforcement learning for the Racetrack model.\nWe \ufb01nd that imitation learning yields agents that follow more risky\npaths. In contrast, the decisions of deep reinforcement learning are more\nforesighted, i.e., avoid states in which fatal decisions are more likely. Our\nevaluations show that for this sequential decision making problem, deep\nreinforcement learning performs best in many aspects even though for\nimitation learning optimal decisions are considered.\nKeywords: Deep Reinforcement Learning \u00b7 Imitation Learning \u00b7 Race-\ntrack.\n1\nIntroduction\nIn recent years, deep learning (DL) and especially deep reinforcement learning\n(DRL) have been applied with great successes to the task of learning near-\noptimal policies for sequential decision making problems. DRL has been applied\nto various applications such as Atari games [11, 12], Go and Chess [16\u201318], or Ru-\nbic\u2019s cube [1]. It relies on a feedback loop between self-play and the improvement\nof the current strategy by reinforcing decisions that lead to good performance.\nPassive imitation learning (PIL) is another well-known approach to solve se-\nquential decision making problems, where a policy is learned based on training\ndata that is labeled by an expert [15]. An extension of this approach is ac-\ntive imitation learning (AIL), where after an initial phase of passive learning,\nadditional data is iteratively generated by exploring the state space based on\nthe current strategy and subsequent expert labeling [7, 14]. AIL has successfully\narXiv:2008.00766v1  [cs.LG]  3 Aug 2020\n",
    "GPTsummary": "- (1): This paper focuses on comparing the performance of deep supervised learning, in particular, imitation learning and reinforcement learning, for a benchmark planning problem from the reinforcement learning domain, the Racetrack. The goal is to investigate the properties of agents derived from different deep learning approaches.\n \n- (2): Past methods for solving sequential decision-making problems include deep reinforcement learning and imitation learning. However, there is no clear understanding of which approach achieves the best results for a particular task. The approach in this paper is well-motivated because Racetrack is ideally suited for a comparison of different learning approaches and allows for the analysis of agent performance as well as their \"driving characteristics.\"\n \n- (3): The research methodology proposed in this paper includes training different agents for Racetrack using deep reinforcement learning and different methods of imitation learning. The authors suggest different variants of data generation to obtain more appropriate sample distributions. They compare the resulting agents considering three different aspects: the success rate, the quality of the resulting action sequences, and the relative number of optimal and fatal decisions.\n \n- (4): The performance achieved in this paper shows that, even though it is based on optimal training data, imitation learning leads to unsafe policies and deep reinforcement learning performs best in many aspects. The evaluations show that the decisions of deep reinforcement learning are more foresighted, i.e., avoiding states in which fatal decisions are more likely, while imitation learning yields agents that follow more risky paths.\n7. Methods: \n\n- (1): The paper focuses on comparing the performance of deep reinforcement learning and imitation learning on the Racetrack problem. Different agents are trained using deep reinforcement learning and various methods of imitation learning, with different variants of data generation to obtain more appropriate sample distributions. \n\n- (2): The resulting agents are compared based on three aspects: success rate, quality of resulting action sequences, and relative number of optimal and fatal decisions. The evaluation shows that deep reinforcement learning agents perform better than imitation learning agents in most aspects, as the former has more foresight and avoids risky paths. \n\n- (3): Two different learning approaches, i.e., imitation learning and deep reinforcement learning, are employed, with various data sets including uniformly sampled positions and velocities, starting positions, and exhaustive, unique, and complete trajectories. Linear discriminant analysis, logistic regression, and neural networks are implemented for passive imitation learning, while Dataset Aggregation (DAGGER) is used for active imitation learning. Deep Q-learning is used for deep reinforcement learning, and state encoding and objective functions are also described in detail. \n\n- (4): The researchers propose different approaches for data generation and explore the use of different machine learning algorithms for training agents on the Racetrack problem. The resulting comparisons provide insights into the properties of agents derived from different deep learning approaches, and the findings shed light on which approach is more suitable for solving specific tasks.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work focuses on comparing the performance of deep reinforcement learning and imitation learning on the Racetrack benchmark. The aim is to investigate the properties of agents derived from different deep learning approaches. The findings provide insights into the suitability of different approaches for solving specific tasks, in particular in the field of autonomous vehicle control algorithms.\n\n- (2): Innovation point: This work proposes different approaches for data generation and explores the use of different machine learning algorithms for training agents on the Racetrack problem. The findings shed light on which approach is more suitable for solving specific tasks. \nPerformance: The evaluations show that, even when based on optimal training data, imitation learning leads to unsafe policies, and deep reinforcement learning performs best in many aspects. \nWorkload: The methods and experiments proposed in this paper require significant computational resources and data generation efforts.\n\n\n",
    "GPTmethods": "- (1): The paper focuses on comparing the performance of deep reinforcement learning and imitation learning on the Racetrack problem. Different agents are trained using deep reinforcement learning and various methods of imitation learning, with different variants of data generation to obtain more appropriate sample distributions. \n\n- (2): The resulting agents are compared based on three aspects: success rate, quality of resulting action sequences, and relative number of optimal and fatal decisions. The evaluation shows that deep reinforcement learning agents perform better than imitation learning agents in most aspects, as the former has more foresight and avoids risky paths. \n\n- (3): Two different learning approaches, i.e., imitation learning and deep reinforcement learning, are employed, with various data sets including uniformly sampled positions and velocities, starting positions, and exhaustive, unique, and complete trajectories. Linear discriminant analysis, logistic regression, and neural networks are implemented for passive imitation learning, while Dataset Aggregation (DAGGER) is used for active imitation learning. Deep Q-learning is used for deep reinforcement learning, and state encoding and objective functions are also described in detail. \n\n- (4): The researchers propose different approaches for data generation and explore the use of different machine learning algorithms for training agents on the Racetrack problem. The resulting comparisons provide insights into the properties of agents derived from different deep learning approaches, and the findings shed light on which approach is more suitable for solving specific tasks.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work focuses on comparing the performance of deep reinforcement learning and imitation learning on the Racetrack benchmark. The aim is to investigate the properties of agents derived from different deep learning approaches. The findings provide insights into the suitability of different approaches for solving specific tasks, in particular in the field of autonomous vehicle control algorithms.\n\n- (2): Innovation point: This work proposes different approaches for data generation and explores the use of different machine learning algorithms for training agents on the Racetrack problem. The findings shed light on which approach is more suitable for solving specific tasks. \nPerformance: The evaluations show that, even when based on optimal training data, imitation learning leads to unsafe policies, and deep reinforcement learning performs best in many aspects. \nWorkload: The methods and experiments proposed in this paper require significant computational resources and data generation efforts.\n\n\n",
    "GPTconclusion": "- (1): This piece of work focuses on comparing the performance of deep reinforcement learning and imitation learning on the Racetrack benchmark. The aim is to investigate the properties of agents derived from different deep learning approaches. The findings provide insights into the suitability of different approaches for solving specific tasks, in particular in the field of autonomous vehicle control algorithms.\n\n- (2): Innovation point: This work proposes different approaches for data generation and explores the use of different machine learning algorithms for training agents on the Racetrack problem. The findings shed light on which approach is more suitable for solving specific tasks. \nPerformance: The evaluations show that, even when based on optimal training data, imitation learning leads to unsafe policies, and deep reinforcement learning performs best in many aspects. \nWorkload: The methods and experiments proposed in this paper require significant computational resources and data generation efforts.\n\n\n"
}