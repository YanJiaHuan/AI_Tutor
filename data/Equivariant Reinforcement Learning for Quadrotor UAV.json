{
    "Abstract": "Abstract\u2014 This paper presents an equivariant reinforcement learning framework for quadrotor unmanned aerial vehicles. Successful training of reinforcement learning often requires numerous interactions with the environments, which hinders its applicability especially when the available computational resources are limited, or when there is no reliable simulation model. We identi\ufb01ed an equivariance property of the quadrotor dynamics such that the dimension of the state required in the training is reduced by one, thereby improving the sampling ef\ufb01ciency of reinforcement learning substantially. This is illustrated by numerical examples with popular reinforcement learning techniques of TD3 and SAC. I. ",
    "Introduction": "INTRODUCTION Deep reinforcement learning (DRL) algorithms have been successfully applied to optimal control of complex dynamic systems through an interactive learning process. They have made remarkable advancements in various applications such as games [1] or natural language processing [2]. Recently, their application domain has been extended to robotics, including unmanned aerial systems, such as drone racing [3] and payload transportation of quadrotors [4]. Compared to traditional model-based control methods [5], [6], [7], DRLbased control does not require any exact mathematical model to achieve its goals. Prior works in reinforcement learning of quadrotors have mainly focused on introducing DRL-based control strategies for hovering and trajectory tracking tasks [8], [9]. For example, to enhance tracking accuracy, a stochastic policy has been trained in [10] with Proximal Policy Optimization (PPO) for an on-policy method, and Twin Delayed Deep Deterministic Policy Gradient (TD3) has been adopted in [11] to develop a deterministic policy as an off-policy technique. In [12], Deep Deterministic Policy Gradients (DDPG) have been used for autonomous landing on a moving platform, which has been validated by simulation and \ufb02ight experiments. In the study of [13] and [14], the authors have focused on mitigating the reality gap that appears when transferring the policy trained in simulation into the real world, while greatly improving robustness. However, since DRL-based control is a data-driven approach relying on deep neural networks, most of their recent successes have faced the challenge of handling complex and high-dimensional data. This process often requires numerous samples for successful learning, thereby degrading its ef\ufb01ciency in both computation and learning. Beomyeol Yu, Taeyoung Lee, Mechanical and Aerospace Engineering, George Washington University, Washington, DC 20051 {yubeomyeol,tylee}@gwu.edu \u2217This research has been supported in part by NSF under the grants CNS1837382 and CMMI-1760928. \u20d7e1 \u20d7e2 \u20d7e3 \u20d7b1 \u20d7b2 \u20d7b3 x \u2208 R3 R \u2208 SO(3) T1 T2 T3 T4 Fig. 1. Quadrotor Model A popular way to address this issue in deep learning is equivariant neural network, which is part of a broad theme of geometric deep learning [15]. Equivariant models can improve sample ef\ufb01ciency and generalization capability by directly utilizing the geometric relationship between the input and output data, such as in translation, rotation, or permutation. The concept of equivariant learning was \ufb01rst proposed in [16], and it has been actively adopted in computer vision [17]. In reinforcement learning, re\ufb02ectional and rotational equivariance have been utilized in the formulation of homomorphic networks for discrete actions [18], [19]. Another approach has employed an equivariant architecture for vision-based robotic manipulation in Q-learning and actor-critic methods [20]. In this paper, we propose an equivariant framework of reinforcement learning for quadrotor low-level control, which directly maps the state of the quadrotor to motor control signals. Particularly, we identify a rotational symmetry, where the optimal control represented in the body-\ufb01xed frame is invariant under the rotation about the gravity direction. By embedding this particular structure in an actor-critic architecture, the dimension of the sample trajectories used in the training is reduced by one, thereby improving the sampling ef\ufb01ciency substantially. Data ef\ufb01ciency is particularly important in aerial robotics with a large-dimensional continuous state-action space. Further, as quadrotors are inherently unstable, it is critical to safely complete learning with a minimal number of trials. We compare our agents trained with the proposed equivariant framework with nonequivariant counterparts in TD3 and SAC, to show signi\ufb01cant computational advantages. In short, our main contributions are two-fold. First, a data-ef\ufb01cient reinforcement learning scheme is proposed for quadrotors where the dimension of sample trajectories is reduced by one. Second, it is shown that the proposed arXiv:2206.01233v2  [cs.LG]  26 Feb 2023 ",
    "Problem Formulation": "PROBLEM FORMULATION We are interested in solving the problem of quadrotor lowlevel control. This section provides a theoretical background in quadrotor dynamics and reinforcement learning. A. Quadrotor Dynamics Consider a quadrotor unmanned aerial vehicle, illustrated at Fig. 1. Let {\u20d7e1,\u20d7e2,\u20d7e3} be the axes of the inertial frame, where the third axis \u20d7e3 is aligned along the gravity pointing downward. And let {\u20d7b1,\u20d7b2,\u20d7b3} be the body-\ufb01xed frame located at the mass center of the quadrotor. The \ufb01rst two axes are aligned toward the center of the corresponding rotors, such that the third axis points downward when hovering. The position and the velocity of the quadrotor in the inertial frame are denoted by x \u2208 R3 and v \u2208 R3, respectively. The attitude is de\ufb01ned by the rotation matrix R \u2208 SO(3) = {R \u2208 R3\u00d73 | RT R = I3\u00d73, det[R] = 1}, which is the linear transformation of the representation of a vector from the body-\ufb01xed frame to the inertial frame. The angular velocity vector resolved in the body-\ufb01xed frame is \u2126 \u2208 R3. The equations of motion for the quadrotor are given by \u02d9x = v, (1) m\u02d9v = mge3 \u2212 fRe3, (2) \u02d9R = R\u02c6\u2126, (3) J \u02d9\u2126 + \u2126 \u00d7 J\u2126 = M, (4) where the hat map \u02c6\u00b7 : R3 \u2192 so(3) = {S \u2208 R3\u00d73 | ST = \u2212S} is de\ufb01ned by the condition that \u02c6xy = x \u00d7 y and \u02c6x is skew-symmetric for any x, y \u2208 R3. The inverse of the hat map is denoted by the vee map \u2228 : so(3) \u2192 R3 Also, m \u2208 R and J \u2208 R3\u00d73 are the mass, and the inertia matrix of the quadrotor with respect to the body-\ufb01xed frame, respectively, and g \u2208 R is the gravitational acceleration. From the thrust of each motor denoted by (T1, T2, T3, T4), the total thrust f = \ufffd4 i=1 Ti \u2208 R and the total moment M \u2208 R3 resolved in the body-\ufb01xed frame can be computed by \uf8ee \uf8ef\uf8ef\uf8f0 f M1 M2 M3 \uf8f9 \uf8fa\uf8fa\uf8fb = \uf8ee \uf8ef\uf8ef\uf8f0 1 1 1 1 0 \u2212d 0 d d 0 \u2212d 0 c\u03c4f \u2212c\u03c4f c\u03c4f \u2212c\u03c4f \uf8f9 \uf8fa\uf8fa\uf8fb \uf8ee \uf8ef\uf8ef\uf8f0 T1 T2 T3 T4 \uf8f9 \uf8fa\uf8fa\uf8fb , (5) where d \u2208 R is the distance between the center of any rotor and the third body-\ufb01xed axis, and c\u03c4f \u2208 R is a constant relating the thrust and the resulting reactive torque. B. Markov Decision Process Markov decision process (MDP) is an extension of Markov chains augmented by actions and rewards, which describe the choices available for each state and the objective to achieve. Speci\ufb01cally, it is de\ufb01ned by a tuple M = (S, A, R, T , \u03b3), where S is the state space, A is the action space, and R : S\u00d7A \u2192 R is the reward function. Next, T : S \u00d7 A \u2192 P(S) denotes the state transition probability. For example, in a discrete-time setting, it is speci\ufb01ed by P(st+1|st, at), i.e., the distribution of the state at the next time step, for the given state and action at the current step. At each time step t, the agent takes an action at drawn from a policy \u03c0(at|st), which is the distribution of the action conditioned by the state, receives a reward rt, and a next state is determined by the transition probability function. This sequence of state-action pairs is called a trajectory or rollout \u03c4 = (s0, a0, s1, a1, \u00b7 \u00b7 \u00b7 , sT , aT ). The goal of MDP is to identify an optimal policy \u03c0\u2217(ak|sk) that maximizes the expected return Jt = \ufffd\u221e k=0 \u03b3krt+k+1 where \u03b3 \u2208 [0, 1] is a discount factor. Reinforcement learning addresses MDP by constructing a policy iteratively while interacting with the dynamic system. C. Reinforcement Learning for Quadrotor In this paper, the control objective is to \ufb01nd an optimal policy such that for a given desired position xd \u2208 R3, x \u2192 xd as t \u2192 \u221e. This is formulated as MDP as follows. The state and the action of the quadrotor are given by s = (x, v, R, \u2126) \u2208 S = R9 \u00d7 SO(3) and a =(T1, T2, T3, T4) \u2208 A = R4, respectively. The state transition probability is determined by the equations of motion (1)\u2013(3), which can be discretized according to a numerical integration scheme. Next, to achieve the above stabilization objective, the reward function is de\ufb01ned as rt(st, at) = cx(1 \u2212 \u2225e\u2032 xt\u2225) \u2212 cv\u2225vt\u2225 \u2212 c\u2126\u2225\u2126t\u2225 \u2212 ca\u2225at \u2212 at\u22121\u2225. (6) where the constants c are positive weighting factors, and e\u2032 xt = (xt \u2212 xd)/exmax \u2208 R3 is the position error normalized such that \u2225e\u2032 x\u2225 \u2264 1 always. More speci\ufb01cally, it is assumed that the i-th element of x is in the domain of [xdi \u2212 exmax, xdi + exmax] with a prescribed exmax > 0 for i \u2208 {1, 2, 3}, and any rollout is terminated once it is violated. In (6), the \ufb01rst term is to minimize the scaled position error, and the next two terms are to mitigate aggressive motions. The last term is to discourage chattering in control inputs, where it is considered that the prior action at\u22121 is prescribed at the t-th step with the convention of a\u22121 = a0. The presented MDP for the quadrotor can be addressed by any reinforcement learning schemes that can handle continuous state spaces and action spaces, such as Twin Delayed Deep Deterministic Policy Gradient (TD3) [21], and Soft Actor-Critic (SAC) [22]. While these algorithms have been successfully applied in various challenging applications, they often require a massive amount of data to train their agents successfully. This motivates the proposed equivariant reinforcement learning as presented below. III. S1\u2013EQUIVARIANT REINFORCEMENT LEARNING FOR QUADROTOR In this section, we present a symmetry property of the quadrotor and we discuss how it yields an equivariance property to be utilized in enhancing the sampling ef\ufb01ciency of reinforcement learning. \u20d7e2 \u20d7e3 \u03c6 T4 T4 T2 T2 (s, a) (GS(s), GA(a)) \u20d7b2 \u20d7b3 \u20d7b2 \u20d7b3 Fig. 2. A control problem of planar quadrotors To illustrate the key idea, we \ufb01rst consider a planar quadrotor shown in Figure 2, which is con\ufb01ned to the plane spanned by \u20d7e2 and \u20d7e3. Here, the state and action of the quadrotor (on the left) are given by s = (x2, x3, v2, v3, \u03c6, \u02d9\u03c6) \u2208 R6 and a = (T2, T4) \u2208 R2, where \u03c6 and \u02d9\u03c6 are the roll angle and the angular velocity, respectively. If we \ufb02ip the quadrotor horizontally to the right side, more precisely by the state transform GS(s) = (\u2212x2, x3, \u2212v2, v3, \u2212\u03c6, \u2212 \u02d9\u03c6) and the action transform GA(a) = (T2, T4), then both systems exhibit the same dynamic characteristics, except that they are on the opposite side. As such, the optimal action on the left can be transformed to the right side, and vice versa, thereby reducing the domain of the state space to be considered for optimization into half. In other words, the above planar quadrotor is symmetric with respect to a horizontal \ufb02ip, and the symmetry yields certain equivariance properties in the value function and the optimal action, to be exploited for improving sample ef\ufb01ciency. This idea is more formally developed as follows. A. S1\u2013Symmetry of Quadrotor More generally, we show that the quadrotor dynamics presented in Section II is symmetric with respect to the rotation about the vertical axis. Speci\ufb01cally, consider the following group action of S1 = {q \u2208 R2 | \u2225q\u2225 = 1} parameterized by \u03b8 \u2208 (\u2212\u03c0, \u03c0]) 1: GS(s; \u03b8) = (exp(\u03b8\u02c6e3)x, exp(\u03b8\u02c6e3)v, exp(\u03b8\u02c6e3)R, \u2126), (7) GA(a) = a. (8) In other words, the group action g\u03b8(s, a) = (GS(s; \u03b8), GA(a)) corresponds to the rotation of the complete system by the angle \u03b8 about \u20d7e3. In (7) and (8), the angular velocity \u2126 and the action a seem to be unchanged. However, this is because both \u2126 and a are resolved in the body-\ufb01xed frame. For example, the angular velocity is rotated from \u03c9 = R\u2126 into exp(\u03b8\u02c6e3)R\u2126 = exp(\u03b8\u02c6e3)\u03c9 by the group action when perceived with respect to the inertial frame. From now on, the group actions on the state and the action are denoted by the same symbol g\u03b8, i.e., GS(s; \u03b8) = g\u03b8s and GA(a) = g\u03b8a. Next, we show that the quadrotor dynamics is symmetric with respect to the group action g\u03b8. 1To avoid any confusion with the action a of MDP, this is referred to as group action. (s, a) (\u02dcs, \u02dca) g\u03b8 = (GS, GA) \u20d7e1 \u20d7e2 \u03b8 goal b1 b2 \u02dcb1 \u02dcb2 Fig. 3. Illustration of the group action corresponding to the rotation about \u20d7e3 Proposition 1: Let the equations of motion (1)\u2013(4) be consolidated into \u02d9s = F(s, a), (9) for F : S \u00d7 A \u2192 TS, where TS denotes the tangent bundle of the state space. Also, let (\u02dcs, \u02dca) = g\u03b8(s, a) for the group action given by (7) and (8). Then, (\u02dcs, \u02dca) also satis\ufb01es the equations of motion, i.e., \u02d9\u02dcs = F(\u02dcs, \u02dca). (10) Or equivalently, F \u25e6 g\u03b8 = g\u03b8 \u25e6 F, considering that the group action is extended to TS. This corresponds to the equivariance of F with respect to g\u03b8, and it is also stated that the dynamics is symmetric about g\u03b8. Proof: Let g\u03b8(x, v, R, \u2126) = (\u02dcx, \u02dcv, \u02dcR, \u02dc\u2126). First, we have \u02d9\u02dcx = exp(\u03b8\u02c6e3)v = \u02dcv. Similarly, we have m\u02d9\u02dcv = m exp(\u03b8\u02c6e3) \u02d9v. Substituting (2) into the above, m\u02d9\u02dcv = exp(\u03b8\u02c6e3)(mge3 \u2212 fRe3) = mge3 \u2212 f \u02dcRe3, as exp(\u03b8\u02c6e3)e3 = e3 for any \u03b8. These are equivalent to the translational dynamics (1) and (2). Next, for (3), \u02d9\u02dcR = exp(\u03b8\u02c6e3) \u02d9R = exp(\u03b8\u02c6e3)R\u02c6\u2126 = \u02dcR\u02c6\u02dc\u2126. Also, as \u02dc\u2126 = \u2126, it trivially satis\ufb01es (4). This implies that when the state-action trajectory (s(t), a(t)) for t \u2208 [0, T] is the solution of (1)\u2013(4) representing the quadrotor dynamics, then the rotated trajectory g\u03b8(s(t), a(t)) also satis\ufb01es (1)\u2013(4) for any \u03b8 \u2208 S1. This is not surprising as the direction of \u20d7e1 (or \u20d7e2) in the horizontal plane is completely arbitrary in the formulation of the quadrotor dynamics. This further allows us to de\ufb01ne an equivalent class for the state-action trajectories over an interval [0, T]. Let (s(t), a(t)) \u223c (\u02dcs(t), \u02dca(t)) (11) if there exists \u03b8 \u2208 S1 such that (\u02dcs(t), \u02dca(t)) = g\u03b8(s(t), a(t)) for all t \u2208 [0, T]. It is straightforward to show the re\ufb02ectivity, symmetry, and transitivity of the above binary relation, to verify that it is an equivalent relation [23]. For (s, a) \u2208 S\u00d7A, de\ufb01ne its equivalent class as [s, a] = {(\u02dcs, \u02dca) \u2208 S \u00d7 A | (s, a) \u223c (\u02dcs, \u02dca)}. (12) Then, the quadrotor dynamics can be characterized completely on the quotient space of S \u00d7 A by the equivalence relation \u223c, denoted by S \u00d7 A/ \u223c. B. Equivariant Reinforcement Learning Next, we show how the symmetry properties can be exploited in the reinforcement learning. It has been shown that if the reward also satis\ufb01es the symmetry property, the corresponding value function is invariant and the optimal policy is equivariant in a discrete-time setting [20]. Here we establish the correspondent results for deterministic, continuous-time dynamics by extending the continuous reinforcement learning formulated in [24], [25]. This is to formulate an equivariant reinforcement learning framework for the inherent, continuous-time quadrotor dynamics, without resorting to any discretization scheme. For 0 < \u03b3 < 1, let the value function of a policy \u03c0 : S \u2192 A be de\ufb01ned by V\u03c0(t, s(t)) = \ufffd \u221e t \u03b3\u03c4r(s(\u03c4), a(\u03c4))d\u03c4, (13) where the action at any time is de\ufb01ned by the policy as a(t) = \u03c0(s(t)). The objective is to construct the optimal policy \u03c0\u2217(s(t)) maximizing V\u03c0(t, s(t)). When the dynamics and the reward are symmetric with respect to a group action as presented in Proposition 1, the value function and the optimal policy satisfy the following properties. Proposition 2: Consider a continuous-time MDP to maximize the value function (13) under the dynamics (9). Suppose that F : S \u00d7 A \u2192 TS is equivariant and r : S \u00d7 A \u2192 R is invariant with respect to a group action g : S \u00d7 A \u2192 S \u00d7 A, i.e., F \u25e6 g = g \u25e6 F, (14) r \u25e6 g = r. (15) For a given policy \u03c0(s) : S \u2192 A, de\ufb01ne a new policy induced by g as \u02dc\u03c0(\u02dcs) = g\u03c0(g\u22121\u02dcs). (16) Then, the following properties hold: (i) The value function is invariant under the group action, i.e., V\u03c0 = V\u02dc\u03c0 \u25e6 g. (ii) The optimal policy is equivariant under the group action, i.e., \u03c0\u2217 \u25e6 g = g \u25e6 \u03c0\u2217. Proof: Let (s(t), a(t)) be a trajectory of (9), driven by a policy \u03c0(s). Then, according to Proposition 1, (\u02dcs(t), \u02dca(t)) = (gs(t), ga(t)) is another trajectory of (9), where \u02dca = ga = g\u03c0(s) = g\u03c0(g\u22121\u02dcs) = \u02dc\u03c0(\u02dcs). Thus, (\u02dcs(t), \u02dca(t)) is a trajectory of (9) from the transformed policy \u02dc\u03c0. We have V\u02dc\u03c0(t, gs(t)) = \ufffd \u221e t \u03b3\u03c4r(\u02dcs(\u03c4), \u02dca(\u03c4))d\u03c4 = \ufffd \u221e t \u03b3\u03c4r(gs(\u03c4), ga(\u03c4))d\u03c4, which reduces to (13) from (15). This shows (i). Next, split the domain of the integration in (13) into two parts such that V\u03c0(t, s(t)) = \ufffd t+\u2206t t \u03b3\u03c4r(s(\u03c4), a(\u03c4))d\u03c4 + \ufffd \u221e t+\u2206t \u03b3\u03c4r(s(\u03c4), a(\u03c4))d\u03c4 = r(s(t), a(t))\u2206t + V\u03c0(t + \u2206t, s(t + \u2206t)) + o(\u2206t). Here, V\u03c0(t + \u2206t, s(t + \u2206t)) is expanded into V\u03c0(t, s(t)) + \u2202V\u03c0 \u2202t \u2206t + \u2202V\u03c0 \u2202s F(s(t), a(t))\u2206t + o(\u2206t). Substituting this into the above and rearranging \u2212\u2202V\u03c0 \u2202t = r(s(t), a(t)) + \u2202V\u03c0 \u2202s F(s(t), a(t)). (17) as \u2206t \u2192 0. The left-hand side is rewritten as follows. The value function can be reorganized into V\u03c0(t, s(t)) = \u03b3t \ufffd \u221e t \u03b3\u03c4\u2212tr(s(\u03c4), a(\u03c4))d\u03c4, where the integral at the second factor is independent of t. Thus, its derivative with respect to t is given by \u2202V\u03c0 \u2202t = \u03b3t log \u03b3 \ufffd \u221e t \u03b3\u03c4\u2212tr(s(\u03c4), a(\u03c4))d\u03c4 = log \u03b3V\u03c0(t, s(t)). Substituting this back to (17), log \u03b3\u22121V\u03c0(t, s(t)) = r(s(t), a(t)) + \u2202V\u03c0(t, s(t)) \u2202s F(s(t), a(t)). Let V \u2217 be the optimal value function, obtained by the optimal policy \u03c0\u2217. The above yields the following HamiltonJacobi-Bellman equation: log \u03b3\u22121V \u2217(t, s(t)) = max a\u2208A \ufffd r(s, a) + \u2202V \u2217(t, s(t)) \u2202s F(s, a) \ufffd , and the optimal policy is given by \u03c0\u2217(s(t)) = arg max a\u2208A \ufffd r(s, a) + \u2202V \u2217(t, s(t)) \u2202s F(s, a) \ufffd . (18) Therefore, the optimal action at \u02dcs = gs is given by (\u03c0\u2217 \u25e6 g)(s) = \u03c0\u2217(\u02dcs) = arg max \u02dca\u2208A \ufffd r(\u02dcs, \u02dca) + \u2202V \u2217(t, \u02dcs) \u2202\u02dcs F(\u02dcs, \u02dca) \ufffd . ",
    "Experiments": "EXPERIMENTS This section presents a neural network structure that respects the above equivariance property, to be utilized in the proposed equivariant reinforcement learning. Then, we show the detailed implementation and simulation environments for benchmark studies. A. Neural Network Structures In reinforcement learning, neural networks that approximate the optimal policy and the value function are referred to as actor and critic networks, respectively. To impose the invariant properties (20) in the actor and the critic, one can develop group equivariant neural networks with respect to the group action of (7) and (8). However, the most of the existing results in equivariant neural networks deal with inputs of images or discrete actions, and they are not suitable for the presented rotational action on vectors and matrices. Instead, from (20), the approximation of the value function and the policy is performed on the quotient space S \u00d7A/ \u223c. \u22ee \u22ee \u22ee 17-dim 256-dim ReLU 256-dim ReLU 4-dim Tanh [x]1 [x]3 [v]1 [v]2 [v]3 [R]11 [R]33 [\u2126]1 [\u2126]2 [\u2126]3 T1 T2 T3 T4 (a) Actor \u22ee 256-dim ReLU 1-dim Linear 21-dim \u22ee \u22ee \u22ee 256-dim ReLU [x]1 [x]3 [v]1 [v]2 [v]3 [R]11 [R]33 [\u2126]1 [\u2126]2 [\u2126]3 T1 T4 Q([s], a) (b) Critic Fig. 4. The architecture of neural networks: (a) Actor and (b) Critic For the equivalent class [s] of any s \u2208 S, we choose a representative element \u02dcs by [s] = g[\u03b8]s, (21) where [\u03b8] = \u2212atan2(x2, x1). Using this, the equivalent class can be identi\ufb01ed with the above representative element, such that the quotient space is considered as a set of the representative elements. Thus, V (s) = V ([s]), \u03c0\u2217(s) = \u03c0\u2217([s]). (22) As the group action is one-dimensional, this reduces the domain of the value and the policy by one. The particular choice of the rotation angle [\u03b8] ensures that the second element of the position is always zero, i.e., [x]2 = 0 (see Figure 3). As such, it can be simply dropped from the input of the actor and the critic. Speci\ufb01cally, the input to the each network is ([x]1, [x]3, [v], [R], [\u2126]) \u2208 R17. Note that (22) can be adopted to any reinforcement learning scheme. In this paper, we apply it to TD3 and SAC, ",
    "Results": "Results We validated the proposed approach comprehensively through numerical simulations to ensure reliability and stability. We took the vanilla TD3 and SAC algorithms, which do not consider the equivariance property, as the baselines for performance comparison. All experiments were executed for TABLE I HYPERPARAMETERS USED IN BENCHMARK STUDIES Parameter Value Optimizer Adam Actor learning rate 3 \u00b7 10\u22124 Critic learning rate 3 \u00b7 10\u22124 Discount factor, \u03b3 0.99 Replay buffer size 106 Batch size 256 Target smoothing coef\ufb01cient, \u03c4 0.005 Twin Delayed DDPG (TD3) Exploration noise 0.1 Target policy noise 0.2 Policy noise clip 0.5 Target update interval 2 Soft Actor-Critic (SAC) Entropy regularization coef\ufb01cient, \u03b1 Autotuned Target update interval 1 6 random seeds, and the average return was reported once every 5,000 steps without action noise for 30 trajectories. During training and evaluation, the quadrotor started with the random states of each episode at an arbitrary initial location in a 3m \u00d7 3m \u00d7 3m space. The agents were trained without any auxiliary aid technique such as PID controllers or pretrained policies. In (6), the coef\ufb01cient cx is set to 2.0, and the penalizing terms are set cv = 0.15, c\u2126 = 0.2, and ca = 0.03 to improve stability and achieve smooth control. Note that too large penalties prevent the quadrotor from moving toward its target by focusing only on stabilization. The reward is normalized into [0, 1] and rescaled by a factor of 0.1 to ensure convergence. Finally, the discount factor is selected as \u03b3 = 0.99. Figure 6 shows training curves in terms of the average reward, where the blue curves denote the proposed equivariant methods and the green corresponds to the baselines. The solid lines and shaded areas represent the mean value and 2\u03c3 bounds, respectively. Figure 6(a) shows the result of training with TD3 and Figure 6(b) shows the results of SAC. In both cases, the proposed equivariant methods outperform the baselines in terms of the learning speed and convergence. In Figure 6(a), it is also illustrated that the equivariant TD3 exhibits more consistent results than the vanilla TD3 with respect to the random seed variations. Additionally, the equivariant methods exhibit higher rewards than their counterparts over the same time period. In other words, the baselines require more accumulated timesteps to achieve a similar level of reward. This con\ufb01rms that our proposed equivariant framework is more sampleef\ufb01cient, leading to faster convergence. Next, to demonstrate the \ufb02ight performance of the trained policy, the state trajectory and the control inputs for a speci\ufb01c episode are presented in Figure 7, where the top four sub\ufb01gures show the position error ex, velocity error ev, angular velocity error e\u2126, and motor thrusts T. The last sub\ufb01gure presents the nine elements of attitude R. As shown ",
    "Conclusion": "CONCLUSION In this paper, we presented a data-ef\ufb01cient reinforcement learning strategy for quadrotors, by exploiting its symmetry. We identi\ufb01ed S1\u2013symmetry of the quadrotor dynamics, which is utilized in the neural network structures for the actor and the critic to reduce the dimension of the input domain by one. Numerical simulation with two popular reinforcement schemes shows that the equivariance property substantially improves sample ef\ufb01ciency, and the trained policy is reasonable. Future work includes incorporating other symmetry properties of the quadrotor. For example, since the structure of the quadrotor is symmetrical about its third body-\ufb01xed axis, the thrust at the opposite side of the quadrotor can be swapped when it is rotated about the third body-\ufb01xed axis by 180\u25e6. 0 1 2 3 4 5 6 7 Time (sec) -1.5 1.0 0.5 0.0 0.5 1.0 1.5 position error, ex (m) ex1 ex2 ex3 0 1 2 3 4 5 6 7 Time (sec) 0.8 0.4 0.0 0.4 0.8 velocity error, ev (m/s) ev1 ev2 ev3 (a) Position error ex 0 1 2 3 4 5 6 7 Time (sec) -1.5 1.0 0.5 0.0 0.5 1.0 1.5 position error, ex (m) ex1 ex2 ex3 0 1 2 3 4 5 6 7 Time (sec) 0.8 0.4 0.0 0.4 0.8 velocity error, ev (m/s) ev1 ev2 ev3 (b) Velocity error ev 0 1 2 3 4 5 6 7 1.5 1.0 0.5 0.0 0.5 1.0 1.5 position error, ex (m) ex1 ex2 ex3 0 1 2 3 4 5 6 7 0.8 0.4 0.0 0.4 0.8 velocity error, ev (m/s) ev1 ev2 ev3 0 1 2 3 4 5 6 7 Time (sec) 0.4 0.2 0.0 0.2 0.4 0.6 angular velocity error, e  (rad/s) e 1 e 2 e 3 0 1 2 3 4 5 6 7 Time (sec) 1 2 3 4 5 6 7 8 9 motor thrust (N) T1 T2 T3 T4 (c) Angular velocity error e\u2126 0 1 2 3 4 5 6 7 1.5 1.0 0.5 0.0 0.5 1.0 1.5 position error, ex (m) ex1 ex2 ex3 0 1 2 3 4 5 6 7 0.8 0.4 0.0 0.4 0.8 velocity error, ev (m/s) ev1 ev2 ev3 0 1 2 3 4 5 6 7 Time (sec) 0.4 0.2 0.0 0.2 0.4 0.6 angular velocity error, e  (rad/s) e 1 e 2 e 3 0 1 2 3 4 5 6 7 Time (sec) 1 2 3 4 5 6 7 8 9 motor thrust (N) T1 T2 T3 T4 (d) Motor thrust (T1, T2, T3, T4) 0 2 4 6 8 10 12 14 0.97 0.98 0.99 1.00 Response 0 2 4 6 8 10 12 14 0.15 0.10 0.05 0.00 0 2 4 6 8 10 12 14 0.10 0.05 0.00 0.05 0 2 4 6 8 10 12 14 0.00 0.05 0.10 0.15 0.20 0 2 4 6 8 10 12 14 0.97 0.98 0.99 1.00 0 2 4 6 8 10 12 14 0.05 0.00 0.05 0.10 0 2 4 6 8 10 12 14 Time (sec) 0.05 0.01 0.03 0.07 0 2 4 6 8 10 12 14 Time (sec) 0.10 0.05 0.00 0.05 0 2 4 6 8 10 12 14 Time (sec) 0.990 0.994 0.998 (e) Attitude R Fig. 7. Controlled trajectory for a single episode Also, instead of utilizing the representative element of the equivalent class, the invariance of the value or the policy can be directly imposed via equivariant neural networks. ",
    "References": "REFERENCES [1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [2] E. Choi, D. Hewlett, J. Uszkoreit, I. Polosukhin, A. Lacoste, and J. Berant, \u201cCoarse-to-\ufb01ne question answering for long documents,\u201d in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017, pp. 209\u2013 220. [3] Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, \u201cAutonomous drone racing with deep reinforcement learning,\u201d in International Conference on Intelligent Robots and Systems, 2021, pp. 1205\u2013 1212. [4] S. Belkhale, R. Li, G. Kahn, R. McAllister, R. Calandra, and S. Levine, \u201cModel-based meta-reinforcement learning for \ufb02ight with suspended payloads,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 1471\u20131478, 2021. [5] S. Bouabdallah, A. Noth, and R. Siegwart, \u201cPID vs LQ control techniques applied to an indoor micro quadrotor,\u201d in International Conference on Intelligent Robots and Systems, vol. 3, 2004, pp. 2451\u2013 2456. [6] R. Xu and U. Ozguner, \u201cSliding mode control of a quadrotor helicopter,\u201d in Proceedings of the IEEE Conference on Decision and Control, 2006, pp. 4957\u20134962. [7] T. Lee, M. Leok, and N. H. McClamroch, \u201cGeometric tracking control of a quadrotor UAV on SE(3),\u201d in IEEE conference on decision and control, 2010, pp. 5420\u20135425. [8] C.-H. Pi, K.-C. Hu, S. Cheng, and I.-C. Wu, \u201cLow-level autonomous control and tracking of quadrotor using reinforcement learning,\u201d Control Engineering Practice, vol. 95, p. 104222, 2020. [9] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, \u201cControl of a quadrotor with reinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 2, no. 4, pp. 2096\u20132103, 2017. [10] G. C. Lopes, M. Ferreira, A. da Silva Simoes, and E. L. Colombini, \u201cIntelligent control of a quadrotor with proximal policy optimization reinforcement learning,\u201d in 2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE), 2018, pp. 503\u2013508. [11] M. Shehab, A. Zaghloul, and A. El-Badawy, \u201cLow-level control of a quadrotor using twin delayed deep deterministic policy gradient (TD3),\u201d in International Conference on Electrical Engineering, Computing Science and Automatic Control, 2021, pp. 1\u20136. [12] A. Rodriguez-Ramos, C. Sampedro, H. Bavle, P. De La Puente, and P. Campoy, \u201cA deep reinforcement learning strategy for UAV autonomous landing on a moving platform,\u201d Journal of Intelligent & Robotic Systems, vol. 93, no. 1, pp. 351\u2013366, 2019. [13] A. Molchanov, T. Chen, W. H\u00a8onig, J. A. Preiss, N. Ayanian, and G. S. Sukhatme, \u201cSim-to-(multi)-real: Transfer of low-level robust control policies to multiple quadrotors,\u201d in International Conference on Intelligent Robots and Systems, 2019, pp. 59\u201366. [14] Y. Wang, J. Sun, H. He, and C. Sun, \u201cDeterministic policy gradient with integral compensator for robust quadrotor control,\u201d IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 50, no. 10, pp. 3713\u20133725, 2019. [15] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, \u201cGeometric deep learning: going beyond euclidean data,\u201d IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18\u201342, 2017. [16] T. Cohen and M. Welling, \u201cGroup equivariant convolutional networks,\u201d in International conference on machine learning. PMLR, 2016, pp. 2990\u20132999. [17] M. Weiler and G. Cesa, \u201cGeneral E(2)-equivariant steerable CNNs,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019. [18] E. van der Pol, D. Worrall, H. van Hoof, F. Oliehoek, and M. Welling, \u201cMDP homomorphic networks: Group symmetries in reinforcement learning,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 4199\u20134210, 2020. [19] E. van der Pol, H. van Hoof, F. A. Oliehoek, and M. Welling, \u201cMulti-agent MDP homomorphic networks,\u201d arXiv preprint arXiv:2110.04495, 2021. [20] D. Wang, R. Walters, and R. Platt, \u201cSO(2)-equivariant reinforcement learning,\u201d in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/ forum?id=7F9cOhdvfk [21] S. Fujimoto, H. Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d in International conference on machine learning. PMLR, 2018, pp. 1587\u20131596. [22] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \u201cSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,\u201d in International conference on machine learning. PMLR, 2018, pp. 1861\u20131870. [23] J. L. Kelley, General topology. Courier Dover Publications, 2017. [24] K. Doya, \u201cReinforcement learning in continuous time and space,\u201d Neural computation, vol. 12, no. 1, pp. 219\u2013245, 2000. [25] R. Munos, \u201cA study of reinforcement learning in the continuous case by the means of viscosity solutions,\u201d Machine Learning, vol. 40, no. 3, pp. 265\u2013299, 2000. [26] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, \u201cOpenai gym,\u201d arXiv preprint arXiv:1606.01540, 2016. [27] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Advances in neural information processing systems, vol. 32, 2019. ",
    "title": "Equivariant Reinforcement Learning for Quadrotor UAV",
    "paper_info": "Equivariant Reinforcement Learning for Quadrotor UAV\nBeomyeol Yu and Taeyoung Lee\nAbstract\u2014 This paper presents an equivariant reinforcement\nlearning framework for quadrotor unmanned aerial vehicles.\nSuccessful training of reinforcement learning often requires\nnumerous interactions with the environments, which hinders\nits applicability especially when the available computational\nresources are limited, or when there is no reliable simulation\nmodel. We identi\ufb01ed an equivariance property of the quadrotor\ndynamics such that the dimension of the state required in the\ntraining is reduced by one, thereby improving the sampling\nef\ufb01ciency of reinforcement learning substantially. This is il-\nlustrated by numerical examples with popular reinforcement\nlearning techniques of TD3 and SAC.\nI. INTRODUCTION\nDeep reinforcement learning (DRL) algorithms have been\nsuccessfully applied to optimal control of complex dynamic\nsystems through an interactive learning process. They have\nmade remarkable advancements in various applications such\nas games [1] or natural language processing [2]. Recently,\ntheir application domain has been extended to robotics,\nincluding unmanned aerial systems, such as drone racing [3]\nand payload transportation of quadrotors [4]. Compared to\ntraditional model-based control methods [5], [6], [7], DRL-\nbased control does not require any exact mathematical model\nto achieve its goals.\nPrior works in reinforcement learning of quadrotors have\nmainly focused on introducing DRL-based control strategies\nfor hovering and trajectory tracking tasks [8], [9]. For\nexample, to enhance tracking accuracy, a stochastic policy\nhas been trained in [10] with Proximal Policy Optimization\n(PPO) for an on-policy method, and Twin Delayed Deep De-\nterministic Policy Gradient (TD3) has been adopted in [11]\nto develop a deterministic policy as an off-policy technique.\nIn [12], Deep Deterministic Policy Gradients (DDPG) have\nbeen used for autonomous landing on a moving platform,\nwhich has been validated by simulation and \ufb02ight experi-\nments. In the study of [13] and [14], the authors have focused\non mitigating the reality gap that appears when transferring\nthe policy trained in simulation into the real world, while\ngreatly improving robustness. However, since DRL-based\ncontrol is a data-driven approach relying on deep neural\nnetworks, most of their recent successes have faced the\nchallenge of handling complex and high-dimensional data.\nThis process often requires numerous samples for successful\nlearning, thereby degrading its ef\ufb01ciency in both computation\nand learning.\nBeomyeol\nYu,\nTaeyoung\nLee,\nMechanical\nand\nAerospace\nEn-\ngineering,\nGeorge\nWashington\nUniversity,\nWashington,\nDC\n20051\n{yubeomyeol,tylee}@gwu.edu\n\u2217This research has been supported in part by NSF under the grants CNS-\n1837382 and CMMI-1760928.\n\u20d7e1\n\u20d7e2\n\u20d7e3\n\u20d7b1\n\u20d7b2\n\u20d7b3\nx \u2208 R3\nR \u2208 SO(3)\nT1\nT2\nT3\nT4\nFig. 1.\nQuadrotor Model\nA popular way to address this issue in deep learning\nis equivariant neural network, which is part of a broad\ntheme of geometric deep learning [15]. Equivariant models\ncan improve sample ef\ufb01ciency and generalization capability\nby directly utilizing the geometric relationship between the\ninput and output data, such as in translation, rotation, or\npermutation. The concept of equivariant learning was \ufb01rst\nproposed in [16], and it has been actively adopted in com-\nputer vision [17]. In reinforcement learning, re\ufb02ectional and\nrotational equivariance have been utilized in the formulation\nof homomorphic networks for discrete actions [18], [19].\nAnother approach has employed an equivariant architecture\nfor vision-based robotic manipulation in Q-learning and\nactor-critic methods [20].\nIn this paper, we propose an equivariant framework of\nreinforcement learning for quadrotor low-level control, which\ndirectly maps the state of the quadrotor to motor control sig-\nnals. Particularly, we identify a rotational symmetry, where\nthe optimal control represented in the body-\ufb01xed frame\nis invariant under the rotation about the gravity direction.\nBy embedding this particular structure in an actor-critic\narchitecture, the dimension of the sample trajectories used\nin the training is reduced by one, thereby improving the\nsampling ef\ufb01ciency substantially. Data ef\ufb01ciency is partic-\nularly important in aerial robotics with a large-dimensional\ncontinuous state-action space. Further, as quadrotors are\ninherently unstable, it is critical to safely complete learning\nwith a minimal number of trials. We compare our agents\ntrained with the proposed equivariant framework with non-\nequivariant counterparts in TD3 and SAC, to show signi\ufb01cant\ncomputational advantages.\nIn short, our main contributions are two-fold. First, a\ndata-ef\ufb01cient reinforcement learning scheme is proposed for\nquadrotors where the dimension of sample trajectories is\nreduced by one. Second, it is shown that the proposed\narXiv:2206.01233v2  [cs.LG]  26 Feb 2023\n",
    "GPTsummary": "- (1): This paper addresses the challenges of data efficiency and computational resources limitations of reinforcement learning methods for quadrotor unmanned aerial vehicles.\n\n- (2): Past methods for reinforcement learning for quadrotor UAVs have focused on hovering and trajectory tracking tasks. However, using deep neural networks to handle complex and high-dimensional data requires many samples for successful learning, making it inefficient in computation and learning. Equivariant neural networks have been proposed to address this issue, and this paper proposes an equivariant framework for reinforcement learning for quadrotor UAVs based on the rotational symmetry of quadrotor dynamics. This approach is well-motivated because it reduces the dimension of the sample trajectories required in training, improving sampling efficiency substantially.\n\n- (3): The proposed methodology is an equivariant framework for reinforcement learning that directly maps the state of the quadrotor to motor control signals. The rotational symmetry of the quadrotor dynamics is embedded into an actor-critic architecture, which reduces the dimension of the sample trajectories used in training by one.\n\n- (4): The proposed method achieves significant computational advantages compared to nonequivariant counterparts in TD3 and SAC. The paper demonstrates the performance of the agents trained with the proposed method in numerical examples using popular reinforcement learning techniques of TD3 and SAC. The results of the numerical examples demonstrate that the proposed method significantly improves sampling efficiency and that the dynamics of quadrotor UAVs are statistically invariant under certain symmetry operations, which justifies the use of equivariant networks for reinforcement learning.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in proposing an equivariant framework for reinforcement learning for quadrotor unmanned aerial vehicles based on rotational symmetry to address the challenges of data efficiency and computational resource limitations in deep neural networks.\n\n- (2): Innovation Point: The paper proposes an innovative approach that utilizes the rotational symmetry of quadrotor dynamics to reduce the dimensionality of the input domain by one, thereby improving sampling efficiency significantly. Performance: The results of the numerical examples demonstrate that the proposed method achieves significant computational advantages compared to nonequivariant counterparts in TD3 and SAC. Workload: The workload of this research is relatively high, involving the development of an appropriate framework for reinforcement learning, numerical simulations, and validation of the proposed method.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in proposing an equivariant framework for reinforcement learning for quadrotor unmanned aerial vehicles based on rotational symmetry to address the challenges of data efficiency and computational resource limitations in deep neural networks.\n\n- (2): Innovation Point: The paper proposes an innovative approach that utilizes the rotational symmetry of quadrotor dynamics to reduce the dimensionality of the input domain by one, thereby improving sampling efficiency significantly. Performance: The results of the numerical examples demonstrate that the proposed method achieves significant computational advantages compared to nonequivariant counterparts in TD3 and SAC. Workload: The workload of this research is relatively high, involving the development of an appropriate framework for reinforcement learning, numerical simulations, and validation of the proposed method.\n\n\n"
}