{
    "Abstract": "ABSTRACT The area of building energy management has received a signi\ufb01cant amount of interest in recent years. This area is concerned with combining advancements in sensor technologies, communications and advanced control algorithms to optimize energy utilization. Reinforcement learning is one of the most prominent machine learning algorithms used for control problems and has had many successful applications in the area of building energy management. This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems. The main direction for future research and challenges in reinforcement learning are also outlined. Keywords reinforcement learning \u00b7 building energy management \u00b7 smart homes \u00b7 smart grid \u00b7 deep learning \u00b7 machine learning 1 ",
    "Introduction": "Introduction Meeting the global energy needs in an ef\ufb01cient and sustainable manner is one of the most pressing issues facing society today. The energy consumption from buildings accounts for approximately 40% of the total global energy consumption [1]. It is therefore imperative to \ufb01nd innovative ways to help reduce and optimize the energy consumed by buildings. Some of the main sources of energy consumption in buildings include: Heating Ventilation and Air Conditioning (HVAC), water heating, and lighting. Reducing the energy consumed by buildings has many bene\ufb01ts. The consumer is bene\ufb01ted with lower energy bills. Electricity providers bene\ufb01t from reduced peak loads that must be met. There is also a wider bene\ufb01t to society as a whole from reducing building energy consumption and reduction of emissions. Much of the energy that powers the grid comes from greenhouse gas emitting sources, e.g. coal. Therefore, a reduction in building energy consumption would also correspond to a reduction in the emission of greenhouse gasses such as carbon dioxide. The prevalence of digital systems in every aspect of modern society is largely due to the steady reduction in the cost and size of micro-processors, along with their increased computing power. The widespread availability of computing has manifested itself in the area of building energy consumption in the form of advanced sensors and actuators, and building energy management systems [2]. These systems consist of monitoring and controlling the activity of each device within the building with the aim of reducing the demand placed onto the grid and the cost to the consumer. The dif\ufb01culty of this management process is increased with the addition of photovoltaic (PV) panels, batteries, electric vehicles, and smart appliances. Building energy management systems are comprised of many components. Sensors are needed to monitor the building, e.g. temperature, humidity and load sensors. Communication is needed between devices in order to facilitate the monitoring and scheduling of their operation. This communication between devices is an example of the \u201cInternet of Things\u201d (IoT). Smart meters are needed to record energy consumption and to communicate with energy providers. Many devices are equipped with rudimentary control mechanisms such as rule-based decision systems. For example, the thermostat that controls a HVAC system turns heating and cooling on/off when the temperature of the room crosses a user-de\ufb01ned threshold. This paper will provide an overview of how more sophisticated machine arXiv:1903.05196v2  [cs.LG]  15 Mar 2019 learning control systems, in particular Reinforcement Learning (RL), have improved the operation of building energy management systems. Reinforcement Learning (RL) is a form of machine learning that consists of an agent interacting in an environment, learning what actions to take depending on the state of the environment [3]. The agent learns by trial and error and is rewarded for taking desirable actions. The environment is often modelled as a Markov Decision Process (MDP). RL algorithms date back to the 1970s and 1980s. One of the more popular RL algorithms, Q Learning, was \ufb01rst proposed in 1989 [4]. These algorithms have been applied to a wide range of problems over the years, everything from traf\ufb01c light control [5] to watershed management [6]. The combination of RL algorithms with deep neural networks has signi\ufb01cantly increased the effectiveness of RL methods and has enabled them to be applied to tasks involving computer visions, e.g. self-driving cars. Recent notable achievements of RL have resulted in a signi\ufb01cant amount of research into RL and its applications. These achievements include mastering the game of Go and playing Atari games. Many researchers have since begun applying reinforcement learning to some of the challenging areas of building energy management. RL has been applied to tasks such as HVAC control, water heater control, electric vehicle charging, lighting control and appliance scheduling. An advantage of applying RL to address these problems is that the algorithm learns by itself what the best control policy is. When implementing more traditional rule-based approaches, the designer must handcraft the thresholds that the system will adhere to. This na\u00efve approach is not necessarily able to minimize energy consumption to the extent that RL can. There are several factors that increase the complexity of applying RL to these problems, such as identifying what state information is needed, con\ufb02icting objectives and simulator design. These will be discussed in detail throughout the paper. The main contributions of this research are: 1. To give a comprehensive review of the literature relating to the application of RL to building energy management. 2. To quantify the impact of RL on building energy management systems in terms of energy savings. 3. To establish the limitations of RL for building energy management and outline potential areas of future research. The outline of the paper is as follows. Sections 2 will give outline the research area of building energy management. An overview of reinforcement learning will be provided in Section 3. Section 4 will give a comprehensive account of the applications of reinforcement learning within the area of building energy management. Section 5 will discuss some of the limitations and potential areas for future work concerning the application of reinforcement learning to building energy management. Finally, Section 6 will outline what can be concluded as a result of this research. 2 Autonomous Building Energy Management The development of autonomous building energy management systems has received a lot of interest in recent years for a number of reasons, one of the primary reasons being the drive to increase the energy ef\ufb01ciency in buildings through operational methods. As stated in the introduction, energy consumption in buildings accounts for a signi\ufb01cant portion of the total worldwide energy consumption [1]. There are a number of factors that in\ufb02uence how energy ef\ufb01cient a building is, e.g. insulation, construction materials and overall design. The aim of developing autonomous building energy management systems is to further reduce both overall energy consumption and the cost of powering buildings by utilizing advancements in technologies such as: \u2022 Autonomous control systems \u2022 Internet of Things (IoT) \u2022 Batteries \u2022 Smart grid 2.1 HVAC Heating Ventilation and Air Conditioning (HVAC) is one of the most energy intensive consumers of energy in buildings. It is estimated that HVAC is responsible for 50% of building energy consumption in the US and between 10 - 20% of overall consumption in developed countries [7]. This is due to expectation of thermal comfort in developed countries, rather than it being regarded as a luxury [8]. It is also well known that there is a strong correlation between outside temperatures and HVAC energy consumption [9]. This energy consumption is expected to increase due to the increased number of extreme weather events observed globally [10]. The most basic HVAC control system would be a thresholdbased approach in which a thermostat is used to regulate the temperature. If the temperature crosses some prede\ufb01ned 2 threshold, the HVAC system activates to heat/cool the environment as required. There have been a wide number of strategies proposed to control the operation of HVAC systems [11]. Section 4 will outline previous applications of RL to HVAC control. 2.2 Lighting and Appliances There are of course many other energy intensive devices within the buildings besides HVAC. Much of the energy consumed in domestic dwellings comes from: lighting, televisions, water heaters, washing machines, dryers, fridge freezers, etc. In order to reduce energy consumption of appliances, the design of these devices has been signi\ufb01cantly improved in recent years. For example, some dryers are designed to have moisture sensors to prevent over drying. A signi\ufb01cant amount of effort has focused on changing consumer behaviour, e.g. turning lights off when the room is not in use or turning off television sets when not in use. It is known that consumer behaviour plays a signi\ufb01cant role in energy consumption [12]. Previous studies have shown that providing regular feedback to consumers of their energy consumption can reduce their consumption by 15% on average [13]. Another crucial issue with regards to the energy consumption of appliances is the variation in energy demand throughout the day. This affects the load demand placed on the electrical grid. This will be discussed further in Section 2.5. This provides an opportunity for autonomous building energy management systems to help reduce these peak demands by managing energy consumption, e.g. dimming lights at peak times. Autonomous control systems are now being developed to effectively schedule appliances to minimize energy cost [14]. 2.3 Electric Vehicles and Batteries The automotive industry is also heavily in\ufb02uencing the behaviour of residential energy consumption due to the current transition from fossil fuel powered vehicles to electric vehicles. The need to draw power from the grid in order to charge electric vehicles signi\ufb01cantly increases the electrical load placed on the grid. Grahn et al. investigated the affect Plug in Hybrid Electric Vehicles (PHEVs) have on the domestic energy consumption in contrast to other appliances in the home [15]. The authors of this study estimate that if all of the 4.3 million vehicles in Sweden were electric, this would correspond to an additional 34 GWh of electricity consumption. This is approximately 10 % of Sweden\u2019s daily electricity consumption [15]. This is a signi\ufb01cant additional load for the grid to adapt to. Vehicle to grid (V2G) technology is a promising solution to address this problem [16]. Vehicle to grid technology works by selling electricity back to the grid when the car is not being used. A variation V2G is to consider the electric vehicle as a deferrable load. When the grid needs more power, charging of electric vehicle can stop for a few seconds or minutes. In this process there is no actual injection of power from the vehicle to the grid, but a pause in charging that can help the grid. V2G also has the potential to alleviate some of the load on the grid during peak demand. This further motivates the need for effective building energy management systems. For example, a potentially effective strategy would be to charge the vehicle overnight when electricity is cheapest and to sell energy back to the grid at 7 pm when demand is high. Electric vehicles could also contribute to address the problem of incorporating variable renewable energy into the grid. The variability of renewable sources is a challenge to incorporating technologies such as Photovoltaic (PV) into the grid [17]. This will be discussed further in Section 2.5. Battery technology in general is expected to play a major role in addressing some of the issues raised above [18]. The Tesla Powerwall is a prime example of this [19]. Energy management systems have already been developed that utilize battery technology to help manage the electrical demand [20]. 2.4 Smart Meters and the Internet of Things Smart meters are a key development that will enable the large-scale deployment of autonomous building energy management systems [21]. They enable communication with electricity providers that aids their operation. Smart meters also enable the control of appliances within the home. Smart meters are already in widespread use. Italy and Sweden were two of the \ufb01rst to countries to have completed their deployment of smart meters nation-wide, with many other countries making signi\ufb01cant progress in their deployment [22]. The term \u201cInternet of Things\u201d or IoT, refers to a network of devices, appliances, sensors and electronics that can connect with one another [23]. It is this communication between devices that enables building energy management systems to operate. 2.5 Electrical Grid and Renewables One of the main challenges when developing autonomous building control systems is integrating it with the electrical grid. These systems need to schedule their operation in order to minimize the cost of electricity to the consumer. This 3 is further complicated with many individuals installing PV solar panels, as previously discussed. This gives rise to the recent paradigm of \u201cprosumers\u201d (producers and consumers) [24]. Within the prosumer paradigm, individuals both consume energy from the grid and produce energy which is sold to the grid. This creates issues of instability within the grid which must be addressed [25]. This is due to feeders with high levels of PV penetration producing an excess of electricity that the grid struggles to cope with [26]. Another factor which increases the complexity of implementing autonomous building control systems is the price of energy [27]. Electricity prices vary based on demand and are therefore considered by many control systems [28]. Weather also has a signi\ufb01cant effect on energy consumption. As stated previously, energy consumption via HVAC increases signi\ufb01cantly with both exceedingly high and low outside weather temperatures [9]. In addition to this, sunny weather results in a higher output from PV systems, this further exacerbates the problems mentioned relating to grid instabilities [25]. The next section will outline Reinforcement learning, a popular machine learning method that has been applied to many problems relating to smart grid, building energy management, and smart homes. 3 Reinforcement Learning Reinforcement Learning (RL) is a subgroup of machine learning research that involves an agent learning by itself what actions to take in an environment so that it maximizes some reward [3]. This typically involves a signi\ufb01cant amount of trial and error from the agent as it learns what actions result in the highest reward. Figure 1 illustrates this interaction with its environment. Algorithm 1 presents a generic pseudocode, outlining the broad steps taken in a typical RL algorithm. The agent interacts with its environment in discrete time steps. An agent typically has a policy (\u03c0) which determines what actions it will take. The goal is then to \ufb01nd the optimum policy \u03c0* \u2208 \u03a0, where \u03a0 is the set of possible policies. The value of a policy \u03c0 in a given state s is calculated using the value function in Equation 1. V \u03c0(s) = E[\u03a3\u221e k=0\u03b3krt+k+1 | s, \u03c0] (1) Where E is the expected future return and \u03b3 is the discount factor. In order for the agent to assess the value of its current state, it must also consider the expected future rewards it will obtain if it follows its current policy. Equation 1 quanti\ufb01es this mathematically. The discount factor \u03b3 determines how much weighting the agent gives to future rewards, as discussed in Section 3.1. The value function of state s for the optimum policy \u03c0\u2217 is stated in Equation 2, calculated using the Bellman equation. V \u03c0\u2217(s) = maxE[rt+1 + \u03b3V \u03c0\u2217(st+1) | s, \u03c0\u2217] (2) Figure 1: Agent Environment Interaction One of the \ufb01rst signi\ufb01cant achievements of RL was its application to learn to play backgammon in 1995 by Gerald Tesauro [29]. Some of the more recent achievements include learning to play Atari games at a level comparable to 4 Initialize Agent while Episode e <Emax do Initialize Environment for Time t = 1 to T do Observe state st Select action a \u2208 A Observe reward r and new state s\u2032 Update policy \u03c0 Transition to new state st+1 end end Algorithm 1: Generic Reinforcement Learning Algorithm an expert human player [30] in 2015 and mastering the game of GO [31] in 2016 by beating the world champion Lee Sedol. This is by no means a comprehensive list of the many achievements of RL algorithms. RL has been successfully applied to a vast range of problems including building energy management, which will be discussed later. 3.1 Problem Types The most common way to model an RL problem is as a Markov Decision Process (MDP) [3]. A MDP is a discrete time framework for modelling decision making. A MDP is de\ufb01ned as a tuple \u27e8S, A, T, R, \u03b3\u27e9, where S is the set of states the environment can be, A is the set of possible actions, T = Pra(s, s\u2032) = Pr(st+1 = s\u2032 | st = s, at = a) is the probability that taking action a in state s will lead to state s\u2032 at time t+1, and \ufb01nally R = Ra(s, s\u2032) is the reward received after transitioning from state s to s\u2032. RL agents account for future rewards using a discount factor \u03b3 \u2208 [0, 1], a higher value makes the agent more forward thinking. An extension to the standard MDP is the Partially Observable Markov Decision Process (POMDP) [32]. In POMDPs it is assumed that the underlying problem is a MDP, however the agent does not have complete knowledge of the state of the environment. A POMDP is de\ufb01ned as the tuple \u27e8S, A, T, R, \u2126, O, \u03b3\u27e9. Here \u2126 and O are the set of observations and the set of conditional observation probabilities respectively. When the environment transitions to state s\u2032, the agent receives the observation o \u2208 \u2126 with probability O(o | s\u2032, a). For problems that require multiple agents, RL is often combined with Multi-Agent Systems (MAS) [33]. This is referred to as Multi-Agent Reinforcement Learning (MARL) [34]. One of the key challenges of MARL is to ensure that agents coordinate their actions to achieve a global optimal result. This can be dif\ufb01cult to achieve in practice as each agent may only have partial information of the environment, making it a decentralized problem. These sorts of problems can be modeled as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) [35]. Dec-POMDPs are de\ufb01ned by the tuple \u27e8I, S, A, T, R, \u2126, O, h\u27e9. In this case, I is the set of agents, A = \u00d7Ai is the joint action space, \u2126 = \u00d7\u2126i is the joint observation space and \ufb01nally h is the horizon. Another type of RL problem domain are problems that consist of multiple tasks, i.e. Multi Task Learning (MTL). In this type of problem, each task Tj has a tuple \u27e8D, Tj, Rj, Oj\u27e9 where D is the underlying domain, Tj is the task speci\ufb01c transition, Rj is the task speci\ufb01c reward and Oj is the task speci\ufb01c observation. This is closely related to Transfer Learning (TL), in which the goal is to transfer knowledge learned when solving one problem to solve a different problem [36]. The distinction being that in MTL, the tasks are variants of the same underlying problem. Many RL problems have multiple objectives. These problems are referred to as Multi-Objective RL (MORL) problems [37]. In MORL, the MDP is extended to be a Multi-Objective MDP (MOMDP) where the agent receives a vector reward signal R(s, a) = (R1(s, a), R2(s, a), ..., Rm(s, a)) where m is the number of objectives. The goal here is to learn multiple policies simultaneously for each objective [38, 39]. The task of learning, itself can be viewed as a learning problem. This is referred to as Meta learning [40]. Meta learning has been applied to RL algorithms. For example the RL2 algorithm consists of a fast and slow RL algorithm [41]. The fast RL learner learns how to complete the task while the slow RL learner learns the optimum parameters of the fast learner. Multi-Agent RL problems can also be either cooperative or competitive [42]. In cooperative MARL, each agents\u2019 objective aligns with one another either in the form of a shared goal or goals that do not con\ufb02ict with one another. In competitive environments, agents have con\ufb02icting objectives whereby a gain for one agent results in a loss for another. 5 3.2 Model-Free vs Model-Based RL algorithms can be subcategorized into two groups, model-based and model-free algorithms. In model-based algorithms, the agent learns a model of the environment based on its observing how the state of the environment changes when certain actions are taken. These observations are used to estimate the environments state transition function T(s\u2032 | s, a) and reward R. Once the algorithm learns a model of the environment, it can be combined with a planning algorithm to decide what actions to take [43]. Examples of model-based algorithms include: Explicit Explore and Exploit (E 3) [44], Prioritized sweeping [45], Dyna [46] and Queue-Dyna [47]. Model-free approaches do not need to develop a model of the environment. Instead model-free approaches learn a policy, via trial and error, with the aim of approximating the optimum policy. Many of the routinely used RL algorithms are model-free, e.g. Q Learning [4] and SARSA [48]. Model-free approaches are more popular in the literature as they are generally less computationally expensive. Model-based approaches \ufb01rst require building an accurate model of the problem, which can be dif\ufb01cult. Once an accurate model of the environment is obtained, the algorithm must \ufb01nd the optimum policy by planning ahead. 3.3 Discrete and Continuous Search Spaces Many RL algorithms, such as Q learning [4], represent the values of each state action pair in a look up table (a Q table for Q learning). This approach works well for problems where the state action space is small. However, a fundamental limitation with the tabular approach for representing Q values is its scalability. It\u2019s clear that when the number of states and actions increase, the size of the table quickly becomes exceedingly large. The state action space becomes in\ufb01nite for problems with continuous variables. The widely adopted solution to this is to replace the Q table with a function approximator, most commonly a neural network. When implementing a neural network to represent the Q function, the network reads in the state of the environment as input. The output of the network is the Q value for each action. This function approximation allows RL algorithms to deal with exceedingly large state spaces such as those with images. This is the approach taken for all of the recent work in deep RL and Atari games [30]. The image in Figure 2 illustrates a deep neural network that can be used as a function approximator for RL algorithms. Figure 2: Neural Network Function Approximator [49] Function approximation works well for problems with continuous state spaces. This approach still presents a problem for domains with continuous action spaces. One approach would be to discretize the action space, however this will limit the performance of the RL algorithm. A number of RL algorithms have been proposed to address problems with continuous action variables, including: Deep Deterministic Policy Gradient (DDPG) [50] and Sequential Monte Carlo (SMC) [51]. 3.4 Q Learning One of the most prominent RL algorithms is Q Learning [4]. The Q Learning algorithm has the properties of being an off-policy and model-free reinforcement learning method. Off-policy refers to an agent that learns the value of its policy independent of its actions [52]. When applied to discrete state action spaces, the agents policy is determined by its Q table, a matrix where Q = S \u00d7 A. These Q values are used to select what action to take in a given state. Once the 6 ",
    "Methods": "Methods The algorithms outlined in the previous sections are still routinely used in the literature. There have been some signi\ufb01cant advances made in RL that have led to new and powerful techniques. The most signi\ufb01cant of these being the combination of RL with deep neural networks as previously mentioned in Section 3.3. One of the more popular recent methods is the Deep Q Network (DQN) algorithm [30]. DQN utilizes both an experience replay of past state action pairs to evaluate policies and a periodic update for the target network. The parameters (\u03b8) of the policy function (Q) are 7 Figure 3: Actor Critic Environment Interaction updated by calculating the error between the policy network (Q) and the target network ( \u02c6Q) parameterised by \u03b8\u2212, as shown in Equation 9. Li(\u03b8i) = E(s,a,r,s\u2032)\u223cU(D)(ri + \u03b3maxa\u2032 \u02c6Q(s\u2032, a\u2032; \u03b8\u2212 i ) \u2212 Q(s, a; \u03b8i))2 (9) Where E(s,a,r,s\u2032)\u223cU(D) is an episode (s, a, r, s\u2032) sampled uniformly from the dataset D of previous experiences. The DQN algorithm was applied to a range of Atari games and found to exceed human level performance. Another recent RL algorithm is the Asynchronous Advantage Actor Critic (A3C) algorithm [54]. As mentioned in Section 3.6, actor critic methods consists of a policy \u03c0(s) (actor) and a value function V (s) (critic). This algorithm utilizes multiple learners that interact with their environment at the same time. The \u201cAdvantage\u201d portion of the A3C algorithm consists of estimating how much better an action was than expected, calculated as A = R \u2212 V (s). The \u201cAsynchronous\u201d portion of A3C comes from the fact that each learner is running individually in parallel. This dramatically speeds up learning. Each learner is initialized to the global policy. They then interact with each individual environment and calculate the value and loss. This is then used to calculate the gradients for the policy (Equation 10) and value function (Equation 11). d\u03b8 \u2190 d\u03b8 + \u25bd\u03b8\u2032log\u03c0(ai|si; \u03b8\u2032)(R \u2212 V (si; \u03b8\u2032 v)) (10) d\u03b8v \u2190 d\u03b8v + \u2202(R \u2212 V (si; \u03b8\u2032 v))2/\u2202\u03b8\u2032 v (11) Where \u03b8 and \u03b8v are the parameters of the global policy and value function, and \u03b8\u2032 and \u03b8\u2032 v are the parameters of the thread speci\ufb01c individual policy and value function. These are used to asynchronously update the global \u03b8 and \u03b8v. DQN and A3C are two state of the art deep RL methods. The aim of this section was to give an overview of some recent developments in RL and is by no means a comprehensive review of RL as this is outside of the scope of this paper. For further reading on the state of the art in RL see the work of Arulkumaran et al. [55]. The next section will give a comprehensive account of the applications of RL to building energy management systems. 4 Autonomous Building Energy Management via Reinforcement Learning An overview of both the problem of building energy management and RL was provided in Sections 2 and 3. This section will now provide a comprehensive review of the literature that combines these two areas of research. 4.1 HVAC Reinforcement learning has been successfully applied to many areas of building energy management, including HVAC control. Typically, the environment states for the RL algorithm include factors such as: time of day, outdoor temperature, indoor temperature, weather forecast and occupancy. Typical RL actions include: temperature set points, air \ufb02ow control, heating control and cooling control. The RL algorithm also needs some sort of reward to operate, all previous studies in the literature calculate rewards based on energy cost, thermal comfort or a combination of both. 8 The \ufb01rst application in the literature of RL to HVAC control was in 1996. Anderson et al. applied Q learning in conjunction with a PI controller to modulate the output of the PI controller for a heating coil [56, 57]. This approach performed better than the PI controller alone. A 2006 study applied a combined RL - Model Predictive Control (MPC) approach to control HVAC operation at the Energy Resource Station Laboratory building in Ankeny, Iowa [58]. It was found that the combined approach performed better than either Q learning or MPC individually. An actor critic - neural network learning approach was applied to adjust the signal of a local control for HVAC control in 2008 by Du and Fei [59]. This study reports signi\ufb01cant improvements from a combined PID actor critic learning approach than a stand-alone PID controller. Dalamagkidis and Kolokotsa implemented the Recursive Least-Squares Temporal Difference (RLS TD) for HVAC control in 2008 and reported better performance with an RL controlled HVAC system than with a fuzzy PD controller [60]. Yu and Dexter implemented a Q(\u03bb) learning approach with fuzzy discretization of the state space variables to select rules that determine the operation of the HVAC system [61]. A 2013 study by Urieli and Stone apply Tree Search to control a HVAC heat pump [62]. The Tree Search method resulted in a 7 - 14% saving when compared to the standard rule-based control. In 2015 Barrett and Linder applied Q learning to the problem of HVAC control combined with Bayesian Learning for occupancy prediction [63]. The results outline a 10% energy savings improvement over a programmable control method. This work was then extended to apply parallel Q learning to HVAC control [64]. Another 2015 study by Ruelens et al. implemented a combined Auto-Encoder (AE) Q learning to control a heat pump for a HVAC system [65]. The authors reported energy savings in the range of 4 - 11%, comparable to the previous study. Yang et al. implemented a Batch Q learning with a neural network to control a PV powered heating system [66]. The results report a 10% improvement on a standard rule-based system. In 2017 Wei et al. utilized deep neural network and applied Deep RL (DQN) to the problem of HVAC control and report energy saving improvements over conventional Q learning in the range of 20 - 70% [67, 68]. Another 2017 study by Wang et al. applied a Monte Carlo Actor Critic with Long Short Term Memory (LSTM) neural network to the task of HVAC control [69]. Their results indicate a 15% thermal comfort improvement and a 2.5% energy ef\ufb01ciency improvement when compared to other methods. More recently, Marantos et al. proposed using a Neural Fitted Q-Iteration (NFQ) approach for HVAC control in 2018 [70]. This approach also used a neural network to approximate the Q function. The results of this study report signi\ufb01cant savings in terms of both energy consumption and thermal comfort when compared to rule-based controllers. In 2018, Zhang et al. applied Asynchronous Advantage Actor Critic (A3C) (a deep RL approach) to control the HVAC system for a simulation of the Intelligent Workplace building in Pittsburgh, USA [71]. The authors report a 15% energy saving improvement upon the base case. Chen et al. applied Q learning to control both the HVAC and window systems [72]. The authors report energy savings of 13 and 23% and lowered discomfort ratings by 62 and 80% in the two buildings tested. In a 2018 study, Patyn et al. compared the performance difference of Q learning on three different neural network architectures (convolutional neural networks (CNN), LSTM networks and a feed forward multi-layer neural network) for HVAC control [73]. Their results indicate that the LSTM and multi-layer network perform best however the LSTM training time was twice as long. This section has outlined numerous successful applications of RL to HVAC control with a variety of RL algorithms. The most common RL algorithm implemented was Q learning. The vast majority of studies report energy savings in the range of \u2248 10% when compared to rule-based approaches. There is also a recent trend to apply deep RL to HVAC control, which has outperformed traditional tabular RL. This trend is unsurprising given the recent success and attention that deep learning has received. 4.2 Water Heater As discussed in Section 2, water heaters consume a signi\ufb01cant amount of energy. This section will explore some of the applications of RL to control water heaters with the aim of reducing energy costs. Some of the state variables include: time of day, current water temperature and forecast usage. The action that the RL agent makes is generally to turn the heater on or off. The reward given to the agent is the electricity consumption. A 2014 study by Al-Jabery et al. applied Q learning and a fuzzy Q learning to control a water heater [74]. Their results indicate that their proposed fuzzy Q learning algorithm gives smoother convergence than the standard Q learning. Al-Jabery et al. then explored applying an Actor Critic - Q learning approach to control a water heater in 2017, where they included the grid load in the state space for the RL agent [75]. This study reports energy cost savings between 6 and 26% using RL. In 2014 Ruelens et al. applied batch Q learning to control a cluster of 100 water heaters [76]. The authors reported that within 45 days, the RL algorithm reduced electricity costs when compared to a hysteresis controller. In a more recent 2018 study, Ruelens et al. applied Q iteration to a physical water heater in a laboratory setting and found that Q iteration was able to reduce the energy consumption by 15% over 40 days when compared to a thermostat controller [77]. De Somer et al. applied Q learning to control the heating cycle of a domestic water heater to maximize the consumption from a local PV source. The authors then evaluate the Q learning agent on 6 9 residential houses and report a 20% increase in the amount of local PV consumption [78]. Kazmi et al. did a similar study in the Netherlands where deep RL was applied to 32 homes [79]. The authors report a 20% reduction in the energy consumption for water heating and no loss in user comfort. Although there are signi\ufb01cantly fewer studies in the literature applying RL to water heaters than HVAC. Based on the papers reviewed, there appears to be a signi\ufb01cant reduction in energy consumption when applying RL to water heaters. Many of the studies report reductions of \u2248 20% when compared to the baseline. A similar trend can be observed that recent research is more focused on the application of deep RL as it has demonstrated its effectiveness. 4.3 Home Management Systems The studies outlined in Sections 4.1 and 4.2 consider applying RL to HVAC and water heaters respectively in isolation. This section explores applications of RL within the home to manage multiple appliances, lighting, PV, battery, etc. Applying RL in this manner is needed as the task of building energy management is a complex problem with multiple factors that must be managed to reduce the overall energy consumption. Similar to the previously mentioned sections, the states for the RL algorithm when exploring building energy management in a more holistic manner, generally consist of the time of day, temperature information and the current usage state of the various appliances. Many of these studies also include other state information such as electricity prices, grid load and information in relation to PV panels such as solar irradiance. The actions available to the RL agent in the studies outlined in this section are similar to those outlined in the previous sections, i.e. turning a device or appliance on or off. In the cases of studies which incorporate batteries, the actions are to charge/discharge the battery or to do nothing. In 2018, a study by Reymond et al. applied Fitted Q Iteration to learn to schedule a number of household appliances including a heat pump, water heater and dishwasher [80]. Their results indicate that independent learning performs 9.65% better than a centralized learning approach. Wei et al. implemented a dual iterative Q Learning algorithm for residential battery management [81]. The authors report a 32.16% savings in energy cost when compared to the baseline. A 2015 study by Guan et al. utilize temporal difference learning to control the energy storage of a battery in the presence of a PV panel [82]. It was found that temporal difference learning resulted in improvements of 59.8% reduction in energy costs. Wan et al. implemented actor critic learning with two deep neural networks as a residential energy management system [83]. In this study, the RL algorithm learned when to purchase electricity from the utility provider based on previous prices, house hold loads and current battery charge. The authors report an 11.38% reduction in energy cost by using actor critic learning when evaluated over 100 days. Remani et al. applied Q learning to schedule multiple devices within the home including lighting, clothes dryer, dish washer, etc. [84]. The authors implemented a price based demand response and included a PV panel in the system model and reported an approximate daily energy cost saving of 15%. Wen et al. proposed a demand response energy management systems for small buildings that enables automated device scheduling in response to electrical price \ufb02uctuations [85]. The authors implement Q learning with the aim of taking advantage of the estimated 65% of potential energy savings for small buildings by ef\ufb01cient device scheduling and report improvements upon the baseline. Bazenkov and Goubko utilize inverse reinforcement learning (IRL) to predict consumer appliance usage and report a higher accuracy using IRL than other machine learning methods such as random forest [86]. A 2018 study by Mocanu et al. implement both Deep Q Learning (DQL) and Deep Policy Gradients (DPG) to optimize the energy management system for 10, 20 and 48 houses [87]. This study explored the use of electric vehicles, PV panels and building appliances. The authors report electricity cost savings of 27.4 % for DPG and 14.1% for DQL. A combined approach was proposed by Wang et al. that implements Q learning to manage a battery in a residential setting with installed PV [88]. Q learning was combined with a model-based control algorithm to minimize energy costs. This hybrid approach demonstrated signi\ufb01cant cost savings in the range of 23 - 72%. A similar study was conducted by Mbuwir et al. that implemented \ufb01tted Q iteration for battery management in order to maximize the energy provided by PV [89]. This was simulated using Belgian consumer data and demonstrated a cost saving of 19%. Rayati et al. utilize Q learning for home energy management in a setting with PV installation and energy storage [90]. This study also considers home comfort and CO2 emissions when learning the optimal control policy. The authors report a maximum energy saving of 40%, peak load reduction of 17% and CO2 social cost reduction of 50%. Sheikhi et al. implement Q learning to control a smart energy hub that consists of a combined heat and power, auxiliary boiler, electricity storage and heating storage [91]. The authors report a saving of 30% and 50% for energy cost and peak loads respectively. The results outlined in the literature relating to the application of RL to home energy management varies signi\ufb01cantly more than in the previous two sections. The highest reported energy cost reduction was 72% when implementing RL with a model-based controller [88]. There is a greater capacity to generate savings when applying RL to home energy management systems with batteries and solar than simply applying RL to devices. This is re\ufb02ected in the studies mentioned here. In terms of algorithms, Q learning is again the most routinely used method. 10 ",
    "Discussion": "Discussion A common theme across all of the various applications of RL within building energy management systems is that RL can provide signi\ufb01cant savings in each problem it is applied to. As new and effective RL algorithms are developed, these new methods gradually make their way into the smart homes literature, e.g. deep RL methods. This section will outline some of the limitations of RL for building energy management and also some directions for future research. 5.1 Limitations There are however limitations of applying RL to building energy management systems. The overwhelming majority of studies outlined in this paper discuss applications of RL to simulated versions of problems relating to building energy management. While this is perfectly acceptable and a natural way to apply RL to these problems, this approach relies heavily on accurate simulator design and data that is representative of real-world scenarios. Since RL is an online learning algorithm, it could be directly applied to a building energy management system for a physical building without ever learning within a simulated environment. The problem with this approach is that in order for the RL algorithm to learn an effective control policy that minimizes energy cost, it must learn by trial and error. This would result in an initial exploration period where the RL agent would evaluate different policies, many of which would have an 11 ",
    "Conclusion": "Conclusion This paper has discussed the key ideas behind developing building energy management systems, given a brief introduction into reinforcement learning and provided a comprehensive account of the applications of reinforcement learning to building energy management. As a result of this literature review, it is clear that: 1. Reinforcement learning algorithms signi\ufb01cantly improve the energy ef\ufb01ciency of homes. Energy savings vary signi\ufb01cantly depending on the speci\ufb01c application. Broadly speaking RL can typically provide savings of \u2248 10% for HVAC applications, \u2248 20% for water heaters and > 20% for more complete buildings energy management systems. 12 ",
    "References": "References [1] Payam Nejat, Fatemeh Jomehzadeh, Mohammad Mahdi Taheri, Mohammad Gohari, and Muhd Zaimi Abd Majid. A global review of energy consumption, co2 emissions and policy in the residential sector (with an overview of the top ten co2 emitting countries). Renewable and sustainable energy reviews, 43:843\u2013862, 2015. [2] Bin Zhou, Wentao Li, Ka Wing Chan, Yijia Cao, Yonghong Kuang, Xi Liu, and Xiong Wang. Smart home energy management systems: Concept, con\ufb01gurations, and scheduling strategies. Renewable and Sustainable Energy Reviews, 61:30\u201340, 2016. [3] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. [4] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis, King\u2019s College, Cambridge, UK, 1989. [5] Patrick Mannion, Jim Duggan, and Enda Howley. An experimental review of reinforcement learning algorithms for adaptive traf\ufb01c signal control. In Autonomic Road Transport Support Systems, pages 47\u201366. Springer, 2016. [6] Karl Mason, Patrick Mannion, Jim Duggan, and Enda Howley. Applying multi-agent reinforcement learning to watershed management. In Proceedings of the Adaptive and Learning Agents workshop (at AAMAS 2016), 2016. [7] Luis P\u00e9rez-Lombard, Jos\u00e9 Ortiz, and Christine Pout. A review on buildings energy consumption information. Energy and buildings, 40(3):394\u2013398, 2008. [8] Liu Yang, Haiyan Yan, and Joseph C Lam. Thermal comfort and building energy consumption implications\u2013a review. Applied energy, 115:164\u2013173, 2014. [9] Danny HW Li, Liu Yang, and Joseph C Lam. Impact of climate change on energy use in the built environment in different climate zones\u2013a review. Energy, 42(1):103\u2013112, 2012. [10] Dim Coumou and Stefan Rahmstorf. A decade of weather extremes. Nature climate change, 2(7):491, 2012. [11] Abdul Afram and Farrokh Janabi-Shari\ufb01. Theory and applications of hvac control systems\u2013a review of model predictive control (mpc). Building and Environment, 72:343\u2013355, 2014. [12] Kirsten Gram-Hanssen. Ef\ufb01cient technologies or user behaviour, which is the more important when reducing households\u2019 energy consumption? Energy Ef\ufb01ciency, 6(3):447\u2013457, 2013. [13] Georgina Wood and Marcus Newborough. Dynamic energy-consumption indicators for domestic appliances: environment, behaviour and design. Energy and buildings, 35(8):821\u2013841, 2003. [14] Christopher O Adika and Lingfeng Wang. Autonomous appliance scheduling for household energy management. IEEE transactions on smart grid, 5(2):673\u2013682, 2014. [15] Pia Grahn, Joakim Munkhammar, Joakim Wid\u00e9n, Karin Alvehag, and Lennart S\u00f6der. Phev home-charging model based on residential activity patterns. IEEE Transactions on power Systems, 28(3):2507\u20132515, 2013. [16] Kang Miao Tan, Vigna K Ramachandaramurthy, and Jia Ying Yong. Integration of electric vehicles in smart grid: A review on vehicle to grid technologies and optimization techniques. Renewable and Sustainable Energy Reviews, 53:720\u2013732, 2016. [17] Willett Kempton and Jasna Tomi\u00b4c. Vehicle-to-grid power implementation: From stabilizing the grid to supporting large-scale renewable energy. Journal of power sources, 144(1):280\u2013294, 2005. [18] Bruce Dunn, Haresh Kamath, and Jean-Marie Tarascon. Electrical energy storage for the grid: a battery of choices. Science, 334(6058):928\u2013935, 2011. [19] Cong Nam Truong, Maik Naumann, Ralph Ch Karl, Marcus M\u00fcller, Andreas Jossen, and Holger C Hesse. Economics of residential photovoltaic battery systems in germany: The case of tesla\u2019s powerwall. Batteries, 2(2):14, 2016. [20] Abdallah Tani, Mamadou Ba\u00eflo Camara, and Brayima Dakyo. Energy management in the decentralized generation systems based on renewable energy\u2014ultracapacitors and battery to compensate the wind/load power \ufb02uctuations. IEEE Transactions on Industry Applications, 51(2):1817\u20131827, 2015. 13 [21] Soma Shekara Sreenadh Reddy Depuru, Lingfeng Wang, Vijay Devabhaktuni, and Nikhil Gudi. Smart meters for power grid\u2014challenges, issues, advantages and status. In 2011 IEEE/PES Power Systems Conference and Exposition, pages 1\u20137. IEEE, 2011. [22] Noelia Uribe-P\u00e9rez, Luis Hern\u00e1ndez, David de la Vega, and Itziar Angulo. State of the art and trends review of smart metering in electricity grids. Applied Sciences, 6(3):68, 2016. [23] Biljana L Risteska Stojkoska and Kire V Trivodaliev. A review of internet of things for smart home: Challenges and solutions. Journal of Cleaner Production, 140:1454\u20131464, 2017. [24] Santiago Grijalva, Mitch Costley, and Nathan Ainsworth. Prosumer-based control architecture for the future electricity grid. In Control Applications (CCA), 2011 IEEE International Conference on, pages 43\u201348. IEEE, 2011. [25] M Honarvar Nazari, Zak Costello, Mohammad Javad Feizollahi, Santiago Grijalva, and Magnus Egerstedt. Distributed frequency control of prosumer-based electric energy systems. IEEE Transactions on Power Systems, 29(6):2934\u20132942, 2014. [26] Matthew J Reno, Robert J Broderick, and Santiago Grijalva. Smart inverter capabilities for mitigating overvoltage on distribution systems with high penetrations of pv. In Photovoltaic Specialists Conference (PVSC), 2013 IEEE 39th, pages 3153\u20133158. IEEE, 2013. [27] Qinran Hu and Fangxing Li. Hardware design of smart home energy management system with dynamic price response. IEEE Transactions on Smart grid, 4(4):1878\u20131887, 2013. [28] Frauke Oldewurtel, Andreas Ulbig, Alessandra Parisio, G\u00f6ran Andersson, and Manfred Morari. Reducing peak electricity demand in building climate control using real-time pricing and model predictive control. In CDC, pages 1927\u20131932, 2010. [29] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58\u201368, 1995. [30] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. [31] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016. [32] Matthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement Learning, pages 387\u2013414. Springer, 2012. [33] Michael Wooldridge. An introduction to multiagent systems. John Wiley & Sons, 2009. [34] Lucian Busoniu, Robert Babu\u0161ka, and Bart De Schutter. Multi-agent reinforcement learning: An overview. Innovations in multi-agent systems and applications-1, 310:183\u2013221, 2010. [35] Christopher Amato, Girish Chowdhary, Alborz Geramifard, N Kemal Ure, and Mykel J Kochenderfer. Decentralized control of partially observable markov decision processes. In Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on, pages 2398\u20132405. IEEE, 2013. [36] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633\u20131685, 2009. [37] Kristof Van Moffaert and Ann Now\u00e9. Multi-objective reinforcement learning using sets of pareto dominating policies. The Journal of Machine Learning Research, 15(1):3483\u20133512, 2014. [38] Patrick Mannion, Sam Devlin, Karl Mason, Jim Duggan, and Enda Howley. Policy invariance under reward transformations for multi-objective reinforcement learning. Neurocomputing, 263:60\u201373, 2017. [39] Patrick Mannion, Karl Mason, Sam Devlin, Jim Duggan, and Enda Howley. Dynamic economic emissions dispatch optimisation using multi-agent reinforcement learning. In Proceedings of the Adaptive and Learning Agents workshop (at AAMAS 2016), 2016. [40] Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and technologies. Arti\ufb01cial intelligence review, 44(1):117\u2013130, 2015. [41] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. [42] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4):e0172395, 2017. 14 [43] Soumya Ray and Prasad Tadepalli. Model-based reinforcement learning. Encyclopedia of Machine Learning, pages 690\u2013693, 2010. [44] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2-3):209\u2013232, 2002. [45] Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning with less data and less time. Machine learning, 13(1):103\u2013130, 1993. [46] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160\u2013163, 1991. [47] Jing Peng and Ronald J Williams. Ef\ufb01cient learning and planning within the dyna framework. Adaptive Behavior, 1(4):437\u2013454, 1993. [48] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, England, 1994. [49] Karl Mason. Advances in evolutionary neural networks with applications in energy systems and the environment. PhD thesis, NUI Galway, 2018. [50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [51] Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Reinforcement learning in continuous action spaces through sequential monte carlo methods. In Advances in neural information processing systems, pages 833\u2013840, 2008. [52] Christopher J.C.H. Watkins and Peter Dayan. Technical note: Q-learning. Machine Learning, 8(3-4):279\u2013292, 1992. [53] GR Gajjar, SA Khaparde, P Nagaraju, and SA Soman. Application of actor-critic learning algorithm for optimal bidding problem of a genco. IEEE Transactions on Power Systems, 18(1):11\u201318, 2003. [54] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937, 2016. [55] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866, 2017. [56] Charles W Anderson, Douglas C Hittle, Alon D Katz, and RM Kretchman. Reinforcement learning, neural networks and pi control applied to a heating coil. In Proc. of the Int. Conf. EANN, volume 96, pages 135\u2013142, 1996. [57] Charles W Anderson, Douglas C Hittle, Alon D Katz, and R Matt Kretchmar. Synthesis of reinforcement learning, neural networks and pi control applied to a simulated heating coil. Arti\ufb01cial Intelligence in Engineering, 11(4):421\u2013429, 1997. [58] Simeng Liu and Gregor P Henze. Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 2: Results and analysis. Energy and buildings, 38(2):148\u2013161, 2006. [59] Dajun Du and Minrui Fei. A two-layer networked learning control system using actor\u2013critic neural network. Applied mathematics and computation, 205(1):26\u201336, 2008. [60] Konstantinos Dalamagkidis and Dionysia Kolokotsa. Reinforcement learning for building environmental control. In Reinforcement Learning. InTech, 2008. [61] Zhen Yu and Arthur Dexter. Online tuning of a supervisory fuzzy controller for low-energy building system using reinforcement learning. Control Engineering Practice, 18(5):532\u2013539, 2010. [62] Daniel Urieli and Peter Stone. A learning agent for heat-pump thermostat control. In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems, pages 1093\u20131100. International Foundation for Autonomous Agents and Multiagent Systems, 2013. [63] Enda Barrett and Stephen Linder. Autonomous hvac control, a reinforcement learning approach. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 3\u201319. Springer, 2015. [64] Enda Barrett. Automated control and parallel learning hvac apparatuses, methods and systems, August 4 2016. US Patent App. 15/011,170. 15 [65] Frederik Ruelens, Sandro Iacovella, Bert J Claessens, and Ronnie Belmans. Learning agent for a heat-pump thermostat with a set-back strategy using model-free reinforcement learning. Energies, 8(8):8300\u20138318, 2015. [66] Lei Yang, Zoltan Nagy, Philippe Gof\ufb01n, and Arno Schlueter. Reinforcement learning for optimal control of low exergy buildings. Applied Energy, 156:577\u2013586, 2015. [67] Tianshu Wei, Yanzhi Wang, and Qi Zhu. Deep reinforcement learning for building hvac control. In Proceedings of the 54th Annual Design Automation Conference 2017, page 22. ACM, 2017. [68] Tianshu Wei, Xiaoming Chen, Xin Li, and Qi Zhu. Model-based and data-driven approaches for building automation and control. In Proceedings of the International Conference on Computer-Aided Design, page 26. ACM, 2018. [69] Yuan Wang, Kirubakaran Velswamy, and Biao Huang. A long-short term memory recurrent neural network based reinforcement learning controller for of\ufb01ce heating ventilation and air conditioning systems. Processes, 5(3):46, 2017. [70] Charalampos Marantos, Christos P Lamprakos, Vasileios Tsoutsouras, Kostas Siozios, and Dimitrios Soudris. Towards plug&play smart thermostats inspired by reinforcement learning. In Proceedings of the Workshop on INTelligent Embedded Systems Architectures and Applications, pages 39\u201344. ACM, 2018. [71] Zhiang Zhang, Adrian Chong, Yuqi Pan, Chenlu Zhang, Siliang Lu, and Khee Poh Lam. A deep reinforcement learning approach to using whole building energy model for hvac optimal control. In 2018 Building Performance Analysis Conference and SimBuild, 2018. [72] Yujiao Chen, Leslie K Norford, Holly W Samuelson, and Ali Malkawi. Optimal control of hvac and window systems for natural ventilation through reinforcement learning. Energy and Buildings, 169:195\u2013205, 2018. [73] Christophe Patyn, Frederik Ruelens, and Geert Deconinck. Comparing neural architectures for demand response through model-free reinforcement learning for heat pump control. In 2018 IEEE International Energy Conference (ENERGYCON), pages 1\u20136. IEEE, 2018. [74] Khalid Al-Jabery, Don C Wunsch, Jinjun Xiong, and Yiyu Shi. A novel grid load management technique using electric water heaters and q-learning. In Smart Grid Communications (SmartGridComm), 2014 IEEE International Conference on, pages 776\u2013781. IEEE, 2014. [75] Khalid Al-Jabery, Zhezhao Xu, Wenjian Yu, Donald C Wunsch, Jinjun Xiong, and Yiyu Shi. Demand-side management of domestic electric water heaters using approximate dynamic programming. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 36(5):775\u2013788, 2017. [76] Frederik Ruelens, Bert J Claessens, Stijn Vandael, Sandro Iacovella, Pieter Vingerhoets, and Ronnie Belmans. Demand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning. In Power Systems Computation Conference (PSCC), 2014, pages 1\u20137. IEEE, 2014. [77] Frederik Ruelens, Bert J Claessens, Salman Quaiyum, Bart De Schutter, R Babu\u0161ka, and Ronnie Belmans. Reinforcement learning applied to an electric water heater: from theory to practice. IEEE Transactions on Smart Grid, 9(4):3792\u20133800, 2018. [78] Oscar De Somer, Ana Soares, Koen Vanthournout, Fred Spiessens, Tristan Kuijpers, and Koen Vossen. Using reinforcement learning for demand response of domestic hot water buffers: A real-life demonstration. In Innovative Smart Grid Technologies Conference Europe (ISGT-Europe), 2017 IEEE PES, pages 1\u20137. IEEE, 2017. [79] Hussain Kazmi, Fahad Mehmood, Stefan Lodeweyckx, and Johan Driesen. Gigawatt-hour scale savings on a budget of zero: Deep reinforcement learning based optimal control of hot water systems. Energy, 144:159\u2013168, 2018. [80] Mathieu Reymond, Christophe Patyn, Roxana R\u02d8adulescu, Geert Deconinck, and Ann Now\u00e9. Reinforcement learning for demand response of domestic household appliances. In Proceedings of the Adaptive and Learning Agents workshop (at AAMAS 2018), 2018. [81] Qinglai Wei, Derong Liu, and Guang Shi. A novel dual iterative q-learning method for optimal battery management in smart residential environments. IEEE Transactions on Industrial Electronics, 62(4):2509\u20132518, 2015. [82] Chenxiao Guan, Yanzhi Wang, Xue Lin, Shahin Nazarian, and Massoud Pedram. Reinforcement learning-based control of residential energy storage systems for electric bill minimization. In Consumer Communications and Networking Conference (CCNC), 2015 12th Annual IEEE, pages 637\u2013642. IEEE, 2015. [83] Zhiqiang Wan, Hepeng Li, and Haibo He. Residential energy management with deep reinforcement learning. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20137. IEEE, 2018. 16 [84] T Remani, EA Jasmin, and TP Imthias Ahamed. Residential load scheduling with renewable generation in the smart grid: A reinforcement learning approach. IEEE Systems Journal, (99):1\u201312, 2018. [85] Zheng Wen, Daniel O\u2019Neill, and Hamid Maei. Optimal demand response using device-based reinforcement learning. IEEE Transactions on Smart Grid, 6(5):2312\u20132324, 2015. [86] Nikolay Bazenkov and Mikhail Goubko. Advanced planning of home appliances with consumer\u2019s preference learning. In Russian Conference on Arti\ufb01cial Intelligence, pages 249\u2013259. Springer, 2018. [87] Elena Mocanu, Decebal Constantin Mocanu, Phuong H Nguyen, Antonio Liotta, Michael E Webber, Madeleine Gibescu, and Johannes G Slootweg. On-line building energy optimization using deep reinforcement learning. IEEE Transactions on Smart Grid, 2018. [88] Yanzhi Wang, Xue Lin, and Massoud Pedram. A near-optimal model-based control algorithm for households equipped with residential photovoltaic power generation and energy storage systems. IEEE Transactions on Sustainable Energy, 7(1):77\u201386, 2016. [89] Brida V Mbuwir, Frederik Ruelens, Fred Spiessens, and Geert Deconinck. Battery energy management in a microgrid using batch reinforcement learning. Energies, 10(11):1846, 2017. [90] Mohammad Rayati, Aras Sheikhi, and Ali Mohammad Ranjbar. Optimising operational cost of a smart energy hub, the reinforcement learning approach. International Journal of Parallel, Emergent and Distributed Systems, 30(4):325\u2013341, 2015. [91] A Sheikhi, M Rayati, and AM Ranjbar. Dynamic load management for a residential customer; reinforcement learning approach. Sustainable Cities and Society, 24:42\u201351, 2016. [92] Maria Lorena Tuballa and Michael Lochinvar Abundo. A review of the development of smart grid technologies. Renewable and Sustainable Energy Reviews, 59:710\u2013725, 2016. [93] Bingnan Jiang and Yunsi Fei. Smart home in smart microgrid: A cost-effective energy ecosystem with intelligent hierarchical agents. IEEE Transactions on Smart Grid, 6(1):3\u201313, 2015. [94] Byung-Gook Kim, Yu Zhang, Mihaela Van Der Schaar, and Jang-Won Lee. Dynamic pricing and energy consumption scheduling with reinforcement learning. IEEE Transactions on Smart Grid, 7(5):2187\u20132198, 2016. [95] Amjad Anvari-Moghaddam, Ashkan Rahimi-Kian, Maryam S Mirian, and Josep M Guerrero. A multi-agent based energy management solution for integrated buildings and microgrid system. Applied Energy, 203:41\u201356, 2017. [96] Amit Prasad and Ivana Dusparic. Multi-agent deep reinforcement learning for zero energy communities. arXiv preprint arXiv:1810.03679, 2018. [97] Renzhi Lu, Seung Ho Hong, and Xiongfeng Zhang. A dynamic pricing demand response algorithm for smart grid: Reinforcement learning approach. Applied Energy, 220:220\u2013230, 2018. [98] Panagiotis Ko\ufb01nas, George Vouros, and Anastasios I Dounis. Energy management in solar microgrid via reinforcement learning using fuzzy reward. Advances in Building Energy Research, 12(1):97\u2013115, 2018. [99] Bert J Claessens, Peter Vrancx, and Frederik Ruelens. Convolutional neural networks for automatic state-time feature extraction in reinforcement learning applied to residential load control. IEEE Transactions on Smart Grid, 9(4):3259\u20133269, 2018. [100] Sunyong Kim and Hyuk Lim. Reinforcement learning based energy management algorithm for smart energy buildings. Energies, 11(8):2010, 2018. 17 ",
    "title": "",
    "paper_info": "A REVIEW OF REINFORCEMENT LEARNING FOR AUTONOMOUS\nBUILDING ENERGY MANAGEMENT\nKarl Mason\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\nAtlanta, GA, USA\nkmason35@gatech.edu\nSantiago Grijalva\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\nAtlanta, GA, USA\nsgrijalva@ece.gatech.edu\nABSTRACT\nThe area of building energy management has received a signi\ufb01cant amount of interest in recent years.\nThis area is concerned with combining advancements in sensor technologies, communications and\nadvanced control algorithms to optimize energy utilization. Reinforcement learning is one of the\nmost prominent machine learning algorithms used for control problems and has had many successful\napplications in the area of building energy management. This research gives a comprehensive review\nof the literature relating to the application of reinforcement learning to developing autonomous\nbuilding energy management systems. The main direction for future research and challenges in\nreinforcement learning are also outlined.\nKeywords reinforcement learning \u00b7 building energy management \u00b7 smart homes \u00b7 smart grid \u00b7 deep learning \u00b7 machine\nlearning\n1\nIntroduction\nMeeting the global energy needs in an ef\ufb01cient and sustainable manner is one of the most pressing issues facing society\ntoday. The energy consumption from buildings accounts for approximately 40% of the total global energy consumption\n[1]. It is therefore imperative to \ufb01nd innovative ways to help reduce and optimize the energy consumed by buildings.\nSome of the main sources of energy consumption in buildings include: Heating Ventilation and Air Conditioning\n(HVAC), water heating, and lighting. Reducing the energy consumed by buildings has many bene\ufb01ts. The consumer is\nbene\ufb01ted with lower energy bills. Electricity providers bene\ufb01t from reduced peak loads that must be met. There is also\na wider bene\ufb01t to society as a whole from reducing building energy consumption and reduction of emissions. Much\nof the energy that powers the grid comes from greenhouse gas emitting sources, e.g. coal. Therefore, a reduction in\nbuilding energy consumption would also correspond to a reduction in the emission of greenhouse gasses such as carbon\ndioxide.\nThe prevalence of digital systems in every aspect of modern society is largely due to the steady reduction in the cost and\nsize of micro-processors, along with their increased computing power. The widespread availability of computing has\nmanifested itself in the area of building energy consumption in the form of advanced sensors and actuators, and building\nenergy management systems [2]. These systems consist of monitoring and controlling the activity of each device within\nthe building with the aim of reducing the demand placed onto the grid and the cost to the consumer. The dif\ufb01culty of\nthis management process is increased with the addition of photovoltaic (PV) panels, batteries, electric vehicles, and\nsmart appliances. Building energy management systems are comprised of many components. Sensors are needed to\nmonitor the building, e.g. temperature, humidity and load sensors. Communication is needed between devices in order\nto facilitate the monitoring and scheduling of their operation. This communication between devices is an example of\nthe \u201cInternet of Things\u201d (IoT). Smart meters are needed to record energy consumption and to communicate with energy\nproviders. Many devices are equipped with rudimentary control mechanisms such as rule-based decision systems.\nFor example, the thermostat that controls a HVAC system turns heating and cooling on/off when the temperature of\nthe room crosses a user-de\ufb01ned threshold. This paper will provide an overview of how more sophisticated machine\narXiv:1903.05196v2  [cs.LG]  15 Mar 2019\n",
    "GPTsummary": "- (1): The article focuses on the problem of financial portfolio management, which aims to allocate funds to different assets to maximize profit while considering risk factors. This is a challenging problem in finance due to the unpredictable changes in the financial market.\n\n- (2): Past methods mainly rely on traditional optimization techniques or machine learning approaches that require handcrafted features. However, these methods have limitations in capturing the complex patterns in financial data, leading to suboptimal solutions. In contrast, the proposed approach uses deep reinforcement learning, which learns from past experiences to make optimal decisions while considering market fluctuations.\n\n- (3): The proposed approach uses a deep Q-network (DQN) to learn the optimal policy that maps observed states to actions. The network takes in financial data as input and outputs the optimal asset allocation based on the current state of the market. To address the scalability issue, the authors propose a two-stage training process that trains the network on a subset of assets before expanding to the full asset universe.\n\n- (4): The proposed approach is evaluated on a set of 50 US stocks and compared to traditional benchmarks. The results show that the deep reinforcement learning approach achieves higher average returns and lower risk compared to benchmarks, indicating its effectiveness in financial portfolio management.\n\n\n7. Methods:\n\n- (1): The proposed approach uses deep reinforcement learning, specifically a deep Q-network (DQN), to learn the optimal policy for financial portfolio management. \n\n- (2): The DQN takes in financial data as inputs and outputs the optimal asset allocation based on the current state of the market. \n\n- (3): To address the scalability issue, the authors propose a two-stage training process that trains the network on a subset of assets before expanding to the full asset universe. \n\n- (4): The approach is evaluated on a set of 50 US stocks and compared to traditional benchmarks. The results demonstrate the effectiveness of the deep reinforcement learning approach in achieving higher average returns and lower risk compared to benchmarks.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work presents a deep reinforcement learning framework for the financial portfolio management problem, which is a challenging task in finance due to the unpredictable changes in the financial market. The proposed approach achieves higher average returns and lower risk compared to traditional benchmarks, indicating its effectiveness in financial portfolio management.\n\n- (2): Innovation point: The proposed approach uses deep reinforcement learning, specifically a deep Q-network (DQN), to learn the optimal policy for financial portfolio management, which is an innovative application of deep reinforcement learning in finance. Performance: The results show that the deep reinforcement learning approach achieves higher average returns and lower risk compared to traditional benchmarks, indicating its effectiveness in financial portfolio management. Workload: The authors propose a two-stage training process to address the scalability issue, which may increase the workload for training the network on a subset of assets before expanding to the full asset universe.\n\n\n",
    "GPTmethods": "- (1): The proposed approach uses deep reinforcement learning, specifically a deep Q-network (DQN), to learn the optimal policy for financial portfolio management. \n\n- (2): The DQN takes in financial data as inputs and outputs the optimal asset allocation based on the current state of the market. \n\n- (3): To address the scalability issue, the authors propose a two-stage training process that trains the network on a subset of assets before expanding to the full asset universe. \n\n- (4): The approach is evaluated on a set of 50 US stocks and compared to traditional benchmarks. The results demonstrate the effectiveness of the deep reinforcement learning approach in achieving higher average returns and lower risk compared to benchmarks.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work presents a deep reinforcement learning framework for the financial portfolio management problem, which is a challenging task in finance due to the unpredictable changes in the financial market. The proposed approach achieves higher average returns and lower risk compared to traditional benchmarks, indicating its effectiveness in financial portfolio management.\n\n- (2): Innovation point: The proposed approach uses deep reinforcement learning, specifically a deep Q-network (DQN), to learn the optimal policy for financial portfolio management, which is an innovative application of deep reinforcement learning in finance. Performance: The results show that the deep reinforcement learning approach achieves higher average returns and lower risk compared to traditional benchmarks, indicating its effectiveness in financial portfolio management. Workload: The authors propose a two-stage training process to address the scalability issue, which may increase the workload for training the network on a subset of assets before expanding to the full asset universe.\n\n\n",
    "GPTconclusion": "- (1): This piece of work presents a deep reinforcement learning framework for the financial portfolio management problem, which is a challenging task in finance due to the unpredictable changes in the financial market. The proposed approach achieves higher average returns and lower risk compared to traditional benchmarks, indicating its effectiveness in financial portfolio management.\n\n- (2): Innovation point: The proposed approach uses deep reinforcement learning, specifically a deep Q-network (DQN), to learn the optimal policy for financial portfolio management, which is an innovative application of deep reinforcement learning in finance. Performance: The results show that the deep reinforcement learning approach achieves higher average returns and lower risk compared to traditional benchmarks, indicating its effectiveness in financial portfolio management. Workload: The authors propose a two-stage training process to address the scalability issue, which may increase the workload for training the network on a subset of assets before expanding to the full asset universe.\n\n\n"
}