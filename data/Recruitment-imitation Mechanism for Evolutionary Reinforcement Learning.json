{
    "Abstract": "Abstract Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample ef\ufb01cient, yet sensitive to hyper-parameters setting and needs ef\ufb01cient exploration; Evolutionary algorithms are stable, but with low sample ef\ufb01ciency; Imitation learning is both sample ef\ufb01cient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-\ufb01tness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-\ufb01tness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability. Reinforcement and imitation learners in this framework can be replaced with any offpolicy actor-critic reinforcement learner or data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM\u2019s components is signi\ufb01cantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using \u2217Corresponding author Email address: lus@jlu.edu.cn (Shuai L\u00a8u) Preprint submitted to Journal of Information Sciences December 16, 2019 arXiv:1912.06310v1  [cs.LG]  13 Dec 2019 ",
    "Introduction": "Introduction An important goal of arti\ufb01cial intelligence is to develop agents with excellent decisionmaking capabilities in complex and uncertain environments. In recent years, the rapid development of deep neural network enables agents based on reinforcement learning methods to perform well in complex control tasks [1] [2] [3]. However, reinforcement learning methods cannot always handle reward sparse problems effectively and their parameters are very sensitive to disturbances. Recent studies have shown that evolutionary algorithms are better at dealing with sparse reward environments [4], and can be employed as an extensible alternative to reinforcement learning in various tasks. When a speci\ufb01c control task has expert data as a guide, imitative learning can also train agents ef\ufb01ciently. Currently, imitative learning-based methods have been successfully applied to drones [5], automated driving [6], and other \ufb01elds. These arti\ufb01cial intelligence algorithms have close relationships with principles of biology [7]. Evolutionary algorithms are inspired by evolution of species. In each generation of the population, there are plenty of random mutations in different individuals. Individuals with positive mutations will be selected by the environment. In imitative learning, individuals with poor \ufb01tness can learn behavior patterns of individuals with good \ufb01tness in a supervised learning way. At the same time, the diversity of genes (parameters) is preserved. The biological basis of reinforcement learning is that individuals learn correct or wrong actions through trials and errors during interactions with the environment. Off-policy reinforcement learning approaches allow individuals to learn and update their policies from historical data [8] [1]. If reinforcement learning individuals are allowed to learn from the entire historical interactions between population and environment, and regularly copy reinforcement learning individuals into the population to participate in the evolution, both the learning process of reinforcement learning and the evolutionary process of evolutionary algorithms can be accelerated [9] 2 Figure 1: Framework of RIM for evolutionary reinforcement learning [10]. However, there are still two problems. \u2022 Reinforcement learning agents can only learn from experience, but not directly accept the guidance of elites in the current population.. \u2022 Reinforcement learning agents must have the same structure as individuals in the population, or at least have a similar parameterized form. Otherwise, reinforcement learning individuals that are copied into the population may not be able to participate in the evolutionary process. In response to the above problems, this paper proposes Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning. The framework of RIM for evolutionary reinforcement learning is presented in Figure 1. The recruitment process allows reinforcement learning (RL) agent to recruit the best individual from population to participate in RL agent decision and learning process. This requires RL agent to have the structure of Figure 2. In Figure 2, gradient policy network and recruitment policy network simultaneously accept the current state as input, and respectively produce actions as output. Critic network compares the potential rewards of actions produced by the two policy networks, and outputs the action with higher reward. The dual-policy decision-making mechanism not only enables the RL agent to produce better experiences, but also enables the RL agent to better estimate Q value, which enables the learning process of RL agent to directly accept the guidance of excellent policy in 3 Figure 2: Structure of dual policy RL agent the population. The imitation process permits low-\ufb01tness individuals in the population to learn behavioral patterns of RL agent. Since the structure of RL agent is inconsistent with that of individuals in the population, RL agent cannot be directly injected into the population and participate in the evolution. The imitation process is designed for addressing this problem. The main contributions of this paper are as follows: \u2022 We propose Recruitment-imitation Mechanism (RIM) and an evolutionary reinforcement learning framework that applies this mechanism. At the core of RIM, a dual-policy RL structure is designed, which allows critic network to determines actions. \u2022 We present a series of optimization techniques for RIM, including an off-policy imitation learning algorithm that directly uses experience replay buffer and soft updating strategies for recruiting networks. \u2022 We compare the performance of RIM with that of previous algorithms on Mujoco 4 ",
    "Related Work": "Related Work Combining reinforcement learning with imitation learning or evolutionary algorithms is not a new idea [11] [12] [13] [14]. The traditional combination methods mostly consider imitation learning or evolutionary algorithms as a sub-process of reinforcement learning. Typical practices in such methods include: leveraging the distribution estimation method to improve the exploration noise in reinforcement learning [15], utilizing the evolutionary method to optimize the approximation function in Q learning [16], or using the imitation learning to optimize the initial stage of the RL process [17]. Unlike these traditional practices, evolution and imitation learning in RIM are not sub-processes of reinforcement learning. In RIM, evolution and reinforcement learning are two relatively independent learning processes, while imitation learning is a way of synchronizing the behavioral policy of dual-policy agents into populations. Evolutionary Reinforcement Learning (ERL) [9] provides a new paradigm for the combination of evolutionary algorithms and reinforcement learning. ERL\u2019s approach is to reuse the interaction data between the population and the environment, and inject RL policy into the population. RIM can be viewed as an extension of ERL. The expanded parts include: 1) Combining evolutionary algorithms from the perspective of reinforcement learning. Namely, recruiting policy directly from the population to participate in reinforcement learning processes; 2) Constructing a dual-policy agent in the RL component. The agent can integrate two policies to generate experience, and give better estimation of Q value when policy learning is insuf\ufb01cient; 5 ",
    "Background": "",
    "Method": "Method Reinforcement Learning (CEM-RL) [10] replaces the combination of standard evolutionary algorithm and DDPG [1] in ERL with a combination of Cross-entropy Method (CEM) and Twin Delayed Deep Deterministic policy gradient (TD3) [19]. Unlike the above works, RIM is neither an optimization for task exploration nor a replacement of sub-components in ERL. Instead, it introduces a recruitment imitation mechanism that enables reinforcement learning, imitation learning, and evolutionary algorithms to be more effectively combined. In other words, RIM, CERL and CEM-RL are independent optimizations in different directions of ERL. 3. Background This section introduces the background of deep reinforcement learning, evolutionary algorithms and imitation learning. 3.1. Deep reinforcement learning Reinforcement learning methods abstract the interaction between an agent and environments into a Markov decision process. At each discrete time step t, the environment provides an observation st to the agent, and the agent takes an action at as a response to this observation. Then, the environment returns a reward rt and the next state st+1 to the agent. The goal of reinforcement learning is to maximize the expectation of Rt = \ufffdK k=0 \u03b3krt+k, where Rt indicates the sum of cumulative discounted rewards of K steps from the current moment. \u03b3 \u2208 (0, 1] is a discount factor. Deep deterministic policy gradient (DDPG) is a widely used model-free reinforcement learning algorithm based on actor-critic structure [1]. In DDPG, actor and critic are parameterized as \u03c0(s|\u03b8\u03c0) and Q(s, a|\u03b8Q) respectively. They are called current networks. In addition, replications of actor and critic networks \u03c0\u2032(s|\u03b8\u03c0\u2032) and Q\u2032(s, a|\u03b8Q\u2032) 6 are used to provide consistent targets for the learning process. They are called target networks. During the learning process, target networks will be soft updated based on current networks and a weighting factor \u03c4. DDPG completes off-policy learning by sampling experiences from a repaly buffer B. That is, for each interaction between an agent and environments, the tuple (st, at, rt, st+1) is stored into the replay buffer. One advantage of off-policy reinforcement learning is the sample ef\ufb01ciency of these algorithms. The other advantage is that learning from historical data can decouple exploration process and learning process. DDPG\u2019s exploration policy is constructed from actor policy and noise: \u03c0e(st) = \u03c0(st|\u03b8\u03c0) + N. The actor and critic networks are updated by sampling mini-batch experiences from the replay buffer. Critic network is updated by minimizing the loss function as follows: L = 1 N \ufffd i (y(i) t \u2212 Q(s(i) t , a(i) t |\u03b8Q))2 (1) where y(i) t = r(i) t + \u03b3Q\u2032(s(i) t+1, \u03c0\u2032(s(i) t+1|\u03b8\u03c0\u2032)|\u03b8Q\u2032). The actor network is updated according to sampled policy gradient: \u25bd\u03b8\u03c0\u03c0|s(i) \u2248 1 N \ufffd i \u25bdaQ(s(i), a|\u03b8Q)|a=\u03c0(s(i)) \u25bd\u03b8\u03c0 \u03c0(s(i)|\u03b8\u03c0) (2) 3.2. Evolutionary algorithms Evolutionary algorithms (EAs) are black-box optimization algorithms that are inspired by natural evolutionary processes. The evolutionary process acts on a population composed of several parameterized candidate solutions (individuals). During each iteration, parameters of individuals in the population are randomly perturbed (mutated), enabling new individuals to be generated. After generating new individuals, the environment evaluates their \ufb01tness. Individuals with higher \ufb01tness have higher probability to be selected, and selected individuals will participate in the next iterative process. When evolutionary algorithms are implemented to solve control tasks, several actor networks will participate in the evolutionary process. During the iterative process, actor networks with higher \ufb01tness will be retained as elites and shielded mutation step [9]. EAs can be employed as an alternative for reinforcement learning techniques based on 7 Markov decision processes [4]. 3.3. Imitation learning In imitation learning, an actor imitates expert behaviors to obtain a policy that is close to expert performance. Dataset aggregation (DAgger) method [20] is a widely used imitation learning algorithm. It is an iterative reduction to online policy training method. Suppose that there is an expert data set D = {s1, a1, ..., sn, an}. During each iteration, the actor is trained on this dataset in a supervised manner. The loss function during training can be denoted as: J(\u03b8) = 1 K \ufffdK k=1 L(\u03c0il(sk), ak), where \u03c0il is a policy to be trained and K is the batch size of samples. After convergence, policy \u03c0il is carried out to generate a state access set D\u03c0 = {sn+1, ..., sn+m}. Then D\u03c0 is labeled by actions output from the expert policy, and the expert data is accumulated: D \u2190 D \u222a D\u03c0. Then proceed to the next iteration. The advantage of DAgger is that it can employ expert policy to teach actors how to recover from errors. DAgger is a class of Follow-The-Leader algorithms [21]. 4. Recruitment-imitation Mechanism Recruitment-imitation mechanism in evolutionary reinforcement learning is presented in this section. 4.1. Dual policy reinforcement learning agent The core of recruitment-imitation mechanism is a dual policy RL agent with dualactors and single-critic. In the iterative process of the evolutionary reinforcement learning algorithm, the dual policy RL agent can recruit excellent individuals from the population to participate in decision-making or reinforcement learning process. In addition, individuals with poor performance in the population can periodically imitate behavior patterns of dual policy RL agent to accelerate evolutionary speed of the population. Dual policy RL agent in recruitment-imitation mechanism still uses the actor-critic structure but has two actor networks, including a gradient policy network \u03c0pg(st|\u03b8pg) and a recruitment policy network \u03c0ea(st|\u03b8ea). When the agent makes decisions, the 8 critic network will identify which policy under the current state st is potentially more pro\ufb01table. Therefore, policy of the dual policy RL agent can be indicated as: \u03c0rl = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u03c0pg, Q(st, \u03c0pg(st|\u03b8pg)|\u03b8Q) \u2265 Q(st, \u03c0ea(st|\u03b8ea)|\u03b8Q) \u03c0ea, otherwise (3) where \u03c0rl indicates the overall behavior policy of the dual policy RL agent. Similar to DDPG, during the learning process, the critic network is updated by minimizing the loss function of (1), except that y(i) t is estimated using \u03c0rl: y(i) t = r(i) t + \u03b3Q\u2032(s(i) t+1, \u03c0rl(s(i) t+1)|\u03b8Q\u2032) (4) The gradient policy network is still updated with the sampled policy gradient according to equation (2). There are two reasons that the dual policy RL agent can make RIM perform better: \u2022 The dual policy RL agent can generate better experiences according to the behavior pattern in equation (3). \u2022 When gradient update is performed, \u03c0rl used in equation (4) can make more accurate estimation of Q\u2032. Then we prove that equation (4) can give more accurate estimation of Q\u2032. Theorem 1. Suppose that Q\u2032 and Q converge at the same position for policy \u03c0\u2217. The converged policy to maximize Q\u2032 or Q is \u03c0\u2217, and the estimation value using \u03c0\u2217 is Q\u2217. The mean of Q\u2032 estimation of DDPG is denoted as Eddpg( \u02c6Q\u2032) and the mean of Q\u2032 estimation of RIM is denoted as Erim( \u02c6Q\u2032). Then, Eddpg( \u02c6Q\u2032) \u2264 Erim( \u02c6Q\u2032) \u2264 Q\u2217. Proof: Since \u03c0\u2217 is the policy that maximizes Q\u2032 (i.e., Q\u2217), any policy that is not \u03c0\u2217 will cause the estimate of Q\u2032 to be less than Q\u2217. So, there is Erim( \u02c6Q\u2032) \u2264 Q\u2217. Erim( \u02c6Q\u2032) = Q\u2217 if and only if the policy of \u03c0rl in equation (3) is always \u03c0\u2217. Also 9 there is Eddpg( \u02c6Q\u2032) \u2264 Q\u2217. Eddpg( \u02c6Q\u2032) = Q\u2217 if and only if the policy \u03c0ddpg of DDPG is always \u03c0\u2217 . Assume that DDPG\u2019s behavior policy \u03c0ddpg causes the estimation of Q\u2032 to be shifted downward by x with the probability of p(x), then Eddpg( \u02c6Q\u2032) = \ufffd +\u221e \u2212\u221e (Q\u2217 \u2212 x)p(x)dx. Since RIM uses the policy in equation (3) to estimate Q\u2032 in equation (4). When \u03c0pg (i.e., \u03c0ddpg) estimates Q\u2032 downwards by x with the probability of p(x), \u03c0ea can upwards correct y (y \u2265 0) with the probability of q(y|x). Then we have: Erim( \u02c6 Q\u2032) = \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e (Q\u2217 \u2212 x + y)q(y|x)p(x)dydx = \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e (Q\u2217 \u2212 x)q(y|x)p(x)dydx + \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e yq(y|x)p(x)dydx = \ufffd +\u221e \u2212\u221e (Q\u2217 \u2212 x)( \ufffd +\u221e \u2212\u221e q(y|x)dy)p(x)dx + \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e yq(y|x)p(x)dydx = \ufffd +\u221e \u2212\u221e (Q\u2217 \u2212 x)p(x)dx + \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e yq(y|x)p(x)dydx = Eddpg( \u02c6 Q\u2032) + \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e yq(y|x)p(x)dydx Considering y \u2265 0 and q(y|x)p(x) \u2265 0, then \ufffd +\u221e \u2212\u221e \ufffd +\u221e \u2212\u221e yq(y|x)p(x)dydx \u2265 0, so it holds that Eddpg( \u02c6Q\u2032) \u2264 Erim( \u02c6Q\u2032). \u25a0 In Theorem 1, we can assume that Q and Q\u2032 converge at the same position because Q and Q\u2032 are very close using soft update setup in continuous domains [19]. Theorem 1 states that the estimation of Q\u2032 using \u03c0rim is more accurate than that estimated using \u03c0ddpg, or closer to Q\u2217. Unlike overestimation problems [22] [23], Theorem 1 states that when \u03c0ddpg does not converge suf\ufb01ciently during the updating process according to equation (2), if the policy \u03c0ea in population gives better actions, the Q\u2032 estimation of RIM will be more accurate. This is one of the reasons the RL component of RIM can learn better. 4.2. Off-policy imitation learning Due to the inconsistencies in structures of a dual policy RL agent (dual-actors and single-critic) and an individual (single-actor) in the population, the dual policy RL 10 Figure 3: Framework of off-policy imitation learning in RIM. agent cannot be directly injected into the population to participate in the evolution. In this case, an ideal solution is to use imitation learning to unify behavior patterns of the RL agent and individuals in the population. As a typical imitation learning algorithm, DAgger can get better performance with only a few iterations when the amount of data is large. Evolutionary reinforcement learning system usually has a huge experience replay buffer for storing historical data. The actors in the population to be trained can directly sample states and actions in the experience replay buffer and be trained according to the following loss function: J0(\u03b8wt) = 1 K K \ufffd k=1 L(\u03c0wt(sk|\u03b8wt), \u03c0rl(sk)) (5) where \u03c0wt indicates the policy of an actor to be trained. \u03c0wt is usually a policy of an actor with the worst performance in the population. Figure 3 shows the framework of off-policy imitation learning and Algorithm 1 describes the detailed process of off-policy imitation learning in RIM. We cancel the data labeling process and accumulation process of DAgger so that the imitation process is off-policy completely. The main function of DAgger\u2019s data labeling process and accumulation process is to recover the imitation learner from errors. In the RIM environment setting, there is a 11 Algorithm 1 Off-policy imitation learning in RIM 1: Get the worst policy \u03c0wt from population 2: for n = 1 to N do 3: Get replay buffer size: size \u2190 len(B) 4: while size > l0 do 5: Sample random batch {si} from replay buffer 6: Get awt i according to \u03c0wt(si|\u03b8wt) 7: Get \u03c0rl according to equation (3) 8: Get expert action ai according to \u03c0rl(si) 9: Calculate loss L between expert action awt i and agent action ai 10: Calculate \u25bd\u03b8wtL 11: Update \u03b8wt \u2190 \u03b8wt + \u03b1 \u00b7 \u25bd\u03b8wtL (a) walker2d-v2 (b) Hopper-v2 Figure 4: Performance of different individuals in the population. large amount of erroneous data in the experience replay buffer, and most of the states sampled from the experience replay buffer is generated by a suboptimal or wrong policy. As shown in Figure 4, the performance of individuals in the population varies widely, and even the interaction information of the worst individual is stored into the replay buffer. Therefore the training process using equation (5) is also a process to teach actors how to recover from an error state. 4.3. Learning and evolution process of RIM The core idea of RIM is to 1) accelerate the learning process by recruiting elite individuals in the population to guide the policy gradient network learning, and 2) accelerate the evolutionary process by enabling individuals with poor performance in the population to imitate behavioral patterns of the dual policy RL agent. 12 The process of RIM is as follows: 1) Some actor networks in the population and actor and critic networks in the dual policy agent are initialized with random weights. 2) In the evolution process, the environment evaluates the \ufb01tness of actors in the population and select a part of the actors to survive with a probability according to the \ufb01tness. 3) The actors are perturbed by mutation and crossover to generate the next generation of actors. 4) After generating the next generation, the dual policy RL agent recruits an actor with the highest \ufb01tness in the population, copies its parameters to the recruitment policy network, and learns for a certain number of times. Algorithm 2 describes the detailed learning process of dual policy RL agent. The worst actor in the population imitates behavioral patterns of the dual policy RL agent every several generations so that the learning outcomes of the dual policy RL agent can be injected into the population. During the evolutionary process, interaction information between actors and environments will be stored into the experience replay buffer. These experiences are not only used for the sampling process of gradient policy network learning in Algorithm 2, but also for the sampling process of the worst actor imitating in population. In the RIM setting of this paper, the structure of gradient policy network is same as that of actors in the population, so when the imitation learning is not performed, the gradient policy network will be periodically copied into the population to accelerate the evolutionary process. The recruitment policy network participates in the decision process of \u03c0rl. Therefore, it affects the value of y(i) t . The learning process of critic network needs to be provided with a consistent y(i) t . Drawing on the idea of soft update in DDPG, we soft update the target gradient policy network and critic network. In order to further ensure consistency in the learning process, we decide to add a target recruitment policy network. Current recruitment policy network \u03c0ea(st|\u03b8ea) uses hard update to copy directly from the population, and the target recruitment policy network uses soft update 13 ",
    "Experiments": "Experiments In this section, we show comparative experiments and a series of analytical results. 14 5.1. Experimental settings We evaluated the performance of RIM on four continuous control tasks in Mujoco [24] hosted through the OpenAI gym [25], which are widely used in the evaluation of reinforcement learning continuous domains [26] [27] [28]. These tasks are shown in Figure 5. (a) Walker2d-v2 (b) Hopper-v2 (c) HalfCheetah-v2 (d) Swimmer-v2 Figure 5: Four continuous control tasks in Mujoco-based environments Baselines for comparison include Evolutionary Reinforcement Learning (ERL) algorithm [9], DDPG, and a standard evolutionary algorithm (EA). ERL is a state of the art algorithm in the Mujoco benchmarks; DDPG is considered as one of the best reinforcement learning algorithms and is widely used in a variety of continuous control tasks. The reinforcement learning parameters of RIM are consistent with those of ERL and DDPG. The crossover and mutation probability is 0 and 0.01 respectively in population of RIM, ERL and EA. In the comparative experiments, RIM uses soft update 15 to recruit policies from population. All actor networks and critic networks have two hidden layers, with recti\ufb01ed linear units (ReLU) between hidden layers. The last layer of actor networks is linked to a tanh unit. We use Adam [29] to update all network parameters. The learning rates of the gradient policy network and the critic network are 10\u22124 and 10\u22123, respectively. The number of individuals in the population is 10. Imitation learning is performed every 10 generations. The loss function of the imitation learning is least absolute error (L1), and the learning rate is 10\u22123. The sample batch size of reinforcement learning and imitation learning are 128 and 32, respectively. The performance of the algorithms involved in the comparison is tested using different random seeds and is recorded every 10,000 frames. When testing, the average of \ufb01ve test results is recorded as the performance at this time. For DDPG, the actor\u2019s exploration noise was removed during the test. For EA, ERL and RIM, the performance of the best individuals in their population are recorded as the \ufb01nal performance. The scores of these algorithms are the rewards returned by environments, and the corresponding number of frames is the cumulative number of frames in which an algorithm interacts with the environment as a whole. These recording methods are consistent with previous comparative experiments [9]. 5.2. Comparison The results in Table 1 and Figure 6 show the \ufb01nal performance and learning curves of the four algorithms on Mujoco-based continuous control benchmarks. Each algorithm is trained for 5M frames in Walker2d-v2, 2.5M frames in Hopper-v2, and 1M frames in HalfCheetah-v2 and Swimmer-v2. Table 1 presents the maximum (Max) reward obtained by each algorithm in the whole learning process, as well as the average (Mean), median (Median) and standard deviation (Std.) of 5 test results of each algorithm under different random seeds. The best statistical results have been bolded in each task. Figure 6 presents the average proformance and error bars, which have been smoothed using a window of size 10. Table 1 shows that the average performance of RIM exceeds the previous methods in all environments, and the maximum reward obtained in the whole learning process and median performance exceed the previous methods in most environments. Both 16 Table 1: Final performance of EA, DDPG, ERL and RIM on 4 Mujoco-based continuous control benchmarks Walker2d-v2 Hopper-v2 Halfcheetah-v2 Swimmer-v2 EA Max 1339.11 1051.19 1755.27 356.01 Mean 1143.68 1032.92 920.60 239.04 Median 1073.38 1027.82 864.62 301.93 Std. 11.37% 0.08% 65.49% 43.21% DDPG Max 3184.08 3575.19 6012.01 55.32 Mean 543.94 484.29 2601.53 26.78 Median 492.85 590.66 2796.43 29.25 Std. 39.77% 67.12% 60.66% 27.86% ERL Max 4472.21 2392.49 5597.36 332.69 Mean 1375.24 1780.90 5098.46 231.38 Median 1476.77 1824.88 5064.25 222.56 Std. 23.51% 30.23% 5.54% 21.37% RIM(ours) Max 5532.66 3439.74 6151.81 343.62 Mean 2852.08 2385.05 5399.68 297.43 Median 3242.61 2386.84 5367.22 287.49 Std. 26.66% 19.66% 6.94% 9.78% RIM and ERL have higher variances on different random seeds because the components, such as EA and DDPG, have high variances. However, the standard deviation of RIM performance is much lower than ERL in Hopper-v2 and Swimmer-v2. The experimental results in Figure 6 show that RIM can learn better than the previous algorithms in four tasks. In Walker2d-v2 and Hopper-v2 environments, the superiority of RIM is more signi\ufb01cant, because the traditional off-policy reinforcement learning method cannot explore ef\ufb01ciently in such environments. EA has a stagnation period in the evolution process. Off-policy reinforcement learning individuals in ERL can help the evolutionary population to break through the stagnation period, but this often requires superior individuals in the population to generate a large amount of experiences. The recruitment mechanism of RIM enables outstanding individuals of EA to directly guide reinforcement learning individuals, instead of guiding them by generating experience. Therefore, RIM can break through the stagnation period earlier than ERL, which allows RIM to learn faster. We reimplemented the EA, DDPG and ERL algorithms. The ERL results we presented in Walker2d and Hopper environments are consistent with Khadka et al. [9]. 17 (a) Walker2d-v2 (b) Hopper-v2 (c) HalfCheetah-v2 (d) Swimmer-v2 Figure 6: Learning curves of EA, DDPG, ERL and RIM on 4 Mujoco-based continuous control benchmarks The results in the Halfcheetah environment are lower than Khadka et al., but close to those reported by Pourchot et al. [10]. In addition, we found that in the Swimmer environment, the performance of EA, ERL and RIM is unstable. In the best case, they can reach 330 or more, but in the worst case, they perform less than 250, even less than 150 in EA. 5.3. Component performance Figure 7 shows performance of the three components in Walker2d and Hopper environments: RL components of RIM, imitation learning (IL) actors in RIM, and RL agent in ERL. The performance of RL agent in RIM is better than RL agent in ERL, which means that the recruitment mechanism can accelerate the learning process of RL actors. The performance of individuals trained by imitation learning in RIM improves 18 as the performance of the RL components improves, which illustrates the effectiveness of off-policy imitation learning. (a) Walker2d-v2 (b) Hopper-v2 Figure 7: Performance of components of different algorithms The externally injected individuals accepted by RIM\u2019s population are imitation learning individuals, while the externally injected individuals accepted by the ERL\u2019s population are RL actors. In Figure 7, the performance of imitation learner in RIM outperforms RL agent in ERL, suggesting that externally injected individuals of the population of RIM are better. This can directly explain why the learning speed of RIM is better than that of ERL. Table 2 presents the selection rate of actors trained through imitation learning during evolution. When the population selects excellent individuals, \u03c0wt with higher perTable 2: Selection rate for \u03c0wt after training Selected Discarded Walker2d-v2 59.87% 40.13% Hopper-v2 30.92% 69.08% HalfCheetah-v2 70.37% 29.63% Swimmer-v2 24.44% 75.56% formance has higher probability to be selected. On the contrary, \u03c0wt with lower performance will have a higher probability of being discarded. The RL components performs better in Walker2d and Halfcheetah, so the selected probability of well-learned \u03c0wt is higher. In Hopper environment, the RL components performs poorly before breaking 19 (a) HalfCheetah-v2 (b) Hopper-v2 Figure 8: Comparison curves between soft update recruitment and hard update recruitment through the long stagnation period. In the Swimmer environment, the RL components cannot play a critical role in the whole learning process, which can be seen from curve of DDPG in Figure 6. These reasons cause \u03c0wt cannot imitate a good policy, so the probability that \u03c0wt is selected is lower. On the whole, individuals performing imitation learning can be selected in each environment, indicating that the policy learned by imitation learning can accelerate the evolution of the population. 5.4. Soft update in recruitment We tested the performance of RIM using hard update, and compared it with the original RIM. As shown in Figure 8, RIM using soft update recruitment performed slightly better than RIM using hard update recruitment. Because in the process of guiding the gradient policy network learning, the soft update recruitment policy network can provide more stable y(i) when calculating equation (4), which ensures better consistency for learning process. However, recruitment using hard update does not make the policy gradient network unable to learn. In fact, as the population iterates, the changes of parameters between generations are usually small. Therefore, parameter changes of the recruitment policy network are small, which can also bring a certain degree of consistency to the learning process. 20 Table 3: Final performance of ERL, RIM and variants of RIM on 4 Mujoco-based continuous control benchmarks Walker2d-v2 Hopper-v2 Halfcheetah-v2 Swimmer-v2 RIM Mean 2852.08 2385.05 5399.68 297.43 Median 3242.61 2386.84 5367.22 287.49 Std. 26.66% 19.66% 6.94% 9.78% RIM-IL Mean 2075.33 1574.41 5229.43 269.68 Median 1937.64 1095.01 5253.77 267.89 Std. 26.43% 56.14% 7.5% 9.45% RIM-EA Mean 1948.16 1753.66 5001.21 258.96 Median 2018.73 1749.26 4923.92 261.63 Std. 49.10% 29.14% 7.67% 20.63% RIM-PG Mean 1777.30 1141.54 5142.67 222.60 Median 1208.66 1078.56 5197.68 209.84 Std. 54.97% 15.23% 13.96% 32.61% ERL Mean 1375.24 1780.90 5098.46 231.38 Median 1476.77 1824.88 5064.25 222.56 Std. 23.51% 30.23% 5.54% 21.37% 5.5. Ablation experiments In order to further prove the effectiveness of the RIM components, we performed ablation experiments on RIM. We tested the performance of RIM without of\ufb02ine imitation learning (RIM-IL), RIM without using the recruitment network to estimate Q\u2032 (RIM-EA), and RIM without using the gradient policy network to estimate Q\u2032 (RIMPG). We compare the test performance of these three RIM variants with RIM as well as ERL. The comparison results are presented in Table 3 and Figure 9. According to the results in Table 3 and Figure 9, RIM outperforms RIM-IL, which shows that imitation learning works. We believe that imitation learning can inject individuals with dual-policy RL agent\u2019s behavior patterns into the population, thereby accelerating the evolution of the population. Performance of RIM is better than that of RIM-EA and RIM-PG, which shows that the dual-policy learning mode can learn better policy. This stems from the fact that the dual-policy can better estimate the Q\u2032 value, as revealed in Theorem 1. To further illustrate the effectiveness of the dual-policy learning model in evolutionary reinforcement learning algorithms, we compared learning curves of RL com21 (a) Walker2d-v2 (b) Hopper-v2 (c) HalfCheetah-v2 (d) Swimmer-v2 Figure 9: Learning curves of ERL, RIM and variants of RIM on 4 Mujoco-based continuous control benchmarks 22 (a) Wakler2d-v2 (b) Hopper-v2 Figure 10: Learning curves of RL components in RIM, RIM-PG, and RIM-EA ponents in RIM, RIM-EA, and RIM-PG algorithms. These curves are shown in Figure 10. RIM\u2019s RL policy is better than RIM-EA, which indicates that RIM estimates the Q\u2032 value better than DDPG. RIM-PG\u2019s RL components performs poorly. This results from the fact that the Q\u2032 estimated by the recruitment network is not accurate in Walker2d-v2 and Hopper-v2. 6. Conclusion and Future Work In this paper, we develop a dual-actors and single critic RL agent, which can be well combined with evolutionary algorithms. Based on this, we propose RIM for evolutionary reinforcement learning. This method not only outperforms the previous evolutionary and off-policy reinforcement learning algorithms, but also exceeds ERL both in overall performance and component performance. Finally, we experimentally demonstrated that the recruitment using soft update enables RL agent to learn faster than that using hard update. Future work can be explored from the following three aspects: \u2022 RIM uses a standard evolutionary algorithm to iterate populations. Other more ef\ufb01cient evolutionary algorithms may provide immediate improvements to RIM\u2019s performance, such as Natural Evolution Strategy (NES) [30], NeuroEvolution of Augmenting Topologies (NEAT) [31]. 23 ",
    "References": "References References [1] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint arXiv:1509.02971. [2] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in: International Conference on Machine Learning, 2016, pp. 1928\u20131937. [3] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region policy optimization, in: International Conference on Machine Learning, 2015, pp. 1889\u2013 1897. 24 [4] T. Salimans, J. Ho, X. Chen, S. Sidor, I. Sutskever, Evolution strategies as a scalable alternative to reinforcement learning, arXiv preprint arXiv:1703.03864. [5] A. Giusti, J. Guzzi, D. C. Cires\u00b8an, F.-L. He, J. P. Rodr\u00b4\u0131guez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, et al., A machine learning approach to visual perception of forest trails for mobile robots, IEEE Robotics and Automation Letters 1 (2) (2016) 661\u2013667. [6] F. Codevilla, M. Miiller, A. L\u00b4opez, V. Koltun, A. Dosovitskiy, End-to-end driving via conditional imitation learning, in: 2018 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2018, pp. 1\u20139. [7] S. Zhang, O. R. Zaiane, Comparing deep reinforcement learning and evolutionary methods in continuous control, arXiv preprint arXiv:1712.00006. [8] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529\u2013533. [9] S. Khadka, K. Tumer, Evolution-guided policy gradient in reinforcement learning, in: Advances in Neural Information Processing Systems, 2018, pp. 1196\u2013 1208. [10] A. Pourchot, O. Sigaud, Cem-rl: Combining evolutionary and gradient-based methods for policy search, arXiv preprint arXiv:1810.01222. [11] S. Ross, J. A. Bagnell, Reinforcement and imitation learning via interactive noregret learning, arXiv preprint arXiv:1406.5979. [12] E. Uchibe, Cooperative and competitive reinforcement and imitation learning for a mixture of heterogeneous learning modules, Frontiers in neurorobotics 12 (2018) 61. [13] D. V. Vargas, Evolutionary reinforcement learning: general models and adaptation., in: GECCO (Companion), 2018, pp. 1017\u20131038. 25 [14] M. M. Drugan, Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms, Swarm and evolutionary computation 44 (2019) 228\u2013 246. [15] H. Tan, K. Balajee, D. Lynn, Integration of evolutionary computing and reinforcement learning for robotic imitation learning, in: 2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC), IEEE, 2014, pp. 407\u2013412. [16] S. Whiteson, P. Stone, Evolutionary function approximation for reinforcement learning, Journal of Machine Learning Research 7 (May) (2006) 877\u2013917. [17] J. Kober, J. Peters, Imitation and reinforcement learning, IEEE Robotics & Automation Magazine 17 (2) (2010) 55\u201362. [18] S. Khadka, S. Majumdar, S. Miret, E. Tumer, T. Nassar, Z. Dwiel, Y. Liu, K. Tumer, Collaborative evolutionary reinforcement learning, in: International Conference on Machine Learning, 2019, pp. 3341\u20133350. [19] S. Fujimoto, H. van Hoof, D. Meger, Addressing function approximation error in actor-critic methods, in: International Conference on Machine Learning, 2018, pp. 1582\u20131591. [20] S. Ross, G. J. Gordon, J. A. Bagnell, No-regret reductions for imitation learning and structured prediction, in: AISTATS, Citeseer, 2011. [21] A. Attia, S. Dayan, Global overview of imitation learning, arXiv preprint arXiv:1801.06503. [22] H. V. Hasselt, Double q-learning, in: Advances in Neural Information Processing Systems, 2010, pp. 2613\u20132621. [23] H. van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double q-learning, in: Thirtieth AAAI conference on arti\ufb01cial intelligence, 2016. [24] E. Todorov, T. Erez, Y. Tassa, Mujoco: A physics engine for model-based control, in: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE, 2012, pp. 5026\u20135033. 26 [25] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym, arXiv preprint arXiv:1606.01540. [26] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, Benchmarking deep reinforcement learning for continuous control, in: International Conference on Machine Learning, 2016, pp. 1329\u20131338. [27] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, D. Meger, Deep reinforcement learning that matters, in: Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018, pp. 3207\u20133214. [28] R. Islam, P. Henderson, M. Gomrokchi, D. Precup, Reproducibility of benchmarked deep reinforcement learning tasks for continuous control, arXiv preprint arXiv:1708.04133. [29] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980. [30] D. Wierstra, T. Schaul, J. Peters, J. Schmidhuber, Natural evolution strategies, in: 2008 IEEE Congress on Evolutionary Computation, IEEE, 2008, pp. 3381\u20133387. [31] K. O. Stanley, R. Miikkulainen, Evolving neural networks through augmenting topologies, Evolutionary Computation 10 (2) (2002) 99\u2013127. [32] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, in: International Conference on Machine Learning, 2018, pp. 1856\u20131865. 27 ",
    "title": "Recruitment-imitation Mechanism for Evolutionary",
    "paper_info": "Recruitment-imitation Mechanism for Evolutionary\nReinforcement Learning\nShuai L\u00a8ua,b,\u2217, Shuai Hana,b, Wenbo Zhoua,b, Junwei Zhanga,b\naKey Laboratory of Symbolic Computation and Knowledge Engineering (Jilin University), Ministry of\nEducation, Changchun 130012, China\nbCollege of Computer Science and Technology, Jilin University, Changchun 130012, China\nAbstract\nReinforcement learning, evolutionary algorithms and imitation learning are three prin-\ncipal methods to deal with continuous control tasks. Reinforcement learning is sam-\nple ef\ufb01cient, yet sensitive to hyper-parameters setting and needs ef\ufb01cient exploration;\nEvolutionary algorithms are stable, but with low sample ef\ufb01ciency; Imitation learning\nis both sample ef\ufb01cient and stable, however it requires the guidance of expert data.\nIn this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary\nreinforcement learning, a scalable framework that combines advantages of the three\nmethods mentioned above. The core of this framework is a dual-actors and single critic\nreinforcement learning agent. This agent can recruit high-\ufb01tness actors from the popu-\nlation of evolutionary algorithms, which instructs itself to learn from experience replay\nbuffer. At the same time, low-\ufb01tness actors in the evolutionary population can imitate\nbehavior patterns of the reinforcement learning agent and improve their adaptability.\nReinforcement and imitation learners in this framework can be replaced with any off-\npolicy actor-critic reinforcement learner or data-driven imitation learner. We evaluate\nRIM on a series of benchmarks for continuous control tasks in Mujoco. The experi-\nmental results show that RIM outperforms prior evolutionary or reinforcement learning\nmethods. The performance of RIM\u2019s components is signi\ufb01cantly better than compo-\nnents of previous evolutionary reinforcement learning algorithm, and the recruitment\nusing soft update enables reinforcement learning agent to learn faster than that using\n\u2217Corresponding author\nEmail address: lus@jlu.edu.cn (Shuai L\u00a8u)\nPreprint submitted to Journal of Information Sciences\nDecember 16, 2019\narXiv:1912.06310v1  [cs.LG]  13 Dec 2019\n",
    "GPTsummary": "- (1): The methodological idea of this article is the Recruitment-imitation Mechanism (RIM), which combines the advantages of reinforcement learning, evolutionary algorithms, and imitation learning in a scalable framework. RIM uses a dual-policy reinforcement learning agent with two actor networks, including a gradient policy network and a recruitment policy network, and a single critic network. The dual-actor policy helps RIM to recruit high-fitness actors from the population of evolutionary algorithms and to imitate behavior patterns of the reinforcement learning agent from low-fitness actors in the evolutionary population. \n\n- (2): The reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. The RIM approach is designed to deal with continuous control tasks and tested using a series of benchmarks in Mujoco. \n\n- (3): The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithms, and the recruitment using soft update enables the reinforcement learning agent to learn faster than using hard update.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing a scalable framework, called Recruitment-imitation Mechanism (RIM), that combines the advantages of reinforcement learning, evolutionary algorithms, and imitation learning in dealing with continuous control tasks. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods, which could have an impact in developing agents that can make excellent decisions in complex and uncertain environments.\n\n- (2): Innovation point: The innovation point is the Recruitment-imitation Mechanism (RIM), which combines evolutionary algorithms, reinforcement learning, and imitation learning in a scalable framework to deal with continuous control tasks. This approach proposes dual-policy reinforcement learning for recruiting high-fitness actors and imitating low-fitness actors in the evolutionary population, which is a new method of combining these three approaches. \n\nPerformance: The performance of RIM is evaluated using a series of benchmarks in Mujoco, and the experimental results show that it outperforms previous evolutionary or reinforcement learning methods. The recruitment using a soft update enables faster learning for the reinforcement learning agent than using a hard update. The performance achieved supports their goals of finding a scalable framework that combines the advantages of the three methods mentioned above.\n\nWorkload: No significant workload issues were identified in this article. However, the proposed RIM framework is complex and requires a good understanding of reinforcement learning, evolutionary algorithms, and imitation learning to be implemented correctly.\n\n\n",
    "GPTmethods": "- (1): The methodological idea of this article is the Recruitment-imitation Mechanism (RIM), which combines the advantages of reinforcement learning, evolutionary algorithms, and imitation learning in a scalable framework. RIM uses a dual-policy reinforcement learning agent with two actor networks, including a gradient policy network and a recruitment policy network, and a single critic network. The dual-actor policy helps RIM to recruit high-fitness actors from the population of evolutionary algorithms and to imitate behavior patterns of the reinforcement learning agent from low-fitness actors in the evolutionary population. \n\n- (2): The reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. The RIM approach is designed to deal with continuous control tasks and tested using a series of benchmarks in Mujoco. \n\n- (3): The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithms, and the recruitment using soft update enables the reinforcement learning agent to learn faster than using hard update.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing a scalable framework, called Recruitment-imitation Mechanism (RIM), that combines the advantages of reinforcement learning, evolutionary algorithms, and imitation learning in dealing with continuous control tasks. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods, which could have an impact in developing agents that can make excellent decisions in complex and uncertain environments.\n\n- (2): Innovation point: The innovation point is the Recruitment-imitation Mechanism (RIM), which combines evolutionary algorithms, reinforcement learning, and imitation learning in a scalable framework to deal with continuous control tasks. This approach proposes dual-policy reinforcement learning for recruiting high-fitness actors and imitating low-fitness actors in the evolutionary population, which is a new method of combining these three approaches. \n\nPerformance: The performance of RIM is evaluated using a series of benchmarks in Mujoco, and the experimental results show that it outperforms previous evolutionary or reinforcement learning methods. The recruitment using a soft update enables faster learning for the reinforcement learning agent than using a hard update. The performance achieved supports their goals of finding a scalable framework that combines the advantages of the three methods mentioned above.\n\nWorkload: No significant workload issues were identified in this article. However, the proposed RIM framework is complex and requires a good understanding of reinforcement learning, evolutionary algorithms, and imitation learning to be implemented correctly.\n\n\n",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing a scalable framework, called Recruitment-imitation Mechanism (RIM), that combines the advantages of reinforcement learning, evolutionary algorithms, and imitation learning in dealing with continuous control tasks. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods, which could have an impact in developing agents that can make excellent decisions in complex and uncertain environments.\n\n- (2): Innovation point: The innovation point is the Recruitment-imitation Mechanism (RIM), which combines evolutionary algorithms, reinforcement learning, and imitation learning in a scalable framework to deal with continuous control tasks. This approach proposes dual-policy reinforcement learning for recruiting high-fitness actors and imitating low-fitness actors in the evolutionary population, which is a new method of combining these three approaches. \n\nPerformance: The performance of RIM is evaluated using a series of benchmarks in Mujoco, and the experimental results show that it outperforms previous evolutionary or reinforcement learning methods. The recruitment using a soft update enables faster learning for the reinforcement learning agent than using a hard update. The performance achieved supports their goals of finding a scalable framework that combines the advantages of the three methods mentioned above.\n\nWorkload: No significant workload issues were identified in this article. However, the proposed RIM framework is complex and requires a good understanding of reinforcement learning, evolutionary algorithms, and imitation learning to be implemented correctly.\n\n\n"
}