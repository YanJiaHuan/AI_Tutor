{
    "Abstract": "Abstract\u2014 In meta-reinforcement learning, an agent is trained in multiple different environments and attempts to learn a meta-policy that can ef\ufb01ciently adapt to a new environment. This paper presents RAMP, a Reinforcement learning Agent using Model Parameters that utilizes the idea that a neural network trained to predict environment dynamics encapsulates the environment information. RAMP is constructed in two phases: in the \ufb01rst phase, a multi-environment parameterized dynamic model is learned. In the second phase, the model parameters of the dynamic model are used as context for the multi-environment policy of the model-free reinforcement learning agent. We show the performance of our novel method in simulated experiments and compare them to existing methods. I. ",
    "Introduction": "INTRODUCTION Common approaches for developing controllers do not rely on machine learning. Instead, engineers manually construct the controller based on general information about the world and the problem. After repetitively testing the controller in the environment, the engineer improves the controller based on the feedback from these tests. That is, a human is an essential part of this iterative process. Reinforcement Learning (RL) reduces human effort by automatically learning from interaction with the environment. Instead of explicitly designing and improving a controller, the engineer develops a general RL agent that learns to improve the controller\u2019s performance without human intervention. The RL agent is usually general and does not include speci\ufb01c information about the target environment; this allows it to adapt to different environments. Indeed, RL agents may achieve higher performance compared to human-crafted controllers [1]\u2013[3]. However, RL agents usually require training from the ground up for every new environment, which requires extensive interaction in the new environment. One solution to speed up the training time is to explicitly provide human-crafted information about the environment (context) to the RL agent [4]. However, such a solution requires explicitly analyzing the target environment, which may be challenging and time-consuming. Instead of relying on the human understanding of the problem for providing such context, a meta-Reinforcement Learning (meta-RL) agent can learn to extract a proper environmental context. To that end, a meta-RL agent is trained on extended interaction in multiple different environments, and then, after a short interaction in a new, unseen environment, it This research was supported, in part, by the Ministry of Science & Technology, Israel. 1 Department of Mechanical Engineering and Mechatronics, Ariel University, Israel 2 Department of Computer Science, Ariel University, Israel gabrielh@ariel.ac.il, amos.azaria@ariel.ac.il is required to perform well in it [5], [6]. Speci\ufb01cally, a metaRL algorithm that is based on context extraction is composed of two phases. First, in the meta-learning phase, the agent learns a general policy suitable to all environments given a context. Additionally, in this phase, the meta-RL agent learns how to extract a context from samples obtained from an environment. Secondly, in the adaptation phase, the meta-RL agent conducts a short interaction in the new environment, and the context is extracted from it. This context is then fed to the general policy, which acts in the new environment. One common approach for context extraction is using a Recurrent Neural Network (RNN). That is, the RNN receives the history of the states, actions, and rewards and is trained to output a context that is useful for the general policy. However, the RNN long-term memory capability usually limits the effective history length [7]. Additionally, since the context vector is not explicitly explainable, it is dif\ufb01cult to examine the learning process and understand if the RNN learned to extract the representative properties of the environments. In this paper, we introduce RAMP \u2013 a Reinforcement learning Agent using Model Parameters. We utilize the idea that a neural network trained to predict environment dynamics encapsulates the environment properties; therefore, its parameters can be used as the context for the policy. During the meta-RL phase, RAMP learns a neural network that predicts the environment dynamic for each environment. However, since the number of the neural network\u2019s parameters is usually high, it is challenging for the policy to use the entire set of parameters as its context. Therefore, the majority of the model\u2019s parameters are shared between all environments, and only a small set of parameters are trained separately in each environment. In that way, the environment-speci\ufb01c parameters represent the speci\ufb01c environment properties. Consequently, a general policy uses only these parameters as context and outputs actions that are suitable for that particular environment. One advantage of RAMP is that the history length used for the context extraction is not limited because the context is extracted from a global dynamic model. Additionally, the combination of model learning and RL in RAMP makes the training process more transparent since it is possible to evaluate the performance of the model learning process independently. We demonstrate the effectiveness of RAMP in several simulated experiments in Sec. V. To summarize, the contributions of this paper are: \u2022 Suggesting a novel method for meta-reinforcement learning. \u2022 Presenting a multi-environment dynamic model learning method that adapts to new environments by updating arXiv:2210.15515v1  [cs.LG]  27 Oct 2022 ",
    "Related Work": "RELATED WORK RL has shown success in numerous domains, such as playing Atari games [1], [8], playing Go [9], and driving autonomous vehicles [3], [10]. Some are designed for one speci\ufb01c environment [11], [12], while others can learn to master multiple environments [2], [8]; however, many algorithms require separate training for each environment. Several approaches were proposed to mitigate the need for long training times by using meta-RL methods. We begin by describing methods that, similarly to ours, learn a context-conditioned, general policy. However, they constructed the context vector in different ways. We note that some previous works term the different training environments \u201ctasks\" since they emphasize the changes in the reward function. However, since our work focuses on environments with different dynamics (transition functions), we use the term \u201cenvironments\u201d. In [13], the environment properties are predicted by a neural network based on a \ufb01xed, small number of steps. However, this approach requires explicitly de\ufb01ning the representative environment properties. Moreover, it assumes that these properties can be estimated based on the immediate environmental dynamics. Rasool et al. [14] introduce TD3-context, a TD3-based RL agent that uses a recurrent neural network (RNN) to create a context vector, which receives the recent states and rewards as input. However, even though types of RNNs such as LSTM [15] and GRU [16] are designed for long-term history, in practice, the number of previous states considered by the RNN is limited [7]. Therefore, if an event that de\ufb01nes an environment occurs too early, the RNN will \u201cforget\" it and not provide an accurate context to the policy. In our method, RAMP, the context consists of the parameters of a global, dynamic model, which is not limited by the history length. Other approaches use the RNN directly as a policy, based on the transitions and rewards during the previous episode [6], [17], instead of creating a context vector for a general policy. These approaches are also vulnerable to this RNN memory limitation. Finn et al. [5] proposed a different principle for metalearning termed \u201cModel-Agnostic Meta-Learning (MAML).\" In MAML, the neural network parameters are trained such that the model will be adapted to a new environment by updating all parameters only with a low number of gradientdescent steps. However, the training process of MAML may be challenging [18]. Furthermore, MAML uses onpolicy RL and therefore is unsuitable for the more samplingef\ufb01cient off-policy methods as in our approach. Nevertheless, since MAML can also be used for regression, we compare our multi-environment dynamic model learning method to MAML in Sec. V-A. Some proposed meta-RL methods are suitable for offpolicy learning [14], [19]. Meta-Q-learning (MQL) [14] updates the policy to new environments by using data from multiple previous environments stored in the replay buffer. The transitions from the replay buffer are reweighed to match the current environment. We compare our method, RAMP, to MQL in our testing environment in Sec. V-B.2. As opposed to all these meta-RL methods, which are model-free, also model-based meta-RL methods were proposed. In model-based meta-RL, the agent learns a model that can quickly adapt to the dynamics of a new environment. Ignasi et al. [20] propose to use recurrence-based or gradientbased (MAML) online adaptation for learning the model. Similarly, Lee et al. [21] train a model that is conditioned on the encoded, previous transitions. In contrast to model-free RL, which learns a direct mapping (i.e., a policy) between the state and actions, model-based RL computes the actions by planning (using a model-predictive controller) based on the learned model. In our work, we combine the model-free and model-based approaches resulting in rapid learning of the environment dynamic model and a direct policy without the need for planning. III. PROBLEM DEFINITION We consider a set of N environments that are modeled as a Markov Decision Processes Mk = {S, A, T k, R}, k = {1, . . . , N}. All environments share the same state space S, action space A, and reward function R and differ only by their unknown transition function T . These N environments are randomly split into training environments Mtrain and testing environments Mtest. The meta-RL agent is trained on the Mtrain environments and must adapt separately to each of the Mtest environments. That is, the agent is permitted to interact with the Mtrain environments for an unlimited number of episodes. Then, the meta-RL agent is given only a short opportunity to interact with each of the Mtest environments (e.g., a single episode, a number of time steps, etc.), and update its policy based on this interaction. Overall, the agent\u2019s goal is to maximize the average expected discounted for each of the Mtest environments. IV. RAMP RAMP is constructed in two phases: in the \ufb01rst phase, a multi-environment dynamic model is learned, and in the second phase, the model parameters of the dynamic model are used as context for the multi-environment policy of the reinforcement learning agent. The following sections \ufb01rst describe how the multi-environment dynamic model is learned by exploiting the environments\u2019 common structure. In the second part, we describe the reinforcement learning agent. A. Multi-Environment Dynamic Model Attempting to approximate the transition function of each environment T k by an individual neural network is likely to work well for the training environments. However, it is unlikely to generalize to the testing environments, as we have only a limited set of data points for them. However, since the environments share a common structure, it will be more ef\ufb01cient to train a neural network that has shared components between all the environments. Namely, we intend to train a general neural network based on the training environments such that it can be adapted to each testing environment using only a limited set of data points. In addition, since RAMP\u2019s second phase uses the neural network\u2019s parameters\u2019 values directly as a context for the RL agent, we wish to use only a small number of parameters that should represent the properties of each speci\ufb01c environment dynamics. Therefore, the general neural network shares the vast part of the parameters between all environments and includes only a small set of environment-speci\ufb01c parameters. The environment-speci\ufb01c parameters are, in fact, a compact representation of each environment; therefore, they can be used by RAMP as a context vector (as described in Sec. IV-B). We approximate the transition function of all environments by a neural network with parameters indexed by \u03d5, which are split to environment-speci\ufb01c parameters indexes \u03c9 \u2286 \u03d5, and to the remaining parameters indexes \u03c3 = \u03d5 \\ \u03c9. The values of the parameters of each environment k are denoted by \u02c6\u03d5k and the environment-speci\ufb01c parameters\u2019 values by \u02c6\u03c9k. The shared parameters\u2019 values, which do not depend on a speci\ufb01c environment, are denoted by \u02c6\u03c3. Our multienvironment dynamic model is denoted by f\u02c6\u03c3,\u02c6\u03c9k. The multienvironment dynamic model is given a state s and action a and outputs a prediction of the state at the following time step s\u2032 for each environment Mk, i.e., s\u2032 = f\u02c6\u03c3,\u02c6\u03c9k(s, a). We now describe how to select \u03c9 and how to train the neural network parameters \u02c6\u03c3 and \u02c6\u03c9k. At \ufb01rst, we gather suf\ufb01cient data in the form of Dk = {(s, a, s\u2032)}, for each environment k. At the beginning of the training process \u03c9 = \u2205 and \u03c3 = \u03d5. The network is trained using the gradient descent algorithm to minimize the loss, which is the squared error between the predicted and real next state for each environment: L(Dk, \u02c6\u03c3, \u02c6\u03c9k) = \ufffd s,a,s\u2032\u2208Dk (s\u2032 \u2212 f\u02c6\u03c3,\u02c6\u03c9k(s, a))2. (1) Initially, \u02c6\u03c3 are trained in all environments to achieve an average model prediction: \u02c6\u03c3 = arg min \u02c6\u03c3 |Mtrain| \ufffd k=1 L(Dk, \u02c6\u03c3, \u02c6\u03c9k) (2) After the initial training phase, the parameters \u03c9 are selected from \u03d5 by the algorithm, one-at-a-time. Intuitively, the algorithm should select parameters for \u03c9 that have the greatest impact on the difference between the environments. Therefore, at each gradient step, the gradient of the loss function L(Dk, \u02c6\u03c3, \u02c6\u03c9k) relative to \u02c6\u03d5k is computed for each environment k: gk = \u2207 \u02c6\u03d5kL(Dk, \u02c6\u03c3, \u02c6\u03c9k), (3) and the parameter with the highest variance between all gradients gk is added to \u03c9: \u03c9 \u2190 \u03c9 \u222a arg max i\u2208\u03d5\\\u03c9 var(g0 i , g1 i , . . . , g|Mtrain| i ). (4) Then, the network is trained to minimize the loss function in all environments: min \u02c6\u03c3 |Mtrain| \ufffd k=1 min \u02c6\u03c9k L(Dk, \u02c6\u03c3, \u02c6\u03c9k). (5) That is achieved by updating the environment-speci\ufb01c parameters \u02c6\u03c9k by the corresponding gradient: \u02c6\u03c9k \u2190 \u02c6\u03c9k \u2212 \u03b1\u03c9gk, (6) and updating the shared parameters by the average gradient: \u02c6\u03c3 \u2190 \u02c6\u03c3 \u2212 \u03b1\u03c3 1 |Mtrain| |Mtrain| \ufffd i=0 gi, (7) where \u03b1\u03c9 and \u03b1\u03c3 are the learning rates. During the training, parameters continue to be added to |\u03c9| until it reaches a prede\ufb01ned size n\u03c9. Algorithm 1 summarizes the multienvironment dynamic model learning. Finally, at the end of the training process (after achieving a low loss value), only parameters \u03c9 need to be adjusted for a new environment to get an accurate dynamic model, while parameters \u03c3 remain constant. That is, \u02c6\u03c9k = arg min \u02c6\u03c9k L(Dk, \u02c6\u03c3, \u02c6\u03c9k). (8) Algorithm 1 Model learning with RAMP Require: n\u03c9 \u25b7 Number of environment-speci\ufb01c parameters Require: ninit \u25b7 Number of steps for initial training Require: ntot \u25b7 Number of total training steps Require: \u03b1\u03c9, \u03b1\u03c3 \u25b7 Learning rates Require: {D0, . . . , D|Mtrain|} \u25b7 Data from the environments \u03c9 \u2190 \u2205 for i \u2190 1,number of training steps do for Mk \u2208 Mtrain do sample a batch of transitions bk \u2208 Dk gk \u2190 \u2207 \u02c6\u03d5kL(bk, \u02c6\u03c3, \u02c6\u03c9k) \u02c6\u03c9k \u2190 \u02c6\u03c9k \u2212 \u03b1\u03c9gk \u02c6\u03c3 \u2190 \u02c6\u03c3 \u2212 \u03b1\u03c3 \u00b7 avg(g0, . . . , g|Mtrain|) if |\u03c9| \u2264 n\u03c9 and i > ninit then \u03c9 \u2190 \u03c9 \u222a arg maxi\u2208\u03d5\\\u03c9 var(g0 i , g1 i , . . . , g|Mtrain| i ) B. Reinforcement Learning With Model Parameters Context The multi-environment dynamic model parameters, described in the previous section, are used as a context for the RL agent. That is, RAMP concatenates the environmentspeci\ufb01c parameters\u2019 values \u02c6\u03c9k to the state s for training the RL agent. Unfortunately, the environment-speci\ufb01c parameters \u02c6\u03c9k do not necessarily converge to the same value when trained in the same environment since the amount of these parameters ",
    "Evaluation": "EVALUATION We evaluate RAMP on two domains. The \ufb01rst domain, a sine waves regression test, evaluates the \ufb01rst phase of RAMP alone, i.e., the multi-environment dynamic model learning algorithm. The second domain is the vehicle target-reaching domain, in which vehicles with different dynamics aim to reach a target. The vehicle target-reaching domain tests the complete RAMP algorithm, composed of both phases. A. Sine Waves Regression We used a sine waves regression test similar to [5]. The multi-environment dynamic model was trained on random samples of a sine wave function with different amplitudes A and phases \u03c6: y = A sin (x + \u03c6). (9) The input to the function, x, is sampled uniformly from the range [\u22125, 5]. The amplitudes of the different functions, are sampled from A \u2208 [0.1, 5], and the phases are sampled from \u03c6 \u2208 [0, \u03c0]. The network consists of two fully connected hidden layers, with 40 neurons in each layer and ReLU activation. The size of the environment-speci\ufb01c parameters is limited to 10, i.e. n\u03c9 = 10, out of a total 1761 parameters. Contrary to the dynamic model prediction, which receives an action in addition to the current state to predict the next state, in this simple sine regression problem, there is a single input and a single output. The multi-environment model Algorithm 2 RAMP (using TD3) Require: ntot \u25b7 Number training steps Require: Mtrain \u25b7 Training environments Require: \u02c6\u03c9k, k = {1, |Mtrain|} Initialize critic, actor, and replay buffer B Add \u02c6\u03c9k to \u2126k for all k = {1, |Mtrain|} while i < ntot do Select environment Mk from Mtrain randomly Select parameters \u02c6\u03c9k from \u2126k randomly while not done do Observe s Execute action a = \u03c0\u03a6(s, \u02c6\u03c9k) + \u03f5), \u03f5 \u223c N(0, \u03c3) Observe new state s\u2032, reward r and done \ufb02ag d Add (s, a, s\u2032, r, d, k) to replay buffer B if i mod H = 0 then for all Mk \u2208 Mtrain do Select random parameters \u02c6\u03c9k from \u2126k Sample one episode from Mk: Dk \u2190 {(st, \u03c0(st, \u02c6\u03c9k), st+1)}t={1,T } Retrain \u02c6\u03c9k new with Dk and add to \u2126k Sample random batch b \u2282 B for all (s, a, s\u2032, r, d, k) \u2208 b do Sample \u02c6\u03c9k from \u2126k Set s\u2032 \u2190 (s\u2032, \u02c6\u03c9k) Set s \u2190 (s, \u02c6\u03c9k) Update critics using b if i mod ntraining = 0 then Update actor using b i \u2190 i + 1 was trained on 100 random sine waves with 10 samples each. It was then retrained by updating only environmentspeci\ufb01c parameters, \u03c9, on 10 samples of new sine waves. We compare the multi-environment dynamic model of RAMP to a small network composed of only 10 parameters trained on each new sine wave separately and to MAML [5], which updates the entire network (1761 parameters). The multi-environment dynamic model achieved a Mean Squared Error (MSE) of 0.021. This result is slightly lower than MAML, which achieved an MSE of 0.037. Nevertheless, since MAML uses 1761 parameters, it is impractical to use them as a context for the RL agent. As expected, the network that contains only 10 parameters resulted in a very high average MSE, 19.25. When training on all sine waves together (i.e., all model\u2019s parameters are shared without environment-speci\ufb01c parameters), the MSE was 1.9. Figure 1 depicts the performance of the multi-environment dynamic model of RAMP on the test set. B. Vehicle Target-Reaching Domain The vehicle target-reaching domain is a simple domain that enables us to provide a precise analysis of RAMP\u2019s behavior and demonstrate the concepts behind RAMP. In this domain, an agent controls the vehicle\u2019s throttle and brake and aims to reach a target line in a minimum time. The vehicle 4 2 0 2 4 4 3 2 1 0 1 2 3 4 Fig. 1: Evaluation of sine functions with different amplitudes and phases. The solid lines represent the ground-truth functions, and the dots are the predictions. must reach the target line at a speed of at most vmax. The state, s = {v, d}, consists of the current vehicle\u2019s speed, v, and the distance to the target d. v ranges from 0 to 30 m/s, the distance to the target at the beginning of the episode is d = 40 m, and the desired maximal speed at the target line vmax is 5 m/s. The continuous throttle/brake action, a ranges from \u22121 to 1. The sampling frequency is 25 Hz. The reward function returns \u22120.002 at each non-terminal step. When approaching the target line with a higher speed, than vmax the reward is 0.01(v \u2212 vmax)2 otherwise 0. We construct 24 vehicle target-reaching environments, split into 22 for training the multi-environment model and two for testing. All vehicles from the different environments have identical acceleration but a different deceleration, which is unknown to the agent. Speci\ufb01cally, the throttle command a = [0, 1] causes an acceleration value \u02d9v = [0, 42] m/s2 in all environments. However, the brake command a = [\u22121, 0) causes a deceleration value \u02d9v = [0, 42ka] that is scaled down by the braking factor ka, which has a value between 0.1 and 1. The braking factor in the test environment is ka = 0.925 and ka = 0.175, which are close to the extremes of all factors. We begin by evaluating the performance of the multienvironment dynamic model learning process, and then we evaluate the performance of the RL learning procedure. Finally, we show the adaptation process in a new environment. 1) Multi-Environment Dynamic Model Learning: The multi-environment neural network is identical to the network used for the sine wave regression. The dynamic model state consists only of the vehicle\u2019s speed, and the network predicts the difference between the current and new states. In each environment, 100 points are randomly sampled for training, and only 10 points are sampled from new environments for the adaptation process. Figure 2 shows speeds of 3 different vehicles. All accelerate at the same rate until reaching the maximal speed, and then, each vehicle applies a maximal braking action (a = \u22121), resulting in different deceleration values. The points represent the predicted speeds, and the solid lines are the real speeds. Our multi-environment dynamic model results in an MSE of 0.00029 on new environments compared to an MSE of 0.045 when trained on all environments together. 0 1 2 3 4 5 Time [s] 0 5 10 15 20 25 30 Velocity [m/s] Fig. 2: Speeds prediction of different vehicles. Points are the predictions, and solid lines are the ground-truth speeds. Recall that the environment-speci\ufb01c parameters \u03c9 are retrained multiple times during the RL training process as described in Sec. IV-B. Figure 3 shows the values of each of the 10 parameters for every environment, in different colors. The different parameter sets are slightly shifted along the horizontal axis. As depicted by the \ufb01gure, the environmentspeci\ufb01c parameters converged to similar values; this can be seen by the consistency of the values between the parameter sets. In addition, the \ufb01gure shows that the different environments result in noticeable, different values for the \ufb01rst three parameters. In contrast, the remaining parameters show only a minor variance between the environments. This result seems reasonable, since not all parameters are required to determine the variance of the vehicle dynamics, which in fact, has only one degree of freedom. 1 2 3 4 5 6 7 8 9 10 Parameter index 1.0 0.5 0.0 0.5 1.0 Parameter Value Fig. 3: Values of the 10 environment-speci\ufb01c parameters. Different colors represent different environments. Multiple parameter sets are shown with a shift along the horizontal axis. Recall that in the RL training phase, the general policy must extract the properties of the environments from only the environment-speci\ufb01c parameters. Therefore, beyond the low loss of the prediction, we tested that it is possible to directly predict the braking factor from the environmentspeci\ufb01c parameters. To that end, we trained a dedicated regressor, which is not used by RAMP, on 58 sets of the trained environment-speci\ufb01c parameters created during the RL training process. 40 environments were used as a training set, and the 18 remaining environments were used as a test set. The regressor is composed of a neural network with two hidden layers with 100 neurons each. Figure 4 shows the prediction error distribution of the braking factor. As depicted by the \ufb01gure, the regressor predicts the braking factor, which ranges from 0.1 to 1.0, with an average error of \u22120.0077. 0.10 0.05 0.00 0.05 0.10 0.15 Prediction error ka 0 10 20 30 40 50 60 70 Fig. 4: Error distribution of predicting the real braking factor of environment k, based only on the environment-speci\ufb01c parameters \u02c6\u03c9k. 2) Multi-Environment Reinforcement Learning: We compared RAMP to the following 3 other RL agents. The Oracle RL receives explicit information about the vehicle; that is, the braking factor is added to the state. With full knowledge of the environmental properties, the Oracle RL is expected to \ufb01nd a nearly optimal solution. Next, we consider a basic RL that is trained in all environments together, without any identi\ufb01cation input, and thus cannot distinguish between different vehicles. Therefore, it is expected to learn a conservative policy that enables safe deceleration to the target line, even for the vehicle with the lowest braking capability. The third RL agent is the meta-Q-learning (MQL) algorithm [14]. The training process of all methods was repeated 5 times with different random seeds and is shown in Fig. 5. Our method\u2019s performance during the training process is comparable to the Oracle RL, achieving consistently higher episode rewards than the basic RL and MQL. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training steps 1e6 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 Reward Oracle RL Basic RL RAMP MQL Fig. 5: Comparison between the training processes. RAMP is close to the Oracle RL. Table I summarizes the average performance at the end of the training procedure. The table shows for all agents: the average reward in both test environments, the time to reach the target line by the vehicles with a low and high braking factor, and the average time. As depicted by the table, RAMP reaches an average reward that is very close to the Oracle\u2019s and also has a very similar average time. Average Reward Low ka Time [s] High ka Time [s] Average Time [s] Basic RL -0.1656 3.160 3.376 3.268 Oracle RL -0.1226 3.0 1.976 2.488 RAMP -0.1230 3.048 2.04 2.544 MQL -0.1510 3.18 2.62 2.90 TABLE I: The performance of all RL agents on the test environments. The table shows for each agent: 1. The loss, averaged over the test episodes following the 5 separate training processes; 2. The average time achieved by the vehicle with the low braking factor; 3. The average time achieved by the vehicle with the high braking factor; 4. The average between these times. Next, we analyze the speed pro\ufb01les of different vehicles driven by policies trained by the different RL agents. The speed pro\ufb01le of the basic RL, Oracle RL, RAMP, and MQL are shown in Figures 6a, 6b, 6c, and 6d respectively. The orange lines represent speed pro\ufb01les of vehicles with a high braking factor ka = 0.925, and the blue lines represent low braking factors ka = 0.175. These values are close to the extremes of the braking factor range to demonstrate the difference between the environments. The bold columns represent the maximal permitted speed at the target for each of the two environments. As depicted by Fig. 6a, the basic RL begins to brake on both vehicles at the same point in time. This happens because the agent cannot know if the vehicle has a higher braking capability that allows braking later or not, which leads to a conservative policy. As shown in Fig. 6b, the Oracle RL begins braking on time in both environments and arrives at the destination at the required maximum target speed. As shown in Fig. 6c, RAMP results in a similar speed pro\ufb01le as the Oracle RL. However, unlike the Oracle agent, RAMP does not receive any explicit information about the environment; instead, it learns this information from the trajectory sampled during one episode. Figure 6d illustrates that the MQL agent can distinguish between the vehicles\u2019 braking differences because the vehicle with the higher braking factor is allowed to gain more speed. However, MQL\u2019s speed pro\ufb01le is not as good as RAMP\u2019s since the MQL agent does not accelerate and decelerate at the maximal values, therefore resulting in longer driving times. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Time [s] 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Velocity [m/s] ka = 0.175 ka = 0.925 (a) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Time [s] 0 5 10 15 20 25 30 Velocity [m/s] ka = 0.175 ka = 0.925 (b) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Time [s] 0 5 10 15 20 25 30 Velocity [m/s] ka = 0.175 ka = 0.925 (c) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Time [s] 0 5 10 15 20 25 Velocity [m/s] ka = 0.175 ka = 0.925 (d) Fig. 6: Speed pro\ufb01les that were achieved by our method and the other agents. Blue - low braking factor, orange - high braking factor, bold line - the maximal desired speed at the target. (a) The basic RL agent resulted in a conservative solution. (b) The Oracle RL agent achieved optimal speed pro\ufb01les. (c) Our agent, RAMP, achieves similar optimal results without prior knowledge about the vehicle dynamics. (d) MQL resulted in sub-optimal results compared to RAMP. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Time [s] 0 5 10 15 20 25 30 Velocity [m/s] Before After (a) 0.0 0.5 1.0 1.5 2.0 Time [s] 0 5 10 15 20 25 30 Velocity [m/s] Before After (b) Fig. 7: RAMP evaluation during two subsequent episodes. The dashed line represents the speed pro\ufb01le used for collecting data in the \ufb01rst episode and the solid line in the second. (a) Low braking factor: the agent learned that this vehicle must brake earlier. (b) High braking factor: the agent learned that this vehicle can brake later. To conclude the evaluation of RAMP\u2019s performance in the target-reaching domain, we analyze RAMP\u2019s adaptation process. As opposed to the Oracle RL, which is given the braking factor information, RAMP must learn it from the driving experience. That is, in the \ufb01rst episode, RAMP collects data points, and the environment-speci\ufb01c parameters are trained on it; in the second episode, RAMP drives the vehicle with the updated context. Figure 7 shows the speed pro\ufb01le of a vehicle during two subsequent episodes for the low braking factor vehicle (Fig. 7a) and for the high breaking-factor (Fig. 7b). As depicted by Fig. 7a, in the \ufb01rst episode (represented by the dashed line), the vehicle brakes too late and therefore crosses the target line at too high a speed. In the second episode (represented by a solid line), the vehicle brakes earlier and cross the target line at a speed that is within the speed limit. Similarly, for the vehicle with a higher braking factor, RAMP learns that the vehicle can brake later. Therefore, in the second episode, it crosses the \ufb01nish line earlier than in the \ufb01rst episode. VI. CONCLUSIONS AND FUTURE WORK This paper presented RAMP, a novel meta-reinforcement learning algorithm. RAMP is constructed in two phases: learning a multi-environment dynamic model and training a general reinforcement learning policy that uses the ",
    "References": "REFERENCES [1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [2] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al., \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, no. 6419, pp. 1140\u20131144, 2018. [3] F. Fuchs, Y. Song, E. Kaufmann, D. Scaramuzza, and P. D\u00fcrr, \u201cSuperhuman performance in gran turismo sport using deep reinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 4257\u20134264, 2021. [4] G. Hartmann, Z. Shiller, and A. Azaria, \u201cDeep reinforcement learning for time optimal velocity control using prior knowledge,\u201d in 2019 IEEE 31st International Conference on Tools with Arti\ufb01cial Intelligence (ICTAI). IEEE, 2019, pp. 186\u2013193. [5] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d in International conference on machine learning. PMLR, 2017, pp. 1126\u20131135. [6] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, \u201cRl2: Fast reinforcement learning via slow reinforcement learning,\u201d arXiv preprint arXiv:1611.02779, 2016. [7] A. Graves, G. Wayne, and I. Danihelka, \u201cNeural turing machines,\u201d arXiv preprint arXiv:1410.5401, 2014. [8] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, \u201cPlaying atari with deep reinforcement learning,\u201d arXiv preprint arXiv:1312.5602, 2013. [9] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d nature, vol. 529, no. 7587, pp. 484\u2013489, 2016. [10] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani, and P. P\u00e9rez, \u201cDeep reinforcement learning for autonomous driving: A survey,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 6, pp. 4909\u20134926, 2022. [11] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al., \u201cMastering the game of go without human knowledge,\u201d nature, vol. 550, no. 7676, pp. 354\u2013359, 2017. [12] F.-H. Hsu, Behind Deep Blue: Building the computer that defeated the world chess champion. Princeton University Press, 2002. [13] W. Yu, J. Tan, C. K. Liu, and G. Turk, \u201cPreparing for the unknown: Learning a universal policy with online system identi\ufb01cation,\u201d in Robotics: Science and Systems XIII, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017, 2017. [Online]. Available: http://www.roboticsproceedings.org/rss13/ p48.html [14] R. Fakoor, P. Chaudhari, S. Soatto, and A. J. Smola, \u201cMeta-q-learning,\u201d in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SJeD3CEFPH [15] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. [16] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014. [17] J. X. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos, C. Blundell, D. Kumaran, and M. M. Botvinick, \u201cLearning to reinforcement learn,\u201d in CogSci, 2017. [Online]. Available: https://mindmodeling.org/cogsci2017/papers/0252/index.html [18] A. Antoniou, H. Edwards, and A. Storkey, \u201cHow to train your MAML,\u201d in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/ forum?id=HJGven05Y7 [19] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, \u201cEf\ufb01cient offpolicy meta-reinforcement learning via probabilistic context variables,\u201d in International conference on machine learning. PMLR, 2019, pp. 5331\u20135340. [20] I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, \u201cLearning to adapt in dynamic, real-world environments through meta-reinforcement learning,\u201d in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=HyztsoC5Y7 [21] K. Lee, Y. Seo, S. Lee, H. Lee, and J. Shin, \u201cContext-aware dynamics model for generalization in model-based reinforcement learning,\u201d in Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 13\u201318 Jul 2020, pp. 5757\u20135766. [Online]. Available: https://proceedings.mlr.press/v119/lee20g.html [22] S. Fujimoto, H. Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d in International conference on machine learning. PMLR, 2018, pp. 1587\u20131596. ",
    "title": "Meta-Reinforcement Learning Using Model Parameters",
    "paper_info": "Meta-Reinforcement Learning Using Model Parameters\nGabriel Hartmann1,2 and Amos Azaria2\nAbstract\u2014 In\nmeta-reinforcement\nlearning,\nan\nagent\nis\ntrained in multiple different environments and attempts to learn\na meta-policy that can ef\ufb01ciently adapt to a new environment.\nThis paper presents RAMP, a\nReinforcement learning Agent\nusing Model Parameters that utilizes the idea that a neural\nnetwork trained to predict environment dynamics encapsulates\nthe environment information. RAMP is constructed in two\nphases: in the \ufb01rst phase, a multi-environment parameterized\ndynamic model is learned. In the second phase, the model\nparameters of the dynamic model are used as context for the\nmulti-environment policy of the model-free reinforcement learn-\ning agent. We show the performance of our novel method in\nsimulated experiments and compare them to existing methods.\nI. INTRODUCTION\nCommon approaches for developing controllers do not rely\non machine learning. Instead, engineers manually construct\nthe controller based on general information about the world\nand the problem. After repetitively testing the controller in\nthe environment, the engineer improves the controller based\non the feedback from these tests. That is, a human is an\nessential part of this iterative process. Reinforcement Learn-\ning (RL) reduces human effort by automatically learning\nfrom interaction with the environment. Instead of explicitly\ndesigning and improving a controller, the engineer develops\na general RL agent that learns to improve the controller\u2019s\nperformance without human intervention. The RL agent is\nusually general and does not include speci\ufb01c information\nabout the target environment; this allows it to adapt to dif-\nferent environments. Indeed, RL agents may achieve higher\nperformance compared to human-crafted controllers [1]\u2013[3].\nHowever, RL agents usually require training from the ground\nup for every new environment, which requires extensive\ninteraction in the new environment.\nOne solution to speed up the training time is to explicitly\nprovide human-crafted information about the environment\n(context) to the RL agent [4]. However, such a solution\nrequires explicitly analyzing the target environment, which\nmay be challenging and time-consuming.\nInstead of relying on the human understanding of the\nproblem for providing such context, a meta-Reinforcement\nLearning (meta-RL) agent can learn to extract a proper envi-\nronmental context. To that end, a meta-RL agent is trained on\nextended interaction in multiple different environments, and\nthen, after a short interaction in a new, unseen environment, it\nThis research was supported, in part, by the Ministry of Science &\nTechnology, Israel.\n1 Department of Mechanical Engineering and Mechatronics, Ariel Uni-\nversity, Israel\n2 Department of Computer Science, Ariel University, Israel\ngabrielh@ariel.ac.il, amos.azaria@ariel.ac.il\nis required to perform well in it [5], [6]. Speci\ufb01cally, a meta-\nRL algorithm that is based on context extraction is composed\nof two phases. First, in the meta-learning phase, the agent\nlearns a general policy suitable to all environments given a\ncontext. Additionally, in this phase, the meta-RL agent learns\nhow to extract a context from samples obtained from an\nenvironment. Secondly, in the adaptation phase, the meta-RL\nagent conducts a short interaction in the new environment,\nand the context is extracted from it. This context is then fed\nto the general policy, which acts in the new environment.\nOne common approach for context extraction is using\na Recurrent Neural Network (RNN). That is, the RNN\nreceives the history of the states, actions, and rewards and\nis trained to output a context that is useful for the general\npolicy. However, the RNN long-term memory capability\nusually limits the effective history length [7]. Additionally,\nsince the context vector is not explicitly explainable, it is\ndif\ufb01cult to examine the learning process and understand if\nthe RNN learned to extract the representative properties of\nthe environments.\nIn this paper, we introduce RAMP \u2013 a Reinforcement\nlearning Agent using Model Parameters. We utilize the idea\nthat a neural network trained to predict environment dynam-\nics encapsulates the environment properties; therefore, its pa-\nrameters can be used as the context for the policy. During the\nmeta-RL phase, RAMP learns a neural network that predicts\nthe environment dynamic for each environment. However,\nsince the number of the neural network\u2019s parameters is\nusually high, it is challenging for the policy to use the entire\nset of parameters as its context. Therefore, the majority of\nthe model\u2019s parameters are shared between all environments,\nand only a small set of parameters are trained separately\nin each environment. In that way, the environment-speci\ufb01c\nparameters represent the speci\ufb01c environment properties.\nConsequently, a general policy uses only these parameters as\ncontext and outputs actions that are suitable for that particular\nenvironment. One advantage of RAMP is that the history\nlength used for the context extraction is not limited because\nthe context is extracted from a global dynamic model.\nAdditionally, the combination of model learning and RL in\nRAMP makes the training process more transparent since it\nis possible to evaluate the performance of the model learning\nprocess independently. We demonstrate the effectiveness of\nRAMP in several simulated experiments in Sec. V.\nTo summarize, the contributions of this paper are:\n\u2022 Suggesting a novel method for meta-reinforcement\nlearning.\n\u2022 Presenting a multi-environment dynamic model learning\nmethod that adapts to new environments by updating\narXiv:2210.15515v1  [cs.LG]  27 Oct 2022\n",
    "GPTsummary": "                     - (1): The background of this paper lies in the fact that, while reinforcement learning (RL) reduces human effort by learning from interaction with the environment, RL agents usually require extensive interaction in new environments. Meta-reinforcement learning (meta-RL) agents address this by learning to extract a proper environmental context and adapt to new environments.\n \n                    - (2): This paper proposes a novel method for meta-RL called RAMP that utilizes the idea that a neural network trained to predict environment dynamics encapsulates the environment properties, and its parameters can be used as context for the policy. This overcomes the limitations of previous methods using Recurrent Neural Networks (RNNs) long-term memory capability and lack of explicit explainability. The proposed approach is well motivated and has advantages such as using a global dynamic model for context extraction and allowing for transparent evaluation of the performance of the model learning process. \n \n                    - (3): RAMP is composed of two phases: in the first phase, a multi-environment parameterized dynamic model is learned, and in the second phase, the model parameters of the dynamic model are used as context for the multi-environment policy of the model-free reinforcement learning agent. This methodology effectively utilizes the neural network trained in the first phase and allows for efficient adaptation to new environments. \n \n                    - (4): The performance of RAMP is demonstrated in several simulated experiments, where it achieves superior performance compared to other meta-RL methods. The results support the objectives of the proposed approach and show the potential of RAMP to improve the efficiency of adapting to new environments in reinforcement learning.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work proposes a novel method for meta-reinforcement learning called RAMP that utilizes the idea that a neural network trained to predict environment dynamics encapsulates the environment properties, and its parameters can be used as context for the policy. This overcomes the limitations of previous methods using RNN's long-term memory capability and lack of explicit explainability. The proposed approach is well motivated and has advantages such as using a global dynamic model for context extraction and allowing for transparent evaluation of the model learning process.\n \n- (2): Innovation point: RAMP utilizes the parameters of a neural network trained to predict environment dynamics as context for the policy, which is a novel and effective approach that overcomes limitations of previous methods. Performance: The RAMP approach achieved superior performance compared to other meta-RL methods in several simulated experiments. Workload: The proposed methodology is well motivated and has advantages such as using a global dynamic model for context extraction and allowing for transparent evaluation of the performance of the model learning process. However, the practical implementation of the approach still needs to be further explored.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work proposes a novel method for meta-reinforcement learning called RAMP that utilizes the idea that a neural network trained to predict environment dynamics encapsulates the environment properties, and its parameters can be used as context for the policy. This overcomes the limitations of previous methods using RNN's long-term memory capability and lack of explicit explainability. The proposed approach is well motivated and has advantages such as using a global dynamic model for context extraction and allowing for transparent evaluation of the model learning process.\n \n- (2): Innovation point: RAMP utilizes the parameters of a neural network trained to predict environment dynamics as context for the policy, which is a novel and effective approach that overcomes limitations of previous methods. Performance: The RAMP approach achieved superior performance compared to other meta-RL methods in several simulated experiments. Workload: The proposed methodology is well motivated and has advantages such as using a global dynamic model for context extraction and allowing for transparent evaluation of the performance of the model learning process. However, the practical implementation of the approach still needs to be further explored.\n\n\n"
}