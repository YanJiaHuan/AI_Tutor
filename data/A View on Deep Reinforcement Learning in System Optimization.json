{
    "Abstract": "ABSTRACT Many real-world systems problems require reasoning about the long term consequences of actions taken to con\ufb01gure and manage the system. These problems with delayed and often sequentially aggregated reward, are often inherently reinforcement learning problems and present the opportunity to leverage the recent substantial advances in deep reinforcement learning. However, in some cases, it is not clear why deep reinforcement learning is a good \ufb01t for the problem. Sometimes, it does not perform better than the state-of-the-art solutions. And in other cases, random search or greedy algorithms could outperform deep reinforcement learning. In this paper, we review, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. We propose a set of essential metrics to guide future works in evaluating the ef\ufb01cacy of using deep reinforcement learning in system optimization. Our evaluation includes challenges, the types of problems, their formulation in the deep reinforcement learning setting, embedding, the model used, ef\ufb01ciency, and robustness. We conclude with a discussion on open challenges and potential directions for pushing further the integration of reinforcement learning in system optimization. 1 ",
    "Introduction": "INTRODUCTION Reinforcement learning (RL) is a class of learning problems framed in the context of planning on a Markov Decision Process (MDP) (Bellman, 1957), when the MDP is not known. In RL, an agent continually interacts with the environment (Kaelbling et al., 1996; Sutton et al., 2018). In particular, the agent observes the state of the environment, and based on this observation takes an action. The goal of the RL agent is then to compute a policy\u2013a mapping between the environment states and actions\u2013that maximizes a long term reward. There are multiple ways to extrapolate the policy. Non-approximation methods usually fail to predict good actions in states that were not visited in the past, and require storing all the action-reward pairs for every visited state, a task that incurs a huge memory overhead and complex computation. Instead, approximation methods have been proposed. Among the most successful ones is using a neural network in conjunction with RL, also known as deep RL. Deep models allow RL algorithms to solve complex problems in an end-to-end fashion, handle unstructured environments, learn complex functions, or predict actions in states that have not been visited in the past. Deep RL is gaining wide interest recently due to its success in robotics, Atari games, and superhuman capabilities (Mnih et al., 2013; Doya, 2000; Kober et al., 2013; Peters et al., 2003). Deep RL was the key technique behind defeating the human European champion in the game of Go, which has long been viewed as the most challenging of classic games for arti\ufb01cial intelligence (Silver et al., 2016). 1Intel Labs 2University of California, Berkeley. Correspondence to: Ameer Haj-Ali <ameerh@berkeley.edu>. Many system optimization problems have a nature of delayed, sparse, aggregated or sequential rewards, where improving the long term sum of rewards is more important than a single immediate reward. For example, an RL environment can be a computer cluster. The state could be de\ufb01ned as a combination of the current resource utilization, available resources, time of the day, duration of jobs waiting to run, etc. The action could be to determine on which resources to schedule each job. The reward could be the total revenue, jobs served in a time window, wait time, energy ef\ufb01ciency, etc., depending on the objective. In this example, if the objective is to minimize the waiting time of all jobs, then a good solution must interact with the computer cluster and monitor the overall wait time of the jobs to determine good schedules. This behavior is inherent in RL. The RL agent has the advantage of not requiring expert labels or knowledge and instead the ability to learn directly from its own interaction with the world. RL can also learn sophisticated system characteristics that a straightforward solution like \ufb01rst come \ufb01rst served allocation scheme cannot. For instance, it could be better to put earlier long-running arrivals on hold if a shorter job requiring fewer resources is expected shortly. In this paper, we review different attempts to overcome system optimization challenges with the use of deep RL. Unlike previous reviews (Hameed et al., 2016; Mahdavinejad et al., 2018; Krauter et al., 2002; Wang et al., 2018; Ashouri et al., 2018; Luong et al., 2019) that focus on machine learning methods without discussing deep RL models or applying them beyond a speci\ufb01c system problem, we focus on deep RL in system optimization in general. From reviewing prior work, it is evident that standardized metrics for assessing arXiv:1908.01275v3  [cs.LG]  4 Sep 2019 ",
    "Background": "BACKGROUND One of the promising machine learning approaches is reinforcement learning (RL), in which an agent learns by continually interacting with an environment (Kaelbling et al., 1996). In RL, the agent observes the state of the environment, and based on this state/observation takes an action as illustrated in \ufb01gure 1. The ultimate goal is to compute a policy\u2013a mapping between the environment states and actions\u2013that maximizes expected reward. RL can be viewed as a stochastic optimization solution for solving Markov Decision Processes (MDPs) (Bellman, 1957), when the MDP is not known. An MDP is de\ufb01ned by a tuple with four elements: S, A, P(s, a), r(s, a) where S is the set of states of the environment, A describes the set of actions or transitions between states, s\u2032\u223cP(s, a) describes the probability distribution of next states given the current state and action and r(s, a) : S \u00d7 A \u2192 R is the reward of taking action a in state s. Given an MDP, the goal of the agent is to gain the largest possible cumulative reward. The objective of an RL algorithm associated with an MDP is to \ufb01nd a decision policy \u03c0\u2217(a|s) : s \u2192 A that achieves this goal for that MDP: \u03c0\u2217 = arg max \u03c0 E\u03c4\u223c\u03c0(\u03c4) [\u03c4] = arg max \u03c0 E\u03c4\u223c\u03c0(\u03c4) \ufffd\ufffd t r(st, at) \ufffd , (1) where \u03c4 is a sequence of states and actions that de\ufb01ne a single episode, and T is the length of that episode. Deep RL leverages a neural network to learn the policy (and sometimes the reward function). Over the past couple of years, a plethora of new deep RL techniques have been proposed (Mnih et al., 2016; Ross et al., 2011; Sutton et al., 2000; Schulman et al., 2017; Lillicrap et al., 2015). Policy Gradient (PG) (Sutton et al., 2000), for example, uses a neural network to represent the policy. This policy is updated directly by differentiating the term in Equation 1 as follows: \u2207\u03b8J = \u2207\u03b8E\u03c4\u223c\u03c0(\u03c4) \ufffd\ufffd t r(st, at) \ufffd = E\u03c4\u223c\u03c0(\u03c4) \ufffd\ufffd\ufffd t \u2207\u03b8log\u03c0\u03b8(at|st) \ufffd \ufffd\ufffd t r(st, at) \ufffd\ufffd \u2248 1 N N \ufffd i=1 \ufffd\ufffd\ufffd t \u2207\u03b8log\u03c0\u03b8(ai,t|si,t) \ufffd \ufffd\ufffd t r(si,t, ai,t) \ufffd\ufffd (2) and updating the network parameters (weights) in the direction of the gradient: \u03b8 \u2190 \u03b8 + \u03b1\u2207\u03b8J, (3) Proximal Policy Optimization (PPO) (Schulman et al., 2017) improves on top of PG for more deterministic, stable, and robust behavior by limiting the updates and ensuring the deviation from the previous policy is not large. In contrast, Q-Learning (Watkins et al., 1992), state-actionreward-state-action (SARSA) (Rummery et al., 1994) and deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) are temporal difference methods, i.e., they update the policy on every timestep (action) rather than on every episode. Furthermore, these algorithms bootstrap and, instead of using a neural network for the policy itself, they learn a Q-function, which estimates the long term reward from taking an action. The policy is then de\ufb01ned using this Q-function. In Q-Learning the Q-function is updated as follows: Q(st, at) \u2190 Q(st, at)+r(st, at)+\u03b3maxa\u2032 t[Q(s\u2032 t, a\u2032 t)]. (4) In other words, the Q-function updates are performed based on the action that maximizes the value of that Q-function. On the other hand, in SARSA, the Q-function is updated as follows: Q(st, at) \u2190 Q(st, at) + r(st, at) + \u03b3Q(st+1, at+1). (5) In this case, the Q-function updates are performed based on the action that the policy would select given state st. DDPG \ufb01ts multiple neural networks to the policy, including the Q-function and target time-delayed copies that slowly track the learned networks and greatly improve stability in learning. A View on Deep Reinforcement Learning in System Optimization Algorithms such as upper-con\ufb01dence-bound and greedy can then be used to determine the policy based on the Qfunction (Auer, 2002; Sutton et al., 2018). The reviewed works in this paper focus on the epsilon greedy method where the policy is de\ufb01ned as follows: \u03c0\u2217(at|st) = \ufffd arg maxat Q(st, at), w.p. 1 \u2212 \u03f5 random action, w.p. \u03f5 (6) A method is considered to be on-policy if the new policy is computed directly from the decisions made by the current policy. PG, PPO, and SARSA are thus on-policy while DDPG and Q-Learning are off-policy. All the mentioned methods are model-free: they do not require a model of the environment to learn, but instead learn directly from the environment by trial and error. In some cases, a model of the environment could be available. It may also be possible to learn a model of the environment. This model could be used for planning and enable more robust training as less interaction with the environment may be required. Most RL methods considered in this review are structured around value function estimation (e.g., Q-values) and using gradients to update the policy. However, this is not always the case. For example, genetic algorithms, simulated annealing, genetic programming, and other gradient-free optimization methods - often called evolutionary methods (Sutton et al., 2018) - can also solve RL problems in a manner analogous to the way biological evolution produces organisms with skilled behavior. Evolutionary methods can be effective if the space of policies is suf\ufb01ciently small, the policies are common and easy to \ufb01nd, and the state of the environment is not fully observable. This review considers only the deep versions of these methods, i.e., using a neural network in conjunction with evolutionary methods typically used to evolve and update the neural network parameters or vice versa. Multi-armed bandits (Berry et al., 1985; Auer et al., 2002) simplify RL by removing the learning dependency on state and thus providing evaluative feedback that depends entirely on the action taken (1-step RL problems). The actions usually are decided upon in a greedy manner by updating the bene\ufb01t estimates of performing each action independently from other actions. To consider the state in a bandit solution, contextual bandits may be used (Chu et al., 2011). In many cases, a bandit solution may perform as well as a more complicated RL solution or even better. Many Bandit algorithms enjoy stronger theoretical guarantees on their performance even under adversarial settings. These bounds would likely be of great value to the systems world as they suggest in the limit that the proposed algorithm would be no worse than using the best \ufb01xed system con\ufb01guration in hindsight. 2.1 Prior RL Works With Alternative Approximation Methods Multiple prior works have proposed to use non-deep neural network approximation methods for RL in system optimization. These works include reliability and monitoring (Das et al., 2014; Zhu et al., 2007; Zeppenfeld et al., 2008), memory management (Ipek et al., 2008; Andreasson et al., 2002; Peled et al., 2015; Diegues et al., 2014) in multicore systems, congestion control (Li et al., 2016; Silva et al., 2016), packet routing (Choi et al., 1996; Littman et al., 2013; Boyan et al., 1994), algorithm selection (Lagoudakis et al., 2000), cloud caching (Sadeghi et al., 2017), energy ef\ufb01ciency (Farahnakian et al., 2014) and performance (Peng et al., 2015; Jamshidi et al., 2015; Barrett et al., 2013; Arabnejad et al., 2017; Mostafavi et al., 2018). Instead of using a neural network to approximate the policy, these works used tables, linear approximations, and other approximation methods to train and represent the policy. Tables were generally used to store the Q-values, i.e., one value for each action, state pair, which are used in training, and this table becomes the ultimate policy. In general, deep neural networks allowed for more complex forms of policies and Q functions (Lin, 1993), and can better approximate good actions in new states. 3 RL IN SYSTEM OPTIMIZATION In this section, we discuss the different system challenges tackled using RL and divide them into two categories: Episodic Tasks, in which the agent-environment interaction naturally breaks down into a sequence of separate terminating episodes, and Continuing Tasks, in which it does not. For example, when optimizing resources in the cloud, the jobs arrive continuously and there is not a clear termination state. But when optimizing the order of SQL joins, the query has a \ufb01nite number of joins, and thus after enough steps the agent arrives at a terminating state. 3.1 Continuing Tasks An important feature of RL is that it can learn from sparse reward signals, does not need expert labels, and the ability to learn direction from its own interaction with the world. Jobs in the cloud arrive in an unpredictable and continuous manner. This might explain why many system optimization challenges tackled with RL are in the cloud (Mao et al., 2016; He et al., 2017a;b; Tesauro et al., 2006; Xu et al., 2017; Liu et al., 2017; Xu et al., 2012; Rao et al., 2009). A good job scheduler in the cloud should make decisions that are good in the long term. Such a scheduler should sometimes forgo short term gains in an effort to realise greater long term bene\ufb01ts. For example, it might be better to delay a long running job if a short running job is expected to arrive soon. The scheduler should also adapt to variations in the underlying resource performance and scale in the presence of new or unseen workloads combined with large numbers of resources. A View on Deep Reinforcement Learning in System Optimization These schedulers have a variety of objectives, including minimizing average performance of jobs and optimizing the resource allocation of virtual machines (Mao et al., 2016; Tesauro et al., 2006; Xu et al., 2012; Rao et al., 2009), optimizing data caching on edge devices and base stations (He et al., 2017a;b), and maximizing energy ef\ufb01ciency (Xu et al., 2017; Liu et al., 2017). The RL algorithms used for addressing each system problem are listed in Table 1 lists the RL algorithms used for addressing each problem. Interestingly, for cloud challenges most works are driven by Q-learning (or the very similar SARSA). In the absence of a complete environmental model, model-free Q-Learning can be used to generate optimal policies. It is able to make predictions incrementally by bootstrapping the current estimate with previous estimates and provide good sample ef\ufb01ciency (Jin et al., 2018). Q-Learning is also characterized by inherent continuous temporal difference behavior where the policy can be updated immediately after each step (not the end of trajectory); something that might be very useful for online adaptation. 3.2 Episodic Tasks Due to the sequential nature of decision making in RL, the order of the actions taken has a major impact on the rewards the RL agent collects. The agent can thus learn these patterns and select more rewarding actions. Previous works took advantage of this behavior in RL to optimize congestion control (Jay et al., 2019; Ruffy et al., 2018), decision trees for packet classi\ufb01cation (Liang et al., 2019), sequence to SQL/program translation (Zhong et al., 2017; Guu et al., 2017; Liang et al., 2016), ordering of SQL joins (Krishnan et al., 2018; Ortiz et al., 2018; Marcus et al., 2018; 2019), compiler phase ordering (Huang et al., 2019; Kulkarni et al., 2012) and device placement (Addanki et al., 2019; Paliwal et al., 2019). After enough steps in these problems, the agent will always arrive at a clear terminating step. For example, in query join order optimization, the number of joins is \ufb01nite and known from the query. In congestion control \u2013 where the routers need to adapt the sending rates to provide high throughput without comprising fairness \u2013 the updates are performed on a \ufb01xed number of senders/receivers known in advance. These updates combined de\ufb01ne one episode. This may explain why there is a trend towards using PG methods for these types of problems, as they don\u2019t require a continuous temporal difference behavior and can often operate in batches of multiple queries. Nevertheless, in some cases, Q-learning is still used, mainly for sample ef\ufb01ciency as the environment step might take a relatively long time. To improve the performance of PG methods, it is possible to take advantage of the way the gradient computation is performed. If the environment is not needed to generate the observation, it is possible to save many environment steps. This is achieved by rolling out the whole episode from interacting only with the policy and performing one environment step at the very end. The sum of rewards will be the same as the reward received from this environment step. For example, in query optimization, since the observations are encoded directly from the actions, and the environment is mainly used to generate the rewards, it will be possible to repeatedly perform an action, form the observation directly from this action, and feed it to the policy network. After the end of the episode, the environment can be triggered to get the \ufb01nal reward, which would be the sum of the intermediate rewards. This can signi\ufb01cantly reduce the training time. 3.3 Discussion: Continuous vs. Episodic Continuous policies can handle both continuous and episodic tasks, while episodic policies cannot. So, for example, Q-Learning can handle all the tasks mentioned in this work, while PG based methods cannot directly handle it without modi\ufb01cation. For example, in (Mao et al., 2016), the authors limited the the scheduler window of jobs to M, allowing the agent in every time step to schedule up to M jobs out of all arrived jobs. The authors also discussed this issue of \u201dbounded time horizon\u201d and hoped to overcome it by using a value network to replace the timedependent baseline. It is interesting to note that prior work on continuous system optimization tasks using non deep RL approaches (Choi et al., 1996; Littman et al., 2013; Boyan et al., 1994; Peng et al., 2015; Jamshidi et al., 2015; Barrett et al., 2013; Arabnejad et al., 2017; Sadeghi et al., 2017; Farahnakian et al., 2014) used Q-Learning. One solution for handling continuing problems without episode boundaries with PG based methods is to de\ufb01ne performance in terms of the average rate of reward per time step (Sutton et al., 2018) (Chapter 13.6). Such approaches can help better \ufb01t the continuous problems to episodic RL algorithms. 4 FORMULATING THE RL ENVIRONMENT Table 1 lists all the works we reviewed and their problem formulations in the context of RL, i.e., the model, observations, actions and rewards de\ufb01nitions. Among the major challenges when formulating the problem in the RL environment is properly de\ufb01ning the system problem as an RL problem, with all of the required inputs and outputs, i.e., state, action spaces and rewards. The rewards are generally sparse and behave similarly for different actions, making the RL training ineffective due to bad gradients. The states are generally de\ufb01ned using hand engineered features that are believed to encode the state of the system. This results in a large state space with some features that are less helpful than others and rarely captures the actual system state. Using model-based RL can alleviate this bottleneck and provide more sample ef\ufb01ciency. (Liu et al., 2017) used auto",
    "Methods": "Methods1/ Q-Learning2/PG2 FCNN device placement (Paliwal et al., 2019)1 (Addanki et al., 2019)2 computation graph placement/schedule of graph node speedup maximize performance & minimize peak memory PG1,2/ Evolutionary Methods1 GNN/FCNN distributed instruction placement (Coons et al., 2008) instruction features instruction placement location speedup maximize performance Evolutionary Methods FCNN encoders to help reduce the state dimensionality. The action space is also large but generally represents actions that are directly related to the objective. Another challenge is the environment step. Some tasks require a long time for the environment to perform one step, signi\ufb01cantly slowing the learning process of the RL agent. Interestingly, most works focus on using simple out-of-thebox FCNNs, while some works that targeted parsing and translation ((Liang et al., 2016; Guu et al., 2017; Zhong et al., 2017)) used RNNs (Graves et al., 2013) due to their ability to parse strings and natural language. While FCNNs are simple and easy to train to learn a linear and non-linear function policy mappings, sometimes having a more complicated network structure suited for the problem could further improve the results. 4.1 Evaluation ",
    "Results": "Results Table 2 lists training, and evaluation results of the reviewed works. We consider the time it takes to perform a step in the environment, the number of steps needed in each iteration of training, number of training iterations, total number of steps needed, and whether the prior work improves the state of the art and compares against random search/bandit solution. The total number of steps and the the cost of each environment step is important to understand the sample ef\ufb01ciency and practicality of the solution, especially when considering RLs inherent sample inef\ufb01ciency (Schaal, 1997; Hester et al., 2018). For different workloads, the number of samples needed varies from thousands to millions. The environment step time also varies from milliseconds to minutes. In multiple cases, the interaction with the environment is very slow. Note that in most cases when the environment step time was a few milliseconds, it was because it was a simulated environment, not a real one. We observe that for faster A View on Deep Reinforcement Learning in System Optimization Table 2. Evaluation results. Work Problem Environment Step Time Number of Steps Per Iteration Number of training Iterations Total Number Of Steps Improves State of the Art Compares Against Bandit/Random Search packet classi\ufb01cation (Liang et al., 2019) 20-600ms up to 60,000 up to 167 1,002,000 (18%) \u0015 congestion control (Jay et al., 2019) 50-500ms 8192 1200 9,830,400 (similar) congestion control (Ruffy et al., 2018) 0.5s N/A N/A 50,000-100,000 \u0015 \u0015 resource allocation (Mao et al., 2016) 10-40ms 20,000 1000 20,000,000 (10-63%) resource allocation (He et al., 2017a) (He et al., 2017b) N/A N/A 20,000 N/A no comparison \u0015 resource allocation (Tesauro et al., 2006) N/A N/A 10,000-20,000 N/A no comparison resource allocation (Xu et al., 2017) N/A N/A N/A N/A no comparison resource allocation (Liu et al., 2017) 1-120 minutes 100,000 20 2,000,000 no comparison \u0015 resource allocation (Rao et al., 2009) (Xu et al., 2012) N/A N/A N/A N/A no comparison \u0015 SQL Joins (Krishnan et al., 2018) 10ms 640 100 64,000 (70%) SQL joins (Ortiz et al., 2018) N/A N/A N/A N/A no comparison \u0015 SQL joins (Marcus et al., 2019) 250ms 100-8,000 100 10,000-80,000 (10-66%) SQL joins (Marcus et al., 2018) 1.08s N/A N/A 10,000 (20%) sequence to SQL (Zhong et al., 2017) N/A 80,654 300 24,196,200 (similar) \u0015 language to program trans. (Guu et al., 2017) N/A N/A N/A 13,000 (56%) \u0015 semantic parsing (Liang et al., 2016) N/A 3,098 200 619,600 (3.4%) \u0015 phase ordering (Huang et al., 2019) 1s N/A N/A 1,000-10,000 (similar) phase ordering (Kulkarni et al., 2012) 13.2 days for all steps N/A N/A N/A \u0015 \u0015 device placement (Addanki et al., 2019)N/A (seconds) N/A N/A 1,600-94,000 (3%) device placement (Paliwal et al., 2019) N/A (seconds) N/A N/A 400,000 (5%) instruction placement (Coons et al., 2008) N/A (minutes) N/A 200 N/A (days) \u0015 \u0015 environment steps more training samples were gathered to leverage that and further improve the performance. This excludes (Liu et al., 2017) where a cluster was used and thus more samples could be processed in parallel. As listed in Table 2, many works did not provide suf\ufb01cient data to reproduce the results. Reproducing the results is necessary to further improve the solution and enable future evaluation and comparison against it. 4.2 Frameworks and Toolkits A few RL benchmark toolkits for developing and comparing reinforcement learning algorithms, and providing a faster simulated system environment, were recently proposed. OpenAI Gym (Brockman et al., 2016) supports an environment for teaching agents everything, from walking to playing games like Pong or Pinball. Iroko (Ruffy et al., 2018) provides a data center emulator to understand the requirements and limitations of applying RL in data center networks. It interfaces with the OpenAI Gym and offers a way to evaluate centralized and decentralized RL algorithms against conventional traf\ufb01c control solutions. Park (Mao et al., 2019) proposes an open platform for easier formulation of the RL environment for twelve real world system optimization problems with one common easy to use API. The platform provides a translation layer between A View on Deep Reinforcement Learning in System Optimization the system and the RL environment making it easier for RL researchers to work on systems problems. That being said, the framework lacks the ability to change the action, state and reward de\ufb01nitions, making it harder to improve the performance by easily modifying these de\ufb01nitions. 5 CONSIDERATIONS FOR EVALUATING DEEP RL IN SYSTEM OPTIMIZATION In this section, we propose a set of questions that can help system optimization researchers determine whether deep RL could be an effective tool in solving their systems optimization challenges. Can the System Optimization Problem Be Modeled by an MDP? The problem of RL is the optimal control of an MDP. MDPs are a classical formalization of sequential decision making, where actions in\ufb02uence not just immediate rewards but also future states and rewards. This involves delayed rewards and the trade-off between delayed and immediate reward. In MDPs, the new state and new reward are dependent only on the preceding state and action. Given a perfect model of the environment, an MDP can compute the optimal policy. MDPs are typically a straightforward formulation of the system problem, as an agent learns by continually interacting with the system to achieve a particular goal, and the system responds to these interactions with a new state and reward. The agent\u2019s goal is to maximize expected reward over time. Is It a Reinforcement Learning Problem? What distinguishes RL from other machine learning approaches is the presence of self exploration and exploitation, and the tradeoff between them. For example, RL is different from supervised learning. The latter is learning from a training set with labels provided by an external supervisor that is knowledgeable. For each example the label is the correct action the system should take. The objective of this kind of learning is to act correctly in new situations not present in the training set. However, supervised learning is not suitable for learning from interaction, as it is often impractical to obtain examples representative of all the cases in which the agent has to act. Are the Rewards Delayed? RL algorithms do not maximize the immediate reward of taking actions but, rather, expected reward over time. For example, an RL agent can choose to take actions that give low immediate rewards but that lead to higher rewards overall, instead of taking greedy actions every step that lead to high immediate rewards but low rewards overall. If the objective is to maximize the immediate reward or the actions are not dependent, then other simpler approaches, such as bandits and greedy algorithms, will perform better than deep RL, as their objective is to maximize the immediate reward. What is Being Learned? It is important to provide insights on what is being learned by the agent. For example, what actions are taken in which states and why? Can the knowledge learned be applied to new states/tasks? Is there a structure to the problem being learned? If a brute-force solution is possible for simpler tasks, it will also be helpful to know how much better the performance of the RL agent is than the brute force solution. In some cases, not all hand-engineered features are useful. Using all of them can result in high variance and prolonged training. Feature analysis can help overcome this challenge. For example, in (Coons et al., 2008) signi\ufb01cant performance gaps were shown for different feature selection. Does It Outperform Random Search and a Bandit Solution? In some cases, the RL solution is just another form of a improved random search. In some cases, good RL results were achieved merely by chance. For instance, if the features used to represent the state are not good or do not have a pattern that could be learned. In such cases, random search might perform as well as RL, or even better, as it is less complicated. For example, in (Huang et al., 2019), the authors showed 10% improvement over the baseline by using random search. In some cases the actions are independent and a greedy or bandit solution can achieve the optimal or near-optimal solution. Using a bandit method is equivalent using a 1-step RL solution, in which the objective is to maximize the immediate reward. Maximizing the immediate reward could deliver the overall maximum reward and, thus, a comparison against a bandit solution can help reveal this. Are the Expert Actions Observable? In some cases it might be possible to have access to expert actions, i.e., optimal actions. For example, if a brute force search is plausible and practical then it is possible to outperform deep RL by using it or using imitation learning (Schaal, 1999), which is a supervised learning approach that learns by imitating expert actions. Is It Possible to Reproduce/Generalize Good Results? The learning process in deep RL is stochastic and thus good results are sometimes achieved due to local maxima, simple tasks, and chance. In (Haarnoja et al., 2018) different results were generated by just changing the random seeds. In many cases, good results cannot be reproduced by retraining, training on new tasks, or generalizing to new tasks. Does It Outperform the State of the Art? The most important metric in the context of system optimization in general is outperforming the state of the art. Improving the state of the art includes different objectives, such as ef\ufb01ciency, performance, throughput, bandwidth, fault tolerance, security, utilization, reliability, robustness, complexity, and energy. If the proposed approach does not perform better than the state of the art in some metric then A View on Deep Reinforcement Learning in System Optimization it is hard to justify using it. Frequently, the state of the art solution is also more stable, practical, and reliable than deep RL. In many prior works listed in Table 2 a comparison against the state of the art is not available or deep RL performs worse. In some cases deep RL can perform as good as the state of the art or slightly worse, but still be a useful solution as it achieves an improvement on other metrics. 6 RL METHODS AND NEURAL NETWORK MODELS Multiple RL methods and neural network models can be used. RL frameworks like RLlib (Liang et al., 2017), Intel\u2019s Coach (Caspi et al., 2017), TensorForce (Kuhnle et al., 2017), Facebook Horizon (Gauci et al., 2018), and Google\u2019s Dopamine (Castro et al., 2018) can help the users pick the right RL model, as they provide implementations of many policies and models for which a convenient interface is available. As a rule of thumb, we rank RL algorithms based on sample ef\ufb01ciency as follows: model-based approaches (most ef\ufb01cient), temporal difference methods, PG methods, and evolutionary algorithms (least ef\ufb01cient). In general, many RL environments run in a simulator. For example (Paliwal et al., 2019; Mao et al., 2019; 2016), run in a simulator as the real environment\u2019s step would take minutes or hours, which signi\ufb01cantly slows down the training. If this simulator is fast enough or training time is not constrained then PG methods can perform well. If the simulator is not fast enough or training time is constrained then temporal difference methods can do better than PG methods as they are more sample ef\ufb01cient. If the environment is the real one, then temporal difference can do well, as long as interaction with the environment is not slow. Model-based RL performs better if the environment is slow. Model-based methods require a model of the environment (that can be learned) and rely mainly on planning rather than learning (Deisenroth et al., 2011; Guo et al., 2014). Since planning is not done in the actual environment, but in much faster simulation steps within the model, it requires less samples from the real environment to learn. Many real-world system problems have well established and often highly accurate models, which model-based methods can leverage. That being said, model-free methods are often used as they are simpler to deploy and have the potential to generalize better from exploration in a real environment. If good policies are easy to \ufb01nd and if either the space of policies is small enough or time is not a bottleneck for the search, then evolutionary methods can be effective. Evolutionary methods also have advantages when the learning agent cannot observe the complete state of the environment. As mentioned earlier, bandit solutions are good if the problem can be viewed as a one-step RL problem. PG methods are in general more stable than methods like QLearning that do not directly use and derive a neural network to represent the agent\u2019s policy. The greedy nature of directly deriving the policy and moving the gradient in the direction of the objective also make PG methods easier to reason about and often more reliable. However, Q-Learning can be applied to data collected from a running system more readily than PG, which must interact with the system during training. The RL methods may be implemented using any number of deep neural network architectures. The preferred architecture depends on the the nature of the observation and action spaces. CNNs that ef\ufb01ciently capture spatially-organized observation spaces lend themselves visual data (e.g., images or video). Networks designed for sequential learning, such as RNNs, are appropriate for observation spaces involving sequence data (e.g., code, queries, temporal event streams). Otherwise, FCNNs are preferred for their general applicability and ease of use, although they tend to be the most computationally-intensive choice. Finally, GNNs or other networks that capture structure within observations can be used in the less frequent case that the designer has a priori knowledge of the representational structure. In this case, the model can even generate structured action spaces (e.g., a query plan tree or computational graph). 7 CHALLENGES In this section, we discuss the primary challenges that face the application of deep RL in system optimization. Interactions with Real Systems Can Be Slow. Generalizing from Faster Simulated Environments Can Be Restrictive. Unlike the case with simulated environments that can run fast, when running on a real system, performing an action can trigger a reward after a lengthy delay. For example, when scheduling jobs on a cluster of nodes, some jobs might require hours to run, and thus improving their performance by monitoring job execution time will be very slow. To speed up this process, some works use simulators as cost models instead of the actual system. These simulators often do not fully capture the actual behavior of the real system and thus the RL agent may not work as well in practice. More comprehensive environment models can aid generalization from simulated environments. RL methods that are more sample ef\ufb01cient will speed up training in real system environments. Instability and High Variance. This is a common problem which leads to bad policies when tackling system problems with deep RL. Such policies can generate a large performance gap when trained multiple times and behave in an unpredictable manner. This is mainly due to poor formulaA View on Deep Reinforcement Learning in System Optimization tion of the problem as an RL problem, limited observation of the state, i.e., the use of embeddings and input features that are not suf\ufb01cient/meaningful, and sparse or similar rewards. Sparse rewards can be due to bad reward de\ufb01nition or the fact that some rewards cannot be computed directly and are known only at the end of the episode. For example, in (Liang et al., 2019), where deep RL is used to optimize decision trees for packet classi\ufb01cation, the reward (the performance of the tree) is known only when the whole tree is built, or after approximately 15,000 steps. In some cases using more robust and stable policies can help. For example, Q-learning is known to have good sample ef\ufb01ciency but unstable behavior. SARSA, double Q-learning (Van Hasselt et al., 2016) and policy gradient methods, on the other hand, are more stable. Subtracting a bias in PG can also help reduce variance (Greensmith et al., 2004). Lack of Reproducibility. Reproducibility is a frequent challenge with many recent works in system optimization that rely on deep RL. It becomes dif\ufb01cult to reproduce the results due to restricted access to the resources, code, and workloads used, lack of a detailed list of the used network hyperparameters and lack of stable, predictable, and scalable behavior of the different RL algorithms. This challenge prevents future deployment, incremental improvements, and proper evaluation. De\ufb01ning Appropriate Rewards, Actions and States. The proper de\ufb01nition of states, actions, and rewards is the key, since otherwise the RL solution is not useful. In the general use case of deep RL, de\ufb01ning the states, actions and rewards is much more straightforward than in the case in system optimization. For example, in atari games, the state is an image representing the current status of the game, the rewards are the points collected while playing and the actions are moves in the game. However, often in system optimization, it is not clear what are the appropriate de\ufb01nitions. Furthermore, in many cases the rewards are sparse or similar, the states are not fully observable to capture the whole system state and have limited features that capture only a small portion of the system state. This results in unstable and inadequate policies. Generally, the action and state spaces are large, requiring a lot of samples to learn and resulting in instability and large variance in the learned network. Therefore, retraining often fails to generate the same results. Lack of Generalization. The lack of generalization is an issue that deep RL solutions often suffer from. This might be bene\ufb01cial when learning a particular structure. For example, in NeuroCuts (Liang et al., 2019), the target is to build the best decision tree for \ufb01xed set of prede\ufb01ned rules and thus the objective of the RL agent is to \ufb01nd the optimal \ufb01t for these rules. However, lack of generalization sometimes results in a solution that works for a particular workload or setting but overall, across various workloads, is not very good. This problem manifests when generalization is important and the RL agent has to deal with new states that it did not visit in the past. For example, in (Paliwal et al., 2019; Addanki et al., 2019), where the RL agent has to learn good resource placements for different computation graphs, the authors avoided the possibility of learning only good placements for particular computation graphs by training and testing on a wide range graphs. Lack of Standardized Benchmarks, Frameworks and Evaluation Metrics. The lack of standardized benchmarks, frameworks and evaluation metrics makes it very dif\ufb01cult to evaluate the effectiveness of the deep RL methods in the context of system optimization. Thus, it is crucial to have proper standardized frameworks and evaluation metrics that de\ufb01ne success. Moreover, benchmarks are needed that enable proper training, evaluation of the results, measuring the generalization of the solution to new problems and performing valid comparisons against baseline approaches. 8 AN ILLUSTRATIVE EXAMPLE We put all the metrics (from Section 5) to work and further highlight the challenges (from Section 7) of implementing deep RL solutions using DeepRM (Mao et al., 2016) as an illustrative example. In DeepRM, the targeted system problem is resource allocation in the cloud. The objective is to avoid job slowdown, i.e., the goal is to minimize the wait time for all jobs. DeepRM uses PG in conjunction with a simulated environment rather than a real cloud environment. This signi\ufb01cantly improves the step time but can result in restricted generalization when used in a real environment. Furthermore, since all the simulation parameters are known, the full state of the simulated environment can be captured. The actions are de\ufb01ned as selecting which job should be scheduled next. The state is de\ufb01ned as the current allocation of cluster resources, as well as the resource pro\ufb01les of jobs waiting to be scheduled. The reward is de\ufb01ned as the sum of of job slowdowns: \u03a3i( \u22121 Ti ) where Ti is the pure execution time of job i without considering the wait time. This reward basically gives a penalty of \u22121 for jobs that are waiting to be scheduled. The penalty is divided by Ti to give a higher priority to shorter jobs. The state, actions and reward clearly de\ufb01ne an MDP and a reinforcement learning problem. Speci\ufb01cally, the agent interacts with the system by making sequential allocations, observing the state of the current allocation of resources and receiving delayed long-term rewards as overall slow downs of jobs. The rewards are delayed because the agent cannot know the effect of the current allocation action on the overall slow down at any particular time step; the agent would have to wait until all the other jobs are allocated to assess the full impact. The agent then learns which jobs to allocate in the current time step to minimize the average job slowdown, given the current resource allocation in the ",
    "Conclusion": "CONCLUSION In this work, we reviewed and discussed multiple challenges in applying deep reinforcement learning to system optimization problems and proposed a set of metrics that can help evaluate the effectiveness of these solutions. Recent applications of deep RL in system optimization are mainly in ",
    "References": "REFERENCES Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H., and Alizadeh, M. Placeto: Learning generalizable device placement algorithms for distributed machine learning. arXiv preprint arXiv:1906.08879, 2019. Andreasson, E., Hoffmann, F., and Lindholm, O. To collect or not to collect? machine learning for memory management. In Java Virtual Machine Research and Technology Symposium, pp. 27\u201339. Citeseer, 2002. Arabnejad, H., Pahl, C., Jamshidi, P., and Estrada, G. A comparison of reinforcement learning techniques for fuzzy cloud auto-scaling. In Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, pp. 64\u201373. IEEE Press, 2017. Ashouri, A. H., Killian, W., Cavazos, J., Palermo, G., and Silvano, C. A survey on compiler autotuning using machine learning. ACM Computing Surveys (CSUR), 51(5): 96, 2018. Auer, P. Using con\ufb01dence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002. Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235\u2013256, 2002. Barrett, E., Howley, E., and Duggan, J. Applying reinforcement learning towards automating resource allocation and application scalability in the cloud. Concurrency and Computation: Practice and Experience, 25(12):1656\u2013 1674, 2013. Bellman, R. A markovian decision process. In Journal of Mathematics and Mechanics, pp. 679\u2013684, 1957. Berry, D. A., and Fristedt, B. Bandit problems: sequential allocation of experiments (monographs on statistics and applied probability). London: Chapman and Hall, 5: 71\u201387, 1985. Boyan, J. A., and Littman, M. L. Packet routing in dynamically changing networks: A reinforcement learning approach. In Advances in neural information processing systems, pp. 671\u2013678, 1994. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym, 2016. Caspi, I., Leibovich, G., Novik, G., and Endrawis, S. Reinforcement learning coach, December 2017. URL https: //doi.org/10.5281/zenodo.1134899. Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http: //arxiv.org/abs/1812.06110. Choi, S. P., and Yeung, D.-Y. Predictive q-routing: A memory-based reinforcement learning approach to adaptive traf\ufb01c control. In Advances in Neural Information Processing Systems, pp. 945\u2013951, 1996. Chu, W., Li, L., Reyzin, L., and Schapire, R. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Arti\ufb01cial Intelligence and Statistics, pp. 208\u2013214, 2011. Coons, K. E., Robatmili, B., Taylor, M. E., Maher, B. A., Burger, D., and McKinley, K. S. Feature selection and policy optimization for distributed instruction placement using reinforcement learning. In Proceedings of the 17th international conference on Parallel architectures and compilation techniques, pp. 32\u201342. ACM, 2008. Das, A., Sha\ufb01k, R. A., Merrett, G. V., Al-Hashimi, B. M., Kumar, A., and Veeravalli, B. Reinforcement learningbased inter-and intra-application thermal optimization for lifetime improvement of multicore systems. In Proceedings of the 51st Annual Design Automation Conference, pp. 1\u20136. ACM, 2014. Deisenroth, M. P., Rasmussen, C. E., and Fox, D. Learning to control a low-cost manipulator using data-ef\ufb01cient reinforcement learning. Robotics: Science and Systems V, pp. 57\u201364, 2011. Diegues, N., and Romano, P. Self-tuning intel transactional synchronization extensions. In 11th International Conference on Autonomic Computing ({ICAC} 14), pp. 209\u2013219, 2014. Doya, K. Reinforcement learning in continuous time and space. Neural computation, 12(1):219\u2013245, 2000. Farahnakian, F., Liljeberg, P., and Plosila, J. Energy-ef\ufb01cient virtual machines consolidation in cloud data centers using reinforcement learning. In 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, pp. 500\u2013507. IEEE, 2014. A View on Deep Reinforcement Learning in System Optimization Gauci, J., Conti, E., Liang, Y., Virochsiri, K., He, Y., Kaden, Z., Narayanan, V., and Ye, X. Horizon: Facebook\u2019s open source applied reinforcement learning platform. arXiv preprint arXiv:1811.00260, 2018. Graves, A., Mohamed, A.-r., and Hinton, G. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645\u20136649. IEEE, 2013. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov): 1471\u20131530, 2004. Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. Deep learning for real-time atari game play using of\ufb02ine monte-carlo tree search planning. In Advances in neural information processing systems, pp. 3338\u20133346, 2014. Guu, K., Pasupat, P., Liu, E. Z., and Liang, P. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. arXiv preprint arXiv:1704.07926, 2017. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018. Hameed, A., Khoshkbarforoushha, A., Ranjan, R., Jayaraman, P. P., Kolodziej, J., Balaji, P., Zeadally, S., Malluhi, Q. M., Tziritas, N., Vishnu, A., et al. A survey and taxonomy on energy ef\ufb01cient resource allocation techniques for cloud computing systems. Computing, 98(7):751\u2013774, 2016. He, Y., Yu, F. R., Zhao, N., Leung, V. C., and Yin, H. Software-de\ufb01ned networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach. IEEE Communications Magazine, 55(12):31\u201337, 2017a. He, Y., Zhao, N., and Yin, H. Integrated networking, caching, and computing for connected vehicles: A deep reinforcement learning approach. IEEE Transactions on Vehicular Technology, 67(1):44\u201355, 2017b. Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Osband, I., et al. Deep q-learning from demonstrations. In ThirtySecond AAAI Conference on Arti\ufb01cial Intelligence, 2018. Huang, Q., Haj-Ali, A., Moses, W., Xiang, J., Stoica, I., Asanovic, K., and Wawrzynek, J. Autophase: Compiler phase-ordering for hls with deep reinforcement learning. In 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pp. 308\u2013308. IEEE, 2019. Ipek, E., Mutlu, O., Mart\u00b4\u0131nez, J. F., and Caruana, R. Selfoptimizing memory controllers: A reinforcement learning approach. In ACM SIGARCH Computer Architecture News, volume 36, pp. 39\u201350. IEEE Computer Society, 2008. Jamshidi, P., Shari\ufb02oo, A. M., Pahl, C., Metzger, A., and Estrada, G. Self-learning cloud controllers: Fuzzy qlearning for knowledge evolution. In 2015 International Conference on Cloud and Autonomic Computing, pp. 208\u2013 211. IEEE, 2015. Jay, N., Rotman, N., Godfrey, B., Schapira, M., and Tamar, A. A deep reinforcement learning perspective on internet congestion control. In International Conference on Machine Learning, pp. 3050\u20133059, 2019. Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. Is q-learning provably ef\ufb01cient? In Advances in Neural Information Processing Systems, pp. 4863\u20134873, 2018. Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: A survey. volume 4, pp. 237\u2013285, 1996. Kober, J., Bagnell, J. A., and Peters, J. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013. Krauter, K., Buyya, R., and Maheswaran, M. A taxonomy and survey of grid resource management systems for distributed computing. Software: Practice and Experience, 32(2):135\u2013164, 2002. Krishnan, S., Yang, Z., Goldberg, K., Hellerstein, J., and Stoica, I. Learning to optimize join queries with deep reinforcement learning. arXiv preprint arXiv:1808.03196, 2018. Kuhnle, A., Schaarschmidt, M., and Fricke, K. Tensorforce: a tensor\ufb02ow library for applied reinforcement learning. Web page, 2017. URL https://github. com/tensorforce/tensorforce. Kulkarni, S., and Cavazos, J. Mitigating the compiler optimization phase-ordering problem using machine learning. In ACM SIGPLAN Notices, volume 47, pp. 147\u2013162. ACM, 2012. Lagoudakis, M. G., and Littman, M. L. Algorithm selection using reinforcement learning. In ICML, pp. 511\u2013518. Citeseer, 2000. Li, W., Zhou, F., Meleis, W., and Chowdhury, K. Learningbased and data-driven tcp design for memory-constrained iot. In 2016 International Conference on Distributed Computing in Sensor Systems (DCOSS), pp. 199\u2013205. IEEE, 2016. A View on Deep Reinforcement Learning in System Optimization Liang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020, 2016. Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gonzalez, J., Goldberg, K., and Stoica, I. Ray rllib: A composable and scalable reinforcement learning library. arXiv preprint arXiv:1712.09381, 2017. Liang, E., Zhu, H., Jin, X., and Stoica, I. Neural packet classi\ufb01cation. arXiv preprint arXiv:1902.10319, 2019. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. Lin, L.-J. Reinforcement learning for robots using neural networks. 1993. Littman, M., and Boyan, J. A distributed reinforcement learning scheme for network routing. In Proceedings of the international workshop on applications of neural networks to telecommunications, pp. 55\u201361. Psychology Press, 2013. Liu, N., Li, Z., Xu, J., Xu, Z., Lin, S., Qiu, Q., Tang, J., and Wang, Y. A hierarchical framework of cloud resource allocation and power management using deep reinforcement learning. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS), pp. 372\u2013382. IEEE, 2017. Luong, N. C., Hoang, D. T., Gong, S., Niyato, D., Wang, P., Liang, Y.-C., and Kim, D. I. Applications of deep reinforcement learning in communications and networking: A survey. IEEE Communications Surveys & Tutorials, 2019. Mahdavinejad, M. S., Rezvan, M., Barekatain, M., Adibi, P., Barnaghi, P., and Sheth, A. P. Machine learning for internet of things data analysis: A survey. Digital Communications and Networks, 4(3):161\u2013175, 2018. Mao, H., Alizadeh, M., Menache, I., and Kandula, S. Resource management with deep reinforcement learning. In Proceedings of the 15th ACM Workshop on Hot Topics in Networks, pp. 50\u201356. ACM, 2016. Mao, H., Negi, P., Narayan, A., Wang, H., Yang, J., Wang, H., Marcus, R., Addanki, R., Khani, M., He, S., et al. Park: An open platform for learning augmented computer systems. 2019. Marcus, R., and Papaemmanouil, O. Deep reinforcement learning for join order enumeration. In Proceedings of the First International Workshop on Exploiting Arti\ufb01cial Intelligence Techniques for Data Management, pp. 3. ACM, 2018. Marcus, R., Negi, P., Mao, H., Zhang, C., Alizadeh, M., Kraska, T., Papaemmanouil, O., and Tatbul, N. Neo: A learned query optimizer. arXiv preprint arXiv:1904.03711, 2019. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u2013 1937, 2016. Mostafavi, S., Ahmadi, F., and Sarram, M. A. Reinforcement-learning-based foresighted task scheduling in cloud computing. arXiv preprint arXiv:1810.04718, 2018. Ortiz, J., Balazinska, M., Gehrke, J., and Keerthi, S. S. Learning state representations for query optimization with deep reinforcement learning. arXiv preprint arXiv:1803.08604, 2018. Paliwal, A., Gimeno, F., Nair, V., Li, Y., Lubin, M., Kohli, P., and Vinyals, O. Regal: Transfer learning for fast optimization of computation graphs. arXiv preprint arXiv:1905.02494, 2019. Peled, L., Mannor, S., Weiser, U., and Etsion, Y. Semantic locality and context-based prefetching using reinforcement learning. In 2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA), pp. 285\u2013297. IEEE, 2015. Peng, Z., Cui, D., Zuo, J., Li, Q., Xu, B., and Lin, W. Random task scheduling scheme based on reinforcement learning in cloud computing. Cluster computing, 18(4): 1595\u20131607, 2015. Peters, J., Vijayakumar, S., and Schaal, S. Reinforcement learning for humanoid robotics. In Proceedings of the third IEEE-RAS international conference on humanoid robots, pp. 1\u201320, 2003. Rao, J., Bu, X., Xu, C.-Z., Wang, L., and Yin, G. Vconf: a reinforcement learning approach to virtual machines autocon\ufb01guration. In Proceedings of the 6th international conference on Autonomic computing, pp. 137\u2013146. ACM, 2009. Ross, S., Gordon, G., and Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online A View on Deep Reinforcement Learning in System Optimization learning. In Proceedings of the fourteenth international conference on arti\ufb01cial intelligence and statistics, pp. 627\u2013635, 2011. Ruffy, F., Przystupa, M., and Beschastnikh, I. Iroko: A framework to prototype reinforcement learning for data center traf\ufb01c control. arXiv preprint arXiv:1812.09975, 2018. Rummery, G. A., and Niranjan, M. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, England, 1994. Sadeghi, A., Sheikholeslami, F., and Giannakis, G. B. Optimal and scalable caching for 5g using reinforcement learning of space-time popularities. IEEE Journal of Selected Topics in Signal Processing, 12(1):180\u2013190, 2017. Schaal, S. Learning from demonstration. In Advances in neural information processing systems, pp. 1040\u20131046, 1997. Schaal, S. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3(6):233\u2013242, 1999. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Silva, A. P., Obraczka, K., Burleigh, S., and Hirata, C. M. Smart congestion control for delay-and disruption tolerant networks. In 2016 13th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), pp. 1\u20139. IEEE, 2016. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016. Sutton, R. S., and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057\u20131063, 2000. Tesauro, G., Das, R., and Jong, N. K. Online performance management using hybrid reinforcement learning. Proceedings of SysML, 2006. Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on arti\ufb01cial intelligence, 2016. Wang, Z., and OBoyle, M. Machine learning in compiler optimization. Proceedings of the IEEE, 106(11):1879\u2013 1901, 2018. Watkins, C. J., and Dayan, P. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992. Xu, C.-Z., Rao, J., and Bu, X. Url: A uni\ufb01ed reinforcement learning approach for autonomic cloud management. Journal of Parallel and Distributed Computing, 72 (2):95\u2013105, 2012. Xu, Z., Wang, Y., Tang, J., Wang, J., and Gursoy, M. C. A deep reinforcement learning based framework for poweref\ufb01cient resource allocation in cloud rans. In 2017 IEEE International Conference on Communications (ICC), pp. 1\u20136. IEEE, 2017. Zeppenfeld, J., Bouajila, A., Stechele, W., and Herkersdorf, A. Learning classi\ufb01er tables for autonomic systems on chip. GI Jahrestagung (2), 134:771\u2013778, 2008. Zhong, V., Xiong, C., and Socher, R. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017. Zhu, Q., and Yuan, C. A reinforcement learning approach to automatic error recovery. In 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN\u201907), pp. 729\u2013738. IEEE, 2007. ",
    "title": "",
    "paper_info": "A VIEW ON DEEP REINFORCEMENT LEARNING IN SYSTEM OPTIMIZATION\nAmeer Haj-Ali 1 2 Nesreen K. Ahmed 1 Ted Willke 1 Joseph E. Gonzalez 2 Krste Asanovic 2 Ion Stoica 2\nABSTRACT\nMany real-world systems problems require reasoning about the long term consequences of actions taken to\ncon\ufb01gure and manage the system. These problems with delayed and often sequentially aggregated reward, are\noften inherently reinforcement learning problems and present the opportunity to leverage the recent substantial\nadvances in deep reinforcement learning. However, in some cases, it is not clear why deep reinforcement learning\nis a good \ufb01t for the problem. Sometimes, it does not perform better than the state-of-the-art solutions. And in\nother cases, random search or greedy algorithms could outperform deep reinforcement learning. In this paper, we\nreview, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. We\npropose a set of essential metrics to guide future works in evaluating the ef\ufb01cacy of using deep reinforcement\nlearning in system optimization. Our evaluation includes challenges, the types of problems, their formulation in\nthe deep reinforcement learning setting, embedding, the model used, ef\ufb01ciency, and robustness. We conclude\nwith a discussion on open challenges and potential directions for pushing further the integration of reinforcement\nlearning in system optimization.\n1\nINTRODUCTION\nReinforcement learning (RL) is a class of learning problems\nframed in the context of planning on a Markov Decision\nProcess (MDP) (Bellman, 1957), when the MDP is not\nknown. In RL, an agent continually interacts with the en-\nvironment (Kaelbling et al., 1996; Sutton et al., 2018). In\nparticular, the agent observes the state of the environment,\nand based on this observation takes an action. The goal of\nthe RL agent is then to compute a policy\u2013a mapping be-\ntween the environment states and actions\u2013that maximizes\na long term reward. There are multiple ways to extrapo-\nlate the policy. Non-approximation methods usually fail to\npredict good actions in states that were not visited in the\npast, and require storing all the action-reward pairs for every\nvisited state, a task that incurs a huge memory overhead\nand complex computation. Instead, approximation methods\nhave been proposed. Among the most successful ones is\nusing a neural network in conjunction with RL, also known\nas deep RL. Deep models allow RL algorithms to solve\ncomplex problems in an end-to-end fashion, handle unstruc-\ntured environments, learn complex functions, or predict\nactions in states that have not been visited in the past. Deep\nRL is gaining wide interest recently due to its success in\nrobotics, Atari games, and superhuman capabilities (Mnih\net al., 2013; Doya, 2000; Kober et al., 2013; Peters et al.,\n2003). Deep RL was the key technique behind defeating the\nhuman European champion in the game of Go, which has\nlong been viewed as the most challenging of classic games\nfor arti\ufb01cial intelligence (Silver et al., 2016).\n1Intel Labs 2University of California, Berkeley. Correspon-\ndence to: Ameer Haj-Ali <ameerh@berkeley.edu>.\nMany system optimization problems have a nature of de-\nlayed, sparse, aggregated or sequential rewards, where im-\nproving the long term sum of rewards is more important\nthan a single immediate reward. For example, an RL en-\nvironment can be a computer cluster. The state could be\nde\ufb01ned as a combination of the current resource utilization,\navailable resources, time of the day, duration of jobs waiting\nto run, etc. The action could be to determine on which re-\nsources to schedule each job. The reward could be the total\nrevenue, jobs served in a time window, wait time, energy\nef\ufb01ciency, etc., depending on the objective. In this example,\nif the objective is to minimize the waiting time of all jobs,\nthen a good solution must interact with the computer cluster\nand monitor the overall wait time of the jobs to determine\ngood schedules. This behavior is inherent in RL. The RL\nagent has the advantage of not requiring expert labels or\nknowledge and instead the ability to learn directly from its\nown interaction with the world. RL can also learn sophisti-\ncated system characteristics that a straightforward solution\nlike \ufb01rst come \ufb01rst served allocation scheme cannot. For\ninstance, it could be better to put earlier long-running ar-\nrivals on hold if a shorter job requiring fewer resources is\nexpected shortly.\nIn this paper, we review different attempts to overcome sys-\ntem optimization challenges with the use of deep RL. Unlike\nprevious reviews (Hameed et al., 2016; Mahdavinejad et al.,\n2018; Krauter et al., 2002; Wang et al., 2018; Ashouri et al.,\n2018; Luong et al., 2019) that focus on machine learning\nmethods without discussing deep RL models or applying\nthem beyond a speci\ufb01c system problem, we focus on deep\nRL in system optimization in general. From reviewing prior\nwork, it is evident that standardized metrics for assessing\narXiv:1908.01275v3  [cs.LG]  4 Sep 2019\n",
    "GPTsummary": "                    - (1): The research background of this article is to apply deep reinforcement learning techniques in system optimization problems with delayed and sequentially aggregated rewards.\n\n                    - (2): Traditional methods for system optimization are not suitable for problems with delayed and sequentially aggregated rewards. Using deep reinforcement learning to solve these problems has shown varying degrees of success in previous studies. This paper aims to review and evaluate recent trends of using deep reinforcement learning for system optimization while proposing essential metrics for evaluating their efficacy.\n\n                    - (3): The research methodology proposed in this paper is to review, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. Additionally, this paper proposes a set of metrics to guide future works in evaluating the efficacy of using deep reinforcement learning in system optimization.\n\n                    - (4): The methods in this paper are evaluated on various tasks, including resource allocation, energy usage, and scheduling optimization problems. These methods show competitive or better performance compared to state-of-the-art solutions in some cases. However, it is also found that random search and greedy algorithms can outperform deep reinforcement learning in certain scenarios. The proposed metrics in this paper can help to guide future works in selecting appropriate methods and evaluating their performance.\n7. Methods:\n\n- (1): The main idea of this paper is to review and evaluate recent trends in applying deep reinforcement learning to system optimization problems with delayed and sequentially aggregated rewards. The authors propose a set of essential metrics for evaluating the efficacy of deep reinforcement learning methods in these problems.\n \n- (2): The authors evaluate the proposed metrics on different tasks related to system optimization, such as resource allocation, energy usage, and scheduling optimization. They compare the performance of deep reinforcement learning methods with state-of-the-art solutions and also consider random search and greedy algorithms in certain scenarios.\n\n- (3): The authors also discuss the challenges and limitations of using deep reinforcement learning in system optimization, such as the high dimensionality of state and action spaces, long environment steps, and the need for more complicated network structures. They suggest using FCNN encoders to reduce the state dimensionality and exploring more complex network structures suited for the problem.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this article lies in proposing a set of essential metrics and conducting a comprehensive evaluation of deep reinforcement learning techniques for system optimization problems with delayed and sequentially aggregated rewards. The proposed metrics can provide guidance in selecting appropriate methods and evaluating their efficacy in future works.\n\n- (2): Innovation point: This article proposes a set of metrics for evaluating the efficacy of deep reinforcement learning techniques in system optimization problems. The authors also discuss the challenges and limitations of applying these techniques and suggest possible solutions, such as using FCNN encoders and exploring more complex network structures. \n\nPerformance: The methods evaluated in this article show competitive or better performance compared to state-of-the-art solutions in some cases. However, it is also found that random search and greedy algorithms can outperform deep reinforcement learning in certain scenarios.\n\nWorkload: The methodology of this article involves reviewing and evaluating recent trends in using deep reinforcement learning for system optimization and comparing their performance with other methods. The workload is reasonable for the research scope and contributes to the advancement of the field.\n\n\n",
    "GPTmethods": "- (1): The main idea of this paper is to review and evaluate recent trends in applying deep reinforcement learning to system optimization problems with delayed and sequentially aggregated rewards. The authors propose a set of essential metrics for evaluating the efficacy of deep reinforcement learning methods in these problems.\n \n- (2): The authors evaluate the proposed metrics on different tasks related to system optimization, such as resource allocation, energy usage, and scheduling optimization. They compare the performance of deep reinforcement learning methods with state-of-the-art solutions and also consider random search and greedy algorithms in certain scenarios.\n\n- (3): The authors also discuss the challenges and limitations of using deep reinforcement learning in system optimization, such as the high dimensionality of state and action spaces, long environment steps, and the need for more complicated network structures. They suggest using FCNN encoders to reduce the state dimensionality and exploring more complex network structures suited for the problem.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this article lies in proposing a set of essential metrics and conducting a comprehensive evaluation of deep reinforcement learning techniques for system optimization problems with delayed and sequentially aggregated rewards. The proposed metrics can provide guidance in selecting appropriate methods and evaluating their efficacy in future works.\n\n- (2): Innovation point: This article proposes a set of metrics for evaluating the efficacy of deep reinforcement learning techniques in system optimization problems. The authors also discuss the challenges and limitations of applying these techniques and suggest possible solutions, such as using FCNN encoders and exploring more complex network structures. \n\nPerformance: The methods evaluated in this article show competitive or better performance compared to state-of-the-art solutions in some cases. However, it is also found that random search and greedy algorithms can outperform deep reinforcement learning in certain scenarios.\n\nWorkload: The methodology of this article involves reviewing and evaluating recent trends in using deep reinforcement learning for system optimization and comparing their performance with other methods. The workload is reasonable for the research scope and contributes to the advancement of the field.\n\n\n",
    "GPTconclusion": "- (1): The significance of this article lies in proposing a set of essential metrics and conducting a comprehensive evaluation of deep reinforcement learning techniques for system optimization problems with delayed and sequentially aggregated rewards. The proposed metrics can provide guidance in selecting appropriate methods and evaluating their efficacy in future works.\n\n- (2): Innovation point: This article proposes a set of metrics for evaluating the efficacy of deep reinforcement learning techniques in system optimization problems. The authors also discuss the challenges and limitations of applying these techniques and suggest possible solutions, such as using FCNN encoders and exploring more complex network structures. \n\nPerformance: The methods evaluated in this article show competitive or better performance compared to state-of-the-art solutions in some cases. However, it is also found that random search and greedy algorithms can outperform deep reinforcement learning in certain scenarios.\n\nWorkload: The methodology of this article involves reviewing and evaluating recent trends in using deep reinforcement learning for system optimization and comparing their performance with other methods. The workload is reasonable for the research scope and contributes to the advancement of the field.\n\n\n"
}