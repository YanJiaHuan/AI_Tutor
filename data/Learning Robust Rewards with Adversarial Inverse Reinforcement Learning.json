{
    "Abstract": "ABSTRACT Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually speci\ufb01ed reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally dif\ufb01cult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under signi\ufb01cant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings. 1 ",
    "Introduction": "INTRODUCTION While reinforcement learning (RL) provides a powerful framework for automating decision making and control, signi\ufb01cant engineering of elements such as features and reward functions has typically been required for good practical performance. In recent years, deep reinforcement learning has alleviated the need for feature engineering for policies and value functions, and has shown promising results on a range of complex tasks, from vision-based robotic control (Levine et al., 2016) to video games such as Atari (Mnih et al., 2015) and Minecraft (Oh et al., 2016). However, reward engineering remains a signi\ufb01cant barrier to applying reinforcement learning in practice. In some domains, this may be dif\ufb01cult to specify (for example, encouraging \u201csocially acceptable\u201d behavior), and in others, a na\u00a8\u0131vely speci\ufb01ed reward function can produce unintended behavior (Amodei et al., 2016). Moreover, deep RL algorithms are often sensitive to factors such as reward sparsity and magnitude, making well performing reward functions particularly dif\ufb01cult to engineer. Inverse reinforcement learning (IRL) (Russell, 1998; Ng & Russell, 2000) refers to the problem of inferring an expert\u2019s reward function from demonstrations, which is a potential method for solving the problem of reward engineering. However, inverse reinforcement learning methods have generally been less ef\ufb01cient than direct methods for learning from demonstration such as imitation learning (Ho & Ermon, 2016), and methods using powerful function approximators such as neural networks have required tricks such as domain-speci\ufb01c regularization and operate inef\ufb01ciently over whole trajectories (Finn et al., 2016b). There are many scenarios where IRL may be preferred over direct imitation learning, such as re-optimizing a reward in novel environments (Finn et al., 2017) or to infer an agent\u2019s intentions, but IRL methods have not been shown to scale to the same complexity of tasks as direct imitation learning. However, adversarial IRL methods (Finn et al., 2016b;a) hold promise for tackling dif\ufb01cult tasks due to the ability to adapt training samples to improve learning ef\ufb01ciency. Part of the challenge is that IRL is an ill-de\ufb01ned problem, since there are many optimal policies that can explain a set of demonstrations, and many rewards that can explain an optimal policy (Ng 1 arXiv:1710.11248v2  [cs.LG]  13 Aug 2018 ",
    "Related Work": "RELATED WORK Inverse reinforcement learning (IRL) is a form of imitation learning and learning from demonstration (Argall et al., 2009). Imitation learning methods seek to learn policies from expert demonstrations, and IRL methods accomplish this by \ufb01rst inferring the expert\u2019s reward function. Previous IRL approaches have included maximum margin approaches (Abbeel & Ng, 2004; Ratliff et al., 2006), and probabilistic approaches such as Ziebart et al. (2008); Boularias et al. (2011). In this work, we work under the maximum causal IRL framework of Ziebart (2010). Some advantages of this framework are that it removes ambiguity between demonstrations and the expert policy, and allows us to cast the reward learning problem as a maximum likelihood problem, connecting IRL to generative model training. Our proposed method most closely resembles the algorithms proposed by Uchibe (2017); Ho & Ermon (2016); Finn et al. (2016a). Generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016) differs from our work in that it is not an IRL algorithm that seeks to recover reward functions. The critic or discriminator of GAIL is unsuitable as a reward since, at optimality, it outputs 0.5 uniformly across all states and actions. Instead, GAIL aims only to recover the expert\u2019s policy, which is a less portable representation for transfer. Uchibe (2017) does not interleave policy optimization with reward learning within an adversarial framework. Improving a policy within an adversarial framework corresponds to training an amortized sampler for an energy-based model, and prior work has shown this is crucial for performance (Finn et al., 2016b). Wulfmeier et al. (2015) also consider learning cost functions with neural networks, but only evaluate on simple domains where analytically solving the problem with value iteration is tractable. Previous methods which aim to learn nonlinear cost functions have used boosting (Ratliff et al., 2007) and Gaussian processes (Levine et al., 2011), but still suffer from the feature engineering problem. Our IRL algorithm builds on the adversarial IRL framework proposed by Finn et al. (2016a), with the discriminator corresponding to an odds ratio between the policy and exponentiated reward distribution. The discussion in Finn et al. (2016a) is theoretical, and to our knowledge no prior work has reported a practical implementation of this method. Our experiments show that direct implementation of the proposed algorithm is ineffective, due to high variance from operating over entire trajectories. While it is straightforward to extend the algorithm to single state-action pairs, as we discuss in Section 4, a simple unrestricted form of the discriminator is susceptible to the reward ambiguity described in (Ng et al., 1999), making learning the portable reward functions dif\ufb01cult. As illustrated in our experiments, this greatly limits the generalization capability of the method: the learned reward functions are not robust to environment changes, and it is dif\ufb01cult to use the algo2 ",
    "Background": "BACKGROUND Our inverse reinforcement learning method builds on the maximum causal entropy IRL framework (Ziebart, 2010), which considers an entropy-regularized Markov decision process (MDP), de\ufb01ned by the tuple (S, A, T , r, \u03b3, \u03c10). S, A are the state and action spaces, respectively, \u03b3 \u2208 (0, 1) is the discount factor. The dynamics or transition distribution T (s\u2032|a, s), the initial state distribution \u03c10(s), and the reward function r(s, a) are unknown in the standard reinforcement learning setup and can only be queried through interaction with the MDP. The goal of (forward) reinforcement learning is to \ufb01nd the optimal policy \u03c0\u2217 that maximizes the expected entropy-regularized discounted reward, under \u03c0, T , and \u03c10: \u03c0\u2217 = arg max\u03c0 E\u03c4\u223c\u03c0 \ufffd T \ufffd t=0 \u03b3t(r(st, at) + H(\u03c0(\u00b7|st))) \ufffd , where \u03c4 = (s0, a0, ...sT , aT ) denotes a sequence of states and actions induced by the policy and dynamics. It can be shown that the trajectory distribution induced by the optimal policy \u03c0\u2217(a|s) takes the form \u03c0\u2217(a|s) \u221d exp{Q\u2217 soft(st, at)} (Ziebart, 2010; Haarnoja et al., 2017), where Q\u2217 soft(st, at) = rt(s, a) + E(st+1,...)\u223c\u03c0[\ufffdT t\u2032=t \u03b3t\u2032(r(st\u2032, at\u2032) + H(\u03c0(\u00b7|st\u2032))] denotes the soft Qfunction. Inverse reinforcement learning instead seeks infer the reward function r(s, a) given a set of demonstrations D = {\u03c41, ..., \u03c4N}. In IRL, we assume the demonstrations are drawn from an optimal policy \u03c0\u2217(a|s). We can interpret the IRL problem as solving the maximum likelihood problem: max \u03b8 E\u03c4\u223cD [log p\u03b8(\u03c4)] , (1) Where p\u03b8(\u03c4) \u221d p(s0) \ufffdT t=0 p(st+1|st, at)e\u03b3tr\u03b8(st,at) parametrizes the reward function r\u03b8(s, a) but \ufb01xes the dynamics and initial state distribution to that of the MDP. Note that under deterministic dynamics, this simpli\ufb01es to an energy-based model where for feasible trajectories, p\u03b8(\u03c4) \u221d e \ufffdT t=0 \u03b3tr\u03b8(st,at) (Ziebart et al., 2008). Finn et al. (2016a) propose to cast optimization of Eqn. 1 as a GAN (Goodfellow et al., 2014) optimization problem. They operate in a trajectory-centric formulation, where the discriminator takes on a particular form (f\u03b8(\u03c4) is a learned function; \u03c0(\u03c4) is precomputed and its value \u201c\ufb01lled in\u201d): D\u03b8(\u03c4) = exp{f\u03b8(\u03c4)} exp{f\u03b8(\u03c4)} + \u03c0(\u03c4), (2) and the policy \u03c0 is trained to maximize R(\u03c4) = log(1 \u2212 D(\u03c4)) \u2212 log D(\u03c4). Updating the discriminator can be viewed as updating the reward function, and updating the policy can be viewed as improving the sampling distribution used to estimate the partition function. If trained to optimality, it can be shown that an optimal reward function can be extracted from the optimal discriminator as f \u2217(\u03c4) = R\u2217(\u03c4)+const, and \u03c0 recovers the optimal policy. We refer to this formulation as generative adversarial network guided cost learning (GAN-GCL) to discriminate it from guided cost learning (GCL) (Finn et al., 2016a). This formulation shares similarities with GAIL (Ho & Ermon, 2016), but GAIL does not place special structure on the discriminator, so the reward cannot be recovered. 4 ADVERSARIAL INVERSE REINFORCEMENT LEARNING (AIRL) In practice, using full trajectories as proposed by GAN-GCL can result in high variance estimates as compared to using single state, action pairs, and our experimental results show that this results in 3 Published as a conference paper at ICLR 2018 very poor learning. We could instead propose a straightforward conversion of Eqn. 2 into the single state and action case, where: D\u03b8(s, a) = exp{f\u03b8(s, a)} exp{f\u03b8(s, a)} + \u03c0(a|s). As in the trajectory-centric case, we can show that, at optimality, f \u2217(s, a) = log \u03c0\u2217(a|s) = A\u2217(s, a), the advantage function of the optimal policy. We justify this, as well as a proof that this algorithm solves the IRL problem in Appendix A . This change results in an ef\ufb01cient algorithm for imitation learning. However, it is less desirable for the purpose of reward learning. While the advantage is a valid optimal reward function, it is a heavily entangled reward, as it supervises each action based on the action of the optimal policy for the training MDP. Based on the analysis in the following Sec. 5, we cannot guarantee that this reward will be robust to changes in environment dynamics. In our experiments we demonstrate several cases where this reward simply encourages mimicking the expert policy \u03c0\u2217, and fails to produce desirable behavior even when changes to the environment are made. 5 THE REWARD AMBIGUITY PROBLEM We now discuss why IRL methods can fail to learn robust reward functions. First, we review the concept of reward shaping. Ng et al. (1999) describe a class of reward transformations that preserve the optimal policy. Their main theoretical result is that under the following reward transformation, \u02c6r(s, a, s\u2032) = r(s, a, s\u2032) + \u03b3\u03a6(s\u2032) \u2212 \u03a6(s) , (3) the optimal policy remains unchanged, for any function \u03a6 : S \u2192 R. Moreover, without prior knowledge of the dynamics, this is the only class of reward transformations that exhibits policy invariance. Because IRL methods only infer rewards from demonstrations given from an optimal agent, they cannot in general disambiguate between reward functions within this class of transformations, unless the class of learnable reward functions is restricted. We argue that shaped reward functions may not be robust to changes in dynamics. We formalize this notion by studying policy invariance in two MDPs M, M \u2032 which share the same reward and differ only in the dynamics, denoted as T and T \u2032, respectively. Suppose an IRL algorithm recovers a shaped, policy invariant reward \u02c6r(s, a, s\u2032) under MDP M where \u03a6 \u0338= 0. Then, there exists MDP pairs M, M \u2032 where changing the transition model from T to T \u2032 breaks policy invariance on MDP M \u2032. As a simple example, consider deterministic dynamics T(s, a) \u2192 s\u2032 and state-action rewards \u02c6r(s, a) = r(s, a) + \u03b3\u03a6(T(s, a)) \u2212 \u03a6(s). It is easy to see that changing the dynamics T to T \u2032 such that T \u2032(s, a) \u0338= T(s, a) means that \u02c6r(s, a) no longer lies in the equivalence class of Eqn. 3 for M \u2032. 5.1 DISENTANGLING REWARDS FROM DYNAMICS First, let the notation Q\u2217 r,T (s, a) denote the optimal Q-function with respect to a reward function r and dynamics T, and \u03c0\u2217 r,T (a|s) denote the same for policies. We \ufb01rst de\ufb01ne our notion of a \u201ddisentangled\u201d reward. De\ufb01nition 5.1 (Disentangled Rewards). A reward function r\u2032(s, a, s\u2032) is (perfectly) disentangled with respect to a ground-truth reward r(s, a, s\u2032) and a set of dynamics T such that under all dynamics T \u2208 T , the optimal policy is the same: \u03c0\u2217 r\u2032,T (a|s) = \u03c0\u2217 r,T (a|s) We could also expand this de\ufb01nition to include a notion of suboptimality. However, we leave this direction to future work. Under maximum causal entropy RL, the following condition is equivalent to two optimal policies being equal, since Q-functions and policies are equivalent representations (up to arbitrary functions of state f(s)): Q\u2217 r\u2032,T (s, a) = Q\u2217 r,T (s, a) \u2212 f(s) To remove unwanted reward shaping with arbitrary reward function classes, the learned reward function can only depend on the current state s. We require that the dynamics satisfy a decomposability 4 Published as a conference paper at ICLR 2018 Algorithm 1 Adversarial inverse reinforcement learning 1: Obtain expert trajectories \u03c4 E i 2: Initialize policy \u03c0 and discriminator D\u03b8,\u03c6. 3: for step t in {1, ..., N} do 4: Collect trajectories \u03c4i = (s0, a0, ..., sT , aT ) by executing \u03c0. 5: Train D\u03b8,\u03c6 via binary logistic regression to classify expert data \u03c4 E i from samples \u03c4i. 6: Update reward r\u03b8,\u03c6(s, a, s\u2032) \u2190 log D\u03b8,\u03c6(s, a, s\u2032) \u2212 log(1 \u2212 D\u03b8,\u03c6(s, a, s\u2032)) 7: Update \u03c0 with respect to r\u03b8,\u03c6 using any policy optimization method. 8: end for condition where functions over current states f(s) and next states g(s\u2032) can be isolated from their sum f(s) + g(s\u2032). This can be satis\ufb01ed for example by adding self transitions at each state to an ergodic MDP, or any of the environments used in our experiments. The exact de\ufb01nition of the condition, as well as proof of the following statements are included in Appendix B. Theorem 5.1. Let r(s) be a ground-truth reward, and T be a dynamics model satisfying the decomposability condition. Suppose IRL recovers a state-only reward r\u2032(s) such that it produces an optimal policy in T: Q\u2217 r\u2032,T (s, a) = Q\u2217 r,T (s, a) \u2212 f(s) Then, r\u2032(s) is disentangled with respect to all dynamics. Theorem 5.2. If a reward function r\u2032(s, a, s\u2032) is disentangled for all dynamics functions, then it must be state-only. i.e. If for all dynamics T, Q\u2217 r,T (s, a) = Q\u2217 r\u2032,T (s, a) + f(s) \u2200s, a Then r\u2032 is only a function of state. In the traditional IRL setup, where we learn the reward in a single MDP, our analysis motivates learning reward functions that are solely functions of state. If the ground truth reward is also only a function of state, this allows us to recover the true reward up to a constant. 6 LEARNING DISENTANGLED REWARDS WITH AIRL In the method presented in Section 4, we cannot learn a state-only reward function, r\u03b8(s), meaning that we cannot guarantee that learned rewards will not be shaped. In order to decouple the reward function from the advantage, we propose to modify the discriminator of Sec. 4 with the form: D\u03b8,\u03c6(s, a, s\u2032) = exp{f\u03b8,\u03c6(s, a, s\u2032)} exp{f\u03b8,\u03c6(s, a, s\u2032)} + \u03c0(a|s), where f\u03b8,\u03c6 is restricted to a reward approximator g\u03b8 and a shaping term h\u03c6 as f\u03b8,\u03c6(s, a, s\u2032) = g\u03b8(s, a) + \u03b3h\u03c6(s\u2032) \u2212 h\u03c6(s). (4) The additional shaping term helps mitigate the effects of unwanted shaping on our reward approximator g\u03b8 (and as we will show, in some cases it can account for all shaping effects). The entire training procedure is detailed in Algorithm 1. Our algorithm resembles GAIL (Ho & Ermon, 2016) and GAN-GCL (Finn et al., 2016a), where we alternate between training a discriminator to classify expert data from policy samples, and update the policy to confuse the discriminator. The advantage of this approach is that we can now parametrize g\u03b8(s) as solely a function of the state, allowing us to extract rewards that are disentangled from the dynamics of the environment in which they were trained. In fact, under this restricted case, we can show the following under deterministic environments with a state-only ground truth reward (proof in Appendix C): g\u2217(s) = r\u2217(s) + const, h\u2217(s) = V \u2217(s) + const, where r\u2217 is the true reward function. Since f \u2217 must recover to the advantage as shown in Sec. 4, h recovers the optimal value function V \u2217, which serves as the reward shaping term. 5 ",
    "Experiments": "EXPERIMENTS In our experiments, we aim to answer two questions: 1. Can AIRL learn disentangled rewards that are robust to changes in environment dynamics? 2. Is AIRL ef\ufb01cient and scalable to high-dimensional continuous control tasks? To answer 1, we evaluate AIRL in transfer learning scenarios, where a reward is learned in a training environment, and optimized in a test environment with signi\ufb01cantly different dynamics. We show that rewards learned with our algorithm under the constraint presented in Section 5 still produce optimal or near-optimal behavior, while na\u00a8\u0131ve methods that do not consider reward shaping fail. We also show that in small MDPs, we can recover the exact ground truth reward function. To answer 2, we compare AIRL as an imitation learning algorithm against GAIL (Ho & Ermon, 2016) and the GAN-based GCL algorithm proposed by Finn et al. (2016a), which we refer to as GAN-GCL, on standard benchmark tasks that do not evaluate transfer. Note that Finn et al. (2016a) does not implement or evaluate GAN-GCL and, to our knowledge, we present the \ufb01rst empirical evaluation of this algorithm. We \ufb01nd that AIRL performs on par with GAIL in a traditional imitation learning setup while vastly outperforming it in transfer learning setups, and outperforms GAN-GCL in both settings. It is worth noting that, except for (Finn et al., 2016b), our method is the only IRL algorithm that we are aware of that scales to high dimensional tasks with unknown dynamics, and although GAIL (Ho & Ermon, 2016) resembles an IRL algorithm in structure, it does not recover disentangled reward functions, making it unable to re-optimize the learned reward under changes in the environment, as we illustrate below. For our continuous control tasks, we use trust region policy optimization (Schulman et al., 2015) as our policy optimization algorithm across all evaluated methods, and in the tabular MDP task, we use soft value iteration. We obtain expert demonstrations by training an expert policy on the ground truth reward, but hide the ground truth reward from the IRL algorithm. In this way, we simulate a scenario where we wish to use RL to solve a task but wish to refrain from manual reward engineering and instead seek to learn a reward function from demonstrations. Our code and additional supplementary material including videos will be available at https://sites.google.com/view/ adversarial-irl, and hyper-parameter and architecture choices are detailed in Appendix D. 7.1 RECOVERING TRUE REWARDS IN TABULAR MDPS We \ufb01rst consider MaxEnt IRL in a toy task with randomly generated MDPs. The MDPs have 16 states, 4 actions, randomly drawn transition matrices, and a reward function that always gives a reward of 1.0 when taking an action from state 0. The initial state is always state 1. The optimal reward, learned reward with a state-only reward function, and learned reward using a state-action reward function are shown in Fig. 1. We subtract a constant offset from all reward functions so that they share the same mean for visualization - this does not in\ufb02uence the optimal policy. AIRL with a state-only reward function is able to recover the ground truth reward, but AIRL with a state-action reward instead recovers a shaped advantage function. We also show that in the transfer learning setup, under a new transition matrix T \u2032, the optimal policy under the state-only reward achieves optimal performance (it is identical to the ground truth reward) whereas the state-action reward only improves marginally over uniform random policy. The learning curve for this experiment is shown in Fig 2. 6 Published as a conference paper at ICLR 2018 Figure 1: Ground truth (a) and learned rewards (b, c) on the random MDP task. Dark blue corresponds to a reward of 1, and white corresponds to 0. Note that AIRL with a state-only reward recovers the ground truth, whereas the state-action reward is shaped. Figure 2: Learning curve for the transfer learning experiment on tabular MDPs. Value iteration steps are plotted on the x-axis, against returns for the policy on the y-axis. 7.2 DISENTANGLING REWARDS IN CONTINUOUS CONTROL TASKS To evaluate whether our method can learn disentangled rewards in higher dimensional environments, we perform transfer learning experiments on continuous control tasks. In each task, a reward is learned via IRL on the training environment, and the reward is used to reoptimize a new policy on a test environment. We train two IRL algorithms, AIRL and GAN-GCL, with state-only and stateaction rewards. We also include results for directly transferring the policy learned with GAIL, and an oracle result that involves optimizing the ground truth reward function with TRPO. Numerical results for these environment transfer experiments are given in Table 1. The \ufb01rst task involves a 2D point mass navigating to a goal position in a small maze when the position of the walls are changed between train and test time. At test time, the agent cannot simply mimic the actions learned during training, and instead must successfully infer that the goal in the maze is to reach the target. The task is shown in Fig. 3. Only AIRL trained with state-only rewards is able to consistently navigate to the goal when the maze is modi\ufb01ed. Direct policy transfer and state-action IRL methods learn rewards which encourage the agent to take the same path taken in the training environment, which is blocked in the test environment. We plot the learned reward in Fig. 4. In our second task, we modify the agent itself. We train a quadrupedal \u201cant\u201d agent to run forwards, and at test time we disable and shrink two of the front legs of the ant such that it must signi\ufb01cantly change its gait.We \ufb01nd that AIRL is able to learn reward functions that encourage the ant to move forwards, acquiring a modi\ufb01ed gait that involves orienting itself to face the forward direction and crawling with its two hind legs. Alternative methods, including transferring a policy learned by GAIL (which achieves near-optimal performance with the unmodi\ufb01ed agent), fail to move forward at all. We show the qualitative difference in behavior in Fig. 5. We have demonstrated that AIRL can learn disentangled rewards that can accommodate signi\ufb01cant domain shift even in high-dimensional environments where it is dif\ufb01cult to exactly extract the true reward. GAN-GCL can presumably learn disentangled rewards, but we \ufb01nd that the trajectorycentric formulation does not perform well even in learning rewards in the original task, let alone transferring to a new domain. GAIL learns successfully in the training domain, but does not acquire a representation that is suitable for transfer to test domains. 7 Published as a conference paper at ICLR 2018 Figure 3: Illustration of the shifting maze task, where the agent (blue) must reach the goal (green). During training the agent must go around the wall on the left side, but during test time it must go around on the right. Figure 4: Reward learned on the point mass shifting maze task. The goal is located at the green star and the agent starts at the white circle. Note that there is little reward shaping, which enables the reward to transfer well. Figure 5: Top row: An ant running forwards (right in the picture) in the training environment. Bottom row: Behavior acquired by optimizing a state-only reward learned with AIRL on the disabled ant environment. Note that the ant must orient itself before crawling forward, which is a qualitatively different behavior from the optimal policy in the original environment, which runs sideways. Table 1: Results on transfer learning tasks. Mean scores (higher is better) are reported over 5 runs. We also include results for TRPO optimizing the ground truth reward, and the performance of a policy learned via GAIL on the training environment. State-Only? Point Mass-Maze Ant-Disabled GAN-GCL No -40.2 -44.8 GAN-GCL Yes -41.8 -43.4 AIRL (ours) No -31.2 -41.4 AIRL (ours) Yes -8.82 130.3 GAIL, policy transfer N/A -29.9 -58.8 TRPO, ground truth N/A -8.45 315.5 7.3 BENCHMARK TASKS FOR IMITATION LEARNING Finally, we evaluate AIRL as an imitation learning algorithm against the GAN-GCL and the stateof-the-art GAIL on several benchmark tasks. Each algorithm is presented with 50 expert demonstrations, collected from a policy trained with TRPO on the ground truth reward function. For AIRL, we use an unrestricted state-action reward function as we are not concerned with reward transfer. Numerical results are presented in Table 2.These experiments do not test transfer, and in a sense can be regarded as \u201ctesting on the training set,\u201d but they match the settings reported in prior work (Ho & Ermon, 2016). 8 ",
    "Conclusion": "CONCLUSION We presented AIRL, a practical and scalable IRL algorithm that can learn disentangled rewards and greatly outperforms both prior imitation learning and IRL algorithms. We show that rewards learned with AIRL transfer effectively under variation in the underlying domain, in contrast to unmodi\ufb01ed IRL methods which tend to recover brittle rewards that do not generalize well and GAIL, which does not recover reward functions at all. In small MDPs where the optimal policy and reward are unambiguous, we also show that we can exactly recover the ground-truth rewards up to a constant. ACKNOWLEDGEMENTS This research was supported by the National Science Foundation through IIS-1651843, IIS-1614653, and IIS-1637443. We would like to thank Roberto Calandra for helpful feedback on the paper. ",
    "References": "REFERENCES Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning (ICML), 2004. Kareem Amin, Nan Jiang, and Satinder P. Singh. Repeated inverse reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), 2017. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00b4e. Concrete problems in AI safety. ArXiv Preprint, abs/1606.06565, 2016. Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009. Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), 2011. Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. abs/1611.03852, 2016a. 9 Published as a conference paper at ICLR 2018 Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning (ICML), 2016b. Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine. Generalizing skills with semisupervised reinforcement learning. In International Conference on Learning Representations (ICLR), 2017. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS). 2014. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning (ICML), 2017. Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2016. Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with gaussian processes. In Advances in Neural Information Processing Systems (NIPS), 2011. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning (JMLR), 2016. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, feb 2015. ISSN 0028-0836. Andrew Ng and Stuart Russell. Algorithms for reinforcement learning. In International Conference on Machine Learning (ICML), 2000. Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning (ICML), 1999. Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active perception, and action in minecraft. In International Conference on Machine Learning (ICML), 2016. Nathan Ratliff, David Bradley, J. Andrew Bagnell, and Joel Chestnutt. Boosting structured prediction for imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2007. Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In International Conference on Machine Learning (ICML), 2006. Stuart Russell. Learning agents for uncertain environments. In Conference On Learning Theory (COLT), 1998. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust Region Policy Optimization. In International Conference on Machine Learning (ICML), 2015. Eiji Uchibe. Model-free deep inverse reinforcement learning by logistic regression. Neural Processing Letters, 2017. ISSN 1573-773X. doi: 10.1007/s11063-017-9702-7. Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learning. In arXiv preprint arXiv:1507.04888, 2015. Brian Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD thesis, Carnegie Mellon University, 2010. Brian Ziebart, Andrew Maas, Andrew Bagnell, and Anind Dey. Maximum entropy inverse reinforcement learning. In AAAI Conference on Arti\ufb01cial Intelligence (AAAI), 2008. 10 Published as a conference paper at ICLR 2018 APPENDICES A JUSTIFICATION OF AIRL In this section, we show that the objective of AIRL matches that of solving the maximum causal entropy IRL problem. We use a similar method as Finn et al. (2016a), which shows the justi\ufb01cation of GAN-GCL for the trajectory-centric formulation. For simplicity we derive everything in the undiscounted case. A.1 SETUP As mentioned in Section 3, the goal of IRL can be seen as training a generative model over trajectories as: max \u03b8 J(\u03b8) = max \u03b8 E\u03c4\u223cD[log p\u03b8(\u03c4)] Where the distribution p\u03b8(\u03c4) is parametrized as p\u03b8(\u03c4) \u221d p(s0) \ufffdT \u22121 t=0 p(st+1|st, at)er\u03b8(st,at). We can compute the gradient with respect to \u03b8 as follows: \u2202 \u2202\u03b8J(\u03b8) = ED[ \u2202 \u2202\u03b8 log p\u03b8(\u03c4)] == ED[ T \ufffd t=0 \u2202 \u2202\u03b8r\u03b8(st, at)] \u2212 \u2202 \u2202\u03b8 log Z\u03b8 = ED[ T \ufffd t=0 \u2202 \u2202\u03b8r\u03b8(st, at)] \u2212 Ep\u03b8[ T \ufffd t=0 \u2202 \u2202\u03b8r\u03b8(st, at)] Let p\u03b8,t(st, at) = \ufffd st\u2032\u0338=t,at\u2032\u0338=t p\u03b8(\u03c4) denote the state-action marginal at time t. Rewriting the above equation, we have: \u2202 \u2202\u03b8J(\u03b8) = T \ufffd t=0 ED[ \u2202 \u2202\u03b8r\u03b8(st, at)] \u2212 Ep\u03b8,t[ \u2202 \u2202\u03b8r\u03b8(st, at)] As it is dif\ufb01cult to draw samples from p\u03b8, we instead train a separate importance sampling distribution \u00b5(\u03c4). For the choice of this distribution, we follow Finn et al. (2016a) and use a mixture policy \u00b5(a|s) = 1 2\u03c0(a|s) + 1 2 \u02c6p(a|s), where \u02c6p(a|s) is a rough density estimate trained on the demonstrations. This is justi\ufb01ed as reducing the variance of the importance sampling estimate when the policy \u03c0(a|s) has poor coverage over the demonstrations in the early stages of training. Thus, our new gradient is: \u2202 \u2202\u03b8J(\u03b8) = T \ufffd t=0 ED[ \u2202 \u2202\u03b8r\u03b8(st, at)] \u2212 E\u00b5t[p\u03b8,t(st, at) \u00b5t(st, at) \u2202 \u2202\u03b8r\u03b8(st, at)] (5) We additionally wish to adapt the importance sampler \u03c0 to reduce variance, by minimizing DKL(\u03c0(\u03c4)||p\u03b8(\u03c4)). The policy trajectory distribution factorizes as \u03c0(\u03c4) = p(s0) \ufffdT \u22121 t=0 p(st+1|st, at)\u03c0(at|st). The dynamics and initial state terms inside \u03c0(\u03c4) and p\u03b8(\u03c4) cancel, leaving the entropy-regularized policy objective: max \u03c0 E\u03c0[ T \ufffd t=0 r\u03b8(st, at) \u2212 log \u03c0(at|st))] (6) In AIRL, we replace the cost learning objective with training a discriminator of the following form: D\u03b8(s, a) = exp{f\u03b8(s, a)} exp{f\u03b8(s, a)} + \u03c0(a|s) (7) The objective of the discriminator is to minimize cross-entropy loss between expert demonstrations and generated samples: L(\u03b8) = T \ufffd t=0 \u2212ED [log D\u03b8(st, at)] \u2212 E\u03c0t [log(1 \u2212 D\u03b8(st, at))] 11 Published as a conference paper at ICLR 2018 We replace the policy optimization objective with the following reward: \u02c6r(s, a) = log(D\u03b8(s, a)) \u2212 log(1 \u2212 D\u03b8(s, a)) A.2 DISCRIMINATOR OBJECTIVE First, we show that training the gradient of the discriminator objective is the same as Eqn. 5. We write the negative loss to turn the minimization problem into maximization, and use \u00b5 to denote a mixture between the dataset and policy samples. \u2212L(\u03b8) = T \ufffd t=0 ED [log D\u03b8(st, at)] + E\u03c0t [log(1 \u2212 D\u03b8(st, at))] = T \ufffd t=0 ED \ufffd log exp{f\u03b8(st, at)} exp{f\u03b8(st, at)} + \u03c0(at|st) \ufffd + E\u03c0t \ufffd log \u03c0(at|st) exp{f\u03b8(st, at)} + \u03c0(at|st) \ufffd = T \ufffd t=0 ED [f\u03b8(st, at)] + E\u03c0t [log \u03c0(at|st)] \u2212 2E\u00af\u00b5t [log (exp{f\u03b8(st, at)} + \u03c0(at|st))] Taking the derivative w.r.t. \u03b8, \u2202 \u2202\u03b8L(\u03b8) = T \ufffd t=0 ED \ufffd \u2202 \u2202\u03b8f\u03b8(st, at) \ufffd \u2212 E\u00b5t \ufffd exp{f\u03b8(st, at)} 1 2 exp{f\u03b8(st, at)} + 1 2\u03c0(at|st) \u2202 \u2202\u03b8f\u03b8(st, at) \ufffd Multiplying the top and bottom of the fraction in the second expectation by the state marginal \u03c0(st) = \ufffd a \u03c0t(st, at), and grouping terms we get: \u2202 \u2202\u03b8L(\u03b8) = T \ufffd t=0 ED \ufffd \u2202 \u2202\u03b8f\u03b8(st, at) \ufffd \u2212 E\u00b5 \ufffd \u02c6p\u03b8,t(st, at) \u02c6\u00b5t(st, at) \u2202 \u2202\u03b8f\u03b8(st, at) \ufffd Where we have written \u02c6p\u03b8,t(st, at) = exp{f\u03b8(st, at)}\u03c0t(st), and \u02c6\u00b5 to denote a mixture between \u02c6p\u03b8(s, a) and policy samples. This expression matches Eqn. 5, with f\u03b8(s, a) serving as the reward function, when \u03c0 maximizes the policy objective so that \u02c6p\u03b8(s, a) = p\u03b8(s, a). A.3 POLICY OBJECTIVE Next, we show that the policy objective matches that of the sampler of Eqn. 6. The objective of the policy is to maximize with respect to the reward \u02c6rt(s, a). First, note that: \u02c6rt(s, a) = log(D\u03b8(s, a)) \u2212 log(1 \u2212 D\u03b8(s, a)) = log ef\u03b8(s,a) ef\u03b8(s,a) + \u03c0(a|s) \u2212 log \u03c0(a|s) ef\u03b8(s,a) + \u03c0(a|s) = f\u03b8(s, a) \u2212 log \u03c0(a|s) Thus, when \u02c6r(s, a) is summed over entire trajectories, we obtain the entropy-regularized policy objective E\u03c0 \ufffd T \ufffd t=0 \u02c6rt(st, at) \ufffd = E\u03c0 \ufffd T \ufffd t=0 f\u03b8(st, at) \u2212 log \u03c0(at|st) \ufffd Where f\u03b8 serves as the reward function. A.4 f\u03b8(s, a) RECOVERS THE ADVANTAGE The global minimum of the discriminator objective is achieved when \u03c0 = \u03c0E, where \u03c0 denotes the learned policy (the \u201dgenerator\u201d of a GAN) and \u03c0E denotes the policy under which demonstrations were collected (Goodfellow et al., 2014). At this point, the output of the discriminator is 1 2 for all values of s, a, meaning we have exp{f\u03b8(s, a)} = \u03c0E(a|s), or f \u2217(s, a) = log \u03c0E(a|s) = A\u2217(s, a). 12 Published as a conference paper at ICLR 2018 B STATE-ONLY INVERSE REINFORCEMENT LEARNING In this section we include proofs for Theorems 5.1 and 5.2, and the condition on the dynamics necessary for them to hold. De\ufb01nition B.1 (Decomposability Condition). Two states s1, s2 are de\ufb01ned as \u201d1-step linked\u201d under a dynamics or transition distribution T(s\u2032|a, s) if there exists a state s that can reach s1 and s2 with positive probability in one time step. Also, we de\ufb01ne that this relationship can transfer through transitivity: if s1 and s2 are linked, and s2 and s3 are linked, then we also consider s1 and s3 to be linked. A transition distribution T satis\ufb01es the decomposability condition if all states in the MDP are linked with all other states. The key reason for needing this condition is that it allows us to decompose the functions state dependent f(s) and next state dependent g(s\u2032) from their sum f(s) + g(s\u2032), as stated below: Lemma B.1. Suppose the dynamics for an MDP satisfy the decomposability condition. Then, for functions a(s), b(s), c(s), d(s), if for all s, s\u2032: a(s) + b(s\u2032) = c(s) + d(s\u2032) Then for for all s, a(s) = c(s) + const b(s) = d(s) + const Proof. Rearranging, we have: a(s) \u2212 c(s) = b(s\u2032) \u2212 d(s\u2032) Let us rewrite f(s) = a(s)\u2212c(s). This means we have f(s) = b(s\u2032)\u2212d(s\u2032) for some function only dependent on s. In order for this to be representable, the term b(s\u2032) \u2212 d(s\u2032) must be equal for all successor states s\u2032 from s. Under the decomposability condition, all successor states must therefore be equal in this manner through transitivity, meaning we have b(s\u2032) \u2212 d(s\u2032) must be constant with respect to s. Therefore, a(s) = c(s) + const. We can then substitute this expression back in to the original equation to derive b(s) = d(s) + const. We consider the case when the ground truth reward is state-only. We now show that if the learned reward is also state-only, then we guarantee learning disentangled rewards, and vice-versa (suf\ufb01ciency and necessity). Theorem 5.1. Let r(s) be a ground-truth reward, and T be a dynamics model satisfying the decomposability condition. Suppose IRL recovers a state-only reward r\u2032(s) such that it produces an optimal policy in T: Q\u2217 r\u2032,T (s, a) = Q\u2217 r,T (s, a) \u2212 f(s) Then, r\u2032(s) is disentangled with respect to all dynamics. Proof. We show that r\u2032(s) must equal the ground-truth reward up to constants (modifying rewards by constants does not change the optimal policy). Let r\u2032(s) = r(s) + \u03c6(s) for some arbitrary function of state \u03c6(s). We have: Q\u2217 r(s, a) = r(s) + \u03b3Es\u2032[softmax a\u2032 Q\u2217 r(s\u2032, a\u2032)] Q\u2217 r(s, a) \u2212 f(s) = r(s) \u2212 f(s) + \u03b3Es\u2032[softmax a\u2032 Q\u2217 r(s\u2032, a\u2032)] Q\u2217 r(s, a) \u2212 f(s) = r(s) + \u03b3Es\u2032[f(s\u2032)] \u2212 f(s) + \u03b3Es\u2032[softmax a\u2032 Q\u2217 r(s\u2032, a\u2032) \u2212 f(s\u2032)] Q\u2217 r\u2032(s, a) = r(s) + \u03b3Es\u2032[f(s\u2032)] \u2212 f(s) + \u03b3Es\u2032[softmax a\u2032 Q\u2217 r\u2032(s\u2032, a\u2032)] From here, we see that: r\u2032(s) = r(s) + \u03b3Es\u2032[f(s\u2032)] \u2212 f(s) 13 Published as a conference paper at ICLR 2018 Meaning we must have for all s, a: \u03c6(s) = \u03b3Es\u2032[f(s\u2032)] \u2212 f(s) This places the requirement that all successor states from s must have the same potential f(s). Under the decomposability condition, every state in the MDP can be linked with such an equality statement, meaning that f(s) is constant. Thus, r\u2032(s) = r(s) + const. Theorem 5.2. If a reward function r\u2032(s, a, s\u2032) is disentangled for all dynamics functions, then it must be state-only. i.e. If for all dynamics T, Q\u2217 r,T (s, a) = Q\u2217 r\u2032,T (s, a) + f(s) \u2200s, a Then r\u2032 is only a function of state. Proof. We show the converse, namely that if r\u2032(s, a, s\u2032) can depend on a or s\u2032, then there exists a dynamics model T such that the optimal policy is changed, i.e. Q\u2217 r,T (s, a) \u0338= Q\u2217 r\u2032,T (s, a) + f(s) \u2200s, a. Consider the following 3-state MDP with deterministic dynamics and starting state S: S A B a, 0 b, 0 s, +1 s, -1 We denote the action with a small letter, i.e. taking the action a from S brings the agent to state A, receiving a reward of 0. For simplicity, assume the discount factor \u03b3 = 1. The optimal policy here takes the a action, returns to s, and repeat for in\ufb01nite positive reward. An action-dependent reward which induces the same optimal policy would be to move the reward from the action returning to s to the action going to a or s: r\u2032(s, a) = State Action Reward S a +1 S b -1 A s 0 B s 0 This corresponds to the shaping potential \u03c6(S) = 0, \u03c6(A) = 1, \u03c6(B) = \u22121. Now suppose we modify the dynamics such that action a leads to B and action b leads to A: S A B b, 0 a, 0 s, +1 s, -1 Optimizing r\u2032 on this new MDP results in a different policy than optimizing r, as the agent visits B, resulting in in\ufb01nite negative reward. C AIRL RECOVERS REWARDS UP TO CONSTANTS In this section, we prove that AIRL can recover the ground truth reward up to constants if the ground truth is only a function of state r(s). For simplicity, we consider deterministic environments, so that s\u2032 is uniquely de\ufb01ned by s, a, and we restrict AIRL\u2019s reward estimator g to only be a function of state. 14 Published as a conference paper at ICLR 2018 Theorem C.1. Suppose we use AIRL with a discriminator of the form f(s, a, s\u2032) = g\u03b8(s) + \u03b3h\u03c6(s\u2032) \u2212 h\u03c6(s) We also assume we have deterministic dynamics, in addition to the decomposability condition on the dynamics from Thm 5.1. Then if AIRL recovers the optimal f \u2217(s, a, s\u2032), we have g\u2217 \u03b8(s) = r(s) + const h\u2217 \u03c6(s) = V \u2217(s) + const Proof. From Appendix A.4, we have f \u2217(s, a, s\u2032) = A\u2217(s, a), so f \u2217(s, a, s\u2032) = Q\u2217(s, a) \u2212 V \u2217(s) = r(s) + \u03b3V \u2217(s\u2032) \u2212 V \u2217(s). Substituting the form of f, we have for all s, s\u2032: g\u2217(s) + \u03b3h\u2217(s\u2032) \u2212 h\u2217(s) = r(s) + \u03b3V \u2217(s\u2032) \u2212 V \u2217(s) Applying Lemma B.1 with a(s) = g\u2217(s) \u2212 h\u2217(s), b(s\u2032) = \u03b3h\u2217(s\u2032), c(s) = r(s) \u2212 V \u2217(s), and d(s\u2032) = \u03b3V \u2217(s\u2032) we have the result. D EXPERIMENT DETAILS In this section we detail hyperparameters and training procedures used for our experiments. These hyperparameters were selected via a grid search. D.1 NETWORK ARCHITECTURES For the tabular MDP environment, we also use a tabular representation for function approximators. For continuous control experiments, we use a two-layer ReLU network with 32 units for the discriminator of GAIL and GAN-GCL. For AIRL, we use a linear function approximator for the reward term g and a 2-layer ReLU network for the shaping term h. For the policy, we use a two-layer (32 units) ReLU gaussian policy. D.2 OTHER HYPERPARAMETERS Entropy regularization: We use an entropy regularizer weight of 0.1 for Ant, Swimmer, and HalfCheetah across all methods. We use an entropy regularizer weight of 1.0 on the point mass environment. TRPO Batch Size: For Ant, Swimmer and HalfCheetah environments, we use a batch size of 10000 steps per TRPO update. For pendulum, we use a batch size of 2000. D.3 OTHER TRAINING DETAILS IRL methods commonly learn rewards which explain behavior locally for the current policy, because the reward can \u201dforget\u201d the signal that it gave to an earlier policy. This makes rewards obtained at the end of training dif\ufb01cult to optimize from scratch, as they over\ufb01t to samples from the current iteration. To somewhat migitate this effect, we mix policy samples from the previous 20 iterations of training as negatives when training the discriminator. We use this strategy for both AIRL and GAN-GCL. 15 ",
    "title": "",
    "paper_info": "Published as a conference paper at ICLR 2018\nLEARNING ROBUST REWARDS WITH ADVERSARIAL\nINVERSE REINFORCEMENT LEARNING\nJustin Fu, Katie Luo, Sergey Levine\nDepartment of Electrical Engineering and Computer Science\nUniversity of California, Berkeley\nBerkeley, CA 94720, USA\njustinjfu@eecs.berkeley.edu,katieluo@berkeley.edu,\nsvlevine@eecs.berkeley.edu\nABSTRACT\nReinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning meth-\nods can remove the need for explicit engineering of policy or value features, but\nstill require a manually speci\ufb01ed reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndif\ufb01cult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learn-\ning algorithm based on an adversarial reward learning formulation. We demon-\nstrate that AIRL is able to recover reward functions that are robust to changes\nin dynamics, enabling us to learn policies even under signi\ufb01cant variation in the\nenvironment seen during training. Our experiments show that AIRL greatly out-\nperforms prior methods in these transfer settings.\n1\nINTRODUCTION\nWhile reinforcement learning (RL) provides a powerful framework for automating decision making\nand control, signi\ufb01cant engineering of elements such as features and reward functions has typically\nbeen required for good practical performance. In recent years, deep reinforcement learning has al-\nleviated the need for feature engineering for policies and value functions, and has shown promising\nresults on a range of complex tasks, from vision-based robotic control (Levine et al., 2016) to video\ngames such as Atari (Mnih et al., 2015) and Minecraft (Oh et al., 2016). However, reward engineer-\ning remains a signi\ufb01cant barrier to applying reinforcement learning in practice. In some domains,\nthis may be dif\ufb01cult to specify (for example, encouraging \u201csocially acceptable\u201d behavior), and in\nothers, a na\u00a8\u0131vely speci\ufb01ed reward function can produce unintended behavior (Amodei et al., 2016).\nMoreover, deep RL algorithms are often sensitive to factors such as reward sparsity and magnitude,\nmaking well performing reward functions particularly dif\ufb01cult to engineer.\nInverse reinforcement learning (IRL) (Russell, 1998; Ng & Russell, 2000) refers to the problem of\ninferring an expert\u2019s reward function from demonstrations, which is a potential method for solv-\ning the problem of reward engineering. However, inverse reinforcement learning methods have\ngenerally been less ef\ufb01cient than direct methods for learning from demonstration such as imitation\nlearning (Ho & Ermon, 2016), and methods using powerful function approximators such as neural\nnetworks have required tricks such as domain-speci\ufb01c regularization and operate inef\ufb01ciently over\nwhole trajectories (Finn et al., 2016b). There are many scenarios where IRL may be preferred over\ndirect imitation learning, such as re-optimizing a reward in novel environments (Finn et al., 2017) or\nto infer an agent\u2019s intentions, but IRL methods have not been shown to scale to the same complexity\nof tasks as direct imitation learning. However, adversarial IRL methods (Finn et al., 2016b;a) hold\npromise for tackling dif\ufb01cult tasks due to the ability to adapt training samples to improve learning\nef\ufb01ciency.\nPart of the challenge is that IRL is an ill-de\ufb01ned problem, since there are many optimal policies\nthat can explain a set of demonstrations, and many rewards that can explain an optimal policy (Ng\n1\narXiv:1710.11248v2  [cs.LG]  13 Aug 2018\n",
    "GPTsummary": "- (1): The research background of this article is the problem of designing reward functions in reinforcement learning. While deep reinforcement learning has removed the need for explicit feature engineering in policies and value functions, the specification of reward functions remains a significant bottleneck in applications of reinforcement learning.\n\n- (2): Past methods for addressing this problem, such as inverse reinforcement learning, have struggled to scale up to high-dimensional tasks with unknown dynamics. The authors propose a novel adversarial inverse reinforcement learning algorithm that is practical and scalable. The approach is motivated by the idea of formulating the problem of inverse reinforcement learning as a two-player game between a reward function generator and a discriminator. \n\n- (3): The research methodology proposed in this paper is the Adversarial Inverse Reinforcement Learning (AIRL) algorithm, which formulates the problem of inverse reinforcement learning as an adversarial training problem. The generator learns to produce reward functions that lead to policies that are indistinguishable from the expert demonstration data, while the discriminator learns to differentiate between the expert data and generated policies. The generator and discriminator are trained in an adversarial way, resulting in a policy that is robust to changes in dynamics.\n\n- (4): The approach is evaluated on several benchmark tasks, including Atari games and MuJoCo environments, under different transfer scenarios. The experimental results demonstrate that the AIRL algorithm can produce reward functions that achieve better performance than prior methods, especially in scenarios with significant variations in the environment. The performance of the method supports the goal of learning robust policies from demonstrations even when the dynamics of the environment are uncertain.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant for the field of reinforcement learning as it proposes a practical and scalable algorithm, Adversarial Inverse Reinforcement Learning (AIRL), to learn reward functions from demonstrations that are robust to changes in dynamics.\n\n- (2): Innovation point: The authors propose a novel adversarial approach to learn reward functions in reinforcement learning, which is a significant improvement over past methods. Performance: The experimental results demonstrate that the AIRL algorithm outperforms prior methods in terms of transferring learned policies to different environments. Workload: The workload is not explicitly mentioned, but the authors mention that the proposed algorithm is practical and scalable.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work is significant for the field of reinforcement learning as it proposes a practical and scalable algorithm, Adversarial Inverse Reinforcement Learning (AIRL), to learn reward functions from demonstrations that are robust to changes in dynamics.\n\n- (2): Innovation point: The authors propose a novel adversarial approach to learn reward functions in reinforcement learning, which is a significant improvement over past methods. Performance: The experimental results demonstrate that the AIRL algorithm outperforms prior methods in terms of transferring learned policies to different environments. Workload: The workload is not explicitly mentioned, but the authors mention that the proposed algorithm is practical and scalable.\n\n\n"
}