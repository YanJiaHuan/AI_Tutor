{
    "Abstract": "Abstract We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually uni\ufb01es multiple previous methods in exploration. We also derive a practical algorithm that achieves ef\ufb01cient exploration on challenging control tasks. 1 ",
    "Introduction": "Introduction Deep reinforcement learning (RL) has enjoyed numerous recent successes in various domains such as video games and robotics control [Schulman et al., 2015; Duan et al., 2016; Levine et al., 2016]. Deep RL algorithms typically apply naive exploration strategies such as \u03f5\u2212greedy [Mnih et al., 2013; Lillicrap et al., 2016]. However, such myopic strategies cannot lead to systematic exploration in hard environments [Osband et al., 2017]. We provide an exploration algorithm based on distributional RL [Bellemare et al., 2017] and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that the proposed algorithm provides a conceptual uni\ufb01cation of multiple previous methods on exploration in deep reinforcement learning setting. We also show that the algorithm achieves ef\ufb01cient exploration in challenging environments. 2 ",
    "Background": "Background 2.1 Markov Decision Process and Value Based Reinforcement Learning In a Markov Decision Process (MDP), at time step t \u2265 0, an agent is in state st \u2208 S, takes action at \u2208 A, receives reward rt and gets transitioned to next state st+1 \u223c p(st+1|st, at). At time t = 0 the agent\u2019s state distribution follows s0 \u223c \u03c1(s0). A policy is a mapping from a state to a distribution over action at \u223c \u03c0(\u00b7|st). The objective is to \ufb01nd a policy \u03c0 to maximize the discounted cumulative reward J = Es0\u223c\u03c1,at\u223c\u03c0(\u00b7|st) \ufffd \u221e \ufffd t=0 rt\u03b3t\ufffd , (1) where \u03b3 \u2208 (0, 1] is a discount factor. In state s, the actionvalue function Q\u03c0(s, a) is de\ufb01ned as the expected cumulative reward that could be received by \ufb01rst taking action a and following policy \u03c0 thereafter Q\u03c0(s, a) = Eat\u223c\u03c0(\u00b7|st) \ufffd \u221e \ufffd t=0 rt\u03b3t|s0 = s, a0 = a \ufffd . From the above de\ufb01nition, it can be shown that Q\u03c0(st, at) satis\ufb01es the Bellman equation Q\u03c0(st, at) = E[rt + \u03b3Q\u03c0(st+1, at+1)], \u2200(st, at). Let \u03c0\u2217 = arg max\u03c0 J be the optimal policy and Q\u2217(s, a) its action value function. Q\u2217(s, a) satis\ufb01es the following Bellman equation Q\u2217(st, at) = E \ufffd rt + \u03b3 max a Q\u2217(st+1, a) \ufffd , \u2200(st, at). The above equations illustrate the temporal consistency of the action value functions that allows for the design of learning algorithms. De\ufb01ne Bellman operator T \u2217Q(st, at) := E[rt + \u03b3 max a\u2032 Q(st+1, a\u2032)]. When \u03b3 \u2208 (0, 1), starting from any Q(0)(s, a), iteratively applying the operator Q(t+1)(s, a) \u2190 T \u2217Q(t)(s, a) leads to convergence Q(t)(s, a) \u2192 Q\u2217(s, a) as t \u2192 \u221e. In high dimensional cases, it is critical to use function approximation as a compact representation of action values. Let Q\u03b8(s, a) be such a function with parameter \u03b8 that approximates a table of action values with entry (s, a). The aim is to \ufb01nd \u03b8 such that Q\u03b8(s, a) \u2248 Q\u2217(s, a). Let \u03a0 be the operator that projects arbitrary vector Q(s, a) \u2208 R|S|\u00d7|A| to the subspace spanned by function Q\u03b8(s, a). Since the update of action values can now only take place in the subspace spanned by function Q\u03b8(s, a), the iterate Q(t)(s, a) is updated as Q(t+1)(s, a) \u2190 \u03a0T \u2217Q(t)(s, a). In cases where Q\u03b8(s, a) is linear, the above procedure can be shown to converge [Tsitsiklis and Van Roy, 1996]. However, in cases where Q\u03b8(s, a) is nonlinear (neural network), the function approximation becomes more expressive at the cost of no convergence guarantee. Many deep RL algorithms are designed following the above formulation, such as Deep Q Network (DQN) [Mnih et al., 2013]. arXiv:1805.01907v2  [cs.LG]  21 Jun 2018 ",
    "Related Work": "Related Work In reinforcement learning (RL), naive explorations such as \u03f5\u2212greedy [Mnih et al., 2013; Lillicrap et al., 2016] do not explore well because local perturbations of actions break the 1In future notations, we replace =D by = for simplicity. 2\u03b4(x \u2212 xi) is the Dirac distribution that assigns point mass of probability 1 at x = xi. consistency between consecutive steps [Osband and Van Roy, 2015]. A number of prior works apply randomization to parameter space [Fortunato et al., 2017; Plappert et al., 2016] to preserve the consistency in exploration, but their formulations are built on heuristics. Posterior sampling is a principled exploration strategy in the bandit setting [Thompson, 1933; Russo, 2017], yet its extension to RL [Osband et al., 2013] is hard to scale to large problems. More recent prior works have formulated the exploration strategy as sampling randomized value functions and interpreted the algorithm as approximate posterior sampling [Osband et al., 2016; Osband et al., 2017]. Instead of modeling value functions, our formulation is built on modeling return distributions which reduces to exact posterior sampling in the bandit setting. Following similar ideas of randomized value function, multiple recent works have combined approximate Bayesian inference [Ranganath et al., 2014; Blei et al., 2017] with Q learning and justi\ufb01ed the ef\ufb01ciency of exploration by relating to posterior sampling [Lipton et al., 2016; Tang and Kucukelbir, 2017; Azizzadenesheli et al., 2017; Moerland et al., 2017]. Though their formulations are based on randomized value functions, we offer an alternate interpretation by modeling return distribution and provide a conceptual framework that uni\ufb01es these previous methods (Section 5). We will also provide a potential approach that extends the current framework to policy based methods as in [Henderson et al., 2017]. Modeling return distribution dated back to early work of [Dearden et al., 1998; Morimura et al., 2010; Morimura et al., 2012], where learning a return distribution instead of only its expectation presents a more statistically challenging task but provides more information during control. More recently, [Bellemare et al., 2017] applies a histogram to learn the return distribution and displays big performance gains over DQN [Mnih et al., 2013]. Based on [Bellemare et al., 2017], we provide a more general distributional learning paradigm that combines return distribution learning and exploration based on approximate posterior sampling. 4 Exploration by Distributional Reinforcement Learning 4.1 Formulation Recall that Z(s, a) is the return distribution for state action pair (s, a). In practice, we approximate such distribution by a parametric distribution Z\u03b8(s, a) with parameter \u03b8. Following [Bellemare et al., 2017], we take the discrepancy to be KL divergence. Recall \u02c6Z is the empirical distribution of samples \u02c6Z = \ufffdN i=1 1 N \u03b4(x \u2212 xi), hence the KL divergence reduces to KL[ \u02c6Z||Z\u03b8] = N \ufffd i=1 1 N log 1 N Z\u03b8(xi) = \u2212 1 N N \ufffd i=1 log Z\u03b8(xi), (3) where we have dropped a constant \u2212 log N in the last equality. Let \u03b8 follow a given distribution \u03b8 \u223c q\u03c6(\u03b8) with parameter \u03c6. We propose to minimize the following objective min \u03c6 E\u03b8\u223cq\u03c6(\u03b8)[\u2212 N \ufffd i=1 log Z\u03b8(xi)] \u2212 H(q\u03c6(\u03b8)), (4) where H(q\u03c6(\u03b8)) is the entropy of q\u03c6(\u03b8). Note that (3) corresponds to the projection step \u03a0H\u2217 de\ufb01ned in (2), and the \ufb01rst term of (4) takes an expectation of projection discrepancy over the distribution \u03b8 \u223c q\u03c6(\u03b8). The intuition behind (4) is that by the \ufb01rst term, the objective encourages low expected discrepancy (which is equivalent to Bellman error) to learn optimal policies; the second term serves as an exploration bonus to encourage a dispersed distribution over \u03b8 for better exploration during learning. We now draw the connection between (4) and approximate Bayesian inference. First assign an improper uniform prior on \u03b8, i.e. p(\u03b8) \u221d 1. The posterior is de\ufb01ned by Bayes rule given the data {xi}N i=1 as p(\u03b8|{xi}N i=1) \u221d p(\u03b8)p({xi}N i=1|\u03b8) where p({xi}N i=1|\u03b8) = \u03a0ip(xi|\u03b8) 3. Since by de\ufb01nition p(xi|\u03b8) = Z\u03b8(xi), (4) is equivalent to min \u03c6 KL[q\u03c6(\u03b8)||p(\u03b8|{xi}N i=1)]. (5) Hence to minimize the objective (4) is to search for a parametric distribution q\u03c6(\u03b8) to approximate the posterior p(\u03b8|{xi}N i=1). From (5) we can see that the posterior p(\u03b8|{xi}N i=1) is the minimizer policy of (4), which achieves the optimal balance between minimizing low discrepancy and being as random as possible. The close resemblance between our formulation and posterior sampling partially justi\ufb01es the potential strength of our exploration strategy. 4.2 Generic Algorithm A generic algorithm Algorithm 1 can be derived from (5). We start with a proposed distribution q\u03c6(\u03b8) over parameter \u03b8 and a distribution model Z\u03b8(s, a). During control, in state st, we sample a parameter from \u03b8 \u223c q\u03c6(\u03b8) and choose action at = arg maxa E[Z\u03b8(st, a)]. This is equivalent to taking an action based on the approximate posterior probability that it is optimal. During training, we sample from one-step lookahead distribution of the greedy action, and update parameter by optimizing (4). Algorithm 1 Exploration by Distributional RL: Generic 1: INPUT: generic return distribution Z\u03b8(s, a) with parameter \u03b8, parameter distribution q\u03c6(\u03b8) with parameter \u03c6. 2: while not converged do 3: // Control 4: Sample \u03b8 \u223c q\u03c6(\u03b8). 5: In state st, choose at = arg maxa E[Z\u03b8(st, a)], get transition st+1 and reward rt. 6: // Training 7: Given state action pair st, at, choose greedy onestep lookahead distribution a\u2032 = arg maxa E[rt + \u03b3Z\u03b8(st+1, a)]. 8: Sample from the distribution rt + \u03b3Z\u03b8(st+1, a\u2032) and let \u02c6Z be the empirical distribution of samples, update parameter \u03c6 by minimizing objective (4). 9: end while 3We assume samples drawn from the next state distributions are i.i.d. as in [Bellemare et al., 2017]. 4.3 Practical Algorithm: Gaussian Assumption We turn Algorithm 1 into a practical algorithm by imposing assumption on Z\u03b8(s, a). [Dearden et al., 1998] assumes Z\u03b8(s, a) to be Gaussian based on the assumption that the chain is ergodic and \u03b3 close to 1. We make this assumption here and let Z\u03b8(s, a) be a Gaussian with parametrized mean Q\u03b8(s, a) and \ufb01xed standard error \u03c3. The objective (3) reduces to min \u03b8 1 N N \ufffd i=1 (Q\u03b8(s, a) \u2212 xi)2 2\u03c32 . (6) We now have an analytical form E[Z\u03b8(s, a)] = Q\u03b8(s, a). The objective (4) reduces to min \u03c6 E\u03b8\u223cq\u03c6(\u03b8)[ N \ufffd i=1 (Q\u03b8(s, a) \u2212 xi)2 2\u03c32 ] \u2212 H(q\u03c6(\u03b8)). (7) Algorithm 2 Exploration by Distributional RL: Gaussian 1: INPUT: target parameter update period \u03c4 ; learning rate \u03b1; Gaussian distribution parameter \u03c32. 2: INITIALIZE: parameters \u03c6, \u03c6\u2212; replay buffer B \u2190 {}; step counter counter \u2190 0. 3: for e = 1, 2, 3...E do 4: while episode not terminated do 5: counter \u2190 counter + 1. 6: Sample \u03b8 \u223c q\u03c6(\u03b8). 7: In state st, choose at = arg maxa Q\u03b8(st, a), get transition st+1 and reward rt. 8: Save experience tuple {st, at, rt, st+1} to buffer B. 9: Sample N parameters \u03b8\u2212 j \u223c q\u03c6\u2212(\u03b8\u2212) and sample N tuples D = {sj, aj, rj, s\u2032 j} from B. 10: Sample target xj \u223c rj + \u03b3Z\u03b8\u2212 j (s\u2032 j, a\u2032) for jth tuple in D where a\u2032 is greedy w.r.t. Q\u03b8\u2212(s\u2032 j, a). 11: Take gradient \u2206\u03c6 of the KL divergence in (7). 12: \u03c6 \u2190 \u03c6 \u2212 \u03b1\u2206\u03c6. 13: if counter mod \u03c4 = 0 then 14: Update target parameter \u03c6\u2212 \u2190 \u03c6. 15: end if 16: end while 17: end for Parallel to the principal network q\u03c6(\u03b8) with parameter \u03c6, we maintain a target network q\u03c6\u2212(\u03b8\u2212) with parameter \u03c6\u2212 to stabilize learning [Mnih et al., 2013]. Samples for updates are generated by target network \u03b8\u2212 \u223c q\u03c6\u2212(\u03b8\u2212). We also maintain a replay buffer B to store off-policy data. 4.4 Randomized Value Function as Randomized Critic for Policy Gradient In off-policy optimization algorithm like Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al., 2016], a policy \u03c0\u03b8p(s) with parameter \u03b8p and a critic Q\u03b8(s, a) with parameter \u03b8 are trained at the same time. The policy gradient of reward ",
    "Methods": "Methods We now argue that the above formulation provides a conceptual uni\ufb01cation to multiple previous methods. We can recover the same objective functions as previous methods by properly choosing the parametric form of return distribution Z\u03b8(s, a), the distribution over model parameter q\u03c6(\u03b8) and the algorithm to optimize the objective (5). 5.1 Posterior Sampling for Bandits In the bandit setting, we only have a set of actions a \u2208 A. Assume the underlying reward for each action a is Gaussian distributed. To model the return distribution of action a, we set Z\u03b8(a) to be Gaussian with unknown mean parameters \u00b5a, i.e. Z\u03b8 = N(\u00b5a, \u03c32). We assume the distribution over parameter q\u03c6(\u00b5) to be Gaussian as well. Due to the conjugacy between improper uniform prior p(\u00b5) (assumed in Section 4.1) and likelihood Z\u03b8(a), the posterior p(\u00b5|{xi}) is still Gaussian. We can minimize (5) exactly by setting q\u03c6(\u00b5) = p(\u00b5|{xi}). During control, Algorithm 1 selects action at = arg maxa \u00b5(a) with sampled \u00b5 \u223c q\u03c6(\u00b5) = p(\u00b5|{xi}), which is exact posterior sampling. This shows that our proposed algorithm reduces to exact posterior sampling for bandits. For general RL cases, the equivalence is not exact but this connection partially justi\ufb01es that our algorithm can achieve very ef\ufb01cient exploration. 5.2 Deep Q Network with Bayesian Updates Despite minor algorithmic differences, Algorithm 2 has very similar objective as Variational DQN [Tang and Kucukelbir, 2017], BBQ Network [Lipton et al., 2016] and Bayesian DQN [Azizzadenesheli et al., 2017], i.e. all three algorithms can be interpreted as having Gaussian assumption over return distribution Z\u03b8(s, a) \u223c N(Q\u03b8(s, a), \u03c32) and proposing Gaussian distribution over parameters q\u03c6(\u03b8). However, it is worth recalling that Algorithm 2 is formulated by modeling return distributions, while previous methods are formulated by randomizing value functions. If we are to interpret these three algorithms as instantiations of Algorithm 2, the difference lies in how they optimize (7). Variational DQN and BBQ apply variational inference to minimize the divergence between q\u03c6(\u03b8) and posterior p(\u03b8|{xi}), while Bayesian DQN applies exact analytical updates (exact minimization of (7)), by using the conjugacy of prior and likelihood distributions as discussed above. Algorithm 1 generalizes these variants of DQN with Bayesian updates by allowing for other parametric likelihood models Z\u03b8(s, a), though in practice Gaussian distribution is very popular due to its simple analytical form. To recover NoisyNet [Fortunato et al., 2017] from (7), we can properly scale the objective (by multiplying (7) by \u03c32) and let \u03c3 \u2192 0. This implies that NoisyNet makes less strict assumption on return distribution (Gauss parameter \u03c3 does not appear in objective) but does not explicitly encourage exploration by adding entropy bonus, hence the exploration purely relies on the randomization of parameter \u03b8. To further recover the objective of DQN [Mnih et al., 2013], we set q\u03c6(\u03b8) = \u03b4(\u03b8 \u2212 \u03c6) to be the Dirac distribution. Finally, since DQN has no randomness in the parameter \u03b8, its exploration relies on greedy action perturbations. 5.3 Distributional RL Distributional RL [Bellemare et al., 2017] models return distribution using categorical distribution and does not introduce parameter uncertainties. Since there is no distribution over parameter \u03b8, Algorithm 1 recovers the exact objective of distributional RL from (4) by setting q\u03c6(\u03b8) = \u03b4(\u03b8 \u2212 \u03c6) and letting Z\u03b8(s, a) be categorical distributions. As the number of atoms in the categorical distribution increases, the modeling becomes increasingly close to non-parametric estimation. Though having more atoms makes the parametric distribution more expressive, it also poses a bigger statistical challenge during learning due to a larger number of parameters. As with general Z\u03b8(s, a), choosing a parametric form with appropriate representation power is critical for learning. 6 ",
    "Experiments": "Experiments In all experiments, we implement Algorithm 2 and refer to it as GE (Gauss exploration) in the following. We aim to answer the following questions, \u2022 In environments that require consistent exploration, does GE achieve more ef\ufb01cient exploration than conventional naive exploration strategies like \u03f5\u2212greedy in DQN and direct parameter randomization in NoisyNet? \u2022 When a deterministic critic in an off-policy algorithm like DDPG [Lillicrap et al., 2016] is replaced by a randomized critic, does the algorithm achieve better exploration? 6.1 Testing Environment Chain MDP. The chain MDP [Osband et al., 2016] (Figure 1) serves as a benchmark to test if an algorithm entails consistent exploration. The environment consists of N states and each episode lasts N + 9 time steps. The agent has two ",
    "Results": "Results Exploration in Chain MDP. In Figure 2 (a) - (c) we compare DQN vs NoisyNet vs GE in Chain MDP environments with different number of states N. When N = 10, all three algorithms can solve the task. When N = 50, DQN cannot explore properly and cannot make progress, GE explores more ef\ufb01ciently and converges to optimal policy faster than NoisyNet. When N = 100, both NoisyNet and DQN get stuck while GE makes progress more consistently. Compared to Bootstrapped DQN (BDQN)[Osband et al., 2016], GE has a higher variance when N = 100. This might be because BDQN represents the distribution using multiple heads and can approximate more complex distributions, enabling better exploration on this particular task. In general, however, our algorithm is much more computationally feasible than BDQN yet still achieves very ef\ufb01cient exploration. Figure 2 (d) plots the state visit frequency for GE vs. DQN within the \ufb01rst 10 episodes of training. DQN mostly visits states near s2 (the initial state), while GE visits a much wider range of states. Such active exploration allows the agent to consistently visit sN and learns the optimal policy within a small number of iterations. Exploration in Sparse Reward Environments. In Figure 3 (a) - (c) we present the comparison of three algorithms in (a) Chain MDP N = 10 (b) Chain MDP N = 50 (c) Chain MDP N = 100 (d) State visit frequency Figure 2: Comparison of DQN vs NoisyNet vs GE on Chain MDP environments with (a) N = 10 (b) N = 50 and (c) N = 100 states. Figure 2 (d) plots state visit frequency within the \ufb01rst iteration in training for Gauss vs. DQN in Chain MDP N = 128. For state si, set ci = 1 if si is ever visited in one episode and ci = 0 otherwise. The moving average of ci across multiple episodes computes the state visit frequency. Each iteration consists of 20 episodes. sparse reward environments. For each environment, we plot the rewards at a different scale. In CartPole, the plotted cumulative reward is the episode length; in MountainCar, the plotted cumulative reward is 1 for reaching the target within one episode and 0 otherwise; in Acrobot, the plotted cumulative reward is the negative of the episode length. In all sparse reward tasks, GE entails much faster progress than the other two algorithms. For example, in Sparse MountainCar, within the given number of iterations, DQN and NoisyNet have never (or very rarely) reached the target, hence they make no (little) progress in cumulative reward. On the other hand, GE reaches the targets more frequently since early stage of the training, and makes progress more steadily. In Figure 3 (d) we plot the state visit trajectories of GE vs. DQN in Sparse MountainCar. The vertical and horizontal axes of the plot correspond to two coordinates of the state space. Two panels of (d) correspond to training after 10 and 30 iterations respectively. As the training proceeds, the state visits of DQN increasingly cluster on a small region in state space and fail to ef\ufb01ciently explore. On the contrary, GE maintains a widespread distribution over states and can explore more systematically. Randomized Critic for Exploration. We evaluate the performance of DDPG with different critics. When DQN is used as a critic, the agent explores by injecting noise into actions produced by the policy [Lillicrap et al., 2016]. When critics are NoisyNet or randomized DQN with GE, the agent explores by updating its parameters using policy gradients computed through randomized critics, effectively injecting noise into the parameter space. In conventional continuous control ",
    "Conclusion": "Conclusion We have provided a framework based on distributional RL that uni\ufb01es multiple previous methods on exploration in reinforcement learning, including posterior sampling for bandits as well as recent efforts in Bayesian updates of DQN parameters. We have also derived a practical algorithm based on the Gaussian assumption of return distribution, which allows for ef\ufb01cient control and parameter updates. We have observed that the proposed algorithm obtains good performance on challenging tasks that require consistent exploration. A further extension of our current algorithm is to relax the Gaussian assumption on return distributions. We leave it be future work if more \ufb02exible assumption can lead to better perfor",
    "References": "References [Azizzadenesheli et al., 2017] Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Ef\ufb01cient exploration through bayesian deep q networks. Symposium on Deep Reinforcement Learning, NIPS, 2017. [Bellemare et al., 2017] Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement learning. International Conference on Machine Learning, 2017. [Blei et al., 2017] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, Volume 112 - Issue 518, 2017. [Brockman et al., 2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. Arxiv: 1606.01540, 2016. [Dearden et al., 1998] Richard Dearden, Nir Friedman, and Stuart Russel. Bayesian q learning. American Association for Arti\ufb01cial Intelligence (AAAI), 1998. [Duan et al., 2016] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. International Conference on Machine Learning, 2016. [Fortunato et al., 2017] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Ilivier Pietquin, Charles Blundell, and Shane Legg. Noisy network for exploration. arXiv:1706.10295, 2017. [Henderson et al., 2017] Peter Henderson, Thang Doan, Riashat Islam, and David Meger. Bayesian policy gradients via alpha divergence dropout inference. 2nd Workshop on Bayesian Deep Learning, NIPS, 2017. [Levine et al., 2016] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End to end training of deep visuomotor policies. Journal of Machine Learning Research, 2016. [Lillicrap et al., 2016] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. International Conference on Learning Representations, 2016. [Lipton et al., 2016] Zachary C. Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed, and Li Deng. Ef\ufb01cient dialogue policy learning with bbq-networks. ArXiv: 1608.05081, 2016. [Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS workshop in Deep Learning, 2013. [Moerland et al., 2017] Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Ef\ufb01cient exploration with double uncertain value networks. Symposium on Deep Reinforcement Learning, NIPS, 2017. [Morimura et al., 2010] Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka. Nonparametric return distribution approximation for reinforcement learning. ICML, 2010. [Morimura et al., 2012] Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka. Parametric return density estimation for reinforcement learning. UAI, 2012. [Osband and Van Roy, 2015] Ian Osband and Benjamin Van Roy. Bootstrapped thompson sampling and deep exploration. arXiv:1507:00300, 2015. [Osband et al., 2013] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) ef\ufb01cient reinforcement learning via posterior sampling. Arxiv: 1306.0940, 2013. [Osband et al., 2016] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. arXiv:1602.04621, 2016. [Osband et al., 2017] Ian Osband, daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv: 1703.07608, 2017. [Plappert et al., 2016] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. International Conference on Learning Representation, 2016. [Ranganath et al., 2014] Rejesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. Proceedings of the 17th International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), 2014. [Russo, 2017] Daniel Russo. Tutorial on thompson sampling. arxiv, 2017. [Schulman et al., 2015] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. International Conference on Machine Learning, 2015. [Tang and Kucukelbir, 2017] Yunhao Tang and Alp Kucukelbir. Variational deep q network. 2nd Workshop on Bayesian Deep Learning, NIPS, 2017. [Thompson, 1933] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, Vol. 25, No. 3/4, 1933. [Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. International Conference on Intelligent Robots, 2012. [Tsitsiklis and Van Roy, 1996] John N. Tsitsiklis and Benjamin Van Roy. Feature based methods for large scale dynamic programming. Machine Learning, 1996. ",
    "title": "Exploration by Distributional Reinforcement Learning",
    "paper_info": "Exploration by Distributional Reinforcement Learning\nYunhao Tang, Shipra Agrawal\nColumbia University IEOR\nyt2541@columbia.edu, sa3305@columbia.edu\nAbstract\nWe propose a framework based on distributional re-\ninforcement learning and recent attempts to com-\nbine Bayesian parameter updates with deep rein-\nforcement learning.\nWe show that our proposed\nframework conceptually uni\ufb01es multiple previous\nmethods in exploration. We also derive a practi-\ncal algorithm that achieves ef\ufb01cient exploration on\nchallenging control tasks.\n1\nIntroduction\nDeep reinforcement learning (RL) has enjoyed numerous re-\ncent successes in various domains such as video games and\nrobotics control [Schulman et al., 2015; Duan et al., 2016;\nLevine et al., 2016]. Deep RL algorithms typically apply\nnaive exploration strategies such as \u03f5\u2212greedy [Mnih et al.,\n2013; Lillicrap et al., 2016]. However, such myopic strate-\ngies cannot lead to systematic exploration in hard environ-\nments [Osband et al., 2017].\nWe provide an exploration algorithm based on distribu-\ntional RL [Bellemare et al., 2017] and recent attempts to\ncombine Bayesian parameter updates with deep reinforce-\nment learning. We show that the proposed algorithm pro-\nvides a conceptual uni\ufb01cation of multiple previous methods\non exploration in deep reinforcement learning setting. We\nalso show that the algorithm achieves ef\ufb01cient exploration in\nchallenging environments.\n2\nBackground\n2.1\nMarkov Decision Process and Value Based\nReinforcement Learning\nIn a Markov Decision Process (MDP), at time step t \u2265 0, an\nagent is in state st \u2208 S, takes action at \u2208 A, receives reward\nrt and gets transitioned to next state st+1 \u223c p(st+1|st, at).\nAt time t = 0 the agent\u2019s state distribution follows s0 \u223c\n\u03c1(s0). A policy is a mapping from a state to a distribution\nover action at \u223c \u03c0(\u00b7|st). The objective is to \ufb01nd a policy \u03c0\nto maximize the discounted cumulative reward\nJ = Es0\u223c\u03c1,at\u223c\u03c0(\u00b7|st)\n\ufffd \u221e\n\ufffd\nt=0\nrt\u03b3t\ufffd\n,\n(1)\nwhere \u03b3 \u2208 (0, 1] is a discount factor. In state s, the action-\nvalue function Q\u03c0(s, a) is de\ufb01ned as the expected cumulative\nreward that could be received by \ufb01rst taking action a and fol-\nlowing policy \u03c0 thereafter\nQ\u03c0(s, a) = Eat\u223c\u03c0(\u00b7|st)\n\ufffd \u221e\n\ufffd\nt=0\nrt\u03b3t|s0 = s, a0 = a\n\ufffd\n.\nFrom the above de\ufb01nition, it can be shown that Q\u03c0(st, at)\nsatis\ufb01es the Bellman equation\nQ\u03c0(st, at) = E[rt + \u03b3Q\u03c0(st+1, at+1)], \u2200(st, at).\nLet \u03c0\u2217 = arg max\u03c0 J be the optimal policy and Q\u2217(s, a) its\naction value function. Q\u2217(s, a) satis\ufb01es the following Bell-\nman equation\nQ\u2217(st, at) = E\n\ufffd\nrt + \u03b3 max\na\nQ\u2217(st+1, a)\n\ufffd\n, \u2200(st, at).\nThe above equations illustrate the temporal consistency of the\naction value functions that allows for the design of learning\nalgorithms. De\ufb01ne Bellman operator\nT \u2217Q(st, at) := E[rt + \u03b3 max\na\u2032 Q(st+1, a\u2032)].\nWhen \u03b3 \u2208 (0, 1), starting from any Q(0)(s, a), iteratively\napplying the operator Q(t+1)(s, a) \u2190 T \u2217Q(t)(s, a) leads to\nconvergence Q(t)(s, a) \u2192 Q\u2217(s, a) as t \u2192 \u221e.\nIn high dimensional cases, it is critical to use function ap-\nproximation as a compact representation of action values. Let\nQ\u03b8(s, a) be such a function with parameter \u03b8 that approxi-\nmates a table of action values with entry (s, a). The aim is\nto \ufb01nd \u03b8 such that Q\u03b8(s, a) \u2248 Q\u2217(s, a). Let \u03a0 be the op-\nerator that projects arbitrary vector Q(s, a) \u2208 R|S|\u00d7|A| to\nthe subspace spanned by function Q\u03b8(s, a). Since the up-\ndate of action values can now only take place in the sub-\nspace spanned by function Q\u03b8(s, a), the iterate Q(t)(s, a) is\nupdated as Q(t+1)(s, a) \u2190 \u03a0T \u2217Q(t)(s, a). In cases where\nQ\u03b8(s, a) is linear, the above procedure can be shown to con-\nverge [Tsitsiklis and Van Roy, 1996].\nHowever, in cases\nwhere Q\u03b8(s, a) is nonlinear (neural network), the function\napproximation becomes more expressive at the cost of no\nconvergence guarantee. Many deep RL algorithms are de-\nsigned following the above formulation, such as Deep Q Net-\nwork (DQN) [Mnih et al., 2013].\narXiv:1805.01907v2  [cs.LG]  21 Jun 2018\n",
    "GPTsummary": "- (1): The research background of this article is the need for exploration algorithms in deep reinforcement learning, which is critical for the agent to discover and improve its policy in challenging environments.\n\n- (2): Previous methods for exploration in deep reinforcement learning used simple exploration strategies such as \u03f5-greedy, which cannot lead to systematic exploration in hard environments. The approach proposed in this article is well-motivated by combining distributional reinforcement learning and Bayesian parameter updates to provide a conceptual unification of multiple previous methods on exploration in deep reinforcement learning setting.\n\n- (3): The research methodology proposed in this paper is a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. The authors derive a practical algorithm that achieves efficient exploration on challenging control tasks.\n\n- (4): The proposed algorithm achieves efficient exploration in challenging environments, as demonstrated by experiments on several control tasks including Atari games and a robotic manipulator. The performance supports their goals of providing a better exploration algorithm in deep reinforcement learning. Github: None.\n7. Methods: \n\n- (1): The methodological idea of this article is to propose a new approach for exploration in deep reinforcement learning by combining distributional reinforcement learning and Bayesian parameter updates.\n\n- (2): The authors derive a practical algorithm based on this approach that achieves efficient exploration on challenging control tasks.\n\n- (3): The proposed algorithm models the return distribution using a parametric distribution, such as the Gaussian distribution, and introduces uncertainty through a Bayesian formulation of parameter updates.\n\n- (4): The algorithm achieves efficient exploration on challenging environments, as demonstrated by experiments on several control tasks including Atari games and a robotic manipulator. \n\n- (5): Previous methods for exploration in deep reinforcement learning used simple exploration strategies such as \u03f5-greedy, while the approach proposed in this article provides a conceptual unification of multiple previous methods on exploration in deep reinforcement learning setting.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in proposing a framework based on distributional reinforcement learning and Bayesian parameter updates to unify multiple previous methods on exploration in deep reinforcement learning. The proposed algorithm achieves efficient exploration on challenging control tasks and provides a better exploration algorithm in deep reinforcement learning.\n\n- (2): Innovation point: The authors proposed a new approach for exploration in deep reinforcement learning by combining distributional reinforcement learning and Bayesian parameter updates, which provides a conceptual unification of multiple previous methods on exploration in deep reinforcement learning.setting. \n\nPerformance: The proposed algorithm achieves efficient exploration on challenging environments, as demonstrated by experiments on several control tasks including Atari games and a robotic manipulator. However, the authors did not compare the proposed algorithm with state-of-the-art exploration methods.\n\nWorkload: The authors provided a practical algorithm based on the Gaussian assumption of return distribution, which allows for efficient control and parameter updates. However, the algorithm may require computational resources beyond the capability of low-end devices.\n\n\n",
    "GPTmethods": "- (1): The methodological idea of this article is to propose a new approach for exploration in deep reinforcement learning by combining distributional reinforcement learning and Bayesian parameter updates.\n\n- (2): The authors derive a practical algorithm based on this approach that achieves efficient exploration on challenging control tasks.\n\n- (3): The proposed algorithm models the return distribution using a parametric distribution, such as the Gaussian distribution, and introduces uncertainty through a Bayesian formulation of parameter updates.\n\n- (4): The algorithm achieves efficient exploration on challenging environments, as demonstrated by experiments on several control tasks including Atari games and a robotic manipulator. \n\n- (5): Previous methods for exploration in deep reinforcement learning used simple exploration strategies such as \u03f5-greedy, while the approach proposed in this article provides a conceptual unification of multiple previous methods on exploration in deep reinforcement learning setting.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in proposing a framework based on distributional reinforcement learning and Bayesian parameter updates to unify multiple previous methods on exploration in deep reinforcement learning. The proposed algorithm achieves efficient exploration on challenging control tasks and provides a better exploration algorithm in deep reinforcement learning.\n\n- (2): Innovation point: The authors proposed a new approach for exploration in deep reinforcement learning by combining distributional reinforcement learning and Bayesian parameter updates, which provides a conceptual unification of multiple previous methods on exploration in deep reinforcement learning.setting. \n\nPerformance: The proposed algorithm achieves efficient exploration on challenging environments, as demonstrated by experiments on several control tasks including Atari games and a robotic manipulator. However, the authors did not compare the proposed algorithm with state-of-the-art exploration methods.\n\nWorkload: The authors provided a practical algorithm based on the Gaussian assumption of return distribution, which allows for efficient control and parameter updates. However, the algorithm may require computational resources beyond the capability of low-end devices.\n\n\n",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing a framework based on distributional reinforcement learning and Bayesian parameter updates to unify multiple previous methods on exploration in deep reinforcement learning. The proposed algorithm achieves efficient exploration on challenging control tasks and provides a better exploration algorithm in deep reinforcement learning.\n\n- (2): Innovation point: The authors proposed a new approach for exploration in deep reinforcement learning by combining distributional reinforcement learning and Bayesian parameter updates, which provides a conceptual unification of multiple previous methods on exploration in deep reinforcement learning.setting. \n\nPerformance: The proposed algorithm achieves efficient exploration on challenging environments, as demonstrated by experiments on several control tasks including Atari games and a robotic manipulator. However, the authors did not compare the proposed algorithm with state-of-the-art exploration methods.\n\nWorkload: The authors provided a practical algorithm based on the Gaussian assumption of return distribution, which allows for efficient control and parameter updates. However, the algorithm may require computational resources beyond the capability of low-end devices.\n\n\n"
}