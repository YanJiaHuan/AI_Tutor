{
    "Abstract": "Abstract Ef\ufb01cient Reinforcement Learning usually takes advantage of demonstration or good exploration strategy. By applying posterior sampling in model-free RL under the hypothesis of GP, we propose GPPSTD algorithm in continuous state space, giving theoretical justi\ufb01cations and empirical results. We also provide theoretical and empirical results that various demonstration could lower expected uncertainty and bene\ufb01t posterior sampling exploration. In this way, we combined the demonstration and exploration process together to achieve a more ef\ufb01cient reinforcement learning. 1. ",
    "Introduction": "Introduction Over the past years, Reinforcement Learning (RL) has achieved a great success in tasks such as Atari Games (Mnih et al., 2015), Go (Silver et al., 2016), robot control (Levine et al., 2016) and high-level decisions (Silver et al., 2013). But in general, the conventional RL approaches can hardly obtain a good performance before a large number of experiences are collected. Therefore, two types of methods have been proposed to realize sample ef\ufb01cient learning, i.e. leveraging human demonstration (e.g. inverse RL (Ng et al., 2000)) and designing better exploration strategies. Although the literature has plenty of interesting studies on either one, there seems lack of work combining them to our best knowledge. In this paper we propose a new model-free exploration strategy which leverages all kinds of demonstrations (even including unsuccessful ones) to improve learning ef\ufb01ciency. Existing works on learning from demonstration are mainly focused on inferring the underlying reward function (in IRL) or imitating of the expert demonstrations (Ng et al., 2000; Abbeel & Ng, 2004; Ho & Ermon, 2016; Hester et al., 2017). Hence, most methods can only exploit demonstrations that are optimal. However, the very optimal demonstrations are *Equal contribution 1Nat\u2019l Engineering Laboratory for Video Technology Cooperative Medianet Innovation Center Key Laboratory of Machine Perception (MoE) Sch\u2019l of EECS, Peking University, Beijing, 100871, China. Correspondence to: Yizhou Wang <yizhou.wang@pku.edu.cn>. hard to obtain in practice since it is known that humans often perform suboptimal behaviors. Therefore, mediocre and unsuccessful demonstrations have long been neglected or even expelled in RL. In this paper, we show how to make use of seemingly-useless demonstrations in the exploration process to improve sample ef\ufb01ciency. Speaking of ef\ufb01cient exploration strategy, it expects an agent to balance between exploring poorly-understood state-action pairs to get better performance in the future and exploiting existing knowledge to get better performance now. The exploration vs exploitation problem also has two families of methods: model-based and model-free. Model-based means the agent explicitly model the Markov Decision Process (MDP) environment, then does planning over the model. In contrast, model-free methods maintain no such environment model. Typical model-free exploration approaches include \u03f5greedy(Sutton & Barto, 1998), optimistic initialization(Ross et al., 2011), and more sophisticated ones such as noisy network (Fortunato et al., 2017) and curiosity(Pathak et al., 2017). These model-free exploration strategies usually are capable to handle large scale real problems, however, they do not have a theoretic guarantee. Whereas, the modelbased explorations are more systematic, thus often have theoretic bounds, such as Optimism in the Face of Uncertainty (OFU)(Jaksch et al., 2010) and Posterior Sampling (PS) Reinforcement Learning (PSRL)(Osband et al., 2013). Despite the beautiful theoretical guarantees, the model-based methods suffer from signi\ufb01cant computation complexity when state-action space is large, hence usually not suitable for large scale real problem. How can we combine the advantage of both demonstration and exploration strategy to gain an even more ef\ufb01cient learning for RL? In this paper, we propose a model-free RL exploration algorithm GPPSTD using posterior sampling on joint Gaussian value function, and provide theoretical analysis about its ef\ufb01ciency in the meantime. We also make use of various demonstrations to decrease the expectation uncertainty of Q value model, and then leverages this advantage in implementing PS on Q values to gain more ef\ufb01cient exploration. In summary our contributions include: \u2022 Show that posterior sampling based on model-free arXiv:1812.04359v1  [cs.LG]  11 Dec 2018 ",
    "Related Work": "Related Work Two typical methods of learning from demonstration, are inverse reinforcement learning (IRL) and imitation learning (IL). Inverse reinforcement learning was introduced in Ng et al. 2000. Its goal is to infer the underlying reward function given the optimal demonstration behavior. Further IRL algorithm includes Bayesian IRL (Ramachandran & Amir, 2007; Michini & How, 2012), Maximum Entropy IRL (Ziebart et al., 2008; Audiffren et al., 2015), Repeated IRL (Amin et al., 2017), etc. But IRL can be intractable when problem scale is large. Earlier imitation learning indicates behavior cloning, which could fail when agent encounters untrained states. Later representative IL algorithm includes Data Aggregation (DAgger) (Ross et al., 2011), Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016), etc. However, their work focuses on imitating optimal demonstration, regarding mediocre and failed demonstration unusable. They also never consider exploration problem after imitating. As for the exploration problem, two intuitive methods, \u03f5greedy(Sutton & Barto, 1998) and Optimistic Initialization(Grze\u00b4s & Kudenko, 2009), are the earliest way to tackle this problem. \u03f5-greedy is to explore with a probability \u03f5. Optimistic Initialization initializes all Q values to rmax 1\u2212\u03b3 , making RL visit each state at least some times. Model based method Optimism in the Face of Uncertainty (OFU) is to assign each state-action pair a biased estimate of future value and selects the action with highest estimate (Jaksch et al., 2010). Posterior sampling method has been proposed since (Strens, 2000), involving sampling a set of values from posterior estimation and selecting the action with maximal sampled value. PSRL proposed by Osband et al.(2013) does PS on the Markov Decision Process (MDP): in every episode, PSRL sample a MDP , run model-based planning algorithm and acts as if it is the true optimal policy. For \ufb01nite horizon algorithms, regret bound of O(HS \u221a AT) is achieved by PSRL (Osband et al., 2013), and O(H \u221a SAT) by GPSRL(Osband & Van Roy, 2017). It is notable that these methods are all model-based with \ufb01nite SA space, which can be a considerable limitation in application. However, since PSRL is a model-based algorithm, it suffers from signi\ufb01cant computation complexity for planning when state and action space are large. Therefore, in this paper we built model on value function based on Gaussian Process (GP), making it model-free, and to achieve both exploration ef\ufb01ciency and tractable computation complexity. Previous model-free algorithms have also been proposed using GP in RL. GP-SARSA (Engel et al., 2005) used GP to update posterior estimation of value function by temporal difference method. iGP-SARSA proposed informative exploration but lacks theoretical analysis (Chung et al., 2013). GPQ for both on-line and batch settings aims at learning Q function which could actually converge as T \u2192 \u221e (Chowdhary et al., 2014) but lacks ef\ufb01cient exploration. DGPQ employed delayed update of Q function to achieve PACMDP(Grande et al., 2014) but still lacks ef\ufb01cient exploration. For regret bounds under GP hypothesis, Srinivas et al.(2012) used GP to analyze the regret bound using information gain in bandit problems, while posterior sampling using GP and related analysis of regret bounds had not been explored yet, which would be discussed in this paper. 3. Theoretical Analysis In this section, we will show that how to choose demonstrations to achieve lower expected estimation variance, analyze related bounds of posterior sampling in RL under the hypothesis of GP for both deterministic and non-deterministic MDPs, and \ufb01nally relate the choice of demonstrations and posterior sampling for ef\ufb01ciency improvement. 3.1. Expectation of variance conditioned on data in GP We choose joint Gaussian distribution on value function \u2013 more speci\ufb01cally, Gaussian Process (GP) \u2013 because GP provides a principled, practical, probabilistic approach to learn in kernel machines(Rasmussen & Williams, 2006). We assume that the values in the value function are joint normal distributed. Under the GP assumption, the posterior distribution are given by f \u2217|X\u2217, X, f \u223d N(K(X\u2217, X)K(X, X)\u22121f, K(X\u2217, X\u2217) \u2212 K(X\u2217, X)K(X, X)\u22121K(X, X\u2217)), (1) where f is the value of the state vector X, and we wish to obtain value estimation f \u2217 over the new observation X\u2217. f and X come from history or what we call experiences. We de\ufb01ne p(x) as the distribution of test points, i.e. the states which occur in RL. In the framework of RL, x is every single state and its visiting distribution p(x) is determined by Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process current policy \u00b5 and the MDP (Markov Decision Process). We will start by a theorem that is quite obvious from intuition but hasn\u2019t been proved yet. Theorem 1 When a set (X\u2032, f) is used to estimate f(x\u2217) in GP, the expectation of variance on test points x\u2217 with distribution p(x) conditioned on all possible training set (X\u2019, f) set would not be less than what conditioned on the training set X sampled from distribution p(x), if the size of sample set is large enough to ignore the approximation error. \ufffd {K(x\u2217, x\u2217) \u2212 K(x\u2217, X)K(X, X)\u22121K(X, x\u2217))}p(x)dx \u2264 \ufffd {K(x\u2217, x\u2217) \u2212 K(x\u2217, X\u2032)K(X\u2032, X\u2032)\u22121K(X\u2032, x\u2217))}p(x)dx (2) Proof Given a kernel K, together with a distribution p(x), there is a corresponding series of eigenfunctions \u03c6(x), s.t. \ufffd k(x, x\u2032)\u03c6(x)d\u00b5(x) = \ufffd k(x, x\u2032)\u03c6(x)p(x)dx = \u03bb\u03c6(x\u2032), and \u2200i, \ufffd \u03c6i(x)\u03c6\u2217 j(x)p(x)dx = \u03b4ij(* here means conjugate transpose) (Rasmussen & Williams, 2006). We consider the expectation of posterior variance over the distribution p(x) given any X\u2032 as \ufffd {K(x\u2217, x\u2217) \u2212 K(x\u2217, X\u2032)K(X\u2032, X\u2032)\u22121K(X\u2032, x\u2217))}p(x)dx. Since \ufffd K(x\u2217, x\u2217)p(x)dx has no relation with X\u2032, we just focus on the latter subtracted part \ufffd K(x, X\u2032)K(X\u2032, X\u2032)\u22121K(X\u2032, x)p(x)d(x). According to Mercer\u2019s theorem, K(x, x\u2032) = \u03a3\u221e i=1\u03bbi\u03c6i(x)\u03c6\u2217 i (x\u2032). \ufffd K(x, X\u2032)K(X\u2032, X\u2032)\u22121K(X\u2032, x)p(x)d(x) = \ufffd {(\u03a3\u221e i=1\u03bbi\u03c6i(x)\u03c6\u2217 i (X\u2032))K(X\u2032, X\u2032)\u22121 (\u03a3\u221e j=1\u03bbj\u03c6j(X\u2032)\u03c6\u2217 j(x))}p(x)dx. (3) If i does not equals to j, the integral would be 0. So \ufffd {\u03a3\u221e i=1\u03bbi\u03c6i(x)\u03c6\u2217 i (X\u2032)K(X\u2032, X\u2032)\u22121\u03bbi\u03c6i(X\u2032)\u03c6\u2217 i (x)}p(x)dx = \u03a3\u221e i=1\u03bb2 i \u03c6\u2217 i (X\u2032)K(X\u2032, X\u2032)\u22121\u03c6i(X\u2032). For each i, focus on \u03c6\u2217 i (X\u2032)K(X\u2032, X\u2032)\u22121\u03c6i(X\u2032). Using numerical approximation of eigenfunctions (Rasmussen & Williams, 2006), when each xl is sampled from the distribution p(x), \u03bbi\u03c6i(x) = \ufffd k(x, x\u2032)p(x)\u03c6i(x) \u22cd 1 n\u03a3n l=1k(xl, x\u2032)\u03c6i(xl). Plugging in x\u2032 = xl, we get K(X, X)ui = \u03bbmat i ui, where X = [xl] and Ki,j = k(xi, xj),and each ui and \u03bbmat i is the eigenvector and eigenvalue of matrix K(X, X), with the approximation \u03c6i(X) \u22cd \u221anui, 1 n\u03bbmat i \u22cd \u03bbi. Given a random set of X\u2032, and a sampled set of X, although we do not know \u03c6(x) exactly, we can use X to estimate the value of eigenfunctions of X\u2032: \u03c6i(X\u2032) \u22cd \u221an \u03bbmat i K(X\u2032, X)ui. Now that \u03c6\u2217 i (X\u2032)K(X\u2032, X\u2032)\u22121\u03c6i(X\u2032) \u22cd n (\u03bbmat i )2 uT i K(X, X\u2032)K(X\u2032, X\u2032)\u22121K(X\u2032, X)ui , when n \u2192 \u221e we could regard all above estimations as asymptotic unbiased estimations, and here we suppose n is large enough to ignore the approximation error so the approximate equations can be seen as equations. Applying matrix decomposition to symmetric non-negative de\ufb01nite matrix K(X\u2032, X\u2032) , K(X\u2032, X\u2032)\u22121 = \u03a3n j=1 1 \u03bb\u2032mat j vjv\u2217 j . So n (\u03bbmat i )2 u\u2217 i K(X, X\u2032)K(X\u2032, X\u2032)\u22121K(X\u2032, X)ui = \u03a3j n (\u03bbmat i )2\u03bb\u2032mat j ||u\u2217 i K(X, X\u2032)vj||2. On each n (\u03bbmat i )2\u03bb\u2032mat j ||u\u2217 i K(X, X\u2032)vj||2, we have: K(X, X\u2032) = \u03a3\u03c6(X)\u03c6(X\u2032)\u2217 = \u03c8(X)\u03c8(X\u2032)\u2217 (4) uiK(X, X)u\u2217 i = \u03bbmat i = ui\u03c8(X)\u03c8(X)\u2217u\u2217 i = ||ui\u03c8(X)||2 (5) vjK(X\u2032, X\u2032)v\u2217 j = \u03bb \u2032mat j = vj\u03c8(X\u2032)\u03c8(X\u2032)T v\u2217 j = ||vi\u03c8(X\u2032)||2 (6) According to Cauchy-Schwartz inequality, |ui\u03c8(X)\u03c8(X\u2032)\u2217v\u2217 j | \u2a7d ||ui\u03c8(X)|| ||vi\u03c8(X\u2032)|| = \ufffd \u03bbmat i \u03bb \u2032mat j . So n (\u03bbmat i )2\u03bb \u2032mat j ||uT i K(X, X\u2032)vj||2 \u2a7d n (\u03bbmat i )2\u03bb \u2032mat j \u03bbmat i \u03bb \u2032mat j = n \u03bbmat i = 1 \u03bbi , (7) and when \u03bb \u2032mat j equals to \u03bbmat i the result can reach its largest, and the lowest expectation of overall conditional variance is \ufffd k(x, x)p(x)dx \u2212 \u03a3\u221e i=1\u03bbi. Especially, when RBF kernel is selected, under any p(x) the lowest expectation would be 1 \u2212 \u03a3\u221e i=1\u03bbi = 1 \u2212 limn\u2192\u221e \u03a3n i=1\u03bbmat i n = 1 \u2212 limn\u2192\u221e trace(KXX) n = 0. Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process Moreover, if the kernel contains noise as below: f \u2217|X\u2217, X, f \u223d N(K(X\u2217, X)[K(X, X) + \u03c32I]\u22121f, K(X\u2217, X\u2217) \u2212 K(X\u2217, X)[K(X, X) + \u03c32I]\u22121K(X, X\u2217)), (8) K(X, X) + \u03c32I would still be a symmetric non-negative de\ufb01nite matrix, so the eigenvalues in previous analysis would all be added with \u03c32, and the eigenvectors remain the same. Obviously the conclusion still remains the same. \u25a1 Notice that during a learning process of RL, if the agent has not learned how to perform perfectly yet, under present policy the states which the agent would come across would not be those of highest real value. So non-perfect demonstrations are necessary to lower the expectation of uncertainty during exploration. 3.2. BayesRegret of GP-based Posterior Sampling 3.2.1. DETERMINISTIC MDP We start with a simple case where transitions are deterministic in MDP. We model each MDP M = {S, A, RM, P M, H} \u223c \u03c6, with potentially in\ufb01nite sets of states S and actions A. H is the length of a single episode. At timestep t of an episode, the agent observe st \u2208 S, select at \u2208 A, receive a reward rM st,at \u223c RM(st, at) and transition st+1 = P M(st, at). \u00afrM st,at = E[rM st,at|rM st,at \u223c RM(st, at)]. \u00b5 is the policy function of state, and value function: V\u00b5,M(s) = E[\u03a3\u221e i=0\u03b3i\u00afrM si+1,ai+1|si+1 = P M(si, ai), ai = \u00b5(si)], where \u03b3 is the rate of discount and satis\ufb01es 0 < \u03b3 \u2264 1. M k is the posterior sample of unknown true MDP M \u2217 given history Hkt, Hkt = {s1,1, a1,1, r1,1, s1,2, ......sk,t\u22121, ak,t\u22121, rk\u22121,t\u22121}. \u00b5Mis the optimistic policy under M, \u00b5k \u2208 argmax\u00b5V\u00b5,M k(s), and particularly, \u00b5k, \u00b5\u2217is the optimistic policy under M k, M \u2217 separately. \u03c0 indicates the learning algorithm which choose a policy \u00b5 for the agent to perform. We assume that given MDP V\u00b5M,M(s) is joint normal on the set of state S with optimal policy \u00b5M in M, which contains the assumption of the model using a model-free method. De\ufb01ne expected cumulative reward of the kth episode: S\u00b5,M k = E[\u03a3Hk t=H(k\u22121)+1\u00afrM st,at |s(t + 1) = P M(st, at), ai = \u00b5(si)]. (9) Regret : Regret(T, \u03c0, M \u2217) = \u03a3 \u2308 T H \u2309 k=1 (S\u00b5\u2217,M\u2217 k \u2212 S\u00b5k,M\u2217 k ). (10) The regret of every episode is random due to the unknown true MDP M \u2217, the learning algorithm \u03c0, the sampling M k of the present episode and previous sampling through history Hk1. Notice that in our algorithm we do not directly sample M k from the posterior distribution \u03c6(\u00b7|Hk1) and we use the posterior distribution of the value to realize our sampling. But for convenience we would use sampled M k to refer to our way of sampling in practice. And Bayesian regret: BayesRegret(T, \u03c0, \u03c6) = E[\u03a3 \u2308 T H \u2309 k=1 (S\u00b5\u2217,M\u2217 k \u2212 S\u00b5k,M\u2217 k )|M \u2217 \u223c \u03c6]. (11) which is actually the same with the regret de\ufb01ned by Osband & Van Roy(2017). Since we have different de\ufb01nition of the value function, we use other notations to avoid confusion. We separate this BayesRegret by episodes, where each episode k conditioned on the previous history Hk1 then taking expectation again in order to achieve the expectation on M \u2217. We discuss the relation between BayesRegret and the conditional regret, which is different from the method of previous work (Osband & Van Roy, 2017). The conditional regret is E[S\u00b5\u2217,M \u2217 k \u2212 S\u00b5k,M \u2217 k |M \u2217 \u223c \u03c6(\u00b7|Hk1)], and \u03a3 \u2308 T H \u2309 k=1 E[S\u00b5\u2217,M \u2217 k \u2212 S\u00b5k,M \u2217 k |M \u2217 \u223c \u03c6] = E[\u03a3 \u2308 T H \u2309 k=1 {E[S\u00b5\u2217,M \u2217 k \u2212 S\u00b5k,M \u2217 k |M \u2217 \u223c \u03c6(\u00b7|Hk1)]|Hk1 \u223c PreviousSampling}]. It is obvious that each Hk1 here contains previous history and not independent. So when we take expectation of a series of Hk1, we actually take expectation on whole history H. Since we will use the stochastic property of M k to analyze the bound, another thing to notice is that since H is actually produced by every sampled M k, taking expectation of H would not disturb the distribution of M k. So if we can bound conditional regret (as described above) on every possible M k from its distribution, then taking the expectation would also bound BayesRegret. Theorem 2 Let M \u2217 be the true MDP with deterministic transitions according to prior \u03c6 with values under GP hypothesis. Then the regret for GPPSTD is bounded: BayesRegret(T, \u03c0GP P ST D, \u03c6) = \u02dcO( \u221a HT). Proof Decomposition: S\u00b5\u2217,M\u2217 k \u2212S\u00b5k,M\u2217 k = (S\u00b5\u2217,M\u2217 k \u2212S\u00b5k,Mk k )+(S\u00b5k,Mk k \u2212S\u00b5k,M\u2217 k ). (12) First we focus on the difference we can observe by the policy Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process \u00b5k that the agent actually follows, i.e. S\u00b5k,M k k \u2212 S\u00b5k,M \u2217 k in (12). Referring to previous de\ufb01nation (9), EM\u2217[S\u00b5k,Mk k \u2212 S\u00b5k,M\u2217 k |Hk1] = V\u00b5k,Mk(s1) + \u03a3H\u22121 t=2 (1 \u2212 \u03b3)V\u00b5k,Mk(st) \u2212 \u03b3V\u00b5k,Mk(sH) \u2212 EM\u2217[V\u00b5k,M\u2217(s1) + \u03a3H\u22121 t=2 (1 \u2212 \u03b3)V\u00b5k,M\u2217(s\u2032 t) \u2212 \u03b3V\u00b5k,M\u2217(s\u2032 H)|Hk1], (13) where EM \u2217[ |Hk1] means taking expectation on M \u2217 \u223c \u03c6(\u00b7|Hk1). Recall our assumption of V in the de\ufb01nition part. Although V\u00b5k,M \u2217 does not satisfy joint normal distribution since its policy is not optimistic of its MDP, M k is still sampled from the posterior distribution of M \u2217, which means that given history Hk1, the posterior sample Rk, P k and unknown true R\u2217, P \u2217 are identically distributed. So the expectation of (13) (on M k while performing posterior sampling) is zero. So EM \u2217[S\u00b5k,M k k \u2212 S\u00b5k,M \u2217 k |Hk1] is totally zero-mean, and is a sum of a series of joint normal variables. We would focus on the variance next. Lemma 1 (Transformation of Joint Normal Variables). If X \u223c Np(\u00b5, \u03a3), A is a matrix of l \u00d7 p and rank(A) = l, Y = AX + b, Then Y \u223c Nl(A\u00b5 + b, A\u03a3AT ). To calculate the sum , let A be a vector \ufb01lled with 1, so we have \u03a3n i=1Xi \u223c (N(\u03a3n i=1\u00b5i, \u03a3n i=1\u03a3n j=1Cov(Xi, Xj))) Noticing that Cov(X, Y ) = E[(X \u2212 E[X])(Y \u2212 E[Y ])] \u2264 \ufffd E[(X \u2212 E[X])2]E[(Y \u2212 E[Y ])2] = \u03c31 \u2217\u03c32 \u2264max \u03c32. So we have proved that given history Hk1, EM \u2217[S\u00b5k,M k k \u2212 S\u00b5k,M \u2217 k |Hk1] is normally distributed with expectation of 0 and variance \u2264 H2 max(k)\u03c32, where max(k)\u03c32 is the max variance of every state in episode k. Now back to the \ufb01rst difference of (12). Lemma 2 (Posterior Sampling). If \u03c6 is the distribution of M \u2217, then for any \u03c3(Hk1)measurable function g, E[g(M \u2217)|Hk1] = E[g(M k)|Hk1]. Using the posterior lemma, the cumulative reward S\u00b5M,M k is \u03c3(Hk1)-measurable, so E[S\u00b5\u2217,M \u2217 k \u2212 S\u00b5k,M k k |Hk1] = 0 (Osband et al., 2013). Recall that S\u00b5,M k is the sum of joint normal variables, so similar to previous analysis, each EM \u2217[S\u00b5k,M k k \u2212S\u00b5\u2217,M \u2217 k |Hk1] is normally distributed with zero-mean and variance \u2264 H2 max(k)\u03c32. So EM \u2217[(S\u00b5\u2217,M \u2217 k \u2212 S\u00b5k,M k k ) + (S\u00b5k,M k k \u2212 S\u00b5k,M \u2217 k )|Hk1] has zero-mean and variance \u2264 4H2 max\u03c32 by analyzing covariance as previous part. For normal distribution X \u223c N(0, \u03c32), and for any 1 > \u03b4 > 0, P(X \u2264 \ufffd \u22122\u03c32log\u03b4) \u2265 1 \u2212 \u03b4, which means there is a probability of 1 \u2212 \u03b4 that X \u2264 \ufffd \u22122\u03c32log\u03b4. So noticing the independence of sampling between episodes, calculate EH[\u03a3 \u2308 T H \u2309 k=1 (S\u00b5\u2217,M \u2217 k \u2212 S\u00b5k,M k k )|Hk1] as analyzed before, where EH means taking expectation on H. Set \u03b4 as 1 T , and let max\u03c32 be the max variance of all states in all episodes (just for worst case bound), and there is a probability of 1 \u2212 1 T that: E[\u03a3 \u2308 T H \u2309 k=1 (S\u00b5k,M \u2217 k \u2212 S\u00b5k,M \u2217 k )|M \u2217 \u223c \u03c6] \u2264 2 \ufffd 2max\u03c32(HT + H)logT. \u25a1 In general cases (like RBF and Matern), \u03c32 is bounded (in a few cases like dot-product kernels, covariance cannot be bounded only in in\ufb01nite spaces, while most continuous spaces in RL has borders), so this could be a sub-linear bound which means the agent would actually learn the real MDP in the end. Notice that we use max\u03c32 only for a worst case bound in brief, while the true regret is related with each variance and covariance of the state. This result is better than previous posterior sampling analysis (PSRL bounds \u221a HSAT empirically but H \u221a SAT theoretically). As GP gets more information of the environment during exploration, the variance would decay, so actually the bound could be even better. 3.2.2. NON-DETERMINISTIC MDP True MDP M \u2217 = {S, A, RM, P M, H, \u03c1} \u223c \u03c6, other notations are just the same as 3.2.1, except that P M is a stochastic transition in M, \u03c1 is the distribution of initial states. Since the transition is not deterministic and the states are continuous, the cumulative reward could be related to countless states of values. Since we do not have assumptions on stochastic transition function, which is necessary for regret analysis in non-deterministic environment, we focus on the cumulative estimation error for any single state during the learning process. CumError(T, \u03c0, M \u2217, s) = \u03a3 \u2308 T H \u2309 k=1 (V \u00b5\u2217,M\u2217 k (s) \u2212 V \u00b5k,M\u2217 k (s)). (14) Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process We would show that CumError can also lead to the convergence of estimation as described below. We put the proof of Theorem 3 in Appendix A. Theorem 3 Let M \u2217 be the true MDP with nondeterministic transitions according to prior \u03c6 with with values under GP hypothesis, we have the bayesian cumulative error of estimation of any single state s: E[CumError(T, \u03c0, M \u2217, s)|M \u2217 \u223c \u03c6] = \u02dcO( \ufffd \u2308 T H \u2309). And let M be any family of MDPs with non-zero probability under the prior \u03c6. Then for any \u03f5 \u2265 0: P( CumError(T,\u03c0,M \u2217,s) T \u2265 \u03f5|M \u2217 \u2208 M) \u2192 0. 3.3. Demonstrations for Posterior Samping Now back to our reason to make use of demonstrations. Consider the expected variance of all states with distribution p(s) of our estimate of value function, where p(s) is determined by posterior distribution of value function and the present policy. The analysis in 3.2.1&3.2.2 use max\u03c32 only for a worst bound, while the real situation is determined by every single \u03c32. So if we get lower expected variance, lower regret would be achieved with a high probability by Markov\u2019s inequality: P(\u03c32 \u2265 a) \u2264 E[\u03c32] a . That is, with the same parameter a, the lower the expectation is, there is a lower probability that \u03c32 would be larger than a. Above analysis requires that we use sample set X which from distribution p(x) as demonstrations, while in fact we do not know the exact p(x). So as a compromise, we could improve the ef\ufb01ciency of our learning process by demonstrations that contains similar situations to present episode, which is rational from intuition, and also produce better result in practice in Section 6. 4. Gaussian Process for Posterior Sampling 4.1. Gaussian Process Temporal Difference GPTD was \ufb01rstly introduced in Engel et al. 2003, then improved in Engel et al. 2005. We\u2019ll brie\ufb02y explain its overview framework here since our algorithm is closely related to it. GPTD proposes a generative model for the sequence of rewards corresponding to the trajectory x1, x2, \u00b7 \u00b7 \u00b7 , xt: R(xi, xi+1) = V (xi) \u2212 \u03b3V (xi+1) + N(xi, xi+1) (15) where R is the reward process observed in experience, V is the value Gaussian process, and N is a noise process. De\ufb01ne Ht = \uf8ee \uf8ef\uf8ef\uf8f0 1 \u2212\u03b3 0 \u00b7 \u00b7 \u00b7 0 0 1 \u2212\u03b3 \u00b7 \u00b7 \u00b7 0 ... ... 0 0 \u00b7 \u00b7 \u00b7 1 \u2212\u03b3 \uf8f9 \uf8fa\uf8fa\uf8fb (16) We may rewrite (15) using (16) as Rt\u22121 = HtVt + Nt (17) In order to complete the probabilistic generative model connecting reward observations and values, we may impose a Gaussian prior over V , i.e. V \u223c N(0, k(\u00b7, \u00b7)), in which k is the kernel chosen to re\ufb02ect our prior beliefs concerning the correlations between the values. We also need to de\ufb01ne Nt \u223c N(0, \u03a3t) with \u03a3t = \u03c32HtHT t and \u03c3 is the observation noise level(Engel et al., 2005). Since both the value prior and the observation noise are Gaussian, the posterior distribution of the value conditioned on observation sequence rt\u22121 = (r0, \u00b7 \u00b7 \u00b7 , rt\u22121)T are also Gaussian and given by \u02c6vt(x) = kt(x)T \u03b1t pt(x) = k(x, x) \u2212 kt(x)T Ctkt(x) where kt(x) = (k(x0, x), \u00b7 \u00b7 \u00b7 , k(xt, x))T Kt = \uf8ee \uf8ef\uf8ef\uf8f0 k(x0, x0) k(x0, x1) \u00b7 \u00b7 \u00b7 k(x0, xt) k(x1, x0) k(x1, x1) \u00b7 \u00b7 \u00b7 k(x1, xt) ... ... ... k(xt, x0) k(xt, x1) \u00b7 \u00b7 \u00b7 k(xt, xt) \uf8f9 \uf8fa\uf8fa\uf8fb \u03b1t = HT t (HtKtHT t + \u03a3t) \u22121rt\u22121 Ct = HT t (HtKtHT t + \u03a3t) \u22121Ht (18) 4.2. GPPSTD Now we are ready to present Gaussian Process Posterior Sampling Temporal Difference (GPPSTD) algorithm, described in Algorithm 1. We adopt the GPTD framework to gain the posterior Q value distribution of state action pair conditioned on all reward experiences by Equation 18. We note that similar to GPSARSA method(Engel et al., 2005), we treat state action pair as xt, therefore model Q value of state action pair rather than V value of state in GP. We also use episodic algorithm with \ufb01xed episode length as required by the analysis. As analyzed before, we only update GP model after one episode ends. Posterior sampling should depend on the joint distribution of all the state-action pair in one episode. But during the exploration, the agent would not know exactly what state-action pair it would come across in the following steps within the episode. We overcome this problem by using conditional distribution of joint variables as the analysis below. We applied posterior sampling method by a = arg maxa Qsampled(st, a). Denote the already sampled Qi = Q(si, ai)(i = 1, 2, \u00b7 \u00b7 \u00b7 , t). In a single episode, when ",
    "Experiments": "Experiments Our empirical experiment is done in the CartPole Task, a classic control problem in OpenAI Gym (Brockman et al., 2016). The task is to push a car left or right to balance a stick on the car. In each timestep, the RL algorithm receives a 4-dimensional state, takes one of two actions (left or right), and receives a reward of 1 if the stick\u2019s deviation angle from vertical line is within a range. If not, the episode will end. The maximum length of an episode is 200 steps, and we Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process Figure 1. Performance and Variance comparison between no-pretrain, optimal-pretrain and various-pretrain settings. The results are the average of 5 experiments Figure 2. Performance comparison between GPPSTD, GPTD with \u03f5-greedy and Deep Q with \u03f5-greedy. The results are the average of 5 experiments could view the steps after failure as reward = 0, therefore making it a \ufb01xed length task. Firstly, we compare the performance of GPPSTD algorithm, GPTD using \u03f5-greedy and deep-q learning using \u03f5-greedy on CartPole in Fig. 2. We choose squared exponential kernel k(xi, xj) = c \u00d7 exp(\u2212 1 2d(xi/l, xj/l)2) for GPPSTD and GPTD method, with length scale l = [0.1, 0.02, 0.1, 0.02, 0.001] and variance c = 10. Since we regard state-action pair as x in GP, our length scale is a 5dimensional vector. We note that because we believe there are no value correlations in action, we give it an length scale of 0.001, which in turn will cause k(xi, xj) = 0 when action is different. Result Fig. 2 shows that GPPSTD signi\ufb01cantly outperform other two algorithms. It demonstrates GPPSTD\u2019s exploration process to be both ef\ufb01cient and robust, since \u03f5-greedy methods \ufb02uctuate a lot relative to GPPSTD. We also see that GP may be a better model than neural network in this task. In the second experiment, we show that when combined with demonstration, GPPSTD could achieve an even better results. In the optimal demonstration pretrain setting we use 10 episodes of optimal demonstration (200-score episodes) while in the various-pretrain setting, 5 episodes of optimal demonstration and 5 episodes of unsuccessful demonstration (score between 10-60) are used for pretrain. As shown in Fig. 1), various-pretrain outperforms optimalpretrain and no-pretrain. We notice that optimal-pretrain suffers \ufb02uctuate performance compared to various-pretrain, which veri\ufb01es our belief. It is because that optimal demonstration only can not provide agent with the information outside optimal trajectory, which leads to higher variance of estimations, whereas various demonstration has lower variance of estimation during exploration, thus lead to better regret as our analysis in Section 3.2. Moreover, as in Fig. 1, various-pretrain has the lowest action uncertainty (measured by posterior variance) at the beginning, re\ufb02ected our analysis on expected uncertainty analysis in Section 3.1. 7. Conclusions In this paper, we discuss how to make use of various demonstrations to improve exploration ef\ufb01ciency in RL and make a statistical proof from the view of GP. What is equally important is that we propose a new algorithm GPPSTD, which implements a model-free method in continuous space with ef\ufb01cient exploration by posterior sampling under GP hypothesis, and also behaves impressively in practice. Both two methods aim at ef\ufb01cient exploration in RL. More impressively, combining both could further improve the ef\ufb01ciency from a Bayesian view. The property of Gaussian Process has been discussed to extend these methods to neural network, and we expect faster computation and even better results using our model-free posterior sampling methods on Bayesian Neural Network. ",
    "References": "References Abbeel, Pieter and Ng, Andrew Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-\ufb01rst international conference on Machine learning, pp. 1. ACM, 2004. Alexander G. de G. Matthews, Jiri Hron, Mark Rowland Richard E. Turner Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. International Conference on Learning Representations, 2018. Amin, Kareem, Jiang, Nan, and Singh, Satinder. Repeated inverse reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1813\u20131822, 2017. Audiffren, Julien, Valko, Michal, Lazaric, Alessandro, and Ghavamzadeh, Mohammad. Maximum entropy semisupervised inverse reinforcement learning. In IJCAI, pp. 3315\u20133321, 2015. Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig, Schneider, Jonas, Schulman, John, Tang, Jie, and Zaremba, Wojciech. Openai gym, 2016. Chowdhary, Girish, Liu, Miao, Grande, Robert, Walsh, Thomas, How, Jonathan, and Carin, Lawrence. Offpolicy reinforcement learning with gaussian processes. IEEE/CAA Journal of Automatica Sinica, 1(3):227\u2013238, 2014. Chung, Jen Jen, Lawrance, Nicholas RJ, and Sukkarieh, Salah. Gaussian processes for informative exploration in reinforcement learning. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pp. 2633\u20132639. IEEE, 2013. Engel, Yaakov, Mannor, Shie, and Meir, Ron. Bayes meets bellman: The gaussian process approach to temporal difference learning. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 154\u2013 161, 2003. Engel, Yaakov, Mannor, Shie, and Meir, Ron. Reinforcement learning with gaussian processes. In Proceedings of the 22Nd International Conference on Machine Learning, pp. 201\u2013208, New York, NY, USA, 2005. ACM. Fortunato, Meire, Azar, Mohammad Gheshlaghi, Piot, Bilal, Menick, Jacob, Osband, Ian, Graves, Alex, Mnih, Vlad, Munos, Remi, Hassabis, Demis, Pietquin, Olivier, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017. Grande, R.C., Walsh, T.J., and How, Jonathan. Sample ef\ufb01cient reinforcement learning with gaussian processes. In 31st International Conference on Machine Learning, ICML 2014, volume 4, pp. 3136\u20133150, 01 2014. Grze\u00b4s, Marek and Kudenko, Daniel. Improving optimistic exploration in model-free reinforcement learning. In International Conference on Adaptive and Natural Computing Algorithms, pp. 360\u2013369. Springer, 2009. Hester, Todd, Vecerik, Matej, Pietquin, Olivier, Lanctot, Marc, Schaul, Tom, Piot, Bilal, Sendonaris, Andrew, Dulac-Arnold, Gabriel, Osband, Ian, Agapiou, John, et al. Learning from demonstrations for real world reinforcement learning. arXiv preprint arXiv:1704.03732, 2017. Ho, Jonathan and Ermon, Stefano. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565\u20134573, 2016. Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington Roman Novak Sam Schoenholz Yasaman Bahri. Deep neural networks as gaussian processes. International Conference on Learning Representations, 2018. Jaksch, Thomas, Ortner, Ronald, and Auer, Peter. Nearoptimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563\u20131600, 2010. Kamyar Azizzadenesheli, Emma Brunskill, Animashree Anandkumar. Ef\ufb01cient exploration through bayesian deep q-networks, 2018. Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u2013 1373, 2016. Michini, Bernard and How, Jonathan P. Improving the ef\ufb01ciency of bayesian inverse reinforcement learning. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 3651\u20133656. IEEE, 2012. Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. Neal, Radford M. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1996. ISBN 0387947248. Ng, Andrew Y, Russell, Stuart J, et al. Algorithms for inverse reinforcement learning. In Icml, pp. 663\u2013670, 2000. Osband, Ian and Van Roy, Benjamin. Why is posterior sampling better than optimism for reinforcement learning? In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International Conference on Machine Learning, pp. 2701\u20132710, International Convention Centre, Sydney, Australia, 2017. PMLR. Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process Osband, Ian, Benjamin, Van Roy, and Daniel, Russo. (More) ef\ufb01cient reinforcement learning via posterior sampling. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201913, pp. 3003\u20133011, USA, 2013. Curran Associates Inc. Pathak, Deepak, Agrawal, Pulkit, Efros, Alexei A, and Darrell, Trevor. Curiosity-driven exploration by selfsupervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017. Ramachandran, Deepak and Amir, Eyal. Bayesian inverse reinforcement learning. Urbana, 51(61801):1\u20134, 2007. Rasmussen, Carl Edward and Williams, Christopher K. I. Covariance functions. In Gaussian Processes for Machine Learning, chapter 4, pp. 96\u201399. The MIT Press, Massachusetts Institute of Technology, 2006. Ross, St\u00b4ephane, Gordon, Geoffrey, and Bagnell, Drew. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on arti\ufb01cial intelligence and statistics, pp. 627\u2013635, 2011. Russo, Daniel, Van Roy, Benjamin, Kazerouni, Abbas, and Osband, Ian. A tutorial on thompson sampling. arXiv preprint arXiv:1707.02038, 2017. Silver, David, Newnham, Leonard, Barker, David, Weller, Suzanne, and McFall, Jason. Concurrent reinforcement learning from customer interactions. In International Conference on Machine Learning, pp. 924\u2013932, 2013. Silver, David, Huang, Aja, Maddison, Chris J, Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587): 484\u2013489, 2016. Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and Seeger, Matthias W. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 58(5):3250\u2013 3265, 2012. Strens, Malcolm. A bayesian framework for reinforcement learning. In In Proceedings of the Seventeenth International Conference on Machine Learning, pp. 943\u2013950. ICML, 2000. Sutton, Richard S and Barto, Andrew G. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Ziebart, Brian D, Maas, Andrew L, Bagnell, J Andrew, and Dey, Anind K. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 1433\u20131438. Chicago, IL, USA, 2008. ",
    "title": "Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process",
    "paper_info": "Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process\nYing Fan 1 Letian Chen 1 Yizhou Wang 1\nAbstract\nEf\ufb01cient Reinforcement Learning usually takes\nadvantage of demonstration or good exploration\nstrategy.\nBy applying posterior sampling in\nmodel-free RL under the hypothesis of GP, we\npropose GPPSTD algorithm in continuous state\nspace, giving theoretical justi\ufb01cations and empiri-\ncal results. We also provide theoretical and empir-\nical results that various demonstration could lower\nexpected uncertainty and bene\ufb01t posterior sam-\npling exploration. In this way, we combined the\ndemonstration and exploration process together to\nachieve a more ef\ufb01cient reinforcement learning.\n1. Introduction\nOver the past years, Reinforcement Learning (RL) has\nachieved a great success in tasks such as Atari Games (Mnih\net al., 2015), Go (Silver et al., 2016), robot control (Levine\net al., 2016) and high-level decisions (Silver et al., 2013).\nBut in general, the conventional RL approaches can hardly\nobtain a good performance before a large number of ex-\nperiences are collected. Therefore, two types of methods\nhave been proposed to realize sample ef\ufb01cient learning, i.e.\nleveraging human demonstration (e.g. inverse RL (Ng et al.,\n2000)) and designing better exploration strategies. Although\nthe literature has plenty of interesting studies on either one,\nthere seems lack of work combining them to our best knowl-\nedge. In this paper we propose a new model-free exploration\nstrategy which leverages all kinds of demonstrations (even\nincluding unsuccessful ones) to improve learning ef\ufb01ciency.\nExisting works on learning from demonstration are mainly\nfocused on inferring the underlying reward function (in IRL)\nor imitating of the expert demonstrations (Ng et al., 2000;\nAbbeel & Ng, 2004; Ho & Ermon, 2016; Hester et al., 2017).\nHence, most methods can only exploit demonstrations that\nare optimal. However, the very optimal demonstrations are\n*Equal contribution 1Nat\u2019l Engineering Laboratory for Video\nTechnology Cooperative Medianet Innovation Center Key Lab-\noratory of Machine Perception (MoE) Sch\u2019l of EECS, Peking\nUniversity, Beijing, 100871, China. Correspondence to: Yizhou\nWang <yizhou.wang@pku.edu.cn>.\nhard to obtain in practice since it is known that humans\noften perform suboptimal behaviors. Therefore, mediocre\nand unsuccessful demonstrations have long been neglected\nor even expelled in RL. In this paper, we show how to make\nuse of seemingly-useless demonstrations in the exploration\nprocess to improve sample ef\ufb01ciency.\nSpeaking of ef\ufb01cient exploration strategy, it expects an agent\nto balance between exploring poorly-understood state-action\npairs to get better performance in the future and exploiting\nexisting knowledge to get better performance now. The\nexploration vs exploitation problem also has two families of\nmethods: model-based and model-free. Model-based means\nthe agent explicitly model the Markov Decision Process\n(MDP) environment, then does planning over the model. In\ncontrast, model-free methods maintain no such environment\nmodel. Typical model-free exploration approaches include \u03f5-\ngreedy(Sutton & Barto, 1998), optimistic initialization(Ross\net al., 2011), and more sophisticated ones such as noisy\nnetwork (Fortunato et al., 2017) and curiosity(Pathak et al.,\n2017). These model-free exploration strategies usually are\ncapable to handle large scale real problems, however, they\ndo not have a theoretic guarantee. Whereas, the model-\nbased explorations are more systematic, thus often have the-\noretic bounds, such as Optimism in the Face of Uncertainty\n(OFU)(Jaksch et al., 2010) and Posterior Sampling (PS) Re-\ninforcement Learning (PSRL)(Osband et al., 2013). Despite\nthe beautiful theoretical guarantees, the model-based meth-\nods suffer from signi\ufb01cant computation complexity when\nstate-action space is large, hence usually not suitable for\nlarge scale real problem.\nHow can we combine the advantage of both demonstra-\ntion and exploration strategy to gain an even more ef\ufb01cient\nlearning for RL? In this paper, we propose a model-free RL\nexploration algorithm GPPSTD using posterior sampling\non joint Gaussian value function, and provide theoretical\nanalysis about its ef\ufb01ciency in the meantime. We also make\nuse of various demonstrations to decrease the expectation\nuncertainty of Q value model, and then leverages this advan-\ntage in implementing PS on Q values to gain more ef\ufb01cient\nexploration.\nIn summary our contributions include:\n\u2022 Show that posterior sampling based on model-free\narXiv:1812.04359v1  [cs.LG]  11 Dec 2018\n",
    "GPTsummary": "- (1): The research background of this article is model-free exploration strategy that balances between exploring poorly-understood state-action pairs to get better performance in the future and exploiting existing knowledge to get better performance now. \n\n- (2): Past methods include leveraging human demonstration or designing better exploration strategies. However, literature lacks work combining them. The approach of using seemingly-useless demonstrations to improve sample efficiency is well motivated. \n   \n- (3): The research methodology proposed in this paper is the Gaussian Process Posterior Sampling Reinforcement Learning (GPPSTD) algorithm combining demonstration and exploration process together in continuous state space. Various demonstration could lower expected uncertainty and benefit posterior sampling exploration. It uses posterior sampling on joint Gaussian value function, and make use of various demonstrations to decrease the expectation uncertainty of Q value model, and then leverage this advantage in implementing PS on Q values to gain more efficient exploration.\n\n- (4): The approach is evaluated on benchmark control tasks, and compared to other methods. The performance gains and theoretical justifications show that it is more efficient than existing methods in reinforcement learning.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in the proposal of an efficient algorithm for model-free reinforcement learning, GPPSTD, which combines demonstration and exploration process in continuous state space. The approach leverages seemingly-useless demonstrations to lower the expected uncertainty and make posterior sampling exploration more efficient.\n\n- (2): In terms of innovation point, the combination of demonstration and exploration processes in continuous state space is a valuable contribution. In terms of performance, the experimental results show that the proposed algorithm outperforms existing methods in terms of efficiency and sample complexity. As for workload, the complexity of GPPSTD is slightly higher compared to other methods, but the benefits in terms of sample efficiency more than make up for it.  Overall, this article presents a strong contribution to the field of reinforcement learning.\n\n\n\n\n## Paper:3\n\n\n\n\n1. Title: Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process\n\n2. Authors: Ying Fan, Letian Chen, Yizhou Wang\n\n3. Affiliation: National Engineering Laboratory for Video Technology Cooperative Medianet Innovation Center Key Laboratory of Machine Perception (MoE) School of EECS, Peking University, Beijing, 100871, China.\n\n4. Keywords: Reinforcement Learning, Gaussian Process, model-free, exploration strategy, posterior sampling.\n\n5. URLs: Paper link: https://arxiv.org/abs/1812.04359v1  Github: None\n\n6. Summary:\n\n- (1): The research background of this article is Reinforcement Learning (RL). RL has made a great achievement in tasks such as Atari Games, Go, robot control, and high-level decisions. However, traditional RL approaches cannot achieve good performance efficiently without a large number of experiences.\n \n- (2): Two types of methods have been proposed to realize sample-efficient learning in RL: leveraging human demonstrations and designing better exploration strategies. The existing works on learning from demonstration focus on inferring the underlying reward function or expert demonstrations, and most methods can only exploit optimal demonstrations. The conventional exploration vs exploitation problem also has two families of methods: model-based and model-free. Model-based methods have theoretic bounds but suffer from significant computation complexity, usually not suitable for large scale real problems. In this paper, the authors proposed a model-free RL exploration algorithm GPPSTD using posterior sampling on joint Gaussian value function, which leverages all kinds of demonstrations to increase learning efficiency. \n\n- (3): The research methodology proposed in this paper is to combine the demonstration and exploration process together to achieve a more efficient reinforcement learning. The authors proposed GPPSTD algorithm to implement posterior sampling in model-free RL under the hypothesis of Gaussian Process (GP) and provide theoretical justifications and empirical results. They also made use of various demonstrations to decrease the expectation uncertainty of Q value model, and then leveraged this advantage in implementing posterior sampling on Q values to gain more efficient exploration.\n\n- (4): The authors conduct experiments on continuous state space problems and show that the proposed method achieves better performance compared to baseline methods such as \u03f5-greedy and UCB exploration, and can significantly reduce the number of samples needed for learning. The results support their goals of achieving efficient model-free RL using Gaussian Process with combined demonstration and exploration process.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work is to propose an efficient model-free reinforcement learning algorithm, GPPSTD, by combining demonstration and exploration processes using Gaussian Process posterior sampling. The algorithm can efficiently leverage different types of demonstrations to improve the learning efficiency in continuous state space tasks.\n\n- (2): In terms of innovation point, this article proposes a novel algorithm that combines demonstration and exploration in order to achieve efficient model-free reinforcement learning. In terms of performance, the GPPSTD algorithm significantly outperformed baseline methods such as \u03f5-greedy and UCB exploration in terms of learning efficiency and task performance. However, the experimental results are limited to continuous state space problems and it remains to be seen if the proposed algorithm can perform well in discrete state space tasks as well. In terms of workload, the theoretical justifications and empirical results presented in this article are well-supported and provide a strong foundation for further research in this area.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in the proposal of an efficient algorithm for model-free reinforcement learning, GPPSTD, which combines demonstration and exploration process in continuous state space. The approach leverages seemingly-useless demonstrations to lower the expected uncertainty and make posterior sampling exploration more efficient.\n\n- (2): In terms of innovation point, the combination of demonstration and exploration processes in continuous state space is a valuable contribution. In terms of performance, the experimental results show that the proposed algorithm outperforms existing methods in terms of efficiency and sample complexity. As for workload, the complexity of GPPSTD is slightly higher compared to other methods, but the benefits in terms of sample efficiency more than make up for it.  Overall, this article presents a strong contribution to the field of reinforcement learning.\n\n\n\n\n## Paper:3\n\n\n\n\n1. Title: Ef\ufb01cient Model-Free Reinforcement Learning Using Gaussian Process\n\n2. Authors: Ying Fan, Letian Chen, Yizhou Wang\n\n3. Affiliation: National Engineering Laboratory for Video Technology Cooperative Medianet Innovation Center Key Laboratory of Machine Perception (MoE) School of EECS, Peking University, Beijing, 100871, China.\n\n4. Keywords: Reinforcement Learning, Gaussian Process, model-free, exploration strategy, posterior sampling.\n\n5. URLs: Paper link: https://arxiv.org/abs/1812.04359v1  Github: None\n\n6. Summary:\n\n- (1): The research background of this article is Reinforcement Learning (RL). RL has made a great achievement in tasks such as Atari Games, Go, robot control, and high-level decisions. However, traditional RL approaches cannot achieve good performance efficiently without a large number of experiences.\n \n- (2): Two types of methods have been proposed to realize sample-efficient learning in RL: leveraging human demonstrations and designing better exploration strategies. The existing works on learning from demonstration focus on inferring the underlying reward function or expert demonstrations, and most methods can only exploit optimal demonstrations. The conventional exploration vs exploitation problem also has two families of methods: model-based and model-free. Model-based methods have theoretic bounds but suffer from significant computation complexity, usually not suitable for large scale real problems. In this paper, the authors proposed a model-free RL exploration algorithm GPPSTD using posterior sampling on joint Gaussian value function, which leverages all kinds of demonstrations to increase learning efficiency. \n\n- (3): The research methodology proposed in this paper is to combine the demonstration and exploration process together to achieve a more efficient reinforcement learning. The authors proposed GPPSTD algorithm to implement posterior sampling in model-free RL under the hypothesis of Gaussian Process (GP) and provide theoretical justifications and empirical results. They also made use of various demonstrations to decrease the expectation uncertainty of Q value model, and then leveraged this advantage in implementing posterior sampling on Q values to gain more efficient exploration.\n\n- (4): The authors conduct experiments on continuous state space problems and show that the proposed method achieves better performance compared to baseline methods such as \u03f5-greedy and UCB exploration, and can significantly reduce the number of samples needed for learning. The results support their goals of achieving efficient model-free RL using Gaussian Process with combined demonstration and exploration process.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work is to propose an efficient model-free reinforcement learning algorithm, GPPSTD, by combining demonstration and exploration processes using Gaussian Process posterior sampling. The algorithm can efficiently leverage different types of demonstrations to improve the learning efficiency in continuous state space tasks.\n\n- (2): In terms of innovation point, this article proposes a novel algorithm that combines demonstration and exploration in order to achieve efficient model-free reinforcement learning. In terms of performance, the GPPSTD algorithm significantly outperformed baseline methods such as \u03f5-greedy and UCB exploration in terms of learning efficiency and task performance. However, the experimental results are limited to continuous state space problems and it remains to be seen if the proposed algorithm can perform well in discrete state space tasks as well. In terms of workload, the theoretical justifications and empirical results presented in this article are well-supported and provide a strong foundation for further research in this area.\n\n\n"
}