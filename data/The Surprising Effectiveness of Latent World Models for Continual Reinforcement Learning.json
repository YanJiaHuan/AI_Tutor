{
    "Abstract": "Abstract We study the use of model-based reinforcement learning methods, in particular, world models for continual reinforcement learning. In continual reinforcement learning, an agent is required to solve one task and then another sequentially while retaining performance and preventing forgetting on past tasks. World models offer a task-agnostic solution: they do not require knowledge of task changes. World models are a straight-forward baseline for continual reinforcement learning for three main reasons. Firstly, forgetting in the world model is prevented by persisting existing experience replay buffers across tasks, experience from previous tasks is replayed for learning the world model. Secondly, they are sample ef\ufb01cient. Thirdly and \ufb01nally, they offer a task-agnostic exploration strategy through the uncertainty in the trajectories generated by the world model. We show that world models are a simple and effective continual reinforcement learning baseline. We study their effectiveness on Minigrid and Minihack continual reinforcement learning benchmarks and show that it outperforms state of the art task-agnostic continual reinforcement learning methods. 1 ",
    "Introduction": "Introduction Previous Task Current Task Next Task World Model Learning Task Agnostic Exploration Policy learning inside world model Policy guides acquisition of experience Policy remembers all previous tasks Figure 1: Overview of CRL with world models. There has been many recent successes in reinforcement learning (RL), such as in games [45], robotics [36] and in scienti\ufb01c applications [35, 10]. However these successes showcase methods for solving individual tasks. Looking beyond, it is conjectured that truly scalable intelligent systems will additionally need to master many tasks in a continual manner [40, 14, 56]. The \ufb01eld of continual reinforcement learning (CRL) aims to develop agents which can solve many tasks, one and then another, continually while retaining performance on all previously seen tasks [21]. This paper aims to explore the possibility of using world models to solve CRL problems. CRL is a multifaceted problem. We showcase a method that satis\ufb01es the traditional CL desiderata: avoids catastrophic forgetting, achieves high average performance, and is scalable. At the same time it is task-agnostic, namely, it does \u2217skessler@robots.ox.ac.uk Deep RL Workshop, NeurIPS 2022 not require external task identi\ufb01cation, neither during training nor deployment. This is highly desirable from the practical point of view, and is also an uncommon capability. The proposed method is built on top of DreamerV2 [13]. Due to this, it inherits two important features: excellent sample ef\ufb01ciency due to the model-based paradigm and implicit task detection via the recurrent networks architecture. The latter enables that the method is task-agnostic [6]. The latent model allows processing image observation with a relatively low computational cost. We further propose to use replay memory [27, 18, 41] to reduce forgetting and a task agnostic exploration method [47] to enable solving sparse reward and partially-observable tasks. Such a method is straightforward to implement and tune. We verify that it successfully solves a challenging suite of Minigrid and Minihack tasks. More generally, our work shows that the model-based paradigm is a viable solution to continual reinforcement learning. 2 Preliminaries 2.1 Reinforcement Learning The environments we consider are partially observable; the agent does not have access to the environment state s \u2208 S. A Partially Observable Markov Decision Process (POMDP [19]) is the following tuple (S, A, P, R, \u2126, O, \u03b3). Here S and A are the sets of states and actions respectively, such that for st, st+1 \u2208 S and at \u2208 A. P(st+1|st, at) is the transition distribution and R(at, st, st+1) is the reward function. The discount factor is \u03b3 \u2208 (0, 1). \u2126 is the set of observations, O : S \u00d7 A \u2192 P(\u2126) is an observation function which de\ufb01nes a distribution over observations. Actions at are chosen using a policy \u03c0 that maps observations to actions: \u2126 \u2192 A. Let us assume we have access to the states st and we are working with a \ufb01nite horizon H. Then the return from a state is Rt = \ufffdH i=t \u03b3(i-t)r(si, ai). In RL the objective is to maximize the expected return J = Eai\u223c\u03c0[R1|s0] given an initial state s0. One approach to maximizing expected return is to use a model-free approach to learn a policy \u03c0\u03c6 : S \u2192 A with a neural network parameterized by \u03c6 guided by an action-value function Q\u03b8(st, at) with parameters \u03b8. Instead of learning a policy directly from experience we can employ model-based RL (MBRL) and learn an intermediate model f, for instance a transition model st+1 = f(st, at) from experience and learn our policy with additional experience generated from the model f [50]. Instead of working with the actual state st our methods consider the observations ot and recurrent action-value functions and models to help better estimate states st [15]. 2.2 Continual Supervised Learning Continual Learning (CL) is a setting whereby a model must master a set of tasks sequentially while maintaining performance across all previously learned tasks. In supervised CL, the model is sequentially shown T tasks, denoted T\u03c4 for \u03c4 = 1, . . . , T. Each task, T\u03c4, is comprised of a dataset D\u03c4 = {(xi, yi)}N\u03c4 i=1 which a neural network is required to learn from, to perform predictions. More generally, a task is denoted by a tuple comprised of the conditional and marginal distributions, {p\u03c4(y|x), p\u03c4(x)}. After task \u03c4, the model will lose access to the training dataset for T\u03c4, however its performance will be continually evaluated on all tasks Ti for i \u2264 \u03c4. For a comprehensive review of CL scenarios see [16, 54]. 2.3 Continual Reinforcement Learning In continual RL the agent will have a budget of N interactions with each task environment T\u03c4. The agent is then required to learn a policy to maximize rewards in this environment, before interacting with a new environment and having to learn a new policy. Each task is de\ufb01ned as a new POMDP, T\u03c4 = (S\u03c4, A\u03c4, P\u03c4, R\u03c4, \u2126\u03c4, O\u03c4, \u03b3\u03c4). The agent is continually evaluated on all past and present tasks and so it is desirable for the agent\u2019s policy to transfer to new tasks while not forgetting how to perform past tasks. CRL is not a new problem setting [53], however its de\ufb01nition has evolved over time and some settings will differ from paper to paper, we employ the setting above which is related to previous recent work in CRL [22, 46, 41, 20, 39, 6, 56]. 2 ",
    "Related Work": "Related Work 3.1 Continual Supervised Learning One approach to CL, referred to as regularization approaches regularizes a NN\u2019s weights to ensure that optimizing for a new task \ufb01nds a solution which is \u201cclose\u201d to the previous task\u2019s [22, 34, 58]. Working with functions can be easier than with NN weights and so task functions can be regularized to ensure that learning new function mappings are \u201cclose\u201d across tasks [26, 4, 5]. By contrast, expansion approaches add new NN components to enable learning new tasks while preserving components for speci\ufb01c tasks [42, 25]. Memory approaches replay data from previous tasks when learning the current task. This can be performed with a generative model [48]. Or samples from previous tasks (memories) [28, 1, 7]. 3.2 Continual Reinforcement Learning Seminal work in CRL, EWC [22] enables DQN [32] to be able to continually learn how to play different Atari games with limited forgetting. EWC learns new Q-functions by regularizing the L2 distance between the new task\u2019s optimal weights and previous task\u2019s optimal weights. EWC requires additional supervision informing it of task changes to update its objective, select speci\ufb01c Q-function head and select a task speci\ufb01c \u03f5-greedy exploration schedule. Progress and Compress [46] applies a regularization to policy and value function feature extractors for an actor-critic approach. Alternatively LPG-FTW [30] learns an actor-critic that factorizes into task speci\ufb01c parameters and shared parameters. Both methods require task supervision and make use of task-speci\ufb01c parameters and shared parameters. Task agnostic methods like CLEAR [41] do not require task information to perform CRL. CLEAR leverages experience replay buffers [27] to prevent forgetting: by using an actor-critic with V-trace importance sampling [11] of past experiences from the replay buffer. Model-based RL approaches to CRL have been demonstrated where the model weights are generated from a hypernetwork which itself is conditioned a task embedding [17]. Recent work demonstrates that recurrent policies for POMDPs can obtain good overall performance on continuous control CRL benchmarks [6]. A number of previous works have studied transfer in multi-task RL settings where the goals within an environment change [3, 44, 2]. In particular by incorporating the task de\ufb01nition directly into the value function [44] and combining this with off-policy learning allows a CRL agent to solve multiple tasks continually, and generalize to new goals [29]. 3.3 Continual Adaptation Instead of focusing on remembering how to perform all past tasks, another line of research investigates quick adaptation to changes in environment. This can be captured by using a latent variable and off-policy RL [57]. Alternatively, one can meta-learn a model such that it can then adapt quickly on new changes in environment [33]. All these works use small environment changes such as reward function changes or changes in gravity or mass of certain agent limbs for instance. The environments which we consider contain different A, S and reward functions such as opening doors with keys or avoiding lava or crossing a river which is quite different in comparison. Continual exploration strategies which use curiosity [38] can be added as an intrinsic reward in the face of non-stationary environments in in\ufb01nite horizon MDPs [49]. Our proposed model uses Plan2Explore which has been shown to outperform curiosity based methods [47]. Another related area of research is open-ended learning which aims to build agents which generalized to unseen environments through a curriculum which starts off with easy tasks and then progresses to harder tasks thereby creating agents which can generalize [55, 52, 37]. 4 World Models for Continual Reinforcement Learning We leverage world models for learning tasks sequentially without forgetting. We use DreamerV2 [13] which introduces a discrete stochastic and recurrent world model that is state of the art on numerous single-GPU RL benchmarks. This is a good choice for CRL since the world model is trained by reconstructing state, action and reward trajectories from experience, we can thus leverage experience 3 replay buffers which persist across tasks to prevent forgetting in the world model. Additionally, we can train a policy in the imagination or in the generated trajectories of the world model, similar to generative experience replay methods in supervised CL which remember previous tasks by replaying generated data [48]. Thus, using a world model is also sample ef\ufb01cient. Also, world models are taskagnostic and do not require external supervision, without signaling to the agent that it is interacting with a new task. Additionally, by generating rollouts in the world model\u2019s imagination the uncertainty in the world model\u2019s predictions, more speci\ufb01cally the disagreement between predictions can be used as a task-agnostic exploration bonus. To summarize, we propose using using model-based RL with recurrent world models as a viable method for CRL, see Algorithm 1 for an overview, which we instantiate using DreamerV2. 4.1 Learning the World Model DreamerV2 learns a recurrent (latent) state-space world model (RSSM) which predicts the forward dynamics of the environment. At each time step t the world model receives an observation ot and is required to reconstruct the observations \u02c6ot conditioned on the previous actions a<t (in addition to reconstructing rewards and discounts). The forward dynamics are modeled using an RNN, ht = GRU(ht\u22121, zt, at) [9] where ht is the hidden state zt are the discrete probabilistic latent states which condition the observation predictions p(ot|zt, ht). Trajectories are sampled from an experience replay buffer and so persisting the replay buffer across different tasks should alleviate forgetting in the world model [41]. 4.2 Policy Learning inside the World Model The policy \u03c0 is learned inside the world model by using an actor-critic [51] while freezing the weights of the RSSM world model. At each step t of the dream inside the RSSM world model a latent state zt is sampled, zt and the RNN hidden state condition the actor \u02c6at \u223c \u03c0( \u00b7 |zt, ht). The reward \u02c6rt+1 is predicted by the world model. The policy, \u03c0 is then used to obtain new trajectories in the real environment. These trajectories are added to the experience replay buffer. An initial observation o1 is used to start generating rollouts for policy learning. This training regime ensures that the policy generalizes to previously seen environments through the world model which imagines trajectories. 4.3 Task Agnostic Exploration The policy learns using the imaged trajectories from the RSSM world model, so world model\u2019s predicted rewards are used as a signal for the agent\u2019s policy and critic. The policy is also used to gain experience inside the real environment. So trajectories which the world model is uncertain how to predict indicate regions of the state and action space which should be prioritized by the policy when exploring the true environment. Hence, the uncertainty in the world model\u2019s trajectory prediction can be used as an additional intrinsic reward. This idea underpins Plan2Explore [47] which naturally \ufb01ts with DreamerV2. The world model quanti\ufb01es the uncertainty in the next latent state prediction by using a deep ensemble; multiple neural networks with independent weights. Deep ensembles are a surprisingly robust baseline for uncertainty quanti\ufb01cation [24] and the ensemble\u2019s variance is used as an intrinsic reward. The exploration neural networks in the ensemble are trained to predict the next RSSM latent features [zt+1, ht+1]. The world model is frozen while the ensemble is trained. The policy \u03c0 observes the reward r = \u03b1iri + \u03b1ere, where re is the extrinsic reward predicted by the world model, ri is the intrinsic reward, the latent disagreement between the next latent state predictions. The coef\ufb01cients \u03b1i and \u03b1e are \u2208 [0, 1]. Hence the policy \u03c0 can be trained inside the world model to seek regions in the state action space which the world model struggles to predict and hence when the policy is deployed in the environment it will seek these same regions in the state-action space to obtain new trajectories to train the RSSM world model. The exploration strategy is signi\ufb01cant for CRL since it is not task dependent unlike using DQN where each task needs an \u03f5-greedy schedule [22, 20] or SAC [12] which needs an entropy regularizer per task [56]. 4 Algorithm 1 Continual Reinforcement Learning with World Models 1: Input: Tasks (environments) T1:T , world model M, policy \u03c0, experience replay buffer D. 2: for T1 to TT do 3: Train world model M on D. 4: Train \u03c0 inside world model M. 5: Execute \u03c0 in task T\u03c4 to gather episodes and append to D. 6: end for 5 Experiments To test the performance of DreamerV2 as a CRL method we consider a set of challenging problems. Firstly we use 3 Minigrid tasks [8]. We also consider one CRL benchmark from the CORA suite [39]: 8 Minihack tasks [43]. Code is available at https://anonymous.4open.science/r/ dv24crl-C594.We use two primary baselines. First, Impala which is a powerful deep RL method not designed for CRL [11]. Second, we consider CLEAR [41] which uses Impala as a base RL algorithm and leverages experience replay buffers to prevent forgetting and is task agnostic. We evaluate our methods by measuring success rates over the course of learning, Fig. 2 and Fig. 3. We also can use average performance, average forgetting and average forward transfer metrics [56] to assess the effectiveness of our proposed baseline. Average Performance. This measures how well a CRL method performs on all tasks at the end of the task sequence. The task performance is p\u03c4(t) = [\u22121, 1] for all \u03c4 < T. Since we have a reward of +1 for completing the task and \u22121 for being killed by a monster or falling into lava. If each task is seen for N environment steps and we have T tasks and the \u03c4-th task is seen over the interval of steps [(\u03c4 \u2212 1) \u00d7 N, \u03c4 \u00d7 N]. The average performance metric for our continual learning agent is de\ufb01ned as: p(tf) = 1 T T \ufffd \u03c4=1 p\u03c4(tf), (1) where tf = N \u00d7 T is the \ufb01nal timestep. Forgetting. The average forgetting is the performance difference after interacting with a task versus the performance at the end of the \ufb01nal task. The average forgetting across all tasks is de\ufb01ned as: F = 1 T T \ufffd \u03c4=1 F\u03c4 where F\u03c4 = p\u03c4(\u03c4 \u00d7 N) \u2212 p\u03c4(tf). (2) By de\ufb01nition the forgetting of the \ufb01nal T-th task is FT = 0. If a CRL agent has better performance at the end of the task sequence compared to after \u03c4-th task at time-step \u03c4 \u00d7 N then F\u03c4 < 0. Forward Transfer. The forward transfer is the difference in task performance during continual learning compared to the single task performance. The forward transfer is de\ufb01ned as: FT = 1 T T \ufffd \u03c4=1 FT\u03c4 where FT\u03c4 = AUC\u03c4 \u2212 AUCref\u03c4 1 \u2212 AUC\u03c4 (3) AUC\u03c4 = 1 N \ufffd \u03c4\u00d7N (\u03c4\u22121)\u00d7N p\u03c4(t)dt and AUCref\u03c4 = 1 N \ufffd N 0 pref\u03c4 (t)dt. (4) FT\u03c4 > 0 means that the CRL agent achieves better performance on task \u03c4 during continual learning versus in isolation. So this metric measures how well a CRL agent transfers knowledge from previous tasks when learning a new task. 5.1 Minigrid Minigrid [8] is a challenging image based, partially observable and sparse reward environment. The agent, in red, will get a reward of +1 when it gets to the green goal, Fig. 2. The agent sees a small region of the Minigrid environment as observation, ot. We use 3 different tasks from Minigrid: 5 Figure 2: Performance of CRL agents on 3 Minigrid tasks. Grey shaded regions indicate the environment which the agent is currently interacting with. All learning curves are a median and inter-quartile range across 10 seeds. On the right we pick a random instantiation of the Minigrid environments that are being evaluated. DoorKey-9x9, SimpleCrossing-SN9 and LavaCrossing-9x9. Each environment has a different skill and so the tasks are diverse. Each method interacts with each task for 0.75M environment interactions, as previously proposed in [20]. We continuously evaluate CRL agents on all tasks, see Fig. 2 for the success rates. The results indicate that DreamerV2 is able to solve dif\ufb01cult exploration tasks like the DoorKey-9x9 which involves the agent having to pick up a key and then use the key to open a door before accessing the goal. Additionally, since DreamerV2 trains its policy inside the world model it is more sample ef\ufb01cient than powerful baselines like CLEAR which need \u00d710 more environment interactions to be able to solve the easier Minigrid tasks SimpleCrossing-SN9 and LavaCrossing-9x9, Table 1. The addition of Plan2Explore enables DreamerV2 to solve these environments even more quickly, see Fig. 2. DreamerV2 does exhibit some forgetting of the DoorKey-9x9 task and this indicates that additional mechanisms to prevent forgetting might be needed. From the metrics in Table 1 we can see that DreamerV2 has strong forward transfer. From the learning curves for individual tasks Fig. 4 we can see that DreamerV2 struggles on independent task learning over the course of 0.75M environment steps. In contrast, when learning continually DreamerV2 is able to solve all tasks indicating that it transfer knowledge from previous tasks. This is not entirely surprising since the levels look similar and so the world model will already be able to reconstruct certain observations from one task to the next. For DreamerV2 we use the model and hyperparameters from [13] with an experience replay buffer for world model learning of size 2M. For DreamerV2 + Plan2Explore we set the reward coef\ufb01cients to \u03b1i = \u03b1e = 0.9 which was found by grid search of various single task Minihack environments over {0.1, 0.5, 0.9} we use the same policy for exploration and evaluation and learn the world model by observation reconstruction only, rather than observation, reward and discount reconstruction. We explore these design decisions using the Minihack benchmark in Appendix B.1. For CLEAR we use an experience replay buffer size of 1M. 5.2 Minihack To test the limits of DreamerV2\u2019s ability to remember previous tasks and its exploration mechanism we look at a longer set of tasks which are harder to solve in the from of Minihack [43]. The effect of 6 0.0 1.6 3.2 4.8 6.4 8.0 1e6 0.0 0.5 1.0 T1: Room-Random-15x15-v0 dreamer v2 dv2 + p2e impala clear 0.0 1.6 3.2 4.8 6.4 8.0 1e6 0.0 0.5 1.0 T2: Room-Monster-15x15-v0 0.0 1.6 3.2 4.8 6.4 8.0 1e6 0.0 0.5 1.0 T3: Room-Trap-15x15-v0 0.0 1.6 3.2 4.8 6.4 8.0 1e6 0.0 0.5 1.0 T4: Room-Ultimate-15x15-v0 0.0 1.6 3.2 4.8 6.4 8.0 Num frames 1e6 0.0 0.5 1.0 T5: River-Narrow-v0 0.0 1.6 3.2 4.8 6.4 8.0 Num frames 1e6 0.0 0.5 1.0 T6: River-v0 0.0 1.6 3.2 4.8 6.4 8.0 Num frames 1e6 0.0 0.5 1.0 T7: River-Monster-v0 0.0 1.6 3.2 4.8 6.4 8.0 Num frames 1e6 0.2 0.0 0.2 0.4 T8: HideNSeek-v0 Proportion of successes (high is better) Figure 3: Performance of various CRL agents on 8 Minihack tasks. Grey shaded regions indicate the environment which the agent is currently interacting with. All learning curves are a median and inter-quartile range across 10 seeds. Avg. Performance (\u2191) Avg. Forgetting (\u2193) Avg. Forward Transfer (\u2191) Impala 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 CLEAR 0.03 \u00b1 0.04 0.02 \u00b1 0.02 0.01 \u00b1 0.01 Impala\u00d710 0.16 \u00b1 0.16 0.06 \u00b1 0.13 CLEAR\u00d710 0.64 \u00b1 0.20 0.00 \u00b1 0.00 DreamerV2 0.72 \u00b1 0.15 \u22120.04 \u00b1 0.16 0.17 \u00b1 0.61 DreamerV2 + Plan2Explore 0.74 \u00b1 0.02 \u22120.02 \u00b1 0.03 1.06 \u00b1 0.83 Table 1: Results on 3 Minigrid tasks. All metrics are an average and standard errors over 10 seeds. We use 0.75M interactions for each task, and 7.5M in methods marked with \u00d710. \u2191 indicates better performance with higher numbers, and \u2193 the opposite. using an additional exploration strategy is apparent as DreamerV2 + Plan2Explore is able to solve harder tasks than the baselines. Additionally DreamerV2 + Plan2Explore is also able to achieve large forward transfer similarly with the Minigrid experiments Table 2. Minihack is a set of diverse image based, sparse reward tasks based on the game of Nethack [23]. We test DreamerV2 performance on 8 tasks from Minihack [43]. In particular we consider the following tasks Room-Random-15x15-v0, Room-Monster-15x15-v0, Room-Trap-15x15-v0, Room-Ultimate-15x15-v0 River-Narrow-v0, River-v0, River-Monster-v0 and HideNSeek-v0, which are a subset of the 12 Minihack tasks from the CORA CRL benchmark [39]. Each task is seen once and has a budget of 1M environment interactions. DreamerV2 is able to solve the easier \ufb01rst three Room environments Room-Random-15x15-v0, Room-Monster-15x15-v0 and Room-Trap-15x15-v0 however struggles to solve the harder later tasks and is susceptible to forgetting. In contrast DreamerV2 + Plan2Explore is able to solve the harder River-Narrow-v0, River-v0 and River-Monster-v0 tasks and is also susceptible to forgetting of the initial Room tasks. CLEAR remembers how to solve the initial Room tasks with little forgetting, however it is unable to solve the more dif\ufb01cult River tasks. For DreamerV2 and DreamerV2 + Plan2Explore we use the same design choices as described for the Minigrid experiments in Section 5.1. For CLEAR we use a replay buffer size of 1M transitions only. The entire task sequence is 8M steps so CLEAR is effective in preventing forgetting with a relatively small experience replay buffer. 7 Avg. Performance (\u2191) Avg. Forgetting (\u2193) Avg. Forward Transfer (\u2191) Impala 0.14 \u00b1 0.05 0.14 \u00b1 0.04 0.22 \u00b1 0.09 CLEAR 0.51 \u00b1 0.07 \u22120.05 \u00b1 0.05 1.05 \u00b1 0.09 DreamerV2 0.15 \u00b1 0.11 0.33 \u00b1 0.11 0.49 \u00b1 0.66 DreamerV2 + Plan2Explore 0.36 \u00b1 0.09 0.24 \u00b1 0.09 0.74 \u00b1 0.26 Table 2: Results on 8 Minihack tasks. All metrics are an average and standard error over 5 seeds. For CLEAR and Impala we use 10 seeds. \u2191 indicates better performance with higher numbers, and \u2193 the opposite. From Table 2 we see that CLEAR has good performance, this can be explained from the learning curves in Fig. 3 where only the simplest Minihack Room tasks are solved and remembered. CLEAR like DreamerV2 + Plan2Explore is also similarly able to effectively transfer knowledge from other tasks when learning a new task with impressive forward transfer. On the other hand DreamerV2 and DreamerV2 + Plan2Explore achieve worse average performance since it is more susceptible to forgetting however they still have high forward transfer. An instance of forward transfer can be seen by looking at the single task performance on Room-Trap-15x15-v0, Room-Monster-15x15-v0 and River-Monster-v0 in Fig. 5 which isn\u2019t as high as in the CRL setting. One simple design choice we can make to alleviate forgetting further, would be to increase the size of the experience replay buffer, this decreases forgetting to 0.01 \u00b1 0.11 and increases the average performance to 0.48 \u00b1 0.09 for a replay buffer size of 8M, but at the same time this decreases forward transfer Fig. 6. 6 Discussion and Future Works We have explored the use of world models as a task-agnostic CRL baseline. World models can be powerful CRL agents as they train the policy inside the world model and can thus be sample ef\ufb01cient. World models are trained by using experience replay buffers and so we can prevent forgetting of past tasks by persisting the replay buffer from the current task to a new tasks. Importantly, the world model\u2019s prediction uncertainty can be used as an additional intrinsic task agnostic reward to help exploration and solve dif\ufb01cult tasks in a task-agnostic fashion [47]. Previous CRL exploration strategies in the literature all require task information to be effective. We use DreamerV2 as the world model [13] and we show that DreamerV2 is a powerful CRL method on two different dif\ufb01cult CRL benchmarks. We show that world models can be a strong baseline for CRL problems compared to state of the art methods such as CLEAR [41]. DreamerV2 with Plan2Explore outperforms CLEAR on Minigrid. It also can achieve comparable performance with CLEAR on Minihack. On Minihack DreamerV2 with Plan2Explore exhibits forgetting, on the other hand it can solve complicated exploration tasks. Future work, will explore making world model less susceptible to forgetting. References [1] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems, 2019. [2] A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Ayg\u00fcn, P. Hamel, D. K. Toyama, J. J. Hunt, S. Mourad, D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. 2019. [3] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. [4] A. S. Benjamin, D. Rolnick, and K. P. Kording. Measuring and Regularizing Networks in Function Space. In International Conference on Learning Representations, 2019. [5] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020. 8 [6] M. Caccia, J. Mueller, T. Kim, L. Charlin, and R. Fakoor. Task-agnostic continual reinforcement learning: In praise of a simple baseline. arXiv preprint arXiv:2205.14495, 2022. [7] A. Chaudhry, M. R. Facebook, A. I. Research, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. S. Torr, and M. . A. Ranzato. On Tiny Episodic Memories in Continual Learning. arxiv.org:1902.10486, 2019. [8] M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018. [9] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [10] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki, D. de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414\u2013419, 2022. [11] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. IMPALA: Scalable distributed deep-RL with importance weighted actorlearner architectures. In International Conference on Machine Learning. 2018. [12] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018. [13] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. [14] D. Hassabis, D. Kumaran, C. Summer\ufb01eld, and M. Botvinick. Neuroscience-inspired arti\ufb01cial intelligence. Neuron, 95(2):245 \u2013 258, 2017. [15] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series, 2015. [16] Y.-C. Hsu, Y.-C. Liu, A. Ramasamy, and Z. Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. [17] Y. Huang, K. Xie, H. Bharadhwaj, and F. Shkurti. Continual model-based reinforcement learning with hypernetworks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 799\u2013805. IEEE, 2021. [18] D. Isele and A. Cosgun. Selective experience replay for lifelong learning. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 32, 2018. [19] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Arti\ufb01cial intelligence, 101(1-2):99\u2013134, 1998. [20] S. Kessler, J. Parker-Holder, P. Ball, S. Zohren, and S. J. Roberts. Same state, different task: Continual reinforcement learning without interference. arXiv preprint arXiv:2106.02940, 2021. [21] K. Khetarpal, M. Riemer, I. Rish, and D. Precup. Towards continual reinforcement learning: A review and perspectives. arXiv preprint arXiv:2012.13490, 2020. [22] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs/1612.00796, 2016. [23] H. K\u00fcttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, and T. Rockt\u00e4schel. The NetHack Learning Environment. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2020. [24] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. [25] S. Lee, J. Ha, D. Zhang, and G. Kim. A neural dirichlet process mixture model for task-free continual learning. arXiv preprint arXiv:2001.00689, 2020. [26] Z. Li and D. Hoiem. Learning without Forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. [27] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach. Learn., 8(3\u20134):293\u2013321, May 1992. 9 [28] D. Lopez-Paz and M. . A. Ranzato. Gradient Episodic Memory for Continual Learning. In Advances in Neural Information Processing Systems, 2017. [29] D. J. Mankowitz, A. \u017d\u00eddek, A. Barreto, D. Horgan, M. Hessel, J. Quan, J. Oh, H. van Hasselt, D. Silver, and T. Schaul. Unicorn: Continual learning with a universal, off-policy agent. arXiv preprint arXiv:1802.08294, 2018. [30] J. A. Mendez, B. Wang, and E. Eaton. Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting. In Advances in Neural Information Processing Systems, 2020. [31] M. Mermillod, A. Bugaiska, and P. Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. [32] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015. [33] A. Nagabandi, C. Finn, and S. Levine. Deep online learning via meta-learning: Continual adaptation for model-based rl. arXiv preprint arXiv:1812.07671, 2018. [34] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018. [35] V. Nguyen, S. Orbell, D. T. Lennon, H. Moon, F. Vigneau, L. C. Camenzind, L. Yu, D. M. Zumb\u00fchl, G. A. D. Briggs, M. A. Osborne, et al. Deep reinforcement learning for ef\ufb01cient measurement of quantum devices. npj Quantum Information, 7(1):1\u20139, 2021. [36] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. J\u00f3zefowicz, B. McGrew, J. W. Pachocki, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018. [37] J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster, E. Grefenstette, and T. Rockt\u00e4schel. Evolving curricula with regret-based environment design. arXiv preprint arXiv:2203.01302, 2022. [38] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In International conference on machine learning, pages 2778\u20132787. PMLR, 2017. [39] S. Powers, E. Xing, E. Kolve, R. Mottaghi, and A. Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. arXiv preprint arXiv:2110.10067, 2021. [40] M. B. Ring. Continual learning in reinforcement environments. PhD thesis, University of Texas at Austin, 1994. [41] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning. In Advances in Neural Information Processing Systems 32, pages 350\u2013360. 2019. [42] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016. [43] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. Kuttler, E. Grefenstette, and T. Rockt\u00e4schel. Minihack the planet: A sandbox for open-ended reinforcement learning research. In Thirty-\ufb01fth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [44] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International conference on machine learning, pages 1312\u20131320. PMLR, 2015. [45] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, T. P. Lillicrap, and D. Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR, abs/1911.08265, 2019. [46] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell. Progress & compress: A scalable framework for continual learning. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4535\u20134544. PMLR, 2018. 10 [47] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583\u20138592. PMLR, 2020. [48] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual Learning with Deep Generative Replay. In Advances in Neural Information Processing Systems, 2017. [49] C. Steinparz, T. Schmied, F. Paischer, M.-C. Dinu, V. Patil, A. Bitto-Nemling, H. Eghbal-zadeh, and S. Hochreiter. Reactive exploration to cope with non-stationarity in lifelong reinforcement learning. arXiv preprint arXiv:2207.05742, 2022. [50] R. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991. [51] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. [52] O. E. L. Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg, M. Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021. [53] S. Thrun and T. M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems, 15(1):25\u201346, 1995. The Biology and Technology of Intelligent Autonomous Agents. [54] G. M. Van de Ven and A. S. Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019. [55] R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019. [56] M. Wolczyk, M. Zajac, R. Pascanu, L. Kucinski, and P. Milos. Continual world: A robotic benchmark for continual reinforcement learning. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 28496\u201328510, 2021. [57] A. Xie, J. Harrison, and C. Finn. Deep reinforcement learning amidst lifelong non-stationarity. arXiv preprint arXiv:2006.10701, 2020. [58] F. Zenke, B. Poole, and S. Ganguli. Continual Learning Through Synaptic Intelligence. In International Conference on Machine Learning, 2017. 11 ",
    "Experiments": "",
    "References": "References [1] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems, 2019. [2] A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Ayg\u00fcn, P. Hamel, D. K. Toyama, J. J. Hunt, S. Mourad, D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. 2019. [3] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. [4] A. S. Benjamin, D. Rolnick, and K. P. Kording. Measuring and Regularizing Networks in Function Space. In International Conference on Learning Representations, 2019. [5] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020. 8 [6] M. Caccia, J. Mueller, T. Kim, L. Charlin, and R. Fakoor. Task-agnostic continual reinforcement learning: In praise of a simple baseline. arXiv preprint arXiv:2205.14495, 2022. [7] A. Chaudhry, M. R. Facebook, A. I. Research, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. S. Torr, and M. . A. Ranzato. On Tiny Episodic Memories in Continual Learning. arxiv.org:1902.10486, 2019. [8] M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018. [9] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [10] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki, D. de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414\u2013419, 2022. [11] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. IMPALA: Scalable distributed deep-RL with importance weighted actorlearner architectures. In International Conference on Machine Learning. 2018. [12] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018. [13] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. [14] D. Hassabis, D. Kumaran, C. Summer\ufb01eld, and M. Botvinick. Neuroscience-inspired arti\ufb01cial intelligence. Neuron, 95(2):245 \u2013 258, 2017. [15] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series, 2015. [16] Y.-C. Hsu, Y.-C. Liu, A. Ramasamy, and Z. Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. [17] Y. Huang, K. Xie, H. Bharadhwaj, and F. Shkurti. Continual model-based reinforcement learning with hypernetworks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 799\u2013805. IEEE, 2021. [18] D. Isele and A. Cosgun. Selective experience replay for lifelong learning. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 32, 2018. [19] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Arti\ufb01cial intelligence, 101(1-2):99\u2013134, 1998. [20] S. Kessler, J. Parker-Holder, P. Ball, S. Zohren, and S. J. Roberts. Same state, different task: Continual reinforcement learning without interference. arXiv preprint arXiv:2106.02940, 2021. [21] K. Khetarpal, M. Riemer, I. Rish, and D. Precup. Towards continual reinforcement learning: A review and perspectives. arXiv preprint arXiv:2012.13490, 2020. [22] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs/1612.00796, 2016. [23] H. K\u00fcttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, and T. Rockt\u00e4schel. The NetHack Learning Environment. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2020. [24] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. [25] S. Lee, J. Ha, D. Zhang, and G. Kim. A neural dirichlet process mixture model for task-free continual learning. arXiv preprint arXiv:2001.00689, 2020. [26] Z. Li and D. Hoiem. Learning without Forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. [27] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach. Learn., 8(3\u20134):293\u2013321, May 1992. 9 [28] D. Lopez-Paz and M. . A. Ranzato. Gradient Episodic Memory for Continual Learning. In Advances in Neural Information Processing Systems, 2017. [29] D. J. Mankowitz, A. \u017d\u00eddek, A. Barreto, D. Horgan, M. Hessel, J. Quan, J. Oh, H. van Hasselt, D. Silver, and T. Schaul. Unicorn: Continual learning with a universal, off-policy agent. arXiv preprint arXiv:1802.08294, 2018. [30] J. A. Mendez, B. Wang, and E. Eaton. Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting. In Advances in Neural Information Processing Systems, 2020. [31] M. Mermillod, A. Bugaiska, and P. Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. [32] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015. [33] A. Nagabandi, C. Finn, and S. Levine. Deep online learning via meta-learning: Continual adaptation for model-based rl. arXiv preprint arXiv:1812.07671, 2018. [34] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018. [35] V. Nguyen, S. Orbell, D. T. Lennon, H. Moon, F. Vigneau, L. C. Camenzind, L. Yu, D. M. Zumb\u00fchl, G. A. D. Briggs, M. A. Osborne, et al. Deep reinforcement learning for ef\ufb01cient measurement of quantum devices. npj Quantum Information, 7(1):1\u20139, 2021. [36] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. J\u00f3zefowicz, B. McGrew, J. W. Pachocki, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018. [37] J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster, E. Grefenstette, and T. Rockt\u00e4schel. Evolving curricula with regret-based environment design. arXiv preprint arXiv:2203.01302, 2022. [38] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In International conference on machine learning, pages 2778\u20132787. PMLR, 2017. [39] S. Powers, E. Xing, E. Kolve, R. Mottaghi, and A. Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. arXiv preprint arXiv:2110.10067, 2021. [40] M. B. Ring. Continual learning in reinforcement environments. PhD thesis, University of Texas at Austin, 1994. [41] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning. In Advances in Neural Information Processing Systems 32, pages 350\u2013360. 2019. [42] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016. [43] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. Kuttler, E. Grefenstette, and T. Rockt\u00e4schel. Minihack the planet: A sandbox for open-ended reinforcement learning research. In Thirty-\ufb01fth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [44] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International conference on machine learning, pages 1312\u20131320. PMLR, 2015. [45] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, T. P. Lillicrap, and D. Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR, abs/1911.08265, 2019. [46] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell. Progress & compress: A scalable framework for continual learning. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 4535\u20134544. PMLR, 2018. 10 [47] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583\u20138592. PMLR, 2020. [48] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual Learning with Deep Generative Replay. In Advances in Neural Information Processing Systems, 2017. [49] C. Steinparz, T. Schmied, F. Paischer, M.-C. Dinu, V. Patil, A. Bitto-Nemling, H. Eghbal-zadeh, and S. Hochreiter. Reactive exploration to cope with non-stationarity in lifelong reinforcement learning. arXiv preprint arXiv:2207.05742, 2022. [50] R. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991. [51] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. [52] O. E. L. Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg, M. Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021. [53] S. Thrun and T. M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems, 15(1):25\u201346, 1995. The Biology and Technology of Intelligent Autonomous Agents. [54] G. M. Van de Ven and A. S. Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019. [55] R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019. [56] M. Wolczyk, M. Zajac, R. Pascanu, L. Kucinski, and P. Milos. Continual world: A robotic benchmark for continual reinforcement learning. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 28496\u201328510, 2021. [57] A. Xie, J. Harrison, and C. Finn. Deep reinforcement learning amidst lifelong non-stationarity. arXiv preprint arXiv:2006.10701, 2020. [58] F. Zenke, B. Poole, and S. Ganguli. Continual Learning Through Synaptic Intelligence. In International Conference on Machine Learning, 2017. 11 0.0 0.2 0.4 0.6 0.8 1.0 Num frames 1e6 0.00 0.25 0.50 0.75 1.00 T1: DoorKey-9x9 0.0 0.2 0.4 0.6 0.8 1.0 Num frames 1e6 T2: SimpleCrossing-SN9 dv2 dv2 + p2e 0.0 0.2 0.4 0.6 0.8 1.0 Num frames 1e6 T3: LavaCrossing-SN9 Proportion of successes Figure 4: Single task performance of on individual tasks from the Minigrid CRL benchmark. All curves are a median and inter-quartile range over 5 seeds. 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 0.0 0.2 0.4 0.6 0.8 1.0 T1: Room-Random-15x15-v0 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T2: Room-Monster-15x15-v0 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T3: Room-Trap-15x15-v0 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T4: Room-Ultimate-15x15-v0 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T5: River-Narrow-v0 dv2 dv2 + p2e 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T6: River-v0 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T7: River-Monster-v0 0.00 0.25 0.50 0.75 1.00 Num frames 1e6 T8: HideNSeek-v0 Proportion of successes Figure 5: Single task performance of on individual tasks from the Minihack CRL benchmark. All curves are a median and inter-quartile range over 10 seeds. Supplementary Material Appendix A Single Task experiments To assess the forward transfer of DreamerV2 for CRL we need the performance of each task as a reference Eq. (3). Single task learning curves for Minigrid are shown in Fig. 4 and single task learning curves for all Minihack tasks in the CRL loop are shown in Fig. 5. Appendix B Further Experiments A couple further experiments are introduced which are referenced in the main paper. In Appendix B.1 we explore various design choices required for DreamerV2 + Plan2Explore to get the best performance for CRL. Secondly, in Appendix B.2 we explore how increasing the size of the experience replay buffer size affects performance in the Minihack CRL benchmark. B.1 DreamerV2 Ablation Experiments We explore various design choices which come from the implementations of DreamerV2 [13] and Plan2Explore [47]. 1. The use of Plan2Explore as an intrinsic reward. 2. World model learning by reconstructing the observations \u02c6ot only and not the observations, rewards and discounts all together. 3. The use of the exploration policy at to evaluate the performance on all current and past tasks rather than having a separate exploration and evaluation policy. 12 Plan2Explore \u02c6o reconstruction only \u03c0exp = \u03c0eval Avg. Performance (\u2191) Avg. Forgetting (\u2193) Avg. Forward Transfer (\u2191) 0.09 \u00b1 0.07 0.37 \u00b1 0.07 0.56 \u00b1 0.86 \u0014 0.28 \u00b1 0.13 0.13 \u00b1 0.08 0.11 \u00b1 0.15 \u0014 \u0014 0.39 \u00b1 0.13 0.19 \u00b1 0.16 0.87 \u00b1 0.95 \u0014 \u0014 \u0014 0.38 \u00b1 0.03 0.22 \u00b1 0.05 0.76 \u00b1 0.25 Table 3: CRL metrics for different design decisions on DreamerV2 for the Minihack CRL benchmark of 8 tasks. All metrics are an average and standard error over 5 seeds. \u2191 indicates better performance with higher numbers, and \u2193 the opposite. 2M 4M 8M Exp. Replay buffer sz 0.05 0.00 0.05 0.10 0.15 0.20 0.25 Avg. Forgetting 2M 4M 8M Exp. Replay buffer sz 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Avg. Fwd Transfer 2M 4M 8M Exp. Replay buffer sz 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Avg. Final Performance Figure 6: CRL metrics for DreamerV2 + Plan2Explore for the Minihack benchmark of 8 tasks versus the experience replay buffer size of the world model for DreamerV2 + Plan2Explore. All metrics are median and inter-quartile range over 5 seeds. The results are shown in Table 3. We decided to pick the model in the \ufb01nal line to report the results in the main paper as they produce the good results on Minihack with relatively small standard errors. B.2 Stability versus Plasticity: Increasing the Size of the Replay Buffer By increasing the replay buffer size for world model learning for DreamerV2 + Plan2Explore we see that forgetting and average performance increases, however the forward transfer simultaneously decreases, Fig. 6. This is an instance of the stability-plasticity trade-off [31] in continual learning neural network based systems. 13 ",
    "title": "The Surprising Effectiveness of Latent World Models Supplementary Material",
    "paper_info": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNum frames\n1e6\n0.00\n0.25\n0.50\n0.75\n1.00\nT1: DoorKey-9x9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNum frames\n1e6\nT2: SimpleCrossing-SN9\ndv2\ndv2 + p2e\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNum frames\n1e6\nT3: LavaCrossing-SN9\nProportion of successes\nFigure 4: Single task performance of on individual tasks from the Minigrid CRL benchmark. All\ncurves are a median and inter-quartile range over 5 seeds.\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT1: Room-Random-15x15-v0\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT2: Room-Monster-15x15-v0\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT3: Room-Trap-15x15-v0\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT4: Room-Ultimate-15x15-v0\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT5: River-Narrow-v0\ndv2\ndv2 + p2e\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT6: River-v0\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT7: River-Monster-v0\n0.00\n0.25\n0.50\n0.75\n1.00\nNum frames\n1e6\nT8: HideNSeek-v0\nProportion of successes\nFigure 5: Single task performance of on individual tasks from the Minihack CRL benchmark. All\ncurves are a median and inter-quartile range over 10 seeds.\nSupplementary Material\nAppendix A\nSingle Task experiments\nTo assess the forward transfer of DreamerV2 for CRL we need the performance of each task as a\nreference Eq. (3). Single task learning curves for Minigrid are shown in Fig. 4 and single task learning\ncurves for all Minihack tasks in the CRL loop are shown in Fig. 5.\nAppendix B\nFurther Experiments\nA couple further experiments are introduced which are referenced in the main paper. In Appendix B.1\nwe explore various design choices required for DreamerV2 + Plan2Explore to get the best performance\nfor CRL. Secondly, in Appendix B.2 we explore how increasing the size of the experience replay\nbuffer size affects performance in the Minihack CRL benchmark.\nB.1\nDreamerV2 Ablation Experiments\nWe explore various design choices which come from the implementations of DreamerV2 [13] and\nPlan2Explore [47].\n1. The use of Plan2Explore as an intrinsic reward.\n2. World model learning by reconstructing the observations \u02c6ot only and not the observations,\nrewards and discounts all together.\n3. The use of the exploration policy at to evaluate the performance on all current and past tasks\nrather than having a separate exploration and evaluation policy.\n12\n",
    "GPTsummary": "- (1): The paper focuses on studying the effectiveness of world models for continual reinforcement learning, which aims to develop agents that can solve multiple tasks sequentially while retaining performance on all previously seen tasks. \n\n- (2): The paper presents a method that uses world models, which offer a task-agnostic solution and are straightforward to implement. The authors show that world models are an effective and simple baseline for continual reinforcement learning and outperform state-of-the-art task-agnostic methods. \n\n- (3): The proposed method is built on top of DreamerV2 and uses latent model processing of image observations to reduce computational cost. It further utilizes replay memory to reduce forgetting and task-agnostic exploration to enable solving sparse reward and partially-observable tasks. \n\n- (4): The effectiveness of the method is tested on the Minigrid and Minihack continual reinforcement learning benchmarks, and it achieves high average performance and avoids catastrophic forgetting, supporting the authors' goals of developing a simple and effective method for continual reinforcement learning.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in its development of a simple and effective method for continual reinforcement learning using world models. World models provide a task-agnostic solution and are straightforward to implement, making them a promising approach for solving multiple tasks sequentially with the ability to retain performance on all previously seen tasks.\n\n- (2): Innovation point: The article proposes using world models for continual reinforcement learning, which is an innovative approach that offers a task-agnostic solution. Performance: The proposed method achieves high performance on the Minigrid and Minihack continual reinforcement learning benchmarks and outperforms state-of-the-art task-agnostic methods. Workload: The proposed method utilizes latent model processing, replay memory, and task-agnostic exploration to reduce computational cost and minimize forgetting. However, the article lacks a detailed analysis of the scalability of the proposed method to real-world scenarios.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in its development of a simple and effective method for continual reinforcement learning using world models. World models provide a task-agnostic solution and are straightforward to implement, making them a promising approach for solving multiple tasks sequentially with the ability to retain performance on all previously seen tasks.\n\n- (2): Innovation point: The article proposes using world models for continual reinforcement learning, which is an innovative approach that offers a task-agnostic solution. Performance: The proposed method achieves high performance on the Minigrid and Minihack continual reinforcement learning benchmarks and outperforms state-of-the-art task-agnostic methods. Workload: The proposed method utilizes latent model processing, replay memory, and task-agnostic exploration to reduce computational cost and minimize forgetting. However, the article lacks a detailed analysis of the scalability of the proposed method to real-world scenarios.\n\n\n"
}