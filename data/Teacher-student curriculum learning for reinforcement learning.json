{
    "Abstract": "Abstract Reinforcement learning (rl) is a popular paradigm for sequential decision making problems. The past decade\u2019s advances in rl have led to breakthroughs in many challenging domains such as video games, board games, robotics, and chip design. The sample ine\ufb03ciency of deep reinforcement learning methods is a signi\ufb01cant obstacle when applying rl to real-world problems. Transfer learning has been applied to reinforcement learning such that the knowledge gained in one task can be applied when training in a new task. Curriculum learning is concerned with sequencing tasks or data samples such that knowledge can be transferred between those tasks to learn a target task that would otherwise be too di\ufb03cult to solve. Designing a curriculum that improves sample e\ufb03ciency is a complex problem. In this thesis, we propose a teacher-student curriculum learning setting where we simultaneously train a teacher that selects tasks for the student while the student learns how to solve the selected task. Our method is independent of human domain knowledge and manual curriculum design. We evaluated our methods on two reinforcement learning benchmarks: grid world and the challenging Google Football environment. With our method, we can improve the sample e\ufb03ciency and generality of the student compared to tabula-rasa reinforcement learning. iv Acknowledgements I want to thank everyone who supported me throughout this thesis. I am thankful for the moral support, open door for my thoughts and concerns, and advice during my thesis work. Firstly, I would like to extend my special thanks to my advisor Prof. Dr. Manfred Vogel, who supported me in my thesis and throughout my master\u2019s studies. I am thankful for his criticism, humor, and his guidance during this time. I would also like to thank Christian Scheller and Lukas Neukom for their proofreading and valuable comments on this thesis. v Symbols Symbols The following list describes important notations used in this work. E[X] expectation of a random variable X \u03b1, \u03b2 step-size parameters \u03b3 discount-rate parameter s, s\u2032 \u2208 S states, s\u2032 is subsequent step of s a \u2208 A an action r \u2208 R a reward S set of all states A(s) set of all actions available in state s R set of all possible rewards, a \ufb01nite subset of R t discrete time step T \ufb01nal time step of an episode at action at time t st state at time t rt reward at time t \u03c0 policy \u03c0(s) action taken in state s under deterministic policy \u03c0 \u03c0(a|s) probability of taking action a in state s under stochastic policy \u03c0 p(s\u2032|s, a) probability of transition to state s\u2032, from state s taking action a r(s, a) expected immediate reward from state s after action a V \u03c0(s) state-value function for policy \u03c0 V \u2217(s) optimal state-value function Q\u03c0(s, a) action-value function under policy \u03c0 Q\u2217(s, a) optimal action-value function \u03b8, \u03b8t parameter vector of target policy \u03c0(a|s, \u03b8) probability of taking action a in state s given parameter vector \u03b8 \u03c0\u03b8 policy corresponding to parameter \u03b8 \u2207\u03c0(a|s, \u03b8) column vector of partial derivatives of \u03c0(a|s, \u03b8) with respect to \u03b8 J(\u03b8) performance measure for the policy \u03c0\u03b8 \u2207J(\u03b8) column vector of partial derivatives of J(\u03b8) with respect to \u03b8 V Set of all task used in curriculum learning vi Acronyms and Abbreviations ALP Absolute learning progress section 3.2.1 EMA Exponential moving average section 3.2.1 FS-EMA Fast and slow exponential moving average section 3.2.1 LP Learning progress section 3.2.1 PTR Previous task reward section 3.2.1 RH Reward history section 3.2.1 RL Reinforcement Learning section 2.1 SMM Super Mini Map section 4.3 MDP Markov decision problem section 4.3.5 POMDP Partially observable Markov decision problem PPO Proximity Policy Optimization section 2.1.5 ",
    "Introduction": "Introduction Reinforcement learning is based on the idea of learning through interaction with an environment. Learning through interaction is a natural way to gain new skills and a concept underlying nearly all theories of learning and intelligence. Infants learn a lot about the cause and e\ufb00ect of their actions by observing the results of their actions. We learn how to achieve speci\ufb01c goals solely by observing the joy of actions leading to a pleasant state in our environment (like grabbing our favorite toys) or getting harmed by other actions (like touching a hot plate). Reinforcement learning (RL) builds upon this idea of learning through interaction. As supervised learning (SL) and unsupervised learning (UL), reinforcement learning is a machine learning paradigm. Due to its generality, researchers apply RL to a broad range of tasks. The ongoing rise of deep learning enabled reinforcement learning on high-dimensional input and highly complex environments. Successful applications are board games [Silver et al., 2016, 2017, 2018], video games [Mnih et al., 2015, OpenAI et al., 2019a, Vinyals et al., 2019] and robotics [OpenAI et al., 2019b]. The use of reinforcement learning in a practical setting is often not realistic due to the sample ine\ufb03ciency of RL. RL agents usually require a large number of interactions with the environment to learn proper behavior. For example, the training of OpenAI Five [OpenAI et al., 2019a] took two months with 150 PFlops / day to surpass professional human-level performance. Training Agent57 [Badia et al., 2020] for the Atari benchmark required 90 billion environment frames to reach human performance. There are multiple ways to improve the sample e\ufb03ciency of reinforcement learning. Imitation learning methods make use of an existing expert demonstration dataset to learn from expert priors. The simplest form of imitation learning is behavior cloning, where a policy is learned in a supervised learning fashion [Pomerleau, 1989]. This method su\ufb00ers from the distribution mismatch of the training data and the actual environment. A slight divergence of trajectories present in the dataset during inference leads to situations unknown to the trained policy. Ross et al. [2011] use a human in the loop process to counter the distribution mismatch by continuously labeling the data for those unknown situations. Another option is to use reinforcement learning to \ufb01ne-tune the initial policy [Scheller et al., 2020, Vinyals et al., 2019]. A di\ufb00erent popular strategy to make RL more 1 2 1.1. Related Work sample e\ufb03cient is to exploit the hierarchical structure of the given problem [Sutton et al., 1998, Dayan and Hinton, 1993, Sutton et al., 1999a]. One can imagine training a sub-policy for every individual task and a general policy that chooses one of those sub-policies. Another way to improve sample e\ufb03ciency is curriculum learning (CL), where the learning process is structured so that new concepts are learned in a sequence and an agent can leverage what he previously learned. Let us consider maths as an analogy: We \ufb01rst have to learn basic arithmetic before we can do linear algebra. The idea of using such a structured learning process to train arti\ufb01cial agents dates back to Elman [1993] for grammar learning. In this work, we build upon the idea of curriculum learning to improve the sample e\ufb03ciency of our RL approach. There are many ways to de\ufb01ne a curriculum. We will specify those in section 2.3. Our work focuses on how we can automatically de\ufb01ne such a curriculum during training by phrasing the task sequencing problem as a meta Markov decision process. We present a novel curriculum learning framework where no domain knowledge for the task sequencing is required. This framework aims to increase the sample e\ufb03ciency as well as the asymptotic reward compared to vanilla reinforcement learning and a set of heuristic-based baselines. This work builds upon a previous project where we applied manually designed curriculum learning on a football simulation. In this previous work, we were able to increase the sample e\ufb03ciency of our algorithm and the asymptotic reward to 1 compared to vanilla reinforcement learning with a reward of -1.4 on the 11 vs. 11 hard environment. In section 1.2 we provide a brief recapitulation of the previous work and its \ufb01ndings. 1.1 Related Work Our work lies in the \ufb01eld of curriculum learning in reinforcement learning. CL comprises three key elements: transfer learning, task sequencing, and task generation. In this work, we focus on transfer learning and task sequencing. Transfer learning is a \ufb01eld in machine learning which studies how knowledge gained by solving a source task can be transferred to a target problem such that the target task can be solved faster. In reinforcement learning, transfer learning increases the sample e\ufb03ciency or the performance on the target task. Knowledge can be transferred by selecting samples collected on a source task and use them as input for batch reinforcement learning [Lazaric et al., 2008, Lazaric and Restelli, 2011]. Options [Sutton et al., 1999b] or macro-actions can be extracted on a source task and included in the action space of the target task [Soni and Singh, 2006, Vezhnevets et al., 2016]. Models of the transition and reward function of a source task can be transferred to allow a hybrid approach of model-free and model-based RL on the target task [Fachantidis et al., 2013]. The parameters of 3 1.1. Related Work a learned policy or value function can also be used to initialize the policy or value function in the target task [Fern\u00b4andez et al., 2010, Taylor et al., 2007, Taylor and Stone, 2005]. Those methods make di\ufb00erent assumptions about the source and target Markov Decision Processes (MDP). The action or state space must be shared to allow the initialization of a policy or value function. Alternatively, a task mapping for states and actions of the source task to their equivalents of the target task is needed in tabular reinforcement learning. For some methods, we need access to a model of the source task, and not all methods allow the use of multiple source tasks. In our work, we focus on transfer learning through policy transfer and reward shaping. We refer to the surveys on transfer learning for reinforcement learning provided by Taylor and Stone [2009], Lazaric [2012] for more insights. Task sequencing is concerned with how the tasks can be sequenced in order to provide a curriculum. The sequencing of those tasks is crucial in CL. If the tasks at the beginning of the sequence are too hard, then the agent may fail to learn and is unable to solve the following tasks. The goal is to sequence the task into a curriculum which allows the agent to learn with a higher sample e\ufb03ciency and \ufb01nal performance compared to an agent trained solely on the target ask. A suitable task sequence depends on the set of tasks, the agent\u2019s characteristics, the target task, and is domain-speci\ufb01c. To perform task sequencing, we must control the environment to a certain degree to create di\ufb00erent tasks. Depending on the level of control we have, we can apply di\ufb00erent task sequencing methods. In practice, the \ufb01rst approach to task sequencing is made manually. We focus on how we can automatically generate a task sequence for our curriculum. The simplest form is a single task curriculum where we reorder recorded experience without changing the MDP, referred to as sample sequencing. We collect this experience into an experience bu\ufb00er during agent-environment interactions. This idea has its roots in supervised learning, where the training data is sampled in a speci\ufb01c order to speed up the training process [Bengio et al., 2009]. One example of sample sequencing in RL is Prioritized Experience Replay for DQN and its follow-up work [Schaul et al., 2016, Andrychowicz et al., 2017, Ren et al., 2018, Kim and Choi, 2018]. In multi-agent environments, we can create a curriculum by controlling the interaction of agents in the same environment. This approach to task sequencing is called co-learning. In its simplest form, one performs self-play where an agent competes against or acts cooperatively with an older version of itself. The agent to train and his counterpart gets progressively better on the task and therefore 4 1.1. Related Work creates an implicit curriculum. This setup has proven to be successful in the well-known Go AI Alpha Go and its successors [Silver et al., 2016, 2018]. In Vinyals et al. [2019] the idea of self-play was extended to a league setting. The goal of this league setup is that the agent faces increasingly stronger opponents and opponents trained to perform speci\ufb01c strategies to exploit weaknesses and prevent mode collapse. In co-learning and sample sequencing, the target MDP is not changed, and no speci\ufb01c level of control over the environment is required. If control over the environment is possible, one can change the target MDP by changing the initial state distribution or the reward function to create a suitable task sequence. One example of this approach is the reverse curriculum generation, where a robot is learning to reach a goal from a set of starting positions increasingly far from the goal [Florensa et al., 2017]. Foglino et al. [2019a] uses metaheuristic search methods such as beam search, tabu search, genetic algorithms, or ant colony search in order to solve the task sequencing problem. In their follow-up paper, they compare those metaheuristic search methods to their Heuristic Task Sequencing for Cumulative Return (HTSCR) algorithm [Foglino et al., 2019b]. In our work, we treat the task sequencing as an MDP [Narvekar et al., 2017, Matiisen et al., 2019, Narvekar and Stone, 2019] where we use reinforcement learning in order to train a teacher agent to perform the task sequencing online. In this setting, we formalize curriculum learning as an interaction of two MDPs. The standard MDP modeling the learning agent interacting with the environment is referred to as student MDP. A meta level MDP for the curriculum agent to perform the task sequencing referred to teacher MDP. At the time of writing, no MDP-based task sequencing method where both student and teacher use neural network-based function approximators and where both are trained using reinforcement learning is known to the author. Therefore the key focus is to propose such a framework and analyze the e\ufb03ciency of such an approach. Task generation speci\ufb01es how and when the source tasks for the curriculum are de\ufb01ned. The quality of its source tasks heavily in\ufb02uences the quality of a curriculum. The tasks can either be created beforehand or online during training. The goal of task generation is to create a set of source tasks that allow a knowledge transfer through them such that it is easier to solve the target task. In Narvekar et al. [2016], a method for creating a set of source tasks by specifying task descriptors, that are controlling the degrees of freedom of the task, is introduced. Those task descriptors specify the environment like the environment size, action set, opponent type, initial states, et cetera. In Powerplay [Schmid5 1.2. Previous Work huber, 2013] a framework where new tasks are generated online is introduced. The system searches for new source tasks such that the agent becomes more and more general. A \ufb01xed computation budget is applied to force the task generator to create new tasks that are only slightly more di\ufb03cult than the previous ones. Jiang et al. [2020] introduced the idea of prioritized level replay. In reinforcement learning environments, there is usually a level identi\ufb01er. This could be a level index or a random seed. Usually, the level to use is sampled uniformly. Which level is sampled can in\ufb02uence the di\ufb03culty of the task as well as the environment dynamics. This diversity among levels implies that di\ufb00erent levels hold di\ufb00erent learning potentials for RL agents at any point in training. Prioritized level replay introduces a new level sampling strategy to prioritize levels based on their learning potential creating an implicit curriculum. 1.2 Previous Work In previous work, we tried to reproduce the work of Kurach et al. [2019] on the Google Research Football environment. We also implemented and evaluated extensions to the Proximal Policy Optimization (PPO) algorithm such as Actor with Variance Estimated Critic (AVEC) [Flet-Berliac et al., 2020]. Additionally, we carried out experiments with curriculum learning in the Google football environment. Detailed information about this environment can be found in section 4.3. We evaluated two manually de\ufb01ned curricula as well as two automatically generated curricula, and a prioritized level replay curriculum. We used the weights obtained by training on the source task as an initialization for the policy on the target task as a transfer method. The results of our previous work are summarized in table 1.1 for a comparison in section 4.4.2. Scenarios curriculum and 11 vs 11 curriculum are both manually de\ufb01ned curricula, the \ufb01rst one over both a set of football academy tasks (see table 4.8) and the 11 vs 11 full game on di\ufb03culty easy, medium and hard. The second curriculum only uses the 11 vs 11 full game environments. The increasing curriculum consistently increases the game di\ufb03culty parameter \u03b4 by 0.05 starting from 0.01 and \ufb01nishing at 0.95. A \u03b4 value of 0.95 is equivalent to the 11 vs 11 hard environment. At the \ufb01rst task change, we increase \u03b4 from 0.01 to 0.05 instead. The di\ufb03culty is increased automatically after 100 training iterations. Smooth increasing curriculum is similar to the increasing curriculum. Instead of adapting the di\ufb03culty depending on training iterations, we increase the di\ufb03culty as soon as the average return is greater than 0.9. Additionally, we increment \u03b4 by 0.001, start with a value of 0.001 and end with a \u03b4 value of 0.95. Although four out of \ufb01ve curricula improve our results, only the smooth increasing curriculum can outperform the baseline signi\ufb01cantly. Using only the 11 vs. 11 tasks includes too hard tasks at the early stages of learning. The smooth increas6 1.3. Problem Statement ing curriculum is superior over the increasing curriculum as the task di\ufb03culty increases more evenly. Scenarios curriculum includes task switches with signi\ufb01cant changes in their initial state distribution and observation space, making this type of curriculum worse than the increasing ones. Experiment Return PPO -1.4 Best scenarios curriculum -0.85 Best 11 vs 11 curriculum -2.08 Best increasing curriculum -0.48 Best smooth increasing curriculum 1 Prioritized level replay -1.05 Baseline -1.39 Table 1.1: Comparison of our di\ufb00erent curriculum learning approaches, the average return over 100 episodes on the 11 vs 11 hard environment is reported. 1.3 Problem Statement In this work, we are developing a teacher-student curriculum learning setup focusing on online task sequencing and transfer learning. We manually do the task generation and focus on the task sequencing problem formulated as a curriculum MDP [Narvekar et al., 2017]. Both the teacher and student are policy gradient reinforcement learning agents using neural networks as function approximators and are trained with PPO. We aim to answer the following research questions: \u2022 Is it possible to train both teacher and student with the curriculum MDP (CMDP) setting from scratch such that the sample e\ufb03ciency or average reward on the target task increases compared to vanilla PPO on the target task? \u2022 What is a suitable reward function for our teacher agent to increase the sample e\ufb03ciency or average reward on the target task? \u2022 How can we de\ufb01ne observations in the CMDP such that the sample e\ufb03ciency or average reward on the target task increases? \u2022 What is a suitable transfer method for the speci\ufb01ed setting? \u2022 Are we able to improve the results of our previous work on the Google Research Football environment with this curriculum learning framework? ",
    "Background": "Background In this chapter, we introduce the core ideas and theoretical foundations used in this work. We introduce the reinforcement learning paradigm focusing on policy gradient methods and highlight some core ideas important in this work. We introduce the proximal policy optimization (PPO) algorithm and additional RL improvements used in this work. Parts of this chapter were written in our previous work [Schraner, 2020] and added to this thesis for a coherent document. 2.1 Reinforcement Learning Reinforcement learning is a popular framework suited to solve sequential decisionmaking processes. An agent learns how to act in an environment by observing a numerical reward signal. The agent has to learn a policy to predict an action based on the environment\u2019s state, such that the cumulative reward is maximized. At the early stages of learning, RL is very similar to trial and error learning. By making progress, the learner usually continuously observes new states of the environment and learns how to act in those new states without forgetting about the correct behavior in the early stages. The only indication wether a single or a series of actions leads to preferable changes in the environment is the reward signal. After every action, the agent receives a representation of the new environment state (observation) and a reward signal. This signal can be either positive or negative, and it does not tell the agent exactly which series of actions lead to that signal. The reward can be delayed, for example, students receive a negative reward signal while studying for their exams, but they receive a large positive reward signal after successfully passing the exam. This way of learning makes RL a general framework, as stated by the reward hypothesis: That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward) [Sutton and Barto, 2018]. Biological systems inspire the use of a reward signal to learn: through experiencing pleasure or pain, we know what actions are good in an immediate sense. In contrast to RL, supervised learning uses a training set of labeled examples. With supervised learning, we are trying to capture the knowledge represented in the training set. The use of a training set proved to be very powerful to learn 7 8 2.1. Reinforcement Learning intelligent behavior, but it also makes supervised learning hard for interactive problems. Often it is impractical or costly to obtain a useful set of examples of the desired behavior that is both correct and representative for all situations. There is an intersection of RL and supervised learning called imitation learning. In imitation learning, the agent receives an initial set of labeled demonstrations of actions in the environment and uses it in a supervised fashion. The agent may be improved further with standard reinforcement learning. With unsupervised learning, one can \ufb01nd structure hidden in a large amount of unlabeled data. There is no reward signal to maximize in unsupervised learning, which di\ufb00erentiates it from RL. 2.1.1 Finite Markov Decision Processes Markov decision processes (MDP) is a formalisation for sequential decision-making processes. A Markov decision process is a 4-tuple (S, A, p, r), where \u2022 S is a set of states \u2022 A is a set of actions \u2022 p(s\u2032|s, a) = P[st+1 = s\u2032|st = s, at = a] is the probability of transitioning from state s to state s\u2032 when taking action a. \u2022 r(s, a) = E[rt+1|st = s, at = a] is the expected reward received by taking action a in state s. MDPs are an idealized form of the reinforcement learning problem and are used to formulate mathematically precise theoretical statements [Sutton and Barto, 2018]. Figure 2.1: The agent-environment interaction in a Markov decision process [Sutton and Barto, 2018]. The agent-environment interaction in a MDP is shown in \ufb01g. 2.1. The decisionmaker is called an agent. The agent interacts with the environment by following a policy denoted by \u03c0. The policy maps from state s to action a \u2208 A and can either be deterministic \u03c0(s) = a or stochastic \u03c0(a|s) = P\u03c0[at = a|st = s]. Everything outside the agent is the environment. Taking an action in the environment leads to a change in the environment\u2019s state. For example in the 9 2.1. Reinforcement Learning grid world environment, an action can be moving forward, picking up a key, or opening a door. The environment receives the action of the agent and returns a reward r and a representation of its new state s \u2208 S. If the entire state of the environment is not observable, e.g., the agent has only a limited \ufb01eld of view, the agent receives an observation o \u2208 O instead of a state. The observation only partially describes the environment\u2019s state. The environment can be deterministic or stochastic and may change itself without interactions from the agent. Agent and environment are interacting in a sequence of discrete-time steps t. t starts from 0 in the \ufb01rst time-step, and goes up to T, with T being the last time-step. At each time step, the agent receives a representation of the environment state st and a reward rt based on which he selects and executes action at. This leads to a sequence of interactions called trajectory: s0a0r1s1a1r2s2a2r3 . . . statrt. A trajectory has to be \ufb01nite. In a \ufb01nite MDP, the sets of states, actions and rewards all have a \ufb01nite number of elements. The next state is only dependent on the preceding state-action pair. Therefore we have a well de\ufb01ned discrete probability distributions for the next state dependent only on the preceding state-action pair [Sutton and Barto, 2018]: P[st+1|st, at] = P[st+1|s1, a1, . . . , st, at] (2.1) This is called the Markov property. To ful\ufb01ll the Markov property, the probabilities given by p can only depend on the preceding state and action and completely characterize the dynamics of the environment. Therefore, a single state must include information about every aspect of the past agent-environment interactions that make a di\ufb00erence for the future. 2.1.2 Policy and Value Functions In reinforcement learning, agents select their actions according to their policy function. A policy has to be time-independent, the trajectory up to the time step t does not in\ufb02uence the action probabilities. The policy \u03c0(a|s) outputs a probability distribution over all possible actions given a state: \u03c0(a|s) = P[at = a|st = s] (2.2) The objective of this policy is to maximize the cumulative future reward, also called return Gt: Gt = rt+1 + \u03b3rt+2 + \u03b32rt+3 + \u00b7 \u00b7 \u00b7 = T\u22121 \ufffd k=t \u03b3krt+k+1 (2.3) 10 2.1. Reinforcement Learning The discount factor \u03b3 \u2208 [0, 1] serves multiple purposes: \u2022 future rewards may have higher uncertainty \u2022 future rewards do not provide immediate bene\ufb01ts. In some cases, immediate rewards are of more value, like in economics where we prefer the money now over later as we could invest it in maximizing future earnings. \u2022 the discount factor provides mathematical convenience, as it solves problems with in\ufb01nite MDPs or loops in the state transition graph [Sutton and Barto, 2018] The performance of a policy is measured by its state- and action-value functions. Those value functions for the policy \u03c0 are the expectation of what return the agent receives by following policy \u03c0 from state s \u2208 S. This estimation is called state-value function V (s) (how good is it to be in a given state) or action-value function Q(s, a) (how good it is to perform a given action in a given state). The state-value function is the expected return when following policy \u03c0 from state s: V \u03c0(s) .= E\u03c0[Gt|st = s] (2.4) The action-value function is the expected return when taking action a in state s and then following policy \u03c0: Q\u03c0(s, a) .= E\u03c0[Gt|st = s, at = a] (2.5) A policy \u03c0 is considered better than another policy \u03c0\u2032 if for all states s \u2208 S the expected return is larger: V \u03c0(s) > V \u03c0\u2032(s), \u2200s \u2208 S (2.6) An optimal policy \u03c0\u2217 is a policy that is better or as good as any other policy in any state: V \u03c0\u2217(s) \u2265 V \u03c0(s), \u2200s \u2208 S \u2227 \u2200\u03c0 \u2208 \u03a0 (2.7) where \u03a0 is the set of all possible policies. The optimal state-value function V \u2217 is the maximum expected return over all policies when being in state s: V \u2217(s) = max\u03c0V \u03c0(s) (2.8) The optimal action-value function Q\u2217 is the maximum expected return over all policies when being in state s and taking action a: Q\u2217(s, a) = max\u03c0Q\u03c0(s, a) (2.9) 11 2.1. Reinforcement Learning Therefore, the optimal policy \u03c0\u2217 can be obtained by acting greedily according to the action that maximizes Q\u2217(s, a): \u03c0\u2217(at|st) = \ufffd 1, if at = argmaxa\u2208AQ\u2217(st, a) 0, otherwise Standard old fashion approaches to \ufb01nd the optimal state-value function V \u2217 or optimal action-value function Q\u2217 are Dynamic Programming, Monte-Carlo Methods, or Temporal-Di\ufb00erence Learning. We will not cover those methods in this report and continue with deep reinforcement learning and policy gradient methods. See Sutton and Barto [2018] for an introduction of those traditional methods. 2.1.3 Deep Reinforcement Learning Tabular methods work well for problems with a small state and action space. For such environments, it is easy to build a table with value or action-value estimates and act according to this table. Those problems are not very interesting and far away from real-world applications. With growing state or action spaces, the memory and computation needs are growing exponentially, with respect to the state or action space, for tabular methods. It is also not feasible to visit every state to \ufb01ll a table with value estimates. With stochastic environments, the problem gets even worse, as we would have to visit every state multiple times to calculate statistics. This raises the need for state approximation to generalize between similar states. There are numerous ways to approximate states, but the advances in deep learning made neural networks the preferred option as a function approximator. The work of Mnih et al. [2015] can be considered as the breakthrough of mixing deep learning with reinforcement learning and allowing a single algorithm to learn how to play Atari games. One challenge of deep learning with RL is that the optimization problem is non-stationary because the agent encounters new states in the environment during the ongoing learning progress. 2.1.4 Policy Gradient Methods Instead of learning value functions and selecting actions based on those functions, we can directly learn a parameterized policy that selects actions without consulting a value function. This approach is called policy gradient method. In policy gradient methods the policy \u03c0\u03b8 is parametrized with a parameter vector \u03b8 \u2208 Rd. When using a neural network as a function approximator for the optimal policy, the last layer of this network is usually a softmax layer in case of discrete actions. The probability distribution will be close to a uniform distribution in the early learning stages, leading to natural exploration. If the optimal policy is stochastic, then the softmax distribution will approximate the optimal stochastic policy. If 12 2.1. Reinforcement Learning the optimal policy is deterministic, then softmax distribution degenerates to a nearly deterministic policy. There is no need to specify that beforehand. Another advantage of policy gradient methods is that we directly optimize for what we care about, which is the optimal policy. It may be simpler to learn the policy directly than to estimate the state or state-action value. Given a performance measure for \u03c0\u03b8 in the form of a di\ufb00erentiable objective function J(\u03b8), we can perform gradient ascent to update the pararameters \u03b8: \u03b8 \u2190 \u03b8 + \u03b1\u2207\u03b8J(\u03b8) (2.10) The learning rate is speci\ufb01ed by \u03b1 and needs to be tuned. The objective of reinforcement learning is to maximize the expected sum of total discounted rewards. Policy gradient methods take this as an objective function to maximize: J(\u03b8) = E\u03c0\u03b8[ T\u22121 \ufffd t=0 \u03b3trt] (2.11) The objective function in this form is usually not di\ufb00erentiable as it is dependent on the stationary state distribution of the environment and thus can not be used for gradient ascent. By the policy gradient theorem [Sutton and Barto, 2018] we can rewrite J(\u03b8) to a di\ufb00erentiable form: \u2207J(\u03b8) = E\u03c0\u03b8[\u2207\u03b8log\u03c0\u03b8(at|st)Q\u03c0\u03b8(st, at)] (2.12) Actor-critic methods are policy gradient methods that are also learning a value function. This value function is used as a critic. The critic is used for bootstrapping as in temporal-di\ufb00erence learning [Sutton and Barto, 2018]. Using one or more estimated values, in this case, an estimate for the action-value, in the update step for the same kind of estimated value is called bootstrapping. Actor-critic methods use a bootstrapped n-step return or directly estimate the action-value function. The advantage of bootstrapping is variance reduction of the estimates and allowing us to take updates on partial episodes. However, it comes at the cost of a bias towards the learned critic as it usually does not match the real action-value function. In advantage actor-critic methods, the same critic used for bootstrapping is also used as a baseline function. The baseline (estimated action-value) is subtracted from the sampled n-step return resulting in a term called advantage. The advantage tells the agent how much better or worse his policy performs than currently estimated by the critic. The actor updates the policy parameters \u03b8 for \u03c0\u03b8(at|st), in the direction suggested by the critic. If the observed return is less than the expected return, we want to lower the probability of taking the given action; otherwise, we want to assign that action a higher probability. Using a baseline leads to further variance reduction and thus increases the convergence speed. 13 2.1. Reinforcement Learning 2.1.5 Proximal Policy Optimization Proximal policy optimization (PPO) is a policy gradient method for reinforcement learning by Schulman et al. [2017]. Similar to trust region policy optimization (TRPO) [Schulman et al., 2015] a constraint on the policy update is added such that parameter updates do not change the policy too much. This improves training stability as it prevents large policy updates. In TRPO an objective function is maximized, while a constraint on the size of policy updates is enforced. The Kullback\u2013Leibler (KL) divergence of the old policy \u03c0\u03b8old and the policy after the updated \u03c0\u03b8 is used for such a constraint: maximize\u03b8E[ \u03c0\u03b8(at|st) \u03c0\u03b8old(at|st)At(st, at)] (2.13) subject to E[KL[\u03c0\u03b8old(\u00b7|st), \u03c0\u03b8(\u00b7|st)]] \u2264 \u03b4 (2.14) where \u03b8old is the vector of policy parameters before the update, and A is the advantage. This constraint can be transformed to a penalty in order to solve an unconstrained optimization problem: maximize\u03b8E[ \u03c0\u03b8(at|st) \u03c0\u03b8old(at|st)At \u2212 \u03b2KL[\u03c0\u03b8old(\u00b7|st), \u03c0\u03b8(\u00b7|st)]] (2.15) where \u03b2 is a scaling coe\ufb03cient. In TRPO, the hard constraint is used instead of a penalty because it is hard to choose a value of \u03b2 that performs well. Especially on problems where the characteristics change throughout learning, as is the case in RL. PPO builds upon this idea of a trust region but implements a more straightforward constraint. A clipped surrogate objective is used to archive this simpli\ufb01cation. The ratio between old an new policies is denoted by \u03c6(\u03b8): \u03c6(\u03b8) = \u03c0\u03b8(at|st) \u03c0\u03b8old(at|st) (2.16) The TRPO objective is, therefore: JTRPO(\u03b8) = E[\u03c6(\u03b8)At(st, at)] (2.17) PPO now imposes the constraint by forcing \u03c6(\u03b8) to stay within a trust region of [1 \u2212 \u03f5, 1 + \u03f5] where \u03f5 is a hyperparameter. Simply clipping updates force the policy to stay in the trust region: JCLIP(\u03b8) = E[min(\u03c6(\u03b8)A, clip(\u03c6(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5)A)] (2.18) 14 2.1. Reinforcement Learning Therefore if the objective value is not in the trust region, the clipped value will be used. When PPO is used in a network architecture with shared parameters for policy (actor) and value (critic) functions, an additional entropy term (blue) is introduced to encourage exploration. Furthermore, the error term on the value estimation (red) is part of the PPO objective function. JCLIP(\u03b8) = E[JCLIP(\u03b8) \u2212 c1(V\u03b8(st) \u2212 Vtarget(st))2 + c2H(st, \u03c0\u03b8(\u00b7, st))] (2.19) where both c1 and c2 are scaling parameters for those losses which need to be tuned. PPO increases the sample e\ufb03ciency of the RL algorithm empirically, and we hence use it in this work. Hsu et al. [2020] has shown that in some cases, PPO fails to converge to a bad local optimum: \u2022 Using standard PPO with a continuous action space, training becomes unstable when rewards vanish outside the trust region. This can happen due to a bad Gaussian policy update, where PPO fails to recover. \u2022 On high-dimensional discrete action spaces, clipping might converge to suboptimal actions with standard softmax policy parametrization. This happens when the policy sees only bad actions (reward 0) and suboptimal actions without observing the optimal action. PPO then tends to increase the probability of suboptimal actions and not exploring new actions. \u2022 PPO can converge to suboptimal actions if they are close to the initialization. 2.1.6 Challenges of Deep Reinforcement Learning There are a lot of unsolved challenges in deep reinforcement learning. The most important two are the exploration vs. exploitation dilemma and the deadly triad. The exploration vs. exploitation dilemma also exists in the real world. We might have our favorite ski resort, where we go skiing every year. However, there are a lot of other ski resorts which we have not visited yet. It could be that one of the new resorts would please us more than our current favorite. If we do not try out di\ufb00erent resorts, we may never \ufb01nd the optimal one, but we risk skiing in a place with only easy slopes and bad restaurants if we try out new ones. The best longterm strategy might involve short-term sacri\ufb01ces to \ufb01nd the optimal ski resort. The di\ufb03culty is to \ufb01nd the optimal ratio of exploring new places vs. exploiting the current best one. This analogy can be adapted to reinforcement learning. We have to explore new actions to learn more about their e\ufb00ectiveness and maximize the expected return in the long run by exploiting new better actions found by 15 2.1. Reinforcement Learning exploring. In stochastic tasks, this dilemma is even worse because we have to explore the same action multiple times due to the stochastic changes. According to Sutton and Barto [2018] this problem remains unresolved and is speci\ufb01c to RL as it does not occur in supervised or unsupervised learning. The deadly triad is a de\ufb01nition by Sutton and Barto [2018]. It states that instability and even divergence while optimizing arises whenever we combine the following three elements: \u2022 Function approximation like neural networks. This element is needed to solve problems with large state and action spaces. \u2022 Bootstrapping Using one or more estimated values in the update step for the same kind of estimated value [Sutton and Barto, 2018]. Bootstrapping adds a bias towards our start estimation, but the updates result in high variance for long trajectories without bootstrapping. Without bootstrapping, we need more samples before our estimate converges, leading to a loss in data e\ufb03ciency and a rise in computational cost. \u2022 O\ufb00-policy training We call our training o\ufb00-policy when we update another policy (target policy) than the one we followed to generate the training trajectories. We need to train multiple value functions and policies in parallel for some use-cases and are therefore o\ufb00-policy. Many of the current state-of-the-art RL algorithms are o\ufb00-policy algorithms. Being o\ufb00-policy is also useful as it allows us to run a learned policy in parallel distributed on hundreds of actors and having a centralized learner who uses the generated trajectory to update a slightly o\ufb00-policy policy. All of the three elements are very useful, and one does not want to give them up. In practice, many RL architectures successfully use all three elements of the deadly triad, like DQN [Mnih et al., 2015]. In the example of DQN, they use many tricks to cope with the instability and prevent the estimates from divergence through training with experience replay and occasionally freeze the target network. 16 2.2. Transfer Learning 2.2 Transfer Learning In reinforcement learning, an agent usually starts with a random policy. The agent then has to learn an optimal policy for the target task using no prior knowledge. For challenging target tasks, for example, due to sparse reward signals or poor state representations, the agent might learn very slowly or entirely fails to learn at all. Transfer learning is one area of research that tries to speed up the training of RL agents by transferring knowledge from one or more source task MDPs to a target task. Instead of learning to solve the target task tabula rasa, the agent acquires knowledge on one or more source tasks. The knowledge can be transferred in form of samples [Lazaric et al., 2008], options [Soni and Singh, 2006], policies [Fern\u00b4andez et al., 2010], models [Fachantidis et al., 2013] or value functions [Taylor and Stone, 2005]. In the case of policy and value function transfer, the parameters of a policy or value function obtained by training on one or multiple source tasks can be used to initialize the policy or value function of the agent. Transferring the policy or value function leads to a bias in the action selection towards the experience acquired in the source task. 2.2.1 Evaluation Metrics for Transfer Learning To quantify the bene\ufb01ts gained from transfer learning, we need meaningful metrics. Typically we compare the learning curve on the target task for an agent after transfer with a tabula rasa agent. We consider the following three metrics: \u2022 Time to Threshold: The time to threshold computes how much faster an agent with knowledge transfer reaches a reward threshold compared to a tabula rasa agent. The time can be measured by CPU / GPU instructions, wall clock time, episodes, or steps. \u2022 Jumpstart: This measurement quanti\ufb01es the initial performance boost we gain as a result of knowledge transfer. \u2022 Asymptotic Performance: The asymptotic performance compares the \ufb01nal performance increase at the end of training. When comparing tabula rasa agents to agents that use transfer learning, we need to specify if we want to include the time spent learning the source tasks in our metrics. We talk about weak transfer when we do not include the costs of training in source tasks. If we consider the time spent in the source tasks when calculation the metrics, we measure the strong transfer. In \ufb01g. 2.2 we illustrate the three metrics once with a weak transfer and once with a strong transfer. As the strong transfer includes costs for training in the source task, the transfer curve starts with a delayed training time in \ufb01g. 2.2 (b). 17 2.2. Transfer Learning (a) Weak transfer (b) Strong transfer Figure 2.2: Performance metrics for transfer learning using (a) weak transfer and (b) strong transfer (\ufb01gure by Narvekar et al. [2020]). 18 2.3. Curriculum 2.3 Curriculum We de\ufb01ne a curriculum as a concept that organizes past experiences and schedules future experiences by training on tasks. Every task is modeled as a Markov Decision Process. In the following we use the curriculum de\ufb01nition of Narvekar et al. [2020]: De\ufb01nition 2.3.1 (Curriculum). Let T be a set of tasks, where mi = (Si, Ai, pi, ri) is a task in T and i is the task identi\ufb01er. Let DT be the set of all possible transition samples from tasks in T : DT = {(s, a, r, s\u2032)|\u2203mi \u2208 T s.t. s \u2208 Si, a \u2208 Ai, s\u2032 \u223c pi(\u00b7|s, a), r \u2190 ri(s, a, s\u2032)}. A curriculum C = (V, E, g, T ) is a directed acyclic graph, where V is the set of vertices, E \u2286 {(x, y)|(x, y) \u2208 V \u00d7 V \u2227 x \u0338= y} is the set of directed edges, and g : V \u2192 P(DT ) is a function that associates vertices to subsets of samples in DT , where P(DT ) is the power set of DT . A directed edge (vj, vk) in C indicates that samples associated with vj \u2208 V should be trained on before samples associated with vk \u2208 V. All paths terminate on a single sink node vt \u2208 V. If the curriculum is created online, then the edges are added dynamically during the agent\u2019s training. If the curriculum is created o\ufb04ine, then the graph is created beforehand. One simpli\ufb01cation to the curriculum de\ufb01nition is the single-task curriculum, where all samples stem from a single task. In a single-task curriculum, we rearrange the order in which we train on the experience samples as in prioritized experience replay [Schaul et al., 2016]. De\ufb01nition 2.3.2 (Single-task Curriculum). A single-task curriculum is a curriculum C where the cardinality of the set of tasks considered for extraction samples |T | = 1, and consists of only the target task mt. A second simpli\ufb01cation of the curriculum de\ufb01nition is the task-level curriculum. In the task-level curriculum, we de\ufb01ne a directed acyclic graph of intermediate tasks. The main challenge here is how to order the intermediate tasks such that the agent can constantly learn to solve more complex tasks while preventing catastrophic forgetting on already solved tasks. The mapping function g determines the set of samples DT i that are available at the next vertex. There are multiple works on task-level curricula [Svetlik et al., 2017, Matiisen et al., 2019]. De\ufb01nition 2.3.3 (Task-level Curriculum). For each task mi \u2208 T , let DT i be the set of all samples associated with task mi : DT i = {(s, a, r, s\u2032)|s \u2208 Si, a \u2208 Ai, s\u2032 \u223c pi(\u00b7|s, a), r \u2190 ri(s, a, s\u2032)}. A task-level curriculum is a curriculum C = (V, E, g, T ) where each vertex is associated with samples from a single task in T . Thus, the mapping function g is de\ufb01ned as g : C \u2192 {DT i |mi \u2208 T }. 19 2.4. Curriculum Learning The simplest form of a curriculum is the sequence curriculum. In the sequence curriculum, all nodes have an indegree and outdegree of at most 1. If combined with the task-level curriculum, we end up with the task-level sequence curriculum, which is an ordered list of tasks [m1, m2, . . . mt] De\ufb01nition 2.3.4 (Sequence Curriculum). A sequence curriculum is a curriculum C where the indegree and outdegree of each vertex v in the graph C is at most 1, and there is exactly one source node and one sink node. 2.4 Curriculum Learning In curriculum learning, we try to \ufb01nd the optimal order in which we feed experience from di\ufb00erent source tasks to the agent in order to maximize one of the metrics de\ufb01ned in section 2.2.1. The intuition behind curriculum learning is that through generalization over source tasks and knowledge transfer through increasingly more complex tasks, we can increase the sample e\ufb03ciency or asymptotic performance of our training algorithms. There are three key elements to curriculum learning: \u2022 Task Generation: In order to produce a bene\ufb01cial curriculum, we need a good set of source tasks. The tasks should provide an increasing di\ufb03culty. The tasks have to be similar to the target task. Otherwise, we might end up with a negative transfer, and using a curriculum over such tasks is hurtful for solving the desired target task. Task generation is, therefore, an important part of curriculum learning. In a task-level curriculum, the tasks are the nodes of the curriculum graph. The task may be generated online during training or pre-speci\ufb01ed. \u2022 Sequencing: Sequencing is concerned with the ordering of the experience samples. According to the graph de\ufb01nition in section 2.3, sequencing de\ufb01nes the vertices in our curriculum graph. These vertices can either be de\ufb01ned online during training or o\ufb04ine before training the agent. In section 2.4.2 we go into details of this aspect to curriculum learning. \u2022 Transfer Learning: In order to bene\ufb01t from a curriculum of tasks, we need a method to transfer knowledge through the curriculum. In section 2.2 we explained multiple methods for transfer knowledge from one or more source tasks to the target task. In curriculum learning, we repeatedly transfer knowledge from task to task, whereas we have only one transfer step in standard transfer learning. We visualized the interaction between the three key elements in \ufb01g. 2.3. When evaluating the curriculum, we use the same metrics as in section 2.2.1. Those metrics have to be extended as we have to consider the costs of building 20 2.4. Curriculum Learning Figure 2.3: We visualize the interaction between the three key elements of curriculum learning. Task generation is concerned with generate a set of tasks for the curriculum. Task sequencing selects a task out of the task set for the agent. The agent use knowledge obtained by training on previously selected task through transfer learning. After or during training on the new task, the obtained knowledge is stored. The task generation and sequencing can be done online and dependent on the student\u2019s current performance or o\ufb04ine before training. the curriculum. It is not clear how the work for the task sequencing or task generation done by humans should be included into the strong transfer metrics. In our work, we ignore those costs for the sake of simplicity. 2.4.1 Curriculum Learning Categorization We can categorize a curriculum learning approach along six dimensions, organized by attributes (in bold) and their respective values (in italics). This categorization was introduced in Narvekar et al. [2020]: 1. Intermediate task generation: target / automatic / domain experts / naive users. The set of tasks can be either de\ufb01ned o\ufb04ine before training or online during training. One can also specify a single task curriculum called target where only the target task is used. The tasks can be speci\ufb01ed by a human, either a domain expert [Schraner, 2020] or a naive user with no special domain knowledge. There are also methods to automatically generate tasks using a set of rules or a generative process as in Wang et al. [2019]. 2. Curriculum representation: single / sequence / graph. The simplest way to represent a curriculum is a single task curriculum, where we simply reorder the recorded experience [Schaul et al., 2016, Andrychowicz et al., 2017]. When using multiple tasks we can either represent the curriculum 21 2.4. Curriculum Learning as a simple sequence of tasks [Schraner, 2020] or as directed acyclic graph of tasks [Svetlik et al., 2017]. In the task representation, we can allow many-to-one, one-to-many, and many-to-many knowledge transfer. 3. Transfer method: policies / value function / task model / partial policies / shaping reward / other / no transfer. In section 2.2 we speci\ufb01ed di\ufb00erent forms of knowledge transfer. The weights of policy [Scheller et al., 2020] or value [Fern\u00b4andez et al., 2010, Taylor et al., 2007, Taylor and Stone, 2005] functions can be transferred from task to task. One can learn task models [Fachantidis et al., 2013] and transfer those from task to task, learn an auxiliary reward function, or extract options [Sutton et al., 1999b] and transfer those to the next task. 4. Curriculum sequencer: automatic / domain experts / naive users. The curriculum sequencing is concerned with the task switches during training. The switch can happen automatic upon speci\ufb01c rules, through a teacher or other methods. In section 2.4.2 we go into detail about this aspect of curriculum learning. We can also sequence the task manually by domain experts or naive users. 5. Curriculum adaptivity: static / adaptive. The adaptivity of a curriculum speci\ufb01es if the curriculum is completely de\ufb01ned before training or if it is dynamically adapted online during training. A static curriculum is de\ufb01ned before training, a adaptive curriculum is changed during training. Adaptive curricula can use the learning progress to estimate, e.g., if a task is easy or hard to learn at the current stage. Static curricula incorporate problem-speci\ufb01c knowledge. 6. Evaluation metric: time to threshold / asymptotic / jumpstart / total reward. In section 2.2.1 we introduced metrics to quantify the bene\ufb01ts gained from curriculum learning. When calculating those metrics, we have to decide if we want to measure the weak or strong transfer. 22 2.4. Curriculum Learning 2.4.2 Task sequencing Task sequencing is concerned with how the tasks can be sequenced in order to provide a curriculum. The tasks need to be sequenced in a way such that the current task at hand is just hard enough to solve. It might also be of interest to add an already learned task that the agent forgets about to allow live long learning. In section 1.1 we already introduced a few methods for task sequencing. In this section, we detail two sequencing methods that use the teacher-student setup with the MDP formulation. Narvekar et al. [2017] formulates curriculum learning as the interaction of two MDPs. A student MDP describes the currently selected task, and a teacher MDP models the selection of the next task for the student. They denote the teacher MDP as a fully observable MDP, where the state space is the set of policies the learning agent can represent. The state is represented as the parameters of the policy. The \ufb01nal states are de\ufb01ned as policies where the return on the target task is higher than a speci\ufb01c threshold. The action space is the set of tasks a student agent can train on. Taking an action results in the student training on the selected task for a \ufb01xed number of steps or until convergence. The transition function describes how the student agents policy changes as a result of learning a task. The reward function is de\ufb01ned as the time needed by the student agent to learn a policy that results in a return surpassing a certain threshold on the target task (time to threshold). The teacher wants to minimize this time to threshold. Therefore the reward is encoded as the negative of the expected time needed to learn the target task starting from a given policy. A recursive MonteCarlo algorithm optimizes the teacher agent, and the student is a tabular RL agent with tile coding, trained with Sarsa(\u03bb) and a value function transfer. In their follow-up paper, they investigated reward shaping as an additional transfer method [Narvekar and Stone, 2019]. Matiisen et al. [2019] uses a similar approach, but the state is not fully observable, making the MDP a partially observable Markov decision process (POMDP). The state and action space of the teacher MDP are the same as in Narvekar et al. [2017], but the teacher has no access to the internal parameters of the student agent. The observation is the reward of the student obtained on the currently selected task. The reward is the average reward of the student evaluated on all tasks at the end of a teacher step. One could also optimize for the reward of the target task, but initially, the student might not archive a reward on the target task leaving the teacher without a meaningful signal. A comprehensive survey of task sequencing methods is provided by Narvekar et al. [2020]. ",
    "Methods": "Methods In this chapter, we describe the methods used in this thesis. We characterize our curriculum learning approach along the six dimensions introduced in section 2.4. Next, we detail our transfer method and the automated curriculum sequencing approach. We propose multiple types of observations and reward signals for our curriculum MDP. Finally, we describe our set of baselines and the evaluation protocol. 3.1 Curriculum Learning In this section, we characterize our curriculum learning approach. We use a tasklevel sequence curriculum. A combination of the task-level curriculum de\ufb01ned in De\ufb01nition 2.3.3 and a sequence curriculum (see De\ufb01nition 2.3.4). Our curriculum learning has the following properties: 1. Intermediate task generation: The tasks, represented as nodes in the curriculum graph, are prede\ufb01ned before training. We use a subset of the tasks provided by the grid world environment and the Google Research Football environment. We initially selected the tasks and kept this selection \ufb01xed throughout this thesis. The exact task selection is described in section 4.2.1 and section 4.4.1. 2. Curriculum representation: We represent the curriculum as a task-level sequence. We only allow one-to-one knowledge transfer between our source tasks and the target task. A source task can be visited multiple times in the sequence. Therefore the agent is allowed to retrain on already known tasks. 3. Transfer method: We experiment with three transfer methods. First, we copy the policy and value function weights from task to task to transfer the learned policy and value function. Second, the learned value function, obtained by training on the previous task, calculates an additional reward signal that we added to the environment\u2019s reward signal. Third, we experiment with a combination of the \ufb01rst and second method. In section 3.1.1 we go into detail about our knowledge transfer methods. 4. Curriculum sequencer: In this work, we experiment with automated 23 24 3.1. Curriculum Learning task sequencers. We formulate the task sequencing problem as a curriculum Markov Decision Process where we have full control over the student\u2019s MDP. In section 3.2 we formalize the CMDP and explain our approach in detail. 5. Curriculum adaptivity: We use an adaptive curriculum. The teacher performs task sequencing online to select tasks with a suitable learning potential for the current learning stage. Our set of source tasks is de\ufb01ned beforehand and kept \ufb01x throughout training. 6. Evaluation metric: We evaluate our agents using a weak transfer with the asymptotic performance improvement. We compare our work against agents trained in Schraner [2020], which uses manually de\ufb01ned curricula. 3.1.1 Transfer Method We use two types of knowledge transfer methods: policy and value function transfer and reward shaping. Our student agent starts with a random initialized policy and value function. After training for a \ufb01xed amount of steps at time-step t on a task mt, we switch to a new source task mt+1. The weights of the policy and value function obtained by training on task mt are used to initialize the agent before training on task mt+1. Our student uses weight sharing for the policy and value function. Therefore we copy all neural network weights from task to task. We assume that both the policy and value function transfer from task to task as the dynamics of the environment does not change in our set of tasks V. Therefore, a state with a true high value in one environment mi \u2208 V has also a true high value in another environment mj \u2208 V. The same applies to our policy. The environments in our set of source tasks di\ufb00er in their initial state distribution, state space and complexity. All tasks in V share the same set of actions A, environment dynamics function p and reward function r. It would also be possible to only transfer the weights of the shared embedding together with either the policy head or the value head from task to task. Like this, either the policy or the value are initialized randomly for each task. The gradients for the policy and the value function \ufb02ow through the same model in our network architecture with weight sharing. If either the policy or the value function is initialized randomly, this could lead to catastrophic updates to the shared weights. Therefore, we transfer all the neural network weights from task to task. Additionally to the policy and value function transfer, we experiment with a shaped reward signal. We use the value function vmt obtained by training on task mt as an additional reward signal when training on the next task mt+1. The reward ri at time-step i is therefore ri = ri + vmt(si) We expect that the value function can help to overcome the spares reward signal provided by the 25 3.2. Teacher Curriculum Markov Decision Process environment. For the \ufb01rst task m1 in our curriculum, we do not add a shaped reward signal as we do not have a value function at hand. Only the value function of the previous task is added as a shaped reward. One could also use the average of the last n value functions as a shaped reward signal. We leave this experiment to future work. The two transfer methods described can be combined, leading to a transfer method with policy and value function transfer and a shaped reward signal. We also experiment with this combined transfer method. 3.2 Teacher Curriculum Markov Decision Process In this section, we formulate the sequencing problem as a Markov Decision Process. In this formulation, we de\ufb01ne curriculum learning as an interaction between two types of MDPs. The \ufb01rst MDP is the standard RL MDP modeling the student interacting with a task. The second one is a higher level meta-MDP called curriculum MDP (CMDP). We use the CMDP to model a teacher selecting tasks for the student. The CMDP is a 4-tuple (S, A, p, r), where S is the set of all possible states equal to all possible policies the student can represent. The action space A is the set of tasks the teacher can assign to the student. Taking an action in the CMDP equals to an entire training cycle on the selected task in the student MDP for a \ufb01xed amount of steps. The environment\u2019s dynamics function p models the transition from one student policy to the next student policy after taking an action in the CMDP. The dynamics function is unknown. The reward for the CMDP is de\ufb01ned by the reward function r. In section 3.2.2 we detail two reward signals used in this research. The interaction between the CMDP and the student MDP is shown in \ufb01g. 3.1. The teacher selects a task, and the student trains for n steps on the selected task. After the student training step, we evaluate the student on a set of evaluation tasks and feed the evaluation result to the CMDP. In our research the set of evaluation tasks Veval is equal to the learning tasks Vlearn. Now that we have de\ufb01ned the sequencing problem as a CMDP, we can use reinforcement learning to \ufb01nd an optimal policy. We are using PPO to learn a teacher policy while training the student at the same time. With this approach, we have to deal with noise from the student training when performing suboptimal task switches, especially at the beginning of our training procedure. The teacher has to learn how to perform task sequencing while the student has to learn how to solve the selected environment. Both are acting randomly, which may lead to the teacher selecting too tricky tasks, and the student has a hard time learning from that task. 26 3.2. Teacher Curriculum Markov Decision Process Figure 3.1: Teacher-student interaction for task sequencing. 3.2.1 Teacher Observation We experiment with di\ufb00erent types of observations for our teacher agent. The state of the CMDP is the current policy of the student agent. We are using a neural network to approximate the optimal student policy \u03c0\u2217 s. Therefore the student weights \u03b8s are the state of the CMDP, and they ful\ufb01ll the Markov property. It is unclear how we should feed this state representation into our teacher agent. We could \ufb02atten the weights \u03b8s to a feature vector, but this would leave us with an enormous input vector. In our case, we have roughly 400\u2032000 weights in our student network. Using such a large input vector is not feasible due to memory limits. Additionally, we assume that such a feature vector is not easy to interpret for our teacher agent. It is hard to relate \u03b8s to the student\u2019s performance and the optimal next task to select. We use principal component analysis (PCA) [F.R.S., 1901] to \ufb01nd a reduced representation for \u03b8s. We build a training set containing student weights at di\ufb00erent training stages and then \ufb01t PCA to this dataset. This dataset is obtained by saving the weights of the student network for di\ufb00erent tasks at di\ufb00erent learning stages. At the end of each student training iteration, we do dimensionality reduction to the \ufb01rst n principal components and use this reduced representation as an input for our teacher. There are other ways to get a reduced representation for \u03b8s like model distillation or auto-encoders, each with its downsides. We leave the question for an optimal way to bring \u03b8s into a meaningful representation to future work. In this work, we refer to this observation type as pca input (PCA). In our work we also experiment with partial observable curriculum MDPs. We work with six di\ufb00erent types of manual de\ufb01ned observation types: reward history (RH), previous task reward (PTR), learning progress (LP), absolute learning progress (ALP), exponential moving average (EMA), and fast and slow exponential moving average (FS-EMA). 27 3.2. Teacher Curriculum Markov Decision Process Reward History : Instead of using the students weights \u03b8s we try to represent the learning potential per task. We can use a tuple of the student\u2019s average reward, obtained in the evaluation cycle at the end of a CMDP step, on each task together with the time-step when this task was last sampled: RH = {(rm eval, tm last sampled)|m \u2208 V} (3.1) Where rm eval is the average reward on all evaluation episodes for task m and tm last sampled is the CMDP time-step when task m last has been sampled. If the task m never has been sampled tm last sampled equals to 0. Previous Task Reward : Instead of using the average rewards of all tasks, we can input the last selected task, one-hot encoded, together with the average reward on that task, obtained in the evaluation cycle at the end of a CMDP step. Learning Progress : The learning progress is de\ufb01ned as the di\ufb00erence between the average reward, obtained in the evaluation cycle at the end of a CMDP step, of the current and the previous time step per task: LP = {(rm t \u2212 rm t\u22121)|m \u2208 V} (3.2) Where rm t is the average reward on the task m at time-step t and rm t\u22121 is the average reward on task m at the previous time-step t \u2212 1. Absolute Learning Progress : ALP is simply the absolute value of LP: ALP = {(|rm t \u2212 rm t\u22121|)|m \u2208 V} (3.3) The intuition behind using the absolute value is that a task at the stage of forgetting, resulting in a negative LP, should be treated similarly to a task with a steep learning curve. This learning progress representation is inspired by Portelas et al. [2019]. Exponential Moving Average : We can interpret the history of evaluation rewards after every CMDP cycle as a time-series. We use an exponential moving average over the history of rewards to estimate the next reward. Depending on the \u03b1 \u2208 [0, 1] value used to calculate the EMA we assign more weight on recent samples than on old ones. The exponential moving average over a vector x is de\ufb01ned as: ema(xt) = \ufffd \u03b1 \u2217 xt + (1 \u2212 \u03b1)ema(xt\u22121), t > 1 x1, t = 1 (3.4) Where ema(xt) is the value of the EMA at any time period t and xt is the value at a time period t. Therefore the EMA input is: EMA = {[ema(xm t )]|m \u2208 V} (3.5) 28 3.2. Teacher Curriculum Markov Decision Process xm is the history of the average reward, obtained in the evaluation cycle at the end of a CMDP step, for task m at all CMDP time-steps. The last time step is denoted as t. Fast and Slow Exponential Moving Average : Kanitscheider et al. [2021] introduce a smoothed EMA version, where they calculate a fast and a slow EMA with a high \u03b1 and a low \u03b1 respectively. The fast and slow EMA is then de\ufb01ned as the di\ufb00erence between the two EMAs: FS-EMA = {[emafast(xm t ) \u2212 emaslow(xm t )]|m \u2208 V} (3.6) In our work, we wanted to test if this method is superior to a normal EMA with a tuned \u03b1 value. 3.2.2 Reward Signal In reinforcement learning, a meaningful reward signal is crucial for success. We need to de\ufb01ne a reward signal for our CMDP that encodes our intention and is rich enough for the teacher agent to learn fast. The goal of the teacher agent is to perform task sequencing such that the asymptotic reward on the target task increases. Therefore we can use the student\u2019s average reward on the target task after a CMDP step as a reward signal. We call this reward signal target task reward. Typically, the target task is hard to solve. Therefore the reward at the beginning of training is usually 0. This reward signal is not meaningful for the teacher as it does not tell if the student is making progress in easier environments or if the student is completely lost. Such a sparse reward signal leads to less sample e\ufb03ciency and as samples in the CMDP are extremely costly, we want our teacher to learn fast. We assume that the source tasks in V are related to the target task. Therefore, if our student achieves a reward on the source tasks, this is a step towards solving the target task. If this is true, then we can overcome the sparse reward signal by de\ufb01ning a new reward signal source task reward: rteacher = \ufffd m\u2208V rm (3.7) Where rm is the average reward on task m obtained in the evaluation cycle at the end of a CMDP step. 29 3.2. Teacher Curriculum Markov Decision Process 3.2.3 Action The action space in the CMDP contains all tasks in V. Taking an action equals to selecting a task m \u2208 V, changing the student\u2019s environment to the selected task, and then train the student for a prede\ufb01ned amount of steps. After the training, the student is evaluated on all tasks in V and the evaluation results are passed to the CMDP agent. The evaluation is carried out according to the section 3.4. In our experiment, we train the student for 100\u2032000 steps. One could also train the student until convergence, surpassing a threshold, or make the number of steps part of the teacher\u2019s action space. It is unclear when the student will converge, which could lead to very compute-intensive CMDP steps, and if the agent can surpass the threshold, which could lead to an in\ufb01nite CMDP step. Therefore, we do not consider these two approaches. We want to keep the teacher\u2019s action space as simple as possible. Therefore we leaf integrating the number of student training steps into the action space for future work. 3.2.4 Network Architecture We keep the teacher network architecture simple. We use a multilayer perceptron (MLP) with three hidden layers with 64, 128, and 64 nodes, respectively, and a ReLU [Agarap, 2018] activation function. We make use of weight sharing as we do it for our student agent. The policy and value function are represented as individual heads on top of the MLP feature embedding. We also experiment with LSTMs [Hochreiter and Schmidhuber, 1997] in order to provide the teacher a memory. In this setting, we use the same MLP for the feature embedding and then feed the feature vector into an LSTM with a hidden state of 128. The hidden state is then used as an input for the policy and value function heads. Due to computation limits, we were not able to tune the network architecture. Another network architecture likely yields better results. 30 3.3. Baselines 3.3 Baselines We compare our teacher task sequencing approach against four baselines: uniform, LP sampling, window and thompson sampling. The last two are introduced in Matiisen et al. [2019]. Uniform : This is the simplest baseline. We select the next task by uniformly sampling them. LP sampling : In this baseline, we select the task with the highest learning progress as de\ufb01ned in eq. (3.2). If two or more tasks have the same LP, we sampling one of those tasks uniformly. Thompson sampling : Similar to LP sampling we sample the next task according to a learning progress metric. In the Thompson sampling baseline, we sample the task with the highest average reward after the last CMDP step. Window : In the window algorithm, we approximate the learning progress with linear regression. We store the history of average evaluation rewards. At each CMDP step, we \ufb01t a linear regression to the reward history. The regression coe\ufb03cient per task is used to estimate the steepness of the learning curve. The task with the steepest learning curve is selected. 3.4 Evaluation Protocol The goal of the teacher task is to improve the asymptotic performance. We evaluate our approach using a weak transfer to allow comparison with the results on the Google Research Football environment from our previous work [Schraner, 2020]. The asymptotic performance is de\ufb01ned as the increase in the average reward at the end of training. Additionally, we evaluate how well the trained student agent generalizes across the environments. The agent is trained overall environments, and it is interesting how well the agent performs on those environments. To measure the generality, we calculate the total average return, which is the sum of all average returns for all tasks in V, and the percentage of environments solved. We declare an environment as solved when the average return is greater than zero. After every CMDP step, we evaluate the student on every task in V for 100 episodes. Chapter 4 Experiments and Results In this chapter we describe the experiments used to evaluate our methods. We begin with an introduction of the environments used, the experimental setup and then provide results of our baselines and teacher student experiments. In depth experiments are carried out on a grid world environment, the most promising settings are then evaluated on the Google Research Football environment. In the following chapters, we use the terms task and environment interchangeably. In our case, the environment is equal to the task to solve, e.g., there is only a single task per environment. The description of the Google Research Football environment is taken from our previous work [Schraner, 2020]. 4.1 Grid World Grid world is a simple, lightweight, and fast environment for reinforcement learning. In a Grid world environment, the agent must reach a target destination by navigating through a maze. The di\ufb03culty of the environment ranges from very simple empty grids to complex mazes where the agent has to \ufb01nd keys in a speci\ufb01c order to unlock the target destination. We use the minimalistic grid world (MiniGrid) implementation by Chevalier-Boisvert et al. [2018]. In MiniGrid, the world is an NxN grid of tiles. Each tile in the grid world contains zero or one object, and each object has an associated discrete color and type. The object types are wall, \ufb02oor, door, key, ball, and goal. Doors have a state open, closed, or locked and behave according to this state. To open a locked door, the agent has to carry a key matching the door\u2019s color. The agent can pick up and carry exactly one object (e.g., ball or key). The simplicity of the grid world environment allows fast iteration and testing of multiple ideas. Training a student agent in the CMDP setting in the grid world environment takes around 18 hours. Training a teacher in the proposed CMDP setting is computationally expensive as we have to train a student agent. The set of environments with their di\ufb00erent levels of complexity is helpful for the curriculum learning setting. Depending on the selected environment, the maximum number of steps changes. An epoch in the simplest environment, Empty-5x5, ends after 100 steps, the most di\ufb03cult environment, KeyCorridor-S6R3, terminates after 1080 steps. 31 32 4.1. Grid World 4.1.1 Grid World Environments We describe the MiniGrid environments in table 4.1 and illustrate them in \ufb01g. 4.1. In a grid world environment, the agent has to navigate through a maze and solve some puzzles to reach the green goal square. In the KeyCorridor environment, the agent has to pick up a hidden ball behind a locked door. To unlock that door, the agent must \ufb01nd the matching key, which is hidden in another room. The agent has to explore the environment and move through open doors to \ufb01nd the hidden key. The bottom row of \ufb01g. 4.1 displays two di\ufb00erent KeyCorridor environments. Figure 4.1: Visualization of a subset of the MiniGrid environments used in this work. The environment names from top left to bottom right: Empty-6x6, FourRooms, DoorKey-16x16, MultiRoom-N4S5, KeyCorridor-S3R2, KeyCorridorS3R3, and KeyCorridor-S6R3. 4.1.2 State & Observations MiniGrid supports a variety of observation types. In our work, we use a fully observable view of the environment. This view has a dimension of N \u00d7 N \u00d7 3, where N is the dimension of the grid world. These values are not pixels, the last channel is encoding the tile at the (X, Y) location. The tile encoding is a threedimensional tuple: (OBJECT INDEX, COLOR INDEX, STATE). Only doors and agents have a state value other than 0. The door state 0 represents an open, 1 a closed, and 2 a locked door. The agents\u2019 state indicates the direction of the agent. We are transferring the policy and value function weights from environment to environment. Therefore the input dimension must stay the same for every environment. We apply padding to have a 25 \u00d7 25 grid world independent of the selected environment. As padding values, we use (1, 0, 0), which is equivalent to a wall. 33 4.1. Grid World Name Description Empty-[X]x[X] This environment is an empty room. Upon reaching the green goal square, the student receives a sparse reward. The agent is starting in a random position. The value of X de\ufb01nes the grid size. We use a 5 by 5, 6 by 6, 8 by 8, and 16 by 16 grid. FourRooms The agent must navigate in a maze of four rooms. Four gaps in the walls connect the rooms. To obtain a reward, the agent must reach the green goal square. Both the agent and the goal square are placed randomly in any of the four rooms. DoorKey-[X]x[X] The agent must pick up a key in order to unlock a door and then reach the green goal square. Due to the sparse reward signal, this environment is di\ufb03cult to solve. The door, wall, agent and green goal square are placed randomly. The value of X de\ufb01nes the grid size. We use a 5 by 5, 6 by 6, 8 by 8, and 16 by 16 grid. MultiRoom-N[X]S[Y] The environment has a series of connected rooms with doors that must be opened in order to get to the next room. The \ufb01nal green goal square is located in the last room. The rooms are all created randomly. This environment is challenging to solve using RL alone. The value of X de\ufb01nes the number of rooms and X the room size. We use N2-S4, N4-S5, and N6-S10 for our experiments. KeyCorridorS[X]R[Y] The agent has to pick up an object which is behind a locked door. The key is hidden in another room, and the agent has to explore the environment to \ufb01nd it. The key, ball, and agent are placed randomly. Due to the exploration required this is a challenging environment. The value of X de\ufb01nes the room size and Y number of rows (see \ufb01g. 4.1). We use an S3R1, S3R2, S3R3, S4R3, S5R3, S6R3 grid. Table 4.1: Description of the MiniGrid environments used in this thesis. All environments impose a penalty for the number of steps taken until reaching the target. 34 4.1. Grid World 4.1.3 Actions The action space in MiniGrid consists of six actions: Turn left, turn right, move forward, pickup, drop and toggle (open door, interact with objects). 4.1.4 Rewards The agent receives a reward of 1 for reaching the goal square and 0 otherwise. A penalty for the number of steps required to reach the target location is imposed. If the agent takes more steps to reach the target, the reward decreases. The grid world reward upon success is calculated according to this equation: r = 1 \u2212 0.9 \u2217 (step count/max steps) (4.1) Where step count is equal to the number of steps taken to reach the goal square and max steps is the maximum number of steps allowed per episode. In \ufb01g. 4.2 we ilustrated the reward calculation for two di\ufb00erent trajectories in the DoorKey-8x8 environment. Figure 4.2: A blue line illustrates the agent\u2019s trajectory in the DoorKey-8x8 environment. The maximum number of steps in the DoorKey-8x8 environment is 640. If the agent takes the direct path, he takes 18 steps until he reaches the green goal square. Therefore the agents reward is 1\u22120.9\u2217(18/640) = 0.9747. The agent on the right takes 28 steps, leaving him with a reward of 1\u22120.9\u2217(28/640) = 0.9606. 4.1.5 MDP Statement Depending on the type of observation used, the grid world environment does not ful\ufb01ll the Markov property. Some observation types only provide a limited \ufb01eld of view. To ease the problem, we only use a fully observable input for all of our student agents. Each observation at every time step fully describes the environment\u2019s state space, and the following environment state is entirely dependent on the current state. 35 4.2. Grid World Experiments 4.2 Grid World Experiments 4.2.1 Experimental Setup In this section, we detail the experimental setup applied to all our grid world experiments. Network architecture Our student agent uses a fully connected network with ReLU activation functions and separate policy and value function heads. The network architecture is depicted in \ufb01g. 4.3. Figure 4.3: Student neural network architecture for the MiniGrid environments. The input is a 25\u00d725\u00d73 fully observable representation of the environments state. After every fully connected layer we use a ReLU activation function. The policy head uses a softmax activation function for the action probability distribution. The value head does not use an activation function. We also evaluated a CNN network architecture. In the appendix A.1 we provide details about the CNN architecture. The 25 \u00d7 25 \u00d7 3 grid world observation has location-dependent features. Therefore by intuition we expect that CNN models work better than MLP models. Flattening the observation into a vector makes it harder to discover patterns and generalize over the state space. As we can see in \ufb01g. 4.4, using a CNN network architecture instead of the MLP architecture improves results on harder environments. MLP models are less noisy than CNN models, especially in the case of DoorKey experiments. We value stability over best possible results in our thesis because reward changes in our CMDP setting also lead to a lot of noise, therefore we use MLP models for the rest of our experiments. 36 4.2. Grid World Experiments Empty-5x5 Empty-6x6 Empty-8x8 Empty-16x16 FourRooms DoorKey-5x5 DoorKey-6x6 DoorKey-8x8 KeyCorridorS3R1 MultiRoom-N2-S4 Environment 0.0 0.2 0.4 0.6 0.8 1.0 Return Average CNN vs MLP model comparison Model MLP CNN Figure 4.4: Comparison of the average return over 100 episodes at the end of training between MLP and CNN models. The agent is trained with PPO for 10 million steps on a single environment. No Curriculum Experiments For all MiniGrid environments, we trained an agent with PPO for 10 million steps and evaluated the agent for 100 episodes at the end of training. We used these experiments to tune hyperparameters as well as the network architecture. In appendix A.2 we report the hyperparameters. Additionally, we used the results to select a subset of the MiniGrid environments for our curriculum learning experiments. We removed the group of MiniGrid environments where our trained agents failed in solving the environment. In the case of KeyCorridor and MultiRoom, training an agent for the simplest version of those environments succeeded. Therefore we kept all KeyCorridor and MultiRoom environments in our task set V. All experiments were repeated three times under di\ufb00erent random seeds. In our results, we report maximum and standard deviations. Curriculum learning For both the baseline and the CMDP experiments, we perform 1\u2032000 teacher steps, where for each teacher step, the student is trained for 10\u2032000 steps in the selected environment. Therefore, the student agent is trained for 10 million steps in total. After each teacher step, the student is evaluated for 100 episodes in each task in V. We \ufb01xed those number of steps to have the same 37 4.2. Grid World Experiments training steps as we used in our no curriculum learning experiments. We had to balance the number of teacher updates and the number of student updates per CMDP step. Using fewer teacher steps might not be su\ufb03cient for the teacher to learn how to perform task sequencing but would allow our student more time to converge in the selected environment. Using more teacher steps is bene\ufb01cial for the teacher, but on the other hand, the student has less time to learn the selected task. This would provide the teacher with a noisy reward signal. We selected the teacher and student steps because they seem reasonable. In future work, the in\ufb02uence of those values should be evaluated. The network architecture for the teacher agent is described in section 3.2.4. We evaluated the MLP and LSTM network architecture in appendix A.5. The MLP architecture is superior to the LSTM architecture. The hyperparameters for the baselines as well as the CMDP teachers are described in appendix A.2 and appendix A.4. All experiments were repeated three times under di\ufb00erent random seeds. In our results, we report maximum and standard deviations. 4.2.2 Results Overview Table 4.2 shows the best agent\u2019s performance on each MiniGrid environment used in this thesis. For each environment, we report the average return of 100 evaluation episodes. The total mean return is de\ufb01ned as the sum over all average returns. The percentage of environments solved is de\ufb01ned as the number of environments with an average reward greater than zero divided by all environments. Overall, using no curriculum results in a higher average return on 8 out of the 19 tested tasks. In contrast, curriculum learning agents outperform standard RL agents in more complex tasks such as Empty-16x16, DoorKey16x16, and KeyCorridor-S6R3. Comparing the best CMDP agent to the best baseline agents we see, the CMDP agent outperforms the baselines regarding the total mean return and percentage of solved environments. In the curriculum learning approach, the agent spends less training time in a single environment than with no curriculum. In easy environments, using no curriculum learning and training on a single task yields better results than the curriculum learning experiments. Training for 10 Million steps in a single environment is better than learning in a curriculum without speci\ufb01cally targeting those easy environments. In the CMDP setting using the source task reward, the teacher is encouraged to maximize the student\u2019s average return over all environments and not a single environment, as is the case when using no curriculum. The following sections evaluate di\ufb00erent teacher reward signals, transfer methods, observation types, hyperparameters as well as the sample e\ufb03ciency and generality of trained agents. 38 4.2. Grid World Experiments Environment / Metric None Uniform LP Thompson Window CMDP Total mean return 1.75 2.71 2.83 2.27 4.44 % environments solved 50% 44% 39% 50% 33% 55% Empty-5x5 0.96 0.83 0.75 0.57 0.37 0.93 Empty-6x6 0.97 0.51 0.83 0.51 0.32 0.9 Empty-8x8 0.96 0.0 0.0 0.62 0.0 0.93 Empty-16x16 0.0 0.0 0.93 0.92 0.67 0.82 FourRooms 0.17 0.07 0.09 0.09 0.0 0.09 DoorKey-5x5 0.96 0.17 0.02 0.02 0.0 0.13 DoorKey-6x6 0.94 0.04 0.0 0.04 0.81 0.13 DoorKey-8x8 0.29 0.07 0.0 0.05 0.0 0.14 DoorKey-16x16 0.0 0.03 0.04 0.0 0.07 0.17 MultiRoom-N2-S4 0.14 0.02 0.04 0.02 0.03 0.15 MultiRoom-N4-S5 0.0 0.0 0.0 0.0 0.0 0.0 MultiRoom-N6-S10 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S3R1 0.07 0.0 0.0 0.0 0.0 0.05 KeyCorridor-S3R2 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S3R3 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S4R3 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S5R3 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S6R3 0.0 0.0 0.0 0.0 0.0 0.01 Table 4.2: Highest average return over 100 episodes for each teacher type at the end of 1000 teacher steps. The None results are the average return over 100 episodes obtained when training only the environment of that row. ",
    "Method": "Method In this section, we evaluate the e\ufb00ect of the transfer methods introduced in section 3.1.1. We evaluate policy transfer, reward shaping transfer, and both transfers combined. All teacher agents use the source task reward, as de\ufb01ned in section 3.2.2. The results for all seven teacher observation types are reported in table 4.3 and the four baselines in table 4.4. Figure 4.5 shows the learning curves of the reinforcement learning agents. All agents with policy transfer surpass agents with reward shaping or the combined knowledge transfer. The learning curve is noisy in policy transfer due to the environment changes, but the performance is steadily increasing. The performance of agents with policy transfer did not converge after 1000 teacher steps. For both reward transfer and the combined transfer, the performance converged during the \ufb01rst 50 to 100 CMDP steps and failed to improve from then on. The agent\u2019s performance with reward transfer scatters around a total mean return of 1 during the whole training. Figure 4.5: Learning curves for di\ufb00erent teacher observations and transfer methods. After each teacher step the sum of the students average return over 100 episodes for each environment in V is plotted. We plot the learning curve with the highest total reward at the end of training for every con\ufb01guration. Approximating the entire student state space by encoding the students\u2019 weights with PCA as described in section 3.2.1 fails. The results of the experiments with PCA observation space are very similar to the uniform baseline results. We, therefore, discard this method for the rest of our experiments. The best results are obtained using the PTR, EMA, and LP observations with a policy transfer. Using the fast-slow EMA as described in section 3.2.1 is not superior to standard EMA. In section 4.2.5 we evaluate the best \u03b1 value for the 40 4.2. Grid World Experiments EMA observation. Using the reward history (RH) is signi\ufb01cantly worse than the other observation types when using policy transfer. RH resulted in the highest total mean return when using reward transfer but su\ufb00ers from a high standard deviation. In general, the results with policy transfer are more stable compared to the results with reward transfer. Environment / Metric RH PTR LP ALP EMA FS-EMA PCA Policy Total mean return 2.34 \u00b1 0.24 4.44 \u00b1 0.33 4.17 \u00b1 0.33 3.35 \u00b1 0.33 4.35 \u00b1 0.42 3.72 \u00b1 0.22 1.64 \u00b1 0.24 % environments solved 55% 55% 55% 55% 55% 61% 44% Empty-16x16 0.58 \u00b1 0.27 0.07 \u00b1 0.03 0.9 \u00b1 0.27 0.08 \u00b1 0.03 0.82 \u00b1 0.34 0.91 \u00b1 0.38 0.0 \u00b1 0.0 FourRooms 0.11 \u00b1 0.03 0.13 \u00b1 0.03 0.11 \u00b1 0.02 0.1 \u00b1 0.04 0.08 \u00b1 0.01 0.06 \u00b1 0.03 0.08 \u00b1 0.02 DoorKey-16x16 0.05 \u00b1 0.01 0.04 \u00b1 0.02 0.07 \u00b1 0.03 0.13 \u00b1 0.05 0.17 \u00b1 0.06 0.06 \u00b1 0.03 0.03 \u00b1 0.01 MultiRoom-N2-S4 0.1 \u00b1 0.02 0.06 \u00b1 0.02 0.09 \u00b1 0.01 0.06 \u00b1 0.03 0.15 \u00b1 0.05 0.09 \u00b1 0.04 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Reward Total mean return 1.38 \u00b1 0.85 0.68 \u00b1 0.3 1.01 \u00b1 0.32 1.11 \u00b1 0.63 0.95 \u00b1 0.1 1.22 \u00b1 0.58 0.63 \u00b1 0.21 % environments solved 50% 44% 61% 50% 11% 50% 34% Empty-16x16 0.56 \u00b1 0.28 0.0 \u00b1 0.0 0.46 \u00b1 0.23 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 FourRooms 0.04 \u00b1 0.002 0.03 \u00b1 0.01 0.05 \u00b1 0.03 0.03 \u00b1 0.01 0.0 \u00b1 0.0 0.03 \u00b1 0.01 0.0 \u00b1 0.0 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.03 \u00b1 0.01 0.02 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.06 \u00b1 0.03 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.02 \u00b1 0.001 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.003 \u00b1 0.001 0.003 \u00b1 0.002 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.004 \u00b1 0.001 0.0 \u00b1 0.0 Both Total mean return 0.0 \u00b1 0.0 0.47 \u00b1 0.24 0.0 \u00b1 0.0 0.32 \u00b1 0.16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 % environments solved 0% 17% 0% 17% 0% 0% 0% Empty-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 FourRooms 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.02 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Table 4.3: We compare the di\ufb00erent knowledge transfer methods for each teacher observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return over 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. Agents with the combined knowledge transfer are failing to learn at all. In \ufb01g. 4.5 we see that the average return drops from around 0.7 to 0 during the \ufb01rst ten teacher steps and fails to recover from there. We visualized the synthetic reward signal added by the reward transfer in \ufb01g. 4.6 for the Empty-8x8 and Empty16x16 environment. Each square in the image equals a square in the grid world environment. The black squares are wall objects, and the element in the bottom right is the green goal square. High state values estimated by the value function obtained in experiments with only reward transfer (images on the left) are scattered randomly around the grid. The estimated state value is between 0.12 and 0.16. In the value function with a combined knowledge transfer, the state values are almost identical for all states and have an estimated value of 10 Million. We 41 4.2. Grid World Experiments believe that the agent starts to optimize and estimate its previous value function and this leads to an ever increasing value estimate. This extremely high additional reward overshadows the actual reward signal of the environment, making it impossible for the agent to learn how to solve the environment successfully. Empty-8x8 - Reward transfer 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 Empty-8x8 - Both transfers 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e7 Empty-16x16 - Reward transfer 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Empty-16x16 - Both transfers 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 1e7 Figure 4.6: We visualize the added reward, when using reward transfer (left column) and combined transfer (right column). For every state in the Empty8x8 (top row) and Empty-16x16 (bottom row) grid world environment we plot the added reward. A light color encodes a high added reward, black equals to zero added reward. Dark tiles at the end of the grid are walls, the black tile in the bottom right corner is the green goal square. In \ufb01g. 4.7 we plot the teacher\u2019s sampling probability distribution over the teacher timesteps next to the teacher\u2019s learning curve. During the \ufb01rst 100 episodes, the teacher samples tasks similar to a uniform distribution. The teacher favors the family of Empty grid world environments during the \ufb01rst 200 episodes. From then on, the teacher selects more challenging environments such as KeyCorridor and DoorKey environments. After roughly 400 teacher steps, the sample distribution does not show the noisy changes in the sample probabilities as at the beginning of the training. We interpret this as the teacher getting more con\ufb01dent in performing task sequencing. The teacher\u2019s learning curve is noisy but steadily increasing. There are some drastic drops in performance, probably due to selecting too challenging environments. The student always manages to recover after those harmful teacher actions. Table 4.4 shows the same properties as described above. The Thompson sam42 4.2. Grid World Experiments Figure 4.7: We visualize the sample probability distribution and learning curve of one teacher agent over its training time. The colors in the legends are ordered upside down to the order in the plot. pling experiment with reward transfer is interesting. This is the only experiment where the knowledge transfer with a reward signal surpassed agents with a policy transfer. Therefore, this approach to transfer learning is feasible but very noisy, which is also indicated by the high standard deviation for the Thompson sampling experiments with reward transfer. While uniformly sampling environments is a strong baseline, it still shows the worst performance across the baselines. In general, sampling environments with the highest learning progress estimate yields the strongest baseline results. 43 4.2. Grid World Experiments Environment / Metric Uniform LP Thompson Window Policy Total mean return 1.75 \u00b1 0.24 2.71 \u00b1 0.33 1.94 \u00b1 0.21 2.28 \u00b1 0.32 % environments solved 44% 39% 22% 34% Empty-16x16 0.0 \u00b1 0.0 0.92 \u00b1 0.23 0.0 \u00b1 0.0 0.67 \u00b1 0.14 FourRooms 0.07 \u00b1 0.02 0.09 \u00b1 0.02 0.0 \u00b1 0.0 0.0 \u00b1 0.0 DoorKey-16x16 0.03 \u00b1 0.01 0.04 \u00b1 0.01 0.0 \u00b1 0.1 0.07 \u00b1 0.03 MultiRoom-N2-S4 0.02 \u00b1 0.01 0.04 \u00b1 0.01 0.03 \u00b1 0.01 0.03 \u00b1 0.01 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Reward Total mean return 0.8 \u00b1 0.34 0.51 \u00b1 0.23 2.83 \u00b1 1.1 0.45 \u00b1 0.12 % environments solved 44% 11% 50% 22% Empty-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.92 \u00b1 0.36 0.0 \u00b1 0.0 FourRooms 0.03 \u00b1 0.01 0.0 \u00b1 0.0 0.09 \u00b1 0.05 0.0 \u00b1 0.0 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.06 \u00b1 0.02 0.0 \u00b1 0.0 0.02 \u00b1 0.01 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.01 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Both Total mean return 0.47 \u00b1 0.21 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.52 \u00b1 0.22 % environments solved 17% 0% 0% 17% Empty-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 FourRooms 0.05 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.02 \u00b1 0.01 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Table 4.4: We compare the di\ufb00erent knowledge transfer methods for each baseline. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum the average students return of 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. 44 4.2. Grid World Experiments 4.2.4 Teacher Reward Signal In this section we evaluate the source task reward and the target reward. Both reward signals are de\ufb01ned in section 3.2.2. We use the KeyCorridor-S3R3 environment as the target task. The agents use a policy knowledge transfer between tasks. In table 4.5 we report experiments for both reward signals with six observation types. For all observation types, the reported results are better when using the source task reward except for the RH experiments. The reported total mean return has a lower standard deviation over three di\ufb00erent random seeds when using the source task reward instead of the target reward. Environment / Metric RH PTR LP ALP EMA FS-EMA Total Eval. Reward Total mean return 2.34 \u00b1 0.24 4.44 \u00b1 0.33 4.17 \u00b1 0.33 3.35 \u00b1 0.33 4.35 \u00b1 0.42 3.72 \u00b1 0.22 % environments solved 55% 55% 55% 55% 55% 61% Empty-16x16 0.58 \u00b1 0.27 0.07 \u00b1 0.03 0.9 \u00b1 0.27 0.08 \u00b1 0.03 0.82 \u00b1 0.34 0.91 \u00b1 0.38 FourRooms 0.11 \u00b1 0.03 0.13 \u00b1 0.03 0.11 \u00b1 0.02 0.1 \u00b1 0.04 0.08 \u00b1 0.01 0.06 \u00b1 0.03 DoorKey-16x16 0.05 \u00b1 0.01 0.04 \u00b1 0.02 0.07 \u00b1 0.03 0.13 \u00b1 0.05 0.17 \u00b1 0.06 0.06 \u00b1 0.03 MultiRoom-N2-S4 0.1 \u00b1 0.02 0.06 \u00b1 0.02 0.09 \u00b1 0.01 0.06 \u00b1 0.03 0.15 \u00b1 0.05 0.09 \u00b1 0.04 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Target Reward Total mean return 3.6 \u00b1 0.49 3.6 \u00b1 0.36 3.38 \u00b1 0.62 2.35 \u00b1 0.66 3.41 \u00b1 0.51 3.38 \u00b1 0.42 % environments solved 55% 55% 55% 55% 55% 61% Empty-16x16 0.58 \u00b1 0.27 0.07 \u00b1 0.03 0.24 \u00b1 0.08 0.5 \u00b1 0.1 0.72 \u00b1 0.21 0.63 \u00b1 0.08 FourRooms 0.11 \u00b1 0.03 0.13 \u00b1 0.03 0.11 \u00b1 0.04 0.1 \u00b1 0.04 0.05 \u00b1 0.01 0.06 \u00b1 0.02 DoorKey-16x16 0.05 \u00b1 0.004 0.02 \u00b1 0.01 0.0 \u00b1 0.0 0.08 \u00b1 0.02 0.09 \u00b1 0.03 0.04 \u00b1 0.01 MultiRoom-N2-S4 0.07 \u00b1 0.01 0.11 \u00b1 0.04 0.01 \u00b1 0.003 0.06 \u00b1 0.03 0.02 \u00b1 0.01 0.02 \u00b1 0.01 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Table 4.5: We compare the di\ufb00erent teacher reward signals for each teacher observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return over 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. In \ufb01g. 4.8 we plot the target reward signal along the steps in the CMDP. The reward signal for the EMA, FS-EMA, LP, and ALP experiments are very close to zero during training, therefore not providing the teacher agent with a helpful signal. For RH and PTR, the reward for the \ufb01rst 200 teacher steps scatters around 0.01 and 0.8 and then degenerates to the same reward signal as for the other observation types. Surprisingly, this reward signal provided enough information to the teacher to better perform task sequencing than the baselines. The target reward pushes the teacher agent towards solving the KeyCorridor45 4.2. Grid World Experiments S3R3 environment, but at the end of training the agent fails to solve this environment with both reward signals. With both approaches, we did not manage to solve the challenging KeyCorridor-S3R3 environment. Figure 4.8: Visualization of the target reward signal for each teacher observation type over the teachers training cycle. 4.2.5 Choosing Alpha for Exponential Moving Average In this section, we optimize the \u03b1 value for the exponential moving average. An \u03b1 value close to one favors current values over old values in a time series. We experimented with \u03b1 \u2208 [0.1, 0.3, 0.5, 0.7, 0.9] and reported the results in table 4.6. The teacher agents use a policy transfer and the source task reward. By looking at the results in table 4.6 it is hard to settle for an \u03b1 value. While an \u03b1 of 0.5 or 0.9 have the highest total mean returns, the results for challenging environments are the best when selecting an alpha value of 0.3. The experiments with a value of 0.7 and 0.9 su\ufb00er from a high standard deviation in the reported total mean return. In \ufb01g. 4.9 we plot the average learning curve over three di\ufb00erent random seeds for all \u03b1 values. An \u03b1 value of 0.1 yields the worst results. The learning curve of a value with 0.3 is below higher \u03b1 values but surpassed most of the other experiments in the last 20 CMDP steps. The learning curves for an \u03b1 value of 0.5, 0.7, and 0.9 follow each other closely. Our experiments do not allow us to select a clear winner. Because of the highest total mean return combined with a relatively small standard deviation, we propose to use an alpha value of 0.5. 46 4.2. Grid World Experiments Environment / Metric 0.1 0.3 0.5 0.7 0.9 Total mean return 3.49 \u00b1 0.43 3.01 \u00b1 0.39 4.35 \u00b1 0.42 3.89 \u00b1 0.58 4.23 \u00b1 0.95 % environments solved 50% 61% 55% 55% 61% Empty-16x16 0.32 \u00b1 0.06 0.58 \u00b1 0.27 0.82 \u00b1 0.34 0.97 \u00b1 0.4 0.94 \u00b1 0.28 FourRooms 0.08 \u00b1 0.01 0.13 \u00b1 0.03 0.08 \u00b1 0.01 0.09 \u00b1 0.04 0.06 \u00b1 0.01 DoorKey-16x16 0.0 \u00b1 0.0 0.05 \u00b1 0.02 0.17 \u00b1 0.06 0.0 \u00b1 0.0 0.02 \u00b1 0.01 MultiRoom-N2-S4 0.15 \u00b1 0.06 0.08 \u00b1 0.03 0.15 \u00b1 0.05 0.13 \u00b1 0.02 0.29 \u00b1 0.12 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.01 \u00b1 0.004 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.004 \u00b1 0.002 Table 4.6: We compare the di\ufb00erent \u03b1 values for the EMA observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return of 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. Figure 4.9: The average learning curve of three runs with three di\ufb00erent random seeds for each \u03b1 value. 47 4.2. Grid World Experiments 4.2.6 Sample E\ufb03ciency In this section, we analyze the sample e\ufb03ciency of an agent when trained in the teacher-student setting compared to trained directly on the target task. Learning curves compare the agent\u2019s experience with their performance. We use the teacher step on the x-axis to measure the experience. One teacher step equals 10\u2019000 steps in an environment. The performance is measured by the average return on 100 evaluation episodes during the training. In \ufb01g. 4.10 we plot three learning curves, one for each of the following environments: Empty-16x16, DoorKey16x16, and FourRooms. The sample e\ufb03ciency of an agent can be measured as the area under the curve with transfer minus the area under the curve without transfer. We do the evaluation manually by looking at the curves. In the Empty16x16 environment, the sample e\ufb03ciency of the agents with curriculum learning is negative for the \ufb01rst 400 teacher steps. After 400 teacher steps, the agent trained directly in the Empty16x16 environment su\ufb00ers from a catastrophic update and fails to recover from then on. It seems like the agent got stuck in a bad local optimum. This is probably due to a bad choice in the hyperparameters. We note that agents trained directly on the Empty16x16 environment with three random seeds showed the same behavior. In the DoorKey16x16 environment, all curriculum learning agents have a positive sample e\ufb03ciency. The learning curve is very noisy due to the training in di\ufb00erent environments. Although we notice an improvement in the sample e\ufb03ciency, the agent\u2019s performance is not monotonically increasing and dropping to the same performance level as when trained directly in the DoorKey16x16 environment. One could perform early stopping when crossing a reward threshold to solve this issue, but de\ufb01ning such a threshold is not straightforward. There is no improvement in the sample e\ufb03ciency in the FourRooms environment. Curriculum learning with the PTR teacher shows the best results with a slight improvement in sample e\ufb03ciency during some learning stages. 48 4.2. Grid World Experiments 0 200 400 600 800 1000 Teacher Step 0.0 0.2 0.4 0.6 0.8 1.0 Average Return Learning Curve - Empty-16x16 RH PTR LP ALP EMA FS-EMA None 0 200 400 600 800 1000 Teacher Step 0.0 0.1 0.2 0.3 0.4 0.5 Average Return Learning Curve - DoorKey-16x16 RH PTR LP ALP EMA FS-EMA None 0 200 400 600 800 1000 Teacher Step 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Average Return Learning Curve - FourRooms RH PTR LP ALP EMA FS-EMA None Figure 4.10: We visualize the learning curves of our agent when trained in the teacher-student setup and directly on the target task for the Empty-16x16, DoorKey16x16, and FourRooms environment. We use a teacher with policy transfer, source task reward signal, and the six partially observable inputs. The learning curve of the agent trained directly on the task is labeled with \u201dNone\u201d. 49 4.2. Grid World Experiments 4.2.7 Generality of Agents We compare the generality of agents trained with curriculum learning with agents trained in a single environment. To measure the generality, we evaluate the agent at the end of training on \ufb01ve di\ufb00erent random seeds for 100 episodes on each environment in the task-set V. After the evaluation, we calculate the total mean return and the percentage of environments solved with \ufb01ve random seeds and report the median of both measurements in table 4.7. Agents trained with curriculum learning are more general than agents trained in a single environment. With every teacher, we could solve more environments than training in one environment. The agent trained solely in Empty-6x6 scored a total mean return of 3.25. Most of the obtained reward is received in the easy Empty environment family. Some environments are especially useful to generalize to other environments. The FourRooms and MultiRoom-N2-S4 environment solve a high percentage of environments. 50 4.2. Grid World Experiments Environment trained on Total mean return % environments solved Empty-5x5 2.1 33% Empty-6x6 3.25 28% Empty-8x8 0.32 22% Empty-16x16 0.02 6% FourRooms 2.41 56% DoorKey-5x5 1.89 28% DoorKey-6x6 2.58 39% DoorKey-8x8 1.0 44% DoorKey-16x16 0.6 39% MultiRoom-N2-S4 1.48 56% MultiRoom-N4-S5 0.66 39% MultiRoom-N6-S10 0.99 33% KeyCorridor-S3R1 0.47 28% KeyCorridor-S3R2 0.53 28% KeyCorridor-S3R3 0.32 22% KeyCorridor-S4R3 0.21 11% KeyCorridor-S5R3 0.11 11% KeyCorridor-S6R3 0.72 28% Curriculum RH 2.64 56% Curriculum PTR 4.14 61% Curriculum LP 3.9 61% Curriculum ALP 3.06 67% Curriculum EMA 3.93 61% Curriculum FS-EMA 3.59 61% Table 4.7: We report the median measurements obtained by evaluating each agent with \ufb01ve random seed for 100 episodes on every environment in V. 51 4.3. Google Football Environment 4.3 Google Football Environment Google Research Football environment [Kurach et al., 2019] is a novel 3D RL environment to provide a highly optimized, stochastic, and open-source simulation. The environment provides single-agent RL, where the agent controls all players of his team, and multi-agent RL, where a separate agent controls each player. It is also possible to research the e\ufb00ect of self-play, where the agent plays against di\ufb00erent versions of itself. The environment provides a comprehensive set of progressively more demanding and diverse scenarios with the Football Academy. These scenarios enable us to analyze our algorithm on a range of tasks requiring di\ufb00erent levels of abstractions and di\ufb00erent tactics. The engine implements a full 11 vs. 11 football game with the standard rules including goal kicks, corner kicks, yellow and red cards, o\ufb00sides, handballs, and penalty kicks as shown in \ufb01g. 4.11. This full 11 vs. 11 football game, consisting of 3000 frames, is called Football Benchmark. Players have di\ufb00erent characteristics like speed or accuracy, but both teams have the same set of players. Further, players are getting tired over time, which in\ufb02uences their behavior and skills. Figure 4.11: The Google Football Engine is a football simulation which supports the major football rules like kicko\ufb00s (top left), goals (top right), fouls, cards (bottom left), corner and penalty kicks (bottom right), and o\ufb00side. [Kurach et al., 2019] 52 4.3. Google Football Environment 4.3.1 State & Observations There are three ways to represent the environment state at the current time step to the reinforcement learning agent (called observation). pixel. The representation is a 1280 x 720 x 3 tensor corresponding to the rendered screen. The scoreboard and a mini-map at the bottom of the image are present in this representation. The mini-map tells the location of the ball and the players of both teams. Super Mini Map (SMM). The SMM is a 72 x 96 x 4 tensor encoding information about both teams, the ball, and the currently active player. The encoding is binary and indicates whether there is a player or the ball at the given coordination or not. Floats. This representation uses a 115-dimensional vector to capture the game state, player coordinates, ball coordinates, possession, and the active player. 4.3.2 Actions The agent can execute one of 20 actions per time step. The currently active player executes all of those actions, except the keeper rush action. The active player moves by selecting one of eight dedicated moving actions. There are four di\ufb00erent ways to kick the ball (short pass, high pass, long pass, shot). The move, sprint, and dribble actions are sticky and have to be ended explicitly by their respective stop action. Finally, there are sliding and do-nothing actions. 4.3.3 Rewards The environment provides two di\ufb00erent reward functions to choose from. It is also possible to de\ufb01ne custom reward functions to look into reward shaping. In this work, we used the out-of-the-box reward functions. SCORING rewards the agent when scoring a goal with a +1 reward and a -1 reward when conceding one. This reward signal is sparse and can lead to no signal during the early stages of learning where the agent does not know how to overcome the opponent\u2019s defense. CHECKPOINT is an additional shaped reward designed to overcome the sparsity of the SCORING reward signal. Once per episode, the agent receives a +0.1 reward for getting closer to the opponent\u2019s goal measured by the Euclidean distance. The opponent\u2019s \ufb01eld is divided into ten checkpoint regions, and the agent receives the +0.1 reward once for every region. The agent also receives all non-collected checkpoint rewards when scoring to avoid penalizing agents that do not go through all the checkpoints before scoring. 53 4.3. Google Football Environment 4.3.4 Football Academy Scenarios In addition to the full 11 vs. 11 football game, the environment allows agents to train on a set of 11 progressively more complex scenarios. This set of tasks is called Football Academy where it is possible to de\ufb01ne custom scenarios to train agents for a particular situation. In table 4.8 the available scenarios are described. 4.3.5 MDP Statement The Google research Football environment does not ful\ufb01ll the Markov property if we only consider the pixel input at time step t as observation ot. We do not know in which direction the ball or the players are moving from a single observation. It is possible to have two identical observations, but in one observation, the ball moves to the left of the pitch, and in the other, it moves to the right. The direction and the velocity of the ball can not be inferred based on a single image. The agent only partially observes its environment\u2019s state, making it a partially observable Markov decision process (POMDP). There are a few possibilities to overcome this issue: \u2022 We can use another state representation than pixels. This representation would have to encode the direction and velocity of the ball and the players. \u2022 We can use a long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] inside the agents function approximator. The observation ot together with the hidden state of the LSTM ht are enough to overcome the uncertainty about the current state st of the environment. \u2022 We can use the last k observations to approximate the true state st. This is the solution proposed by Mnih et al. [2015]. In our work, we used the last four observations stacked on top of each other, illustrated in \ufb01g. 4.12, as input to approximate the true Markov state. Figure 4.12: To approximate the Markov property we use a frame stack of the last 4 frames as an input for our neural networks. 54 4.3. Google Football Environment Name Description Empty Goal Close Our player starts inside the box with the ball and needs to score against an empty goal. Empty Goal Our player starts in the middle of the \ufb01eld with the ball and needs to score against an empty goal. Run to Score Our player starts in the middle of the \ufb01eld with the ball and needs to score against an empty goal. Five opponent players chase ours from behind. Pass and Shoot with Keeper Two of our players try to score from the edge of the box. One is on the side with the ball and next to a defender. The other is at the center, unmarked and facing the opponent keeper. Run, Pass and Shoot with Keeper Two of our players try to score from the edge of the box. One is on the side with the ball and unmarked. The other is at the center, next to a defender, and facing the opponent keeper. Easy Counter-Attack 4 versus 1 counter-attack with keeper; all the remaining players of both teams run back towards the ball. Hard Counter-Attack 4 versus 2 counter-attack with keeper; all the remaining players of both teams run back towards the ball. 11 versus 11 with Lazy Opponents Full 11 versus 11 game, where the opponents cannot move but intercept the ball if it is close enough to them. Our center-back defender has the ball at \ufb01rst. The maximum duration of the episode is 3000 frames instead of 400 frames. 11 versus 11 easy Full 11 versus 11 game, with a duration of 3000 frames per episode. The game starts with a kicko\ufb00, the team starting with the kick-o\ufb00 is assigned randomly. The agent receives a reward of -1 when receiving a goal and a reward of +1 when scoring one. The opponents di\ufb03culty is set to easy. 11 versus 11 medium Same as 11 versus 11 easy, but the opponents di\ufb03culty is set to medium. 11 versus 11 hard Same as 11 versus 11 easy, but the opponents di\ufb03culty is set to hard. Table 4.8: Description of the Google Football environments taken directly from Kurach et al. [2019]. All scenarios end after 400 frames, or if the ball is lost, a team scores or the game stops (e.g., if the ball leaves the pitch or a free kick is awarded). 55 4.4. Google Football Experiments 4.4 Google Football Experiments 4.4.1 Experimental Setup The experimental setup for the Google Football environment is similar to the grid world environment. As in grid world, we train the teacher agent for 1\u2032000 steps, and for each teacher step, we train the student agent for 10\u2032000 steps in the selected environment. The student agent is therefore trained for 10 million steps in total. After each step, we evaluate the student on all environments in V for 100 episodes. In our previous work [Schraner, 2020], we train the student agent for 50 million steps. Due to a limit in computational resources, we limit the training in this thesis to 10 million student steps and only one random seed per experiment. Executing one experiment with the described amount of steps takes around \ufb01ve days. We \ufb01xed the knowledge transfer to policy transfer and used the source task reward signal. The hyperparameters are reported in appendix A.3. We use all Google Football environments listed in table 4.8. The student agent receives the SCORING reward signal. Network architecture The network architecture for the teacher agent is described in section 3.2.4. The student\u2019s network architecture is the same as in our previous work [Schraner, 2020] and depicted in \ufb01g. 4.13. Actor-critic algorithms can share the network torso and represent the policy and value function as individual heads. This allows weight sharing between the two function approximators. Sharing weights can improve the training time and forces the shared part to be more general. This may prevent over-\ufb01tting. As we share network weights, we get di\ufb00erent gradients from di\ufb00erent loss functions. In our work, we made use of sharing weights. Each network uses the SMM observation, with a shape of 72\u00d796\u00d74. The football \ufb01eld has a dimension of 72 \u00d7 96, and there is a separate channel for each team, the ball, and the currently active player. To approximate the Markov property, we use a frame stack of the last four frames, as shown in \ufb01g. 4.12. 56 4.4. Google Football Experiments Figure 4.13: CNN network architecture with the stacked Super Mini Map observation as input. The one-hot encoded previous action at\u22121 is used as an additional input. The policy and value function are two heads sharing the network torso. ",
    "Results": "Results We transfer the best CMDP teacher settings from the MiniGrid environment to the Google Research Football environment. The four baseline methods and our results in our previous work are used to compare the results in table 4.9. We report the average return of 100 episodes per environment after training on one random seed. Therefore the results should be interpreted with care. The LP and Thompson baseline are by far the worst. In the case of Thompson sampling, only one out of eleven environments were solved. The LP teacher fails for the 11 vs. 11 full game. Sampling environments uniformly works well on easy environments but fails for more challenging ones such as the counterattack and 11 vs. 11 environments. The window baseline is the strongest of all baselines. With our proposed teacher-student setting we surpass our baselines on all environments. The LP based teacher had the best results on empty goal close, empty goal and run to score, but the improvement over the other experiments is marginal. Compared to the results in our previous work [Schraner, 2020] reported in table 4.10 we do not match the results. Training optimized PPO for 50 million steps leads to a return of -1.4 in the 11 vs. 11 hard environment. The highest score we obtained in this environment was -1.45 with the window baseline and LP teacher. Using a manually de\ufb01ned curriculum, we archive a return between -2.08 and 1, depending on the curriculum. Using the ALP, RH, PTR, and LP observation types leads to the best results. Whereas using ALP gives the highest total average and PTR the highest number of solved environments. The EMA-based teachers fall behind, probably due to a not-tuned \u03b1 value for the Google Football environments. 58 4.4. Google Football Experiments Environment / Metric Uniform LP Thompson Window RH PTR LP ALP EMA FS-EMA Total mean return -3.26 -11.62 -2.67 -0.81 -0.3 -0.28 -0.5 1.84 -1.36 -1.51 % environments solved 45% 27% 9% 55% 55% 64% 45% 45% 45% 45% Empty Goal Close 0.99 1 -0.05 1 0.96 0.98 1 0.94 0.79 1 Empty Goal 0.84 1 0.64 0.93 0.85 0.67 0.98 0.93 0.6 0.76 Run to Score 0.88 0.98 0.0 0.76 0.95 0.7 0.05 0.98 0.67 0.91 Pass and Shoot with Keeper 0.12 0.0 -0.02 0.0 0.33 0.03 0.0 0.51 0.0 -0.07 Run, Pass and Shoot with Keeper 0.02 0.0 0.0 0.0 -0.11 -0.02 0.0 0.05 0.0 -0.17 Easy Counter-Attack 0.0 -0.02 0.0 0.23 0.33 0.17 0.0 0.0 0.02 0.0 Hard Counter-Attack 0.0 0.0 0.0 0.22 0.18 0.05 0.03 0.0 -0.05 0.02 11 vs. 11 lazy 0.0 -0.1 0.0 0.05 0.0 0.02 0.16 0.0 0.12 0.08 11 vs. 11 easy -1.55 -3.8 -0.55 -0.53 -0.42 0.0 0.0 -0.23 -0.85 -0.55 11 vs. 11 medium -1.98 -5.28 -1.48 -1.23 -1.75 -1.2 -1.08 -0.65 -1.58 -1.78 11 vs. 11 hard -2.63 -4.98 -1.5 -1.45 -1.88 -1.85 -1.45 -1.7 -1.9 -1.7 Table 4.9: Google football results obtained on one random seed after training for 1\u2019000 CMDP steps, equal to 10 million student steps. The average return over 100 episodes is reported. We use policy transfer between the source tasks and the source task reward signal. Experiment Return PPO -1.4 Best scenarios curriculum -0.85 Best 11 vs 11 curriculum -2.08 Best increasing curriculum -0.48 Best smooth increasing curriculum 1 Prioritized level replay -1.05 Table 4.10: Comparison of our di\ufb00erent curriculum learning approaches of our previous work Schraner [2020], the average return over 100 episodes on the 11 vs 11 hard environment is reported. ",
    "Experiments": "Experiments In table 4.11 we report our results obtained in our previous work [Schraner, 2019] by training an agent with IMPALA in a single Google Football environments. With our curriculum learning approach, we can match and surpass the results of our previous work by only using a \ufb01fth of the samples. Compared to the baselines published with the Google Football environment [Kurach et al., 2019] we archived better results on the easy environments (Empty Goal, Run to Score, Pass and Shoot with Keeper) but slightly worse on the di\ufb03cult environments (Run, Pass and Shoot with Keeper, Easy Counter-Attack, Hard Counter-Attack, and 11 vs. 11 lazy). Scenario Ours@50M Google Baseline@50M Empty Goal Close 0.99 1.0 Empty Goal 0.84 0.86 Run to Score 0.88 0.88 Pass and Shoot with Keeper 0.0 0.66 Run, Pass and Shoot with Keeper -0.05 0.18 Easy Counter-Attack 0.0 0.5 Hard Counter-Attack -0.02 0.2 11 vs. 11 lazy 0.01 0.2 11 vs. 11 easy -1.59 -0.35 11 vs. 11 medium -1.6 -0.79 11 vs. 11 hard -2.78 -1.16 Table 4.11: Results on the Football Academy scenarios obtained in our previous work [Schraner, 2019] and the Google baseline [Kurach et al., 2019] with the IMPALA algorithm. The reported results is the average reward of 100 episodes after a training on 50 million frames. ",
    "Discussion": "Discussion Recent work in curriculum learning for rl has shown promising results in improving sample e\ufb03ciency and asymptotic performance in challenging environments. Most of these works focused on automatic task generation e.g., by using procedural generated environments [Wang et al., 2019], changing the initial state distribution to easy starts [Florensa et al., 2017], creating additional synthetic goals to guide the student [Racani`ere et al., 2020] or generating auxiliary tasks [Riedmiller et al., 2018]. Using self-play to create a curriculum was a core idea in AlphaStar [Vinyals et al., 2019]. Methods to automatically select tasks in a teacher-student setting are also popular. Most of those methods use heuristics or sampling-based approaches to select the tasks [Matiisen et al., 2019, Foglino et al., 2019a,b, Narvekar et al., 2017, Narvekar and Stone, 2019, Kanitscheider et al., 2021]. In our thesis, we propose a new approach to formulate the task sequencing problem as an MDP and train a teacher rl agent to perform the task sequencing while training the student simultaneously. Our experiments show that performing task sequencing with an rl teacher agent is superior to heuristic-based task sequencing. On the grid world environment, we were able to outperform all baselines signi\ufb01cantly. Our approach failed to increase the asymptotic performance on most of the MiniGrid when compared to tablua-rasa rl. Training an agent directly in easy grid world environments leads to better performance compared to curriculum learning approaches. Our Google Football experiments improved the asymptotic performance on easy environments such as Empty Goal, Run to Score, and Pass and Shoot with Keeper. We believe that the root cause of failing on challenging environments lies in the teacher\u2019s reward signal. Curriculum learning with the source task reward signal is in some cases bene\ufb01cial for more challenging environments such as DoorKey-16x16, but it is di\ufb03cult to target the teacher agent towards speci\ufb01c environments. Using a targeted reward signal to guide the teacher agent towards more challenging environments is often insu\ufb03cient, as the teacher faces a sparse reward signal in that setting. When providing the teacher with a sparse reward signal, our teacher-student settings collapses towards a uniform sampling approach. The teacher agent\u2019s learning curves did not converge after 1\u2019000 steps. Training both agents beyond 1\u2019000 CMDP steps might increase the asymptotic performance and help the student agent solve more challenging environments. This would increase the asymptotic performance at the cost of sample e\ufb03ciency. We 60 61 \ufb01xed the CMDP and student steps to match the number of training steps used in our no curriculum learning experiments to carry out a fair comparison. When specifying the number of teacher and student steps, we have to balance the teacher\u2019s and student\u2019s performance. Using fewer teacher steps might not be su\ufb03cient for the teacher to learn how to perform task sequencing but would give the student more time to converge in the selected environment. Using more teacher steps is bene\ufb01cial for the teacher, but on the other hand, the student has less time to learn the selected task. Using fewer student steps might lead to a noisy reward signal for the teacher agent. In future work, it would be interesting to analyze the importance and e\ufb00ect of those values. One could also train the student until convergence or make the number of student steps part of the teacher\u2019s action space. When comparing the three transfer methods policy, reward and combined, it turns out that the policy transfer leads to the best results. Reward transfer is volatile for both the rl teacher agents and the baselines. Combining reward transfer with policy transfer fails. After a few CMDP steps, the added reward bonus covers the environment\u2019s reward, forcing the student agent to maximize its reward signal. We compared six partial observable state representations and one approximately fully observable state representation (PCA) for the CMDP state. The PCA representation type failed utterly. There are other ways to represent neural network weights in a compact embeddings, such as autoencoders or network distillations. In future work, we can investigate other methods to represent the student weights. The results of the six partial observable representations did not di\ufb00er too much from each other. Overall the PTR, LP, and EMA representation resulted in the highest asymptotic rewards and the most general student agents for the MiniGrid environments. In our Google Football environment RH, PTR, LP and ALP achived the highest asymptotic reward and percentage of solved environments. In that environment tuning the \u03b1 value for the EMA representation might improve the results. The proposed curriculum learning approach is bene\ufb01cial for multi-task reinforcement learning. The reward across all tasks and the overall percentage of solved environments are signi\ufb01cantly higher compared to our baselines or when training an agent on a single environment. We investigated the sample e\ufb03ciency of our teacher-student setup in three grid world environments. The sample e\ufb03ciency improves in the Empty-16x16 and DoorKey16x16 environment but remains similar in the FourRooms environment. We discovered that training in the proposed curriculum setup is noisy regarding the student\u2019s performance throughout training. The sample e\ufb03ciency on the Google Football environments increased considerably. By only using one-\ufb01fth of the samples, we almost matched or surpassed the performance on 8 out of 11 en62 vironments compared to directly training in those environments. Training agents for the 11 vs. 11 football game remains di\ufb03cult with our proposed curriculum learning approach. The asymptotic performance remained the same compared to training directly in an environment. Using a careful, manually de\ufb01ned curriculum improves the asymptotic performance compared to our teacher-student setup. Our previous work concluded that de\ufb01ning such a curriculum by hand requires a lot of tweaking and domain knowledge. In this work, we analyzed di\ufb00erent knowledge transfer methods, teacher observation types, and reward signals. Using policy transfer combined with the PTR, LP, or EMA representation and the source task reward reward signal, we observe the following: \u2022 The generality of our agent improves. \u2022 The asymptotic performance in some environments increases compared to using no curriculum. \u2022 Training an rl agent to perform task sequencing is superior over our heuristicbased baselines. \u2022 The sample e\ufb03ciency compared to no curriculum does increase in one out of three analyzed grid world environments. \u2022 The sample e\ufb03ciency compared to no curriculum does increase by a factor of \ufb01ve on 8 out of 11 Google Football environments. The improvement in the generality and sample e\ufb03ciency with curriculum has to be investigated more carefully in future research. Another weakness of our work is the limited hyperparameter search. The e\ufb00ect of training teachers or students for more steps remains unclear. Other types of rl algorithms such as DQN or traditional tabular methods for the teacher agent have to be tested. The teacher network architecture should be tuned further. Our agents trained without curriculum learning on the grid world environments results in zero rewards for hard environments. To perform more meaningful analysis, we propose to tune those baseline agents and then transfer them to the teacher-student setting. It remains unclear if other PPO hyperparameters for the teacher training would improve performance and how well these parameters transfer across all methods. ",
    "Conclusion": "",
    "Related Work": "Related Work creates an implicit curriculum. This setup has proven to be successful in the well-known Go AI Alpha Go and its successors [Silver et al., 2016, 2018]. In Vinyals et al. [2019] the idea of self-play was extended to a league setting. The goal of this league setup is that the agent faces increasingly stronger opponents and opponents trained to perform speci\ufb01c strategies to exploit weaknesses and prevent mode collapse. In co-learning and sample sequencing, the target MDP is not changed, and no speci\ufb01c level of control over the environment is required. If control over the environment is possible, one can change the target MDP by changing the initial state distribution or the reward function to create a suitable task sequence. One example of this approach is the reverse curriculum generation, where a robot is learning to reach a goal from a set of starting positions increasingly far from the goal [Florensa et al., 2017]. Foglino et al. [2019a] uses metaheuristic search methods such as beam search, tabu search, genetic algorithms, or ant colony search in order to solve the task sequencing problem. In their follow-up paper, they compare those metaheuristic search methods to their Heuristic Task Sequencing for Cumulative Return (HTSCR) algorithm [Foglino et al., 2019b]. In our work, we treat the task sequencing as an MDP [Narvekar et al., 2017, Matiisen et al., 2019, Narvekar and Stone, 2019] where we use reinforcement learning in order to train a teacher agent to perform the task sequencing online. In this setting, we formalize curriculum learning as an interaction of two MDPs. The standard MDP modeling the learning agent interacting with the environment is referred to as student MDP. A meta level MDP for the curriculum agent to perform the task sequencing referred to teacher MDP. At the time of writing, no MDP-based task sequencing method where both student and teacher use neural network-based function approximators and where both are trained using reinforcement learning is known to the author. Therefore the key focus is to propose such a framework and analyze the e\ufb03ciency of such an approach. Task generation speci\ufb01es how and when the source tasks for the curriculum are de\ufb01ned. The quality of its source tasks heavily in\ufb02uences the quality of a curriculum. The tasks can either be created beforehand or online during training. The goal of task generation is to create a set of source tasks that allow a knowledge transfer through them such that it is easier to solve the target task. In Narvekar et al. [2016], a method for creating a set of source tasks by specifying task descriptors, that are controlling the degrees of freedom of the task, is introduced. Those task descriptors specify the environment like the environment size, action set, opponent type, initial states, et cetera. In Powerplay [Schmid5 1.2. Previous Work huber, 2013] a framework where new tasks are generated online is introduced. The system searches for new source tasks such that the agent becomes more and more general. A \ufb01xed computation budget is applied to force the task generator to create new tasks that are only slightly more di\ufb03cult than the previous ones. Jiang et al. [2020] introduced the idea of prioritized level replay. In reinforcement learning environments, there is usually a level identi\ufb01er. This could be a level index or a random seed. Usually, the level to use is sampled uniformly. Which level is sampled can in\ufb02uence the di\ufb03culty of the task as well as the environment dynamics. This diversity among levels implies that di\ufb00erent levels hold di\ufb00erent learning potentials for RL agents at any point in training. Prioritized level replay introduces a new level sampling strategy to prioritize levels based on their learning potential creating an implicit curriculum. 1.2 Previous Work In previous work, we tried to reproduce the work of Kurach et al. [2019] on the Google Research Football environment. We also implemented and evaluated extensions to the Proximal Policy Optimization (PPO) algorithm such as Actor with Variance Estimated Critic (AVEC) [Flet-Berliac et al., 2020]. Additionally, we carried out experiments with curriculum learning in the Google football environment. Detailed information about this environment can be found in section 4.3. We evaluated two manually de\ufb01ned curricula as well as two automatically generated curricula, and a prioritized level replay curriculum. We used the weights obtained by training on the source task as an initialization for the policy on the target task as a transfer method. The results of our previous work are summarized in table 1.1 for a comparison in section 4.4.2. Scenarios curriculum and 11 vs 11 curriculum are both manually de\ufb01ned curricula, the \ufb01rst one over both a set of football academy tasks (see table 4.8) and the 11 vs 11 full game on di\ufb03culty easy, medium and hard. The second curriculum only uses the 11 vs 11 full game environments. The increasing curriculum consistently increases the game di\ufb03culty parameter \u03b4 by 0.05 starting from 0.01 and \ufb01nishing at 0.95. A \u03b4 value of 0.95 is equivalent to the 11 vs 11 hard environment. At the \ufb01rst task change, we increase \u03b4 from 0.01 to 0.05 instead. The di\ufb03culty is increased automatically after 100 training iterations. Smooth increasing curriculum is similar to the increasing curriculum. Instead of adapting the di\ufb03culty depending on training iterations, we increase the di\ufb03culty as soon as the average return is greater than 0.9. Additionally, we increment \u03b4 by 0.001, start with a value of 0.001 and end with a \u03b4 value of 0.95. Although four out of \ufb01ve curricula improve our results, only the smooth increasing curriculum can outperform the baseline signi\ufb01cantly. Using only the 11 vs. 11 tasks includes too hard tasks at the early stages of learning. The smooth increas6 1.3. Problem Statement ing curriculum is superior over the increasing curriculum as the task di\ufb03culty increases more evenly. Scenarios curriculum includes task switches with signi\ufb01cant changes in their initial state distribution and observation space, making this type of curriculum worse than the increasing ones. Experiment Return PPO -1.4 Best scenarios curriculum -0.85 Best 11 vs 11 curriculum -2.08 Best increasing curriculum -0.48 Best smooth increasing curriculum 1 Prioritized level replay -1.05 Baseline -1.39 Table 1.1: Comparison of our di\ufb00erent curriculum learning approaches, the average return over 100 episodes on the 11 vs 11 hard environment is reported. 1.3 Problem Statement In this work, we are developing a teacher-student curriculum learning setup focusing on online task sequencing and transfer learning. We manually do the task generation and focus on the task sequencing problem formulated as a curriculum MDP [Narvekar et al., 2017]. Both the teacher and student are policy gradient reinforcement learning agents using neural networks as function approximators and are trained with PPO. We aim to answer the following research questions: \u2022 Is it possible to train both teacher and student with the curriculum MDP (CMDP) setting from scratch such that the sample e\ufb03ciency or average reward on the target task increases compared to vanilla PPO on the target task? \u2022 What is a suitable reward function for our teacher agent to increase the sample e\ufb03ciency or average reward on the target task? \u2022 How can we de\ufb01ne observations in the CMDP such that the sample e\ufb03ciency or average reward on the target task increases? \u2022 What is a suitable transfer method for the speci\ufb01ed setting? \u2022 Are we able to improve the results of our previous work on the Google Research Football environment with this curriculum learning framework? Chapter 2 Background In this chapter, we introduce the core ideas and theoretical foundations used in this work. We introduce the reinforcement learning paradigm focusing on policy gradient methods and highlight some core ideas important in this work. We introduce the proximal policy optimization (PPO) algorithm and additional RL improvements used in this work. Parts of this chapter were written in our previous work [Schraner, 2020] and added to this thesis for a coherent document. 2.1 Reinforcement Learning Reinforcement learning is a popular framework suited to solve sequential decisionmaking processes. An agent learns how to act in an environment by observing a numerical reward signal. The agent has to learn a policy to predict an action based on the environment\u2019s state, such that the cumulative reward is maximized. At the early stages of learning, RL is very similar to trial and error learning. By making progress, the learner usually continuously observes new states of the environment and learns how to act in those new states without forgetting about the correct behavior in the early stages. The only indication wether a single or a series of actions leads to preferable changes in the environment is the reward signal. After every action, the agent receives a representation of the new environment state (observation) and a reward signal. This signal can be either positive or negative, and it does not tell the agent exactly which series of actions lead to that signal. The reward can be delayed, for example, students receive a negative reward signal while studying for their exams, but they receive a large positive reward signal after successfully passing the exam. This way of learning makes RL a general framework, as stated by the reward hypothesis: That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward) [Sutton and Barto, 2018]. Biological systems inspire the use of a reward signal to learn: through experiencing pleasure or pain, we know what actions are good in an immediate sense. In contrast to RL, supervised learning uses a training set of labeled examples. With supervised learning, we are trying to capture the knowledge represented in the training set. The use of a training set proved to be very powerful to learn 7 8 2.1. Reinforcement Learning intelligent behavior, but it also makes supervised learning hard for interactive problems. Often it is impractical or costly to obtain a useful set of examples of the desired behavior that is both correct and representative for all situations. There is an intersection of RL and supervised learning called imitation learning. In imitation learning, the agent receives an initial set of labeled demonstrations of actions in the environment and uses it in a supervised fashion. The agent may be improved further with standard reinforcement learning. With unsupervised learning, one can \ufb01nd structure hidden in a large amount of unlabeled data. There is no reward signal to maximize in unsupervised learning, which di\ufb00erentiates it from RL. 2.1.1 Finite Markov Decision Processes Markov decision processes (MDP) is a formalisation for sequential decision-making processes. A Markov decision process is a 4-tuple (S, A, p, r), where \u2022 S is a set of states \u2022 A is a set of actions \u2022 p(s\u2032|s, a) = P[st+1 = s\u2032|st = s, at = a] is the probability of transitioning from state s to state s\u2032 when taking action a. \u2022 r(s, a) = E[rt+1|st = s, at = a] is the expected reward received by taking action a in state s. MDPs are an idealized form of the reinforcement learning problem and are used to formulate mathematically precise theoretical statements [Sutton and Barto, 2018]. Figure 2.1: The agent-environment interaction in a Markov decision process [Sutton and Barto, 2018]. The agent-environment interaction in a MDP is shown in \ufb01g. 2.1. The decisionmaker is called an agent. The agent interacts with the environment by following a policy denoted by \u03c0. The policy maps from state s to action a \u2208 A and can either be deterministic \u03c0(s) = a or stochastic \u03c0(a|s) = P\u03c0[at = a|st = s]. Everything outside the agent is the environment. Taking an action in the environment leads to a change in the environment\u2019s state. For example in the 9 2.1. Reinforcement Learning grid world environment, an action can be moving forward, picking up a key, or opening a door. The environment receives the action of the agent and returns a reward r and a representation of its new state s \u2208 S. If the entire state of the environment is not observable, e.g., the agent has only a limited \ufb01eld of view, the agent receives an observation o \u2208 O instead of a state. The observation only partially describes the environment\u2019s state. The environment can be deterministic or stochastic and may change itself without interactions from the agent. Agent and environment are interacting in a sequence of discrete-time steps t. t starts from 0 in the \ufb01rst time-step, and goes up to T, with T being the last time-step. At each time step, the agent receives a representation of the environment state st and a reward rt based on which he selects and executes action at. This leads to a sequence of interactions called trajectory: s0a0r1s1a1r2s2a2r3 . . . statrt. A trajectory has to be \ufb01nite. In a \ufb01nite MDP, the sets of states, actions and rewards all have a \ufb01nite number of elements. The next state is only dependent on the preceding state-action pair. Therefore we have a well de\ufb01ned discrete probability distributions for the next state dependent only on the preceding state-action pair [Sutton and Barto, 2018]: P[st+1|st, at] = P[st+1|s1, a1, . . . , st, at] (2.1) This is called the Markov property. To ful\ufb01ll the Markov property, the probabilities given by p can only depend on the preceding state and action and completely characterize the dynamics of the environment. Therefore, a single state must include information about every aspect of the past agent-environment interactions that make a di\ufb00erence for the future. 2.1.2 Policy and Value Functions In reinforcement learning, agents select their actions according to their policy function. A policy has to be time-independent, the trajectory up to the time step t does not in\ufb02uence the action probabilities. The policy \u03c0(a|s) outputs a probability distribution over all possible actions given a state: \u03c0(a|s) = P[at = a|st = s] (2.2) The objective of this policy is to maximize the cumulative future reward, also called return Gt: Gt = rt+1 + \u03b3rt+2 + \u03b32rt+3 + \u00b7 \u00b7 \u00b7 = T\u22121 \ufffd k=t \u03b3krt+k+1 (2.3) 10 2.1. Reinforcement Learning The discount factor \u03b3 \u2208 [0, 1] serves multiple purposes: \u2022 future rewards may have higher uncertainty \u2022 future rewards do not provide immediate bene\ufb01ts. In some cases, immediate rewards are of more value, like in economics where we prefer the money now over later as we could invest it in maximizing future earnings. \u2022 the discount factor provides mathematical convenience, as it solves problems with in\ufb01nite MDPs or loops in the state transition graph [Sutton and Barto, 2018] The performance of a policy is measured by its state- and action-value functions. Those value functions for the policy \u03c0 are the expectation of what return the agent receives by following policy \u03c0 from state s \u2208 S. This estimation is called state-value function V (s) (how good is it to be in a given state) or action-value function Q(s, a) (how good it is to perform a given action in a given state). The state-value function is the expected return when following policy \u03c0 from state s: V \u03c0(s) .= E\u03c0[Gt|st = s] (2.4) The action-value function is the expected return when taking action a in state s and then following policy \u03c0: Q\u03c0(s, a) .= E\u03c0[Gt|st = s, at = a] (2.5) A policy \u03c0 is considered better than another policy \u03c0\u2032 if for all states s \u2208 S the expected return is larger: V \u03c0(s) > V \u03c0\u2032(s), \u2200s \u2208 S (2.6) An optimal policy \u03c0\u2217 is a policy that is better or as good as any other policy in any state: V \u03c0\u2217(s) \u2265 V \u03c0(s), \u2200s \u2208 S \u2227 \u2200\u03c0 \u2208 \u03a0 (2.7) where \u03a0 is the set of all possible policies. The optimal state-value function V \u2217 is the maximum expected return over all policies when being in state s: V \u2217(s) = max\u03c0V \u03c0(s) (2.8) The optimal action-value function Q\u2217 is the maximum expected return over all policies when being in state s and taking action a: Q\u2217(s, a) = max\u03c0Q\u03c0(s, a) (2.9) 11 2.1. Reinforcement Learning Therefore, the optimal policy \u03c0\u2217 can be obtained by acting greedily according to the action that maximizes Q\u2217(s, a): \u03c0\u2217(at|st) = \ufffd 1, if at = argmaxa\u2208AQ\u2217(st, a) 0, otherwise Standard old fashion approaches to \ufb01nd the optimal state-value function V \u2217 or optimal action-value function Q\u2217 are Dynamic Programming, Monte-Carlo Methods, or Temporal-Di\ufb00erence Learning. We will not cover those methods in this report and continue with deep reinforcement learning and policy gradient methods. See Sutton and Barto [2018] for an introduction of those traditional methods. 2.1.3 Deep Reinforcement Learning Tabular methods work well for problems with a small state and action space. For such environments, it is easy to build a table with value or action-value estimates and act according to this table. Those problems are not very interesting and far away from real-world applications. With growing state or action spaces, the memory and computation needs are growing exponentially, with respect to the state or action space, for tabular methods. It is also not feasible to visit every state to \ufb01ll a table with value estimates. With stochastic environments, the problem gets even worse, as we would have to visit every state multiple times to calculate statistics. This raises the need for state approximation to generalize between similar states. There are numerous ways to approximate states, but the advances in deep learning made neural networks the preferred option as a function approximator. The work of Mnih et al. [2015] can be considered as the breakthrough of mixing deep learning with reinforcement learning and allowing a single algorithm to learn how to play Atari games. One challenge of deep learning with RL is that the optimization problem is non-stationary because the agent encounters new states in the environment during the ongoing learning progress. 2.1.4 Policy Gradient Methods Instead of learning value functions and selecting actions based on those functions, we can directly learn a parameterized policy that selects actions without consulting a value function. This approach is called policy gradient method. In policy gradient methods the policy \u03c0\u03b8 is parametrized with a parameter vector \u03b8 \u2208 Rd. When using a neural network as a function approximator for the optimal policy, the last layer of this network is usually a softmax layer in case of discrete actions. The probability distribution will be close to a uniform distribution in the early learning stages, leading to natural exploration. If the optimal policy is stochastic, then the softmax distribution will approximate the optimal stochastic policy. If 12 2.1. Reinforcement Learning the optimal policy is deterministic, then softmax distribution degenerates to a nearly deterministic policy. There is no need to specify that beforehand. Another advantage of policy gradient methods is that we directly optimize for what we care about, which is the optimal policy. It may be simpler to learn the policy directly than to estimate the state or state-action value. Given a performance measure for \u03c0\u03b8 in the form of a di\ufb00erentiable objective function J(\u03b8), we can perform gradient ascent to update the pararameters \u03b8: \u03b8 \u2190 \u03b8 + \u03b1\u2207\u03b8J(\u03b8) (2.10) The learning rate is speci\ufb01ed by \u03b1 and needs to be tuned. The objective of reinforcement learning is to maximize the expected sum of total discounted rewards. Policy gradient methods take this as an objective function to maximize: J(\u03b8) = E\u03c0\u03b8[ T\u22121 \ufffd t=0 \u03b3trt] (2.11) The objective function in this form is usually not di\ufb00erentiable as it is dependent on the stationary state distribution of the environment and thus can not be used for gradient ascent. By the policy gradient theorem [Sutton and Barto, 2018] we can rewrite J(\u03b8) to a di\ufb00erentiable form: \u2207J(\u03b8) = E\u03c0\u03b8[\u2207\u03b8log\u03c0\u03b8(at|st)Q\u03c0\u03b8(st, at)] (2.12) Actor-critic methods are policy gradient methods that are also learning a value function. This value function is used as a critic. The critic is used for bootstrapping as in temporal-di\ufb00erence learning [Sutton and Barto, 2018]. Using one or more estimated values, in this case, an estimate for the action-value, in the update step for the same kind of estimated value is called bootstrapping. Actor-critic methods use a bootstrapped n-step return or directly estimate the action-value function. The advantage of bootstrapping is variance reduction of the estimates and allowing us to take updates on partial episodes. However, it comes at the cost of a bias towards the learned critic as it usually does not match the real action-value function. In advantage actor-critic methods, the same critic used for bootstrapping is also used as a baseline function. The baseline (estimated action-value) is subtracted from the sampled n-step return resulting in a term called advantage. The advantage tells the agent how much better or worse his policy performs than currently estimated by the critic. The actor updates the policy parameters \u03b8 for \u03c0\u03b8(at|st), in the direction suggested by the critic. If the observed return is less than the expected return, we want to lower the probability of taking the given action; otherwise, we want to assign that action a higher probability. Using a baseline leads to further variance reduction and thus increases the convergence speed. 13 2.1. Reinforcement Learning 2.1.5 Proximal Policy Optimization Proximal policy optimization (PPO) is a policy gradient method for reinforcement learning by Schulman et al. [2017]. Similar to trust region policy optimization (TRPO) [Schulman et al., 2015] a constraint on the policy update is added such that parameter updates do not change the policy too much. This improves training stability as it prevents large policy updates. In TRPO an objective function is maximized, while a constraint on the size of policy updates is enforced. The Kullback\u2013Leibler (KL) divergence of the old policy \u03c0\u03b8old and the policy after the updated \u03c0\u03b8 is used for such a constraint: maximize\u03b8E[ \u03c0\u03b8(at|st) \u03c0\u03b8old(at|st)At(st, at)] (2.13) subject to E[KL[\u03c0\u03b8old(\u00b7|st), \u03c0\u03b8(\u00b7|st)]] \u2264 \u03b4 (2.14) where \u03b8old is the vector of policy parameters before the update, and A is the advantage. This constraint can be transformed to a penalty in order to solve an unconstrained optimization problem: maximize\u03b8E[ \u03c0\u03b8(at|st) \u03c0\u03b8old(at|st)At \u2212 \u03b2KL[\u03c0\u03b8old(\u00b7|st), \u03c0\u03b8(\u00b7|st)]] (2.15) where \u03b2 is a scaling coe\ufb03cient. In TRPO, the hard constraint is used instead of a penalty because it is hard to choose a value of \u03b2 that performs well. Especially on problems where the characteristics change throughout learning, as is the case in RL. PPO builds upon this idea of a trust region but implements a more straightforward constraint. A clipped surrogate objective is used to archive this simpli\ufb01cation. The ratio between old an new policies is denoted by \u03c6(\u03b8): \u03c6(\u03b8) = \u03c0\u03b8(at|st) \u03c0\u03b8old(at|st) (2.16) The TRPO objective is, therefore: JTRPO(\u03b8) = E[\u03c6(\u03b8)At(st, at)] (2.17) PPO now imposes the constraint by forcing \u03c6(\u03b8) to stay within a trust region of [1 \u2212 \u03f5, 1 + \u03f5] where \u03f5 is a hyperparameter. Simply clipping updates force the policy to stay in the trust region: JCLIP(\u03b8) = E[min(\u03c6(\u03b8)A, clip(\u03c6(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5)A)] (2.18) 14 2.1. Reinforcement Learning Therefore if the objective value is not in the trust region, the clipped value will be used. When PPO is used in a network architecture with shared parameters for policy (actor) and value (critic) functions, an additional entropy term (blue) is introduced to encourage exploration. Furthermore, the error term on the value estimation (red) is part of the PPO objective function. JCLIP(\u03b8) = E[JCLIP(\u03b8) \u2212 c1(V\u03b8(st) \u2212 Vtarget(st))2 + c2H(st, \u03c0\u03b8(\u00b7, st))] (2.19) where both c1 and c2 are scaling parameters for those losses which need to be tuned. PPO increases the sample e\ufb03ciency of the RL algorithm empirically, and we hence use it in this work. Hsu et al. [2020] has shown that in some cases, PPO fails to converge to a bad local optimum: \u2022 Using standard PPO with a continuous action space, training becomes unstable when rewards vanish outside the trust region. This can happen due to a bad Gaussian policy update, where PPO fails to recover. \u2022 On high-dimensional discrete action spaces, clipping might converge to suboptimal actions with standard softmax policy parametrization. This happens when the policy sees only bad actions (reward 0) and suboptimal actions without observing the optimal action. PPO then tends to increase the probability of suboptimal actions and not exploring new actions. \u2022 PPO can converge to suboptimal actions if they are close to the initialization. 2.1.6 Challenges of Deep Reinforcement Learning There are a lot of unsolved challenges in deep reinforcement learning. The most important two are the exploration vs. exploitation dilemma and the deadly triad. The exploration vs. exploitation dilemma also exists in the real world. We might have our favorite ski resort, where we go skiing every year. However, there are a lot of other ski resorts which we have not visited yet. It could be that one of the new resorts would please us more than our current favorite. If we do not try out di\ufb00erent resorts, we may never \ufb01nd the optimal one, but we risk skiing in a place with only easy slopes and bad restaurants if we try out new ones. The best longterm strategy might involve short-term sacri\ufb01ces to \ufb01nd the optimal ski resort. The di\ufb03culty is to \ufb01nd the optimal ratio of exploring new places vs. exploiting the current best one. This analogy can be adapted to reinforcement learning. We have to explore new actions to learn more about their e\ufb00ectiveness and maximize the expected return in the long run by exploiting new better actions found by 15 2.1. Reinforcement Learning exploring. In stochastic tasks, this dilemma is even worse because we have to explore the same action multiple times due to the stochastic changes. According to Sutton and Barto [2018] this problem remains unresolved and is speci\ufb01c to RL as it does not occur in supervised or unsupervised learning. The deadly triad is a de\ufb01nition by Sutton and Barto [2018]. It states that instability and even divergence while optimizing arises whenever we combine the following three elements: \u2022 Function approximation like neural networks. This element is needed to solve problems with large state and action spaces. \u2022 Bootstrapping Using one or more estimated values in the update step for the same kind of estimated value [Sutton and Barto, 2018]. Bootstrapping adds a bias towards our start estimation, but the updates result in high variance for long trajectories without bootstrapping. Without bootstrapping, we need more samples before our estimate converges, leading to a loss in data e\ufb03ciency and a rise in computational cost. \u2022 O\ufb00-policy training We call our training o\ufb00-policy when we update another policy (target policy) than the one we followed to generate the training trajectories. We need to train multiple value functions and policies in parallel for some use-cases and are therefore o\ufb00-policy. Many of the current state-of-the-art RL algorithms are o\ufb00-policy algorithms. Being o\ufb00-policy is also useful as it allows us to run a learned policy in parallel distributed on hundreds of actors and having a centralized learner who uses the generated trajectory to update a slightly o\ufb00-policy policy. All of the three elements are very useful, and one does not want to give them up. In practice, many RL architectures successfully use all three elements of the deadly triad, like DQN [Mnih et al., 2015]. In the example of DQN, they use many tricks to cope with the instability and prevent the estimates from divergence through training with experience replay and occasionally freeze the target network. 16 2.2. Transfer Learning 2.2 Transfer Learning In reinforcement learning, an agent usually starts with a random policy. The agent then has to learn an optimal policy for the target task using no prior knowledge. For challenging target tasks, for example, due to sparse reward signals or poor state representations, the agent might learn very slowly or entirely fails to learn at all. Transfer learning is one area of research that tries to speed up the training of RL agents by transferring knowledge from one or more source task MDPs to a target task. Instead of learning to solve the target task tabula rasa, the agent acquires knowledge on one or more source tasks. The knowledge can be transferred in form of samples [Lazaric et al., 2008], options [Soni and Singh, 2006], policies [Fern\u00b4andez et al., 2010], models [Fachantidis et al., 2013] or value functions [Taylor and Stone, 2005]. In the case of policy and value function transfer, the parameters of a policy or value function obtained by training on one or multiple source tasks can be used to initialize the policy or value function of the agent. Transferring the policy or value function leads to a bias in the action selection towards the experience acquired in the source task. 2.2.1 Evaluation Metrics for Transfer Learning To quantify the bene\ufb01ts gained from transfer learning, we need meaningful metrics. Typically we compare the learning curve on the target task for an agent after transfer with a tabula rasa agent. We consider the following three metrics: \u2022 Time to Threshold: The time to threshold computes how much faster an agent with knowledge transfer reaches a reward threshold compared to a tabula rasa agent. The time can be measured by CPU / GPU instructions, wall clock time, episodes, or steps. \u2022 Jumpstart: This measurement quanti\ufb01es the initial performance boost we gain as a result of knowledge transfer. \u2022 Asymptotic Performance: The asymptotic performance compares the \ufb01nal performance increase at the end of training. When comparing tabula rasa agents to agents that use transfer learning, we need to specify if we want to include the time spent learning the source tasks in our metrics. We talk about weak transfer when we do not include the costs of training in source tasks. If we consider the time spent in the source tasks when calculation the metrics, we measure the strong transfer. In \ufb01g. 2.2 we illustrate the three metrics once with a weak transfer and once with a strong transfer. As the strong transfer includes costs for training in the source task, the transfer curve starts with a delayed training time in \ufb01g. 2.2 (b). 17 2.2. Transfer Learning (a) Weak transfer (b) Strong transfer Figure 2.2: Performance metrics for transfer learning using (a) weak transfer and (b) strong transfer (\ufb01gure by Narvekar et al. [2020]). 18 2.3. Curriculum 2.3 Curriculum We de\ufb01ne a curriculum as a concept that organizes past experiences and schedules future experiences by training on tasks. Every task is modeled as a Markov Decision Process. In the following we use the curriculum de\ufb01nition of Narvekar et al. [2020]: De\ufb01nition 2.3.1 (Curriculum). Let T be a set of tasks, where mi = (Si, Ai, pi, ri) is a task in T and i is the task identi\ufb01er. Let DT be the set of all possible transition samples from tasks in T : DT = {(s, a, r, s\u2032)|\u2203mi \u2208 T s.t. s \u2208 Si, a \u2208 Ai, s\u2032 \u223c pi(\u00b7|s, a), r \u2190 ri(s, a, s\u2032)}. A curriculum C = (V, E, g, T ) is a directed acyclic graph, where V is the set of vertices, E \u2286 {(x, y)|(x, y) \u2208 V \u00d7 V \u2227 x \u0338= y} is the set of directed edges, and g : V \u2192 P(DT ) is a function that associates vertices to subsets of samples in DT , where P(DT ) is the power set of DT . A directed edge (vj, vk) in C indicates that samples associated with vj \u2208 V should be trained on before samples associated with vk \u2208 V. All paths terminate on a single sink node vt \u2208 V. If the curriculum is created online, then the edges are added dynamically during the agent\u2019s training. If the curriculum is created o\ufb04ine, then the graph is created beforehand. One simpli\ufb01cation to the curriculum de\ufb01nition is the single-task curriculum, where all samples stem from a single task. In a single-task curriculum, we rearrange the order in which we train on the experience samples as in prioritized experience replay [Schaul et al., 2016]. De\ufb01nition 2.3.2 (Single-task Curriculum). A single-task curriculum is a curriculum C where the cardinality of the set of tasks considered for extraction samples |T | = 1, and consists of only the target task mt. A second simpli\ufb01cation of the curriculum de\ufb01nition is the task-level curriculum. In the task-level curriculum, we de\ufb01ne a directed acyclic graph of intermediate tasks. The main challenge here is how to order the intermediate tasks such that the agent can constantly learn to solve more complex tasks while preventing catastrophic forgetting on already solved tasks. The mapping function g determines the set of samples DT i that are available at the next vertex. There are multiple works on task-level curricula [Svetlik et al., 2017, Matiisen et al., 2019]. De\ufb01nition 2.3.3 (Task-level Curriculum). For each task mi \u2208 T , let DT i be the set of all samples associated with task mi : DT i = {(s, a, r, s\u2032)|s \u2208 Si, a \u2208 Ai, s\u2032 \u223c pi(\u00b7|s, a), r \u2190 ri(s, a, s\u2032)}. A task-level curriculum is a curriculum C = (V, E, g, T ) where each vertex is associated with samples from a single task in T . Thus, the mapping function g is de\ufb01ned as g : C \u2192 {DT i |mi \u2208 T }. 19 2.4. Curriculum Learning The simplest form of a curriculum is the sequence curriculum. In the sequence curriculum, all nodes have an indegree and outdegree of at most 1. If combined with the task-level curriculum, we end up with the task-level sequence curriculum, which is an ordered list of tasks [m1, m2, . . . mt] De\ufb01nition 2.3.4 (Sequence Curriculum). A sequence curriculum is a curriculum C where the indegree and outdegree of each vertex v in the graph C is at most 1, and there is exactly one source node and one sink node. 2.4 Curriculum Learning In curriculum learning, we try to \ufb01nd the optimal order in which we feed experience from di\ufb00erent source tasks to the agent in order to maximize one of the metrics de\ufb01ned in section 2.2.1. The intuition behind curriculum learning is that through generalization over source tasks and knowledge transfer through increasingly more complex tasks, we can increase the sample e\ufb03ciency or asymptotic performance of our training algorithms. There are three key elements to curriculum learning: \u2022 Task Generation: In order to produce a bene\ufb01cial curriculum, we need a good set of source tasks. The tasks should provide an increasing di\ufb03culty. The tasks have to be similar to the target task. Otherwise, we might end up with a negative transfer, and using a curriculum over such tasks is hurtful for solving the desired target task. Task generation is, therefore, an important part of curriculum learning. In a task-level curriculum, the tasks are the nodes of the curriculum graph. The task may be generated online during training or pre-speci\ufb01ed. \u2022 Sequencing: Sequencing is concerned with the ordering of the experience samples. According to the graph de\ufb01nition in section 2.3, sequencing de\ufb01nes the vertices in our curriculum graph. These vertices can either be de\ufb01ned online during training or o\ufb04ine before training the agent. In section 2.4.2 we go into details of this aspect to curriculum learning. \u2022 Transfer Learning: In order to bene\ufb01t from a curriculum of tasks, we need a method to transfer knowledge through the curriculum. In section 2.2 we explained multiple methods for transfer knowledge from one or more source tasks to the target task. In curriculum learning, we repeatedly transfer knowledge from task to task, whereas we have only one transfer step in standard transfer learning. We visualized the interaction between the three key elements in \ufb01g. 2.3. When evaluating the curriculum, we use the same metrics as in section 2.2.1. Those metrics have to be extended as we have to consider the costs of building 20 2.4. Curriculum Learning Figure 2.3: We visualize the interaction between the three key elements of curriculum learning. Task generation is concerned with generate a set of tasks for the curriculum. Task sequencing selects a task out of the task set for the agent. The agent use knowledge obtained by training on previously selected task through transfer learning. After or during training on the new task, the obtained knowledge is stored. The task generation and sequencing can be done online and dependent on the student\u2019s current performance or o\ufb04ine before training. the curriculum. It is not clear how the work for the task sequencing or task generation done by humans should be included into the strong transfer metrics. In our work, we ignore those costs for the sake of simplicity. 2.4.1 Curriculum Learning Categorization We can categorize a curriculum learning approach along six dimensions, organized by attributes (in bold) and their respective values (in italics). This categorization was introduced in Narvekar et al. [2020]: 1. Intermediate task generation: target / automatic / domain experts / naive users. The set of tasks can be either de\ufb01ned o\ufb04ine before training or online during training. One can also specify a single task curriculum called target where only the target task is used. The tasks can be speci\ufb01ed by a human, either a domain expert [Schraner, 2020] or a naive user with no special domain knowledge. There are also methods to automatically generate tasks using a set of rules or a generative process as in Wang et al. [2019]. 2. Curriculum representation: single / sequence / graph. The simplest way to represent a curriculum is a single task curriculum, where we simply reorder the recorded experience [Schaul et al., 2016, Andrychowicz et al., 2017]. When using multiple tasks we can either represent the curriculum 21 2.4. Curriculum Learning as a simple sequence of tasks [Schraner, 2020] or as directed acyclic graph of tasks [Svetlik et al., 2017]. In the task representation, we can allow many-to-one, one-to-many, and many-to-many knowledge transfer. 3. Transfer method: policies / value function / task model / partial policies / shaping reward / other / no transfer. In section 2.2 we speci\ufb01ed di\ufb00erent forms of knowledge transfer. The weights of policy [Scheller et al., 2020] or value [Fern\u00b4andez et al., 2010, Taylor et al., 2007, Taylor and Stone, 2005] functions can be transferred from task to task. One can learn task models [Fachantidis et al., 2013] and transfer those from task to task, learn an auxiliary reward function, or extract options [Sutton et al., 1999b] and transfer those to the next task. 4. Curriculum sequencer: automatic / domain experts / naive users. The curriculum sequencing is concerned with the task switches during training. The switch can happen automatic upon speci\ufb01c rules, through a teacher or other methods. In section 2.4.2 we go into detail about this aspect of curriculum learning. We can also sequence the task manually by domain experts or naive users. 5. Curriculum adaptivity: static / adaptive. The adaptivity of a curriculum speci\ufb01es if the curriculum is completely de\ufb01ned before training or if it is dynamically adapted online during training. A static curriculum is de\ufb01ned before training, a adaptive curriculum is changed during training. Adaptive curricula can use the learning progress to estimate, e.g., if a task is easy or hard to learn at the current stage. Static curricula incorporate problem-speci\ufb01c knowledge. 6. Evaluation metric: time to threshold / asymptotic / jumpstart / total reward. In section 2.2.1 we introduced metrics to quantify the bene\ufb01ts gained from curriculum learning. When calculating those metrics, we have to decide if we want to measure the weak or strong transfer. 22 2.4. Curriculum Learning 2.4.2 Task sequencing Task sequencing is concerned with how the tasks can be sequenced in order to provide a curriculum. The tasks need to be sequenced in a way such that the current task at hand is just hard enough to solve. It might also be of interest to add an already learned task that the agent forgets about to allow live long learning. In section 1.1 we already introduced a few methods for task sequencing. In this section, we detail two sequencing methods that use the teacher-student setup with the MDP formulation. Narvekar et al. [2017] formulates curriculum learning as the interaction of two MDPs. A student MDP describes the currently selected task, and a teacher MDP models the selection of the next task for the student. They denote the teacher MDP as a fully observable MDP, where the state space is the set of policies the learning agent can represent. The state is represented as the parameters of the policy. The \ufb01nal states are de\ufb01ned as policies where the return on the target task is higher than a speci\ufb01c threshold. The action space is the set of tasks a student agent can train on. Taking an action results in the student training on the selected task for a \ufb01xed number of steps or until convergence. The transition function describes how the student agents policy changes as a result of learning a task. The reward function is de\ufb01ned as the time needed by the student agent to learn a policy that results in a return surpassing a certain threshold on the target task (time to threshold). The teacher wants to minimize this time to threshold. Therefore the reward is encoded as the negative of the expected time needed to learn the target task starting from a given policy. A recursive MonteCarlo algorithm optimizes the teacher agent, and the student is a tabular RL agent with tile coding, trained with Sarsa(\u03bb) and a value function transfer. In their follow-up paper, they investigated reward shaping as an additional transfer method [Narvekar and Stone, 2019]. Matiisen et al. [2019] uses a similar approach, but the state is not fully observable, making the MDP a partially observable Markov decision process (POMDP). The state and action space of the teacher MDP are the same as in Narvekar et al. [2017], but the teacher has no access to the internal parameters of the student agent. The observation is the reward of the student obtained on the currently selected task. The reward is the average reward of the student evaluated on all tasks at the end of a teacher step. One could also optimize for the reward of the target task, but initially, the student might not archive a reward on the target task leaving the teacher without a meaningful signal. A comprehensive survey of task sequencing methods is provided by Narvekar et al. [2020]. Chapter 3 Methods In this chapter, we describe the methods used in this thesis. We characterize our curriculum learning approach along the six dimensions introduced in section 2.4. Next, we detail our transfer method and the automated curriculum sequencing approach. We propose multiple types of observations and reward signals for our curriculum MDP. Finally, we describe our set of baselines and the evaluation protocol. 3.1 Curriculum Learning In this section, we characterize our curriculum learning approach. We use a tasklevel sequence curriculum. A combination of the task-level curriculum de\ufb01ned in De\ufb01nition 2.3.3 and a sequence curriculum (see De\ufb01nition 2.3.4). Our curriculum learning has the following properties: 1. Intermediate task generation: The tasks, represented as nodes in the curriculum graph, are prede\ufb01ned before training. We use a subset of the tasks provided by the grid world environment and the Google Research Football environment. We initially selected the tasks and kept this selection \ufb01xed throughout this thesis. The exact task selection is described in section 4.2.1 and section 4.4.1. 2. Curriculum representation: We represent the curriculum as a task-level sequence. We only allow one-to-one knowledge transfer between our source tasks and the target task. A source task can be visited multiple times in the sequence. Therefore the agent is allowed to retrain on already known tasks. 3. Transfer method: We experiment with three transfer methods. First, we copy the policy and value function weights from task to task to transfer the learned policy and value function. Second, the learned value function, obtained by training on the previous task, calculates an additional reward signal that we added to the environment\u2019s reward signal. Third, we experiment with a combination of the \ufb01rst and second method. In section 3.1.1 we go into detail about our knowledge transfer methods. 4. Curriculum sequencer: In this work, we experiment with automated 23 24 3.1. Curriculum Learning task sequencers. We formulate the task sequencing problem as a curriculum Markov Decision Process where we have full control over the student\u2019s MDP. In section 3.2 we formalize the CMDP and explain our approach in detail. 5. Curriculum adaptivity: We use an adaptive curriculum. The teacher performs task sequencing online to select tasks with a suitable learning potential for the current learning stage. Our set of source tasks is de\ufb01ned beforehand and kept \ufb01x throughout training. 6. Evaluation metric: We evaluate our agents using a weak transfer with the asymptotic performance improvement. We compare our work against agents trained in Schraner [2020], which uses manually de\ufb01ned curricula. 3.1.1 Transfer Method We use two types of knowledge transfer methods: policy and value function transfer and reward shaping. Our student agent starts with a random initialized policy and value function. After training for a \ufb01xed amount of steps at time-step t on a task mt, we switch to a new source task mt+1. The weights of the policy and value function obtained by training on task mt are used to initialize the agent before training on task mt+1. Our student uses weight sharing for the policy and value function. Therefore we copy all neural network weights from task to task. We assume that both the policy and value function transfer from task to task as the dynamics of the environment does not change in our set of tasks V. Therefore, a state with a true high value in one environment mi \u2208 V has also a true high value in another environment mj \u2208 V. The same applies to our policy. The environments in our set of source tasks di\ufb00er in their initial state distribution, state space and complexity. All tasks in V share the same set of actions A, environment dynamics function p and reward function r. It would also be possible to only transfer the weights of the shared embedding together with either the policy head or the value head from task to task. Like this, either the policy or the value are initialized randomly for each task. The gradients for the policy and the value function \ufb02ow through the same model in our network architecture with weight sharing. If either the policy or the value function is initialized randomly, this could lead to catastrophic updates to the shared weights. Therefore, we transfer all the neural network weights from task to task. Additionally to the policy and value function transfer, we experiment with a shaped reward signal. We use the value function vmt obtained by training on task mt as an additional reward signal when training on the next task mt+1. The reward ri at time-step i is therefore ri = ri + vmt(si) We expect that the value function can help to overcome the spares reward signal provided by the 25 3.2. Teacher Curriculum Markov Decision Process environment. For the \ufb01rst task m1 in our curriculum, we do not add a shaped reward signal as we do not have a value function at hand. Only the value function of the previous task is added as a shaped reward. One could also use the average of the last n value functions as a shaped reward signal. We leave this experiment to future work. The two transfer methods described can be combined, leading to a transfer method with policy and value function transfer and a shaped reward signal. We also experiment with this combined transfer method. 3.2 Teacher Curriculum Markov Decision Process In this section, we formulate the sequencing problem as a Markov Decision Process. In this formulation, we de\ufb01ne curriculum learning as an interaction between two types of MDPs. The \ufb01rst MDP is the standard RL MDP modeling the student interacting with a task. The second one is a higher level meta-MDP called curriculum MDP (CMDP). We use the CMDP to model a teacher selecting tasks for the student. The CMDP is a 4-tuple (S, A, p, r), where S is the set of all possible states equal to all possible policies the student can represent. The action space A is the set of tasks the teacher can assign to the student. Taking an action in the CMDP equals to an entire training cycle on the selected task in the student MDP for a \ufb01xed amount of steps. The environment\u2019s dynamics function p models the transition from one student policy to the next student policy after taking an action in the CMDP. The dynamics function is unknown. The reward for the CMDP is de\ufb01ned by the reward function r. In section 3.2.2 we detail two reward signals used in this research. The interaction between the CMDP and the student MDP is shown in \ufb01g. 3.1. The teacher selects a task, and the student trains for n steps on the selected task. After the student training step, we evaluate the student on a set of evaluation tasks and feed the evaluation result to the CMDP. In our research the set of evaluation tasks Veval is equal to the learning tasks Vlearn. Now that we have de\ufb01ned the sequencing problem as a CMDP, we can use reinforcement learning to \ufb01nd an optimal policy. We are using PPO to learn a teacher policy while training the student at the same time. With this approach, we have to deal with noise from the student training when performing suboptimal task switches, especially at the beginning of our training procedure. The teacher has to learn how to perform task sequencing while the student has to learn how to solve the selected environment. Both are acting randomly, which may lead to the teacher selecting too tricky tasks, and the student has a hard time learning from that task. 26 3.2. Teacher Curriculum Markov Decision Process Figure 3.1: Teacher-student interaction for task sequencing. 3.2.1 Teacher Observation We experiment with di\ufb00erent types of observations for our teacher agent. The state of the CMDP is the current policy of the student agent. We are using a neural network to approximate the optimal student policy \u03c0\u2217 s. Therefore the student weights \u03b8s are the state of the CMDP, and they ful\ufb01ll the Markov property. It is unclear how we should feed this state representation into our teacher agent. We could \ufb02atten the weights \u03b8s to a feature vector, but this would leave us with an enormous input vector. In our case, we have roughly 400\u2032000 weights in our student network. Using such a large input vector is not feasible due to memory limits. Additionally, we assume that such a feature vector is not easy to interpret for our teacher agent. It is hard to relate \u03b8s to the student\u2019s performance and the optimal next task to select. We use principal component analysis (PCA) [F.R.S., 1901] to \ufb01nd a reduced representation for \u03b8s. We build a training set containing student weights at di\ufb00erent training stages and then \ufb01t PCA to this dataset. This dataset is obtained by saving the weights of the student network for di\ufb00erent tasks at di\ufb00erent learning stages. At the end of each student training iteration, we do dimensionality reduction to the \ufb01rst n principal components and use this reduced representation as an input for our teacher. There are other ways to get a reduced representation for \u03b8s like model distillation or auto-encoders, each with its downsides. We leave the question for an optimal way to bring \u03b8s into a meaningful representation to future work. In this work, we refer to this observation type as pca input (PCA). In our work we also experiment with partial observable curriculum MDPs. We work with six di\ufb00erent types of manual de\ufb01ned observation types: reward history (RH), previous task reward (PTR), learning progress (LP), absolute learning progress (ALP), exponential moving average (EMA), and fast and slow exponential moving average (FS-EMA). 27 3.2. Teacher Curriculum Markov Decision Process Reward History : Instead of using the students weights \u03b8s we try to represent the learning potential per task. We can use a tuple of the student\u2019s average reward, obtained in the evaluation cycle at the end of a CMDP step, on each task together with the time-step when this task was last sampled: RH = {(rm eval, tm last sampled)|m \u2208 V} (3.1) Where rm eval is the average reward on all evaluation episodes for task m and tm last sampled is the CMDP time-step when task m last has been sampled. If the task m never has been sampled tm last sampled equals to 0. Previous Task Reward : Instead of using the average rewards of all tasks, we can input the last selected task, one-hot encoded, together with the average reward on that task, obtained in the evaluation cycle at the end of a CMDP step. Learning Progress : The learning progress is de\ufb01ned as the di\ufb00erence between the average reward, obtained in the evaluation cycle at the end of a CMDP step, of the current and the previous time step per task: LP = {(rm t \u2212 rm t\u22121)|m \u2208 V} (3.2) Where rm t is the average reward on the task m at time-step t and rm t\u22121 is the average reward on task m at the previous time-step t \u2212 1. Absolute Learning Progress : ALP is simply the absolute value of LP: ALP = {(|rm t \u2212 rm t\u22121|)|m \u2208 V} (3.3) The intuition behind using the absolute value is that a task at the stage of forgetting, resulting in a negative LP, should be treated similarly to a task with a steep learning curve. This learning progress representation is inspired by Portelas et al. [2019]. Exponential Moving Average : We can interpret the history of evaluation rewards after every CMDP cycle as a time-series. We use an exponential moving average over the history of rewards to estimate the next reward. Depending on the \u03b1 \u2208 [0, 1] value used to calculate the EMA we assign more weight on recent samples than on old ones. The exponential moving average over a vector x is de\ufb01ned as: ema(xt) = \ufffd \u03b1 \u2217 xt + (1 \u2212 \u03b1)ema(xt\u22121), t > 1 x1, t = 1 (3.4) Where ema(xt) is the value of the EMA at any time period t and xt is the value at a time period t. Therefore the EMA input is: EMA = {[ema(xm t )]|m \u2208 V} (3.5) 28 3.2. Teacher Curriculum Markov Decision Process xm is the history of the average reward, obtained in the evaluation cycle at the end of a CMDP step, for task m at all CMDP time-steps. The last time step is denoted as t. Fast and Slow Exponential Moving Average : Kanitscheider et al. [2021] introduce a smoothed EMA version, where they calculate a fast and a slow EMA with a high \u03b1 and a low \u03b1 respectively. The fast and slow EMA is then de\ufb01ned as the di\ufb00erence between the two EMAs: FS-EMA = {[emafast(xm t ) \u2212 emaslow(xm t )]|m \u2208 V} (3.6) In our work, we wanted to test if this method is superior to a normal EMA with a tuned \u03b1 value. 3.2.2 Reward Signal In reinforcement learning, a meaningful reward signal is crucial for success. We need to de\ufb01ne a reward signal for our CMDP that encodes our intention and is rich enough for the teacher agent to learn fast. The goal of the teacher agent is to perform task sequencing such that the asymptotic reward on the target task increases. Therefore we can use the student\u2019s average reward on the target task after a CMDP step as a reward signal. We call this reward signal target task reward. Typically, the target task is hard to solve. Therefore the reward at the beginning of training is usually 0. This reward signal is not meaningful for the teacher as it does not tell if the student is making progress in easier environments or if the student is completely lost. Such a sparse reward signal leads to less sample e\ufb03ciency and as samples in the CMDP are extremely costly, we want our teacher to learn fast. We assume that the source tasks in V are related to the target task. Therefore, if our student achieves a reward on the source tasks, this is a step towards solving the target task. If this is true, then we can overcome the sparse reward signal by de\ufb01ning a new reward signal source task reward: rteacher = \ufffd m\u2208V rm (3.7) Where rm is the average reward on task m obtained in the evaluation cycle at the end of a CMDP step. 29 3.2. Teacher Curriculum Markov Decision Process 3.2.3 Action The action space in the CMDP contains all tasks in V. Taking an action equals to selecting a task m \u2208 V, changing the student\u2019s environment to the selected task, and then train the student for a prede\ufb01ned amount of steps. After the training, the student is evaluated on all tasks in V and the evaluation results are passed to the CMDP agent. The evaluation is carried out according to the section 3.4. In our experiment, we train the student for 100\u2032000 steps. One could also train the student until convergence, surpassing a threshold, or make the number of steps part of the teacher\u2019s action space. It is unclear when the student will converge, which could lead to very compute-intensive CMDP steps, and if the agent can surpass the threshold, which could lead to an in\ufb01nite CMDP step. Therefore, we do not consider these two approaches. We want to keep the teacher\u2019s action space as simple as possible. Therefore we leaf integrating the number of student training steps into the action space for future work. 3.2.4 Network Architecture We keep the teacher network architecture simple. We use a multilayer perceptron (MLP) with three hidden layers with 64, 128, and 64 nodes, respectively, and a ReLU [Agarap, 2018] activation function. We make use of weight sharing as we do it for our student agent. The policy and value function are represented as individual heads on top of the MLP feature embedding. We also experiment with LSTMs [Hochreiter and Schmidhuber, 1997] in order to provide the teacher a memory. In this setting, we use the same MLP for the feature embedding and then feed the feature vector into an LSTM with a hidden state of 128. The hidden state is then used as an input for the policy and value function heads. Due to computation limits, we were not able to tune the network architecture. Another network architecture likely yields better results. 30 3.3. Baselines 3.3 Baselines We compare our teacher task sequencing approach against four baselines: uniform, LP sampling, window and thompson sampling. The last two are introduced in Matiisen et al. [2019]. Uniform : This is the simplest baseline. We select the next task by uniformly sampling them. LP sampling : In this baseline, we select the task with the highest learning progress as de\ufb01ned in eq. (3.2). If two or more tasks have the same LP, we sampling one of those tasks uniformly. Thompson sampling : Similar to LP sampling we sample the next task according to a learning progress metric. In the Thompson sampling baseline, we sample the task with the highest average reward after the last CMDP step. Window : In the window algorithm, we approximate the learning progress with linear regression. We store the history of average evaluation rewards. At each CMDP step, we \ufb01t a linear regression to the reward history. The regression coe\ufb03cient per task is used to estimate the steepness of the learning curve. The task with the steepest learning curve is selected. 3.4 Evaluation Protocol The goal of the teacher task is to improve the asymptotic performance. We evaluate our approach using a weak transfer to allow comparison with the results on the Google Research Football environment from our previous work [Schraner, 2020]. The asymptotic performance is de\ufb01ned as the increase in the average reward at the end of training. Additionally, we evaluate how well the trained student agent generalizes across the environments. The agent is trained overall environments, and it is interesting how well the agent performs on those environments. To measure the generality, we calculate the total average return, which is the sum of all average returns for all tasks in V, and the percentage of environments solved. We declare an environment as solved when the average return is greater than zero. After every CMDP step, we evaluate the student on every task in V for 100 episodes. Chapter 4 Experiments and Results In this chapter we describe the experiments used to evaluate our methods. We begin with an introduction of the environments used, the experimental setup and then provide results of our baselines and teacher student experiments. In depth experiments are carried out on a grid world environment, the most promising settings are then evaluated on the Google Research Football environment. In the following chapters, we use the terms task and environment interchangeably. In our case, the environment is equal to the task to solve, e.g., there is only a single task per environment. The description of the Google Research Football environment is taken from our previous work [Schraner, 2020]. 4.1 Grid World Grid world is a simple, lightweight, and fast environment for reinforcement learning. In a Grid world environment, the agent must reach a target destination by navigating through a maze. The di\ufb03culty of the environment ranges from very simple empty grids to complex mazes where the agent has to \ufb01nd keys in a speci\ufb01c order to unlock the target destination. We use the minimalistic grid world (MiniGrid) implementation by Chevalier-Boisvert et al. [2018]. In MiniGrid, the world is an NxN grid of tiles. Each tile in the grid world contains zero or one object, and each object has an associated discrete color and type. The object types are wall, \ufb02oor, door, key, ball, and goal. Doors have a state open, closed, or locked and behave according to this state. To open a locked door, the agent has to carry a key matching the door\u2019s color. The agent can pick up and carry exactly one object (e.g., ball or key). The simplicity of the grid world environment allows fast iteration and testing of multiple ideas. Training a student agent in the CMDP setting in the grid world environment takes around 18 hours. Training a teacher in the proposed CMDP setting is computationally expensive as we have to train a student agent. The set of environments with their di\ufb00erent levels of complexity is helpful for the curriculum learning setting. Depending on the selected environment, the maximum number of steps changes. An epoch in the simplest environment, Empty-5x5, ends after 100 steps, the most di\ufb03cult environment, KeyCorridor-S6R3, terminates after 1080 steps. 31 32 4.1. Grid World 4.1.1 Grid World Environments We describe the MiniGrid environments in table 4.1 and illustrate them in \ufb01g. 4.1. In a grid world environment, the agent has to navigate through a maze and solve some puzzles to reach the green goal square. In the KeyCorridor environment, the agent has to pick up a hidden ball behind a locked door. To unlock that door, the agent must \ufb01nd the matching key, which is hidden in another room. The agent has to explore the environment and move through open doors to \ufb01nd the hidden key. The bottom row of \ufb01g. 4.1 displays two di\ufb00erent KeyCorridor environments. Figure 4.1: Visualization of a subset of the MiniGrid environments used in this work. The environment names from top left to bottom right: Empty-6x6, FourRooms, DoorKey-16x16, MultiRoom-N4S5, KeyCorridor-S3R2, KeyCorridorS3R3, and KeyCorridor-S6R3. 4.1.2 State & Observations MiniGrid supports a variety of observation types. In our work, we use a fully observable view of the environment. This view has a dimension of N \u00d7 N \u00d7 3, where N is the dimension of the grid world. These values are not pixels, the last channel is encoding the tile at the (X, Y) location. The tile encoding is a threedimensional tuple: (OBJECT INDEX, COLOR INDEX, STATE). Only doors and agents have a state value other than 0. The door state 0 represents an open, 1 a closed, and 2 a locked door. The agents\u2019 state indicates the direction of the agent. We are transferring the policy and value function weights from environment to environment. Therefore the input dimension must stay the same for every environment. We apply padding to have a 25 \u00d7 25 grid world independent of the selected environment. As padding values, we use (1, 0, 0), which is equivalent to a wall. 33 4.1. Grid World Name Description Empty-[X]x[X] This environment is an empty room. Upon reaching the green goal square, the student receives a sparse reward. The agent is starting in a random position. The value of X de\ufb01nes the grid size. We use a 5 by 5, 6 by 6, 8 by 8, and 16 by 16 grid. FourRooms The agent must navigate in a maze of four rooms. Four gaps in the walls connect the rooms. To obtain a reward, the agent must reach the green goal square. Both the agent and the goal square are placed randomly in any of the four rooms. DoorKey-[X]x[X] The agent must pick up a key in order to unlock a door and then reach the green goal square. Due to the sparse reward signal, this environment is di\ufb03cult to solve. The door, wall, agent and green goal square are placed randomly. The value of X de\ufb01nes the grid size. We use a 5 by 5, 6 by 6, 8 by 8, and 16 by 16 grid. MultiRoom-N[X]S[Y] The environment has a series of connected rooms with doors that must be opened in order to get to the next room. The \ufb01nal green goal square is located in the last room. The rooms are all created randomly. This environment is challenging to solve using RL alone. The value of X de\ufb01nes the number of rooms and X the room size. We use N2-S4, N4-S5, and N6-S10 for our experiments. KeyCorridorS[X]R[Y] The agent has to pick up an object which is behind a locked door. The key is hidden in another room, and the agent has to explore the environment to \ufb01nd it. The key, ball, and agent are placed randomly. Due to the exploration required this is a challenging environment. The value of X de\ufb01nes the room size and Y number of rows (see \ufb01g. 4.1). We use an S3R1, S3R2, S3R3, S4R3, S5R3, S6R3 grid. Table 4.1: Description of the MiniGrid environments used in this thesis. All environments impose a penalty for the number of steps taken until reaching the target. 34 4.1. Grid World 4.1.3 Actions The action space in MiniGrid consists of six actions: Turn left, turn right, move forward, pickup, drop and toggle (open door, interact with objects). 4.1.4 Rewards The agent receives a reward of 1 for reaching the goal square and 0 otherwise. A penalty for the number of steps required to reach the target location is imposed. If the agent takes more steps to reach the target, the reward decreases. The grid world reward upon success is calculated according to this equation: r = 1 \u2212 0.9 \u2217 (step count/max steps) (4.1) Where step count is equal to the number of steps taken to reach the goal square and max steps is the maximum number of steps allowed per episode. In \ufb01g. 4.2 we ilustrated the reward calculation for two di\ufb00erent trajectories in the DoorKey-8x8 environment. Figure 4.2: A blue line illustrates the agent\u2019s trajectory in the DoorKey-8x8 environment. The maximum number of steps in the DoorKey-8x8 environment is 640. If the agent takes the direct path, he takes 18 steps until he reaches the green goal square. Therefore the agents reward is 1\u22120.9\u2217(18/640) = 0.9747. The agent on the right takes 28 steps, leaving him with a reward of 1\u22120.9\u2217(28/640) = 0.9606. 4.1.5 MDP Statement Depending on the type of observation used, the grid world environment does not ful\ufb01ll the Markov property. Some observation types only provide a limited \ufb01eld of view. To ease the problem, we only use a fully observable input for all of our student agents. Each observation at every time step fully describes the environment\u2019s state space, and the following environment state is entirely dependent on the current state. 35 4.2. Grid World Experiments 4.2 Grid World Experiments 4.2.1 Experimental Setup In this section, we detail the experimental setup applied to all our grid world experiments. Network architecture Our student agent uses a fully connected network with ReLU activation functions and separate policy and value function heads. The network architecture is depicted in \ufb01g. 4.3. Figure 4.3: Student neural network architecture for the MiniGrid environments. The input is a 25\u00d725\u00d73 fully observable representation of the environments state. After every fully connected layer we use a ReLU activation function. The policy head uses a softmax activation function for the action probability distribution. The value head does not use an activation function. We also evaluated a CNN network architecture. In the appendix A.1 we provide details about the CNN architecture. The 25 \u00d7 25 \u00d7 3 grid world observation has location-dependent features. Therefore by intuition we expect that CNN models work better than MLP models. Flattening the observation into a vector makes it harder to discover patterns and generalize over the state space. As we can see in \ufb01g. 4.4, using a CNN network architecture instead of the MLP architecture improves results on harder environments. MLP models are less noisy than CNN models, especially in the case of DoorKey experiments. We value stability over best possible results in our thesis because reward changes in our CMDP setting also lead to a lot of noise, therefore we use MLP models for the rest of our experiments. 36 4.2. Grid World Experiments Empty-5x5 Empty-6x6 Empty-8x8 Empty-16x16 FourRooms DoorKey-5x5 DoorKey-6x6 DoorKey-8x8 KeyCorridorS3R1 MultiRoom-N2-S4 Environment 0.0 0.2 0.4 0.6 0.8 1.0 Return Average CNN vs MLP model comparison Model MLP CNN Figure 4.4: Comparison of the average return over 100 episodes at the end of training between MLP and CNN models. The agent is trained with PPO for 10 million steps on a single environment. No Curriculum Experiments For all MiniGrid environments, we trained an agent with PPO for 10 million steps and evaluated the agent for 100 episodes at the end of training. We used these experiments to tune hyperparameters as well as the network architecture. In appendix A.2 we report the hyperparameters. Additionally, we used the results to select a subset of the MiniGrid environments for our curriculum learning experiments. We removed the group of MiniGrid environments where our trained agents failed in solving the environment. In the case of KeyCorridor and MultiRoom, training an agent for the simplest version of those environments succeeded. Therefore we kept all KeyCorridor and MultiRoom environments in our task set V. All experiments were repeated three times under di\ufb00erent random seeds. In our results, we report maximum and standard deviations. Curriculum learning For both the baseline and the CMDP experiments, we perform 1\u2032000 teacher steps, where for each teacher step, the student is trained for 10\u2032000 steps in the selected environment. Therefore, the student agent is trained for 10 million steps in total. After each teacher step, the student is evaluated for 100 episodes in each task in V. We \ufb01xed those number of steps to have the same 37 4.2. Grid World Experiments training steps as we used in our no curriculum learning experiments. We had to balance the number of teacher updates and the number of student updates per CMDP step. Using fewer teacher steps might not be su\ufb03cient for the teacher to learn how to perform task sequencing but would allow our student more time to converge in the selected environment. Using more teacher steps is bene\ufb01cial for the teacher, but on the other hand, the student has less time to learn the selected task. This would provide the teacher with a noisy reward signal. We selected the teacher and student steps because they seem reasonable. In future work, the in\ufb02uence of those values should be evaluated. The network architecture for the teacher agent is described in section 3.2.4. We evaluated the MLP and LSTM network architecture in appendix A.5. The MLP architecture is superior to the LSTM architecture. The hyperparameters for the baselines as well as the CMDP teachers are described in appendix A.2 and appendix A.4. All experiments were repeated three times under di\ufb00erent random seeds. In our results, we report maximum and standard deviations. 4.2.2 Results Overview Table 4.2 shows the best agent\u2019s performance on each MiniGrid environment used in this thesis. For each environment, we report the average return of 100 evaluation episodes. The total mean return is de\ufb01ned as the sum over all average returns. The percentage of environments solved is de\ufb01ned as the number of environments with an average reward greater than zero divided by all environments. Overall, using no curriculum results in a higher average return on 8 out of the 19 tested tasks. In contrast, curriculum learning agents outperform standard RL agents in more complex tasks such as Empty-16x16, DoorKey16x16, and KeyCorridor-S6R3. Comparing the best CMDP agent to the best baseline agents we see, the CMDP agent outperforms the baselines regarding the total mean return and percentage of solved environments. In the curriculum learning approach, the agent spends less training time in a single environment than with no curriculum. In easy environments, using no curriculum learning and training on a single task yields better results than the curriculum learning experiments. Training for 10 Million steps in a single environment is better than learning in a curriculum without speci\ufb01cally targeting those easy environments. In the CMDP setting using the source task reward, the teacher is encouraged to maximize the student\u2019s average return over all environments and not a single environment, as is the case when using no curriculum. The following sections evaluate di\ufb00erent teacher reward signals, transfer methods, observation types, hyperparameters as well as the sample e\ufb03ciency and generality of trained agents. 38 4.2. Grid World Experiments Environment / Metric None Uniform LP Thompson Window CMDP Total mean return 1.75 2.71 2.83 2.27 4.44 % environments solved 50% 44% 39% 50% 33% 55% Empty-5x5 0.96 0.83 0.75 0.57 0.37 0.93 Empty-6x6 0.97 0.51 0.83 0.51 0.32 0.9 Empty-8x8 0.96 0.0 0.0 0.62 0.0 0.93 Empty-16x16 0.0 0.0 0.93 0.92 0.67 0.82 FourRooms 0.17 0.07 0.09 0.09 0.0 0.09 DoorKey-5x5 0.96 0.17 0.02 0.02 0.0 0.13 DoorKey-6x6 0.94 0.04 0.0 0.04 0.81 0.13 DoorKey-8x8 0.29 0.07 0.0 0.05 0.0 0.14 DoorKey-16x16 0.0 0.03 0.04 0.0 0.07 0.17 MultiRoom-N2-S4 0.14 0.02 0.04 0.02 0.03 0.15 MultiRoom-N4-S5 0.0 0.0 0.0 0.0 0.0 0.0 MultiRoom-N6-S10 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S3R1 0.07 0.0 0.0 0.0 0.0 0.05 KeyCorridor-S3R2 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S3R3 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S4R3 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S5R3 0.0 0.0 0.0 0.0 0.0 0.0 KeyCorridor-S6R3 0.0 0.0 0.0 0.0 0.0 0.01 Table 4.2: Highest average return over 100 episodes for each teacher type at the end of 1000 teacher steps. The None results are the average return over 100 episodes obtained when training only the environment of that row. 39 4.2. Grid World Experiments 4.2.3 Transfer Method In this section, we evaluate the e\ufb00ect of the transfer methods introduced in section 3.1.1. We evaluate policy transfer, reward shaping transfer, and both transfers combined. All teacher agents use the source task reward, as de\ufb01ned in section 3.2.2. The results for all seven teacher observation types are reported in table 4.3 and the four baselines in table 4.4. Figure 4.5 shows the learning curves of the reinforcement learning agents. All agents with policy transfer surpass agents with reward shaping or the combined knowledge transfer. The learning curve is noisy in policy transfer due to the environment changes, but the performance is steadily increasing. The performance of agents with policy transfer did not converge after 1000 teacher steps. For both reward transfer and the combined transfer, the performance converged during the \ufb01rst 50 to 100 CMDP steps and failed to improve from then on. The agent\u2019s performance with reward transfer scatters around a total mean return of 1 during the whole training. Figure 4.5: Learning curves for di\ufb00erent teacher observations and transfer methods. After each teacher step the sum of the students average return over 100 episodes for each environment in V is plotted. We plot the learning curve with the highest total reward at the end of training for every con\ufb01guration. Approximating the entire student state space by encoding the students\u2019 weights with PCA as described in section 3.2.1 fails. The results of the experiments with PCA observation space are very similar to the uniform baseline results. We, therefore, discard this method for the rest of our experiments. The best results are obtained using the PTR, EMA, and LP observations with a policy transfer. Using the fast-slow EMA as described in section 3.2.1 is not superior to standard EMA. In section 4.2.5 we evaluate the best \u03b1 value for the 40 4.2. Grid World Experiments EMA observation. Using the reward history (RH) is signi\ufb01cantly worse than the other observation types when using policy transfer. RH resulted in the highest total mean return when using reward transfer but su\ufb00ers from a high standard deviation. In general, the results with policy transfer are more stable compared to the results with reward transfer. Environment / Metric RH PTR LP ALP EMA FS-EMA PCA Policy Total mean return 2.34 \u00b1 0.24 4.44 \u00b1 0.33 4.17 \u00b1 0.33 3.35 \u00b1 0.33 4.35 \u00b1 0.42 3.72 \u00b1 0.22 1.64 \u00b1 0.24 % environments solved 55% 55% 55% 55% 55% 61% 44% Empty-16x16 0.58 \u00b1 0.27 0.07 \u00b1 0.03 0.9 \u00b1 0.27 0.08 \u00b1 0.03 0.82 \u00b1 0.34 0.91 \u00b1 0.38 0.0 \u00b1 0.0 FourRooms 0.11 \u00b1 0.03 0.13 \u00b1 0.03 0.11 \u00b1 0.02 0.1 \u00b1 0.04 0.08 \u00b1 0.01 0.06 \u00b1 0.03 0.08 \u00b1 0.02 DoorKey-16x16 0.05 \u00b1 0.01 0.04 \u00b1 0.02 0.07 \u00b1 0.03 0.13 \u00b1 0.05 0.17 \u00b1 0.06 0.06 \u00b1 0.03 0.03 \u00b1 0.01 MultiRoom-N2-S4 0.1 \u00b1 0.02 0.06 \u00b1 0.02 0.09 \u00b1 0.01 0.06 \u00b1 0.03 0.15 \u00b1 0.05 0.09 \u00b1 0.04 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Reward Total mean return 1.38 \u00b1 0.85 0.68 \u00b1 0.3 1.01 \u00b1 0.32 1.11 \u00b1 0.63 0.95 \u00b1 0.1 1.22 \u00b1 0.58 0.63 \u00b1 0.21 % environments solved 50% 44% 61% 50% 11% 50% 34% Empty-16x16 0.56 \u00b1 0.28 0.0 \u00b1 0.0 0.46 \u00b1 0.23 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 FourRooms 0.04 \u00b1 0.002 0.03 \u00b1 0.01 0.05 \u00b1 0.03 0.03 \u00b1 0.01 0.0 \u00b1 0.0 0.03 \u00b1 0.01 0.0 \u00b1 0.0 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.03 \u00b1 0.01 0.02 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.06 \u00b1 0.03 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.02 \u00b1 0.001 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.003 \u00b1 0.001 0.003 \u00b1 0.002 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.004 \u00b1 0.001 0.0 \u00b1 0.0 Both Total mean return 0.0 \u00b1 0.0 0.47 \u00b1 0.24 0.0 \u00b1 0.0 0.32 \u00b1 0.16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 % environments solved 0% 17% 0% 17% 0% 0% 0% Empty-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 FourRooms 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.02 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Table 4.3: We compare the di\ufb00erent knowledge transfer methods for each teacher observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return over 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. Agents with the combined knowledge transfer are failing to learn at all. In \ufb01g. 4.5 we see that the average return drops from around 0.7 to 0 during the \ufb01rst ten teacher steps and fails to recover from there. We visualized the synthetic reward signal added by the reward transfer in \ufb01g. 4.6 for the Empty-8x8 and Empty16x16 environment. Each square in the image equals a square in the grid world environment. The black squares are wall objects, and the element in the bottom right is the green goal square. High state values estimated by the value function obtained in experiments with only reward transfer (images on the left) are scattered randomly around the grid. The estimated state value is between 0.12 and 0.16. In the value function with a combined knowledge transfer, the state values are almost identical for all states and have an estimated value of 10 Million. We 41 4.2. Grid World Experiments believe that the agent starts to optimize and estimate its previous value function and this leads to an ever increasing value estimate. This extremely high additional reward overshadows the actual reward signal of the environment, making it impossible for the agent to learn how to solve the environment successfully. Empty-8x8 - Reward transfer 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 Empty-8x8 - Both transfers 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e7 Empty-16x16 - Reward transfer 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Empty-16x16 - Both transfers 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 1e7 Figure 4.6: We visualize the added reward, when using reward transfer (left column) and combined transfer (right column). For every state in the Empty8x8 (top row) and Empty-16x16 (bottom row) grid world environment we plot the added reward. A light color encodes a high added reward, black equals to zero added reward. Dark tiles at the end of the grid are walls, the black tile in the bottom right corner is the green goal square. In \ufb01g. 4.7 we plot the teacher\u2019s sampling probability distribution over the teacher timesteps next to the teacher\u2019s learning curve. During the \ufb01rst 100 episodes, the teacher samples tasks similar to a uniform distribution. The teacher favors the family of Empty grid world environments during the \ufb01rst 200 episodes. From then on, the teacher selects more challenging environments such as KeyCorridor and DoorKey environments. After roughly 400 teacher steps, the sample distribution does not show the noisy changes in the sample probabilities as at the beginning of the training. We interpret this as the teacher getting more con\ufb01dent in performing task sequencing. The teacher\u2019s learning curve is noisy but steadily increasing. There are some drastic drops in performance, probably due to selecting too challenging environments. The student always manages to recover after those harmful teacher actions. Table 4.4 shows the same properties as described above. The Thompson sam42 4.2. Grid World Experiments Figure 4.7: We visualize the sample probability distribution and learning curve of one teacher agent over its training time. The colors in the legends are ordered upside down to the order in the plot. pling experiment with reward transfer is interesting. This is the only experiment where the knowledge transfer with a reward signal surpassed agents with a policy transfer. Therefore, this approach to transfer learning is feasible but very noisy, which is also indicated by the high standard deviation for the Thompson sampling experiments with reward transfer. While uniformly sampling environments is a strong baseline, it still shows the worst performance across the baselines. In general, sampling environments with the highest learning progress estimate yields the strongest baseline results. 43 4.2. Grid World Experiments Environment / Metric Uniform LP Thompson Window Policy Total mean return 1.75 \u00b1 0.24 2.71 \u00b1 0.33 1.94 \u00b1 0.21 2.28 \u00b1 0.32 % environments solved 44% 39% 22% 34% Empty-16x16 0.0 \u00b1 0.0 0.92 \u00b1 0.23 0.0 \u00b1 0.0 0.67 \u00b1 0.14 FourRooms 0.07 \u00b1 0.02 0.09 \u00b1 0.02 0.0 \u00b1 0.0 0.0 \u00b1 0.0 DoorKey-16x16 0.03 \u00b1 0.01 0.04 \u00b1 0.01 0.0 \u00b1 0.1 0.07 \u00b1 0.03 MultiRoom-N2-S4 0.02 \u00b1 0.01 0.04 \u00b1 0.01 0.03 \u00b1 0.01 0.03 \u00b1 0.01 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Reward Total mean return 0.8 \u00b1 0.34 0.51 \u00b1 0.23 2.83 \u00b1 1.1 0.45 \u00b1 0.12 % environments solved 44% 11% 50% 22% Empty-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.92 \u00b1 0.36 0.0 \u00b1 0.0 FourRooms 0.03 \u00b1 0.01 0.0 \u00b1 0.0 0.09 \u00b1 0.05 0.0 \u00b1 0.0 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.06 \u00b1 0.02 0.0 \u00b1 0.0 0.02 \u00b1 0.01 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.01 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Both Total mean return 0.47 \u00b1 0.21 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.52 \u00b1 0.22 % environments solved 17% 0% 0% 17% Empty-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 FourRooms 0.05 \u00b1 0.01 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.02 \u00b1 0.01 DoorKey-16x16 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 MultiRoom-N2-S4 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Table 4.4: We compare the di\ufb00erent knowledge transfer methods for each baseline. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum the average students return of 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. 44 4.2. Grid World Experiments 4.2.4 Teacher Reward Signal In this section we evaluate the source task reward and the target reward. Both reward signals are de\ufb01ned in section 3.2.2. We use the KeyCorridor-S3R3 environment as the target task. The agents use a policy knowledge transfer between tasks. In table 4.5 we report experiments for both reward signals with six observation types. For all observation types, the reported results are better when using the source task reward except for the RH experiments. The reported total mean return has a lower standard deviation over three di\ufb00erent random seeds when using the source task reward instead of the target reward. Environment / Metric RH PTR LP ALP EMA FS-EMA Total Eval. Reward Total mean return 2.34 \u00b1 0.24 4.44 \u00b1 0.33 4.17 \u00b1 0.33 3.35 \u00b1 0.33 4.35 \u00b1 0.42 3.72 \u00b1 0.22 % environments solved 55% 55% 55% 55% 55% 61% Empty-16x16 0.58 \u00b1 0.27 0.07 \u00b1 0.03 0.9 \u00b1 0.27 0.08 \u00b1 0.03 0.82 \u00b1 0.34 0.91 \u00b1 0.38 FourRooms 0.11 \u00b1 0.03 0.13 \u00b1 0.03 0.11 \u00b1 0.02 0.1 \u00b1 0.04 0.08 \u00b1 0.01 0.06 \u00b1 0.03 DoorKey-16x16 0.05 \u00b1 0.01 0.04 \u00b1 0.02 0.07 \u00b1 0.03 0.13 \u00b1 0.05 0.17 \u00b1 0.06 0.06 \u00b1 0.03 MultiRoom-N2-S4 0.1 \u00b1 0.02 0.06 \u00b1 0.02 0.09 \u00b1 0.01 0.06 \u00b1 0.03 0.15 \u00b1 0.05 0.09 \u00b1 0.04 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Target Reward Total mean return 3.6 \u00b1 0.49 3.6 \u00b1 0.36 3.38 \u00b1 0.62 2.35 \u00b1 0.66 3.41 \u00b1 0.51 3.38 \u00b1 0.42 % environments solved 55% 55% 55% 55% 55% 61% Empty-16x16 0.58 \u00b1 0.27 0.07 \u00b1 0.03 0.24 \u00b1 0.08 0.5 \u00b1 0.1 0.72 \u00b1 0.21 0.63 \u00b1 0.08 FourRooms 0.11 \u00b1 0.03 0.13 \u00b1 0.03 0.11 \u00b1 0.04 0.1 \u00b1 0.04 0.05 \u00b1 0.01 0.06 \u00b1 0.02 DoorKey-16x16 0.05 \u00b1 0.004 0.02 \u00b1 0.01 0.0 \u00b1 0.0 0.08 \u00b1 0.02 0.09 \u00b1 0.03 0.04 \u00b1 0.01 MultiRoom-N2-S4 0.07 \u00b1 0.01 0.11 \u00b1 0.04 0.01 \u00b1 0.003 0.06 \u00b1 0.03 0.02 \u00b1 0.01 0.02 \u00b1 0.01 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0 Table 4.5: We compare the di\ufb00erent teacher reward signals for each teacher observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return over 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. In \ufb01g. 4.8 we plot the target reward signal along the steps in the CMDP. The reward signal for the EMA, FS-EMA, LP, and ALP experiments are very close to zero during training, therefore not providing the teacher agent with a helpful signal. For RH and PTR, the reward for the \ufb01rst 200 teacher steps scatters around 0.01 and 0.8 and then degenerates to the same reward signal as for the other observation types. Surprisingly, this reward signal provided enough information to the teacher to better perform task sequencing than the baselines. The target reward pushes the teacher agent towards solving the KeyCorridor45 4.2. Grid World Experiments S3R3 environment, but at the end of training the agent fails to solve this environment with both reward signals. With both approaches, we did not manage to solve the challenging KeyCorridor-S3R3 environment. Figure 4.8: Visualization of the target reward signal for each teacher observation type over the teachers training cycle. 4.2.5 Choosing Alpha for Exponential Moving Average In this section, we optimize the \u03b1 value for the exponential moving average. An \u03b1 value close to one favors current values over old values in a time series. We experimented with \u03b1 \u2208 [0.1, 0.3, 0.5, 0.7, 0.9] and reported the results in table 4.6. The teacher agents use a policy transfer and the source task reward. By looking at the results in table 4.6 it is hard to settle for an \u03b1 value. While an \u03b1 of 0.5 or 0.9 have the highest total mean returns, the results for challenging environments are the best when selecting an alpha value of 0.3. The experiments with a value of 0.7 and 0.9 su\ufb00er from a high standard deviation in the reported total mean return. In \ufb01g. 4.9 we plot the average learning curve over three di\ufb00erent random seeds for all \u03b1 values. An \u03b1 value of 0.1 yields the worst results. The learning curve of a value with 0.3 is below higher \u03b1 values but surpassed most of the other experiments in the last 20 CMDP steps. The learning curves for an \u03b1 value of 0.5, 0.7, and 0.9 follow each other closely. Our experiments do not allow us to select a clear winner. Because of the highest total mean return combined with a relatively small standard deviation, we propose to use an alpha value of 0.5. 46 4.2. Grid World Experiments Environment / Metric 0.1 0.3 0.5 0.7 0.9 Total mean return 3.49 \u00b1 0.43 3.01 \u00b1 0.39 4.35 \u00b1 0.42 3.89 \u00b1 0.58 4.23 \u00b1 0.95 % environments solved 50% 61% 55% 55% 61% Empty-16x16 0.32 \u00b1 0.06 0.58 \u00b1 0.27 0.82 \u00b1 0.34 0.97 \u00b1 0.4 0.94 \u00b1 0.28 FourRooms 0.08 \u00b1 0.01 0.13 \u00b1 0.03 0.08 \u00b1 0.01 0.09 \u00b1 0.04 0.06 \u00b1 0.01 DoorKey-16x16 0.0 \u00b1 0.0 0.05 \u00b1 0.02 0.17 \u00b1 0.06 0.0 \u00b1 0.0 0.02 \u00b1 0.01 MultiRoom-N2-S4 0.15 \u00b1 0.06 0.08 \u00b1 0.03 0.15 \u00b1 0.05 0.13 \u00b1 0.02 0.29 \u00b1 0.12 KeyCorridor-S3R3 0.0 \u00b1 0.0 0.01 \u00b1 0.004 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.004 \u00b1 0.002 Table 4.6: We compare the di\ufb00erent \u03b1 values for the EMA observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return of 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. Figure 4.9: The average learning curve of three runs with three di\ufb00erent random seeds for each \u03b1 value. 47 4.2. Grid World Experiments 4.2.6 Sample E\ufb03ciency In this section, we analyze the sample e\ufb03ciency of an agent when trained in the teacher-student setting compared to trained directly on the target task. Learning curves compare the agent\u2019s experience with their performance. We use the teacher step on the x-axis to measure the experience. One teacher step equals 10\u2019000 steps in an environment. The performance is measured by the average return on 100 evaluation episodes during the training. In \ufb01g. 4.10 we plot three learning curves, one for each of the following environments: Empty-16x16, DoorKey16x16, and FourRooms. The sample e\ufb03ciency of an agent can be measured as the area under the curve with transfer minus the area under the curve without transfer. We do the evaluation manually by looking at the curves. In the Empty16x16 environment, the sample e\ufb03ciency of the agents with curriculum learning is negative for the \ufb01rst 400 teacher steps. After 400 teacher steps, the agent trained directly in the Empty16x16 environment su\ufb00ers from a catastrophic update and fails to recover from then on. It seems like the agent got stuck in a bad local optimum. This is probably due to a bad choice in the hyperparameters. We note that agents trained directly on the Empty16x16 environment with three random seeds showed the same behavior. In the DoorKey16x16 environment, all curriculum learning agents have a positive sample e\ufb03ciency. The learning curve is very noisy due to the training in di\ufb00erent environments. Although we notice an improvement in the sample e\ufb03ciency, the agent\u2019s performance is not monotonically increasing and dropping to the same performance level as when trained directly in the DoorKey16x16 environment. One could perform early stopping when crossing a reward threshold to solve this issue, but de\ufb01ning such a threshold is not straightforward. There is no improvement in the sample e\ufb03ciency in the FourRooms environment. Curriculum learning with the PTR teacher shows the best results with a slight improvement in sample e\ufb03ciency during some learning stages. 48 4.2. Grid World Experiments 0 200 400 600 800 1000 Teacher Step 0.0 0.2 0.4 0.6 0.8 1.0 Average Return Learning Curve - Empty-16x16 RH PTR LP ALP EMA FS-EMA None 0 200 400 600 800 1000 Teacher Step 0.0 0.1 0.2 0.3 0.4 0.5 Average Return Learning Curve - DoorKey-16x16 RH PTR LP ALP EMA FS-EMA None 0 200 400 600 800 1000 Teacher Step 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Average Return Learning Curve - FourRooms RH PTR LP ALP EMA FS-EMA None Figure 4.10: We visualize the learning curves of our agent when trained in the teacher-student setup and directly on the target task for the Empty-16x16, DoorKey16x16, and FourRooms environment. We use a teacher with policy transfer, source task reward signal, and the six partially observable inputs. The learning curve of the agent trained directly on the task is labeled with \u201dNone\u201d. 49 4.2. Grid World Experiments 4.2.7 Generality of Agents We compare the generality of agents trained with curriculum learning with agents trained in a single environment. To measure the generality, we evaluate the agent at the end of training on \ufb01ve di\ufb00erent random seeds for 100 episodes on each environment in the task-set V. After the evaluation, we calculate the total mean return and the percentage of environments solved with \ufb01ve random seeds and report the median of both measurements in table 4.7. Agents trained with curriculum learning are more general than agents trained in a single environment. With every teacher, we could solve more environments than training in one environment. The agent trained solely in Empty-6x6 scored a total mean return of 3.25. Most of the obtained reward is received in the easy Empty environment family. Some environments are especially useful to generalize to other environments. The FourRooms and MultiRoom-N2-S4 environment solve a high percentage of environments. 50 4.2. Grid World Experiments Environment trained on Total mean return % environments solved Empty-5x5 2.1 33% Empty-6x6 3.25 28% Empty-8x8 0.32 22% Empty-16x16 0.02 6% FourRooms 2.41 56% DoorKey-5x5 1.89 28% DoorKey-6x6 2.58 39% DoorKey-8x8 1.0 44% DoorKey-16x16 0.6 39% MultiRoom-N2-S4 1.48 56% MultiRoom-N4-S5 0.66 39% MultiRoom-N6-S10 0.99 33% KeyCorridor-S3R1 0.47 28% KeyCorridor-S3R2 0.53 28% KeyCorridor-S3R3 0.32 22% KeyCorridor-S4R3 0.21 11% KeyCorridor-S5R3 0.11 11% KeyCorridor-S6R3 0.72 28% Curriculum RH 2.64 56% Curriculum PTR 4.14 61% Curriculum LP 3.9 61% Curriculum ALP 3.06 67% Curriculum EMA 3.93 61% Curriculum FS-EMA 3.59 61% Table 4.7: We report the median measurements obtained by evaluating each agent with \ufb01ve random seed for 100 episodes on every environment in V. 51 4.3. Google Football Environment 4.3 Google Football Environment Google Research Football environment [Kurach et al., 2019] is a novel 3D RL environment to provide a highly optimized, stochastic, and open-source simulation. The environment provides single-agent RL, where the agent controls all players of his team, and multi-agent RL, where a separate agent controls each player. It is also possible to research the e\ufb00ect of self-play, where the agent plays against di\ufb00erent versions of itself. The environment provides a comprehensive set of progressively more demanding and diverse scenarios with the Football Academy. These scenarios enable us to analyze our algorithm on a range of tasks requiring di\ufb00erent levels of abstractions and di\ufb00erent tactics. The engine implements a full 11 vs. 11 football game with the standard rules including goal kicks, corner kicks, yellow and red cards, o\ufb00sides, handballs, and penalty kicks as shown in \ufb01g. 4.11. This full 11 vs. 11 football game, consisting of 3000 frames, is called Football Benchmark. Players have di\ufb00erent characteristics like speed or accuracy, but both teams have the same set of players. Further, players are getting tired over time, which in\ufb02uences their behavior and skills. Figure 4.11: The Google Football Engine is a football simulation which supports the major football rules like kicko\ufb00s (top left), goals (top right), fouls, cards (bottom left), corner and penalty kicks (bottom right), and o\ufb00side. [Kurach et al., 2019] 52 4.3. Google Football Environment 4.3.1 State & Observations There are three ways to represent the environment state at the current time step to the reinforcement learning agent (called observation). pixel. The representation is a 1280 x 720 x 3 tensor corresponding to the rendered screen. The scoreboard and a mini-map at the bottom of the image are present in this representation. The mini-map tells the location of the ball and the players of both teams. Super Mini Map (SMM). The SMM is a 72 x 96 x 4 tensor encoding information about both teams, the ball, and the currently active player. The encoding is binary and indicates whether there is a player or the ball at the given coordination or not. Floats. This representation uses a 115-dimensional vector to capture the game state, player coordinates, ball coordinates, possession, and the active player. 4.3.2 Actions The agent can execute one of 20 actions per time step. The currently active player executes all of those actions, except the keeper rush action. The active player moves by selecting one of eight dedicated moving actions. There are four di\ufb00erent ways to kick the ball (short pass, high pass, long pass, shot). The move, sprint, and dribble actions are sticky and have to be ended explicitly by their respective stop action. Finally, there are sliding and do-nothing actions. 4.3.3 Rewards The environment provides two di\ufb00erent reward functions to choose from. It is also possible to de\ufb01ne custom reward functions to look into reward shaping. In this work, we used the out-of-the-box reward functions. SCORING rewards the agent when scoring a goal with a +1 reward and a -1 reward when conceding one. This reward signal is sparse and can lead to no signal during the early stages of learning where the agent does not know how to overcome the opponent\u2019s defense. CHECKPOINT is an additional shaped reward designed to overcome the sparsity of the SCORING reward signal. Once per episode, the agent receives a +0.1 reward for getting closer to the opponent\u2019s goal measured by the Euclidean distance. The opponent\u2019s \ufb01eld is divided into ten checkpoint regions, and the agent receives the +0.1 reward once for every region. The agent also receives all non-collected checkpoint rewards when scoring to avoid penalizing agents that do not go through all the checkpoints before scoring. 53 4.3. Google Football Environment 4.3.4 Football Academy Scenarios In addition to the full 11 vs. 11 football game, the environment allows agents to train on a set of 11 progressively more complex scenarios. This set of tasks is called Football Academy where it is possible to de\ufb01ne custom scenarios to train agents for a particular situation. In table 4.8 the available scenarios are described. 4.3.5 MDP Statement The Google research Football environment does not ful\ufb01ll the Markov property if we only consider the pixel input at time step t as observation ot. We do not know in which direction the ball or the players are moving from a single observation. It is possible to have two identical observations, but in one observation, the ball moves to the left of the pitch, and in the other, it moves to the right. The direction and the velocity of the ball can not be inferred based on a single image. The agent only partially observes its environment\u2019s state, making it a partially observable Markov decision process (POMDP). There are a few possibilities to overcome this issue: \u2022 We can use another state representation than pixels. This representation would have to encode the direction and velocity of the ball and the players. \u2022 We can use a long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] inside the agents function approximator. The observation ot together with the hidden state of the LSTM ht are enough to overcome the uncertainty about the current state st of the environment. \u2022 We can use the last k observations to approximate the true state st. This is the solution proposed by Mnih et al. [2015]. In our work, we used the last four observations stacked on top of each other, illustrated in \ufb01g. 4.12, as input to approximate the true Markov state. Figure 4.12: To approximate the Markov property we use a frame stack of the last 4 frames as an input for our neural networks. 54 4.3. Google Football Environment Name Description Empty Goal Close Our player starts inside the box with the ball and needs to score against an empty goal. Empty Goal Our player starts in the middle of the \ufb01eld with the ball and needs to score against an empty goal. Run to Score Our player starts in the middle of the \ufb01eld with the ball and needs to score against an empty goal. Five opponent players chase ours from behind. Pass and Shoot with Keeper Two of our players try to score from the edge of the box. One is on the side with the ball and next to a defender. The other is at the center, unmarked and facing the opponent keeper. Run, Pass and Shoot with Keeper Two of our players try to score from the edge of the box. One is on the side with the ball and unmarked. The other is at the center, next to a defender, and facing the opponent keeper. Easy Counter-Attack 4 versus 1 counter-attack with keeper; all the remaining players of both teams run back towards the ball. Hard Counter-Attack 4 versus 2 counter-attack with keeper; all the remaining players of both teams run back towards the ball. 11 versus 11 with Lazy Opponents Full 11 versus 11 game, where the opponents cannot move but intercept the ball if it is close enough to them. Our center-back defender has the ball at \ufb01rst. The maximum duration of the episode is 3000 frames instead of 400 frames. 11 versus 11 easy Full 11 versus 11 game, with a duration of 3000 frames per episode. The game starts with a kicko\ufb00, the team starting with the kick-o\ufb00 is assigned randomly. The agent receives a reward of -1 when receiving a goal and a reward of +1 when scoring one. The opponents di\ufb03culty is set to easy. 11 versus 11 medium Same as 11 versus 11 easy, but the opponents di\ufb03culty is set to medium. 11 versus 11 hard Same as 11 versus 11 easy, but the opponents di\ufb03culty is set to hard. Table 4.8: Description of the Google Football environments taken directly from Kurach et al. [2019]. All scenarios end after 400 frames, or if the ball is lost, a team scores or the game stops (e.g., if the ball leaves the pitch or a free kick is awarded). 55 4.4. Google Football Experiments 4.4 Google Football Experiments 4.4.1 Experimental Setup The experimental setup for the Google Football environment is similar to the grid world environment. As in grid world, we train the teacher agent for 1\u2032000 steps, and for each teacher step, we train the student agent for 10\u2032000 steps in the selected environment. The student agent is therefore trained for 10 million steps in total. After each step, we evaluate the student on all environments in V for 100 episodes. In our previous work [Schraner, 2020], we train the student agent for 50 million steps. Due to a limit in computational resources, we limit the training in this thesis to 10 million student steps and only one random seed per experiment. Executing one experiment with the described amount of steps takes around \ufb01ve days. We \ufb01xed the knowledge transfer to policy transfer and used the source task reward signal. The hyperparameters are reported in appendix A.3. We use all Google Football environments listed in table 4.8. The student agent receives the SCORING reward signal. Network architecture The network architecture for the teacher agent is described in section 3.2.4. The student\u2019s network architecture is the same as in our previous work [Schraner, 2020] and depicted in \ufb01g. 4.13. Actor-critic algorithms can share the network torso and represent the policy and value function as individual heads. This allows weight sharing between the two function approximators. Sharing weights can improve the training time and forces the shared part to be more general. This may prevent over-\ufb01tting. As we share network weights, we get di\ufb00erent gradients from di\ufb00erent loss functions. In our work, we made use of sharing weights. Each network uses the SMM observation, with a shape of 72\u00d796\u00d74. The football \ufb01eld has a dimension of 72 \u00d7 96, and there is a separate channel for each team, the ball, and the currently active player. To approximate the Markov property, we use a frame stack of the last four frames, as shown in \ufb01g. 4.12. 56 4.4. Google Football Experiments Figure 4.13: CNN network architecture with the stacked Super Mini Map observation as input. The one-hot encoded previous action at\u22121 is used as an additional input. The policy and value function are two heads sharing the network torso. 57 4.4. Google Football Experiments 4.4.2 Results We transfer the best CMDP teacher settings from the MiniGrid environment to the Google Research Football environment. The four baseline methods and our results in our previous work are used to compare the results in table 4.9. We report the average return of 100 episodes per environment after training on one random seed. Therefore the results should be interpreted with care. The LP and Thompson baseline are by far the worst. In the case of Thompson sampling, only one out of eleven environments were solved. The LP teacher fails for the 11 vs. 11 full game. Sampling environments uniformly works well on easy environments but fails for more challenging ones such as the counterattack and 11 vs. 11 environments. The window baseline is the strongest of all baselines. With our proposed teacher-student setting we surpass our baselines on all environments. The LP based teacher had the best results on empty goal close, empty goal and run to score, but the improvement over the other experiments is marginal. Compared to the results in our previous work [Schraner, 2020] reported in table 4.10 we do not match the results. Training optimized PPO for 50 million steps leads to a return of -1.4 in the 11 vs. 11 hard environment. The highest score we obtained in this environment was -1.45 with the window baseline and LP teacher. Using a manually de\ufb01ned curriculum, we archive a return between -2.08 and 1, depending on the curriculum. Using the ALP, RH, PTR, and LP observation types leads to the best results. Whereas using ALP gives the highest total average and PTR the highest number of solved environments. The EMA-based teachers fall behind, probably due to a not-tuned \u03b1 value for the Google Football environments. ",
    "Experiment": "Experiments Environment / Metric Uniform LP Thompson Window RH PTR LP ALP EMA FS-EMA Total mean return -3.26 -11.62 -2.67 -0.81 -0.3 -0.28 -0.5 1.84 -1.36 -1.51 % environments solved 45% 27% 9% 55% 55% 64% 45% 45% 45% 45% Empty Goal Close 0.99 1 -0.05 1 0.96 0.98 1 0.94 0.79 1 Empty Goal 0.84 1 0.64 0.93 0.85 0.67 0.98 0.93 0.6 0.76 Run to Score 0.88 0.98 0.0 0.76 0.95 0.7 0.05 0.98 0.67 0.91 Pass and Shoot with Keeper 0.12 0.0 -0.02 0.0 0.33 0.03 0.0 0.51 0.0 -0.07 Run, Pass and Shoot with Keeper 0.02 0.0 0.0 0.0 -0.11 -0.02 0.0 0.05 0.0 -0.17 Easy Counter-Attack 0.0 -0.02 0.0 0.23 0.33 0.17 0.0 0.0 0.02 0.0 Hard Counter-Attack 0.0 0.0 0.0 0.22 0.18 0.05 0.03 0.0 -0.05 0.02 11 vs. 11 lazy 0.0 -0.1 0.0 0.05 0.0 0.02 0.16 0.0 0.12 0.08 11 vs. 11 easy -1.55 -3.8 -0.55 -0.53 -0.42 0.0 0.0 -0.23 -0.85 -0.55 11 vs. 11 medium -1.98 -5.28 -1.48 -1.23 -1.75 -1.2 -1.08 -0.65 -1.58 -1.78 11 vs. 11 hard -2.63 -4.98 -1.5 -1.45 -1.88 -1.85 -1.45 -1.7 -1.9 -1.7 Table 4.9: Google football results obtained on one random seed after training for 1\u2019000 CMDP steps, equal to 10 million student steps. The average return over 100 episodes is reported. We use policy transfer between the source tasks and the source task reward signal. Experiment Return PPO -1.4 Best scenarios curriculum -0.85 Best 11 vs 11 curriculum -2.08 Best increasing curriculum -0.48 Best smooth increasing curriculum 1 Prioritized level replay -1.05 Table 4.10: Comparison of our di\ufb00erent curriculum learning approaches of our previous work Schraner [2020], the average return over 100 episodes on the 11 vs 11 hard environment is reported. 59 4.4. Google Football Experiments In table 4.11 we report our results obtained in our previous work [Schraner, 2019] by training an agent with IMPALA in a single Google Football environments. With our curriculum learning approach, we can match and surpass the results of our previous work by only using a \ufb01fth of the samples. Compared to the baselines published with the Google Football environment [Kurach et al., 2019] we archived better results on the easy environments (Empty Goal, Run to Score, Pass and Shoot with Keeper) but slightly worse on the di\ufb03cult environments (Run, Pass and Shoot with Keeper, Easy Counter-Attack, Hard Counter-Attack, and 11 vs. 11 lazy). Scenario Ours@50M Google Baseline@50M Empty Goal Close 0.99 1.0 Empty Goal 0.84 0.86 Run to Score 0.88 0.88 Pass and Shoot with Keeper 0.0 0.66 Run, Pass and Shoot with Keeper -0.05 0.18 Easy Counter-Attack 0.0 0.5 Hard Counter-Attack -0.02 0.2 11 vs. 11 lazy 0.01 0.2 11 vs. 11 easy -1.59 -0.35 11 vs. 11 medium -1.6 -0.79 11 vs. 11 hard -2.78 -1.16 Table 4.11: Results on the Football Academy scenarios obtained in our previous work [Schraner, 2019] and the Google baseline [Kurach et al., 2019] with the IMPALA algorithm. The reported results is the average reward of 100 episodes after a training on 50 million frames. Chapter 5 Discussion Recent work in curriculum learning for rl has shown promising results in improving sample e\ufb03ciency and asymptotic performance in challenging environments. Most of these works focused on automatic task generation e.g., by using procedural generated environments [Wang et al., 2019], changing the initial state distribution to easy starts [Florensa et al., 2017], creating additional synthetic goals to guide the student [Racani`ere et al., 2020] or generating auxiliary tasks [Riedmiller et al., 2018]. Using self-play to create a curriculum was a core idea in AlphaStar [Vinyals et al., 2019]. Methods to automatically select tasks in a teacher-student setting are also popular. Most of those methods use heuristics or sampling-based approaches to select the tasks [Matiisen et al., 2019, Foglino et al., 2019a,b, Narvekar et al., 2017, Narvekar and Stone, 2019, Kanitscheider et al., 2021]. In our thesis, we propose a new approach to formulate the task sequencing problem as an MDP and train a teacher rl agent to perform the task sequencing while training the student simultaneously. Our experiments show that performing task sequencing with an rl teacher agent is superior to heuristic-based task sequencing. On the grid world environment, we were able to outperform all baselines signi\ufb01cantly. Our approach failed to increase the asymptotic performance on most of the MiniGrid when compared to tablua-rasa rl. Training an agent directly in easy grid world environments leads to better performance compared to curriculum learning approaches. Our Google Football experiments improved the asymptotic performance on easy environments such as Empty Goal, Run to Score, and Pass and Shoot with Keeper. We believe that the root cause of failing on challenging environments lies in the teacher\u2019s reward signal. Curriculum learning with the source task reward signal is in some cases bene\ufb01cial for more challenging environments such as DoorKey-16x16, but it is di\ufb03cult to target the teacher agent towards speci\ufb01c environments. Using a targeted reward signal to guide the teacher agent towards more challenging environments is often insu\ufb03cient, as the teacher faces a sparse reward signal in that setting. When providing the teacher with a sparse reward signal, our teacher-student settings collapses towards a uniform sampling approach. The teacher agent\u2019s learning curves did not converge after 1\u2019000 steps. Training both agents beyond 1\u2019000 CMDP steps might increase the asymptotic performance and help the student agent solve more challenging environments. This would increase the asymptotic performance at the cost of sample e\ufb03ciency. We 60 61 \ufb01xed the CMDP and student steps to match the number of training steps used in our no curriculum learning experiments to carry out a fair comparison. When specifying the number of teacher and student steps, we have to balance the teacher\u2019s and student\u2019s performance. Using fewer teacher steps might not be su\ufb03cient for the teacher to learn how to perform task sequencing but would give the student more time to converge in the selected environment. Using more teacher steps is bene\ufb01cial for the teacher, but on the other hand, the student has less time to learn the selected task. Using fewer student steps might lead to a noisy reward signal for the teacher agent. In future work, it would be interesting to analyze the importance and e\ufb00ect of those values. One could also train the student until convergence or make the number of student steps part of the teacher\u2019s action space. When comparing the three transfer methods policy, reward and combined, it turns out that the policy transfer leads to the best results. Reward transfer is volatile for both the rl teacher agents and the baselines. Combining reward transfer with policy transfer fails. After a few CMDP steps, the added reward bonus covers the environment\u2019s reward, forcing the student agent to maximize its reward signal. We compared six partial observable state representations and one approximately fully observable state representation (PCA) for the CMDP state. The PCA representation type failed utterly. There are other ways to represent neural network weights in a compact embeddings, such as autoencoders or network distillations. In future work, we can investigate other methods to represent the student weights. The results of the six partial observable representations did not di\ufb00er too much from each other. Overall the PTR, LP, and EMA representation resulted in the highest asymptotic rewards and the most general student agents for the MiniGrid environments. In our Google Football environment RH, PTR, LP and ALP achived the highest asymptotic reward and percentage of solved environments. In that environment tuning the \u03b1 value for the EMA representation might improve the results. The proposed curriculum learning approach is bene\ufb01cial for multi-task reinforcement learning. The reward across all tasks and the overall percentage of solved environments are signi\ufb01cantly higher compared to our baselines or when training an agent on a single environment. We investigated the sample e\ufb03ciency of our teacher-student setup in three grid world environments. The sample e\ufb03ciency improves in the Empty-16x16 and DoorKey16x16 environment but remains similar in the FourRooms environment. We discovered that training in the proposed curriculum setup is noisy regarding the student\u2019s performance throughout training. The sample e\ufb03ciency on the Google Football environments increased considerably. By only using one-\ufb01fth of the samples, we almost matched or surpassed the performance on 8 out of 11 en62 vironments compared to directly training in those environments. Training agents for the 11 vs. 11 football game remains di\ufb03cult with our proposed curriculum learning approach. The asymptotic performance remained the same compared to training directly in an environment. Using a careful, manually de\ufb01ned curriculum improves the asymptotic performance compared to our teacher-student setup. Our previous work concluded that de\ufb01ning such a curriculum by hand requires a lot of tweaking and domain knowledge. In this work, we analyzed di\ufb00erent knowledge transfer methods, teacher observation types, and reward signals. Using policy transfer combined with the PTR, LP, or EMA representation and the source task reward reward signal, we observe the following: \u2022 The generality of our agent improves. \u2022 The asymptotic performance in some environments increases compared to using no curriculum. \u2022 Training an rl agent to perform task sequencing is superior over our heuristicbased baselines. \u2022 The sample e\ufb03ciency compared to no curriculum does increase in one out of three analyzed grid world environments. \u2022 The sample e\ufb03ciency compared to no curriculum does increase by a factor of \ufb01ve on 8 out of 11 Google Football environments. The improvement in the generality and sample e\ufb03ciency with curriculum has to be investigated more carefully in future research. Another weakness of our work is the limited hyperparameter search. The e\ufb00ect of training teachers or students for more steps remains unclear. Other types of rl algorithms such as DQN or traditional tabular methods for the teacher agent have to be tested. The teacher network architecture should be tuned further. Our agents trained without curriculum learning on the grid world environments results in zero rewards for hard environments. To perform more meaningful analysis, we propose to tune those baseline agents and then transfer them to the teacher-student setting. It remains unclear if other PPO hyperparameters for the teacher training would improve performance and how well these parameters transfer across all methods. Chapter 6 Conclusion In this thesis, we successfully layout a teacher-student curriculum learning approach with automated online task sequencing to improve the generality and sample e\ufb03ciency of our rl agents. Our CMDP framework allows training a teacher and a student agent simultaneously, where the teacher selects tasks for the student. The student\u2019s asymptotic performance does not increase for most grid world and Google Football environments compared to non-cl agents. We evaluated multiple transfer methods, teacher reward signals, and CMDP observations on the MiniGrid and the Google Research Football environment. This work is a step towards successful multi-task reinforcement learning agents. We found that knowledge transfer through policy transfer works well and is robust across a variety of teachers. In future work, it might be interesting to experiment with additional knowledge transfer methods such as macro actions [Vezhnevets et al., 2016] and task models [Shao et al., 2019]. The choice of teacher observation is not as important as the transfer method. PTR, LP, ALP, and EMA proved to provide useful observations. The teacher\u2019s reward signal is essential for training the teacher fast. The source task reward provides the teacher agent with a rich reward signal and encourages to \ufb01nd a general student agent. With this reward signal, it is di\ufb03cult to target speci\ufb01c environments to solve, but using the target reward signal is too sparse. One could use a weighted form of the source task reward to lay focus on a subset of the tasks while still providing a rich reward signal. Another option would be a combination of both reward signals, where the source task reward signal is used at the beginning of training and slowly exchanged with the target reward. For future work, it would be interesting to experiment with di\ufb00erent rl algorithms to train the teacher. One could use traditional rl methods such as Q learning or o\ufb00-policy algorithms with experience replay such as algorithms from the DQN family. More work must be dedicated to the hyperparameter tuning of the rl algorithms, teacher-student step settings, and network architectures. Although our work did not increase the asymptotic performance on most of the MiniGrid and Google Football environments, we increased the overall performance across the tasks and the sample e\ufb03ciency. Future work should experiment with other domains such as robotics and further tune the hyperparameters to better understand our \ufb01ndings. 63 Bibliography D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016. D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, \u201cMastering the game of go without human knowledge,\u201d Nature, vol. 550, no. 7676, pp. 354\u2013359, 2017. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan, and D. Hassabis, \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, no. 6419, pp. 1140\u20131144, 2018. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. OpenAI, :, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. J\u00b4ozefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang, \u201cDota 2 with large scale deep reinforcement learning,\u201d 2019. O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfa\ufb00, Y. Wu, R. Ring, D. Yogatama, D. W\u00a8unsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver, \u201cGrandmaster level in starcraft ii using multi-agent reinforcement learning,\u201d Nature, vol. 575, no. 7782, pp. 350\u2013354, 2019. 64 65 Bibliography OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, \u201cSolving rubik\u2019s cube with a robot hand,\u201d 2019. A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell, \u201cAgent57: Outperforming the atari human benchmark,\u201d 2020. D. A. Pomerleau, ALVINN: An Autonomous Land Vehicle in a Neural Network. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1989, p. 305\u2013313. S. Ross, G. Gordon, and D. Bagnell, \u201cA reduction of imitation learning and structured prediction to no-regret online learning,\u201d ser. Proceedings of Machine Learning Research, G. Gordon, D. Dunson, and M. Dud\u00b4\u0131k, Eds., vol. 15. Fort Lauderdale, FL, USA: JMLR Workshop and Conference Proceedings, 11\u201313 Apr 2011, pp. 627\u2013635. C. Scheller, Y. Schraner, and M. Vogel, \u201cSample e\ufb03cient reinforcement learning through learning from demonstrations in minecraft,\u201d ser. Proceedings of Machine Learning Research, H. J. Escalante and R. Hadsell, Eds., vol. 123. Vancouver, CA: PMLR, 08\u201314 Dec 2020, pp. 67\u201376. R. S. Sutton, D. Precup, and S. P. Singh, \u201cIntra-option learning about temporally abstract actions,\u201d in Proceedings of the Fifteenth International Conference on Machine Learning, ser. ICML \u201998. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1998, p. 556\u2013564. P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d in Advances in Neural Information Processing Systems 5, S. J. Hanson, J. D. Cowan, and C. L. Giles, Eds. Morgan-Kaufmann, 1993, pp. 271\u2013278. R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Artif. Intell., vol. 112, no. 1\u20132, p. 181\u2013211, Aug. 1999. J. L. Elman, \u201cLearning and development in neural networks: The importance of starting small,\u201d Cognition, vol. 48, no. 1, pp. 71\u201399, 1993. A. Lazaric, M. Restelli, and A. Bonarini, \u201cTransfer of samples in batch reinforcement learning,\u201d in Proceedings of the 25th International Conference on Machine Learning, ser. ICML \u201908. New York, NY, USA: Association for Computing Machinery, 2008, p. 544\u2013551. A. Lazaric and M. Restelli, \u201cTransfer from multiple mdps,\u201d in Advances in Neural Information Processing Systems, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, Eds., vol. 24. Curran Associates, Inc., 2011. 66 Bibliography R. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Arti\ufb01cial Intelligence, vol. 112, pp. 181\u2013211, 1999. V. Soni and S. P. Singh, \u201cUsing homomorphisms to transfer options across continuous reinforcement learning domains,\u201d in Proceedings, The Twenty-First National Conference on Arti\ufb01cial Intelligence and the Eighteenth Innovative Applications of Arti\ufb01cial Intelligence Conference, July 16-20, 2006, Boston, Massachusetts, USA. AAAI Press, 2006, pp. 494\u2013499. A. S. Vezhnevets, V. Mnih, J. Agapiou, S. Osindero, A. Graves, O. Vinyals, and K. Kavukcuoglu, \u201cStrategic attentive writer for learning macro-actions,\u201d in Proceedings of the 30th International Conference on Neural Information Processing Systems, ser. NIPS\u201916. Red Hook, NY, USA: Curran Associates Inc., 2016, p. 3494\u20133502. A. Fachantidis, I. Partalas, G. Tsoumakas, and I. Vlahavas, \u201cTransferring task models in reinforcement learning agents,\u201d Neurocomputing, vol. 107, pp. 23\u201332, 2013, timely Neural Networks Applications in Engineering. F. Fern\u00b4andez, J. Garc\u00b4\u0131a, and M. Veloso, \u201cProbabilistic policy reuse for inter-task transfer learning,\u201d Robotics and Autonomous Systems, vol. 58, no. 7, pp. 866\u2013871, 2010, advances in Autonomous Robots for Service and Entertainment. M. E. Taylor, P. Stone, and Y. Liu, \u201cTransfer learning via inter-task mappings for temporal di\ufb00erence learning,\u201d Journal of Machine Learning Research, vol. 8, no. 1, pp. 2125\u20132167, 2007. M. E. Taylor and P. Stone, \u201cBehavior transfer for value-function-based reinforcement learning,\u201d in The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, F. Dignum, V. Dignum, S. Koenig, S. Kraus, M. P. Singh, and M. Wooldridge, Eds. New York, NY: ACM Press, July 2005, pp. 53\u201359. \u2014\u2014, \u201cTransfer learning for reinforcement learning domains: A survey,\u201d Journal of Machine Learning Research, vol. 10, no. 56, pp. 1633\u20131685, 2009. A. Lazaric, \u201cTransfer in reinforcement learning: A framework and a survey,\u201d in Adaptation, Learning, and Optimization. Springer Berlin Heidelberg, 2012, pp. 143\u2013173. Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum learning,\u201d in International Conference on Machine Learning, ICML, 2009. 67 Bibliography T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d in International Conference on Learning Representations, Puerto Rico, 2016. M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, \u201cHindsight experience replay,\u201d in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. Z. Ren, D. Dong, H. Li, and C. Chen, \u201cSelf-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 6, pp. 2216\u20132226, 2018. T. Kim and J. Choi, \u201cScreenernet: Learning curriculum for neural networks,\u201d CoRR, vol. abs/1801.00904, 2018. C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel, \u201cReverse curriculum generation for reinforcement learning,\u201d in Proceedings of the 1st Annual Conference on Robot Learning, ser. Proceedings of Machine Learning Research, S. Levine, V. Vanhoucke, and K. Goldberg, Eds., vol. 78. PMLR, 13\u201315 Nov 2017, pp. 482\u2013495. F. Foglino, C. C. Christakou, and M. Leonetti, \u201cAn optimization framework for task sequencing in curriculum learning,\u201d in 2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2019, pp. 207\u2013214. F. Foglino, C. Coletto Christakou, R. Luna Gutierrez, and M. Leonetti, \u201cCurriculum learning for cumulative return maximization,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-19. International Joint Conferences on Arti\ufb01cial Intelligence Organization, 7 2019, pp. 2308\u20132314. S. Narvekar, J. Sinapov, and P. Stone, \u201cAutonomous task sequencing for customized curriculum design in reinforcement learning,\u201d in Proceedings of the Twenty-Sixth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-17, 2017, pp. 2536\u20132542. T. Matiisen, A. Oliver, T. Cohen, and J. Schulman, \u201cTeacher-student curriculum learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u20139, 2019. S. Narvekar and P. Stone, \u201cLearning curriculum policies for reinforcement learning,\u201d in Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), May 2019. 68 Bibliography S. Narvekar, J. Sinapov, M. Leonetti, and P. Stone, \u201cSource task creation for curriculum learning,\u201d in Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016), Singapore, May 2016. J. Schmidhuber, \u201cPowerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem,\u201d Frontiers in Psychology, vol. 4, p. 313, 2013. M. Jiang, E. Grefenstette, and T. Rockt\u00a8aschel, \u201cPrioritized level replay,\u201d 2020. K. Kurach, A. Raichuk, P. Stanczyk, M. Zajac, O. Bachem, L. Espeholt, C. Riquelme, D. Vincent, M. Michalski, O. Bousquet, and S. Gelly, \u201cGoogle research football: A novel reinforcement learning environment,\u201d 2019. Y. Flet-Berliac, R. Ouhamma, O.-A. Maillard, and P. Preux, \u201cIs standard deviation the new standard? revisiting the critic in deep policy gradients,\u201d 2020. Y. Schraner, ReinforcementLearning on the GoogleFootball environment, 2020. R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. The MIT Press, 2018. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d CoRR, vol. abs/1707.06347, 2017. J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, \u201cTrust region policy optimization,\u201d ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37. Lille, France: PMLR, 07\u201309 Jul 2015, pp. 1889\u20131897. C. C. Hsu, C. Mendler-D\u00a8unner, and M. Hardt, \u201cRevisiting design choices in proximal policy optimization,\u201d CoRR, vol. abs/2009.10897, 2020. S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and P. Stone, \u201cCurriculum learning for reinforcement learning domains: A framework and survey,\u201d J. Mach. Learn. Res., vol. 21, pp. 181:1\u2013181:50, 2020. M. Svetlik, M. Leonetti, J. Sinapov, R. Shah, N. Walker, and P. Stone, \u201cAutomatic curriculum graph generation for reinforcement learning agents,\u201d Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, vol. 31, no. 1, Feb. 2017. R. Wang, J. Lehman, J. Clune, and K. O. Stanley, \u201cPoet: Open-ended coevolution of environments and their optimized solutions,\u201d in Proceedings of the Genetic and Evolutionary Computation Conference, ser. GECCO \u201919. New York, NY, USA: Association for Computing Machinery, 2019, p. 142\u2013151. 69 Bibliography K. P. F.R.S., \u201cLiii. on lines and planes of closest \ufb01t to systems of points in space,\u201d The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 2, no. 11, pp. 559\u2013572, 1901. R. Portelas, C. Colas, K. Hofmann, and P. Oudeyer, \u201cTeacher algorithms for curriculum learning of deep RL in continuously parameterized environments,\u201d in 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, ser. Proceedings of Machine Learning Research, L. P. Kaelbling, D. Kragic, and K. Sugiura, Eds., vol. 100. PMLR, 2019, pp. 835\u2013853. I. Kanitscheider, J. Huizinga, D. Farhi, W. H. Guss, B. Houghton, R. Sampedro, P. Zhokhov, B. Baker, A. Eco\ufb00et, J. Tang, O. Klimov, and J. Clune, \u201cMulti-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft,\u201d CoRR, vol. abs/2106.14876, 2021. A. F. Agarap, \u201cDeep learning using recti\ufb01ed linear units (relu),\u201d 2018, cite arxiv:1803.08375Comment: 7 pages, 11 \ufb01gures, 9 tables. S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. M. Chevalier-Boisvert, L. Willems, and S. Pal, \u201cMinimalistic gridworld environment for openai gym,\u201d https://github.com/maximecb/gym-minigrid, 2018. Y. Schraner, IP7 - Reinforcement Learning on the Google Football environment, 2019. S. Racani`ere, A. K. Lampinen, A. Santoro, D. P. Reichert, V. Firoiu, and T. P. Lillicrap, \u201cAutomated curriculum generation through setter-solver interactions,\u201d in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. van de Wiele, V. Mnih, N. Heess, and J. T. Springenberg, \u201cLearning by playing solving sparse reward tasks from scratch,\u201d in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. PMLR, 10\u201315 Jul 2018, pp. 4344\u20134353. K. Shao, Y. Zhu, and D. Zhao, \u201cStarcraft micromanagement with reinforcement learning and curriculum transfer learning,\u201d IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 3, no. 1, pp. 73\u201384, 2019. List of Figures 2.1 The agent-environment interaction in a Markov decision process [Sutton and Barto, 2018]. . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Performance metrics for transfer learning using (a) weak transfer and (b) strong transfer (\ufb01gure by Narvekar et al. [2020]). . . . . . 17 2.3 We visualize the interaction between the three key elements of curriculum learning. Task generation is concerned with generate a set of tasks for the curriculum. Task sequencing selects a task out of the task set for the agent. The agent use knowledge obtained by training on previously selected task through transfer learning. After or during training on the new task, the obtained knowledge is stored. The task generation and sequencing can be done online and dependent on the student\u2019s current performance or o\ufb04ine before training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.1 Teacher-student interaction for task sequencing. . . . . . . . . . . 26 4.1 Visualization of a subset of the MiniGrid environments used in this work. The environment names from top left to bottom right: Empty-6x6, FourRooms, DoorKey-16x16, MultiRoom-N4S5, KeyCorridorS3R2, KeyCorridor-S3R3, and KeyCorridor-S6R3. . . . . . . . . . 32 4.2 A blue line illustrates the agent\u2019s trajectory in the DoorKey-8x8 environment. The maximum number of steps in the DoorKey-8x8 environment is 640. If the agent takes the direct path, he takes 18 steps until he reaches the green goal square. Therefore the agents reward is 1\u22120.9\u2217(18/640) = 0.9747. The agent on the right takes 28 steps, leaving him with a reward of 1 \u2212 0.9 \u2217 (28/640) = 0.9606. 34 4.3 Student neural network architecture for the MiniGrid environments. The input is a 25\u00d725\u00d73 fully observable representation of the environments state. After every fully connected layer we use a ReLU activation function. The policy head uses a softmax activation function for the action probability distribution. The value head does not use an activation function. . . . . . . . . . . . . . . 35 4.4 Comparison of the average return over 100 episodes at the end of training between MLP and CNN models. The agent is trained with PPO for 10 million steps on a single environment. . . . . . . 36 70 71 List of Figures 4.5 Learning curves for di\ufb00erent teacher observations and transfer methods. After each teacher step the sum of the students average return over 100 episodes for each environment in V is plotted. We plot the learning curve with the highest total reward at the end of training for every con\ufb01guration. . . . . . . . . . . . . . . . . . . . . . . . . 39 4.6 We visualize the added reward, when using reward transfer (left column) and combined transfer (right column). For every state in the Empty-8x8 (top row) and Empty-16x16 (bottom row) grid world environment we plot the added reward. A light color encodes a high added reward, black equals to zero added reward. Dark tiles at the end of the grid are walls, the black tile in the bottom right corner is the green goal square. . . . . . . . . . . . . . . . . . . . 41 4.7 We visualize the sample probability distribution and learning curve of one teacher agent over its training time. The colors in the legends are ordered upside down to the order in the plot. . . . . . 42 4.8 Visualization of the target reward signal for each teacher observation type over the teachers training cycle. . . . . . . . . . . . . . . 45 4.9 The average learning curve of three runs with three di\ufb00erent random seeds for each \u03b1 value. . . . . . . . . . . . . . . . . . . . . . 46 4.10 We visualize the learning curves of our agent when trained in the teacher-student setup and directly on the target task for the Empty-16x16, DoorKey16x16, and FourRooms environment. We use a teacher with policy transfer, source task reward signal, and the six partially observable inputs. The learning curve of the agent trained directly on the task is labeled with \u201dNone\u201d. . . . . . . . . 48 4.11 The Google Football Engine is a football simulation which supports the major football rules like kicko\ufb00s (top left), goals (top right), fouls, cards (bottom left), corner and penalty kicks (bottom right), and o\ufb00side. [Kurach et al., 2019] . . . . . . . . . . . . . . . . . . 51 4.12 To approximate the Markov property we use a frame stack of the last 4 frames as an input for our neural networks. . . . . . . . . . 53 4.13 CNN network architecture with the stacked Super Mini Map observation as input. The one-hot encoded previous action at\u22121 is used as an additional input. The policy and value function are two heads sharing the network torso. . . . . . . . . . . . . . . . . . . . 56 A.1 Student CNN neural network architecture for the MiniGrid environments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 List of Tables 1.1 Comparison of our di\ufb00erent curriculum learning approaches, the average return over 100 episodes on the 11 vs 11 hard environment is reported. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.1 Description of the MiniGrid environments used in this thesis. All environments impose a penalty for the number of steps taken until reaching the target. . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.2 Highest average return over 100 episodes for each teacher type at the end of 1000 teacher steps. The None results are the average return over 100 episodes obtained when training only the environment of that row. . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.3 We compare the di\ufb00erent knowledge transfer methods for each teacher observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return over 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. . . . . . . . . . . . 40 4.4 We compare the di\ufb00erent knowledge transfer methods for each baseline. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum the average students return of 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. . . . . . . 43 4.5 We compare the di\ufb00erent teacher reward signals for each teacher observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return over 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. . . . . . . . . . . . . . 44 4.6 We compare the di\ufb00erent \u03b1 values for the EMA observation type. Each con\ufb01guration is run with three di\ufb00erent random seeds. The maximum return of 100 episodes at the end of 1\u2019000 teacher steps is reported. Additionally, we provide the standard deviation between the three runs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.7 We report the median measurements obtained by evaluating each agent with \ufb01ve random seed for 100 episodes on every environment in V. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.8 Description of the Google Football environments taken directly from Kurach et al. [2019]. All scenarios end after 400 frames, or if the ball is lost, a team scores or the game stops (e.g., if the ball leaves the pitch or a free kick is awarded). . . . . . . . . . . . . . 54 72 73 List of Tables 4.9 Google football results obtained on one random seed after training for 1\u2019000 CMDP steps, equal to 10 million student steps. The average return over 100 episodes is reported. We use policy transfer between the source tasks and the source task reward signal. . . . . 58 4.10 Comparison of our di\ufb00erent curriculum learning approaches of our previous work Schraner [2020], the average return over 100 episodes on the 11 vs 11 hard environment is reported. . . . . . . . . . . . 58 4.11 Results on the Football Academy scenarios obtained in our previous work [Schraner, 2019] and the Google baseline [Kurach et al., 2019] with the IMPALA algorithm. The reported results is the average reward of 100 episodes after a training on 50 million frames. . . . 59 A.1 Hyperparameters used for training a student in the CMDP setting on the grid world environments. . . . . . . . . . . . . . . . . . . . 75 A.2 Important \ufb01xed student hyperparameters used in our Google Football CMDP experiments. . . . . . . . . . . . . . . . . . . . . . . . 76 A.3 Hyperparameters used for training a teacher in the CMDP setting used in both the grid world and Google Football experiments. . . 77 A.4 Comparison of the MLP and LSTM teacher architecture. We report the total average return over 100 episodes on all environments in V at the end of training. . . . . . . . . . . . . . . . . . . . . . . 78 Appendix A Appendix A.1 Grid World CNN Architecture In \ufb01g. A.1 we depict the CNN network architecture for grid world environments. The network input is a 25 \u00d7 25 \u00d7 3 fully observable representation of the environment\u2019s state. We use four convolution blocks with [16, 16, 32, 64] channels, a [2\u00d72, 3\u00d73, 3\u00d73, 3\u00d73] kernel and a stride of [1, 1, 1, 3]. A ReLU activation function follows each convolution block. After the convolution blocks, we use a fully connected layer with 128 units. The policy and value function are separate heads on top of the fully connected layer. The policy head uses a softmax activation function. Figure A.1: Student CNN neural network architecture for the MiniGrid environments. 74 75 A.2. Grid World Student Hyperparameters A.2 Grid World Student Hyperparameters We optimized a limited number of hyperparameters for our students in the grid world environment. The hyperparameters were tuned by training an rl agent with PPO on a single environment for 10 million steps. We evaluate all hyperparameters on all environments in V. The parameters are reported in table A.1. Parameters with only a single \u201drange\u201d value in table A.1 were set to common values used for PPO. Parameter Range Best Learning rate [0.01, 0.03, 0.001, 0.003, 0.0001, 0.0003] 0.0003 Discount [0.97, 0.99, 0.997, 0.999] 0.999 Entropy loss coe\ufb03cient 0.01 0.01 Value loss coe\ufb03cient 0.5 0.5 Gradient norm clip 0.5 0.5 GAE \u03bb 0.95 0.95 Clipping range 0.2 0.2 Normalize advantage True True Minibatches 1 1 Epochs [1,2,4] 2 Optimizer adam adam Training steps 10\u2032000 10\u2032000 Unroll length [256, 512] 256 Number of actors 40 40 Table A.1: Hyperparameters used for training a student in the CMDP setting on the grid world environments. 76 A.3. Google Football Student Hyperparameters A.3 Google Football Student Hyperparameters In table A.2 we report the hyperparameters used in our Google Football CMDP experiments. These values were tuned in our previous work [Schraner, 2020]. Therefore we do not tune any of those parameters in this thesis. Parameter Value Learning rate 0.0011879 Discount 0.997 Entropy loss coe\ufb03cient 0.00155 Gradient norm clip 0.76 Value loss coe\ufb03cient 0.5 GAE \u03bb 0.95 Clipping range 0.115 Normalize advantage True Minibatches 4 Epochs 2 Optimizer adam Reward Scoring Observation SMM Training steps 10\u2032000 Unroll length 512 Number of actors 40 Table A.2: Important \ufb01xed student hyperparameters used in our Google Football CMDP experiments. 77 A.4. Teacher Hyperparameters A.4 Teacher Hyperparameters We optimized a limited number of hyperparameters for our teacher on the grid world environment. The hyperparameters were tuned by training a teacher agent for 1000 steps with PPO, using policy transfer, total average return and the RH observation. The parameters are reported in table A.3 and used for both the grid world and Google Football experiments. Parameters with only a single \u201drange\u201d value in table A.3 were set to common values used for PPO. Parameter Range Best Learning rate [0.1, 0.3, 0.01, 0.03] 0.03 Linear learning rate schedule True True Discount 1 1 Entropy loss coe\ufb03cient 0.01 0.01 Value loss coe\ufb03cient 0.5 0.5 Gradient norm clip 0.5 0.5 GAE \u03bb 0.95 0.95 Clipping range 0.2 0.2 Normalize advantage True True Minibatches 1 1 Epochs 4 4 Optimizer adam adam Training steps 1000 1000 Unroll length 4 4 Table A.3: Hyperparameters used for training a teacher in the CMDP setting used in both the grid world and Google Football experiments. 78 A.5. MLP vs. LSTM Teacher Model A.5 MLP vs. LSTM Teacher Model We evaluated teachers with an MLP network architecture and an LSTM architecture as described in section 3.2.4. Executing experiments with the LSTM architecture takes around 1.5 times as long as using the MLP architecture. In table A.4 we compare both network architectures. We report the total average return. Each teacher is trained for 1\u2019000 CMDP steps, where for each step, the student is trained for 10\u2019000 frames in the selected environment. The total average return reward signal and policy transfer are used in all experiments. We do not report results for LP and EMA for the LSTM model, as those observation types were developed after this evaluation has been carried out. The MLP architecture is superior to the LSTM architecture for all experiments except the RH observation type. Therefore, we use the MLP architecture for the experiments reported in this thesis. Model RH PTR LP ALP EMA FS-EMA MLP 2.34 4.44 4.17 3.35 4.35 3.72 LSTM 3.12 2.97 2.95 2.76 Table A.4: Comparison of the MLP and LSTM teacher architecture. We report the total average return over 100 episodes on all environments in V at the end of training. ",
    "title": "Master Thesis Teacher-student curriculum Yanick Schraner Contents Abstract Acknowledgements Symbols Chapter 1 Introduction Chapter 2 Background Chapter 3 Methods Chapter 4 Experiments and Results Chapter 5 Discussion Chapter 6 Conclusion Bibliography List of Figures List of Tables Appendix A Appendix",
    "paper_info": "Appendix A\nAppendix\nA.1\nGrid World CNN Architecture\nIn \ufb01g. A.1 we depict the CNN network architecture for grid world environments.\nThe network input is a 25 \u00d7 25 \u00d7 3 fully observable representation of the envi-\nronment\u2019s state. We use four convolution blocks with [16, 16, 32, 64] channels, a\n[2\u00d72, 3\u00d73, 3\u00d73, 3\u00d73] kernel and a stride of [1, 1, 1, 3]. A ReLU activation func-\ntion follows each convolution block. After the convolution blocks, we use a fully\nconnected layer with 128 units. The policy and value function are separate heads\non top of the fully connected layer. The policy head uses a softmax activation\nfunction.\nFigure A.1: Student CNN neural network architecture for the MiniGrid environ-\nments.\n74\n",
    "GPTsummary": "                    - (1): The research background of this paper is reinforcement learning, which is a popular paradigm for sequential decision-making problems. However, the sample inefficiency of deep reinforcement learning methods has become a significant obstacle when applying RL to real-world problems.\n\n                    - (2): The past methods for improving sample efficiency of RL include transfer learning, hierarchical structure, and curriculum learning. However, designing a curriculum that improves sample efficiency is a complex problem, and previous approaches have limitations. The teacher-student curriculum learning framework proposed in this paper is well motivated by addressing the limitations of previous approaches.\n\n                    - (3): The research methodology proposed in this paper is a teacher-student curriculum learning framework, where a teacher agent selects tasks for the student agent to learn a target task that would otherwise be too difficult to solve. The proposed method is independent of human domain knowledge and manual curriculum design.\n\n                    - (4): The proposed method is evaluated on two RL benchmarks: grid world and the Google Football environment, and achieves improved sample efficiency and generality of the student agent compared to tabula-rasa RL. The results prove the effectiveness of the proposed framework in reducing the sample inefficiency of RL.\n\n\n7. Methods: \n\n- (1): The proposed methodology is a teacher-student curriculum learning framework for improving the sample efficiency of reinforcement learning. In this framework, a teacher agent selects tasks for the student agent to learn a target task that would otherwise be too difficult to solve. \n\n- (2): The curriculum learning approach used in this framework is a task-level sequence curriculum where the teacher agent selects a subset of pre-defined tasks for the student to learn. The curriculum is represented as a sequence, and the student is allowed to retrain on already known tasks. \n\n- (3): The transfer methods used in this framework include policy and value function transfer and reward shaping. The policy and value function transfer involves copying all neural network weights from task to task, while reward shaping uses the value function obtained by training on the previous task to calculate an additional reward signal. \n\n- (4): The curriculum sequencer used in this framework is an automated Markov decision process where the teacher agent selects tasks with a suitable learning potential for the current learning stage. The evaluation metric used to evaluate the effectiveness of the proposed framework is weak transfer with the asymptotic performance improvement.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing a teacher-student curriculum learning framework for improving the sample efficiency of reinforcement learning. The proposed method proves to be effective in reducing sample inefficiency and achieving generality of the student agent, which contributes to the practical application of RL in real-world problems. \n\n- (2): Innovation point: The teacher-student curriculum learning framework proposed in this article is novel and tackles the challenge of designing an effective curriculum for improving sample efficiency of RL. The curriculum design is automated, independent of human domain knowledge, and improves the generality of the student agent.\n\n Performance: The proposed method achieves improved sample efficiency and generality of the student agent compared to tabula-rasa RL on two RL benchmarks.\n\n Workload: The article presents a clear and concise description of the proposed methodology, but the technical details for the implementation of the framework are not well elaborated, which requires more effort to reproduce the results.\n\n(The corresponding content output to xxx, means putting the relevant content in place of xxx)\n\n\n",
    "GPTmethods": "- (1): The proposed methodology is a teacher-student curriculum learning framework for improving the sample efficiency of reinforcement learning. In this framework, a teacher agent selects tasks for the student agent to learn a target task that would otherwise be too difficult to solve. \n\n- (2): The curriculum learning approach used in this framework is a task-level sequence curriculum where the teacher agent selects a subset of pre-defined tasks for the student to learn. The curriculum is represented as a sequence, and the student is allowed to retrain on already known tasks. \n\n- (3): The transfer methods used in this framework include policy and value function transfer and reward shaping. The policy and value function transfer involves copying all neural network weights from task to task, while reward shaping uses the value function obtained by training on the previous task to calculate an additional reward signal. \n\n- (4): The curriculum sequencer used in this framework is an automated Markov decision process where the teacher agent selects tasks with a suitable learning potential for the current learning stage. The evaluation metric used to evaluate the effectiveness of the proposed framework is weak transfer with the asymptotic performance improvement.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing a teacher-student curriculum learning framework for improving the sample efficiency of reinforcement learning. The proposed method proves to be effective in reducing sample inefficiency and achieving generality of the student agent, which contributes to the practical application of RL in real-world problems. \n\n- (2): Innovation point: The teacher-student curriculum learning framework proposed in this article is novel and tackles the challenge of designing an effective curriculum for improving sample efficiency of RL. The curriculum design is automated, independent of human domain knowledge, and improves the generality of the student agent.\n\n Performance: The proposed method achieves improved sample efficiency and generality of the student agent compared to tabula-rasa RL on two RL benchmarks.\n\n Workload: The article presents a clear and concise description of the proposed methodology, but the technical details for the implementation of the framework are not well elaborated, which requires more effort to reproduce the results.\n\n(The corresponding content output to xxx, means putting the relevant content in place of xxx)\n\n\n",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing a teacher-student curriculum learning framework for improving the sample efficiency of reinforcement learning. The proposed method proves to be effective in reducing sample inefficiency and achieving generality of the student agent, which contributes to the practical application of RL in real-world problems. \n\n- (2): Innovation point: The teacher-student curriculum learning framework proposed in this article is novel and tackles the challenge of designing an effective curriculum for improving sample efficiency of RL. The curriculum design is automated, independent of human domain knowledge, and improves the generality of the student agent.\n\n Performance: The proposed method achieves improved sample efficiency and generality of the student agent compared to tabula-rasa RL on two RL benchmarks.\n\n Workload: The article presents a clear and concise description of the proposed methodology, but the technical details for the implementation of the framework are not well elaborated, which requires more effort to reproduce the results.\n\n(The corresponding content output to xxx, means putting the relevant content in place of xxx)\n\n\n"
}