{
    "Introduction": "Introduction Appendix A provides mathematical background information and notation. Appendix B provides a chapter-length overview of machine learning and deep supervised learning. If you wish to refresh your knowledge of deep learning, please go to this appendix before you read Chap. 3. Appendix C provides lists of useful software environments and software packages for deep reinforcement learning. Chapter 2 Tabular Value-Based Reinforcement Learning This chapter will introduce the classic, tabular, feld of reinforcement learning, to build a foundation for the next chapters. First, we will introduce the concepts of agent and environment. Next come Markov decision processes, the formalism that is used to reason mathematically about reinforcement learning. We discuss at some length the elements of reinforcement learning: states, actions, values, policies. We learn about transition functions, and solution methods that are based on dynamic programming using the transition model. There are many situations where agents do not have access to the transition model, and state and reward information must be acquired from the environment. Fortunately, methods exist to fnd the optimal policy without a model, by querying the environment. These methods, appropriately named model-free methods, will be introduced in this chapter. Valuebased model-free methods are the most basic learning approach of reinforcement learning. They work well in problems with deterministic environments and discrete action spaces, such as mazes and games. Model-free learning makes few demands on the environment, building up the policy function \ud835\udf0b(\ud835\udc60) \u2192 \ud835\udc4e by sampling the environment. After we have discussed these concepts, it is time to apply them, and to understand the kinds of sequential decision problems that we can solve. We will look at Gym, a collection of reinforcement learning environments. We will also look at simple Grid world puzzles, and see how to navigate those. This is a non-deep chapter: in this chapter functions are exact, states are stored in tables, an approach that works as long as problems are small enough to ft in memory. The next chapter shows how function approximation with neural networks works when there are more states than ft in memory. The chapter is concluded with exercises, a summary, and pointers to further reading. 23 24 2 Tabular Value-Based Reinforcement Learning Core Concepts \u2022 Agent, environment \u2022 MDP: state, action, reward, value, policy \u2022 Planning and learning \u2022 Exploration and exploitation \u2022 Gym, baselines Core Problem \u2022 Learn a policy from interaction with the environment Core Algorithms \u2022 Value iteration (Listing 2.1) \u2022 Temporal diference learning (Sect. 2.2.4.2) \u2022 Q-learning (Listing 2.5) Finding a Supermarket Imagine that you have just moved to a new city, you are hungry, and you want to buy some groceries. There is a somewhat unrealistic catch: you do not have a map of the city and you forgot to charge your smartphone. It is a sunny day, you put on your hiking shoes, and after some random exploration you have found a way to a supermarket and have bought your groceries. You have carefully noted your route in a notebook, and you retrace your steps, fnding your way back to your new home. What will you do the next time that you need groceries? One option is to follow exactly the same route, exploiting your current knowledge. This option is guaranteed to bring you to the store, at no additional cost for exploring possible alternative routes. Or you could be adventurous, and explore, trying to fnd a new route that may actually be quicker than the old route. Clearly, there is a trade-of: you should not spend so much time exploring that you can not recoup the gains of a potential shorter route before you move elsewhere. Reinforcement learning is a natural way of learning the optimal route as we go, by trial and error, from the efects of the actions that we take in our environment. This little story contained many of the elements of a reinforcement learning problem, and how to solve it. There is an agent (you), an environment (the city), there 2.1 Sequential Decision Problems 25 Fig. 2.1 Grid World with a goal, an \u201cun-goal,\u201d and a wall are states (your location at diferent points in time), actions (assuming a Manhattanstyle grid, moving a block left, right, forward, or back), there are trajectories (the routes to the supermarket that you tried), there is a policy (that tells which action you will take at a particular location), there is a concept of cost/reward (the length of your current path), we see exploration of new routes, exploitation of old routes, a trade-of between them, and your notebook in which you have been sketching a map of the city (your local transition model). By the end of this chapter you will have learned which role all these topics play in reinforcement learning. 2.1 Sequential Decision Problems Reinforcement learning is used to solve sequential decision problems [27, 255]. Before we dive into the algorithms, let us have a closer look at these problems, to better understand the challenges that the agents must solve. In a sequential decision problem the agent has to make a sequence of decisions in order to solve a problem. Solving implies to fnd the sequence with the highest (expected cumulative future) reward. The solver is called the agent, and the problem is called environment (or sometimes the world). We will now discuss basic examples of sequential decision problems. Grid Worlds Some of the frst environments that we encounter in reinforcement learning are Grid worlds (Fig. 2.1). These environments consist of a rectangular grid of squares, with a start square, and a goal square. The aim is for the agent to fnd the sequence of actions that it must take (up, down, left, right) to arrive at the goal square. In fancy versions a \u201closs\u201d square is added, that scores minus points, or a \u201cwall\u201d square, that is impenetrable for the agent. By exploring the grid, taking diferent actions, and recording the reward (whether it reached the goal square), the agent can fnd a route\u2014and when it has a route, it can try to improve that route, to fnd a shorter route to the goal. 26 2 Tabular Value-Based Reinforcement Learning Fig. 2.2 The Longleat Hedge Maze in Wiltshire, England Fig. 2.3 Sokoban Puzzle [136] Grid world is a simple environment that is well-suited for manually playing around with reinforcement learning algorithms, to build up intuition of what the algorithms do. In this chapter we will model reinforcement learning problems formally, and encounter algorithms that fnd optimal routes in Grid world. Mazes and Box Puzzles After Grid world problems, there are more complicated problems, with extensive wall structures to make navigation more difcult (see Fig. 2.2). Trajectory planning algorithms play a central role in robotics [456, 265]; there is a long tradition of using 2D and 3D mazes for path-fnding problems in reinforcement learning. The 2.2 Tabular Value-Based Agents 27 Agent Environment Action \ud835\udc4e\ud835\udc61 State \ud835\udc60\ud835\udc61+1 Reward \ud835\udc5f\ud835\udc61+1 Fig. 2.4 Agent and environment [743] Taxi domain was introduced by Dietterich [196], and box-pushing problems such as Sokoban have also been used frequently [386, 204, 543, 878], see Fig. 2.3. The challenge in Sokoban is that boxes can only be pushed, not pulled. Actions can have the efect of creating an inadvertent dead-end for into the future, making Sokoban a difcult puzzle to play. The action space of these puzzles and mazes is discrete. Small versions of the mazes can be solved exactly by planning, larger instances are only suitable for approximate planning or learning methods. Solving these planning problems exactly is NP-hard or PSPACE-hard [169, 322], as a consequence the computational time required to solve problem instances exactly grows exponentially with the problem size, and becomes quickly infeasible for all but the smallest problems. Let us see how we can model agents to act in these types of environments. 2.2 Tabular Value-Based Agents Reinforcement learning fnds the best policy to operate in the environment by interacting with it. The reinforcement learning paradigm consists of an agent (you, the learner) and an environment (the world, which is in a certain state, and gives you feedback on your actions). 2.2.1 Agent and Environment In Fig. 2.4 the agent and environment are shown, together with action \ud835\udc4e\ud835\udc61, next state \ud835\udc60\ud835\udc61+1, and its reward \ud835\udc5f\ud835\udc61+1. Let us have a closer look at the fgure. The environment is in a certain state \ud835\udc60\ud835\udc61 at time \ud835\udc61. Then, the agent performs action \ud835\udc4e\ud835\udc61, resulting in a transition in the environment from state \ud835\udc60\ud835\udc61 to \ud835\udc60\ud835\udc61+1 at the next time step, also denoted as \ud835\udc60 \u2192 \ud835\udc60\u2032. Along with this new state comes a reward value \ud835\udc5f\ud835\udc61+1 (which may be a positive or a negative value). The goal of reinforcement learning is to fnd the sequence of actions that gives the best reward. More formally, the goal is to fnd the optimal policy function \ud835\udf0b\u2605 that gives in each state the best action to take in that state. By trying diferent actions, and accumulating the rewards, 28 2 Tabular Value-Based Reinforcement Learning the agent can fnd the best action for each state. In this way, with the reinforcing reward values, the optimal policy is learned from repeated interaction with the environment, and the problem is \u201csolved.\u201d In reinforcement learning the environment gives us only a number as an indication of the quality of an action that we performed, and we are left to derive the correct action policy from that, as we can see in Fig. 2.4. On the other hand, reinforcement learning allows us to generate as many action-reward pairs as we need, without a large hand-labeled dataset, and we can choose ourselves which actions to try. 2.2.2 Markov Decision Process Sequential decision problems can be modelled as Markov decision processes (MDPs) [483]. Markov decision problems have the Markov property: the next state depends only on the current state and the actions available in it (no historical memory of previous states or information from elsewhere infuences the next state) [352]. The no-memory property is important because it makes reasoning about future states possible using only the information present in the current state. If previous histories would infuence the current state, and these would all have to be taken into account, then reasoning about the current state would be much harder or even infeasible. Markov processes are named after Russian mathematician Andrey Markov (1856\u2013 1922) who is best known for his work on these stochastic processes. See [27, 255] for an introduction into MDPs. The MDP formalism is the mathematical basis under reinforcement learning, and we will introduce the relevant elements in this chapter. We follow Moerland [524] and Fran\u00e7ois-Lavet et al. [255] for some of the notation and examples in this section. Formalism We defne a Markov decision process for reinforcement learning as a 5-tuple (\ud835\udc46, \ud835\udc34,\ud835\udc47\ud835\udc4e, \ud835\udc45\ud835\udc4e, \ud835\udefe): \u2022 \ud835\udc46 is a fnite set of legal states of the environment; the initial state is denoted as \ud835\udc600 \u2022 \ud835\udc34 is a fnite set of actions (if the set of actions difers per state, then \ud835\udc34\ud835\udc60 is the fnite set of actions in state \ud835\udc60) \u2022 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) = Pr(\ud835\udc60\ud835\udc61+1 = \ud835\udc60\u2032|\ud835\udc60\ud835\udc61 = \ud835\udc60, \ud835\udc4e\ud835\udc61 = \ud835\udc4e) is the probability that action \ud835\udc4e in state \ud835\udc60 at time \ud835\udc61 will transition to state \ud835\udc60\u2032 at time \ud835\udc61 + 1 in the environment \u2022 \ud835\udc45\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) is the reward received after action \ud835\udc4e transitions state \ud835\udc60 to state \ud835\udc60\u2032 \u2022 \ud835\udefe \u2208 [0, 1] is the discount factor representing the diference between future and present rewards. 2.2 Tabular Value-Based Agents 29 2.2.2.1 State \ud835\udc7a Let us have a deeper look at the Markov-tuple \ud835\udc46, \ud835\udc34,\ud835\udc47\ud835\udc4e, \ud835\udc45\ud835\udc4e, \ud835\udefe, to see their role in the reinforcement learning paradigm, and how, together, they can model and describe reward-based learning processes. At the basis of every Markov decision process is a description of the state \ud835\udc60\ud835\udc61 of the system at a certain time \ud835\udc61. State Representation The state \ud835\udc60 contains the information to uniquely represent the confguration of the environment. Often there is a straightforward way to uniquely represent the state in a computer memory. For the supermarket example, each identifying location is a state (such as: I am at the corner of 8th Av and 27nd St). For chess, this can be the location of all pieces on the board (plus information for the 50 move repetition rule, castling rights, and en-passant state). For robotics this can be the orientation of all joints of the robot, and the location of the limbs of the robot. For Atari, the state comprises the values of all screen pixels. Using its current behavior policy, the agent chooses an action \ud835\udc4e, which is performed in the environment. How the environment reacts to the action is defned by the transition model \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) that is internal to the environment, which the agent does not know. The environment returns the new state \ud835\udc60\u2032, as well as a reward value \ud835\udc5f\u2032 for the new state. Deterministic and Stochastic Environment In discrete deterministic environments the transition function defnes a one-step transition, as each action (from a certain old state) deterministically leads to a single new state. This is the case in Grid worlds, Sokoban, and in games such as chess and checkers, where a move action deterministically leads to one new board position. An example of a non-deterministic situation is a robot movement in an environment. In a certain state, a robot arm is holding a bottle. An agent-action can be turning the bottle in a certain orientation (presumably to pour a drink in a cup). The next state may be a full cup, or it may be a mess, if the bottle was not poured in the correct orientation, or location, or if something happened in the environment such as someone bumping the table. The outcome of the action is unknown beforehand by the agent, and depends on elements in the environment, that are not known to the agent. 30 2 Tabular Value-Based Reinforcement Learning 2.2.2.2 Action \ud835\udc68 Now that we have looked at the state, it is time to look at the second item that defnes an MDP, the action. Irreversible Environment Action When the agent is in state \ud835\udc60, it chooses an action \ud835\udc34 to perform, based on its current behavior policy \ud835\udf0b(\ud835\udc4e|\ud835\udc60) (policies are explained soon). The agent communicates the selected action \ud835\udc4e to the environment (Fig. 2.4). For the supermarket example, an example of an action could be walking along a block in a certain direction (such as: East). For Sokoban, an action can be pushing a box to a new location in the warehouse. Note that in diferent states the possible actions may difer. For the supermarket example, walking East may not be possible at each street corner, and in Sokoban pushing a box in a certain direction will only be possible in states where this direction is not blocked by a wall. An action changes the state of the environment irreversibly. In the reinforcement learning paradigm, there is no undo operator for the environment (nor is there in the real world). When the environment has performed a state transition, it is fnal. The new state is communicated to the agent, together with a reward value. The actions that the agent performs in the environment are also known as its behavior, just as the actions of a human in the world constitute the human\u2019s behavior. Discrete or Continuous Action Space The actions are discrete in some applications, continuous in others. For example, the actions in board games, and choosing a direction in a navigation task in a grid, are discrete. In contrast, arm and joint movements of robots, and bet sizes in certain games, are continuous (or span a very large range of values). Applying algorithms to continuous or very large action spaces either requires discretization of the continuous space (into buckets) or the development of a diferent kind of algorithm. As we will see in Chaps. 3 and 4, value-based methods work well for discrete action spaces, and policy-based methods work well for both action spaces. For the supermarket example we can actually choose between modeling our actions discrete or continuous. From every state, we can move any number of steps, small or large, integer or fractional, in any direction. We can even walk a curvy path. So, strictly speaking, the action space is continuous. However, if, as in some cities, the streets are organized in a rectangular Manhattan-pattern, then it makes sense to discretize the continuous space, and to only consider discrete actions that 2.2 Tabular Value-Based Agents 31 \ud835\udc60 \ud835\udf0b \ud835\udc4e \ud835\udc60\u2032 \ud835\udc61\ud835\udc4e, \ud835\udc5f\ud835\udc4e \ud835\udc60 \ud835\udf0b \ud835\udc4e \ud835\udc60\u2032 \ud835\udc61\ud835\udc4e, \ud835\udc5f\ud835\udc4e \ud835\udc60 \ud835\udf0b \ud835\udc4e, \ud835\udc60\u2032 \ud835\udc61\ud835\udc4e, \ud835\udc5f\ud835\udc4e Fig. 2.5 Backup Diagrams for MDP Transitions: Stochastic (left) and Deterministic (middle and right) [743] take us to the next street corner. Then, our action space has become discrete, by using extra knowledge of the problem structure.1 2.2.2.3 Transition \ud835\udc7b\ud835\udc82 After having discussed state and action, it is time to look at the transition function \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032). The transition function \ud835\udc47 \ud835\udc4e determines how the state changes after an action has been selected. In model-free reinforcement learning the transition function is implicit to the solution algorithm: the environment has access to the transition function, and uses it to compute the next state \ud835\udc60\u2032, but the agent has not. (In Chap. 5 we will discuss model-based reinforcement learning. There the agent has its own transition function, an approximation of the environment\u2019s transition function, which is learned from the environment feedback.) Graph View of the State Space We have discussed states, actions and transitions. The dynamics of the MDP are modelled by transition function \ud835\udc47\ud835\udc4e(\u00b7) and reward function \ud835\udc45\ud835\udc4e(\u00b7). The imaginary space of all possible states is called the state space. The state space is typically large. The two functions defne a two-step transition from state \ud835\udc60 to \ud835\udc60\u2032, via action \ud835\udc4e: \ud835\udc60 \u2192 \ud835\udc4e \u2192 \ud835\udc60\u2032. To help our understanding of the transitions between states we can use a graphical depiction, as in Fig. 2.5. In the fgure, states and actions are depicted as nodes (vertices), and transitions are links (edges) between the nodes. States are drawn as open circles, and actions as smaller black circles. In a certain state \ud835\udc60, the agent can choose which action \ud835\udc4e to 1 If we assume that supermarkets are large, block-sized, items that typically can be found on street corners, then we can discretize the action space. Note that we may miss small sub-block-sized supermarkets, because of this simplifcation. Another, better, simplifcation, would be to discretize the action space into walking distances of the size of the smallest supermarket that we expect to ever encounter. 32 2 Tabular Value-Based Reinforcement Learning perform, that is then acted out in the environment. The environment returns the new state \ud835\udc60\u2032 and the reward \ud835\udc5f\u2032. Figure 2.5 shows a transition graph of the elements of the MDP tuple \ud835\udc60, \ud835\udc4e, \ud835\udc61\ud835\udc4e, \ud835\udc5f\ud835\udc4e as well as \ud835\udc60\u2032, and policy \ud835\udf0b, and how the value can be calculated. The root node at the top is state \ud835\udc60, where policy \ud835\udf0b allows the agent to choose between three actions \ud835\udc4e, that, following distribution Pr, each can transition to two possible states \ud835\udc60\u2032, with their reward \ud835\udc5f\u2032. In the fgure, a single transition is shown. Please use your imagination to picture the other transitions as the graph extends down. In the left panel of the fgure the environment can choose which new state it returns in response to the action (stochastic environment), in the middle panel there is only one state for each action (deterministic environment); the tree can then be simplifed, showing only the states, as in the right panel. To calculate the value of the root of the tree a backup procedure can be followed. Such a procedure calculates the value of a parent from the values of the children, recursively, in a bottom-up fashion, summing or maxing their values from the leaves to the root of the tree. This calculation uses discrete time steps, indicated by subscripts to the state and action, as in \ud835\udc60\ud835\udc61, \ud835\udc60\ud835\udc61+1, \ud835\udc60\ud835\udc61+2, . . .. For brevity, \ud835\udc60\ud835\udc61+1 is sometimes written as \ud835\udc60\u2032. The fgure shows a single transition step; an episode in reinforcement learning typically consists of a sequence of many time steps. Trial and Error, Down and Up A graph such as the one in the center and right panel of Fig. 2.5, where child nodes have only one parent node and without cycles, is known as a tree. In computer science the root of a tree is at the top, and branches grow downward to the leaves. As actions are performed and states and rewards are returned backup the tree, a learning process is taking place in the agent. We can use Fig. 2.5 to better understand the learning process that is unfolding. The rewards of actions are learned by the agent by interacting with the environment, performing the actions. In the tree of Fig. 2.5 an action selection moves downward, towards the leaves. At the deeper states, we fnd the rewards, which we propagate to the parent states upwards. Reward learning is learning by backpropagation: in Fig. 2.5 the reward information fows upward in the diagram from the leaves to the root. Action selection moves down, reward learning fows up. Reinforcement learning is learning by trial and error. Trial is selecting an action down (using the behavior policy) to perform in the environment. Error is moving up the tree, receiving a feedback reward from the environment, and reporting that back up the tree to the state to update the current behavior policy. The downward selection policy chooses which actions to explore, and the upward propagation of the error signal performs the learning of the policy. Figures such as the one in Fig. 2.5 are useful for seeing how values are calculated. The basic notions are trial, and error, or down, and up. 2.2 Tabular Value-Based Agents 33 2.2.2.4 Reward \ud835\udc79\ud835\udc82 The reward function \ud835\udc45\ud835\udc4e is of central importance in reinforcement learning. It indicates the measure of quality of that state, such solved, or distance. Rewards are associated with single states, indicating their quality. However, we are most often interested in the quality of a full decision making sequence from root to leaves (this sequence of decisions would be one possible answer to our sequential decision problem). The reward of such a full sequence is called the return, sometimes denoted confusingly as \ud835\udc45, just as the reward. The expected cumulative discounted future reward of a state is called the value function \ud835\udc49 \ud835\udf0b(\ud835\udc60). The value function \ud835\udc49 \ud835\udf0b(\ud835\udc60) is the expected cumulative reward of \ud835\udc60 where actions are chosen according to policy \ud835\udf0b. The value function plays a central role in reinforcement learning algorithms; in a few moments we will look deeper into return and value. 2.2.2.5 Discount Factor \ud835\udf38 We distinguish between two types of tasks: (1) continuous time, long running, tasks, and (2) episodic tasks\u2014tasks that end. In continuous and long running tasks it makes sense to discount rewards from far in the future in order to more strongly value current information at the present time. To achieve this a discount factor \ud835\udefe is used in our MDP that reduces the impact of far away rewards. Many continuous tasks use discounting, \ud835\udefe \u2260 1. However, in this book we will often discuss episodic problems, where \ud835\udefe is irrelevant. Both the supermarket example and the game of chess are episodic, and discounting does not make sense in these problems, \ud835\udefe = 1. 2.2.2.6 Policy \ud835\udf45 Of central importance in reinforcement learning is the policy function \ud835\udf0b. The policy function \ud835\udf0b answers the question how the diferent actions \ud835\udc4e at state \ud835\udc60 should be chosen. Actions are anchored in states. The central question of MDP optimization is how to choose our actions. The policy \ud835\udf0b is a conditional probability distribution that for each possible state specifes the probability of each possible action. The function \ud835\udf0b is a mapping from the state space to a probability distribution over the action space: \ud835\udf0b : \ud835\udc46 \u2192 \ud835\udc5d(\ud835\udc34) where \ud835\udc5d(\ud835\udc34) can be a discrete or continuous probability distribution. For a particular probability (density) from this distribution we write \ud835\udf0b(\ud835\udc4e|\ud835\udc60) 34 2 Tabular Value-Based Reinforcement Learning Example: For a discrete state space and discrete action space, we may store an explicit policy as a table, e.g.: \ud835\udc60 \ud835\udf0b(\ud835\udc4e=up|\ud835\udc60) \ud835\udf0b(\ud835\udc4e=down|\ud835\udc60) \ud835\udf0b(\ud835\udc4e=left|\ud835\udc60) \ud835\udf0b(\ud835\udc4e=right|\ud835\udc60) 1 0.2 0.8 0.0 0.0 2 0.0 0.0 0.0 1.0 3 0.7 0.0 0.3 0.0 etc. . . . . A special case of a policy is a deterministic policy, denoted by \ud835\udf0b(\ud835\udc60) where \ud835\udf0b : \ud835\udc46 \u2192 \ud835\udc34 A deterministic policy selects a single action in every state. Of course the deterministic action may difer between states, as in the example below: Example: An example of a deterministic discrete policy is \ud835\udc60 \ud835\udf0b(\ud835\udc4e=up|\ud835\udc60) \ud835\udf0b(\ud835\udc4e=down|\ud835\udc60) \ud835\udf0b(\ud835\udc4e=left|\ud835\udc60) \ud835\udf0b(\ud835\udc4e=right|\ud835\udc60) 1 0.0 1.0 0.0 0.0 2 0.0 0.0 0.0 1.0 3 1.0 0.0 0.0 0.0 etc. . . . . We would write \ud835\udf0b(\ud835\udc60 = 1) = down, \ud835\udf0b(\ud835\udc60 = 2) = right, etc. 2.2.3 MDP Objective Finding the optimal policy function is the goal of the reinforcement learning problem, and the remainder of this book will discuss many diferent algorithms to achieve this goal under diferent circumstances. Let us have a closer look at the objective of reinforcement learning. Before we can do so, we will look at traces, their return, and value functions. 2.2.3.1 Trace \ud835\udf49 As we start interacting with the MDP, at each timestep \ud835\udc61, we observe \ud835\udc60\ud835\udc61, take an action \ud835\udc4e\ud835\udc61 and then observe the next state \ud835\udc60\ud835\udc61+1 \u223c \ud835\udc47\ud835\udc4e\ud835\udc61 (\ud835\udc60) and reward \ud835\udc5f\ud835\udc61 = \ud835\udc45\ud835\udc4e\ud835\udc61 (\ud835\udc60\ud835\udc61, \ud835\udc60\ud835\udc61+1). 2.2 Tabular Value-Based Agents 35 \ud835\udc60 \ud835\udc4e \ud835\udc60 \ud835\udc4e \ud835\udc47 \ud835\udc47 Fig. 2.6 Single Transition Step versus Full 3-Step Trace/Episode/Trajectory Repeating this process leads to a sequence or trace in the environment, which we denote by \ud835\udf0f\ud835\udc5b \ud835\udc61 : \ud835\udf0f\ud835\udc5b \ud835\udc61 = {\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc60\ud835\udc61+1, .., \ud835\udc4e\ud835\udc61+\ud835\udc5b, \ud835\udc5f\ud835\udc61+\ud835\udc5b, \ud835\udc60\ud835\udc61+\ud835\udc5b+1} Here, \ud835\udc5b denotes the length of the \ud835\udf0f. In practice, we often assume \ud835\udc5b = \u221e, which means that we run the trace until the domain terminates. In those cases, we will simply write \ud835\udf0f\ud835\udc61 = \ud835\udf0f\u221e \ud835\udc61 . Traces are one of the basic building blocks of reinforcement learning algorithms. They are a single full rollout of a sequence from the sequential decision problem. They are also called trajectory, episode, or simply sequence (Fig. 2.6 shows a single transition step, and an example of a three-step trace). Example: A short trace with three actions could look like: \ud835\udf0f2 0 = {\ud835\udc600=1, \ud835\udc4e0=up, \ud835\udc5f0=\u22121, \ud835\udc601=2, \ud835\udc4e1=up, \ud835\udc5f1=\u22121, \ud835\udc602=3, \ud835\udc4e2=left, \ud835\udc5f2=20, \ud835\udc603=5} Since both the policy and the transition dynamics can be stochastic, we will not always get the same trace from the start state. Instead, we will get a distribution over traces. The distribution of traces from the start state (distribution) is denoted by \ud835\udc5d(\ud835\udf0f0). The probability of each possible trace from the start is actually given by the product of the probability of each specifc transition in the trace: \ud835\udc5d(\ud835\udf0f0) = \ud835\udc5d0(\ud835\udc600) \u00b7 \ud835\udf0b(\ud835\udc4e0|\ud835\udc600) \u00b7 \ud835\udc47\ud835\udc4e0 (\ud835\udc600, \ud835\udc601) \u00b7 \ud835\udf0b(\ud835\udc4e1|\ud835\udc601)... = \ud835\udc5d0(\ud835\udc600) \u00b7 \u221e \ufffd \ud835\udc61=0 \ud835\udf0b(\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61) \u00b7 \ud835\udc47\ud835\udc4e\ud835\udc61 (\ud835\udc60\ud835\udc61, \ud835\udc60\ud835\udc61+1) (2.1) Policy-based reinforcement learning depends heavily on traces, and we will discuss traces more deeply in Chap. 4. Value-based reinforcement learning (this chapter) uses single transition steps. 36 2 Tabular Value-Based Reinforcement Learning Return \ud835\udc79 We have not yet formally defned what we actually want to achieve in the sequential decision-making task\u2014which is, informally, the best policy. The sum of the future reward of a trace is known as the return. The return of trace \ud835\udf0f\ud835\udc61 is: \ud835\udc45(\ud835\udf0f\ud835\udc61) = \ud835\udc5f\ud835\udc61 + \ud835\udefe \u00b7 \ud835\udc5f\ud835\udc61+1 + \ud835\udefe2 \u00b7 \ud835\udc5f\ud835\udc61+2 + ... = \ud835\udc5f\ud835\udc61 + \u221e \u2211\ufe01 \ud835\udc56=1 \ud835\udefe\ud835\udc56\ud835\udc5f\ud835\udc61+\ud835\udc56 (2.2) where \ud835\udefe \u2208 [0, 1] is the discount factor. Two extreme cases are: \u2022 \ud835\udefe = 0: A myopic agent, which only considers the immediate reward, \ud835\udc45(\ud835\udf0f\ud835\udc61) = \ud835\udc5f\ud835\udc61 \u2022 \ud835\udefe = 1: A far-sighted agent, which treats all future rewards as equal, \ud835\udc45(\ud835\udf0f\ud835\udc61) = \ud835\udc5f\ud835\udc61 + \ud835\udc5f\ud835\udc61+1 + \ud835\udc5f\ud835\udc61+2 + . . . Note that if we would use an infnite-horizon return (Eq. 2.2) and \ud835\udefe = 1.0, then the cumulative reward may become unbounded. Therefore, in continuous problems, we use a discount factor close to 1.0, such as \ud835\udefe = 0.99. Example: For the previous trace example we assume \ud835\udefe = 0.9. The return (cumulative reward) is equal to: \ud835\udc45(\ud835\udf0f2 0) = \u22121 + 0.9 \u00b7 \u22121 + 0.92 \u00b7 20 = 16.2 \u2212 1.9 = 14.3 2.2.3.2 State Value \ud835\udc7d The real measure of optimality that we are interested in is not the return of just one trace. The environment can be stochastic, and so can our policy, and for a given policy we do not always get the same trace. Therefore, we are actually interested in the expected cumulative reward that a certain policy achieves. The expected cumulative discounted future reward of a state is better known as the value of that state. We defne the state value \ud835\udc49 \ud835\udf0b(\ud835\udc60) as the return we expect to achieve when an agent starts in state \ud835\udc60 and then follows policy \ud835\udf0b, as: \ud835\udc49 \ud835\udf0b(\ud835\udc60) = E\ud835\udf0f\ud835\udc61\u223c\ud835\udc5d(\ud835\udf0f\ud835\udc61) \ufffd \u221e \u2211\ufe01 \ud835\udc56=0 \ud835\udefe\ud835\udc56 \u00b7 \ud835\udc5f\ud835\udc61+\ud835\udc56|\ud835\udc60\ud835\udc61 = \ud835\udc60 \ufffd (2.3) 2.2 Tabular Value-Based Agents 37 Example: Imagine that we have a policy \ud835\udf0b, which from state \ud835\udc60 can result in two traces. The frst trace has a cumulative reward of 20, and occurs in 60% of the times. The other trace has a cumulative reward of 10, and occurs 40% of the times. What is the value of state \ud835\udc60? \ud835\udc49 \ud835\udf0b(\ud835\udc60) = 0.6 \u00b7 20 + 0.4 \u00b7 10 = 16. The average return (cumulative reward) that we expect to get from state \ud835\udc60 under this policy is 16. Every policy \ud835\udf0b has one unique associated value function \ud835\udc49 \ud835\udf0b(\ud835\udc60). We often omit \ud835\udf0b to simplify notation, simply writing \ud835\udc49(\ud835\udc60), knowing a state value is always conditioned on a certain policy. The state value is defned for every possible state \ud835\udc60 \u2208 \ud835\udc46. \ud835\udc49(\ud835\udc60) maps every state to a real number (the expected return): \ud835\udc49 : \ud835\udc46 \u2192 R Example: In a discrete state space, the value function can be represented as a table of size |\ud835\udc46|. \ud835\udc60 \ud835\udc49 \ud835\udf0b(\ud835\udc60) 1 2.0 2 4.0 3 1.0 etc. . Finally, the state value of a terminal state is by defnition zero: \ud835\udc60 = terminal \u21d2 \ud835\udc49(\ud835\udc60) := 0. 2.2.3.3 State-Action Value \ud835\udc78 In addition to state values \ud835\udc49 \ud835\udf0b(\ud835\udc60), we also defne state-action value \ud835\udc44 \ud835\udf0b(\ud835\udc60, \ud835\udc4e).2 The only diference is that we now condition on a state and action. We estimate the average return we expect to achieve when taking action \ud835\udc4e in state \ud835\udc60, and follow policy \ud835\udf0b afterwards: \ud835\udc44 \ud835\udf0b(\ud835\udc60, \ud835\udc4e) = E\ud835\udf0f\ud835\udc61\u223c\ud835\udc5d(\ud835\udf0f\ud835\udc61) \ufffd \u221e \u2211\ufe01 \ud835\udc56=0 \ud835\udefe\ud835\udc56 \u00b7 \ud835\udc5f\ud835\udc61+\ud835\udc56|\ud835\udc60\ud835\udc61 = \ud835\udc60, \ud835\udc4e\ud835\udc61 = \ud835\udc4e \ufffd (2.4) 2 The reason for the choice for letter Q is lost in the mists of time. Perhaps it is meant to indicate quality. 38 2 Tabular Value-Based Reinforcement Learning Every policy \ud835\udf0b has only one unique associated state-action value function \ud835\udc44 \ud835\udf0b(\ud835\udc60, \ud835\udc4e). We often omit \ud835\udf0b to simplify notation. Again, the state-action value is a function \ud835\udc44 : \ud835\udc46 \u00d7 \ud835\udc34 \u2192 R which maps every state-action pair to a real number. Example: For a discrete state and action space, \ud835\udc44(\ud835\udc60, \ud835\udc4e) can be represented as a table of size |\ud835\udc46| \u00d7 |\ud835\udc34|. Each table entry stores a \ud835\udc44(\ud835\udc60, \ud835\udc4e) estimate for the specifc \ud835\udc60, \ud835\udc4e combination: \ud835\udc4e=up \ud835\udc4e=down \ud835\udc4e=left \ud835\udc4e=right \ud835\udc60=1 4.0 3.0 7.0 1.0 \ud835\udc60=2 2.0 -4.0 0.3 1.0 \ud835\udc60=3 3.5 0.8 3.6 6.2 etc. . . . . The state-action value of a terminal state is by defnition zero: \ud835\udc60 = terminal \u21d2 \ud835\udc44(\ud835\udc60, \ud835\udc4e) := 0, \u2200\ud835\udc4e 2.2.3.4 Reinforcement Learning Objective We now have the ingredients to formally state the objective \ud835\udc3d(\u00b7) of reinforcement learning. The objective is to achieve the highest possible average return from the start state: \ud835\udc3d(\ud835\udf0b) = \ud835\udc49 \ud835\udf0b(\ud835\udc600) = E\ud835\udf0f0\u223c\ud835\udc5d(\ud835\udf0f0 | \ud835\udf0b) \ufffd \ud835\udc45(\ud835\udf0f0) \ufffd . (2.5) for \ud835\udc5d(\ud835\udf0f0) given in Eq. 2.1. There is one optimal value function, which achieves higher or equal value than all other value functions. We search for a policy that achieves this optimal value function, which we call the optimal policy \ud835\udf0b\u2605: \ud835\udf0b\u2605(\ud835\udc4e|\ud835\udc60) = arg max \ud835\udf0b \ud835\udc49 \ud835\udf0b(\ud835\udc600) (2.6) This function \ud835\udf0b\u2605 is the optimal policy, it uses the arg max function to select the policy with the optimal value. The goal in reinforcement learning is to fnd this optimal policy for start state \ud835\udc600. A potential beneft of state-action values \ud835\udc44 over state values \ud835\udc49 is that stateaction values directly tell what every action is worth. This may be useful for action selection, since, for discrete action spaces, \ud835\udc4e\u2605 = arg max \ud835\udc4e\u2208\ud835\udc34 \ud835\udc44\u2605(\ud835\udc60, \ud835\udc4e) 2.2 Tabular Value-Based Agents 39 the Q function directly identifes the best action. Equivalently, the optimal policy can be obtained directly from the optimal Q function: \ud835\udf0b\u2605(\ud835\udc60) = arg max \ud835\udc4e\u2208\ud835\udc34 \ud835\udc44\u2605(\ud835\udc60, \ud835\udc4e). We will now turn to construct algorithms to compute the value function and the policy function. 2.2.3.5 Bellman Equation To calculate the value function, let us look again at the tree in Fig. 2.5 on page 31, and imagine that it is many times larger, with subtrees that extend to fully cover the state space. Our task is to compute the value of the root, based on the reward values at the real leaves, using the transition function \ud835\udc47\ud835\udc4e. One way to calculate the value \ud835\udc49(\ud835\udc60) is to traverse this full state space tree, computing the value of a parent node by taking the reward value and the sum of the children, discounting this value by \ud835\udefe. This intuitive approach was frst formalized by Richard Bellman in 1957. Bellman showed that discrete optimization problems can be described as a recursive backward induction problem [72]. He introduced the term dynamic programming to recursively traverse the states and actions. The so-called Bellman equation shows the relationship between the value function in state \ud835\udc60 and the future child state \ud835\udc60\u2032, when we follow the transition function. The discrete Bellman equation of the value of state \ud835\udc60 after following policy \ud835\udf0b is:3 \ud835\udc49 \ud835\udf0b(\ud835\udc60) = \u2211\ufe01 \ud835\udc4e\u2208\ud835\udc34 \ud835\udf0b(\ud835\udc4e|\ud835\udc60) \ufffd \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc45\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \ud835\udc49 \ud835\udf0b(\ud835\udc60\u2032) \ufffd\ufffd (2.7) where \ud835\udf0b is the probability of action \ud835\udc4e in state \ud835\udc60,\ud835\udc47 is the stochastic transition function, \ud835\udc45 is the reward function and \ud835\udefe is the discount rate. Note the recursion on the value function, and that for the Bellman equation the transition and reward functions must be known for all states by the agent. Together, the transition and reward model are referred to as the dynamics model of the environment. The dynamics model is often not known by the agent, and model-free methods have been developed to compute the value function and policy function without them. The recursive Bellman equation is the basis of algorithms to compute the value function, and other relevant functions to solve reinforcement learning problems. In the next section we will study these solution methods. 3 State-action value and continuous Bellman equations can be found in Appendix A.4. 40 2 Tabular Value-Based Reinforcement Learning Fig. 2.7 Recursion: Droste efect 2.2.4 MDP Solution Methods The Bellman equation is a recursive equation: it shows how to calculate the value of a state, out of the values of applying the function specifcation again on the successor states. Figure 2.7 shows a recursive picture, of a picture in a picture, in a picture, etc. In algorithmic form, dynamic programming calls its own code on states that are closer and closer to the leaves, until the leaves are reached, and the recursion can not go further. Dynamic programming uses the principle of divide and conquer: it begins with a start state whose value is to be determined by searching a large subtree, which it does by going down into the recursion, fnding the value of sub-states that are closer to terminals. At terminals the reward values are known, and these are then used in the construction of the parent values, as it goes up, back out of the recursion, and ultimately arrives at the root value itself. A simple dynamic programming method to iteratively traverse the state space to calculate Bellman\u2019s equation is value iteration (VI). Pseudocode for a basic version of VI is shown in Listing 2.1, based on [15]. Value iteration converges to the optimal value function by iteratively improving the estimate of \ud835\udc49(\ud835\udc60). The value function \ud835\udc49(\ud835\udc60) is frst initialized to random values. Value iteration repeatedly updates \ud835\udc44(\ud835\udc60, \ud835\udc4e) and \ud835\udc49(\ud835\udc60) values, looping over the states and their actions, until convergence occurs (when the values of \ud835\udc49(\ud835\udc60) stop changing much). Value iteration works with a fnite set of actions. It has been proven to converge to the optimal values, but, as we can see in the pseudocode in Listing 2.1, it does so quite inefciently by essentially repeatedly enumerating the entire state space in a triply nested loop, traversing the state space many times. Soon we will see more efcient methods. 2.2 Tabular Value-Based Agents 41 1 def value_iteration (): 2 initialize(V) 3 while not convergence (V): 4 for s in range(S): 5 for a in range(A): 6 Q[s,a] = \ufffd \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e (\ud835\udc60, \ud835\udc60\u2032) (\ud835\udc45\ud835\udc4e (\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe\ud835\udc49 [\ud835\udc60\u2032]) 7 V[s] = max_a(Q[s,a]) 8 return V Listing 2.1 Value Iteration pseudocode 2.2.4.1 Hands On: Value Iteration in Gym We have discussed in detail how to model a reinforcement learning problem with an MDP. We have talked in depth and at length about states, actions, and policies. It is now time for some hands-on work, to experiment with the theoretical concepts. We will start with the environment. OpenAI Gym OpenAI has created the Gym suite of environments for Python, which has become the de facto standard in the feld [108]. The Gym suite can be found at OpenAI4 and on GitHub.5 Gym works on Linux, macOS and Windows. An active community exists and new environments are created continuously and uploaded to the Gym website. Many interesting environments are available for experimentation, to create your own agent algorithm for, and test it. If you browse Gym on GitHub, you will see diferent sets of environments, from easy to advanced. There are the classics, such as Cartpole and Mountain car. There are also small text environments. Taxi is there, and the Arcade Learning Environment [71], which was used in the paper that introduced DQN [522], as we will discuss at length in the next chapter. MuJoCo6 is also available, an environment for experimentation with simulated robotics [780], or you can use pybullet.7 You should now install Gym. Go to the Gym page on https://gym.openai.com and read the documentation. Make sure Python is installed on your system (does typing python at the command prompt work?), and that your Python version is up to date (version 3.10 at the time of this writing). Then type pip install gym 4 https://gym.openai.com 5 https://github.com/openai/gym 6 http://www.mujoco.org 7 https://pybullet.org/wordpress/ 42 2 Tabular Value-Based Reinforcement Learning 1 import gym 2 3 env = gym.make(\u2019CartPole -v0\u2019) 4 env.reset () 5 for _ in range (1000): 6 env.render () 7 env.step(env. action_space .sample ()) # take a random action 8 env.close () Listing 2.2 Running the Gym CartPole Environment from Gym Fig. 2.8 Taxi world [395] to install Gym with the Python package manager. Soon, you will also be needing deep learning suites, such as TensorFlow or PyTorch. It is recommended to install Gym in the same virtual environment as your upcoming PyTorch and TensorFlow installation, so that you can use both at the same time (see Sect. B.3.3). You may have to install or update other packages, such as numpy, scipy and pyglet, to get Gym to work, depending on your system installation. You can check if the installation works by trying if the CartPole environment works, see Listing 2.2. A window should appear on your screen in which a Cartpole is making random movements (your window system should support OpenGL, and you may need a version of pyglet newer than version 1.5.11 on some operating systems). Taxi Example with Value Iteration The Taxi example (Fig. 2.8) is an environment where taxis move up, down, left, and right, and pickup and drop of passengers. Let us see how we can use value iteration to solve the Taxi problem. The Gym documentation describes the Taxi world as follows. There are four designated locations in the Grid world indicated by 2.2 Tabular Value-Based Agents 43 1 import gym 2 import numpy as np 3 4 def iterate_value_function (v_inp , gamma , env): 5 ret = np.zeros(env.nS) 6 for sid in range(env.nS): 7 temp_v = np.zeros(env.nA) 8 for action in range(env.nA): 9 for (prob , dst_state , reward , is_final) in env.P[sid ][ action ]: 10 temp_v[action] += prob *( reward + gamma*v_inp[ dst_state ]*( not is_final)) 11 ret[sid] = max(temp_v) 12 return ret 13 14 def build_greedy_policy (v_inp , gamma , env): 15 new_policy = np.zeros(env.nS) 16 for state_id in range(env.nS): 17 profits = np.zeros(env.nA) 18 for action in range(env.nA): 19 for (prob , dst_state , reward , is_final) in env.P[ state_id ][ action ]: 20 profits[action] += prob *( reward + gamma*v[ dst_state ]) 21 new_policy[state_id] = np.argmax(profits) 22 return new_policy 23 24 25 env = gym.make(\u2019Taxi -v3\u2019) 26 gamma = 0.9 27 cum_reward = 0 28 n_rounds = 500 29 env.reset () 30 for t_rounds in range(n_rounds): 31 # init env and value function 32 observation = env.reset () 33 v = np.zeros(env.nS) 34 35 # solve MDP 36 for _ in range (100): 37 v_old = v.copy () 38 v = iterate_value_function (v, gamma , env) 39 if np.all(v == v_old): 40 break 41 policy = build_greedy_policy (v, gamma , env).astype(np.int) 42 43 # apply policy 44 for t in range (1000): 45 action = policy[ observation ] 46 observation , reward , done , info = env.step(action) 47 cum_reward += reward 48 if done: 49 break 50 if t_rounds % 50 == 0 and t_rounds > 0: 51 print(cum_reward * 1.0 / (t_rounds + 1)) 52 env.close () Listing 2.3 Value Iteration for Gym Taxi 44 2 Tabular Value-Based Reinforcement Learning R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts of at a random square and the passenger is at a random location. The taxi drives to the passenger\u2019s location, picks up the passenger, drives to the passenger\u2019s destination (another one of the four specifed locations), and then drops of the passenger. Once the passenger is dropped of, the episode ends. The Taxi problem has 500 discrete states: there are 25 taxi positions, fve possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations (25 \u00d7 5 \u00d7 4). The environment returns a new result tuple at each step. There are six discrete deterministic actions for the Taxi driver: 0: Move south 1: Move north 2: Move east 3: Move west 4: Pick up passenger 5: Drop of passenger There is a reward of \u22121 for each action and an additional reward of +20 for delivering the passenger, and a reward of \u221210 for executing actions pickup and dropof illegally. The Taxi environment has a simple transition function, which is used by the agent in the value iteration code.8 Listing 2.3 shows an implementation of value iteration that uses the Taxi environment to fnd a solution. This code is written by Mikhail Trofmov, and illustrates clearly how value iteration frst creates the value function for the states, and then that a policy is formed by fnding the best action in each state, in the build-greedy-policy function.9 To get a feeling for how the algorithms work, please use the value iteration code with the Gym Taxi environment, see to Listing 2.3. Run the code, and play around with some of the hyperparameters to familiarize yourself a bit with Gym and with planning by value iteration. Try to visualize for yourself what the algorithm is doing. This will prepare you for the more complex algorithms that we will look into next. 2.2.4.2 Model-Free Learning The value iteration algorithm can compute the policy function. It uses the transition model in its computation. Frequently, we are in a situation when the exact transition probabilities are not known to the agent, and we need other methods to compute the policy function. For this situation, model-free algorithms have been developed, 8 Note that the code uses the environment to compute the next state, so that we do not have to implement a version of the transition function for the agent. 9 https://gist.github.com/geffy/b2d16d01cbca1ae9e13f11f678fa96fd#file-taxi-vipy 2.2 Tabular Value-Based Agents 45 Name Approach Ref Value Iteration Model-based enumeration [72, 15] SARSA On-policy temporal diference model-free [645] Q-learning Of-policy temporal diference model-free [831] Table 2.1 Tabular Value-Based Approaches where the agent can compute the policy without knowing any of the transition probabilities itself. The development of these model-free methods is a major milestone of reinforcement learning, and we will spend some time to understand how they work. We will start with value-based model-free algorithms. We will see how, when the agent does not know the transition function, an optimal policy can be learned by sampling rewards from the environment. Table 2.1 lists value iteration in conjunction with the value-based model-free algorithms that we cover in this chapter. (Policy-based model-free algorithms will be covered in Chap. 4.) First we will discuss how the principle of temporal diference uses sampling and bootstrapping to construct the value function. We will see how the value function can be use to fnd the best actions, to form the policy. Second, we will discuss which mechanisms for action selection exist, where we will encounter the exploration/exploitation trade-of. Third, we will discuss how to learn from the rewards of the selected actions. We will encounter on-policy learning and of-policy learning. We wil discuss two simple algorithms: SARSA and Q-learning. Let us now start by having a closer look at sampling actions with temporal diference learning. Temporal Diference Learning In the previous section the value function was calculated recursively by using the value function of successor states, following Bellman\u2019s equation (Eq. 2.7). Bootstrapping is a process of subsequent refnement by which old estimates of a value are refned with new updates. It means literally: pull yourself up by your boot straps. Bootstrapping solves the problem of computing a fnal value when we only know how to compute step-by-step intermediate values. Bellman\u2019s recursive computation is a form of bootstrapping. In model-free learning, the role of the transition function is replaced by an iterative sequence of environment samples. A bootstrapping method that can be used to process the samples, and to refne them to approximate the fnal value, is temporal diference learning. Temporal diference learning, TD for short, was introduced by Sutton [740] in 1988. The temporal diference in the name refers to the diference in values between two time steps, which are used to calculate the value at the new time step. Temporal diference learning works by updating the current estimate of the state value \ud835\udc49(\ud835\udc60) (the bootstrap-value) with an error value based on the estimate of the next state that it has gotten through sampling the environment: 46 2 Tabular Value-Based Reinforcement Learning \ud835\udc49(\ud835\udc60) \u2190 \ud835\udc49(\ud835\udc60) + \ud835\udefc[\ud835\udc5f\u2032 + \ud835\udefe\ud835\udc49(\ud835\udc60\u2032) \u2212 \ud835\udc49(\ud835\udc60)] (2.8) Here \ud835\udc60 is the current state, \ud835\udc60\u2032 the new state, and \ud835\udc5f\u2032 the reward of the new state. Note the introduction of \ud835\udefc, the learning rate, which controls how fast the algorithm learns (bootstraps). It is an important parameter; setting the value too high can be deterimental since the last value then dominates the bootstrap process too much. Finding the optimal value will require experimentation. The \ud835\udefe parameter is the discount rate. The last term \u2212\ud835\udc49(\ud835\udc60) subtracts the value of the current state, to compute the temporal diference. Another way to write this update rule is \ud835\udc49(\ud835\udc60) \u2190 \ud835\udefc[\ud835\udc5f\u2032 + \ud835\udefe\ud835\udc49(\ud835\udc60\u2032)] + (1 \u2212 \ud835\udefc)\ud835\udc49(\ud835\udc60) as the diference between the new temporal diference target and the old value. Note the absence of transition model \ud835\udc47 in the formula; temporal diference is a model-free update formula. The introduction of the temporal diference method has allowed model-free methods to be used successfully in various reinforcement learning settings. Most notably, it was the basis of the program TD-Gammon, that beat human worldchampions in the game of Backgammon in the early 1990s [763]. Find Policy by Value-based Learning The goal of reinforcement learning is to construct the policy with the highest cumulative reward. Thus, we must fnd the best action \ud835\udc4e in each state \ud835\udc60. In the value-based approach we know the value functions \ud835\udc49(\ud835\udc60) or \ud835\udc44(\ud835\udc60, \ud835\udc4e). How can that help us to fnd action \ud835\udc4e? In a discrete action space, there is at least one discrete action with the highest value. Thus, if we have the optimal state-value \ud835\udc49\u2605, then the optimal policy can be found by fnding the action with that value. This relationship is given by \ud835\udf0b\u2605 = max \ud835\udf0b \ud835\udc49 \ud835\udf0b(\ud835\udc60) = max \ud835\udc4e, \ud835\udf0b \ud835\udc44 \ud835\udf0b(\ud835\udc60, \ud835\udc4e) and the arg max function fnds the best action for us \ud835\udc4e\u2605 = arg max \ud835\udc4e\u2208\ud835\udc34 \ud835\udc44\u2605(\ud835\udc60, \ud835\udc4e). In this way the optimal policy sequence of best actions \ud835\udf0b\u2605(\ud835\udc60) can be recovered from the value functions, hence the name value-based method [846]. A full reinforcement learning algorithm consists of a rule for the selection (downward) and a rule for the learning (upward) step. Now that we know how to calculate the value function (the up-motion in the tree diagram), let us see how we can select the action in our model-free algorithm (the down-motion in the tree diagram). 2.2 Tabular Value-Based Agents 47 2.2.4.3 Exploration Since there is no local transition function, model-free methods perform their state changes directly in the environment. This may be an expensive operation, for example, when a real-world robot arm has to perform a movement. The sampling policy should choose promising actions to reduce the number of samples as much as possible, and not waste any actions. What behavior policy should we use? It is tempting to favor at each state the actions with the highest Q-value, since then we would be following what is currently thought to be the best policy. This approach is called the greedy approach. It appears attractive, but is shortsighted and risks settling for local maxima. Following the trodden path based on only a few early samples risks missing a potential better path. Indeed, the greedy approach is high variance, using values based on few samples, resulting in a high uncertainty. We run the risk of circular reinforcement, if we update the same behavior policy that we use to choose our samples from. In addition to exploiting known good actions, a certain amount of exploration of unknown actions is necessary. Smart sampling strategies use a mix of the current behavior policy (exploitation) and randomness (exploration) to select which action to perform in the environment. Bandit Theory The exploration/exploitation trade-of, the question of how to get the most reliable information at the least cost, has been studied extensively in the literature [346, 845]. The feld has the colorful name of multi-armed bandit theory [30, 443, 279, 632]. A bandit in this context refers to a casino slot machine, with not one arm, but many arms, each with a diferent and unknown payout probability. Each trial costs a coin. The multi-armed bandit problem is then to fnd a strategy that fnds the arm with the highest payout at the least cost. A multi-armed bandit is a single-state single-decision reinforcement learning problem, a one-step non-sequential decision making problem, with the arms representing the possible actions. This simplifed model of stochastic decision making allows the in-depth study of exploration/exploitation strategies. Single-step exploration/exploitation questions arise for example in clinical trials, where new drugs are tested on test-subjects (real people). The bandit is the trial, and the arms are the choice how many of the test subjects are given the real experimental drug, and how many are given the placebo. This is a serious setting, since the cost may be measured in the quality of human lives. In a conventional fxed randomized controlled trial (supervised setup) the sizes of the groups that get the experimental drugs and the control group would be fxed, and the confdence interval and the duration of the test would also be fxed. In an adaptive trial (bandit setup) the sizes would adapt during the trial depending on the outcomes, with more people getting the drug if it appears to work, and fewer if it does not. 48 2 Tabular Value-Based Reinforcement Learning Fig. 2.9 Adaptive Trial [5] Let us have a look at Fig. 2.9. Assume that the learning process is a clinical trial in which three new compounds are tested for their medical efect on test subjects. In the fxed trial (left panel) all test subjects receive the medicine of their group to the end of the test period, after which the data set is complete and we can determine which of the compounds has the best efect. At that point we know which group has had the best medicine, and which two thirds of the subjects did not, with possibly harmful efect. Clearly, this is not a satisfactory situation. It would be better if we could gradually adjust the proportion of the subjects that receive the medicine that currently looks best, as our confdence in our test results increases as the trial progresses. Indeed, this is what reinforcement learning does (Fig. 2.9, right panel). It uses a mix of exploration and exploitation, adapting the treatment, giving more subjects the promising medicine, while achieving the same confdence as the static trial at the end [443, 442]. \ud835\udf50-greedy Exploration A popular pragmatic approach is to use a fxed ratio of exploration versus exploitation. This approach is known as the \ud835\udf16-greedy approach, which is to mostly try the (greedy) action that currently has the highest policy value except to explore an \ud835\udf16 fraction of times a randomly selected other action. If \ud835\udf16 = 0.1 then 90% of the times the currently-best action is taken, and 10% of the times a random other action. The algorithmic choice between greedily exploiting known information and exploring unknown actions to gain new information is called the exploration/exploitation trade-of. It is a central concept in reinforcement learning; it determines how much confdence we have in our outcome, and how quickly the confdence can be increased and the variance reduced. A second approach is to use an adapative \ud835\udf16-ratio, that changes over time, or over other statistics of the learning process. Other popular approaches to add exploration are to add Dirichlet-noise [425] or to use Thompson sampling [770, 648]. 2.2 Tabular Value-Based Agents 49 2.2.4.4 Of-Policy Learning In addition to the exploration/exploitation selection question, another main theme in the design of reinforcement learning algorithms is how the behavior policy learns. Should it update strictly on-policy\u2014only from its recent actions\u2014or of-policy, learning from all available information? Reinforcement learning is concerned with learning a policy from actions and rewards. The agent selects an action, and learns from the reward that it gets back from the environment. Normally, the learning takes place by consistently backing up the value of the selected action back to the same behavior policy function that was used to select the action: the learning is on-policy. There is, however, an alternative. If the learning takes place by backing up values of another action, not the one selected by the behavior policy, then this is known as of-policy learning. This makes sense when the behavior policy explores by selecting a non-optimal action (it does not perform the greedy exploitation action; this usually results in an inferior reward value). Onpolicy learning would then have to backup the value of the non-optimal exploration action (since otherwise it would not learn on-policy). Of-policy learning, however, is free to backup the value of the best action instead, and not the inferior one selected by the exploration policy, not polluting the policy with a known inferior choice. Thus, in the case of exploration, of-policy learning can be more efcient, by not stubbornly backing up the value of the action selected by the behavior policy, but the value of an older, better, action instead. (In the case of an exploiting step by the policy, on-policy learning and of-policy learning are the same.) Learning methods face a trade-of: they try to learn the best target action from current behavior that is known so far (learning-exploitation), or they choose new (but sometimes non-optimal) behavior. Do we penalize a node when exploration found a worse value (as expected) or do we keep the optimistic value? Thus, ofpolicy learning uses two policies: frst, the behavior policy that is used for actual action selection behavior (sometimes exploring), and second, the target policy that is updated by backing up values. The frst policy performs downward selection to expand states, the second policy performs upward value propagation to update the target policy. In on-policy learning these policies are the same, in of-policy learning the behavior policy and the target policy are separate. A well-known tabular on-policy algorithm is SARSA.10 An even more wellknown tabular of-policy algorithm is Q-learning. Both SARSA and Q-learning converge to the optimal action-value function, although convergence in of-policy can be slower, since older, non-current, values are used. 10 The name of the SARSA algorithm is a play on the MDP symbols as they occur in the action value update formula: \ud835\udc60, \ud835\udc4e, \ud835\udc5f, \ud835\udc60, \ud835\udc4e. 50 2 Tabular Value-Based Reinforcement Learning On-Policy SARSA In on-policy learning a single policy function is used for (downward) action selection and (upward) value backup towards the learning target. SARSA is an on-policy algorithm [645]. On-policy learning updates values directly on the single policy. The same policy function is used for exploration behavior and for the target policy. The SARSA update formula is \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2190 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) + \ud835\udefc[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)] (2.9) Going back to temporal diference (Eq. 2.8), we see that the SARSA formula looks very much like TD, although now we deal with state-action values, and temporal diference dealt with state values. On-policy learning selects an action, evaluates it in the environment, and moves on to better actions, guided by the behavior policy (which is not specifed in the formula, but might be \ud835\udf16-greedy). On-policy learning begins with a behavior policy, samples the state space with this policy, and improves the policy by backing up values of the selected actions. Note that the term \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1) can also be written as \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udf0b(\ud835\udc60\ud835\udc61+1)), highlighting the diference with of-policy learning. SARSA updates its Q-values using the Q-value of the next state \ud835\udc60 and the current policy\u2019s action. The primary advantage of on-policy learning is that it directly optimizes the target of interest, and converges quickly by learning with the direct behavior values. The biggest drawback is sample inefciency, since the target policy is updated with sub-optimal explorative rewards. Of-Policy Q-Learning Of-policy learning is more complicated; it uses separate behavior and target policies: one for exploratory downward selection behavior, and one to update as the current target backup policy. Learning (backing up) is from data of the downward behavior policy, and the whole method is therefore called of-policy learning. The most well-known of-policy algorithm is Q-learning [831]. It gathers information from explored moves, it evaluates states as if a greedy policy was used, even when the actual behavior performed an exploration step. The Q-learning update formula is \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2190 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) + \ud835\udefc[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe max \ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)] (2.10) The diference from on-policy learning is that the \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1) term from Eq. 2.9 has been replaced by \ud835\udefe max\ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e). The learning is from backup values of the best action, not the one that was actually evaluated. Listing 2.4 shows the full pseudocode for Q-learning. 2.2 Tabular Value-Based Agents 51 1 def qlearn(environment , alpha =0.001 , gamma =0.9 , epsilon =0.05): 2 Q[TERMINAL ,_] = 0 # policy 3 for episode in range( max_episodes ): 4 s = s0 5 while s not TERMINAL: # perform steps of one full episode 6 a = epsilongreedy (Q[s], epsilon) 7 (r, sp) = environment (s, a) 8 Q[s,a] = Q[s,a] + alpha *(r+gamma*max(Q[sp])-Q[s,a]) 9 s = sp 10 return Q Listing 2.4 Q-learning pseudocode [831, 743] The reason that Q-learning is of-policy is that it updates its Q-values using the Qvalue of the next state \ud835\udc60, and the greedy action (not necessarily the behavior policy\u2019s action\u2014it is learning of the behavior policy). In this sense, of-policy learning collects all available information and uses it simultaneously to construct the best target policy. On-policy targets follow the behavioral policy and convergence is typically more stable (low variance). Of-policy targets can learn the optimal policy/value (low bias), but they can be unstable due to the max operation, especially in combination with function approximation, as we will see in the next chapter. Sparse Rewards and Reward Shaping Before we conclude this section, we should discuss sparsity. For some environments a reward exists for each state. For the supermarket example a reward can be calculated for each state that the agent has walked to. The reward is actually the opposite of the cost expended in walking. Environments in which a reward exists in each state are said to have a dense reward structure. For other environments rewards may exist for only some of the states. For example, in chess, rewards only exist at terminal board positions where there is a win or a draw. In all other states the return depends on the future states and must be calculated by the agent by propagating reward values from future states up towards the root state \ud835\udc600. Such an environment is said to have a sparse reward structure. Finding a good policy is more complicated when the reward structure is sparse. A graph of the landscape of such a sparse reward function would show a fat landscape with a few sharp mountain peaks. Many of the algorithms that we will see in future chapters use the reward-gradient to fnd good returns. Finding the optimum in a fat landscape where the gradient is zero, is hard. In some applications it is possible to change the reward function to have a shape more amenable to gradient-based optimization algorithms such as we use in deep learning. Reward shaping can make all the diference when no solution can be found with a naive reward function. It is a way of incorporating heuristic knowledge into the MDP. A large literature on 52 2 Tabular Value-Based Reinforcement Learning reward shaping and heuristic information exists [560]. The use of heuristics on board games such as chess and checkers can also be regarded as reward shaping. 2.2.4.5 Hands On: Q-learning on Taxi To get a feeling for how these algorithms work in practice, let us see how Q-learning solves the Taxi problem. In Sect. 2.2.4.1 we discussed how value iteration can be used for the Taxi problem, provided that the agent has access to the transition model. We will now see how we solve this problem if we do not have the transition model. Q-learning samples actions, and records the reward values in a Q-table, converging to the state-action value function. When in all states the best values of the best actions are known, then these can be used to sequence the optimal policy. Let us see how a value-based model-free algorithm solves a simple 5 \u00d7 5 Taxi problem. Refer to Fig. 2.8 on page 42 for an illustration of Taxi world. Please recall that in Taxi world, the taxi can be in one of 25 locations and there are 25 \u00d7 (4 + 1) \u00d7 4 = 500 diferent states that the environment can be in. We follow the reward model as it is used in the Gym Taxi environment. Recall that our goal is to fnd a policy (actions in each state) that leads to the highest cumulative reward. Q-learning learns the best policy through guided sampling. The agent records the rewards that it gets from actions that it performs in the environment. The Q-values are the expected rewards of the actions in the states. The agent uses the Q-values to guide which actions it will sample. Q-values \ud835\udc44(\ud835\udc60, \ud835\udc4e) are stored in an array that is indexed by state and action. The Q-values guide the exploration, higher values indicate better actions. Listing 2.5 shows the full Q-learning algorithm, in Python, after [395]. It uses an \ud835\udf16-greedy behavior policy: mostly the best action is followed, but in a certain fraction a random action is chosen, for exploration. Recall that the Q-values are updated according to the Q-learning formula: \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2190 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) + \ud835\udefc[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe max \ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)] where 0 \u2264 \ud835\udefe \u2264 1 is the discount factor and 0 < \ud835\udefc \u2264 1 the learning rate. Note that Q-learning uses bootstrapping, and the initial Q-values are set to a random value (their value will disappear slowly due to the learning rate). Q-learning learns the best action in the current state by looking at the reward for the current state-action combination, plus the maximum rewards for the next state. Eventually the best policy is found in this way, and the taxi will consider the route consisting of a sequence of the best rewards. To summarize informally: 1. Initialize the Q-table to random values 2. Select a state \ud835\udc60 3. For all possible actions from \ud835\udc60 select the one with the highest Q-value and travel to this state, which becomes the new \ud835\udc60, or, with \ud835\udf16 greedy, explore 2.2 Tabular Value-Based Agents 53 1 # Q learning for OpenAI Gym Taxi environment 2 import gym 3 import numpy as np 4 import random 5 # Environment Setup 6 env = gym.make(\"Taxi -v2\") 7 env.reset () 8 env.render () 9 # Q[state ,action] table implementation 10 Q = np.zeros ([env. observation_space .n, env. action_space .n]) 11 gamma = 0.7 # discount factor 12 alpha = 0.2 # learning rate 13 epsilon = 0.1 # epsilon greedy 14 for episode in range (1000): 15 done = False 16 total_reward = 0 17 state = env.reset () 18 while not done: 19 if random.uniform (0, 1) < epsilon: 20 action = env. action_space .sample () # Explore state space 21 else: 22 action = np.argmax(Q[state ]) # Exploit learned values 23 next_state , reward , done , info = env.step(action) # invoke Gym 24 next_max = np.max(Q[next_state ]) 25 old_value = Q[state ,action] 26 27 new_value = old_value + alpha * (reward + gamma * next_max - old_value) 28 29 Q[state ,action] = new_value 30 total_reward += reward 31 state = next_state 32 if episode % 100 == 0: 33 print(\"Episode\u2423{}\u2423Total\u2423Reward:\u2423{}\".format(episode , total_reward )) Listing 2.5 Q-learning Taxi example, after [395] 4. Update the values in the Q-array using the equation 5. Repeat until the goal is reached; when the goal state is reached, repeat the process until the Q-values stop changing (much), then stop. Listing 2.5 shows Q-learning code for fnding the policy in Taxi world. The optimal policy can be found by sequencing together the actions with the highest Q-value in each state. Listing 2.6 shows the code for this. The number of illegal pickups/drop-ofs is shown as penalty. This example shows how the optimal policy can be found by the introduction of a Q-table that records the quality of irreversible actions in each state, and uses that 54 2 Tabular Value-Based Reinforcement Learning 1 total_epochs , total_penalties = 0, 0 2 ep = 100 3 for _ in range(ep): 4 state = env.reset () 5 epochs , penalties , reward = 0, 0, 0 6 done = False 7 while not done: 8 action = np.argmax(Q[state ]) 9 state , reward , done , info = env.step(action) 10 if reward == -10: 11 penalties += 1 12 epochs += 1 13 total_penalties += penalties 14 total_epochs += epochs 15 print(f\"Results\u2423after\u2423{ep}\u2423episodes:\") 16 print(f\"Average\u2423timesteps\u2423per\u2423episode:\u2423{ total_epochs \u2423/\u2423ep}\") 17 print(f\"Average\u2423penalties\u2423per\u2423episode:\u2423{ total_penalties \u2423/\u2423ep}\") Listing 2.6 Evaluate the optimal Taxi result, after [395] table to converge the rewards to the value function. In this way the optimal policy can be found model-free. Tuning your Learning Rate Go ahead, implement and run this code, and play around to become familiar with the algorithm. Q-learning is an excellent algorithm to learn the essence of how reinforcement learning works. Try out diferent values for hyperparameters, such as the exploration parameter \ud835\udf16, the discount factor \ud835\udefe and the learning rate \ud835\udefc. To be successful in this feld, it helps to have a feeling for these hyperparameters. A choice close to 1 for the discount parameter is usually a good start, and a choice close to 0 for the learning rate is a good start. You may feel a tendency to do the opposite, to choose the learning rate as high as possible (close to 1) to learn as quickly as possible. Please go ahead and see which works best in Q-learning (you can have a look at [231]). In many deep learning environments a high learning rate is a recipe for disaster, your algorithm may not converge at all, and Q-values can become unbounded. Play around with tabular Q-learning, and approach your deep learning slowly, with gentle steps! The Taxi example is small, and you will get results quickly. It is well suited to build up useful intuition. In later chapters, we will do experiments with deep learning that take longer to converge, and acquiring intuition for tuning hyperparameter values will be more expensive. 2.3 Classic Gym Environments 55 Conclusion We have now seen how a value function can be learned by an agent without having the transition function, by sampling the environment. Model-free methods use actions that are irreversible for the agent. The agent samples states and rewards from the environment, using a behavior policy with the current best action, and following an exploration/exploitation trade-of. The backup rule for learning is based on bootstrapping, and can follow the rewards of the actions on-policy, including the value of the occasional explorative action, or of-policy, always using the value of the best action. We have seen two model-free tabular algorithms, SARSA and Q-learning, where the value function is assumed to be stored in an exact table structure. In the next chapter we will move to network-based algorithms for highdimensional state spaces, based on function approximation with a deep neural network. 2.3 Classic Gym Environments Now that we have discussed at length the tabular agent algorithms, it is time to have a look at the environments, the other part of the reinforcement learning model. Without them, progress cannot be measured, and results cannot be compared in a meaningful way. In a real sense, environments defne the kind of intelligence that our artifcial methods can be trained to perform. In this chapter we will start with a few smaller environments, that are suited for the tabular algorithms that we have discussed. Two environments that have been around since the early days of reinforcement learning are Mountain car and Cartpole (see Fig. 2.10). 2.3.1 Mountain Car and Cartpole Mountain car is a physics puzzle in which a car on a one-dimensional road is in a valley between two mountains. The goal for the car is to drive up the mountain and reach the fag on the right. The car\u2019s engine can go forward and backward. The problem is that the car\u2019s engine is not strong enough to climb the mountain by itself in a single pass [532], but it can do so with the help of gravity: by repeatedly going back and forth the car can build up momentum. The challenge for the reinforcement learning agent is to apply alternating backward and forward forces at the right moment. Cartpole is a pole-balancing problem. A pole is attached by a joint to a movable cart, which can be pushed forward or backward. The pendulum starts upright, and must be kept upright by applying either a force of +1 or \u22121 to the cart. The puzzle 56 2 Tabular Value-Based Reinforcement Learning Fig. 2.10 Cartpole and Mountain Car ends when the pole falls over, or when the cart runs too far left or right [57]. Again the challenge is to apply the right force at the right moment, solely by feedback of the pole being upright or too far down. 2.3.2 Path Planning and Board Games Navigation tasks and board games provide environments for reinforcement learning that are simple to understand. They are well suited to reason about new agent algorithms. Navigation problems, and the heuristic search trees built for board games, can be of moderate size, and are then suited for determining the best action by dynamic programming methods, such as tabular Q-learning, A*, branch and bound, and alpha-beta [647]. These are straightforward search methods that do not attempt to generalize to new, unseen, states. They fnd the best action in a space of states, all of which are present at training time\u2014the optimization methods do not perform generalization from training to test time. Path Planning Path planning (Fig 2.1) is a classic problem that is related to robotics [456, 265]. Popular versions are mazes, as we have seen earlier (Fig. 2.2). The Taxi domain (Fig. 2.8) was originally introduced in the context of hierarchical problem solving [196]. Boxpushing problems such as Sokoban are frequently used as well [386, 204, 543, 878], see Fig. 2.3. The action space of these puzzles and mazes is discrete. Basic path and motion planning can enumerate possible solutions [169, 322]. Small versions of mazes can be solved exactly by enumeration, larger instances are only suitable for approximation methods. Mazes can be used to test algorithms for path fnding problems and are frequently used to do so. Navigation tasks and box-pushing games such as Sokoban can feature rooms or subgoals, that may then 2.3 Classic Gym Environments 57 be used to test algorithms for hierarchically structured problems [236, 298, 615, 239] (Chap. 8). The problems can be made more difcult by enlarging the grid and by inserting more obstacles. Board Games Board games are a classic group of benchmarks for planning and learning since the earliest days of artifcial intelligence. Two-person zero-sum perfect information board games such as tic tac toe, chess, checkers, Go, and shogi have been used to test algorithms since the 1950s. The action space of these games is discrete. Notable achievements were in checkers, chess, and Go, where human world champions were defeated in 1994, 1997, and 2016, respectively [663, 124, 703]. The board games are typically used \u201cas is\u201d and are not changed for diferent experiments (in contrast to mazes, that are often adapted in size or complexity for specifc purposes of the experiment). Board games are used for the difculty of the challenge. The ultimate goals is to beat human grandmasters or even the world champion. Board games have been traditional mainstays of artifcial intelligence, mostly associated with the search-based symbolic reasoning approach to artifcial intelligence [647]. In contrast, the benchmarks in the next chapter are associated with connectionist artifcial intelligence. Summary and Further Reading This has been a long chapter, to provide a solid basis for the rest of the book. We will summarize the chapter, and provide references for further reading. Summary Reinforcement learning can learn behavior that achieves high rewards, using feedback from the environment. Reinforcement learning has no supervisory labels, it can learn beyond a teacher, as long as there is an environment that provides feedback. Reinforcement learning problems are modeled as a Markov decision problem, consisting of a 5-tuple (\ud835\udc46, \ud835\udc34,\ud835\udc47\ud835\udc4e, \ud835\udc45\ud835\udc4e, \ud835\udefe) for states, actions, transition, reward, and discount factor. The agent performs an action, and the environment returns the new state and the reward value to be associated with the new state. Games and robotics are two important felds of application. Fields of application can be episodic (they end\u2014such as a game of chess) or continuous (they do not end\u2014a robot remains in the world). In continuous problems it often makes sense to 58 2 Tabular Value-Based Reinforcement Learning discount behavior that is far from the present, episodic problems typically do not bother with a discount factor\u2014a win is a win. Environments can be deterministic (many board games are deterministic\u2014boards don\u2019t move) or stochastic (many robotic worlds are stochastic\u2014the world around a robot moves). The action space can be discrete (a piece either moves to a square or it does not) or continuous (typical robot joints move continuously over an angle). The goal in reinforcement learning is to fnd the optimal policy that gives for all states the best actions, maximizing the cumulative future reward. The policy function is used in two diferent ways. In a discrete environment the policy function \ud835\udc4e = \ud835\udf0b(\ud835\udc60) returns for each state the best action in that sate. (Alternatively the value function returns the value of each action in each state, out of which the argmax function can be used to fnd the actions with the highest value.) The optimal policy can be found by fnding the maximal value of a state. The value function \ud835\udc49(\ud835\udc60) returns the expected reward for a state. When the transition function \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) is present, the agent can use Bellman\u2019s equation, or a dynamic programming method to recursively traverse the behavior space. Value iteration is one such dynamic programming method. Value iteration traverses all actions of all states, backing up reward values, until the value function stops changing. The state-action value \ud835\udc44(\ud835\udc60, \ud835\udc4e) determines the value of an action of a state. Bellman\u2019s equation calculates the value of a state by calculating the value of successor states. Accessing successor states (by following the action and transition) is also called expanding a successor state. In a tree diagram successor states are called child nodes, and expanding is a downward action. Backpropagating the reward values to the parent node is a movement upward in the tree. Methods where the agent makes use of the transition model are called modelbased methods. When the agent does not use the transition model, they are modelfree methods. In many situations the learning agent does not have access to the transition model of the environment, and planning methods cannot be used by the agent. Value-based model-free methods can fnd an optimal policy by using only irreversible actions, sampling the environment to fnd the value of the actions. A major determinant in model-free reinforcement learning is the exploration/exploitation trade-of, or how much of the information that has been learned from the environment is used in choosing actions to sample. We discussed the advantages of exploiting the latest knowledge in settings where environment actions are very costly, such as clinial trials. A well-known exploration/exploitation method is \ud835\udf16greedy, where the greedy (best) action is followed from the behavior policy, except in \ud835\udf16 times, when random exploration is performed. Always following the policy\u2019s best action runs the risk of getting stuck in a cycle. Exploring random nodes allows breaking free of such cycles. So far we have discussed the action selection operation. How should we process the rewards that are found at nodes? Here we introduced another fundamental element of reinforcement learning: bootstrapping, or fnding a value by refning a previous value. Temporal diference learning uses the principle of bootstrapping to fnd the value of a state by adding appropriately discounted future reward values to the state value function. 2.3 Classic Gym Environments 59 We have now discussed both up and down motions, and can construct full modelfree algorithms. The best-known algorithm may well be Q-learning, which learns the action-value function of each action in each state through of-policy temporal diference learning. Of-policy algorithms improve the best policy function always with the value of the best action, even if the behavior action was less. Of-policy learning thus has a behavior policy that is diferent form the policy that is optimized. In the next chapters we will look at value-based and policy-based model-free methods for large, complex problems, that make use of function approximation (deep learning). Further Reading There is a rich literature on tabular reinforcement learning. A standard work for tabular value-based reinforcement learning is Sutton and Barto [743]. Two condensed introductions to reinforcement learning are [27, 255]. Another major work on reinforcement learning is Bertsekas and Tsitsiklis [86]. Kaelbling has written an important survey article on the feld [389]. The early works of Richard Bellman on dynamic programming, and planning algorithms are [72, 73]. For a recent treatment of games and reinforcement learning, with a focus on heuristic search methods and the methods behind AlphaZero, see [600]. The methods of this chapter are based on bootstrapping [72] and temporal diference learning [740]. The on-policy algorithm SARSA [645] and the of-policy algorithm Q-Learning [831] are among the best known exact, tabular, value-based model-free algorithms. Mazes and Sokoban grids are sometimes procedurally generated [692, 329, 781]. The goal for the algorithms is typically to fnd a solution for a grid of a certain difculty class, to fnd a shortest path solution, or, in transfer learning, to learn to solve a class of grids by training on a diferent class of grids [859]. For general reference, one of the major textbooks on artifcial intelligence is written by Russell and Norvig [647]. A more specifc textbook on machine learning is by Bishop [93]. Exercises We will end with questions on key concepts, with programming exercises to build up more experience, 60 2 Tabular Value-Based Reinforcement Learning Questions The questions below are meant to refresh your memory, and should be answered with yes, no, or short answers of one or two sentences. 1. In reinforcement learning the agent can choose which training examples are generated. Why is this benefcial? What is a potential problem? 2. What is Grid world? 3. Which fve elements does an MDP have to model reinforcement learning problems? 4. In a tree diagram, is successor selection of behavior up or down? 5. In a tree diagram, is learning values through backpropagation up or down? 6. What is \ud835\udf0f? 7. What is \ud835\udf0b(\ud835\udc60)? 8. What is \ud835\udc49(\ud835\udc60)? 9. What is \ud835\udc44(\ud835\udc60, \ud835\udc4e)? 10. What is dynamic programming? 11. What is recursion? 12. Do you know a dynamic programming method to determine the value of a state? 13. Is an action in an environment reversible for the agent? 14. Mention two typical application areas of reinforcement learning. 15. Is the action space of games typically discrete or continuous? 16. Is the action space of robots typically discrete or continuous? 17. Is the environment of games typically deterministic or stochastic? 18. Is the environment of robots typically deterministic or stochastic? 19. What is the goal of reinforcement learning? 20. Which of the fve MDP elements is not used in episodic problems? 21. Which model or function is meant when we say \u201cmodel-free\u201d or \u201cmodel-based\u201d? 22. What type of action space and what type of environment are suited for valuebased methods? 23. Why are value-based methods used for games and not for robotics? 24. Name two basic Gym environments. Exercises There is an even better way to learn about deep reinforcement learning then reading about it, and that is to perform experiments yourself, to see the learning processes unfold before your own eyes. The following exercises are meant as starting points for your own discoveries in the world of deep reinforcement learning. Consider using Gym to implement these exercises. Section 2.2.4.1 explains how to install Gym. 1. Q-learning Implement Q-learning for Taxi, including the procedure to derive the best policy for the Q-table. Go to Sect. 2.2.4.5 and implement it. Print the Q-table, to see the values on the squares. You could print a live policy as the 2.3 Classic Gym Environments 61 1 # SARSA for OpenAI Gym Taxi environment 2 import gym 3 import numpy as np 4 import random 5 # Environment Setup 6 env = gym.make(\"Taxi -v2\") 7 env.reset () 8 env.render () 9 # Q[state ,action] table implementation 10 Q = np.zeros ([env. observation_space .n, env. action_space .n]) 11 gamma = 0.7 # discount factor 12 alpha = 0.2 # learning rate 13 epsilon = 0.1 # epsilon greedy 14 for episode in range (1000): 15 done = False 16 total_reward = 0 17 current_state = env.reset () 18 if random.uniform (0, 1) < epsilon: 19 current_action = env. action_space .sample () # Explore state space 20 else: 21 current_action = np.argmax(Q[ current_state ]) # Exploit learned values 22 while not done: 23 next_state , reward , done , info = env.step( current_action ) # invoke Gym 24 if random.uniform (0, 1) < epsilon: 25 next_action = env. action_space .sample () # Explore state space 26 else: 27 next_action = np.argmax(Q[ next_state ]) # Exploit learned values 28 sarsa_value = Q[next_state , next_action ] 29 old_value = Q[current_state , current_action ] 30 31 new_value = old_value + alpha * (reward + gamma * sarsa_value - old_value) 32 33 Q[current_state , current_action ] = new_value 34 total_reward += reward 35 current_state = next_state 36 current_action = next_action 37 if episode % 100 == 0: 38 print(\"Episode\u2423{}\u2423Total\u2423Reward:\u2423{}\".format(episode , total_reward )) Listing 2.7 SARSA Taxi example, after [395] 62 2 Tabular Value-Based Reinforcement Learning search progresses. Try diferent values for \ud835\udf16, the exploration rate. Does it learn faster? Does it keep fnding the optimal solution? Try diferent values for \ud835\udefc, the learning rate. Is it faster? 2. SARSA Implement SARSA, the code is in Listing 2.7. Compare your results to Q-learning, can you see how SARSA chooses diferent paths? Try diferent \ud835\udf16 and \ud835\udefc. 3. Problem size How large can problems be before converging starts taking too long? 4. Cartpole Run Cartpole with the greedy policy computed by value iteration. Can you make it work? Is value iteration a suitable algorithm for Cartpole? If not, why do you think it is not? Chapter 3 Deep Value-Based Reinforcement Learning The previous chapter introduced the feld of classic reinforcement learning. We learned about agents and environments, and about states, actions, values, and policy functions. We also saw our frst planning and learning algorithms: value iteration, SARSA and Q-learning. The methods in the previous chapter were exact, tabular, methods, that work for problems of moderate size that ft in memory. In this chapter we move to high-dimensional problems with large state spaces that no longer ft in memory. We will go beyond tabular methods and use methods to approximate the value function and to generalize beyond trained behavior. We will do so with deep learning. The methods in this chapter are deep, model-free, value-based methods, related to Q-learning. We will start by having a closer look at the new, larger, environments that our agents must now be able to solve (or rather, approximate). Next, we will look at deep reinforcement learning algorithms. In reinforcement learning the current behavior policy determines which action is selected next, a process that can be self-reinforcing. There is no ground truth, as in supervised learning. The targets of loss functions are no longer static, or even stable. In deep reinforcement learning convergence to \ud835\udc49 and \ud835\udc44 values is based on a bootstrapping process, and the frst challenge is to fnd training methods that converge to stable function values. Furthermore, since with neural networks the states are approximated based on their features, convergence proofs can no longer count on identifying states individually. For many years it was assumed that deep reinforcement learning is inherently unstable due to a so-called deadly triad of bootstrapping, function approximation and of-policy learning. However, surprisngly, solutions have been found for many of the challenges. By combining a number of approaches (such as the replay bufer and increased exploration) the Deep Q-Networks algorithm (DQN) was able to achieve stable learning in a high-dimensional environment. The success of DQN spawned a large research efort to improve training further. We will discuss some of these new methods. The chapter is concluded with exercises, a summary, and pointers to further reading. 63 64 3 Deep Value-Based Reinforcement Learning Deep Learning: Deep reinforcement learning builds on deep supervised learning, and this chapter and the rest of the book assume a basic understanding of deep learning. When your knowledge of parameterized neural networks and function approximation is rusty, this is the time to go to Appendix B and take an in-depth refresher. The appendix also reviews essential concepts such as training, testing, accuracy, overftting and the bias-variance trade-of. When in doubt, try to answer the questions on page 328. Core Concepts \u2022 Stable convergence \u2022 Replay bufer Core Problem \u2022 Achieve stable deep reinforcement learning in large problems Core Algorithm \u2022 Deep Q-network (Listing 3.6) End-to-end Learning Before the advent of deep learning, traditional reinforcement learning had been used mostly on smaller problems such as puzzles, or the supermarket example. Their state space fts in the memories of our computers. Reward shaping, in the form of domain-specifc heuristics, can be used to shoehorn the problem into a computer, for example, in chess and checkers [124, 354, 662]. Impressive results are achieved, but at the cost of extensive problem-specifc reward shaping and heuristic engineering [600]. Deep learning changed this situation, and reinforcement learning is now used on high-dimensional problems that are too large to ft into memory. In the feld of supervised learning, a yearly competition had created years of steady progress in which the accuracy of image classifcation had steadily improved. Progress was driven by the availability of ImageNet, a large database of labeled 3 Deep Value-Based Reinforcement Learning 65 Fig. 3.1 Example Game from the Arcade Learning Environment [71] Fig. 3.2 Atari Experiments on the Cover of Nature images [237, 191], by increases in computation power through GPUs, and by steady improvement of machine learning algorithms, especially in deep convolutional neural networks. In 2012, a paper by Krizhevsky, Sutskever and Hinton presented a method that out-performed other algorithms by a large margin, and approached the performance of human image recognition [431]. The paper introduced the AlexNet architecture (after the frst name of the frst author) and 2012 is often regarded as the year of the breakthrough of deep learning. (See Appendix B.3.1 for details.) This breakthrough raised the question whether something similar in deep reinforcement learning could be achieved. 66 3 Deep Value-Based Reinforcement Learning We did not have to wait long, only a year later, in 2013, at the deep learning workshop of one of the main AI conferences, a paper was presented on an algorithm that could play 1980s Atari video games just by training on the pixel input of the video screen (Fig. 3.1). The algorithm used a combination of deep learning and Q-learning, and was named Deep Q-Network, or DQN [522, 523]. An illuminating video of how it learned to play the game Breakout is here.1 This was a breakthrough for reinforcement learning. Many researchers at the workshop could relate to this achievement, perhaps because they had spent hours playing Space Invaders, PacMan and Pong themselves when they were younger. Two years after the presentation at the deep learning workshop a longer article appeared in the journal Nature in which a refned and expanded version of DQN was presented (see Fig. 3.2 for the journal cover). Why was this such a momentous achievement? Besides the fact that the problem that was solved was easily understood, true eye-hand coordination of this complexity had not been achieved by a computer before; furthermore, the end-to-end learning from pixel to joystick implied artifcial behavior that was close to how humans play games. DQN essentially launched the feld of deep reinforcement learning. For the frst time the power of deep learning had been successfully combined with behavior learning, for an imaginative problem. A major technical challenge that was overcome by DQN was the instability of the deep reinforcement learning process. In fact, there were convincing theoretical analyses at the time that this instability was fundamental, and it was generally assumed that it would be next to impossible to overcome [48, 283, 787, 743], since the target of the loss-function depended on the convergence of the reinforcement learning process itself. By the end of this chapter we will have covered the problems of convergence and stability in reinforcement learning. We will have seen how DQN addresses these problems, and we will also have discussed some of the many further solutions that were devised after DQN. But let us frst have a look at the kind of new, high-dimensional, environments that were the cause of these developments. 3.1 Large, High-Dimensional, Problems In the previous chapter, Grid worlds and mazes were introduced as basic sequential decision making problems in which exact, tabular, reinforcement learning methods work well. These are problems of moderate complexity. The complexity of a problem is related to the number of unique states that a problem has, or how large the state space is. Tabular methods work for small problems, where the entire state space fts in memory. This is for example the case with linear regression, which has only one variable \ud835\udc65 and two parameters \ud835\udc4e and \ud835\udc4f, or the Taxi problem, which has a state space of size 500. In this chapter we will be more ambitious and introduce various 1 https://www.youtube.com/watch?v=TmPfTpjtdgg 3.1 Large, High-Dimensional, Problems 67 Fig. 3.3 Atari 2600 console games, most notably Atari arcade games. The state space of a single frame of Atari video input is 210 \u00d7 160 pixels of 256 RGB color values = 25633600. There is a qualitative diference between small (500) and large (25633600) problems.2 For small problems the policy can be learned by loading all states of a problem in memory. States are identifed individually, and each has its own best action, that we can try to fnd. Large problems, in contrast, do not ft in memory, the policy cannot be memorized, and states are grouped together based on their features (see Sect. B.1.3, where we discuss feature learning). A parameterized network maps states to actions and values; states are no longer individually identifable in a lookup table. When deep learning methods were introduced in reinforcement learning, larger problems than before could be solved. Let us have a look at those problems. 3.1.1 Atari Arcade Games Learning actions directly from high-dimensional sound and vision inputs is one of the long-standing challenges of artifcial intelligence. To stimulate this research, in 2012 a test-bed was created designed to provide challenging reinforcement learning tasks. It was called the Arcade learning environment, or ALE [71], and it was based on a simulator for 1980s Atari 2600 video games. Figure 3.3 shows a picture of a distinctly retro Atari 2600 gaming console. Among other things ALE contains an emulator of the Atari 2600 console. ALE presents agents with a high-dimensional3 visual input (210 \u00d7 160 RGB video at 60 Hz, or 60 images per second) of tasks that were designed to be interesting and challenging for human players (Fig. 3.1 showed an example of such a game and 2 See Sect. B.1.2, where we discuss the curse of dimensionality. 3 That is, high dimensional for machine learning. 210 \u00d7 160 pixels is not exactly high-defnition video quality. 68 3 Deep Value-Based Reinforcement Learning Fig. 3.4 Screenshots of 4 Atari Games (Breakout, Pong, Montezuma\u2019s Revenge, and Private Eye) Fig. 3.4 shows a few more). The game cartridge ROM holds 2-4 kB of game code, while the console random-access memory is small, just 128 bytes (really, just 128 bytes, although the video memory is larger, of course). The actions can be selected via a joystick (9 directions), which has a fre button (fre on/of), giving 18 actions in total. The Atari games provide challenging eye-hand coordination and reasoning tasks, that are both familiar and challenging to humans, providing a good test-bed for learning sequential decision making. Atari games, with high-resolution video input at high frame rates, are an entirely diferent kind of challenge than Grid worlds or board games. Atari is a step closer to a human environment in which visual inputs should quickly be followed by correct actions. Indeed, the Atari benchmark called for very diferent agent algorithms, prompting the move from tabular algorithms to algorithms based on function approximation and deep learning. ALE has become a standard benchmark in deep reinforcement learning research. 3.1.2 Real-Time Strategy and Video Games Real-time strategy games provide an even greater challenge than simulated 1980s Atari consoles. Games such as StarCraft (Fig. 1.6) [573], and Capture the Flag [373] have very large state spaces. These are games with large maps, many players, many pieces, and many types of actions. The state space of StarCraft is estimated at 3.2 Deep Value-Based Agents 69 101685 [573], more than 1500 orders of magnitude larger than Go (10170) [540, 786] and more than 1635 orders of magnitude large than chess (1047) [355]. Most real time strategy games are multi-player, non-zero-sum, imperfect information games that also feature high-dimensional pixel input, reasoning, and team collaboration. The action space is stochastic and is a mix of discrete and continuous actions. Despite the challenging nature, impressive achievements have been reported recently in three games where human performance was matched or even exceeded [813, 80, 373], see also Chap. 7. Let us have a look at the methods that can solve these very diferent types of problems. 3.2 Deep Value-Based Agents We will now turn to agent algorithms for solving large sequential decision problems. The main challenge of this section is to create an agent algorithm that can learn a good policy by interacting with the world\u2014with a large problem, not a toy problem. From now on, our agents will be deep learning agents. The questions that we are faced with, are the following. How can we use deep learning for high-dimensional and large sequential decision making environments? How can tabular value and policy functions \ud835\udc49, \ud835\udc44, and \ud835\udf0b be transformed into \ud835\udf03 parameterized functions \ud835\udc49\ud835\udf03, \ud835\udc44 \ud835\udf03, and \ud835\udf0b\ud835\udf03? 3.2.1 Generalization of Large Problems with Deep Learning Recall from Appendix B that deep supervised learning uses a static dataset to approximate a function, and that the labels are static targets in an optimization process where the loss-function is minimized. Deep reinforcement learning is based on the observation that bootstrapping is also a kind of minimization process in which an error (or diference) is minimized. In reinforcement learning this bootstrapping process converges on the true state value and state-action value functions. However, the Q-learning bootstrapping process lacks static ground truths; our data items are generated dynamically, and our lossfunction targets move. The movement of the loss-function targets is infuenced by the same policy function that the convergence process is trying to learn. It has taken quite some efort to fnd deep learning algorithms that converge to stable functions on these moving targets. Let us try to understand in more detail how the supervised methods have to be adapted in order to work in reinforcement learning. We do this by comparing three algorithmic structures: supervised minimization, tabular Q-learning, and deep Q-learning. 70 3 Deep Value-Based Reinforcement Learning 1 def train_sl(data , net , alpha =0.001): # train classifier 2 for epoch in range(max_epochs ): # an epoch is one pass 3 sum_sq = 0 # reset to zero for each pass 4 for (image , label) in data: 5 output = net. forward_pass (image) # predict 6 sum_sq += (output - label)**2 # compute error 7 grad = net.gradient(sum_sq) # derivative of error 8 net. backward_pass (grad , alpha) # adjust weights 9 return net Listing 3.1 Network training pseudocode for supervised learning 1 def qlearn(environment , alpha =0.001 , gamma =0.9 , epsilon =0.05): 2 Q[TERMINAL ,_] = 0 # policy 3 for episode in range( max_episodes ): 4 s = s0 5 while s not TERMINAL: # perform steps of one full episode 6 a = epsilongreedy (Q[s], epsilon) 7 (r, sp) = environment (s, a) 8 Q[s,a] = Q[s,a] + alpha *(r+gamma*max(Q[sp])-Q[s,a]) 9 s = sp 10 return Q Listing 3.2 Q-learning pseudocode [831, 743] 3.2.1.1 Minimizing Supervised Target Loss Listing 3.1 shows pseudocode for a typical supervised deep learning training algorithm, consisting of an input dataset, a forward pass that calculates the network output, a loss computation, and a backward pass. See Appendix B or [280] for more details. We see that the code consists of a double loop: the outer loop controls the training epochs. Epochs consist of forward approximation of the target value using the parameters, computation of the gradient, and backward adjusting of the parameters with the gradient. In each epoch the inner loop serves all examples of the static dataset to the forward computation of the output value, the loss and the gradient computation, so that the parameters can be adjusted in the backward pass. The dataset is static, and all that the inner loop does is deliver the samples to the backpropagation algorithm. Note that each sample is independent of the other, samples are chosen with equal probability. After an image of a white horse is sampled, the probability that the next image is of a black grouse or a blue moon is equally (un)likely. 3.2 Deep Value-Based Agents 71 1 def train_qlearn (environment , Qnet , alpha =0.001 , gamma =0.0 , epsilon =0.05 2 s = s0 # initialize start state 3 for epoch in range(max_epochs ): # an epoch is one pass 4 sum_sq = 0 # reset to zero for each pass 5 while s not TERMINAL: # perform steps of one full episode 6 a = epsilongreedy (Qnet(s,a)) # net: Q[s,a]-values 7 (r, sp) = environment (a) 8 output = Qnet. forward_pass (s, a) 9 target = r + gamma * max(Qnet(sp)) 10 sum_sq += (target - output)**2 11 s = sp 12 grad = Qnet.gradient(sum_sq) 13 Qnet. backward_pass (grad , alpha) 14 return Qnet # Q-values Listing 3.3 Network training pseudocode for reinforcement learning 3.2.1.2 Bootstrapping Q-Values Let us now look at Q-learning. Reinforcement learning chooses the training examples diferently. For convergence of algorithms such as Q-learning, the selection rule must guarantee that eventually all states will be sampled by the environment [831]. For large problems, this is not the case; this condition for convergence to the value function does not hold. Listing 3.2 shows the short version of the bootstrapping tabular Q-learning pseudocode from the previous chapter. As in the previous deep learning algorithm, the algorithm consists of a double loop. The outer loop controls the Q-value convergence episodes, and each episode consists of a single trace of (time) steps from the start state to a terminal state. The Q-values are stored in a Python-array indexed by \ud835\udc60 and \ud835\udc4e, since Q is the state-action value. Convergence of the Q-values is assumed to have occurred when enough episodes have been sampled. The Q-formula shows how the Q-values are built up by bootstrapping on previous values, and how Q-learning is learning of-policy, taking the max value of an action. A diference with the supervised learning is that in Q-learning subsequent samples are not independent. The next action is determined by the current policy, and will most likely be the best action of the state (\ud835\udf16-greedy). Furthermore, the next state will be correlated to the previous state in the trajectory. After a state of the ball in the upper left corner of the feld has been sampled, the next sample will with very high probability also be of a state where the ball is close to the upper left corner of the feld. Training can be stuck in local minima, hence the need for exploration. 72 3 Deep Value-Based Reinforcement Learning 3.2.1.3 Deep Reinforcement Learning Target-Error The two algorithms\u2014deep learning and Q-learning\u2014look similar in structure. Both consist of a double loop in which a target is optimized, and we can wonder if bootstrapping can be combined with loss-function minimization. This is indeed the case, as Mnih et al. [522] showed in 2013. Our third listing, Listing 3.3, shows a naive deep learning version of Q-learning [522, 524], based on the double loop that now bootstraps Q-values by minimizing a loss function through adjusting the \ud835\udf03 parameters. Indeed, a Q-network can be trained with a gradient by minimizing a sequence of loss functions. The loss function for this bootstrap process is quite literally based on the Q-learning update formula. The loss function is the squared diference between the new Q-value \ud835\udc44 \ud835\udf03\ud835\udc61 (\ud835\udc60, \ud835\udc4e) from the forward pass and the old update target \ud835\udc5f + \ud835\udefe max\ud835\udc4e\u2032 \ud835\udc44 \ud835\udf03\ud835\udc61\u22121 (\ud835\udc60\u2032, \ud835\udc4e\u2032).4 An important observation is that the update targets depend on the previous network weights \ud835\udf03\ud835\udc61\u22121 (the optimization targets move during optimization); this is in contrast with the targets used in a supervised learning process, that are fxed before learning begins [522]. In other words, the loss function of deep Q-learning minimizes a moving target, a target that depends on the network being optimized. 3.2.2 Three Challenges Let us have a closer look at the challenges that deep reinforcement learning faces. There are three problems with our naive deep Q-learner. First, convergence to the optimal Q-function depends on full coverage of the state space, yet the state space is too large to sample fully. Second, there is a strong correlation between subsequent training samples, with a real risk of local optima. Third, the loss function of gradient descent literally has a moving target, and bootstrapping may diverge. Let us have a look at these three problems in more detail. 3.2.2.1 Coverage Proofs that algorithms such as Q-learning converge to the optimal policy depend on the assumption of full state space coverage; all state-action pairs must be sampled. Otherwise, the algorithms will not converge to an optimal action value for each state. Clearly, in large state spaces where not all states are sampled, this situation does not hold, and there is no guarantee of convergence. 4 Deep Q-learning is a fxed-point iteration [507]. The gradient of this loss function is \u2207\ud835\udf03\ud835\udc56 L\ud835\udc56 (\ud835\udf03\ud835\udc56) = E\ud835\udc60,\ud835\udc4e\u223c\ud835\udf0c(\u00b7);\ud835\udc60\u2032\u223cE \ufffd\ufffd\ud835\udc5f + \ud835\udefe max\ud835\udc4e\u2032 \ud835\udc44\ud835\udf03\ud835\udc56\u22121 (\ud835\udc60\u2032, \ud835\udc4e\u2032) \u2212 \ud835\udc44\ud835\udf03\ud835\udc56 (\ud835\udc60, \ud835\udc4e)\ufffd \u2207\ud835\udf03\ud835\udc56\ud835\udc44\ud835\udf03\ud835\udc56 (\ud835\udc60, \ud835\udc4e) \ufffd where \ud835\udf0c is the behavior distribution and E the Atari emulator. Further details are in [522]. 3.2 Deep Value-Based Agents 73 3.2.2.2 Correlation In reinforcement learning a sequence of states is generated in an agent/environment loop. The states difer only by a single action, one move or one stone, all other features of the states remain unchanged, and thus, the values of subsequent samples are correlated, which may result in a biased training. The training may cover only a part of the state space, especially when greedy action selection increases the tendency to select a small set of actions and states. The bias can result in the so-called specialization trap (when there is too much exploitation, and too little exploration). Correlation between subsequent states contributes to the low coverage that we discussed before, reducing convergence towards the optimal Q-function, increasing the probability of local optima and feedback loops. This happens, for example, when a chess program has been trained on a particular opening, and the opponent plays a diferent one. When test examples are diferent from training examples, then generalization will be bad. This problem is related to out-of-distribution training, see for example [485]. 3.2.2.3 Convergence When we naively apply our deep supervised methods to reinforcement learning, we encounter the problem that in a bootstrap process, the optimization target is part of the bootstrap process itself. Deep supervised learning uses a static dataset to approximate a function, and loss-function targets are therefore stable. However, deep reinforcement learning uses as bootstrap target the Q-update from the previous time step, which changes during the optimization. The loss is the squared diference between the Q-value \ud835\udc44 \ud835\udf03\ud835\udc61 (\ud835\udc60, \ud835\udc4e) and the old update target \ud835\udc5f + \ud835\udefe max\ud835\udc4e\u2032 \ud835\udc44 \ud835\udf03\ud835\udc61\u22121 (\ud835\udc60\u2032, \ud835\udc4e\u2032). Since both depend on parameters \ud835\udf03 that are optimized, the risk of overshooting the target is real, and the optimization process can easily become unstable. It has taken quite some efort to fnd algorithms that can tolerate these moving targets. Deadly Triad Multiple works [48, 283, 787] showed that a combination of of-policy reinforcement learning with nonlinear function approximation (such as deep neural networks) could cause Q-values to diverge. Sutton and Barto [743] further analyze three elements for divergent training: function approximation, bootstrapping, and ofpolicy learning. Together, they are called deadly triad. Function approximation may attribute values to states inaccurately. In contrast to exact tabular methods, that are designed to identify individual states exactly, neural networks are designed to individual features of states. These features can be shared by diferent states, and values attributed to those features are shared also by 74 3 Deep Value-Based Reinforcement Learning other states. Function approximation may thus cause mis-identifcation of states, and reward values and Q-values that are not assigned correctly. If the accuracy of the approximation of the true function values is good enough, then states may be identifed well enough to reduce or prevent divergent training processes and loops [523]. Bootstrapping of values builds up new values on the basis of older values. This occurs in Q-learning and temporal-diference learning where the current value depends on the previous value. Bootstrapping increases the efciency of the training because values do not have to be calculated from the start. However, errors or biases in initial values may persist, and spill over to other states as values are propagated incorrectly due to function approximation. Bootstrapping and function approximation can thus increase divergence. Of-policy learning uses a behavior policy that is diferent from the target policy that we are optimizing for (Sect. 2.2.4.4). When the behavior policy is improved, the of-policy values may not improve. Of-policy learning converges generally less well than on-policy learning as it converges independently from the behavior policy. With function approximation convergence may be even slower, due to values being assigned to incorrect states. 3.2.3 Stable Deep Value-Based Learning These considerations discouraged further research in deep reinforcement learning for many years. Instead, research focused for some time on linear function approximators, which have better convergence guarantees. Nevertheless, work on convergent deep reinforcement learning continued [654, 324, 88, 495], and algorithms such as neural ftted Q-learning were developed, which showed some promise [631, 453, 482]. After the further results of DQN [522] showed convincingly that convergence and stable learning could be achieved in a non-trivial problem, even more experimental studies were performed to fnd out under which circumstances convergence can be achieved and the deadly triad can be overcome. Further convergence and diversity-enhancing techniques were developed, some of which we will cover in Sect. 3.2.4. Although the theory provides reasons why function approximation may preclude stable reinforcement learning, there were, in fact, indications that stable training is possible. Starting at the end of the 1980s, Tesauro had written a program that played very strong Backgammon based on a neural network. The program was called Neurogammon, and used supervised learning from grand-master games [762]. In order to improve the strength of the program, he switched to temporal diference reinforcement learning from self-play games [764]. TD-Gammon [763] learned by playing against itself, achieving stable learning in a shallow network. TD-Gammon\u2019s training used a temporal diference algorithm similar to Q-learning, approximating the value function with a network with one hidden layer, using raw board input 3.2 Deep Value-Based Agents 75 enhanced with hand-crafted heuristic features [763]. Perhaps some form of stable reinforcement learning was possible, at least in a shallow network? TD-Gammon\u2019s success prompted attempts with TD learning in checkers [139] and Go [738, 154]. Unfortunately the success could not be replicated in these games, and it was believed for some time that Backgammon was a special case, well suited for reinforcement learning and self-play [604, 678]. However, as there came further reports of successful applications of deep neural networks in a reinforcement learning setting [324, 654], more work followed. The results in Atari [523] and later in Go [706] as well as further work [799] have now provided clear evidence that both stable training and generalizing deep reinforcement learning are indeed possible, and have improved our understanding of the circumstances that infuence stability and convergence. Let us have a closer look at the methods that are used to achieve stable deep reinforcement learning. 3.2.3.1 Decorrelating States As mentioned in the introduction of this chapter, in 2013 Mnih et al. [522, 523] published their work on end-to-end reinforcement learning in Atari games. The original focus of DQN is on breaking correlations between subsequent states, and also on slowing down changes to parameters in the training process to improve stability. The DQN algorithm has two methods to achieve this: (1) experience replay and (2) infrequent weight updates. We will frst look at experience replay. Experience Replay In reinforcement learning training samples are created in a sequence of interactions with the environment, and subsequent training states are strongly correlated to preceding states. There is a tendency to train the network on too many samples of a certain kind or in a certain area, and other parts of the state space remain under-explored. Furthermore, through function approximation and bootstrapping, some behavior may be forgotten. When an agent reaches a new level in a game that is diferent from previous levels, the agent may forget how to play the other level. We can reduce correlation\u2014and the local minima they cause\u2014by adding a small amount of supervised learning. To break correlations and to create a more diverse set of training examples, DQN uses experience replay. Experience replay introduces a replay bufer [481], a cache of previously explored states, from which it samples training states at random.5 Experience replay stores the last \ud835\udc41 examples in the replay memory, and samples uniformly when performing updates. A typical number for \ud835\udc41 is 106 [875]. By using a bufer, a dynamic dataset from which recent training examples are sampled, we train states from a more diverse set, instead of only from 5 Originally experience replay is, as so much in artifcial intelligence, a biologically inspired mechanism [506, 572, 482]. 76 3 Deep Value-Based Reinforcement Learning the most recent one. The goal of experience replay is to increase the independence of subsequent training examples. The next state to be trained on is no longer a direct successor of the current state, but one somewhere in a long history of previous states. In this way the replay bufer spreads out the learning over more previously seen states, breaking temporal correlations between samples. DQN\u2019s replay bufer (1) improves coverage, and (2) reduces correlation. DQN treats all examples equal, old and recent alike. A form of importance sampling might diferentiate between important transitions, as we will see in the next section. Note that, curiously, training by experience replay is a form of of-policy learning, since the target parameters are diferent from those used to generate the sample. Of-policy learning is one of the three elements of the deadly triad, and we fnd that stable learning can actually be improved by a special form of one of its problems. Experience replay works well in Atari [523]. However, further analysis of replay bufers has pointed to possible problems. Zhang et al. [875] study the deadly triad with experience replay, and fnd that larger networks resulted in more instabilities, but also that longer multi-step returns yielded fewer unrealistically high reward values. In Sect. 3.2.4 we will see many further enhancements to DQN-like algorithms. 3.2.3.2 Infrequent Updates of Target Weights The second improvement in DQN is infrequent weight updates, introduced in the 2015 paper on DQN [523]. The aim of this improvement is to reduce divergence that is caused by frequent updates of weights of the target \ud835\udc44-value. Again, the aim is to improve the stability of the network optimization by improving the stability of the \ud835\udc44-target in the loss function. Every \ud835\udc5b updates, the network \ud835\udc44 is cloned to obtain target network \u02c6\ud835\udc44, which is used for generating the targets for the following \ud835\udc5b updates to \ud835\udc44. In the original DQN implementation a single set of network weights \ud835\udf03 are used, and the network is trained on a moving loss-target. Now, with infrequent updates the weights of the target network change much slower than those of the behavior policy, improving the stability of the Q-targets. The second network improves the stability of Q-learning, where normally an update to \ud835\udc44 \ud835\udf03 (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) also changes the target at each time step, quite possibly leading to oscillations and divergence of the policy. Generating the targets using an older set of parameters adds a delay between the time an update to \ud835\udc44 \ud835\udf03 is made and the time the update changes the targets, making oscillations less likely. 3.2.3.3 Hands On: DQN and Breakout Gym Example To get some hands-on experience with DQN, we will now have a look at how DQN can be used to play the Atari game Breakout. 3.2 Deep Value-Based Agents 77 The feld of deep reinforcement learning is an open feld where most codes of algorithms are freely shared on GitHub and where test environments are available. The most widely used environment is Gym, in which benchmarks such as ALE and MuJoCo can be found, see also Appendix C. The open availability of the software allows for easy replication, and, importantly, for further improvement of the methods. Let us have a closer look at the code of DQN, to experience how it works. The DQN papers come with source code. The original DQN code from [523] is available at Atari DQN.6 This code is the original code, in the programming language Lua, which may be interesting to study, if you are familiar with this language. A modern reference implementation of DQN, with further improvements, is in the (stable) baselines.7 The RL Baselines Zoo even provides a collection of pretrained agents, at Zoo [603, 270].8 The Network Zoo is especially useful if your desired application happens to be in the Zoo, to prevent long training times. Install Stable Baselines The environment is only half of the reinforcement learning experiment, we also need an agent algorithm to learn the policy. OpenAI also provides implementations of agent algorithms, called the Baselines, at the Gym GitHub repository Baselines.9 Most algorithms that are covered in this book are present. You can download them, study the code, and experiment to gain an insight into their behavior. In addition to OpenAI\u2019s Baselines, there is Stable Baselines, a fork of the OpenAI algorithms; it has more documentation and other features. It can be found at Stable Baselines,10 and the documentation is at docs.11 The stable release from the Stable Baselines is installed by typing pip install stable-baselines or pip install stable-baselines[mpi] if support for OpenMPI is desired (a parallel message passing implementation for cluster computers). A very quick check to see if everything works is to run the PPO trainer from Listing 3.4. PPO is a policy-based algorithm that will be discussed in the next chapter in Sect. 4.2.5. The Cartpole should appear again, but should now learn to stabilize for a brief moment. 6 https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner 7 https://stable-baselines.readthedocs.io/en/master/index.html 8 https://github.com/araffin/rl-baselines-zoo 9 https://github.com/openai/baselines 10 https://github.com/hill-a/stable-baselines 11 https://stable-baselines.readthedocs.io/en/master/ 78 3 Deep Value-Based Reinforcement Learning 1 import gym 2 3 from stable_baselines .common.policies import MlpPolicy 4 from stable_baselines .common.vec_env import DummyVecEnv 5 from stable_baselines import PPO2 6 7 env = gym.make(\u2019CartPole -v1\u2019) 8 9 model = PPO2(MlpPolicy , env , verbose =1) 10 model.learn( total_timesteps =10000) 11 12 obs = env.reset () 13 for i in range (1000): 14 action , _states = model.predict(obs) 15 obs , rewards , dones , info = env.step(action) 16 env.render () Listing 3.4 Running Stable Baseline PPO on the Gym Cartpole Environment 1 from stable_baselines .common. atari_wrappers import make_atari 2 from stable_baselines .deepq.policies import MlpPolicy , CnnPolicy 3 from stable_baselines import DQN 4 5 env = make_atari(\u2019BreakoutNoFrameskip -v4\u2019) 6 7 model = DQN(CnnPolicy , env , verbose =1) 8 model.learn( total_timesteps =25000) 9 10 obs = env.reset () 11 while True: 12 action , _states = model.predict(obs) 13 obs , rewards , dones , info = env.step(action) 14 env.render () Listing 3.5 Deep Q-Network Atari Breakout example with Stable Baselines The DQN Code After having studied tabular Q-learning on Taxi in Sect. 2.2.4.5, let us now see how the network-based DQN works in practice. Listing 3.5 illustrates how easy it is to use the Stable Baselines implementation of DQN on the Atari Breakout environment. (See Sect. 2.2.4.1 for installation instructions of Gym.) After you have run the DQN code and seen that it works, it is worthwhile to study how the code is implemented. Before you dive into the Python implementation of Stable Baselines, let us look at the pseudocode to refresh how the elements of DQN work together. See Listing 3.6. In this pseudocode we follow the 2015 version of DQN [523]. (The 2013 version of DQN did not use the target network [522].) 3.2 Deep Value-Based Agents 79 1 def dqn: 2 initialize replay_buffer empty 3 initialize Q network with random weights 4 initialize Qt target network with random weights 5 set s = s0 6 while not convergence : 7 # DQN in Atari uses preprocessing ; not shown 8 epsilon -greedy select action a in argmax(Q(s,a)) # action selection depends on Q (moving target) 9 sx ,reward = execute action in environment 10 append (s,a,r,sx) to buffer 11 sample minibatch from buffer # break temporal correlation 12 take target batch R (when terminal) or Qt 13 do gradient descent step on Q # loss function uses target Qt network Listing 3.6 Pseudocode for DQN, after [523] DQN is based on Q-learning, with as extra a replay bufer and a target network to improve stability and convergence. First, at the start of the code, the replay bufer is initialized to empty, and the weights of the Q network and the separate Q target network are initialized. The state \ud835\udc60 is set to the start state. Next is the optimization loop, that runs until convergence. At the start of each iteration an action is selected at the state \ud835\udc60, following an \ud835\udf16-greedy approach. The action is executed in the environment, and the new state and the reward are stored in a tuple in the replay bufer. Then, we train the Q-network. A minibatch is sampled randomly from the replay bufer, and one gradient descent step is performed. For this step the loss function is calculated with the separate Q-target network \u02c6\ud835\udc44 \ud835\udf03, that is updated less frequently than the primary Q-network \ud835\udc44 \ud835\udf03. In this way the loss function L\ud835\udc61 (\ud835\udf03\ud835\udc61) = E\ud835\udc60,\ud835\udc4e\u223c\ud835\udf0c(\u00b7) \ufffd\ufffdE\ud835\udc60\u2032\u223cE(\ud835\udc5f + \ud835\udefe max \ud835\udc4e\u2032 \u02c6\ud835\udc44 \ud835\udf03\ud835\udc61\u22121 (\ud835\udc60\u2032, \ud835\udc4e\u2032)|\ud835\udc60, \ud835\udc4e) \u2212 \ud835\udc44 \ud835\udf03\ud835\udc61 (\ud835\udc60, \ud835\udc4e)\ufffd2\ufffd is more stable, causing better convergence; \ud835\udf0c(\ud835\udc60, \ud835\udc4e) is the behavior distribution over \ud835\udc60 and \ud835\udc4e, and E is the Atari emulator [522]. Sampling the minibatch reduces the correlation that is inherent in reinforcement learning between subsequent states. Conclusion In summary, DQN was able to successfully learn end-to-end behavior policies for many diferent games (although similar and from the same benchmark set). Minimal prior knowledge was used to guide the system, and the agent only got to see the pixels and the game score. The same network architecture and procedure was used on each game; however, a network trained for one game could not be used to play another game. 80 3 Deep Value-Based Reinforcement Learning Name Principle Applicability Efectiveness DQN [522] replay bufer Atari stable Q learning Double DQN [800] de-overestimate values DQN convergence Prioritized experience [666] decorrelation replay bufer convergence Distributional [70] probability distr stable gradients generalization Random noise [254] parametric noise stable gradients more exploration Table 3.1 Deep Value-Based Approaches The DQN achievement was an important milestone in the history of deep reinforcement learning. The main problems that were overcome by Mnih et al. [522] were training divergence and learning instability. The nature of most Atari 2600 games is that they require eye-hand refexes. The games have some strategic elements, credit assignment is mostly over a short term, and can be learned with a surprisingly simple neural network. Most Atari games are more about immediate refexes than about longer term reasoning. In this sense, the problem of playing Atari well is not unlike an image categorization problem: both problems are to fnd the right response that matches an input consisting of a set of pixels. Mapping pixels to categories is not that diferent from mapping pixels to joystick actions (see also the observations in [400]). The Atari results have stimulated much subsequent research. Many blogs have been written on reproducing the result, which is not a straightforward task, requiring the fne-tuning of many hyperparameters [58]. 3.2.4 Improving Exploration The DQN results have spawned much activity among reinforcement learning researchers to improve training stability and convergence further, and many refnements have been devised, some of which we will review in this section. Many of the topics that are covered by the enhancements are older ideas that work well in deep reinforcement learning. DQN applies random sampling of its replay bufer, and one of the frst enhancements was prioritized sampling [666]. It was found that DQN, being an of-policy algorithm, typically overestimates action values (due to the max operation, Sect. 2.2.4.4). Double DQN addresses overestimation [800], and dueling DDQN introduces the advantage function to standardize action values [830]. Other approaches look at variance in addition to expected value, the efect of random noise on exploration was tested [254], and distributional DQN showed that networks that use probability distributions work better than networks that only use single point expected values [70]. In 2017 Hessel et al. [335] performed a large experiment that combined seven important enhancements. They found that the enhancements worked well together. The paper has become known as the Rainbow paper, since the major graph showing the cumulative performance over 57 Atari games of the seven enhancements is 3.2 Deep Value-Based Agents 81 Fig. 3.5 Rainbow graph: performance over 57 Atari games [335] multi-colored (Fig. 3.5). Table 3.1 summarizes the enhancements, and this section provides an overview of the main ideas. The enhancements were tested on the same benchmarks (ALE, Gym), and most algorithm implementations can be found on the OpenAI Gym GitHub site in the baselines.12 3.2.4.1 Overestimation Van Hasselt et al. introduce double deep Q learning (DDQN) [800]. DDQN is based on the observation that Q-learning may overestimate action values. On the Atari 2600 games DQN sufers from substantial over-estimations. Remember that DQN uses Q-learning. Because of the max operation in Q-learning this results in an overestimation of the Q-value. To resolve this issue, DDQN uses the Q-Network to choose the action but uses the separate target Q-Network to evaluate the action. Let us compare the training target for DQN \ud835\udc66 = \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44 \ud835\udf03\ud835\udc61 (\ud835\udc60\ud835\udc61+1, arg max \ud835\udc4e \ud835\udc44 \ud835\udf03\ud835\udc61 (\ud835\udc60\ud835\udc61+1, \ud835\udc4e) with the training target for DDQN (the diference is a single \ud835\udf19) \ud835\udc66 = \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44 \ud835\udf19\ud835\udc61 (\ud835\udc60\ud835\udc61+1, arg max \ud835\udc4e \ud835\udc44 \ud835\udf03\ud835\udc61 (\ud835\udc60\ud835\udc61+1, \ud835\udc4e). 12 https://github.com/openai/baselines 82 3 Deep Value-Based Reinforcement Learning The DQN target uses the same set of weights \ud835\udf03\ud835\udc61 twice, for selection and evaluation; the DDQN target use a separate set of weights \ud835\udf19\ud835\udc61 for evaluation, preventing overestimation due to the max operator. Updates are assigned randomly to either set of weights. Earlier Van Hasselt et al. [314] introduced the double Q learning algorithm in a tabular setting. The later paper shows that this idea also works with a large deep network. They report that the DDQN algorithm not only reduces the overestimations but also leads to better performance on several games. DDQN was tested on 49 Atari games and achieved about twice the average score of DQN with the same hyperparameters, and four times the average DQN score with tuned hyperparameters [800]. Prioritized Experience Replay DQN samples uniformly over the entire history in the replay bufer, where Qlearning uses only the most recent (and important) state. It stands to reason to see if a solution in between these two extremes performs well. Prioritized experience replay, or PEX, is such an attempt. It was introduced by Schaul et al. [666]. In the Rainbow paper PEX is combined with DDQN, and, as we can see, the blue line (with PEX) indeed outperforms the purple line. In DQN experience replay lets agents reuse examples from the past, although experience transitions are uniformly sampled, and actions are simply replayed at the same frequency that they were originally experienced, regardless of their signifcance. The PEX approach provides a framework for prioritizing experience. Important actions are replayed more frequently, and therefore learning efciency is improved. As measure of importance, standard proportional prioritized replay is used, with the absolute TD error to prioritize actions. Prioritized replay is used widely in value-based deep reinforcement learning. The measure can be computed in the distributional setting using the mean action values. In the Rainbow paper all distributional variants prioritize actions by the Kullback-Leibler loss [335]. Advantage Function The original DQN uses a single neural network as function approximator; DDQN (double deep Q-network) uses a separate target Q-Network to evaluate an action. Dueling DDQN [830], also known as DDDQN, improves on this architecture by using two separate estimators: a value function and an advantage function \ud835\udc34(\ud835\udc60, \ud835\udc4e) = \ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2212 \ud835\udc49(\ud835\udc60). Advantage functions are related to the actor-critic approach (see Chap. 4). An advantage function computes the diference between the value of an action and the value of the state. The function standardizes values on a baseline for the actions 3.3 Atari 2600 Environments 83 of a state [293]. Advantage functions provide better policy evaluation when many actions have similar values. 3.2.4.2 Distributional Methods The original DQN learns a single value, which is the estimated mean of the state value. This approach does not take uncertainty into account. To remedy this, distributional Q-learning [70] learns a categorical probability distribution of discounted returns instead, increasing exploration. Bellemare et al. design a new distributional algorithm which applies Bellman\u2019s equation to the learning of distributions, a method called distributional DQN. Moerland et al. [526, 527] propose uncertain value networks. Interestingly, a link between the distributional approach and biology has been reported. Dabney et al. [174] showed correspondence between distributional reinforcement learning algorithms and the dopamine levels in mice, suggesting that the brain represents possible future rewards as a probability distribution. Noisy DQN Another distributional method is noisy DQN [254]. Noisy DQN uses stochastic network layers that add parametric noise to the weights. The noise induces randomness in the agent\u2019s policy, which increases exploration. The parameters that govern the noise are learned by gradient descent together with the remaining network weights. In their experiments the standard exploration heuristics for A3C (Sect. 4.2.4), DQN, and dueling agents (entropy reward and \ud835\udf16-greedy) were replaced with NoisyNet. The increased exploration yields substantially higher scores for Atari (dark red line). 3.3 Atari 2600 Environments In their original 2013 workshop paper Mnih et al. [522] achieved human-level play for some of the games. Training was performed on 50 million frames in total on seven Atari games. The neural network performed better than an expert human player on Breakout, Enduro, and Pong. On Seaqest, Q*Bert, and Space Invaders performance was far below that of a human. In these games a strategy must be found that extends over longer time periods. In their follow-up journal article two years later they were able to achieve human level play for 49 of the 57 games that are in ALE [523], and performed better than human-level play in 29 of the 49 games. Some of the games still proved difcult, notably games that require longerrange planning, where long stretches of the game do not give rewards, such as in Montezuma\u2019s Revenge, where the agent has to walk long distances, and pick up 84 3 Deep Value-Based Reinforcement Learning Fig. 3.6 DQN architecture [361] a key to reach new rooms to enter new levels. In reinforcement learning terms, delayed credit assignment over long periods is hard. Towards the end of the book we will see Montezuma\u2019s Revenge again, when we discuss hierarchical reinforcement learning methods, in Chap 8. These methods are specifcally developed to take large steps in the state space. The Go-Explore algorithm was able to solve Montezuma\u2019s Revenge [221, 222]. 3.3.1 Network Architecture End-to-end learning of challenging problems is computationally intensive. In addition to the two algorithmic innovations, the success of DQN is also due to the creation of a specialized efcient training architecture [523]. Playing the Atari games is a computationally intensive task for a deep neural network: the network trains a behavior policy directly from pixel frame input. Therefore, the training architecture contains reduction steps. To start with, the network consists of only three hidden layers (one fully connected, two convolutional), which is simpler than what is used in most supervised learning tasks. The pixel-images are high-resolution data. Since working with the full resolution of 210 \u00d7 160 pixels of 128 color-values at 60 frames per second would be computationally too intensive, the images are reduced in resolution. The 210 \u00d7 160 with a 128 color palette is reduced to gray scale and 110 \u00d7 84 pixels, which is further cropped to 84\u00d784. The frst hidden layer convolves 16 8\u00d78 flters with stride 4 and ReLU neurons. The second hidden layer convolves 32 4 \u00d7 4 flters with stride 2 and ReLU neurons. The third hidden layer is fully connected and consists of 256 ReLU neurons. The output layer is also fully connected with one output per action (18 joystick actions). The outputs correspond to the Q-values of the individual action. Figure 3.6 shows the architecture of DQN. The network receives the change in game score as a number from the emulator, and derivative updates are mapped to {\u22121, 0, +1} to indicate decrease, no change, or improvement of the score (the Huber loss [58]). 3.3 Atari 2600 Environments 85 To reduce computational demands further, frame skipping is employed. Only one in every 3\u20134 frames was used, depending on the game. To take game history into account, the net takes as input the last four resulting frames. This allows movement to be seen by the net. As optimizer RMSprop is used [642]. A variant of \ud835\udf16-greedy is used, that starts with an \ud835\udf16 of 1.0 (fully exploring) going down to 0.1 (90% exploiting). 3.3.2 Benchmarking Atari To end the Atari story, we discuss two fnal algorithms. Of the many value-based model-free deep reinforcement learning algorithms that have been developed, one more algorithm that we discuss is R2D2 [397], because of its performance. R2D2 is not part of the Rainbow experiments, but is a signifcant further improvement of the algorithms. R2D2 stands for Recurrent Replay Distributed DQN. It is built upon prioritized distributed replay and 5-step double Q-learning. Furthermore, it uses a dueling network architecture and an LSTM layer after the convolutional stack. Details about the architecture can be found in [830, 295]. The LSTM uses the recurrent state to exploit long-term temporal dependencies, which improve performance. The authors also report that the LSTM allows for better representation learning. R2D2 achieved good results on all 57 Atari games [397]. A more recent benchmark achievement has been published as Agent57. Agent57 is the frst program that achieves a score higher than the human baseline on all 57 Atari 2600 games from ALE. It uses a controller that adapts the long and short-term behavior of the agent, training for a range of policies, from very exploitative to very explorative, depending on the game [46]. Conclusion Progress has come a long way since the replay bufer of DQN. Performance has been improved greatly in value-based model-free deep reinforcement learning and now super-human performance in all 57 Atari games of ALE has been achieved. Many enhancements that improve coverage, correlation, and convergence have been developed. The presence of a clear benchmark was instrumental for progress so that researchers could clearly see which ideas worked and why. The earlier mazes and navigation games, OpenAI\u2019s Gym [108], and especially the ALE [71], have enabled this progress. In the next chapter we will look at the other main branch of model-free reinforcement learning: policy-based algorithms. We will see how they work, and that they are well suited for a diferent kind of application, with continuous action spaces. 86 3 Deep Value-Based Reinforcement Learning Summary and Further Reading This has been the frst chapter in which we have seen deep reinforcement learning algorithms learn complex, high-dimensional, tasks. We end with a summary and pointers to the literature. Summary The methods that have been discussed in the previous chapter were exact, tabular methods. Most interesting problems have large state spaces that do not ft into memory. Feature learning identifes states by their common features. Function values are not calculated exactly, but are approximated, with deep learning. Much of the recent success of reinforcement learning is due to deep learning methods. For reinforcement learning a problem arises when states are approximated. Since in reinforcement learning the next state is determined by the previous state, algorithms may get stuck in local minima or run in circles when values are shared with diferent states. Another problem is training convergence. Supervised learning has a static dataset and training targets are also static. In reinforcement learning the loss function targets depend on the parameters that are being optimized. This causes further instability. DQN caused a breakthrough by showing that with a replay bufer and a separate, more stable, target network, enough stability could be found for DQN to converge and learn how to play Atari arcade games. Many further improvements to increase stability have been found. The Rainbow paper implements some of these improvements, and fnds that they are complementary, and together achieve very strong play. Further Reading Deep learning revolutionized reinforcement learning. A comprehensive overview of the feld is provided by Dong et al. [201]. For more on deep learning, see Goodfellow et al. [280], a book with much detail on deep learning; a major journal article is [459]. A brief survey is [27]. Also see Appendix B. In 2013 the Arcade Learning Environment was presented [71, 494]. Experimenting with reinforcement learning was made even more accessible with OpenAI\u2019s Gym [108], with clear and easy to use Python bindings. Deep learning versions of value-based tabular algorithms sufer from convergence and stability problems [787], yet the idea that stable deep reinforcement learning might be practical took hold with [324, 654]. Zhang et al. [875] study the deadly triad with experience replay. Deep gradient TD methods were proven to converge for evaluating a fxed policy [88]. Riedmiller et al. relaxed the fxed 3.3 Atari 2600 Environments 87 control policy in neural ftted Q learning algorithm (NFQ) [631]. NFQ builds on work on stable function approximation [282, 229] and experience replay [481], and more recently on least-squares policy iteration [441]. In 2013 the frst DQN paper appeared, showing results on a small number of Atari games [522] with the replay bufer to reduce temporal correlations. In 2015 the followup Nature paper reported results in more games [523], with a separate target network to improve training convergence. A well-known overview paper is the Rainbow paper [335, 387]. The use of benchmarks is of great importance for reproducible reinforcement learning experiments [328, 371, 410, 365]. For TensorFlow and Keras, see [146, 270]. Exercises We will end this chapter with some questions to review the concepts that we have covered. Next are programming exercises to get some more exposure on how to use the deep reinforcement learning algorithms in practice. Questions Below are some questions to check your understanding of this chapter. Each question is a closed question where a simple, single sentence answer is expected. 1. What is Gym? 2. What are the Stable Baselines? 3. The loss function of DQN uses the Q-function as target. What is a consequence? 4. Why is the exploration/exploitation trade-of central in reinforcement learning? 5. Name one simple exploration/exploitation method. 6. What is bootstrapping? 7. Describe the architecture of the neural network in DQN. 8. Why is deep reinforcement learning more susceptible to unstable learning than deep supervised learning? 9. What is the deadly triad? 10. How does function approximation reduce stability of Q-learning? 11. What is the role of the replay bufer? 12. How can correlation between states lead to local minima? 13. Why should the coverage of the state space be sufcient? 14. What happens when deep reinforcement learning algorithms do not converge? 15. How large is the state space of chess estimated to be? 1047, 10170 or 101685? 16. How large is the state space of Go estimated to be? 1047, 10170 or 101685? 17. How large is the state space of StarCraft estimated to be? 1047, 10170 or 101685? 18. What does the rainbow in the Rainbow paper stand for, and what is the main message? 19. Mention three Rainbow improvements that are added to DQN. 88 3 Deep Value-Based Reinforcement Learning Exercises Let us now start with some exercises. If you have not done so already, install Gym, PyTorch13 or TensorFlow and Keras (see Sect. 2.2.4.1 and B.3.3 or go to the TensorFlow page).14 Be sure to check the right versions of Python, Gym, TensorFlow, and the Stable Baselines to make sure that they work well together. The exercises below are designed to be done with Keras. 1. DQN Implement DQN from the Stable Baselines on Breakout from Gym. Turn of Dueling and Priorities. Find out what the values are for \ud835\udefc, the training rate, for \ud835\udf16, the exploration rate, what kind of neural network architecture is used, what the replay bufer size is, and how frequently the target network is updated. 2. Hyperparameters Change all those hyperparameters, up, and down, and note the efect on training speed, and the training outcome: how good is the result? How sensitive is performance to hyperparameter optimization? 3. Cloud Use diferent computers, experiment with GPU versions to speed up training, consider Colab, AWS, or another cloud provider with fast GPU (or TPU) machines. 4. Gym Go to Gym and try diferent problems. For what kind of problems does DQN work, what are characteristics of problems for which it works less well? 5. Stable Baselines Go to the Stable baselines and implement diferent agent algorithms. Try Dueling algorithms, Prioritized experience replay, but also other algorithm, such as Actor critic or policy-based. (These algorithms will be explained in the next chapter.) Note their performance. 6. Tensorboard With Tensorboard you can follow the training process as it progresses. Tensorboard works on log fles. Try TensorBoard on a Keras exercise and follow diferent training indicators. Also try TensorBoard on the Stable Baselines and see which indicators you can follow. 7. Checkpointing Long training runs in Keras need checkpointing, to save valuable computations in case of a hardware or software failure. Create a large training job, and setup checkpointing. Test everything by interrupting the training, and try to re-load the pre-trained checkpoint to restart the training where it left of. 13 https://pytorch.org 14 https://www.tensorflow.org Chapter 4 Policy-Based Reinforcement Learning Some of the most successful applications of deep reinforcement learning have a continuous action space, such as applications in robotics, self-driving cars, and real-time strategy games. The previous chapters introduced value-based reinforcement learning. Valuebased methods fnd the policy in a two-step process. First they fnd the best actionvalue of a state, for which then the accompanying actions are found (by means of arg max). This works in environments with discrete actions, where the highestvalued action is clearly separate from the next-best action. Examples of continuous action spaces are robot arms that can move over arbitrary angles, or poker bets that can be any monetary value. In these action spaces value-based methods become unstable and arg max is not appropriate. Another approach works better: policy-based methods. Policy-based methods do not use a separate value function but fnd the policy directly. They start with a policy function, which they then improve, episode by episode, with policy gradient methods. Policy-based methods are applicable to more domains than value-based methods. They work well with deep neural networks and gradient learning; they are some of the most popular methods of deep reinforcement learning, and this chapter introduces you to them. We start by looking at applications with continuous action spaces. Next, we look at policy-based agent algorithms. We will introduce basic policy search algorithms, and the policy gradient theorem. We will also discuss algorithms that combine value-based and policy-based approaches: the so-called Actor critic algorithms. At the end of the chapter we discuss larger environments for policy-based methods in more depth, where we will discuss progress in visuo-motor robotics and locomotion environments. The chapter concludes with exercises, a summary, and pointers to further reading. 89 90 4 Policy-Based Reinforcement Learning Core Concepts \u2022 Policy gradient \u2022 Bias-variance trade-of; Actor critic Core Problem \u2022 Find a low variance continuous action policy directly Core Algorithms \u2022 REINFORCE (Alg. 4.2) \u2022 Asynchronous advantage actor critic (Alg. 4.4) \u2022 Proximal policy optimization (Sect. 4.2.5) Jumping Robots One of the most intricate problems in robotics is learning to walk, or more generally, how to perform locomotion. Much work has been put into making robots walk, run and jump. A video of a simulated robot that taught itself to jump over an obstacle course can be found on YouTube1 [325]. Learning to walk is a challenge that takes human infants months to master. (Cats and dogs are quicker.) Teaching robots to walk is a challenging problem that is studied extensively in artifcial intelligence and engineering. Movies abound on the internet of robots that try to open doors, and fall over, or just try to stand upright, and still fall over.2 Locomotion of legged robots is a difcult sequential decision problem. For each leg, many diferent joints are involved. They must be actuated in the right order, turned with the right force, over the right duration, to the right angle. Most of these angles, forces, and durations are continuous. The algorithm has to decide how many degrees, Newtons, and seconds, constitute the optimal policy. All these actions are continuous quantities. Robot locomotion is a difcult problem, that is studied frequently in policy-based deep reinforcement learning. 1 https://www.youtube.com/watch?v=hx_bgoTF7bs 2 See, for example, https://www.youtube.com/watch?v=g0TaYhjpOfo. 4.1 Continuous Problems 91 4.1 Continuous Problems In this chapter, our actions are continuous, and stochastic. We will discuss both of these aspects, and some of the challenges they pose. We will start with continuous action policies. 4.1.1 Continuous Policies In the previous chapter we discussed environments with large state spaces. We will now move our attention to action spaces. The action spaces of the problems that we have seen so far\u2014Grid worlds, mazes, and high-dimensional Atari games\u2014were actually action spaces that were small and discrete\u2014we could walk north, east, west, south, or we could choose from 9 joystick movements. In board games such as chess the action space is larger, but still discrete. When you move your pawn to e4, you do not move it to e4\u00bd. In this chapter the problems are diferent. Steering a self driving car requires turning the steering wheel a certain angle, duration, and angular velocity, to prevent jerky movements. Throttle movements should also be smooth and continuous. Actuation of robot joints is continuous, as we mentioned in the introduction of this chapter. An arm joint can move 1 degree, 2 degrees, or 90 or 180 degrees or anything in between. An action in a continuous space is not one of a set of discrete choices, such as {\ud835\udc41, \ud835\udc38, \ud835\udc4a, \ud835\udc46}, but rather a value over a continuous range, such as [0, 2\ud835\udf0b] or R+; the number of possible values is infnite. How can we fnd the optimum value in an infnite space in a fnite amount of time? Trying out all possible combinations of setting joint 1 to \ud835\udc65 degrees and applying force \ud835\udc66 in motor 2 will take infnitely long. A solution could be to discretize the actions, although that introduces potential quantization errors. When actions are not discrete, the arg max operation can not be used to identify \u201cthe\u201d best action, and value-based methods are no longer sufcient. Policy-based methods fnd suitable continuous or stochastic policies directly, without the intermediate step of a value function and the need for the arg max operation to construct the fnal policy. 4.1.2 Stochastic Policies We will now turn to the modeling of stochastic policies. 92 4 Policy-Based Reinforcement Learning When a robot moves its hand to open a door, it must judge the distance correctly. A small error, and it may fail (as many movie clips show).3 Stochastic environments cause stability problems for value-based methods [480]. Small perturbations in Q-values may lead to large changes in the policy of value-based methods. Convergence can typically only be achieved at slow learning rates, to smooth out the randomness. A stochastic policy (a target distribution) does not sufer from this problem. Stochastic policies have another advantage. By their nature they perform exploration, without the need to separately code \ud835\udf16-greediness or other exploration methods, since a stochastic policy returns a distribution over actions. Policy-based methods fnd suitable stochastic policies directly. A potential disadvantage of purely episodic policy-based methods is that they are high-variance; they may fnd local optima instead of global optima, and converge slower than value-based methods. Newer (actor critic) methods, such as A3C, TRPO, and PPO, were designed to overcome these problems. We will discuss these algorithms later in this chapter. Before we will explain these policy-based agent algorithms, we will have a closer look at some of the applications for which they are needed. 4.1.3 Environments: Gym and MuJoCo Robotic experiments play an important role in reinforcement learning. However, because of the cost associated with real-world robotics experiments, in reinforcement learning often simulated robotics systems are used. This is especially important in model-free methods, that tend to have a high sample complexity (real robots wear down when trials run in the millions). These software simulators model behavior of the robot and the efects on the environment, using physics models. This prevents the expense of real experiments with real robots, although some precision is lost to modeling error. Two well-known physics models are MuJoCo [780] and PyBullet [167]. They can be used easily via the Gym environment. 4.1.3.1 Robotics Most robotic applications are more complicated than the classics such as mazes, Mountain car and Cart pole. Robotic control decisions involve more joints, directions of travel, and degrees of freedom, than a single cart that moves in one dimension. Typical problems involve learning of visuo-motor skills (eye-hand coordination, grasping), or learning of diferent locomotion gaits of multi-legged \u201canimals.\u201d Some examples of grasping and walking are illustrated in Fig. 4.1. 3 Even worse, when a robot thinks it stands still, it may actually be in the process of falling over (and, of course, robots can not think, they only wished they could). 4.1 Continuous Problems 93 Fig. 4.1 Robot Grasping and Gait [501] The environments for these actions are unpredictable to a certain degree: they require reactions to disturbances such as bumps in the road, or the moving of objects in a scene. 4.1.3.2 Physics Models Simulating robot motion involves modeling forces, acceleration, velocity, and movement. It also includes modeling mass and elasticity for bouncing balls, tactile/grasping mechanics, and the efect of diferent materials. A physics mechanics model needs to simulate the result of actions in the real world. Among the goals of such a simulation is to model grasping, locomotion, gaits, and walking and running (see also Sect. 4.3.1). The simulations should be accurate. Furthermore, since model-free learning algorithms often involve millions of actions, it is important that the physics simulations are fast. Many diferent physics environments for model-based robotics have been created, among them Bullet, Havok, ODE and PhysX, see [228] for a comparison. Of the models, MuJoCo [780], and PyBullet [167] are the most popular in reinforcement learning, especially MuJoCo is used in many experiments. Although MuJoCo calculations are deterministic, the initial state of environments is typically randomized, resulting in an overall non-deterministic environment. Despite many code optimizations in MuJoCo, simulating physics is still an expensive proposition. Most MuJoCo experiments in the literature therefore are based on stick-like entities, that simulate limited motions, in order to limit the computational demands. Figures 4.2 and 4.3 illustrate a few examples of some of the common Gym/MuJoCo problems that are often used in reinforcement learning: Ant, Half-cheetah, and Humanoid. 94 4 Policy-Based Reinforcement Learning Fig. 4.2 Gym MuJoCo Ant and Half-Cheetah [108] Fig. 4.3 Gym MuJoCo Humanoid 4.1.3.3 Games In real time video games and certain card games the decisions are also continuous. For example, in some variants of poker, the size of monetary bets can be any amount, which makes the action space quite large (although strictly speaking still discrete). In games such as StarCraft and Capture the Flag, aspects of the physical world are modeled, and movement of agents can vary in duration and speed. The environment for these games is also stochastic: some information is hidden for the agent. This increases the size of the state space greatly. We will discuss these games in Chap. 7 when we discuss multi-agent methods. 4.2 Policy-Based Agents Now that we have discussed the problems and environments that are used with policy-based methods, it is time to see how policy-based algorithms work. Policybased methods are a popular approach in model-free deep reinforcement learning. Many algorithms have been developed that perform well. Table 4.1 lists some of the better known algorithms that will be covered in this chapter. 4.2 Policy-Based Agents 95 Name Approach Ref REINFORCE Policy-gradient optimization [844] A3C Distributed Actor Critic [521] DDPG Derivative of continuous action function [480] TRPO Dynamically sized step size [681] PPO Improved TRPO, frst order [683] SAC Variance-based Actor Critic for robustness [306] Table 4.1 Policy-Based Algorithms: REINFORCE, Asynchronous Advantage Actor Critic, Deep Deterministic Policy Gradient, Trust Region Policy Optimization, Proximal Policy Optimization, Soft Actor Critic We will frst provide an intuitive explanation of the idea behind the basic policybased approach. Then we will discuss some of the theory behind it, as well as advantages and disadvantages of the basic policy-based approach. Most of these disadvantages are alleviated by the actor critic method, that is discussed next. Let us start with the basic idea behind policy-based methods. 4.2.1 Policy-Based Algorithm: REINFORCE Policy-based approaches learn a parameterized policy, that selects actions without consulting a value function.4 In policy-based methods the policy function is represented directly, allowing policies to select a continuous action, something that is difcult to do in value-based methods. The Supermarket: To build some intuition on the nature of policy-based methods, let us think back again at the supermarket navigation task, that we used in Chap. 2. In this navigation problem we can try to assess our current distance to the supermarket with the Q-value-function, as we have done before. The Q-value assesses the distance of each direction to take; it tells us how far each action is from the goal. We can then use this distance function to fnd our path. In contrast, the policy-based alternative would be to ask a local the way, who tells us, for example, to go straight and then left and then right at the Opera House and straight until we reach the supermarket on our left. The local just gave us a full path to follow, without having to infer which action was the closest and then use that information to determine the way to go. We can subsequently try to improve this full trajectory. 4 Policy-based methods may use a value function to learn the policy parameters \ud835\udf03, but do not use it for action selection. 96 4 Policy-Based Reinforcement Learning Let us see how we can optimize such a direct policy directly, without the intermediate step of the Q-function. We will develop a frst, generic, policy-based algorithm to see how the pieces ft together. The explanation will be intuitive in nature. The basic framework for policy-based algorithms is straightforward. We start with a parameterized policy function \ud835\udf0b\ud835\udf03. We frst (1) initialize the parameters \ud835\udf03 of the policy function, (2) sample a new trajectory \ud835\udf0f, (3) if \ud835\udf0f is a good trajectory, increase the parameters \ud835\udf03 towards \ud835\udf0f, otherwise decrease them, and (4) keep going until convergence. Algorithm 4.1 provides a framework in pseudocode. Please note the similarity with the codes in the previous chapter (Listing 3.1\u20133.3), and especially the deep learning algorithms, where we also optimized function parameters in a loop. The policy is represented by a set of parameters \ud835\udf03 (these can be the weights in a neural network). Together, the parameters \ud835\udf03 map the states \ud835\udc46 to action probabilities \ud835\udc34. When we are given a set of parameters, how should we adjust them to improve the policy? The basic idea is to randomly sample a new policy, and if it is better, adjust the parameters a bit in the direction of this new policy (and away if it is worse). Let us see in more detail how this idea works. To know which policy is best, we need some kind of measure of its quality. We denote the quality of the policy that is defned by the parameters as \ud835\udc3d(\ud835\udf03). It is natural to use the value function of the start state as our measure of quality \ud835\udc3d(\ud835\udf03) = \ud835\udc49 \ud835\udf0b(\ud835\udc600). We wish to maximize \ud835\udc3d(\u00b7). When the parameters are diferentiable, then all we need to do is to fnd a way to improve the gradient \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) = \u2207\ud835\udf03\ud835\udc49 \ud835\udf0b(\ud835\udc600) of this expression to maximize our objective function \ud835\udc3d(\u00b7). Policy-based methods apply gradient-based optimization, using the derivative of the objective to fnd the optimum. Since we are maximizing, we apply gradient ascent. In each time step \ud835\udc61 of the algorithm we perform the following update: \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 + \ud835\udefc \u00b7 \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) for learning rate \ud835\udefc \u2208 R+ and performance objective \ud835\udc3d, see the gradient ascent algorithm in Alg. 4.1. Remember that \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) is the probability of taking action \ud835\udc4e in state \ud835\udc60. This function \ud835\udf0b is represented by a neural network \ud835\udf03, mapping states \ud835\udc46 at the input side of the network to action probabilities on the output side of the network. The parameters \ud835\udf03 determine the mapping of our function \ud835\udf0b. Our goal is to update the parameters so that \ud835\udf0b\ud835\udf03 becomes the optimal policy. The better the action \ud835\udc4e is, the more we want to increase the parameters \ud835\udf03. If we now would know, by some magical way, the optimal action \ud835\udc4e\u2605, then we could use the gradient to push each parameter \ud835\udf03\ud835\udc61, \ud835\udc61 \u2208 trajectory, of the policy, in the direction of the optimal action, as follows 4.2 Policy-Based Agents 97 Algorithm 4.1 Gradient ascent optimization Input: a diferentiable objective \ud835\udc3d (\ud835\udf03), learning rate \ud835\udefc \u2208 R+, threshold \ud835\udf16 \u2208 R+ Initialization: randomly initialize \ud835\udf03 in R\ud835\udc51 repeat Sample trajectory \ud835\udf0f and compute gradient \u2207\ud835\udf03 \ud835\udf03 \u2190 \ud835\udf03 + \ud835\udefc \u00b7 \u2207\ud835\udf03 \ud835\udc3d (\ud835\udf03) until \u2207\ud835\udf03 \ud835\udc3d (\ud835\udf03) converges below \ud835\udf16 return parameters \ud835\udf03 \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 + \ud835\udefc\u2207\ud835\udf0b\ud835\udf03\ud835\udc61 (\ud835\udc4e\u2605|\ud835\udc60). Unfortunately, we do not know which action is best. We can, however, take a sample trajectory and use estimates of the value of the actions of the sample. This estimate can use the regular \u02c6\ud835\udc44 function from the previous chapter, or the discounted return function, or an advantage function (to be introduced shortly). Then, by multiplying the push of the parameters (the probability) with our estimate, we get \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 + \ud835\udefc \u02c6\ud835\udc44(\ud835\udc60, \ud835\udc4e)\u2207\ud835\udf0b\ud835\udf03\ud835\udc61 (\ud835\udc4e|\ud835\udc60). A problem with this formula is that not only are we going to push harder on actions with a high value, but also more often, because the policy \ud835\udf0b\ud835\udf03\ud835\udc61 (\ud835\udc4e|\ud835\udc60) is the probability of action \ud835\udc4e in state \ud835\udc60. Good actions are thus doubly improved, which may cause instability. We can correct by dividing by the general probability: \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 + \ud835\udefc \u02c6\ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2207\ud835\udf0b\ud835\udf03\ud835\udc61 (\ud835\udc4e|\ud835\udc60) \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) . In fact, we have now almost arrived at the classic policy-based algorithm, REINFORCE, introduced by Williams in 1992 [844]. In this algorithm our formula is expressed in a way that is reminiscent of a logarithmic cross-entropy loss function. We can arrive at such a log-formulation by using the basic fact from calculus that \u2207 log \ud835\udc53 (\ud835\udc65) = \u2207 \ud835\udc53 (\ud835\udc65) \ud835\udc53 (\ud835\udc65) . Substituting this formula into our equation, we arrive at \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 + \ud835\udefc \u02c6\ud835\udc44(\ud835\udc60, \ud835\udc4e)\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60). This formula is indeed the core of REINFORCE, the prototypical policy-based algorithm, which is shown in full in Alg. 4.2, with discounted cumulative reward. To summarize, the REINFORCE formula pushes the parameters of the policy in the direction of the better action (multiplied proportionally by the size of the estimated action-value) to know which action is best. We have arrived at a method to improve a policy that can be used directly to indicate the action to take. The method whether the action is discrete, continuous, or 98 4 Policy-Based Reinforcement Learning Algorithm 4.2 Monte Carlo policy gradient (REINFORCE) [844] Input: A diferentiable policy \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60), learning rate \ud835\udefc \u2208 R+, threshold \ud835\udf16 \u2208 R+ Initialization: Initialize parameters \ud835\udf03 in R\ud835\udc51 repeat Generate full trace \ud835\udf0f = {\ud835\udc600, \ud835\udc4e0, \ud835\udc5f0, \ud835\udc601, .., \ud835\udc60\ud835\udc47 } following \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) for \ud835\udc61 \u2208 0, . . . , \ud835\udc47 \u2212 1 do \u22b2 Do for each step of the episode \ud835\udc45 \u2190 \ufffd\ud835\udc47 \u22121 \ud835\udc58=\ud835\udc61 \ud835\udefe\ud835\udc58\u2212\ud835\udc61 \u00b7 \ud835\udc5f\ud835\udc58 \u22b2 Sum Return from trace \ud835\udf03 \u2190 \ud835\udf03 + \ud835\udefc\ud835\udefe\ud835\udc61 \ud835\udc45\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61) \u22b2 Adjust parameters end for until \u2207\ud835\udf03 \ud835\udc3d (\ud835\udf03) converges below \ud835\udf16 return Parameters \ud835\udf03 stochastic, without having to go through intermediate value or arg max functions to fnd it. Algorithm 4.2 shows the full algorithm, which is called Monte Carlo policy gradient. The algorithm is called Monte Carlo because it samples a trajectory. Online and Batch The versions of gradient ascent (Alg. 4.1) and REINFORCE (Alg. 4.2) that we show, update the parameters inside the innermost loop. All updates are performed as the time steps of the trajectory are traversed. This method is called the online approach. When multiple processes work in parallel to update data, the online approach makes sure that information is used as soon as it is known. The policy gradient algorithm can also be formulated in batch-fashion: all gradients are summed over the states and actions, and the parameters are updated at the end of the trajectory. Since parameter updates can be expensive, the batch approach can be more efcient. An intermediate form that is frequently applied in practice is to work with mini-batches, trading of computational efciency for information efciency. Let us now take a step back and look at the algorithm and assess how well it works. 4.2.2 Bias-Variance Trade-Of in Policy-Based Methods Now that we have seen the principles behind a policy-based algorithm, let us see how policy-based algorithms work in practice, and compare advantages and disadvantages of the policy-based approach. Let us start with the advantages. First of all, parameterization is at the core of policy-based methods, making them a good match for deep learning. For valuebased methods deep learning had to be retroftted, giving rise to complications as we saw in Sect. 3.2.3. Second, policy-based methods can easily fnd stochastic policies; value-based methods fnd deterministic policies. Due to their stochastic 4.2 Policy-Based Agents 99 nature, policy-based methods naturally explore, without the need for methods such as \ud835\udf16-greedy, or more involved methods, that may require tuning to work well. Third, policy-based methods are efective in large or continuous action spaces. Small changes in \ud835\udf03 lead to small changes in \ud835\udf0b, and to small changes in state distributions (they are smooth). Policy-based algorithms do not sufer (as much) from convergence and stability issues that are seen in arg max-based algorithms in large or continuous action spaces. On the other hand, there are disadvantages to the episodic Monte Carlo version of the REINFORCE algorithm. Remember that REINFORCE generates a full random episode in each iteration, before it assesses the quality. (Value-based methods use a reward to select the next action in each time step of the episode.) Because of this, policy-based is low bias, since full random trajectories are generated. However, they are also high variance, since the full trajectory is generated randomly (whereas valuebased uses the value for guidance at each selection step). What are the consequences? First, policy evaluation of full trajectories has low sample efciency and high variance. As a consequence, policy improvement happens infrequently, leading to slow convergence compared to value-based methods. Second, this approach often fnds a local optimum, since convergence to the global optimum takes too long. Much research has been performed to address the high variance of the episodebased vanilla policy gradient [57, 421, 420, 293]. The enhancements that have been found have greatly improved performance, so much so that policy-based approaches\u2014such as A3C, PPO, SAC, DDPG\u2014have become favorite model-free reinforcement learning algorithms for many applications. The enhancements to reduce high variance that we discuss are: \u2022 Actor critic introduces within-episode value-based critics based on temporal diference value bootstrapping; \u2022 Baseline subtraction introduces an advantage function to lower variance; \u2022 Trust regions reduce large policy parameter changes; \u2022 Exploration is crucial to get out of local minima and for more robust result; high entropy action distributions are often used. Let us have a look at these enhancements. 4.2.3 Actor Critic Bootstrapping The actor critic approach combines value-based elements with the policy-based method. The actor stands for the action, or policy-based, approach; the critic stands for the value-based approach [743]. Action selection in episodic REINFORCE is random, and hence low bias. However, variance is high, since the full episode is sampled (the size and direction of the update can strongly vary between diferent samples). The actor critic approach is designed to combine the advantage of the value-based approach (low variance) with the advantage of the policy-based approach (low bias). Actor critic methods are 100 4 Policy-Based Reinforcement Learning Algorithm 4.3 Actor critic with bootstrapping Input: A policy \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60), a value function \ud835\udc49\ud835\udf19 (\ud835\udc60) An estimation depth \ud835\udc5b, learning rate \ud835\udefc, number of episodes \ud835\udc40 Initialization: Randomly initialize \ud835\udf03 and \ud835\udf19 repeat for \ud835\udc56 \u2208 1, . . . , \ud835\udc40 do Sample trace \ud835\udf0f = {\ud835\udc600, \ud835\udc4e0, \ud835\udc5f0, \ud835\udc601, .., \ud835\udc60\ud835\udc47 } following \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) for \ud835\udc61 \u2208 0, . . . , \ud835\udc47 \u2212 1 do \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ufffd\ud835\udc5b\u22121 \ud835\udc58=0 \ud835\udefe\ud835\udc58 \u00b7 \ud835\udc5f\ud835\udc61+\ud835\udc58 + \ud835\udefe\ud835\udc5b \u00b7 \ud835\udc49\ud835\udf19 (\ud835\udc60\ud835\udc61+\ud835\udc5b) \u22b2 \ud835\udc5b-step target end for end for \ud835\udf19 \u2190 \ud835\udf19 \u2212 \ud835\udefc \u00b7 \u2207\ud835\udf19 \ufffd \ud835\udc61 \ufffd \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49\ud835\udf19 (\ud835\udc60\ud835\udc61)\ufffd2 \u22b2 Descent value loss \ud835\udf03 \u2190 \ud835\udf03 + \ud835\udefc \u00b7 \ufffd \ud835\udc61 [ \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u00b7 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61)] \u22b2 Ascent policy gradient until \u2207\ud835\udf03 \ud835\udc3d (\ud835\udf03) converges below \ud835\udf16 return Parameters \ud835\udf03 popular because they work well. It is an active feld where many diferent algorithms have been developed. The variance of policy methods can originate from two sources: (1) high variance in the cumulative reward estimate, and (2) high variance in the gradient estimate. For both problems a solution has been developed: bootstrapping for better reward estimates, and baseline subtraction to lower the variance of gradient estimates. Both of these methods use the learned value function, which we denote by \ud835\udc49\ud835\udf19(\ud835\udc60). The value function can use a separate neural network, with separate parameters \ud835\udf19, or it can use a value head on top of the actor parameters \ud835\udf03. In this case the actor and the critic share the lower layers of the network, and the network has two separate top heads: a policy and a value head. We will use \ud835\udf19 for the parameters of the value function, to discriminate them from the policy parameters \ud835\udf03. Temporal Diference Bootstrapping To reduce the variance of the policy gradient, we can increase the number of traces \ud835\udc40 that we sample. However, the possible number of diferent traces is exponential in the length of the trace for a given stochastic policy, and we cannot aford to sample them all for one update. In practice the number of sampled traces \ud835\udc40 is small, sometimes even \ud835\udc40 = 1, updating the policy parameters from a single trace. The return of the trace depends on many random action choices; the update has high variance. A solution is to use a principle that we known from temporal diference learning, to bootstrap the value function step by step. Bootstrapping uses the value function to compute intermediate \ud835\udc5b-step values per episode, trading-of variance for bias. The \ud835\udc5b-step values are in-between full-episode Monte Carlo and single step temporal diference targets. We can use bootstrapping to compute an \ud835\udc5b-step target 4.2 Policy-Based Agents 101 \u02c6\ud835\udc44n(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc5b\u22121 \u2211\ufe01 \ud835\udc58=0 \ud835\udc5f\ud835\udc61+\ud835\udc58 + \ud835\udc49\ud835\udf19(\ud835\udc60\ud835\udc61+\ud835\udc5b), and we can then update the value function, for example on a squared loss L(\ud835\udf19|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ufffd \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49\ud835\udf19(\ud835\udc60\ud835\udc61)\ufffd2 and update the policy with the standard policy gradient but with that (improved) value \u02c6\ud835\udc44\ud835\udc5b \u2207\ud835\udf03L(\ud835\udf03|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u00b7 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61). We are now using the value function prominently in the algorithm, which is parameterized by a separate set of parameters, denoted by \ud835\udf19; the policy parameters are still denoted by \ud835\udf03. The use of both policy and value is what gives the actor critic approach its name. An example algorithm is shown in Alg. 4.3. When we compare this algorithm with Alg. 4.2, we see how the policy gradient ascent update now uses the \ud835\udc5b-step \u02c6\ud835\udc44\ud835\udc5b value estimate instead of the trace return \ud835\udc45. We also see that this time the parameter updates are in batch mode, with separate summations. 4.2.4 Baseline Subtraction with Advantage Function Another method to reduce the variance of the policy gradient is by baseline subtraction. Subtracting a baseline from a set of numbers reduces the variance, but leaves the expectation unafected. Assume, in a given state with three available actions, that we sample action returns of 65, 70, and 75, respectively. Policy gradient will then try to push the probability of each action up, since the return for each action is positive. The above method may lead to a problem, since we are pushing all actions up (only somewhat harder on one of them). It might be better if we only push up on actions that are higher than the average (action 75 is higher than the average of 70 in this example), and push down on actions that are below average (65 in this example). We can do so through baseline subtraction. The most common choice for the baseline is the value function. When we subtract the value \ud835\udc49 from a state-action value estimate \ud835\udc44, the function is called the advantage function: \ud835\udc34(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49(\ud835\udc60\ud835\udc61). The \ud835\udc34 function subtracts the value of the state \ud835\udc60 from the state-action value. It now estimates how much better a particular action is compared to the expectation of a particular state. We can combine baseline subtraction with any bootstrapping method to estimate the cumulative reward \u02c6\ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). We compute \u02c6\ud835\udc34n(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49\ud835\udf19(\ud835\udc60\ud835\udc61) 102 4 Policy-Based Reinforcement Learning Algorithm 4.4 Actor critic with bootstrapping and baseline subtraction Input: A policy \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60), a value function \ud835\udc49\ud835\udf19 (\ud835\udc60) An estimation depth \ud835\udc5b, learning rate \ud835\udefc, number of episode \ud835\udc40 Initialization: Randomly initialize \ud835\udf03 and \ud835\udf19 while not converged do for \ud835\udc56 = 1, . . . , \ud835\udc40 do Sample trace \ud835\udf0f = {\ud835\udc600, \ud835\udc4e0, \ud835\udc5f0, \ud835\udc601, .., \ud835\udc60\ud835\udc47 } following \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) for \ud835\udc61 = 0, . . . , \ud835\udc47 \u2212 1 do \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ufffd\ud835\udc5b\u22121 \ud835\udc58=0 \ud835\udefe\ud835\udc58 \u00b7 \ud835\udc5f\ud835\udc61+\ud835\udc58 + \ud835\udefe\ud835\udc5b \u00b7 \ud835\udc49\ud835\udf19 (\ud835\udc60\ud835\udc61+\ud835\udc5b) \u22b2 \ud835\udc5b-step target \u02c6\ud835\udc34n(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49\ud835\udf19 (\ud835\udc60\ud835\udc61) \u22b2 Advantage end for end for \ud835\udf19 \u2190 \ud835\udf19 \u2212 \ud835\udefc \u00b7 \u2207\ud835\udf19 \ufffd \ud835\udc61 \ufffd \u02c6\ud835\udc34\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)\ufffd2 \u22b2 Descent Advantage loss \ud835\udf03 \u2190 \ud835\udf03 + \ud835\udefc \u00b7 \ufffd \ud835\udc61 [ \u02c6\ud835\udc34\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u00b7 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61)] \u22b2 Ascent policy gradient end while return Parameters \ud835\udf03 and update the policy with \u2207\ud835\udf03L(\ud835\udf03|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \u02c6\ud835\udc34\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u00b7 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61). We have now seen the ingredients to construct a full actor critic algorithm. An example algorithm is shown in Alg. 4.4. Generic Policy Gradient Formulation With these two ideas we can formulate an entire spectrum of policy gradient methods, depending on the type of cumulative reward estimate that they use. In general, the policy gradient estimator takes the following form, where we now introduce a new target \u03a8\ud835\udc61 that we sample from the trajectories \ud835\udf0f: \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) = E\ud835\udf0f0\u223c\ud835\udc5d\ud835\udf03 (\ud835\udf0f0) \ufffd \ud835\udc5b \u2211\ufe01 \ud835\udc61=0 \u03a8\ud835\udc61\u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61) \ufffd There is a variety of potential choices for \u03a8\ud835\udc61, based on the use of bootstrapping and baseline substraction: 4.2 Policy-Based Agents 103 \u03a8\ud835\udc61 = \u02c6\ud835\udc44\ud835\udc40\ud835\udc36 (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \u221e \u2211\ufe01 \ud835\udc56=\ud835\udc61 \ud835\udefe\ud835\udc56 \u00b7 \ud835\udc5f\ud835\udc56 Monte Carlo target \u03a8\ud835\udc61 = \u02c6\ud835\udc44\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc5b\u22121 \u2211\ufe01 \ud835\udc56=\ud835\udc61 \ud835\udefe\ud835\udc56 \u00b7 \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc5b\ud835\udc49\ud835\udf03 (\ud835\udc60\ud835\udc5b) bootstrap (\ud835\udc5b-step target) \u03a8\ud835\udc61 = \u02c6\ud835\udc34\ud835\udc40\ud835\udc36 (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \u221e \u2211\ufe01 \ud835\udc56=\ud835\udc61 \ud835\udefe\ud835\udc56 \u00b7 \ud835\udc5f\ud835\udc56 \u2212 \ud835\udc49\ud835\udf03 (\ud835\udc60\ud835\udc61) baseline subtraction \u03a8\ud835\udc61 = \u02c6\ud835\udc34\ud835\udc5b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc5b\u22121 \u2211\ufe01 \ud835\udc56=\ud835\udc61 \ud835\udefe\ud835\udc56 \u00b7 \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc5b\ud835\udc49\ud835\udf03 (\ud835\udc60\ud835\udc5b) \u2212 \ud835\udc49\ud835\udf03 (\ud835\udc60\ud835\udc61) baseline + bootstrap \u03a8\ud835\udc61 = \ud835\udc44 \ud835\udf19(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) Q-value approximation Actor critic algorithms are among the most popular model-free reinforcement learning algorithms in practice, due to their good performance. After having discussed relevant theoretical background, it is time to look at how actor critic can be implemented in a practical, high performance, algorithm. We will start with A3C. Asynchronous Advantage Actor Critic Many high performance implementations are based on the actor critic approach. For large problems the algorithm is typically parallelized and implemented on a large cluster computer. A well-known parallel algorithm is Asynchronous advantage actor critic (A3C). A3C is a framework that uses asynchronous (parallel and distributed) gradient descent for optimization of deep neural network controllers [521]. There is also a non-parallel version of A3C, the synchronous variant A2C [849]. Together they popularized this approach to actor critic methods. Figure 4.4 shows the distributed architecture of A3C [382]; Alg. 4.5 shows the pseudocode, from Mnih et al. [521]. The A3C network will estimate both a value function \ud835\udc49\ud835\udf19(\ud835\udc60) and an advantage function \ud835\udc34\ud835\udf19(\ud835\udc60, \ud835\udc4e), as well as a policy function \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60). In the experiments on Atari [521], the neural networks were separate fully-connected policy and value heads at the top (orange in Fig. 4.4), followed by joint convolutional networks (blue). This network architecture is replicated over the distributed workers. Each of these workers are run on a separate processor thread and are synced with global parameters from time to time. A3C improves on classic REINFORCE in the following ways: it uses an advantage actor critic design, it uses deep learning, and it makes efcient use of parallelism in the training stage. The gradient accumulation step at the end of the code can be considered as a parallelized reformulation of minibatch-based stochastic gradient update: the values of \ud835\udf19 or \ud835\udf03 are adjusted in the direction of each training thread independently. A major contribution of A3C comes from its parallelized and asynchronous architecture: multiple actor-learners are dispatched to separate instantiations of the environment; they all interact with the environment and collect 104 4 Policy-Based Reinforcement Learning Fig. 4.4 A3C network [382] experience, and asynchronously push their gradient updates to a central target network (just as DQN). It was found that the parallel actor-learners have a stabilizing efect on training. A3C surpassed the previous state-of-the-art on the Atari domain and succeeded on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using high-resolution visual input [521]. 4.2.5 Trust Region Optimization Another important approach to further reduce the variance of policy methods is the trust region approach. Trust region policy optimization (TRPO) aims to further reduce the high variability in the policy parameters, by using a special loss function with an additional constraint on the optimization problem [681]. A naive approach to speed up an algorithm is to try to increase the step size of hyperparameters, such as the learning rate, and the policy parameters. This approach will fail to uncover solutions that are hidden in fner grained trajectories, and the optimization will converge to local optima. For this reason the step size should not be too large. A less naive approach is to use an adaptive step size that depends on the output of the optimization progress. Trust regions are used in general optimization problems to constrain the update size [734]. The algorithms work by computing the quality of the approximation; if 4.2 Policy-Based Agents 105 Algorithm 4.5 Asynchronous advantage actor-critic pseudocode for each actorlearner thread [521] Input: Assume global shared parameter vectors \ud835\udf03 and \ud835\udf19 and global shared counter \ud835\udc47 = 0 Assume thread-specifc parameter vectors \ud835\udf03\u2032 and \ud835\udf19\u2032 Initialize thread step counter \ud835\udc61 \u2190 1 repeat Reset gradients: \ud835\udc51\ud835\udf03 \u2190 0 and \ud835\udc51\ud835\udf19 \u2190 0. Synchronize thread-specifc parameters \ud835\udf03\u2032 = \ud835\udf03 and \ud835\udf19\u2032 = \ud835\udf19 \ud835\udc61\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 = \ud835\udc61 Get state \ud835\udc60\ud835\udc61 repeat Perform \ud835\udc4e\ud835\udc61 according to policy \ud835\udf0b(\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61; \ud835\udf03\u2032) Receive reward \ud835\udc5f\ud835\udc61 and new state \ud835\udc60\ud835\udc61+1 \ud835\udc61 \u2190 \ud835\udc61 + 1 \ud835\udc47 \u2190 \ud835\udc47 + 1 until terminal \ud835\udc60\ud835\udc61 or \ud835\udc61 \u2212 \ud835\udc61\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 == \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc45 = \ufffd 0 for terminal \ud835\udc60\ud835\udc61 \ud835\udc49 (\ud835\udc60\ud835\udc61, \ud835\udf19\u2032) for non-terminal \ud835\udc60\ud835\udc61 // Bootstrap from last state for \ud835\udc56 \u2208 {\ud835\udc61 \u2212 1, . . . , \ud835\udc61\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 } do \ud835\udc45 \u2190 \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc45 Accumulate gradients wrt \ud835\udf03\u2032: \ud835\udc51\ud835\udf03 \u2190 \ud835\udc51\ud835\udf03 + \u2207\ud835\udf03\u2032 log \ud835\udf0b(\ud835\udc4e\ud835\udc56 |\ud835\udc60\ud835\udc56; \ud835\udf03\u2032) (\ud835\udc45 \u2212 \ud835\udc49 (\ud835\udc60\ud835\udc56; \ud835\udf19\u2032)) Accumulate gradients wrt \ud835\udf19\u2032: \ud835\udc51\ud835\udf19 \u2190 \ud835\udc51\ud835\udf19 + \ud835\udf15 (\ud835\udc45 \u2212 \ud835\udc49 (\ud835\udc60\ud835\udc56; \ud835\udf19\u2032))2/\ud835\udf15\ud835\udf19\u2032 end for Perform asynchronous update of \ud835\udf03 using \ud835\udc51\ud835\udf03 and of \ud835\udf19 using \ud835\udc51\ud835\udf19. until \ud835\udc47 > \ud835\udc47\ud835\udc5a\ud835\udc4e\ud835\udc65 it is still good, then the trust region is expanded. Alternatively, the region can be shrunk if the divergence of the new and current policy is getting large. Schulman et al. [681] introduced trust region policy optimization (TRPO) based on this ideas, trying to take the largest possible parameter improvement step on a policy, without accidentally causing performance to collapse. To this end, as it samples policies, TRPO compares the old and the new policy: L(\ud835\udf03) = E\ud835\udc61 \ufffd \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61) \ud835\udf0b\ud835\udf03old(\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61) \u00b7 \ud835\udc34\ud835\udc61 \ufffd . In order to increase the learning step size, TRPO tries to maximize this loss function L, subject to the constraint that the old and the new policy are not too far away. In TRPO the Kullback-Leibler divergence5 is used for this purpose: E\ud835\udc61 [KL(\ud835\udf0b\ud835\udf03old(\u00b7|\ud835\udc60\ud835\udc61), \ud835\udf0b\ud835\udf03 (\u00b7|\ud835\udc60\ud835\udc61))] \u2264 \ud835\udeff. TRPO scales to complex high-dimensional problems. Original experiments demonstrated its robust performance on simulated robotic Swimming, Hopping, Walking gaits, and Atari games. TRPO is commonly used in experiments and as a baseline for developing new algorithms. A disadvantage of TRPO is that it is 5 The Kullback-Leibler divergence is a measure of distance between probability distributions [436, 93]. 106 4 Policy-Based Reinforcement Learning a complicated algorithm that uses second order derivatives; we will not cover the pseudocode here. Implementations can be found at Spinning Up6 and Stable Baselines.7 Proximal policy optimzation (PPO) [683] was developed as an improvement of TRPO. PPO has some of the benefts of TRPO, but is simpler to implement, is more general, has better empirical sample complexity and has better run time complexity. It is motivated by the same question as TRPO, to take the largest possible improvement step on a policy parameter without causing performance collapse. There are two variants of PPO: PPO-Penalty and PPO-Clip. PPO-Penalty approximately solves a KL-constrained update (like TRPO), but merely penalizes the KL-divergence in the objective function instead of making it a hard constraint. PPO-Clip does not use a KL-divergence term in the objective and has no constraints either. Instead it relies on clipping in the objective function to remove incentives for the new policy to get far from the old policy; it clips the diference between the old and the new policy within a fxed range [1 \u2212 \ud835\udf16, 1 + \ud835\udf16] \u00b7 \ud835\udc34\ud835\udc61. While simpler than TRPO, PPO is still a complicated algorithm to implement, and we omit the code here. The authors of PPO provide an implementation as a baseline.8 Both TRPO and PPO are on-policy algorithms. Hsu et al. [353] refect on design choices of PPO. 4.2.6 Entropy and Exploration A problem in many deep reinforcement learning experiments where only a fraction of the state space is sampled, is brittleness: the algorithms get stuck in local optima, and diferent choices for hyperparameters can cause large diferences in performance. Even a diferent choice for seed for the random number generator can cause large diferences in performance for many algorithms. For large problems, exploration is important, in value-based and policy-based approaches alike. We must provide the incentive to sometimes try an action which currently seems suboptimal [524]. Too little exploration results in brittle, local, optima. When we learn a deterministic policy \ud835\udf0b\ud835\udf03 (\ud835\udc60) \u2192 \ud835\udc4e, we can manually add exploration noise to the behavior policy. In a continuous action space we can use Gaussian noise, while in a discrete action space we can use Dirichlet noise [425]. For example, in a 1D continuous action space we could use: \ud835\udf0b\ud835\udf03,behavior(\ud835\udc4e|\ud835\udc60) = \ud835\udf0b\ud835\udf03 (\ud835\udc60) + N (0, \ud835\udf0e), where N (\ud835\udf07, \ud835\udf0e) is the Gaussian (normal) distribution with hyperparameters mean \ud835\udf07 = 0 and standard deviation \ud835\udf0e; \ud835\udf0e is our exploration hyperparameter. 6 https://spinningup.openai.com 7 https://stable-baselines.readthedocs.io 8 https://openai.com/blog/openai-baselines-ppo/#ppo 4.2 Policy-Based Agents 107 Soft Actor Critic When we learn a stochastic policy \ud835\udf0b(\ud835\udc4e|\ud835\udc60), then exploration is already partially ensured due to the stochastic nature of our policy. For example, when we predict a Gaussian distribution, then simply sampling from this distribution will already induce variation in the chosen actions. \ud835\udf0b\ud835\udf03,behavior(\ud835\udc4e|\ud835\udc60) = \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) However, when there is not sufcient exploration, a potential problem is the collapse of the policy distribution. The distribution then becomes too narrow, and we lose the exploration pressure that is necessary for good performance. Although we could simply add additional noise, another common approach is to use entropy regularization (see Sect. A.2 for details). We then add an additional penalty to the loss function, that enforces the entropy \ud835\udc3b of the distribution to stay larger. Soft actor critic (SAC) is a well-known algorithm that focuses on exploration [306, 307].9 SAC extends the policy gradient equation to \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 + \ud835\udc45 \u00b7 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61) + \ud835\udf02\u2207\ud835\udf03\ud835\udc3b[\ud835\udf0b\ud835\udf03 (\u00b7|\ud835\udc60\ud835\udc61)] where \ud835\udf02 \u2208 R+ is a constant that determines the amount of entropy regularization. SAC ensures that we will move \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60) to the optimal policy, while also ensuring that the policy stays as wide as possible (trading of the two against eachother). Entropy is computed as \ud835\udc3b = \u2212 \ufffd \ud835\udc56 \ud835\udc5d\ud835\udc56 log \ud835\udc5d\ud835\udc56 where \ud835\udc5d\ud835\udc56 is the probability of being in state \ud835\udc56; in SAC entropy is the negative log of the stochastic policy function \u2212 log \ud835\udf0b\ud835\udf03 (\ud835\udc4e|\ud835\udc60). High-entropy policies favor exploration. First, the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues. Second, with improved exploration comes improved learning speed. Most policy-based algorithms (including A3C, TRPO, and PPO) only optimize for expected value. By including entropy explicitly in the optimization goal, SAC is able to increase the stability of outcome policies, achieving stable results for diferent random seeds, and reducing the sensitivity to hyperparameter settings. Including entropy into the optimization goal has been studied widely, see, for example, [305, 396, 779, 880, 547]. A further element that SAC uses to improve stability and sample efciency is a replay bufer. Many policy-based algorithms are on-policy learners (including A3C, TRPO, and PPO). In on-policy algorithms each policy improvement uses feedback on actions according to the most recent version of the behavior policy. On-policy methods converge well, but tend to require many samples to do so. In contrast, many value-based algorithms are of-policy: each policy improvement can use feedback collected at any earlier point during training, regardless of how the behavior policy was acting to explore the environment at the time when the feedback was obtained. The replay bufer is such a mechanism, breaking out of local maxima. Large replay 9 https://github.com/haarnoja/sac 108 4 Policy-Based Reinforcement Learning bufers cause of-policy behavior, improving sample efciency by learning from behavior of the past, but also potentially causing convergence problems. Like DQN, SAC has overcome these problems, and achieves stable of-policy performance. 4.2.7 Deterministic Policy Gradient Actor critic approaches improve the policy-based approach with various value-based ideas, and with good results. Another method to join policy and value approaches is to use a learned value function as a diferentiable target to optimize the policy against\u2014we let the policy follow the value function [524]. An example is the deterministic policy gradient [705]. Imagine we collect data \ud835\udc37 and train a value network \ud835\udc44 \ud835\udf19(\ud835\udc60, \ud835\udc4e). We can then attempt to optimize the parameters \ud835\udf03 of a deterministic policy by optimizing the prediction of the value network: \ud835\udc3d(\ud835\udf03) = E\ud835\udc60\u223c\ud835\udc37 \ufffd \ud835\udc5b \u2211\ufe01 \ud835\udc61=0 \ud835\udc44 \ud835\udf19(\ud835\udc60, \ud835\udf0b\ud835\udf03 (\ud835\udc60)) \ufffd , which by the chain-rule gives the following gradient expression \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) = \ud835\udc5b \u2211\ufe01 \ud835\udc61=0 \u2207\ud835\udc4e\ud835\udc44 \ud835\udf19(\ud835\udc60, \ud835\udc4e) \u00b7 \u2207\ud835\udf03\ud835\udf0b\ud835\udf03 (\ud835\udc60). In essence, we frst train a state-action value network based on sampled data, and then let the policy follow the value network, by simply chaining the gradients. Thereby, we push the policy network in the direction of those actions \ud835\udc4e that increase the value network prediction, towards actions that perform better. Lillicrap et al. [480] present Deep deterministic policy gradient (DDPG). It is based on DQN, with the purpose of applying it to continuous action functions. In DQN, if the optimal action-value function \ud835\udc44\u2605(\ud835\udc60, \ud835\udc4e) is known, then the optimal action \ud835\udc4e\u2605(\ud835\udc60) can be found via \ud835\udc4e\u2605(\ud835\udc60) = arg max\ud835\udc4e \ud835\udc44\u2605(\ud835\udc60, \ud835\udc4e). DDPG uses the derivative of a continuous function \ud835\udc44(\ud835\udc60, \ud835\udc4e) with respect to the action argument to efciently approximate max\ud835\udc4e \ud835\udc44(\ud835\udc60, \ud835\udc4e). DDPG is also based on the algorithms Deterministic policy gradients (DPG) [705] and Neurally ftted Q-learning with continuous actions (NFQCA) [311], two actor critic algorithms. The pseudocode of DDPG is shown in Alg. 4.6. DDPG has been shown to work well on simulated physics tasks, including classic problems such as Cartpole, Gripper, Walker, and Car driving, being able to learn policies directly from raw pixel inputs. DDPG is of-policy and uses a replay bufer and a separate target network to achieve stable deep reinforcement learning (just as DQN). 4.2 Policy-Based Agents 109 Algorithm 4.6 DDPG algorithm [480] Randomly initialize critic network \ud835\udc44\ud835\udf19 (\ud835\udc60, \ud835\udc4e) and actor \ud835\udf0b\ud835\udf03 (\ud835\udc60) with weights \ud835\udf19 and \ud835\udf03. Initialize target network \ud835\udc44\u2032 and \ud835\udf0b\u2032 with weights \ud835\udf19\u2032 \u2190 \ud835\udf19, \ud835\udf03\u2032 \u2190 \ud835\udf03 Initialize replay bufer \ud835\udc45 for episode = 1, M do Initialize a random process N for action exploration Receive initial observation state \ud835\udc601 for t = 1, T do Select action \ud835\udc4e\ud835\udc61 = \ud835\udf0b\ud835\udf03 (\ud835\udc60\ud835\udc61) + N\ud835\udc61 according to the current policy and exploration noise Execute action \ud835\udc4e\ud835\udc61 and observe reward \ud835\udc5f\ud835\udc61 and observe new state \ud835\udc60\ud835\udc61+1 Store transition (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc60\ud835\udc61+1) in \ud835\udc45 Sample a random minibatch of \ud835\udc41 transitions (\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56, \ud835\udc60\ud835\udc56+1) from \ud835\udc45 Set \ud835\udc66\ud835\udc56 = \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc44\ud835\udf19\u2032 (\ud835\udc60\ud835\udc56+1, \ud835\udf0b\ud835\udf03\u2032 (\ud835\udc60\ud835\udc56+1)) Update critic by minimizing the loss: \ud835\udc3f = 1 \ud835\udc41 \ufffd \ud835\udc56 (\ud835\udc66\ud835\udc56 \u2212 \ud835\udc44\ud835\udf19 (\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56))2 Update the actor policy using the sampled policy gradient: \u2207\ud835\udf03 \ud835\udc3d \u2248 1 \ud835\udc41 \u2211\ufe01 \ud835\udc56 \u2207\ud835\udc4e\ud835\udc44\ud835\udf19 (\ud835\udc60, \ud835\udc4e) |\ud835\udc60=\ud835\udc60\ud835\udc56,\ud835\udc4e=\ud835\udf07(\ud835\udc60\ud835\udc56) \u2207\ud835\udf03 \ud835\udf0b\ud835\udf03 (\ud835\udc60) |\ud835\udc60\ud835\udc56 Update the target networks: \ud835\udf19\u2032 \u2190 \ud835\udf0f\ud835\udf19 + (1 \u2212 \ud835\udf0f) \ud835\udf19\u2032 \ud835\udf03\u2032 \u2190 \ud835\udf0f \ud835\udf03 + (1 \u2212 \ud835\udf0f) \ud835\udf03\u2032 end for end for DDPG is a popular actor critic algorithm. Annotated pseudocode and efcient implementations can be found at Spinning Up10 and Stable Baselines11 in addition to the original paper [480]. Conclusion We have seen quite some algorithms that combine the policy and value approach, and we have discussed possible combinations of these building blocks to construct working algorithms. Figure 4.5 provides a conceptual map of how the diferent approaches are related, including two approaches that will be discussed in later chapters (AlphaZero and Evolutionary approaches). Researchers have constructed many algorithms and performed experiments to see when they perform best. Quite a number of actor critic algorithms have been developed. Working high-performance Python implementations can be found on GitHub in the Stable Baselines.12 10 https://spinningup.openai.com 11 https://stable-baselines.readthedocs.io 12 https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html 110 4 Policy-Based Reinforcement Learning Fig. 4.5 Value-based, policy-based and Actor critic methods [524]. 4.2.8 Hands On: PPO and DDPG MuJoCo Examples OpenAI\u2019s Spinning Up provides a tutorial on policy gradient algorithms, complete with TensorFlow and PyTorch versions of REINFORCE to learn Gym\u2019s Cartpole.13 with the TensorFlow code14 or PyTorch.15 Now that we have discussed these algorithms, let us see how they work in practice, to get a feeling for the algorithms and their hyperparameters. MuJoCo is the most frequently used physics simulator in policy-based learning experiments. Gym, the (Stable) Baselines and Spinning up allow us to run any mix of learning algorithms and experimental environments. You are encouraged to try these experiments yourself. Please be warned, however, that attempting to install all necessary pieces of software may invite a minor version-hell. Diferent versions of your operating system, of Python, of GCC, of Gym, of the Baselines, of TensorFlow or PyTorch, and of MuJoCo all need to line up before you can see beautiful images of moving arms, legs and jumping humanoids. Unfortunately not all of these versions are backwardscompatible, specifcally the switch from Python 2 to 3 and from TensorFlow 1 to 2 introduced incompatible language changes. Getting everything to work may be an efort, and may require switching machines, operating systems and languages, but you should really try. This is the 13 Tutorial: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html# deriving-the-simplest-policy-gradient 14 TensorFlow: https://github.com/openai/spinningup/blob/master/spinup/examples/ tf1/pg_math/1_simple_pg.py 15 PyTorch: https://github.com/openai/spinningup/blob/master/spinup/examples/ pytorch/pg_math/1_simple_pg.py 4.3 Locomotion and Visuo-Motor Environments 111 disadvantage of being part of one of the fastest moving felds in machine learning research. If things do not work with your current operating system and Python version, in general a combination of Linux Ubuntu (or macOS), Python 3.7, TensorFlow 1 or PyTorch, Gym, and the Baselines may be a good idea to start with. Search on the GitHub repositories or Stackoverfow when you get error messages. Sometimes downgrading to the one-but latest version will be necessary, or fddling with include or library paths. If everything works, then both Spinning up and the Baselines provide convenient scripts that facilitate mixing and matching algorithms and environments from the command line. For example, to run Spinup\u2019s PPO on MuJoCo\u2019s Walker environment, with a 32 \u00d7 32 hidden layer, the following command line does the job: python -m spinup.run ppo --hid \"[32,32]\" --env Walker2d-v2 --exp_name mujocotest To train DDPG from the Baselines on the Half-cheetah, the command is: python -m base-lines.run --alg=ddpg --env=HalfCheetah-v2 --num_timesteps=1e6 All hyperparameters can be controlled via the command line, providing for a fexible way to run experiments. A fnal example command line: python scripts/all_plots.py -a ddpg -e HalfCheetah Ant Hopper Walker2D -f logs/ -o logs/ddpg_results The Stable Baselines site explains what this command line does. 4.3 Locomotion and Visuo-Motor Environments We have seen many diferent policy-based reinforcement learning algorithms that can be used in agents with continuous action spaces. Let us have a closer look at the environments that they have been used in, and how well they perform. Policy-based methods, and especially the actor critic policy/value hybrid, work well for many problems, both with discrete and with continuous action spaces. Policy-based methods are often tested on complex high-dimensional robotics applications [417]. Let us have a look at the kind of environments that have been used to develop PPO, A3C, and the other algorithms. Two application categories are robot locomotion, and visuo-motor interaction. These two problems have drawn many researchers, and many new algorithms have 112 4 Policy-Based Reinforcement Learning Fig. 4.6 Humanoid Standing Up [682] been devised, some of which were able to learn impressives performance. For each of the two problems, we will discuss a few results in more detail. 4.3.1 Locomotion One of the problems of locomotion of legged entities is the problem of learning gaits. Humans, with two legs, can walk, run, and jump, amongst others. Dogs and horses, with four legs, have other gaits, where their legs may move in even more interesting patterns, such as the trot, canter, pace and gallop. The challenges that we pose robots are often easier. Typical reinforcement learning tasks are for a one-legged robot to learn to jump, for biped robots to walk and jump, and for a quadruped to get to learn to use its multitude of legs in any coordinated fashion that results in forward moving. Learning such policies can be quite computationally expensive, and a curious simulated virtual animal has emerged that is cheaper to simulate: the two-legged half-cheetah, whose task it is to run forward. We have already seen some of these robotic creatures in Figs. 4.2\u20134.3. The frst approach that we will discuss is by Schulman et al. [682]. They report experiments where human-like bipeds and quadrupeds must learn to stand up and learn running gaits. These are challenging 3D locomotion tasks that were formerly attempted with hand-crafted policies. Figure 4.6 shows a sequence of states. The challenge in these situations is actually somewhat spectacular: the agent is only provided with a positive reward for moving forward; based on nothing more it has to learn to control all its limbs by itself, through trial and error; no hint is given on how to control a leg or what its purpose is. These results are best watched in the movies that have been made16 about the learning process. 16 Such as the movie from the start of this chapter: https://www.youtube.com/watch?v=hx_ bgoTF7bs 4.3 Locomotion and Visuo-Motor Environments 113 Fig. 4.7 Walker Obstacle Course [325] Fig. 4.8 Quadruped Obstacle Course [325] The authors use an Advantage actor critic algorithm with trust regions. The algorithm is fully model-free, and learning with simulated physics was reported to take one to two weeks of real time. Learning to walk is quite a complicated challenge, as the movies illustrate. They also show the robot learning to scale an obstacle run all by itself. In another study, Heess et al. [325] report on end-to-end learning of complex robot locomotion from pixel input to (simulated) motor-actuation. Figure 4.7 shows how a walker scales an obstacle course and Fig. 4.8 shows a time lapse of how a quadruped traverses a course. Agents learned to run, jump, crouch and turn as the environment required, without explicit reward shaping or other hand-crafted features. For this experiment a distributed version of PPO was used. Interestingly, the researchers stress that the use of a rich\u2014varied, difcult\u2014environment helps to promote learning of complex behavior, that is also robust across a range of tasks. 4.3.2 Visuo-Motor Interaction Most experiments in \u201cend-to-end\u201d learning of robotic locomotion are set up so that the input is received directly from features that are derived from the states as calculated by the simulation software. A step further towards real-world interaction is to learn directly from camera pixels. We then model eye-hand coordination in visuo114 4 Policy-Based Reinforcement Learning Fig. 4.9 DeepMind Control Suite. Top: Acrobot, Ball-in-cup, Cart-pole, Cheetah, Finger, Fish, Hopper. Bottom: Humanoid, Manipulator, Pendulum, Point-mass, Reacher, Swimmer (6 and 15 links), Walker [755] motor interaction tasks, and the state of the environment has to be inferred from camera or other visual means, and then be translated in joint (muscle) actuations. Visuo-motor interaction is a difcult task, requiring many techniques to work together. Diferent environments have been introduced to test algorithms. Tassa et al. report on benchmarking eforts in robot locomotion with MuJoCo [755], introducing the DeepMind control suite, a suite of environments consisting of diferent MuJoCo control tasks (see Fig. 4.9). The authors also present baseline implementations of learning agents that use A3C, DDPG and D4PG (distributional distributed deep deterministic policy gradients\u2014an algorithm that extends DDPG). In addition to learning from state derived features, results are presented where the agent learns from 84 \u00d7 84 pixel information, in a simulated form of visuo-motor interaction. The DeepMind control suite is especially designed for further research in the feld [757, 511, 508, 510, 509]. Other environment suites are Meta-World [864], Surreal [234], RLbench [375]. Visuo-motor interaction is a challenging problem that remains an active area of research. 4.3.3 Benchmarking Benchmarking eforts are of great importance in the feld [212]. Henderson et al. [328] published an infuential study of the sensitivity of outcomes to diferent hyperparameter settings, and the infuence of non-determinism, by trying to reproduce many published works in the feld. They fnd large variations in outcomes, and in general that reproducibilty of results is problematic. They conclude that without signifcance metrics and tighter standardization of experimental reporting, it is difcult to determine whether improvements over the prior state-of-the-art are 4.3 Locomotion and Visuo-Motor Environments 115 meaningful [328]. Further studies confrmed these fndings [7],17 and today more works are being published with code, hyperparameters, and environments. Taking inspiration from the success of the Arcade Learning Environment in game playing, benchmark suites of continuous control tasks with high state and action dimensionality have been introduced [212, 261].18 The tasks include 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. The locomotion tasks are: Swimmer, Hopper, Walker, Half-cheetah, Ant, simple Humanoid and full Humanoid, with the goal being to move forward as fast as possible. These are difcult tasks because of the high degree of freedom of movement. Partial observation is achieved by adding noise or leaving out certain parts of the regular observations. The hierarchical tasks consist of low level tasks such as learning to move, and a high level task such as fnding the way out of a maze. Summary and Further Reading This chapter is concerned with the second kind of model-free algorithms: policybased methods. We summarize what we have learned, and provide pointers to further reading. Summary Policy-based model-free methods are some of the most popular methods of deep reinforcement learning. For large, continuous action spaces, indirect value-based methods are not well suited, because of the use of the arg max function to recover the best action to go with the value. Where value-based methods work step-by-step, vanilla policy-based methods roll out a full future trajectory or episode. Policy-based methods work with a parameterized current policy, which is well suited for a neural network as policy function approximator. After the full trajectory has been rolled out, the reward and the value of the trajectory is calculated and the policy parameters are updated, using gradient ascent. Since the value is only known at the end of an episode, classic policy-based methods have a higher variance than value based methods, and may converge to a local optimum. The best known classic policy method is called REINFORCE. Actor critic methods add a value network to the policy network, to achieve the benefts of both approaches. To reduce variance, \ud835\udc5b-step temporal diference bootstrapping can be added, and a baseline value can be subtracted, so that we get the so-called advantage function (which subtracts the value of the parent state from 17 https://github.com/google-research/rliable 18 The suite in the paper is called RLlab. A newer version of the suite is named Garage. See also Appendix C. 116 4 Policy-Based Reinforcement Learning the action values of the future states, bringing their expected value closer to zero). Well known actor critic methods are A3C, DDPG, TRPO, PPO, and SAC.19 A3C features an asynchronous (parallel, distributed) implementation, DDPG is an actor critic version of DQN for continous action spaces, TRPO and PPO use trust regions to achieve adaptive step sizes in non linear spaces, SAC optimizes for expected value and entropy of the policy. Benchmark studies have shown that the performance of these actor critic algorithm is as good or better than value-based methods [212, 328]. Robot learning is among the most popular applications for policy-based method. Model-free methods have low sample efciency, and to prevent the cost of wear after millions of samples, most experiments use a physics simulation as environment, such as MuJoCo. Two main application areas are locomotion (learning to walk, learning to run) and visuo-motor interaction (learning directly from camera images of one\u2019s own actions). Further Reading Policy-based methods have been an active research area for some time. Their natural suitability for deep function approximation for robotics applications and other applications with continuous action spaces has spurred a large interest in the research community. The classic policy-based algorithm is Williams\u2019 REINFORCE [844], which is based on the policy gradient theorem, see [744]. Our explanation is based on [220, 192, 255]. Joining policy and value-based methods as we do in actor critic is discussed in Barto et al. [57]. Mnih et al. [521] introduce a modern efcient parallel implementation named A3C. After the success of DQN a version for the continuous action space of policy-based methods was introduced as DDPG by Lillicrap et al. [480]. Schulman et al. have worked on trust regions, yielding efcient popular algorithms TRPO [681] and PPO [683]. Important benchmark studies of policy-based methods are Duan et al. [212] and Henderson et al. [328]. These papers have stimulated reproducibility in reinforcement learning research. Software environments that are used in testing policy-based methods are MuJoCo [780] and PyBullet [167]. Gym [108] and the DeepMind control suite [755] incorporate MuJoCo and provide an easy to use Python interface. An active research community has emerged around the DeepMind control suite. Exercises We have come to the end of this chapter, and it is time to test our understanding with questions, exercises, and a summary. 19 Asynchronous advantage actor critic; Deep deterministic policy gradients; Trust region policy optimization; Proximal policy optimization; Soft actor critic. 4.3 Locomotion and Visuo-Motor Environments 117 Questions Below are some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. 1. Why are value-based methods difcult to use in continuous action spaces? 2. What is MuJoCo? Can you name a few example tasks? 3. What is an advantage of policy-based methods? 4. What is a disadvantage of full-trajectory policy-based methods? 5. What is the diference between actor critic and vanilla policy-based methods? 6. How many parameter sets are used by actor critic? How can they be represented in a neural network? 7. Describe the relation between Monte Carlo REINFORCE, \ud835\udc5b-step methods, and temporal diference bootstrapping. 8. What is the advantage function? 9. Describe a MuJoCo task that methods such as PPO can learn to perform well. 10. Give two actor critic approaches to further improve upon bootstrapping and advantage functions, that are used in high-performing algorithms such as PPO and SAC. 11. Why is learning robot actions from image input hard? Exercises Let us now look at programming exercises. If you have not already done so, install MuJoCo or PyBullet, and install the DeepMind control suite.20 We will use agent algorithms from the Stable baselines. Furthermore, browse the examples directory of the DeepMind control suite on GitHub, and study the Colab notebook. 1. REINFORCE Go to the Medium blog21 and reimplement REINFORCE. You can choose PyTorch, or TensorFlow/Keras, in which case you will have to improvise. Run the algorithm on an environment with a discrete action space, and compare with DQN. Which works better? Run in an environment with a continuous action space. Note that Gym ofers a discrete and a continuous version of Mountain Car. 2. Algorithms Run REINFORCE on a Walker environment from the Baselines. Run DDPG, A3C, and PPO. Run them for diferent time steps. Make plots. Compare training speed, and outcome quality. Vary hyperparameters to develop an intuition for their efect. 3. Suite Explore the DeepMind control suite. Look around and see what environments have been provided, and how you can use them. Consider extending an environment. What learning challenges would you like to introduce? First do 20 https://github.com/deepmind/dm_control 21 https://medium.com/@ts1829/policy-gradient-reinforcement-learning-inpytorch-df1383ea0baf 118 4 Policy-Based Reinforcement Learning a survey of the literature that has been published about the DeepMind control suite. Chapter 5 Model-Based Reinforcement Learning The previous chapters discussed model-free methods, and we saw their success in video games and simulated robotics. In model-free methods the agent updates a policy directly from the feedback that the environment provides on its actions. The environment performs the state transitions and calculates the reward. A disadvantage of deep model-free methods is that they can be slow to train; for stable convergence or low variance often millions of environment samples are needed before the policy function converges to a high quality optimum. In contrast, with model-based methods the agent frst builds its own internal transition model from the environment feedback. The agent can then use this local transition model to fnd out about the efect of actions on states and rewards. The agent can use a planning algorithm to play what-if games, and generate policy updates, all without causing any state changes in the environment. This approach promises higher quality at lower sample complexity. Generating policy updates from the internal model is called planning or imagination. Model-based methods update the policy indirectly: the agent frst learns a local transition model from the environment, which the agent then uses to update the policy. Indirectly learning the policy function has two consequences. On the positive side, as soon as the agent has its own model of the state transitions of the world, it can learn the best policy for free, without further incurring the cost of acting in the environment. Model-based methods thus may have a lower sample complexity. The downside is that the learned transition model may be inaccurate, and the resulting policy may be of low quality. No matter how many samples can be taken for free from the model, if the agent\u2019s local transition model does not refect the environment\u2019s real transition model, then the locally learned policy function will not work in the environment. Thus, dealing with uncertainty and model bias are important elements in model-based reinforcement learning. The idea to frst learn an internal representation of the environment\u2019s transition function has been conceived many years ago, and transition models have been implemented in many diferent ways. Models can be tabular, or they can be based on various kinds of deep learning, as we will see. 119 120 5 Model-Based Reinforcement Learning This chapter will start with an example showing how model-based methods work. Next, we describe in more detail diferent kinds of model-based approaches; approaches that focus on learning an accurate model, and approaches for planning with an imperfect model. Finally, we describe application environments for which model-based methods have been used in practice, to see how well the approaches perform. The chapter is concluded with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Imagination \u2022 Uncertainty models \u2022 World models, Latent models \u2022 Model-predictive control \u2022 Deep end-to-end planning and learning Core Problem \u2022 Learn and use accurate transition models for high-dimensional problems Core Algorithms \u2022 Dyna-Q (Alg. 5.3) \u2022 Ensembles and model-predictive control (Alg. 5.4, 5.6) \u2022 Value prediction networks (Alg. 5.5) \u2022 Value iteration networks (Sect. 5.2.2.2) Building a Navigation Map To illustrate basic concepts of model-based reinforcement learning, we return to the supermarket example. Let us compare how model-free and model-based methods fnd their way to the supermarket in a new city.1 In this example we will use value-based Q-learning; 1 We use distance to the supermarket as negative reward, in order to formulate this as a distance minimization problem, while still being able to reason in our familiar reward maximization setting. 5 Model-Based Reinforcement Learning 121 our policy \ud835\udf0b(\ud835\udc60, \ud835\udc4e) will be derived directly from the \ud835\udc44(\ud835\udc60, \ud835\udc4e) values with arg max, and writing \ud835\udc44 is in this sense equivalent to writing \ud835\udf0b. Model-free Q-learning: the agent picks the start state \ud835\udc600, and uses (for example) an \ud835\udf16-greedy behavior policy on the action-value function \ud835\udc44(\ud835\udc60, \ud835\udc4e) to select the next action. The environment then executes the action, computes the next state \ud835\udc60\u2032 and reward \ud835\udc5f, and returns these to the agent. The agent updates its action-value function \ud835\udc44(\ud835\udc60, \ud835\udc4e) with the familiar update rule \ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2190 \ud835\udc44(\ud835\udc60, \ud835\udc4e) + \ud835\udefc[\ud835\udc5f + \ud835\udefe max \ud835\udc4e \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e) \u2212 \ud835\udc44(\ud835\udc60, \ud835\udc4e)]. The agent repeats this procedure until the values in the \ud835\udc44-function no longer change greatly. Thus we pick our start location in the city, perform one walk along a block in an \ud835\udf16-greedy direction, and record the reward and the new state at which we arrive. We use the information to update the policy, and from our new location, we walk again in an \ud835\udf16-greedy direction using the policy. If we fnd the supermarket, we start over again, trying to fnd a shorter path, until our policy values no longer change (this may take many environment interactions). Then the best policy is the path with the shortest distances. Model-based planning and learning: the agent uses the \ud835\udc44(\ud835\udc60, \ud835\udc4e) function as behavior policy as before to sample the new state and reward from the environment, and to update the policy (\ud835\udc44-function). In addition, however, the agent will record the new state and reward in a local transition \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) and reward function \ud835\udc45\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032). Because the agent now has these local entries we can also sample from our local functions to update the policy. We can choose: sample from the (expensive) environment transition function, or from the (cheap) local transition function. There is a caveat with sampling locally, however. The local functions may contain fewer entries\u2014or only high variance entries\u2014especially in the early stages, when few environment samples have been performed. The usefulness of the local functions increases as more environment samples are performed. Thus, we now have a local map on which to record the new states and rewards. We will use this map to peek, as often as we like and at no cost, at a location on that map, to update the \ud835\udc44-function. As more environment samples come in, the map will have more and more locations for which a distance to the supermarket is recorded. When glances at the map do not improve the policy anymore, we have to walk in the environment again, and, as before, update the map and the policy. In conclusion, model-free fnds all policy updates outside the agent, from the environment feedback; model-based also2 uses policy updates from within the agent, using information from its local map (see Fig. 5.1). In both methods all updates to the policy are ultimately derived from the environment feedback; model-based ofers a diferent way to use the information to update the policy, a way that may 2 One option is to only update the policy from the agent\u2019s internal transition model, and not by the environment samples anymore. However, another option is to keep using the environment samples to also update the policy in the model-free way. Sutton\u2019s Dyna [741] approach is a well-known example of this last, hybrid, approach. Compare also Fig. 5.2 and Fig. 5.4. 122 5 Model-Based Reinforcement Learning Value/Policy Environment Model acting model learning planning direct RL Fig. 5.1 Direct and Indirect Reinforcement Learning [743] be more information-efcient, by keeping information from each sample within the agent transition model and re-using that information. 5.1 Dynamics Models of High-Dimensional Problems The application environments for model-based reinforcement learning are the same as for model-free; our goal, however, is to solve larger and more complex problems in the same amount of time, by virtue of the lower sample complexity and, as it were, a deeper understanding of the environment. Transition Model and Knowledge Transfer The principle of model-based learning is as follows. Where model-free methods sample the environment to learn the state-to-action policy function \ud835\udf0b(\ud835\udc60, \ud835\udc4e) based on action rewards, model-based methods sample the environment to learn the stateto-state transition function \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) based on action rewards. Once the accuracy of this local transition function is good enough, the agent can sample from this local function to improve the policy \ud835\udf0b(\ud835\udc60, \ud835\udc4e) as often as it likes, without incurring the cost of actual environment samples. In the model-based approach, the agent builds its own local state-to-state transition (and reward) model of the environment, so that, in theory at least, it does not need the environment anymore. This brings us to another reason for the interest in model-based methods. For sequential decision problems, knowing the transition function is a natural way of capturing the essence of how the environment works\u2014\ud835\udf0b gives the next action, \ud835\udc47 gives the next state. This is useful, for example, when we switch to a related environment. When the transition function of the environment is known by the agent, then the agent can be adapted quickly, without having to learn a whole new policy by sampling the environment. When a good local transition function of the domain is known by the agent, then new, but related, problems might be solved efciently. Hence, 5.2 Learning and Planning Agents 123 model-based reinforcement learning may contribute to efcient transfer learning (see Chap. 9). Sample Efciency The sample efciency of an agent algorithm tells us how many environment samples it needs for the policy to reach a certain accuracy. To achieve high sample efciency, model-based methods learn a dynamics model. Learning high-accuracy high-capacity models of high-dimensional problems requires a high number of training examples, to prevent overftting (see Sect. B.2.7). Thus, reducing overftting in learning the transition model would negate (some of) the advantage of the low sample complexity that model-based learning of the policy function achieves. Constructing accurate deep transition models can be difcult in practice, and for many complex sequential decision problems the best results are often achieved with model-free methods, although deep model-based methods are becoming stronger (see, for example, Wang et al. [828]). 5.2 Learning and Planning Agents The promise of model-based reinforcement learning is to fnd a high-accuracy behavior policy at a low cost, by building a local model of the world. This will only work if the learned transition model provides accurate predictions, and if the extra cost of planning with the model is reasonable. Let us see which solutions have been developed for deep model-based reinforcement learning. In Sect. 5.3 we will have a closer look at the performance in diferent environments. First, in this section, we will look at four diferent algorithmic approaches, and at a classic approach: Dyna\u2019s tabular imagination. Tabular Imagination A classic approach is Dyna [741], which popularized the idea of model-based reinforcement learning. In Dyna, environment samples are used in a hybrid modelfree/model-based manner, to train the transition model, use planning to improve the policy, while also training the policy function directly. Why is Dyna a hybrid approach? Strict model-based methods update the policy only by planning using the agent\u2019s transition model, see Alg. 5.1. In Dyna, however, environment samples are used to also update the policy directly (see Fig. 5.2 and Alg. 5.2). Thus we get a hybrid approach combining model-based and model-free learning. This hybrid model-based planning is called imagination because looking ahead with the agent\u2019s own dynamics model resembles imagining environment 124 5 Model-Based Reinforcement Learning Algorithm 5.1 Strict Learned Dynamics Model repeat Sample environment \ud835\udc38 to generate data \ud835\udc37 = (\ud835\udc60, \ud835\udc4e, \ud835\udc5f\u2032, \ud835\udc60\u2032) Use \ud835\udc37 to learn \ud835\udc40 = \ud835\udc47\ud835\udc4e (\ud835\udc60, \ud835\udc60\u2032), \ud835\udc45\ud835\udc4e (\ud835\udc60, \ud835\udc60\u2032) \u22b2 learning for \ud835\udc5b = 1, . . . , \ud835\udc41 do Use \ud835\udc40 to update policy \ud835\udf0b(\ud835\udc60, \ud835\udc4e) \u22b2 planning end for until \ud835\udf0b converges Environment Dynamics Model Policy/Value learning learning acting planning Fig. 5.2 Hybrid Model-Based Imagination Algorithm 5.2 Hybrid Model-Based Imagination repeat Sample env \ud835\udc38 to generate data \ud835\udc37 = (\ud835\udc60, \ud835\udc4e, \ud835\udc5f\u2032, \ud835\udc60\u2032) Use \ud835\udc37 to update policy \ud835\udf0b(\ud835\udc60, \ud835\udc4e) \u22b2 learning Use \ud835\udc37 to learn \ud835\udc40 = \ud835\udc47\ud835\udc4e (\ud835\udc60, \ud835\udc60\u2032), \ud835\udc45\ud835\udc4e (\ud835\udc60, \ud835\udc60\u2032) \u22b2 learning for \ud835\udc5b = 1, . . . , \ud835\udc41 do Use \ud835\udc40 to update policy \ud835\udf0b(\ud835\udc60, \ud835\udc4e) \u22b2 planning end for until \ud835\udf0b converges samples outside the real environment inside the \u201cmind\u201d of the agent. In this approach the imagined samples augment the real (environment) samples at no sample cost.3 Imagination is a mix of model-based and model-free reinforcement learning. Imagination performs regular direct reinforcement learning, where the environment is sampled with actions according to the behavior policy, and the feedback is used to update the same behavior policy. Imagination also uses the environment sample to update the dynamics model {\ud835\udc47\ud835\udc4e, \ud835\udc45\ud835\udc4e}. This extra model is also sampled, and provides extra updates to the behavior policy, in between the model-free updates. The diagram in Fig. 5.2 shows how sample feedback is used both for updating the policy directly and for updating the model, which then updates the policy, by planning \u201cimagined\u201d feedback. In Alg. 5.2 the general imagination approach is shown as pseudocode. Sutton\u2019s Dyna-Q [741, 743], which is shown in more detail in Alg. 5.3, is a concrete implementation of the imagination approach. Dyna-Q uses the Q-function 3 The term imagination is used somewhat loosely in the feld. In a strict sense imagination refers only to updating the policy from the internal model by planning. In a wider sense imagination refers to hybrid schemes where the policy is updated from both the internal model and the environment. Sometimes the term dreaming is used for agents imagining environments. 5.2 Learning and Planning Agents 125 Algorithm 5.3 Dyna-Q [741] Initialize \ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2192 R randomly Initialize \ud835\udc40 (\ud835\udc60, \ud835\udc4e) \u2192 R \u00d7 \ud835\udc46 randomly \u22b2 Model repeat Select \ud835\udc60 \u2208 \ud835\udc46 randomly \ud835\udc4e \u2190 \ud835\udf0b(\ud835\udc60) \u22b2 \ud835\udf0b(\ud835\udc60) can be \ud835\udf16 -greedy(\ud835\udc60) based on \ud835\udc44 (\ud835\udc60\u2032, \ud835\udc5f) \u2190 \ud835\udc38 (\ud835\udc60, \ud835\udc4e) \u22b2 Learn new state and reward from environment \ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2190 \ud835\udc44(\ud835\udc60, \ud835\udc4e) + \ud835\udefc \u00b7 [\ud835\udc5f + \ud835\udefe \u00b7 max\ud835\udc4e\u2032 \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) \u2212 \ud835\udc44(\ud835\udc60, \ud835\udc4e)] \ud835\udc40 (\ud835\udc60, \ud835\udc4e) \u2190 (\ud835\udc60\u2032, \ud835\udc5f) for \ud835\udc5b = 1, . . . , \ud835\udc41 do Select \u02c6\ud835\udc60 and \u02c6\ud835\udc4e randomly (\ud835\udc60\u2032, \ud835\udc5f) \u2190 \ud835\udc40 (\u02c6\ud835\udc60, \u02c6\ud835\udc4e) \u22b2 Plan imagined state and reward from model \ud835\udc44(\u02c6\ud835\udc60, \u02c6\ud835\udc4e) \u2190 \ud835\udc44(\u02c6\ud835\udc60, \u02c6\ud835\udc4e) + \ud835\udefc \u00b7 [\ud835\udc5f + \ud835\udefe \u00b7 max\ud835\udc4e\u2032 \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) \u2212 \ud835\udc44(\u02c6\ud835\udc60, \u02c6\ud835\udc4e)] end for until \ud835\udc44 converges as behavior policy \ud835\udf0b(\ud835\udc60) to perform \ud835\udf16-greedy sampling of the environment. It then updates this policy with the reward, and an explicit model \ud835\udc40. When the model \ud835\udc40 has been updated, it is used \ud835\udc41 times by planning with random actions to update the Q-function. The pseudocode shows the learning steps (from environment \ud835\udc38) and \ud835\udc41 planning steps (from model \ud835\udc40). In both cases the Q-function state-action values are updated. The best action is then derived from the Q-values as usual. Thus, we see that the number of updates to the policy can be increased without more environment samples. By choosing the value for \ud835\udc41, we can tune how many of the policy updates will be environment samples, and how many will be model samples. In the larger problems that we will see later in this chapter, the ratio of environment-to-model samples is often set at, for example, 1 : 1000, greatly reducing sample complexity. The questions then become, of course: how good is the model, and: how far is the resulting policy from a model-free baseline? Hands On: Imagining Taxi Example It is time to illustrate how Dyna-Q works with an example. For that, we turn to one of our favorites, the Taxi world. Let us see what the efect of imagining with a model can be. Please refer to Fig. 5.3. We use our simple maze example, the Taxi maze, with zero imagination (\ud835\udc41 = 0), and with large imagination (\ud835\udc41 = 50). Let us assume that the reward at all states returned by the environment is 0, except for the goal, where the reward is +1. In states the usual actions are present (north, east, west, south), except at borders or walls. When \ud835\udc41 = 0 Dyna-Q performs exactly Q-learning, randomly sampling action rewards, building up the Q-function, and using the Q-values following the \ud835\udf16-greedy policy for action selection. The purpose of the Q-function is to act as a vessel of information to fnd the goal. How does our vessel get flled with information? Sampling starts of randomly, and the Q-values fll slowly, since the reward landscape 126 5 Model-Based Reinforcement Learning Fig. 5.3 Taxi world [395] is fat, or sparse: only the goal state returns +1, all other states return 0. In order to fll the Q-values with actionable information on where to fnd the goal, frst the algorithm must be lucky enough to choose a state next to the goal, including the appropriate action to reach the goal. Only then the frst useful reward information is found and the frst non-zero step towards fnding the goal can be entered into the Q-function. We conclude that, with \ud835\udc41 = 0, the Q-function is flled up slowly, due to sparse rewards. What happens when we turn on planning? When we set \ud835\udc41 to a high value, such as 50, we perform 50 planning steps for each learning step. As we can see in the algorithm, the model is built alongside the \ud835\udc44-function, from environment returns. As long as the \ud835\udc44-function is still fully zero, then planning with the model will also be useless. But as soon as one goal entry is entered into \ud835\udc44 and \ud835\udc40, then planning will start to shine: it will perform 50 planning samples on the M-model, probably fnding the goal information, and possibly building up an entire trajectory flling states in the \ud835\udc44-function with actions towards the goal. In a way, the model-based planning amplifes any useful reward information that the agent has learned from the environment, and plows it back quickly into the policy function. The policy is learned much quicker, with fewer environment samples. Reversible Planning and Irreversible Learning Model-free methods sample the environment and learn the policy function \ud835\udf0b(\ud835\udc60, \ud835\udc4e) directly, in one step. Model-based methods sample the environment to learn the policy indirectly, using a dynamics model {\ud835\udc47\ud835\udc4e, \ud835\udc45\ud835\udc4e} (as we see in Fig. 5.1 and 5.4, and in Alg. 5.1). 5.2 Learning and Planning Agents 127 Environment Dynamics Model Policy/Value learning acting planning Environment Policy/Value learning acting Fig. 5.4 Model-based (left) and Model-free (right). Learning changes the environment state irreversibly (single arrow); planning changes the agent state reversibly (undo, double arrow) Planning Learning Transition model in: Agent Environment Agent can Undo: Yes No State is: Reversible by agent Irreversible by agent Dynamics: Backtrack Forward only Data structure: Tree Path New state: In agent Sample from environment Reward: By agent Sample from environment Synonyms: Imagination, simulation Sampling, rollout Table 5.1 Diference between Planning and Learning It is useful to step back for a moment to consider the place of learning and planning algorithms in the reinforcement learning paradigm. Please refer to Table 5.1 for a summary of diferences between planning and learning. Planning with an internal transition model is reversible. When the agent uses its own transition model to perform local actions on a local state, then the actions can be undone, since the agent applied them to a copy in its own memory [528].4 Because of this local state memory, the agent can return to the old state, reversing the local state change caused by the local action that it has just performed. The agent can then try an alternative action (which it can also reverse). The agent can use tree-traversal methods to traverse the state space, backtracking to try other states. In contrast to planning, learning is done when the agent does not have access to its own transition function \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032). The agent can get reward information by sampling real actions in the environment. These actions are not played out inside the agent but executed in the actual environment; they are irreversible and can not be undone by the agent. Learning uses actions that irreversibly change the state of the environment. Learning does not permit backtracking; learning algorithms learn a policy by repeatedly sampling the environment. Note the similarity between learning and planning: learning samples rewards from the external environment, planning from the internal model; both use the samples to update the policy function \ud835\udf0b(\ud835\udc60, \ud835\udc4e). 4 In our dreams we can undo our actions, play what-if, and imagine alternative realities. 128 5 Model-Based Reinforcement Learning Four Types of Model-Based Methods In model-based reinforcement learning the challenge is to learn deep, high-dimensional transition models from limited data. Our methods should be able to account for model uncertainty, and plan over these models to achieve policy and value functions that perform as well or better than model-free methods. Let us look in more detail at specifc model-based reinforcement learning methods to see how this can be achieved. Over the years, many diferent approaches for high-accuracy high-dimensional model-based reinforcement learning have been devised. Following [601], we group the methods into four main approaches. We start with two approaches for learning the model, and then two approaches for planning, using the model. For each we will take a few representative papers from the literature that we describe in more depth. After we have done so, we will look at their performance in diferent environments. But let us start with the methods for learning a deep model frst. 5.2.1 Learning the Model In model-based approaches, the transition model is learned from sampling the environment. If this model is not accurate, then planning will not improve the value or policy function, and the method will perform worse than model-free methods. When the learning/planning ratio is set to 1/1000, as it is in some experiments, inaccuracy in the models will reveal itself quickly in a low accuracy policy function. Much research has focused on achieving high accuracy dynamics models for high-dimensional problems. Two methods to achieve better accuracy are uncertainty modeling and latent models. We will start with uncertainty modeling. 5.2.1.1 Modeling Uncertainty The variance of the transition model can be reduced by increasing the number of environment samples, but there are also other approaches that we will discuss. A popular approach for smaller problems is to use Gaussian processes, where the dynamics model is learned by giving an estimate of the function and of the uncertainty around the function with a covariance matrix on the entire dataset [93]. A Gaussian model can be learned from few data points, and the transition model can be used to plan the policy function successfully. An example of this approach is the PILCO system, which stands for Probabilistic Inference for Learning Control [188, 189]. This system was efective on Cartpole and Mountain car, but does not scale to larger problems. We can also sample from a trajectory distribution optimized for cost, and use that to train the policy, with a policy-based method [469]. Then we can optimize policies with the aid of locally-linear models and a stochastic trajectory optimizer. 5.2 Learning and Planning Agents 129 Algorithm 5.4 Planning with an Ensemble of Models [439] Initialize policy \ud835\udf0b\ud835\udf03 and the models \u02c6\ud835\udc5a\ud835\udf191, \u02c6\ud835\udc5a\ud835\udf191, . . . , \u02c6\ud835\udc5a\ud835\udf19\ud835\udc3e \u22b2 ensemble Initialize an empty dataset \ud835\udc37 repeat \ud835\udc37 \u2190 sample with \ud835\udf0b\ud835\udf03 from environment \ud835\udc38 Learn models \u02c6\ud835\udc5a\ud835\udf191, \u02c6\ud835\udc5a\ud835\udf191, . . . , \u02c6\ud835\udc5a\ud835\udf19\ud835\udc3e using \ud835\udc37 \u22b2 ensemble repeat \ud835\udc37\u2032 \u2190 sample with \ud835\udf0b\ud835\udf03 from { \u02c6\ud835\udc5a\ud835\udf19\ud835\udc56 }\ud835\udc3e \ud835\udc56=1 Update \ud835\udf0b\ud835\udf03 with TRPO using \ud835\udc37\u2032 \u22b2 planning Estimate performance of trajectories \u02c6\ud835\udf02\ud835\udf0f (\ud835\udf03, \ud835\udf19\ud835\udc56) for \ud835\udc56 = 1, . . . , \ud835\udc3e until performance converges until \ud835\udf0b\ud835\udf03 performs well in environment \ud835\udc38 This is the approach that is used in Guided policy search (GPS), which been shown to train complex policies with thousands of parameters, learning tasks in MuJoCo such as Swimming, Hopping and Walking. Another popular method to reduce variance in machine learning is the ensemble method. Ensemble methods combine multiple learning algorithms to achieve better predictive performance; for example, a random forest of decision trees often has better predictive performance than a single decision tree [93, 574]. In deep modelbased methods the ensemble methods are used to estimate the variance and account for it during planning. A number of researchers have reported good results with ensemble methods on larger problems [156, 376]. For example, Chua et al. use an ensemble of probabilistic neural network models [149] in their approach named Probabilistic ensembles with trajectory sampling (PETS). They report good results on high-dimensional simulated robotic tasks (such as Half-cheetah and Reacher). Kurutach et al. [439] combine an ensemble of models with TRPO, in ME-TRPO.5 In ME-TRPO an ensemble of deep neural networks is used to maintain model uncertainty, while TRPO is used to control the model parameters. In the planner, each imagined step is sampled from the ensemble predictions (see Alg. 5.4). Uncertainty modeling tries to improve the accuracy of high-dimensional models by probabilistic methods. A diferent approach, specifcally designed for highdimensional deep models, is the latent model approach, which we will discuss next. 5.2.1.2 Latent Models Latent models focus on dimensionality reduction of high-dimensional problems. The idea behind latent models is that in most high-dimensional environments some elements are less important, such as buildings in the background that never move and that have no relation with the reward. We can abstract these unimportant elements away from the model, reducing the efective dimensionality of the space. 5 A video is available at https://sites.google.com/view/me-trpo. The code is at https: //github.com/thanard/me-trpo. A blog post is at [362]. 130 5 Model-Based Reinforcement Learning Latent models do so by learning to represent the elements of the input and the reward. Since planning and learning are now possible in a lower-dimensional latent space, the sampling complexity of learning from the latent models improves. Even though latent model approaches are often complicated designs, many works have been published that show good results [391, 309, 308, 688, 710, 310, 304]. Latent models use multiple neural networks, as well as diferent learning and planning algorithms. To understand this approach, we will briefy discuss one such latent-model approach: the Value prediction network (VPN) by Oh et al. [564].6 VPN uses four diferentiable functions, that are trained to predict the value [290], Fig. 5.5 shows how the core functions. The core idea in VPN is not to learn directly in the actual observation space, but frst to transform the state respresentations to a smaller latent representation model, also known as abstract model. The other functions, such as value, reward, and next-state, then work on these smaller latent states, instead of on the more complex high-dimensional states. In this way, planning and learning occur in a space where states are encouraged only to contain the elements that infuence value changes. Latent space is lower-dimensional, and training and planning become more efcient. The four functions in VPN are: (1) an encoding function, (2) a reward function, (3) a value function, and (4) a transition function. All functions are parameterized with their own set of parameters. To distinghuish these latent-based functions from the conventional observation-based functions \ud835\udc45,\ud835\udc49,\ud835\udc47 they are denoted as \ud835\udc53 \ud835\udc52\ud835\udc5b\ud835\udc50 \ud835\udf03\ud835\udc52 , \ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51 \ud835\udf03\ud835\udc5f , \ud835\udc53 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 \ud835\udf03\ud835\udc63 , \ud835\udc53 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60 \ud835\udf03\ud835\udc61 . \u2022 The encoding function \ud835\udc53 \ud835\udc52\ud835\udc5b\ud835\udc50 \ud835\udf03\ud835\udc52 : \ud835\udc60\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \u2192 \ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 maps the observation \ud835\udc60\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 to the abstract state using neural network \ud835\udf03\ud835\udc52, such as a CNN for visual observations. This is the function that performs the dimensionality reduction. \u2022 The latent-reward function \ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51 \ud835\udf03\ud835\udc5f : (\ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61, \ud835\udc5c) \u2192 \ud835\udc5f, \ud835\udefe maps the latent state \ud835\udc60 and option \ud835\udc5c (a kind of action) to the reward and discount factor. If the option takes \ud835\udc58 primitive actions, the network should predict the discounted sum of the \ud835\udc58 immediate rewards as a scalar. (The role of options is explained in the paper [564].) The network also predicts option-discount factor \ud835\udefe for the number of steps taken by the option. \u2022 The latent-value function \ud835\udc53 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 \ud835\udf03\ud835\udc63 : \ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 \u2192 \ud835\udc49\ud835\udf03\ud835\udc63 (\ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61) maps the abstract state to its value using a separate neural network \ud835\udf03\ud835\udc63. This value is the value of the latent state, not of the actual observation state \ud835\udc49(\ud835\udc60\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59). \u2022 The latent-transition function \ud835\udc53 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60 \ud835\udf03\ud835\udc61 : (\ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61, \ud835\udc5c) \u2192 \ud835\udc60\u2032 \ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 maps the latent state to the next latent state, depending also on the option. Figure 5.5 shows how the core functions work together in the smaller, latent, space; with \ud835\udc65 the observed actual state, and \ud835\udc60 the encoded latent state [564]. The fgure shows a single rollout step, planning one step ahead. However, a model also allows looking further into the future, by performing multi-step rollouts. 6 See https://github.com/junhyukoh/value-prediction-network for the code. 5.2 Learning and Planning Agents 131 Fig. 5.5 Architecture of latent model [564] Algorithm 5.5 Multi-step planning [564] function Q-Plan(s, o, \ud835\udc51) \ud835\udc5f, \ud835\udefe, \ud835\udc49 (\ud835\udc60\u2032), \ud835\udc60\u2032 \u2192 \ud835\udc53 \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udf03 (\ud835\udc60, \ud835\udc5c) \u22b2 Perform the four latent functions if \ud835\udc51 = 1 then return \ud835\udc5f + \ud835\udefe\ud835\udc49 (\ud835\udc60\u2032) end if \ud835\udc34 \u2190 \ud835\udc4f-best options based on \ud835\udc5f + \ud835\udefe\ud835\udc49\ud835\udf03 (\ud835\udc60\u2032) \u22b2 See paper for other expansion strategies for \ud835\udc5c\u2032 \u2208 \ud835\udc34 do \ud835\udc5e\ud835\udc5c\u2032 \u2190 Q-Plan(\ud835\udc60\u2032, \ud835\udc5c\u2032, \ud835\udc51 \u2212 1) end for return \ud835\udc5f + \ud835\udefe[ 1 \ud835\udc51 \ud835\udc49 (\ud835\udc60\u2032) + \ud835\udc51\u22121 \ud835\udc51 max\ud835\udc5c\u2032\u2208\ud835\udc34 \ud835\udc5e\ud835\udc5c\u2032 ] end function Of course, this requires a highly accurate model, otherwise the accumulated inaccuracies diminish the accuracy of the far-into-the-future lookahead. Algorithm 5.5 shows the pseudocode for a \ud835\udc51-step planner for the value prediction network. The networks are trained with \ud835\udc5b-step Q-learning and TD search [709]. Trajectories are generated with an \ud835\udf16-greedy policy using the planning algorithm from Alg. 5.5. VPN achieved good results on Atari games such as Pacman and Seaquest, outperforming model-free DQN, and outperforming observation-based planning in stochastic domains. Another relevant approach is presented in a sequence of papers by Hafner et al. [308, 309, 310]. Their PlaNet and Dreamer approaches use latent models based on a Recurrent State Space Model (RSSM), that consists of a transition model, an observation model, a variational encoder and a reward model, to improve consistency between one-step and multi-step predictions in latent space [398, 121, 199]. The latent-model approach reduces the dimensionality of the observation space. Dimensionality reduction is related to unsupervised learning (Sect. 1.2.2), and autoencoders (Sect. B.2.6). The latent-model approach is also related to world models, a term used by Ha and Schmidhuber [303, 304]. World models are inspired by the manner in which humans are thought to construct a mental model of the world in which we live. Ha et al. implement world models using generative recurrent neural networks that generate states for simulation using a variational autoencoder [411, 412] and a recurrent network. Their approach learns a compressed spatial and temporal representation of the environment. By using features extracted from the 132 5 Model-Based Reinforcement Learning world model as inputs to the agent, a compact and simple policy can be trained to solve a task, and planning occurs in the compressed world. The term world model goes back to 1990, see Schmidhuber [670]. Latent models and world models achieve promising results and are, despite their complexity, an active area of research, see, for example [873]. In the next section we will further discuss the performance of latent models, but we will frst look at two methods for planning with deep transition models. 5.2.2 Planning with the Model We have discussed in some depth methods to improve the accuracy of models. We will now switch from how to create deep models, to how to use them. We will describe two planning approaches that are designed to be forgiving for models that contain inaccuracies. The planners try to reduce the impact of the inaccuracy of the model, for example, by planning ahead with a limited horizon, and by re-learning and re-planning at each step of the trajectory. We will start with planning with a limited horizon. 5.2.2.1 Trajectory Rollouts and Model-Predictive Control At each planning step, the local transition model\ud835\udc47\ud835\udc4e(\ud835\udc60) \u2192 \ud835\udc60\u2032 computes the new state, using the local reward to update the policy. Due to the inaccuracies of the internal model, planning algorithms that perform many steps will quickly accumulate model errors [296]. Full rollouts of long, inaccurate, trajectories are therefore problematic. We can reduce the impact of accumulated model errors by not planning too far ahead. For example, Gu et al. [296] perform experiments with locally linear models that roll out planning trajectories of length 5 to 10. This reportedly works well for MuJoCo tasks Gripper and Reacher. In another experiment, Feinberg et al. [238] allow imagination to a fxed lookahead depth, after which value estimates are split into a near-future model-based component and a distant future model-free component (Model-based value expansion, MVE). They experiment with horizons of 1, 2, and 10, and fnd that 10 generally performs best on typical MuJoCo tasks such as Swimmer, Walker, and Half-cheetah. The sample complexity in their experiments is better than model-free methods such as DDPG [705]. Similarly good results are reported by others [376, 393], with a model horizon that is much shorter than the task horizon. Model-Predictive Control Taking the idea of shorter trajectories for planning than for learning further, we arrive at decision-time planning [467], also known as Model-predictive control 5.2 Learning and Planning Agents 133 Algorithm 5.6 Neural Network Dynamics for Model-Based Deep Reinforcement Learning (based on [549]) Initialize the model \u02c6\ud835\udc5a\ud835\udf19 Initialize an empty dataset \ud835\udc37 for \ud835\udc56 = 1, . . . , \ud835\udc3c do \ud835\udc37 \u2190 \ud835\udc38\ud835\udc4e \u22b2 sample action from environment Train \u02c6\ud835\udc5a\ud835\udf19 (\ud835\udc60, \ud835\udc4e) on \ud835\udc37 minimizing the error by gradient descent for \ud835\udc61 = 1, . . . , \ud835\udc47 Horizon do \u22b2 planning \ud835\udc34\ud835\udc61 \u2190 \u02c6\ud835\udc5a\ud835\udf19 \u22b2 estimate optimal action sequence with fnite MPC horizon Execute frst action \ud835\udc4e\ud835\udc61 from sequence \ud835\udc34\ud835\udc61 \ud835\udc37 \u2190 (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) end for end for (MPC) [440, 262]. Model-predictive control is a well-known approach in process engineering, to control complex processes with frequent re-planning over a limited time horizon. Model-predictive control uses the fact that many real-world processes are approximately linear over a small operating range (even though they can be highly non-linear over a longer range). In MPC the model is optimized for a limited time into the future, and then it is re-learned after each environment step. In this way small errors do not get a chance to accumulate and infuence the outcome greatly. Related to MPC are other local re-planning methods. All try to reduce the impact of the use of an inaccurate model by not planning too far into the future and by updating the model frequently. Applications are found in the automotive industry and in aerospace, for example for terrain-following and obstacle-avoidance algorithms [394]. MPC has been used in various deep model learning approaches. Both Finn et al. and Ebert et al. [244, 218] use a form of MPC in the planning for their Visual foresight robotic manipulation system. The MPC part uses a model that generates the corresponding sequence of future frames based on an image to select the leastcost sequence of actions. This approach is able to perform multi-object manipulation, pushing, picking and placing, and cloth-folding tasks (which adds the difculty of material that changes shape as it is being manipulated). Another approach is to use ensemble models for learning the transition model, with MPC for planning. PETS [149] uses probabilistic ensembles [448] for learning, based on cross-entropy-methods (CEM) [183, 99]. In MPC-fashion only the frst action from the CEM-optimized sequence is used, re-planning at every environmentstep. Many model-based approaches combine MPC and the ensemble method, as we will also see in the overview in Table 5.2 at the end of the next section. Algorithm 5.6 shows in pseudocode an example of Model-predictive control (based on [549], only the model-based part is shown).7 MPC is a simple and efective planning method that is well-suited for use with inaccurate models, by restricting the planning horizon and by re-planning. It has also been used with success in combination with latent models [309, 391]. 7 The code is at https://github.com/anagabandi/nn_dynamics. 134 5 Model-Based Reinforcement Learning It is now time to look at the fnal method, which is a very diferent approach to planning. 5.2.2.2 End-to-end Learning and Planning-by-Network Up until now, the learning of the dynamics model and its use are performed by separate algorithms. In the previous subsection diferentiable transition models were learned through backpropagation and then the models were used by a conventional hand-crafted procedural planning algorithm, such as depth-limited search, with hand-coded selection and backup rules. A trend in machine learning is to replace all hand-crafted algorithms by diferentiable approaches, that are trained by example, end-to-end. These diferentiable approaches often are more general and perform better than their hand-crafted versions.8 We could ask the question if it would be possible to make the planning phase diferentiable as well? Or, to see if the planning rollouts can be implemented in a single computational model, the neural network? At frst sight, it may seem strange to think of a neural network as something that can perform planning and backtracking, since we often think of a neural network as a state-less mathematical function. Neural networks normally perform transformation and flter activities to achieve selection or classifcation. Planning consists of action selection and state unrolling. Note, however, that recurrent neural networks and LSTMs contain implicit state, making them a candidate to be used for planning (see Sect. B.2.5). Perhaps it is not so strange to try to implement planning in a neural network. Let us have a look at attempts to perform planning with a neural network. Tamar et al. [748] introduced Value Iteration Networks (VIN), convolutional networks for planning in Grid worlds. A VIN is a diferentiable multi-layer convolutional network that can execute the steps of a simple planning algorithm [562]. The core idea it that in a Grid world, value iteration can be implemented by a multi-layer convolutional network: each layer does a step of lookahead (refer back to Listing 2.1 for value iteration). The value iterations are rolled-out in the network layers \ud835\udc46 with \ud835\udc34 channels, and the CNN architecture is shaped specifcally for each problem task. Through backpropagation the model learns the value iteration parameters including the transition function. The aim is to learn a general model, that can navigate in unseen environments. Let us look in more detail at the value iteration algorithm. It is a simple algorithm that consists of a doubly nested loop over states and actions, calculating the sum of rewards \ufffd \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032)(\ud835\udc45\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe\ud835\udc49 [\ud835\udc60\u2032]) and a subsequent maximization opera8 Note that here we use the term end-to-end to indicate the use of diferentiable methods for the learning and use of a deep dynamics model\u2014to replace hand-crafted planning algorithms to use the learned model. Elsewhere, in supervised learning, the term end-to-end is used diferently, to describe learning both features and their use from raw pixels for classifcation\u2014to replace hand-crafted feature recognizers to pre-process the raw pixels and use in a hand-crafted machine learning algorithm. 5.2 Learning and Planning Agents 135 tion \ud835\udc49 [\ud835\udc60] = max\ud835\udc4e(\ud835\udc44[\ud835\udc60, \ud835\udc4e]). This double loop is iterated to convergence. The insight is that each iteration can be implemented by passing the previous value function \ud835\udc49\ud835\udc5b and reward function \ud835\udc45 through a convolution layer and max-pooling layer. In this way, each channel in the convolution layer corresponds to the Q-function for a specifc action\u2014the innermost loop\u2014and the convolution kernel weights correspond to the transitions. Thus by recurrently applying a convolution layer \ud835\udc3e times, \ud835\udc3e iterations of value iteration are performed. The value iteration module is simply a neural network that has the capability of approximating a value iteration computation. Representing value iteration in this form makes learning the MDP parameters and functions natural\u2014by backpropagating through the network, as in a standard CNN. In this way, the classic value iteration algorithm can be approximated by a neural network. Why would we want to have a fully diferentiable algorithm that can only give an approximation, if we have a perfectly good classic procedural implementation that can calculate the value function \ud835\udc49 exactly? The reason is generalization. The exact algorithm only works for known transition probabilities. The neural network can learn \ud835\udc47(\u00b7) when it is not given, from the environment, and it learns the reward and value functions at the same time. By learning all functions all at once in an end-to-end fashion, the dynamics and value functions might be better integrated than when a separately hand-crafted planning algorithm uses the results of a learned dynamics model. Indeed, reported results do indicate good generalization to unseen problem instances [748]. The idea of planning by gradient descent has existed for some time\u2014actually, the idea of learning all functions by example has existed for some time\u2014several authors explored learning approximations of dynamics in neural networks [403, 671, 369]. The VINs can be used for discrete and continuous path planning, and have been tried in Grid world problems and natural language tasks. Later work has extended the approach to other applications of more irregular shape, by adding abstraction networks [668, 724, 710]. The addition of latent models increases the power and versatility of end-to-end learning of planning and transitions even further. Let us look briefy in more detail at one such extension of VIN, to illustrate how latent models and planning go together. TreeQN by Farquhar et al. [236] is a fully diferentiable model learner and planner, using observation abstraction so that the approach works on applications that are less regular than mazes. TreeQN consists of fve diferentiable functions, four of which we have seen in the previous section in Value Prediction Networks [564], Fig. 5.5 on page 131. \u2022 The encoding function consists of a series of convolutional layers that embed the actual state in a lower dimensional state \ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 \u2190 \ud835\udc53 \ud835\udc52\ud835\udc5b\ud835\udc50 \ud835\udf03\ud835\udc52 (\ud835\udc60\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59) \u2022 The transition function uses a fully connected layer per action to calculate the next-state representation \ud835\udc60\u2032 \ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 \u2190 \ud835\udc53 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60 \ud835\udf03\ud835\udc61 (\ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61, \ud835\udc4e\ud835\udc56)\ud835\udc3c \ud835\udc56=0. \u2022 The reward function predicts the immediate reward for every action \ud835\udc4e\ud835\udc56 \u2208 \ud835\udc34 in state \ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 using a ReLU layer \ud835\udc5f \u2190 \ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51 \ud835\udf03\ud835\udc5f (\ud835\udc60\u2032 \ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61). \u2022 The value function of a state is estimated with a vector of weights \ud835\udc49(\ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61) \u2190 \ud835\udc64\u22a4\ud835\udc60\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61 + \ud835\udc4f. 136 5 Model-Based Reinforcement Learning \u2022 The backup function applies a softmax function9 recursively to calculate the tree backup value \ud835\udc4f(\ud835\udc65) \u2190 \ufffd\ud835\udc3c \ud835\udc56=0 \ud835\udc65\ud835\udc56 softmax(\ud835\udc65)\ud835\udc56. These functions together can learn a model, and can also execute \ud835\udc5b-step Q-learning, to use the model to update a policy. Further details can be found in [236] and the GitHub code.10 TreeQN has been applied on games such as box-pushing and some Atari games, and outperformed model-free DQN. A limitation of VIN is that the tight connection between problem domain, iteration algorithm, and network architecture limited the applicability to other problems. Another system that addresses this limitation os Predictron. Like TreeQN, the Predictron [710] introduces an abstract model to reduce this limitation. As in VPN, the latent model consists of four diferentiable components: a representation model, a next-state model, a reward model, and a discount model. The goal of the abstract model in Predictron is to facilitate value prediction (not state prediction) or prediction of pseudo-reward functions that can encode special events, such as staying alive or reaching the next room. The planning part rolls forward its internal model \ud835\udc58 steps. Unlike VPN, Predictron uses joint parameters. The Predictron has been applied to procedurally generated mazes and a simulated pool domain. In both cases it out-performed model-free algorithms. End-to-end model-based learning-and-planning is an active area of research. Challenges include understanding the relation between planning and learning [18, 289], achieving performance that is competitive with classical planning algorithms and with model-free methods, and generalizing the class of applications. In Sect. 5.3 more methods will be shown. Conclusion In the previous sections we have discussed two methods to reduce the inaccuracy of the model, and two methods to reduce the impact of the use of an inaccurate model. We have seen a range of diferent approaches to model-based algorithms. Many of the algorithms were developed recently. Deep model-based reinforcement learning is an active area of research. Ensembles and MPC have improved the performance of model-based reinforcement learning. The goal of latent or world models is to learn the essence of the domain, reducing the dimensionality, and for end-to-end, to also include the planning part in the learning. Their goal is generalization in a fundamental sense. Model-free learns a policy of which action to take in each state. Model-based methods learn the transition model, from state (via action) to state. Model-free teaches you how to best respond to actions in your world, model-based helps you to understand your world. By learning the transition model (and possibly even how to best plan with it) it is hoped that new generalization methods can be learned. 9 The softmax function normalizes an input vector of real numbers to a probability distribution [0, 1]; \ud835\udc5d\ud835\udf03 (\ud835\udc66|\ud835\udc65) = softmax( \ud835\udc53\ud835\udf03 (\ud835\udc65)) = \ud835\udc52 \ud835\udc53\ud835\udf03 (\ud835\udc65) \ufffd \ud835\udc58 \ud835\udc52 \ud835\udc53\ud835\udf03,\ud835\udc58 (\ud835\udc65) 10 See https://github.com/oxwhirl/treeqn for the code of TreeQN. 5.3 High-Dimensional Environments 137 The goal of model-based methods is to get to know the environment so intimitely that the sample complexity can be reduced while staying close to the solution quality of model-free methods. A second goal is that the generalization power of the methods improves so much, that new classes of problems can be solved. The literature is rich and contains many experiments of these approaches on diferent environments. Let us now look at the environments to see if we have succeeded. 5.3 High-Dimensional Environments We have now looked in some detail at approaches for deep model-based reinforcement learning. Let us now change our perspective from the agent to the environment, and look at the kinds of environments that can be solved with these approaches. 5.3.1 Overview of Model-Based Experiments The main goal of model-based reinforcement learning is to learn the transition model accurately\u2014not just the policy function that fnds the best action, but the function that fnds the next state. By learning the full essence of the environment a substantial reduction of sample complexity can be achieved. Also, the hope is that the model allows us to solve new classes of problems. In this section we will try to answer the question if these approaches have succeeded. The answer to this question can be measured in training time and in run-time performance. For performance, most benchmark domains provide easily measurable quantities, such as the score in an Atari game. For model-based approaches, the scores achieved by state-of-the-art model-free algorithms such as DQN, DDPG, PPO, SAC and A3C are a useful baseline. For training time, the reduction in sample complexity is an obvious choice. However, many model-based approaches use a fxed hyperparameter to determine the relation between external environment samples and internal model samples (such as 1 : 1000). Then the number of time steps needed for high performance to be reached becomes an important measure, and this is indeed published by most authors. For model-free methods, we often see time steps in the millions per training run, and sometimes even billions. With so many time steps it becomes quite important how much processing each time step takes. For model-based methods, individual time steps may take longer than for model-free, since more processing for learning and planning has to be performed. In the end, wall-clock time is important, and this is also often published. There are two additional questions. First we are interested in knowing whether a model-based approach allows new types of problems to be solved, that could not be solved by model-free methods. Second is the question of brittleness. In many experiments the numerical results are quite sensitive to diferent settings of hyperparameters (including the random seeds). This is the case in many model-free 138 5 Model-Based Reinforcement Learning Name Learning Planning Environment Ref PILCO Uncertainty Trajectory Pendulum [188] iLQG Uncertainty MPC Small [756] GPS Uncertainty Trajectory Small [468] SVG Uncertainty Trajectory Small [326] VIN CNN e2e Mazes [748] VProp CNN e2e Mazes [551] Planning CNN/LSTM e2e Mazes [298] TreeQN Latent e2e Mazes [236] I2A Latent e2e Mazes [615] Predictron Latent e2e Mazes [710] World Model Latent e2e Car Racing [304] Local Model Uncertainty Trajectory MuJoCo [296] Visual Foresight Video Prediction MPC Manipulation [244] PETS Ensemble MPC MuJoCo [149] MVE Ensemble Trajectory MuJoCo [238] Meta Policy Ensemble Trajectory MuJoCo [156] Policy Optim Ensemble Trajectory MuJoCo [376] PlaNet Latent MPC MuJoCo [309] Dreamer Latent Trajectory MuJoCo [308] Plan2Explore Latent Trajectory MuJoCo [688] L3P Latent Trajectory MuJoCo [873] Video-prediction Latent Trajectory Atari [563] VPN Latent Trajectory Atari [564] SimPLe Latent Trajectory Atari [391] Dreamer-v2 Latent Trajectory Atari [310] MuZero Latent e2e/MCTS Atari/Go [679] Table 5.2 Model-Based Reinforcement Learning Approaches [601] and model-based results [328]. However, when the transition model is accurate, the variance may diminish, and some model-based approaches might be more robust. Table 5.2 lists 26 experiments with model-based methods [601]. In addition to the name, the table provides an indication of the type of model learning that the agent uses, of the type of planning, and of the application environment in which it was used. The categories in the table are described in the previous section, where e2e means end-to-end. In the table, the approaches are grouped by environment. At the top are smaller applications such as mazes and navigation tasks. In the middle are larger MuJoCo tasks. At the bottom are high-dimensional Atari tasks. Let us look in more depth at the three groups of environments: small navigation, robotics, and Atari games. 5.3 High-Dimensional Environments 139 5.3.2 Small Navigation Tasks We see that a few approaches use smaller 2D Grid world navigation tasks such as mazes, or block puzzles, such as Sokoban, and Pacman. Grid world tasks are some of the oldest problems in reinforcement learning, and they are used frequently to test out new ideas. Tabular imagination approaches such as Dyna, and some latent model and end-to-end learning and planning, have been evaluated with these environments. They typically achieve good results, since the problems are of moderate complexity. Grid world navigation problems are quintessential sequential decision problems. Navigation problems are typically low-dimensional, and no visual recognition is involved; transition functions are easy to learn. Navigation tasks are also used for latent model and end-to-end learning. Three latent model approaches in Table 5.2 use navigation problems. I2A deals with model imperfections by introducing a latent model, based on Chiappa et al. and Buesing et al. [144, 121]. I2A is applied to Sokoban and Mini-Pacman by [615, 121]. Performance compares favorably to model-free learning and to planning algorithms (MCTS). Value iteration networks introduced the concept of end-to-end diferentiable learning and planning [748, 562], after [403, 671, 369]. Through backpropagation the model learns to perform value iteration. The aim to learn a general model that can navigate in unseen environments was achieved, although diferent extensions were needed for more complex environments. 5.3.3 Robotic Applications Next, we look at papers that use MuJoCo to model continuous robotic problems. Robotic problems are high-dimensional problems with continuous action spaces. MuJoCo is used by most experiments in this category to simulate the physical behavior of robotic movement and the environment. Uncertainty modeling with ensembles and MPC re-planning try to reduce or contain inaccuracies. The combination of ensemble methods with MPC is well suited for robotic problems, as we have seen in individual approaches such as PILCO and PETS. Robotic applications are more complex than Grid worlds; model-free methods can take many time steps to fnd good policies. It is important to know if modelbased methods succeed in reducing sample complexity in these problems. When we have a closer look at how well uncertainty modeling and MPC succeed at achieving our frst goal, we fnd a mixed picture. A benchmark study by Wang et al. [828] looked into the performance of ensemble methods and Model-predictive control on MuJoCo tasks. It fnds that these methods mostly fnd good policies, and do so in signifcantly fewer time steps than modelfree methods, typcially in 200k time steps versus 1 million for model-free. So, it would appear that the lower sample complexity is achieved. However, they also 140 5 Model-Based Reinforcement Learning note that per time step, the more complex model-based methods perform more processing than the simpler model-free methods. Although the sample complexity may be lower, the wall-clock time is not, and model-free methods such as PPO ans SAC are still much faster for many problems. Furthermore, the score that the policy achieves varies greatly for diferent problems, and is sensitive to diferent hyperparameter values. Another fnding is that in some experiments with a large number of time steps, the performance of model-based methods plateaus well below model-free performance, and the performance of the model-based methods themselves difers substantially. There is a need for further research in deep model-based methods, especially into robustness of results. More benchmarking studies are needed that compare diferent methods. 5.3.4 Atari Games Applications Some experiments in Table 5.2 use the Arcade learning environment (ALE). ALE features high-dimensional inputs, and provides one of the most challenging environments of the table. Especially latent models choose Atari games to showcase their performance, and some do indeed achieve impressive results, in that they are able to solve new problems, such as playing all 57 Atari games well (Dreamer-v2) [310] and learning the rules of Atari and chess (MuZero) [679]. Hafner et al. published the papers Dream to control: learning behaviors by latent imagination, and Dreamer v2 [308, 310]. Their work extends the work on VPN and PlaNet with more advanced latent models and reinforcement learning methods [564, 309]. Dreamer uses an actor-critic approach to learn behaviors that consider rewards beyond the horizon. Values are backpropagated through the value model, similar to DDPG [480] and Soft actor critic [306]. An important advantage of model-based reinforcement learning is that it can generalize to unseen environments with similar dynamics [688]. The Dreamer experiments showed that latent models are indeed more robust to unseen environments than model-free methods. Dreamer is tested with applications from the DeepMind control suite (Sect. 4.3.2). Value prediction networks are another latent approach. They outperform modelfree DQN on mazes and Atari games such as Seaquest, QBert, Krull, and Crazy Climber. Taking the development of end-to-end learner/planners such as VPN and Predictron further is the work on MuZero [679, 290, 359]. In MuZero a new architecture is used to learn the transition functions for a range of diferent games, from Atari to the board games chess, shogi and Go. MuZero learns the transition model for all games from interaction with the environment.11 The MuZero model includes diferent modules: a representation, dynamics, and prediction function. Like 11 This is a somewhat unusual usage of board games. Most researchers use board games because the transition function is given (see next chapter). MuZero instead does not know the rules of chess, but starts from scratch learning the rules from interaction with the environment. 5.3 High-Dimensional Environments 141 AlphaZero, MuZero uses a refned version of MCTS for planning (see Sect. 6.2.1.2 in the next chapter). This MCTS planner is used in a self-play training loop for policy improvement. MuZero\u2019s achievements are impressive: it is able to learn the rules of Atari games as well as board games, learning to play the games from scratch, in conjunction with learning the rules of the games. The MuZero achievements have created follow up work to provide more insight into the relationship between actual and latent representations, and to reduce the computational demands [290, 186, 39, 334, 18, 680, 289, 860]. Latent models reduce observational dimensionality to a smaller model to perform planning in latent space. End-to-end learning and planning is able to learn new problems\u2014the second of our two goals: it is able to learn to generalize navigation tasks, and to learn the rules of chess and Atari. These are new problems, that are out of reach for model-free methods (although the sample complexity of MuZero is quite large). Conclusion In deep model-based reinforcement learning benchmarks drive progress. We have seen good results with ensembles and local re-planning in continuous problems, and with latent models in discrete problems. In some applications, both the frst goal, of better sample complexity, and the other goals, of learning new applications and reducing brittleness, are achieved. The experiments used many diferent environments within the ALE and MuJoCo suites, from hard to harder. In the next two chapters we will study multi-agent problems, where we encounter a new set of benchmarks, with a state space of many combinations, including hidden information and simultaneous actions. These provide even more complex challenges for deep reinforcement learning methods. 5.3.5 Hands On: PlaNet Example Before we go to the next chaper, let us take a closer look at how one of these methods achieves efcient learning of a complex high-dimensional task. We will look at PlaNet, a well-documented project by Hafner et al. [309]. Code is available,12 scripts are available, videos are available, and a blog is available13 inviting us to take the experiments further. The name of the work is Learning latent dynamics from pixels, which describes what the algorithm does: use high dimensional visual input, convert it to latent space, and plan in latent space to learn robot locomotion dynamics. 12 https://github.com/google-research/planet 13 https://planetrl.github.io 142 5 Model-Based Reinforcement Learning Fig. 5.6 Locomotion tasks of PlaNet [309] PlaNet solves continuous control tasks that include contact dynamics, partial observability, and sparse rewards. The applications used in the PlaNet experiments are: (a) Cartpole (b) Reacher (c) Cheetah (d) Finger (e) Cup and (f) Walker (see Fig. 5.6). The Cartpole task is a swing-up task, with a fxed viewpoint. The cart can be out of sight, requiring the agent to remember information from previous frames. The Finger spin task requires predicting the location of two separate objects and their interactions. The Cheetah task involves learning to run. It includes contacts of the feet with the ground that requires a model to predict multiple futures. The Cup task must catch a ball in a cup. It provides a sparse reward signal once the ball is caught, requiring accurate predictions far into the future. The Walker task involves a simulated robot that begins lying on the ground, and must learn to stand up and then walk. PlaNet performs well on these tasks. On DeepMind control tasks it achieves higher accuracy than an A3C or an D4PG agent. It reportedly does so using 5000% fewer interactions with the environment on average. It is instructive to experiment with PlaNet. The code can be found on GitHub.14 Scripts are available to run the experiments with simple one line commands: python3 -m planet.scripts.train --logdir /path/to/logdir --params \u2019{tasks: [cheetah_run]}\u2019 As usual, this does require having the right versions of the right libraries installed, which may be a challenge and may require some creativity on your part. The required versions are listed on the GitHub page. The blog also contains videos and pictures of what to expect, including comparisons to model-free baselines from the DeepMind Control Suite (A3C, D4PG). The experiments show the viability of the idea to use rewards and values to compress actual states into lower dimensional latent states, and then plan with these latent states. Value-based compression reduces details in the high-dimensional actual states as noise that is not relevant to improve the value function [290]. To help understand how the actual state map to the latent states, see, for example [186, 472, 401]. 14 https://github.com/google-research/planet 5.3 High-Dimensional Environments 143 Summary and Further Reading This has been a diverse chapter. We will summarize the chapter, and provide references for further reading. Summary Model-free methods sample the environment using the rewards to learn the policy function, providing actions for all states for an environment. Model-based methods use the rewards to learn the transition function, and then use planning methods to sample the policy from this internal model. Metaphorically speaking: modelfree learns how to act in the environment, model-based learns how to be the environment. The learned transition model acts as a multiplier on the amount of information that is used from each environment sample. A consequence is that model-based methods have a lower sample complexity, although, when the agent\u2019s transition model does not perfectly refect the environment\u2019s transition function, the performance of the policy may be worse than a model-free policy (since that always uses the environment to sample from). Another, and perhaps more important aspect of the model-based approach, is generalization. Model-based reinforcement learning builds a dynamics model of the domain. This model can be used multiple times, for new problem instances, but also for related problem classes. By learning the transition and reward model, model-based reinforcement learning may be better at capturing the essence of a domain than model-free methods, and thus be able to generalize to variations of the problem. Imagination showed how to learn a model and use it to fll in extra samples based on the model (not the environment). For problems where tabular methods work, imagination can be many times more efcient than model-free methods. When the agent has access to the transition model, it can apply reversible planning algorithms, in additon to one-way learning with samples. There is a large literature on backtracking and tree-traversal algorithms. Using a look-ahead of more than one step can increase the quality of the reward even more. When the problem size increases, or when we perform deep multi-step look-ahead, the accuracy of the model becomes critical. For high-dimensional problems high capacity networks are used that require many samples to prevent overftting. Thus a trade-of exists, to keep sample complexity low. Methods such as PETS aim to take the uncertainty of the model into account in order to increase modeling accuracy. Model-predictive control methods re-plan at each environment step to prevent over-reliance on the accuracy of the model. Classical tabular approaches and Gaussian Process approaches have been quite succesful in achieving low sample complexity for small problems [743, 190, 417]. Latent models observe that in many high-dimensional problems the factors that infuence changes in the value function are often lower-dimensional. For example, 144 5 Model-Based Reinforcement Learning the background scenery in an image may be irrelevant for the quality of play in a game, and has no efect on the value. Latent models use an encoder to translate the high-dimensional actual state space into a lower-dimensional latent state space. Subsequent planning and value functions work on the (much smaller) latent space. Finally, we considered end-to-end model-based algorithms. These fully diferentiable algorithms not only learn the dynamics model, but also learn the planning algorithm that uses the model. The work on Value iteration networks [748] inspired recent work on end-to-end learning, where both the transition model and the planning algorithm are learned, end-to-end. Combined with latent models (or World models [304]) impressive results were achieved [710], and the model and planning accuracy was improved to the extent that tabula rasa self-learning of game-rules was achieved, in Muzero [679] for both chess, shogi, Go, and Atari games. Further Reading Model-based reinforcement learning promises more sample efcient learning. The feld has a long history. For exact tabular methods Sutton\u2019s Dyna-Q is a classical approach that illustrates the basic concept of model-based learning [741, 742]. There is an extensive literature on the approaches that were discussed in this chapter. For uncertainty modeling see [188, 189, 756], and for ensembles [574, 149, 156, 376, 439, 391, 469, 468, 326, 244]. For Model-predictive control see [549, 262, 505, 426, 296, 238, 38]. Latent models is an active feld of research. Two of the earlier works are [563, 564], although the ideas go back to World models [304, 403, 671, 369]. Later, a sequence of PlaNet and Dreamer papers was infuential [309, 308, 688, 310, 874]. The literature on end-to-end learning and planning is also extensive, starting with VIN [748], see [22, 704, 706, 710, 551, 298, 236, 679, 239]. As applications became more challenging, notably in robotics, other methods were developed, mostly based on uncertainty, see for surveys [190, 417]. Later, as high-dimensional problems became prevalent, latent and end-to-end methods were developed. The basis for the section on environments is an overview of recent modelbased approaches [601]. Another survey is [529], a comprehensive benchmark study is [828]. Exercises Let us go to the Exercises. 5.3 High-Dimensional Environments 145 Questions Below are frst some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. 1. What is the advantage of model-based over model-free methods? 2. Why may the sample complexity of model-based methods sufer in highdimensional problems? 3. Which functions are part of the dynamics model? 4. Mention four deep model-based approaches. 5. Do model-based methods achieve better sample complexity than model-free? 6. Do model-based methods achieve better performance than model-free? 7. In Dyna-Q the policy is updated by two mechanisms: learning by sampling the environment and what other mechanism? 8. Why is the variance of ensemble methods lower than of the individual machine learning approaches that are used in the ensemble? 9. What does model-predictive control do and why is this approach suited for models with lower accuracy? 10. What is the advantage of planning with latent models over planning with actual models? 11. How are latent models trained? 12. Mention four typical modules that constitute the latent model. 13. What is the advantage of end-to-end planning and learning? 14. Mention two end-to-end planning and learning methods. Exercises It is now time to introduce a few programming exercises. The main purpose of the exercises is to become more familiar with the methods that we have covered in this chapter. By playing around with the algorithms and trying out diferent hyperparameter settings you will develop some intuition for the efect on performance and run time of the diferent methods. The experiments may become computationally expensive. You may want to consider running them in the cloud, with Google Colab, Amazon AWS, or Microsoft Azure. They may have student discounts, and they will have the latest GPUs or TPUs for use with TensorFlow or PyTorch. 1. Dyna Implement tabular Dyna-Q for the Gym Taxi environment. Vary the amount of planning \ud835\udc41 and see how performance is infuenced. 2. Keras Make a function approximation version of Dyna-Q and Taxi, with Keras. Vary the capacity of the network and the amount of planning. Compare against a pure model-free version, and note the diference in performance for diferent tasks and in computational demands. 146 5 Model-Based Reinforcement Learning 3. Planning In Dyna-Q, planning has so far been with single step model samples. Implement a simple depth-limited multi-step look-ahead planner, and see how performance is infuenced for the diferent look-ahead depths. 4. MPC Read the paper by Nagabandi et al. [549] and download the code.15 Acquire the right versions of the libraries, and run the code with the supplied scripts, just for the MB (model-based) versions. Note that plotting is also supported by the scripts. Run with diferent MPC horizons. Run with diferent ensemble sizes. What are the efects on performance and run time for the diferent applications? 5. PlaNet Go to the PlaNet blog and read it (see previous section).16 Go to the PlaNet GitHub site and download and install the code.17 Install the DeepMind control suite,18 and all necessary versions of the support libraries. Run Reacher and Walker in PlaNet, and compare against the model-free methods D4PG and A3C. Vary the size of the encoding network and note the efect on performance and run time. Now turn of the encoder, and run with planning on actual states (you may have to change network sizes to achieve this). Vary the capacity of the latent model, and of the value and reward functions. Also vary the amount of planning, and note its efect. 6. End-to-end As you have seen, these experiments are computationally expensive. We will now turn to end-to-end planning and learning (VIN and MuZero). This exercise is also computationally expensive. Use small applications, such as small mazes, and Cartpole. Find and download a MuZero implementation from GitHub and explore using the experience that you have gained from the previous exercises. Focus on gaining insight into the shape of the latent space. Try MuZero-General [215],19 or a MuZero visualization [186] to get insight into latent space.20 (This is a challenging exercise, suitable for a term project or thesis.) 15 https://github.com/anagabandi/nn_dynamics 16 https://planetrl.github.io 17 https://github.com/google-research/planet 18 https://github.com/deepmind/dm_control 19 https://github.com/werner-duvaud/muzero-general 20 https://github.com/kaesve/muzero Chapter 6 Two-Agent Self-Play Previous chapters were concerned with how a single agent can learn optimal behavior for its environment. This chapter is diferent. We turn to problems where two agents operate whose behavior will both be modeled (and, in the next chapter, more than two). Two-agent problems are interesting for two reasons. First, the world around us is full of active entities that interact, and modeling two agents and their interaction is a step closer to understanding the real world than modeling a single agent. Second, in two-agent problems exceptional results were achieved\u2014reinforcement learning agents teaching themselves to become stronger than human world champions\u2014and by studying these methods we may fnd a way to achieve similar results in other problems. The kind of interaction that we model in this chapter is zero-sum: my win is your loss and vice versa. These two-agent zero-sum dynamics are fundamentally diferent from single-agent dynamics. In single agent problems the environment lets you probe it, lets you learn how it works, and lets you fnd good actions. Although the environment may not be your friend, it is also not working against you. In two-agent zero-sum problems the environment does try to win from you, it actively changes its replies to minimize your reward, based on what it learns from your behavior. When learning our optimal policy we should take all possible counter-actions into account. A popular way to do so is to implement the environment\u2019s actions with self-play: we replace the environment by a copy of ourselves. In this way we let ourselves play against an opponent that has all the knowledge that we currently have, and agents learn from eachother. We start with a short review of two-agent problems, after which we dive into selflearning. We look at the situation when both agents know the transition function perfectly, so that model accuracy is no longer a problem. This is the case, for example, in games such as chess and Go, where the rules of the game determine how we can go from one state to another. In self-learning the environment is used to generate training examples for the agent to train a better policy, after which the better agent policy is used in this 147 148 6 Two-Agent Self-Play environment to train the agent, and again, and again, creating a virtuous cycle of self-learning and mutual improvement. It is possible for an agent to teach itself to play a game without any prior knowledge at all, so-called tabula rasa learning, learning from a blank slate. The self-play systems that we describe in this chapter use model-based methods, and combine planning and learning approaches. There is a planning algorithm that we have mentioned a few times, but have not yet explained in detail. In this chapter we will discuss Monte Carlo Tree Search, or MCTS, a highly popular planning algorithm. MCTS can be used in single agent and in two-agent situations, and is the core of many successful applications, including MuZero and the self-learning AlphaZero series of programs. We will explain how self-learning and self-play work in AlphaGo Zero, and why they work so well. We will then discuss the concept of curriculum learning, which is behind the success of self-learning. The chapter is concluded with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Self-play \u2022 Curriculum learning Core Problem \u2022 Use a given transition model for self-play, in order to become stronger than the current best players Core Algorithms \u2022 Minimax (Listing 6.2) \u2022 Monte Carlo Tree Search (Listing 6.3) \u2022 AlphaZero tabula rasa learning (Listing 6.1) Self-Play in Games We have seen in Chap. 5 that when the agent has a transition model of the environment, it can achieve greater performance, especially when the model has high accuracy. What if the accuracy of our model were perfect, if the agent\u2019s transition 6 Two-Agent Self-Play 149 Fig. 6.1 Backgammon and Tesauro function is the same as the environment\u2019s, how far would that bring us? And what if we could improve our environment as part of our learning process, can we then transcend our teacher, can the sorcerer\u2019s apprentice outsmart the wizard? To set the scene for this chapter, let us describe the frst game where this has happened: backgammon. Learning to Play Backgammon In Sect. 3.2.3 we briefy discussed research into backgammon. Already in the early 1990s, the program TD-Gammon achieved stable reinforcement learning with a shallow network. This work was started at the end of the 1980s by Gerald Tesauro, a researcher at IBM laboratories. Tesauro was faced with the problem of getting a program to learn beyond the capabilities of any existing entity. (In Fig. 6.1 we see Tesauro in front of his program; image by IBM Watson Media.) In the 1980s computing was diferent. Computers were slow, datasets were small, and neural networks were shallow. Against this background, the success of Tesauro is quite remarkable. His programs were based on neural networks that learned good patterns of play. His frst program, Neurogammon, was trained using supervised learning, based on games of human experts. In supervised learning the model cannot become stronger than the human games it is trained on. Neurogammon achieved an intermediate level of play [762]. His second program, TD-Gammon, was based on reinforcement learning, using temporal diference learning and self-play. Combined with handcrafted heuristics and some planning, in 1992 it played at human championship level, becoming the frst computer program to do so in a game of skill [765]. TD-Gammon is named after temporal diference learning because it updates its neural net after each move, reducing the diference between the evaluation of previous and current positions. The neural network used a single hidden layer with up to 80 units. TD-Gammon initially learned from a state of zero knowledge, tabula rasa. Tesauro describes TD-Gammon\u2019s self-play as follows: The move that is selected is the move with maximum expected outcome for the side making the move. In other 150 6 Two-Agent Self-Play words, the neural network is learning from the results of playing against itself. This self-play training paradigm is used even at the start of learning, when the network\u2019s weights are random, and hence its initial strategy is a random strategy [764]. TD-Gammon performed tabula rasa learning, its neural network weights initiliazed to small random numbers. It reached world-champion level purely by playing against itself, learning the game as it played along. Such autonomous self-learning is one of the main goals of artifcial intelligence. TD-Gammon\u2019s success inspired many researchers to try neural networks and selfplay approaches, culminating eventually, many years later, in high-profle results in Atari [522] and AlphaGo [703, 706], which we will describe in this chapter.1 In Sect. 6.1 two-agent zero-sum environments will be described. Next, in Sect. 6.2 the tabula rasa self-play method is described in detail. In Sect. 6.3 we focus on the achievements of the self-play methods. Let us now start with two-agent zero-sum problems. 6.1 Two-Agent Zero-Sum Problems Before we look into self-play algorithms, let us look for a moment at the two-agent games that have fascinated artifcial intelligence researchers for such a long time. Games come in many shapes and sizes. Some are easy, some are hard. The characteristics of games are described in a fairly standard taxonomy. Important characteristics of games are: the number of players, whether the game is zero-sum or non-zero-sum, whether it is perfect or imperfect information, what the complexity of taking decisions is, and what the state space complexity is. We will look at these characteristics in more detail. \u2022 Number of Players One of the most important elements of a game is the number of players. One-player games are normally called puzzles, and are modeled as a standard MDP. The goal of a puzzle is to fnd a solution. Two-player games are \u201creal\u201d games. Quite a number of two-player games exist that provide a nice balance between being too easy and being too hard for players (and for computer programmers) [172]. Examples of two-player games that are popular in AI are chess, checkers, Go, Othello, and shogi. Multi-player games are played by three or more players. Well-known examples of multiplayer games are the card games bridge and poker, and strategy games such as Risk, Diplomacy, and StarCraft. \u2022 Zero Sum versus Non Zero Sum An important aspect of a game is whether it is competitive or cooperative. Most two-player games are competitive: the win (+1) of player A is the loss (\u22121) of player B. These games are called zero sum because the sum of the wins for the players remains a constant zero. Competition is an 1 A modern reimplementation of TD-Gammon in TensorFlow is available on GitHub at TDGammon https://github.com/fomorians/td-gammon 6.1 Two-Agent Zero-Sum Problems 151 important element in the real world, and these games provide a useful model for the study of confict and strategic behavior. In contrast, in cooperative games the players win if they can fnd win/win situations. Examples of cooperative games are Hanabi, bridge, Diplomacy [429, 185], poker and Risk. The next chapter will discuss multi-agent and cooperative games. \u2022 Perfect versus Imperfect Information In perfect information games all relevant information is known to all players. This is the case in typical board games such as chess and checkers. In imperfect information games some information may be hidden from some players. This is the case in card games such as bridge and poker, where not all cards are known to all players. Imperfect information games can be modeled as partially observable Markov processes, POMDP [569, 693]. A special form of (im)perfect information games are games of chance, such as backgammon and Monopoly, in which dice play an important role. There is no hidden information in these games, and these games are sometimes considered to be perfect information games, despite the uncertainty present at move time. Stochasticity is not the same as imperfect information. \u2022 Decision Complexity The difculty of playing a game depends on the complexity of the game. The decision complexity is the number of end positions that defne the value (win, draw, or loss) of the initial game position (also known as the critical tree or proof tree [416]). The larger the number of actions in a position, the larger the decision complexity. Games with small board sizes such as tic tac toe (3 \u00d7 3) have a smaller complexity than games with larger boards, such as gomoku (19 \u00d7 19). When the action space is very large, it can often be treated as a continuous action space. In poker, for example, the monetary bets can be of any size, defning an action size that is practically continuous. \u2022 State Space Complexity The state space complexity of a game is the number of legal positions reachable from the initial position of a game. State space and decision complexity are normally positively correlated, since games with high decision complexity typically have high state space complexity. Determining the exact state space complexity of a game is a nontrivial task, since positions may be illegal or unreachable.2 For many games approximations of the state space have been calculated. In general, games with a larger state space complexity are harder to play (\u201crequire more intelligence\u201d) for humans and computers. Note that the dimensionality of the states may not correlate with the size of the state space, for example, the rules of some of the simpler Atari games limit the number of reachable states, although the states themselves are high-dimensional (they consist of many video pixels). 2 For example, the maximal state space of tic tac toe is 39 = 19683 positions (9 squares of \u2019X\u2019, \u2019O\u2019, or blank), where only 765 positions remain if we remove symmetrical and illegal positions [661]. 152 6 Two-Agent Self-Play Name board state space zero-sum information Chess 8 \u00d7 8 1047 zero-sum perfect Checkers 8 \u00d7 8 1018 zero-sum perfect Othello 8 \u00d7 8 1028 zero-sum perfect Backgammon 24 1020 zero-sum chance Go 19 \u00d7 19 10170 zero-sum perfect Shogi 9 \u00d7 9 1071 zero-sum perfect Poker card 10161 non-zero imperfect Table 6.1 Characteristics of games Fig. 6.2 Deep Blue and Garry Kasparov in May 1997 in New York Zero-Sum Perfect-Information Games Two-person zero-sum games of perfect information, such as chess, checkers, and Go, are among the oldest applications of artifcial intelligence. Turing and Shannon published the frst ideas on how to write a program to play chess more than 70 years ago [788, 694]. To study strategic reasoning in artifcial intelligence, these games are frequently used. Strategies, or policies, determine the outcome. Table 6.1 summarizes some of the games that have played an important role in artifcial intelligence research. 6.1.1 The Difculty of Playing Go After the 1997 defeat of chess world champion Garry Kasparov by IBM\u2019s Deep Blue computer (Fig. 6.2; image by Chessbase), the game of Go (Fig. 1.4) became the next 6.1 Two-Agent Zero-Sum Problems 153 benchmark game, the Drosophila3 of AI, and research activity in Go intensifed signifcantly. The game of Go is more difcult than chess. It is played on a larger board (19 \u00d7 19 vs. 8 \u00d7 8), the action space is larger (around 250 moves available in a position versus some 25 in chess), the game takes longer (typically 300 moves versus 70) and the state space complexity is much larger: 10170 for Go, versus 1047 for chess. Furthermore, rewards in Go are sparse. Only at the end of a long game, after many moves have been played, is the outcome (win/loss) known. Captures are not so frequent in Go, and no good efciently computable heuristic has been found. In chess, in contrast, the material balance in chess can be calculated efciently, and gives a good indication of how far ahead we are. For the computer, much of the playing in Go happens in the dark. In contrast, for humans, it can be argued that the visual patterns of Go may be somewhat easier to interpret than the deep combinatorial lines of chess. For reinforcement learning, credit assignment in Go is challenging. Rewards only occur after a long sequence of moves, and it is unclear which moves contributed the most to such an outcome, or whether all moves contributed equally. Many games will have to be played to acquire enough outcomes. In conclusion, Go is more difcult to master with a computer than chess. Traditionally, computer Go programs followed the conventional chess design of a minimax search with a heuristic evaluation function, that, in the case of Go, was based on the infuence of stones (see Sect. 6.2.1 and Fig. 6.4) [515]. This chess approach, however, did not work for Go, or at least not well enough. The level of play was stuck at mid-amateur level for many years. The main problems were the large branching factor, and the absence of an efcient and good evaluation function. Subsequently, Monte Carlo Tree Search was developed, in 2006. MCTS is a variable depth adaptive search algorithm, that did not need a heuristic function, but instead used random playouts to estimate board strength. MCTS programs caused the level of play to improve from 10 kyu to 2-3 dan, and even stronger on the small 9 \u00d7 9 board.4 However, again, at that point, performance stagnated, and researchers expected that world champion level play was still many years into the future. Neural networks had been tried, but were slow, and did not improve performance much. Playing Strength in Go Let us compare the three programming paradigms of a few diferent Go programs that have been written over the years (Fig. 6.3). The programs fall into three categories. First are the programs that use heuristic planning, the minimax-style 3 Drosophila Melanogaster is also known as the fruitfy, a favorite species of genetics researchers to test their theories, because experiments produce quick and clear answers. 4 Absolute beginners in Go start at 30 kyu, progressing to 10 kyu, and advancing to 1 kyu (30k\u20131k). Stronger amateur players then achieve 1 dan, progressing to 7 dan, the highest amateur rating for Go (1d\u20137d). Professional Go players have a rating from 1 dan to 9 dan, written as 1p\u20139p. 154 6 Two-Agent Self-Play Fig. 6.3 Go Playing Strength of Top Programs over the Years [20] programs. GNU Go is a well-known example of this group of programs. The heuristics in these programs are hand-coded. The level of play of these programs was at medium amateur level. Next come the MCTS-based programs. They reached strong amateur level. Finally come the AlphaGo programs, in which MCTS is combined with deep self-play. These reached super-human performance. The fgure also shows other programs that follow a related approach. Thus, Go provided a large and sparse state space, providing a highly challenging test, to see how far self-play with a perfect transition function can come. Let us have a closer look at the achievements of AlphaGo. 6.1.2 AlphaGo Achievements In 2016, after decades of research, the efort in Go paid of. In the years 2015\u20132017 the DeepMind AlphaGo team played three matches in which it beat all human champions that it played, Fan Hui, Lee Sedol, and Ke Jie. The breakthrough performance of AlphaGo came as a surprise. Experts in computer games had expected grandmaster level play to be at least ten years away. The techniques used in AlphaGo are the result of many years of research, and cover a wide range of topics. The game of Go worked very well as Drosophila. Important new algorithms were developed, most notably Monte Carlo Tree Search (MCTS), as well as major progress was made in deep reinforcement learning. We will provide a high-level overview of the research that culminated in AlphaGo (that 6.1 Two-Agent Zero-Sum Problems 155 Fig. 6.4 Infuence in the game of Go. Empty intersections are marked as being part of Black\u2019s or White\u2019s Territory Fig. 6.5 AlphaGo versus Lee Sedol in 2016 in Seoul beat the champions), and its successor, AlphaGo Zero (that learns Go tabula rasa). First we will describe the Go matches. The games against Fan Hui were played in October 2015 in London as part of the development efort of AlphaGo. Fan Hui is the 2013, 2014, and 2015 European Go Champion, then rated at 2p dan. The games against Lee Sedol were played in May 2016 in Seoul, and were widely covered by the media (see Fig. 6.5; image by 156 6 Two-Agent Self-Play Fig. 6.6 AlphaGo on the Cover of Nature DeepMind). Although there is no ofcial worldwide ranking in international Go, in 2016 Lee Sedol was widely considered one of the four best players in the world. A year later another match was played, this time in China, against the Chinese champion Ke Jie, who was ranked number one in the Korean, Japanese, and Chinese ranking systems at the time of the match. All three matches were won convincingly by AlphaGo. Beating the best Go players appeared on the cover of the journal Nature, see Fig. 6.6. The AlphaGo series of programs actually consists of three programs: AlphaGo, AlphaGo Zero, and AlphaZero. AlphaGo is the program that beat the human Go champions. It consists of a combination of supervised learning from grandmaster games and from self-play games. The second program, AlphaGo Zero, is a full re-design, based solely on self-play. It performs tabula rasa learning of Go, and plays stronger than AlphaGo. AlphaZero is a generalization of this program that also plays chess and shogi. Section 6.3 will describe the programs in more detail. Let us now have an in-depth look at the self-play algorithms as featured in AlphaGo Zero and AlphaZero. 6.2 Tabula Rasa Self-Play Agents 157 Agent2 Agent1 \ud835\udc5f\u2032 1 \ud835\udc60\u2032 1 \ud835\udc4e2 \ud835\udc5f\u2032 2 \ud835\udc60\u2032 2 \ud835\udc4e1 Fig. 6.7 Agent-Agent World 6.2 Tabula Rasa Self-Play Agents Model-based reinforcement learning showed us that by learning a local transition model, good sample efciency can be achieved when the accuracy of the model is sufcient. When we have perfect knowledge of the transitions, as we have in this chapter, then we can plan far into the future, without error. In regular agent-environment reinforcement learning the complexity of the environment does not change as the agent learns, and as a consequence, the intelligence of the agent\u2019s policy is limited by the complexity of the environment. However, in self-play a cycle of mutual improvement occurs; the intelligence of the environment improves because the agent is learning. With self-play, we can create a system that can transcend the original environment, and keep growing, and growing, in a virtuous cycle of mutual learning. Intelligence emerging out of nothing. This is the kind of system that is needed when we wish beat the best known entity in a certain domain, since copying from a teacher will not help us to transcend it. Studying how such a high level of play is achieved is interesting, for three reasons: (1) it is exciting to follow an AI success story, (2) it is interesting to see which techniques were used and how it is possible to achieve beyond-human intelligence, and (3) it is interesting to see if we can learn a few techniques that can be used in other domains, beyond two-agent zero-sum games, to see if we can achieve super-intelligence there as well. Let us have a closer look at the self-learning agent architecture that is used by AlphaGo Zero. We will see that two-agent self-play actually consists of three levels of self-play: move-level self-play, example-level self-play, and tournament-level self-play. First, we will discuss the general architecture, and how it creates a cycle of virtuous improvement. Next, we will describe the levels in detail. 158 6 Two-Agent Self-Play Transition Rules Opponent playing Policy/Value learning/planning acting Fig. 6.8 Playing with Known Transition Rules Cycle of Virtuous Improvement In contrast to the agent/environment model, we now have two agents (Fig. 6.7). In comparison with the model-based world of Chap. 5 (Fig. 6.8) our learned model has been replaced by perfect knowledge of the transition rules, and the environment is now called opponent: the negative version of the same agent playing the role of agent2. The goal in this chapter is to reach the highest possible performance in terms of level of play, without using any hand-coded domain knowledge. In applications such as chess and Go a perfect transition model is present. Together with a learned reward function and a learned policy function, we can create a self-learning system in which a virtuous cycle of ever improving performance occurs. Figure 6.9 illustrates such a system: (1) the searcher uses the evaluation network to estimate reward values and policy actions, and the search results are used in games against the opponent in self-play, (2) the game results are then collected in a bufer, which is used to train the evaluation network in self-learning, and (3) by playing a tournament against a copy of ourselves a virtuous cycle of ever-increasing function improvement is created. AlphaGo Zero Self-Play in Detail Let us look in more detail at how self-learning works in AlphaGo Zero. AlphaGo Zero uses a model-based actor critic approach with a planner that improves a single value/policy network. For policy improvement it uses MCTS, for learning a single deep residual network with a policy head and a value head (Sect. B.2.6), see Fig. 6.9. MCTS improves the quality of the training examples in each iteration (left panel), and the net is trained with these better examples, improving its quality (right panel). The output of MCTS is used to train the evaluation network, whose output is then used as evalution function in that same MCTS. A loop is wrapped around the search-eval functions to keep training the network with the game results, creating a learning curriculum. Let us put these ideas into pseudocode. 6.2 Tabula Rasa Self-Play Agents 159 search eval train game examples new net reward opponent games net0 net1 net2 net3 . . . MCTS MCTS MCTS MCTS Fig. 6.9 Self-play loop improving quality of net 1 for tourn in range (1, max_tourns ): # curric. of tournaments 2 for game in range (1, max_games): # play a tourn. of games 3 trim(triples) # if buffer full: replace old entries 4 while not game_over (): # generate the states of one game 5 move = mcts(board , eval(net)) # move is (s,a) pair 6 game_pairs += move 7 make_move_and_switch_side (board , move) 8 triples += add(game_pairs , game_outcome ( game_pairs )) # add to buf 9 net = train(net , triples) # retrain with (s,a,outc) triples Listing 6.1 Self-play pseudocode The Cycle in Pseudocode Conceptually self-play is as ingenious as it is elegant: a double training loop around an MCTS player with a neural network as evaluation and policy function that help MCTS. Figure 6.10 and Listing 6.1 show the self-play loop in detail. The numbers in the fgure correspond to the line numbers in the pseudocode. Let us perform an outside-in walk-through of this system. Line 1 is the main self-play loop. It controls how long the execution of the curriculum of self-play tournaments will continue. Line 2 executes the training episodes, the tournaments of self-play games after which the network is retrained. Line 4 plays such a game to create (state, action) pairs for each move, and the outcome of the game. Line 5 calls MCTS to generate an action in each state. MCTS performs the simulations where it uses the policy head of the net in P-UCT selection, and the value head of the net at the MCTS leaves. Line 6 appends the state/action pair to the list of game 160 6 Two-Agent Self-Play 5 game_pairs \u2190 mcts 5 pol/val \u2190 eval(net(state)) 9 net \u2190 train(net, triples) 8 triples 1 tourn: iterate with new net policy/value state 2/4 game_pairs Fig. 6.10 A diagram of self-play with line-numbers moves. Line 7 performs the move on the board, and switches color to the other player, for the next move in the while loop. At line 8 a full game has ended, and the outcome is known. Line 8 adds the outcome of each game to the (state, action)-pairs, to make the (state, action, outcome)-triples for the network to train on. Note that since the network is a two-headed policy/value net, both an action and an outcome are needed for network training. On the last line this triples-bufer is then used to train the network. The newly trained network is used in the next self-play iteration as the evaluation function by the searcher. With this net another tournament is played, using the searcher\u2019s look-ahead to generate a next batch of higher-quality examples, resulting in a sequence of stronger and stronger networks (Fig. 6.9 right panel). In the pseudocode we see the three self-play loops where the principle of playing against a copy of yourself is used: 1. Move-level: in the MCTS playouts, our opponent actually is a copy of ourselves (line 5)\u2014hence, self-play at the level of game moves 2. Example-level: the input for self-training the approximator for the policy and the reward functions is generated by our own games (line 2)\u2014hence, self-play at the level of the value/policy network. 3. Tournament-level: the self-play loop creates a training curriculum that starts tabula rasa and ends at world champion level. The system trains at the level of the player against itself (line 1)\u2014hence, self-play, of the third kind. All three of these levels use their own kind of self-play, of which we will describe the details in the following sections. We start with move-level self-play. 6.2.1 Move-Level Self-Play At the innermost level, we use the agent to play against itself, as its own opponent. Whenever it is my opponent\u2019s turn to move, I play its move, trying to fnd the best move for my opponent (which will be the worst possible move for me). This scheme uses the same knowledge for player and opponent. This is diferent from the real world, where the agents are diferent, with diferent brains, diferent reasoning 6.2 Tabula Rasa Self-Play Agents 161 eval search value action/state Fig. 6.11 Search-Eval Architecture of Games skills, and diferent experience. Our scheme is symmetrical: when we assume that our agent plays a strong game, then the opponent is also assumed to play strongly, and we can hope to learn from the strong counter play. (We thus assume that our agent plays with the same knowledge as we have; we are not trying to consciously exploit opponent weaknesses.)5 6.2.1.1 Minimax This principle of generating the counter play by playing yourself while switching perspectives has been used since the start of artifcial intelligence. It is known as minimax. The games of chess, checkers and Go are challenging games. The architecture that has been used to program chess and checkers players has been the same since the earliest paper designs of Turing [788]: a search routine based on minimax which searches to a certain depth, and an evaluation function to estimate the score of board positions using heuristic rules of thumb when this depth is reached. In chess and checkers, for example, the number of pieces on the board of a player is a crude but efective approximation of the strength of a state for that player. Figure 6.11 shows a diagram of this classic search-eval architecture.6 Based on this principle many successful search algorithms have been developed, of which alpha-beta is the best known [416, 593]. Since the size of the state space is exponential in the depth of lookahead, however, many enhancements had to be developed to manage the size of the state space and to allow deep lookahead to occur [600]. The word minimax is a contraction of maximizing/minimizing (and then reversed for easy pronunciation). It means that in zero-sum games the two players alternate making moves, and that on even moves, when player A is to choose a move, the 5 There is also research into opponent modeling, where we try to exploit our opponent\u2019s weaknesses [320, 91, 259]. Here, we assume an identical opponent, which often works best in chess and Go. 6 Because the agent knows the transition function \ud835\udc47 , it can calculate the new state \ud835\udc60\u2032 for each action \ud835\udc4e. The reward \ud835\udc5f is calculated at terminal states, where it is equal to the value \ud835\udc63. Hence, in this diagram the search function provides the state to the eval function. See [788, 600] for an explanation of the search-eval architecture. 162 6 Two-Agent Self-Play 2 1 6 1 3 2 3 4 2 1 1 6 5 Fig. 6.12 Minimax tree best move is the one that maximizes the score for player A, while on odd moves the best move for player B is the move that minimizes the score for player A. Figure 6.12 depicts this situation in a tree. The score values in the nodes are chosen to show how minimax works. At the top is the root of the tree, level 0, a square node where player A is to move. Since we assume that all players rationally choose the best move, the value of the root node is determined by the value of the best move, the maximum of its children. Each child, at level 1, is a circle node where player B chooses its best move, in order to minimize the score for player A. The leaves of this tree, at level 2, are again max squares (even though there is no child to choose from anymore). Note how for each circle node the value is the minimum of its children, and for the square node, the value is the maximum of the tree circle nodes. Python pseudocode for a recursive minimax procedure is shown in Listing 6.2. Note the extra hyperparameter d. This is the search depth counting upwards from the leaves. At depth 0 are the leaves, where the heuristic evaluation function is called to score the board.7 Also note that the code for making moves on the board\u2014 transitioning actions into the new states\u2014is not shown in the listing. It is assumed to happen inside the children dictionary. We frivolously mix actions and states in these sections, since an action fully determines which state will follow. (At the end of this chapter, the exercises provide more detail about move making and unmaking.) AlphaGo Zero uses MCTS, a more advanced search algorithm than minimax, that we will discuss shortly. 7 The heuristic evaluation function is originally a linear combination of hand-crafted heuristic rules, such as material balance (which side has more pieces) or center control. At frst, the linear combinations (coefcients) were not only hand-coded, but also hand-tuned. Later they were trained by supervised learning [61, 613, 771, 253]. More recently, NNUE was introduced as a non-linear neural network to use as evaluation function in an alpha-beta framework [557]. 6.2 Tabula Rasa Self-Play Agents 163 1 INF = 99999 # a value assumed to be larger than eval ever returns 2 3 def minimax(n, d): 4 if d <= 0: 5 return heuristic_eval (n) 6 elif n[\u2019type \u2019] == \u2019MAX \u2019: 7 g = -INF 8 for c in n[\u2019children \u2019]: 9 g = max(g, minimax(c, d -1)) 10 elif n[\u2019type \u2019] == \u2019MIN \u2019: 11 g = INF 12 for c in n[\u2019children \u2019]: 13 g = min(g, minimax(c, d -1)) 14 return g 15 16 print(\"Minimax\u2423value:\u2423\", minimax(root , 2)) Listing 6.2 Minimax code [600] Fig. 6.13 Three Lines of Play [65] Beyond Heuristics Minimax-based procedures traverse the state space by recursively following all actions in each state that they visit [788]. Minimax works just like a standard depth-frst search procedure, such as we have been taught in our undergraduate algorithms and data structures courses. It is a straightforward, rigid, approach, that searches all branches of the node to the same search depth. To focus the search efort on promising parts of the tree, researchers have subsequently introduced many algorithmic enhancements, such as alpha-beta cutofs, iterative deepening, transposition tables, null windows, and null moves [416, 602, 422, 714, 203]. In the early 1990s experiments with a diferent approach started, based on random playouts of a single line of play [6, 102, 118] (Fig. 6.13 and 6.14). In Fig. 6.14 164 6 Two-Agent Self-Play Fig. 6.14 Searching a Tree versus Searching a Path this diferent approach is illustrated. We see a search of a single line of play versus a search of a full subtree. It turned out that averaging many such playouts could also be used to approximate the value of the root, in addition to the classic recursive tree search approach. In 2006, a tree version of this approach was introduced that proved successful in Go. This algorithms was called Monte Carlo Tree Search [164, 117]. Also in that year Kocsis and Szepesv\u00e1ri created a selection rule for the exploration/exploitation trade-of that performed well and converged to the minimax value [419]. Their rule is called UCT, for upper confdence bounds applied to trees. 6.2.1.2 Monte Carlo Tree Search Monte Carlo Tree Search has two main advantages over minimax and alpha-beta. First, MCTS is based on averaging single lines of play, instead of recursively traversing subtrees. The computational complexity of a path from the root to a leaf is polynomial in the search depth. The computational complexity of a tree is exponential in the search depth. Especially in applications with many actions per state it is much easier to manage the search time with an algorithm that expands one path at a time.8 Second, MCTS does not need a heuristic evaluation function. It plays out a line of play in the game from the root to an end position. In end-positions the score of the game, a win or a loss, is known. By averaging many of these playouts the value of the root is approximated. Minimax has to cope with an exponential search tree, which it cuts of after a certain search depth, at which point it uses the heuristic to estimate the scores at the leaves. There are, however, games where no efcient 8 Compare chess and Go: in chess the typical number of moves in a position is 25, for Go this number is 250. A chess-tree of depth 5 has 255 = 9765625 leaves. A Go-tree of depth 5 has 2505 = 976562500000 leaves. A depth-5 minimax search in Go would take prohibitively long; an MCTS search of 1000 expansions expands the same number of paths from root to leaf in both games. 6.2 Tabula Rasa Self-Play Agents 165 Fig. 6.15 Monte Carlo Tree Search [117] heuristic evaluation function can be found. In this case MCTS has a clear advantage, since it works without a heuristic score function. MCTS has proven to be successful in many diferent applications. Since its introduction in 2006 MCTS has transformed the feld of heuristic search. Let us see in more detail how it works. Monte Carlo Tree Search consists of four operations: select, expand, playout, and backpropagate (Fig. 6.15). The third operation (playout) is also called rollout, simulation, and sampling. Backpropagation is sometimes called backup. Select is the downward policy action trial part, backup is the upward error/learning part of the algorithm. We will discuss the operations in more detail in a short while. MCTS is a succesful planning-based reinforcement learning algorithm, with an advanced exploration/exploitation selection rule. MCTS starts from the initial state \ud835\udc600, using the transition function to generate successor states. In MCTS the state space is traversed iteratively, and the tree data structure is built in a step by step fashion, node by node, playout by playout. A typical size of an MCTS search is to do 1000\u201310,000 iterations. In MCTS each iteration starts at the root \ud835\udc600, traversing a path in the tree down to the leaves using a selection rule, expanding a new node, and performing a random playout. The result of the playout is then propagated back to the root. During the backpropagation, statistics at all internal nodes are updated. These statistics are then used in future iterations by the selection rule to go to the currently most interesting part of the tree. The statistics consist of two counters: the win count \ud835\udc64 and the visit count \ud835\udc63. During backpropagation, the visit count \ud835\udc63 at all nodes that are on the path back from the leaf to the root are incremented. When the result of the playout was a win, then the win count \ud835\udc64 of those nodes is also incremented. If the result was a loss, then the win count is left unchanged. The selection rule uses the win rate \ud835\udc64/\ud835\udc63 and the visit count \ud835\udc63 to decide whether to exploit high-win-rate parts of the tree or to explore low-visit-count parts. An 166 6 Two-Agent Self-Play 1 def monte_carlo_tree_search (root): 2 while resources_left (time , computational power): 3 leaf = select(root) # leaf = unvisited node 4 simulation_result = rollout(leaf) 5 backpropagate (leaf , simulation_result ) 6 return best_child(root) # or: child with highest visit count 7 8 def select(node): 9 while fully_expanded (node): 10 node = best_child (node) # traverse down path of best UCT nodes 11 return expand(node.children) or node # no children/node is terminal 12 13 def rollout(node): 14 while non_terminal (node): 15 node = rollout_policy (node) 16 return result(node) 17 18 def rollout_policy (node): 19 return pick_random (node.children) 20 21 def backpropagate (node , result): 22 if is_root(node) return 23 node.stats = update_stats (node , result) 24 backpropagate (node.parent) 25 26 def best_child(node , c_param =1.0): 27 choices_weights = [ 28 (c.q / c.n) + c_param * np.sqrt ((np.log(node.n) / c.n)) # UCT 29 for c in node.children 30 ] 31 return node.children[np.argmax( choices_weights )] Listing 6.3 MCTS pseudo-Python [117, 173] often-used selection rule is UCT (Sect. 6.2.1.2). It is this selection rule that governs the exploration/exploitation trade-of in MCTS. The Four MCTS Operations Let us look in more detail at the four operations. Please refer to Listing 6.3 and Fig. 6.15 [117]. As we see in the fgure and the listing, the main steps are repeated as long as there is time left. Per step, the activities are as follows. 1. Select In the selection step the tree is traversed from the root node down until a leaf of the MCTS search tree is reached where a new child is selected that is not part of the tree yet. At each internal state the selection rule is followed to 6.2 Tabula Rasa Self-Play Agents 167 determine which action to take and thus which state to go to next. The UCT rule works well in many applications [419]. The selections at these states are part of the policy \ud835\udf0b(\ud835\udc60) of actions of the state. 2. Expand Then, in the expansion step, a child is added to the tree. In most cases only one child is added. In some MCTS versions all successors of a leaf are added to the tree [117]. 3. Playout Subsequently, during the playout step random moves are played in a form of self-play until the end of the game is reached. (These nodes are not added to the MCTS tree, but their search result is, in the backpropagation step.) The reward \ud835\udc5f of this simulated game is +1 in case of a win for the frst player, 0 in case of a draw, and \u22121 in case of a win for the opponent.9 4. Backpropagation In the backpropagation step, reward \ud835\udc5f is propagated back upwards in the tree, through the nodes that were traversed down previously. Two counts are updated: the visit count, for all nodes, and the win count, depending on the reward value. Note that in a two-agent game, nodes in the MCTS tree alternate color. If white has won, then only white-to-play nodes are incremented; if black has won, then only the black-to-play nodes. MCTS is on-policy: the values that are backed up are those of the nodes that were selected. Pseudocode Many websites contain useful resources on MCTS, including example code (see Listing 6.3).10 The pseudocode in the listing is from an example program for game play. The MCTS algorithm can be coded in many diferent ways. For implementation details, see [173] and the comprehensive survey [117]. MCTS is a popular algorithm. An easy way to use it in Python is by installing it from a pip package (pip install mcts). Policies At the end of the search, after the predetermined iterations have been performed, or when time is up, MCTS returns the value and the action with the highest visit count. An alternative would be to return the action with the highest win rate. However, the visit count takes into account the win rate (through UCT) and the number of simulations on which it is based. A high win rate may be based on a low number of simulations, and can thus be high variance. High visit counts will be low variance. 9 Originally, playouts were random (the Monte Carlo part in the name of MCTS) following Br\u00fcgmann\u2019s [118] and Bouzy and Helmstetter\u2019s [102] original approach. In practice, most Go playing programs improve on the random playouts by using databases of small 3 \u00d7 3 patterns with best replies and other fast heuristics [269, 165, 137, 702, 170]. Small amounts of domain knowledge are used after all, albeit not in the form of a heuristic evaluation function. 10 https://int8.io/monte-carlo-tree-search-beginners-guide/ 168 6 Two-Agent Self-Play Due to selection rule, high visit count implies high win-rate with high confdence, while high win rate may be low confdence [117]. The action of this initial state \ud835\udc600 constitutes the deterministic policy \ud835\udf0b(\ud835\udc600). UCT Selection The adaptive exploration/exploitation behavior of MCTS is governed by the selection rule, for which often UCT is chosen. UCT is an adaptive exploration/exploitation rule that achieves high performance in many diferent domains. UCT was introduced in 2006 by Kocsis and Szepesv\u00e1ri [419]. The paper provides a theoretical guarantee of eventual convergence to the minimax value. The selection rule was named UCT, for upper confdence bounds for multi-armed bandits applied to trees. (Bandit theory was also mentioned in Sect. 2.2.4.3). The selection rule determines the way in which the current values of the children infuence which part of the tree will be explored more. The UCT formula is UCT(\ud835\udc4e) = \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e + \ud835\udc36\ud835\udc5d \u221a\ufe04 ln \ud835\udc5b \ud835\udc5b\ud835\udc4e (6.1) where \ud835\udc64\ud835\udc4e is the number of wins in child \ud835\udc4e, \ud835\udc5b\ud835\udc4e is the number of times child \ud835\udc4e has been visited, \ud835\udc5b is the number of times the parent node has been visited, and \ud835\udc36\ud835\udc5d \u2265 0 is a constant, the tunable exploration/exploitation hyperparameter. The frst term in the UCT equation, the win rate \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e , is the exploitation term. A child with a high win rate receives through this term an exploitation bonus. The second term \u221a\ufe03 ln \ud835\udc5b \ud835\udc5b\ud835\udc4e is for exploration. A child that has been visited infrequently has a higher exploration term. The level of exploration can be adjusted by the \ud835\udc36\ud835\udc5d constant. A low \ud835\udc36\ud835\udc5d does little exploration; a high \ud835\udc36\ud835\udc5d has more exploration. The selection rule then is to select the child with the highest UCT sum (the familiar arg max function of value-based methods). The UCT formula balances win rate \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e and \u201cnewness\u201d \u221a\ufe03 ln \ud835\udc5b \ud835\udc5b\ud835\udc4e in the selection of nodes to expand.11 Alternative selection rules have been proposed, such as Auer\u2019s UCB1 [30, 31, 32] and P-UCT [638, 504]. P-UCT We should note that the MCTS that is used in the AlphaGo Zero program is a little diferent. MCTS is used inside the training loop, as an integral part of the 11 The square-root term is a measure of the variance (uncertainty) of the action value. The use of the natural logarithm ensures that, since increases get smaller over time, old actions are selected less frequently. However, since logarithm values are unbounded, eventually all actions will be selected [743]. 6.2 Tabula Rasa Self-Play Agents 169 self-generation of training examples, to enhance the quality of the examples for every self-play iteration, using both value and policy inputs to guide the search. Also, in the AlphaGo Zero program MCTS backups rely fully on the value function approximator; no playout is performed anymore. The MC part in the name of MCTS, which stands for the Monte Carlo playouts, really has become a misnomer for this neural network-guided tree searcher. Furthermore, selection in self-play MCTS is diferent. UCT-based node selection now also uses the input from the policy head of the trained function approximators, in addition to the win rate and newness. What remains is that through the UCT mechanism MCTS can focus its search efort greedily on the part with the highest win rate, while at the same time balancing exploration of parts of the tree that are underexplored. The formula that is used to incorporate input from the policy head of the deep network is a variant of P-UCT [706, 530, 638, 504] (for predictor-UCT). Let us compare P-UCT with UCT. The P-UCT formula adds the policy head \ud835\udf0b(\ud835\udc4e|\ud835\udc60) to Eq. 6.1 P-UCT(\ud835\udc4e) = \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e + \ud835\udc36\ud835\udc5d\ud835\udf0b(\ud835\udc4e|\ud835\udc60) \u221a\ud835\udc5b 1 + \ud835\udc5b\ud835\udc4e . P-UCT adds the \ud835\udf0b(\ud835\udc4e|\ud835\udc60) term specifying the probability of the action \ud835\udc4e to the exploration part of the UCT formula.12 Exploration/Exploitation The search process of MCTS is guided by the statistics values in the tree. MCTS discovers during the search where the promising parts of the tree are. The tree expansion of MCTS is inherently variable-depth and variable-width (in contrast to minimax-based algorithms such as alpha-beta, which are inherently fxed-depth and fxed-width). In Fig. 6.16 we see a snapshot of the search tree of an actual MCTS optimization. Some parts of the tree are searched more deeply than others [808]. An important element of MCTS is the exploration/exploitation trade-of, that can be tuned with the \ud835\udc36\ud835\udc5d hyperparameter. The efectiveness of MCTS in diferent applications depends on the value of this hyperparameter. Typical initial choices for Go programs are \ud835\udc36\ud835\udc5d = 1 or \ud835\udc36\ud835\udc5d = 0.1 [117], although in AlphaGo we see highly explorative choices such as \ud835\udc36\ud835\udc5d = 5. In general, experience has learned that when compute power is low, \ud835\udc36\ud835\udc5d should be low, and when more compute power is available, more exploration (higher \ud835\udc36\ud835\udc5d) is advisable [117, 434]. 12 Note further the small diferences under the square root (no logarithm, and the 1 in the denominator) also change the UCT function profle somewhat, ensuring correct behavior at unvisited actions [530]. 170 6 Two-Agent Self-Play 0 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 21 20 22 23 22 20 21 23 20 21 24 20 21 22 25 20 21 22 23 24 20 26 20 27 20 28 20 21 29 30 31 20 32 33 34 20 26 20 21 22 23 27 20 21 22 23 24 25 28 20 21 22 23 24 25 26 27 29 30 31 32 33 29 20 21 21 22 23 20 24 20 25 26 27 20 28 20 30 20 31 32 33 34 30 20 21 21 20 22 20 23 20 24 20 25 20 26 20 27 20 21 22 28 29 20 21 22 23 21 20 22 23 22 23 20 24 20 21 22 23 25 20 21 22 26 20 21 27 20 28 20 21 31 20 21 22 32 20 21 22 23 24 21 22 25 26 21 27 21 28 21 31 21 22 33 21 34 21 21 20 22 23 22 20 21 23 20 21 24 20 21 22 21 22 20 23 20 25 20 21 22 26 20 21 27 20 28 20 21 22 31 20 21 22 33 20 21 34 20 21 22 25 20 21 21 20 22 23 20 24 20 21 26 20 27 20 28 20 31 20 21 22 33 20 21 22 34 20 26 20 21 22 23 24 25 27 27 20 21 22 23 28 20 21 22 23 31 20 21 22 23 21 20 22 22 20 23 20 24 20 21 22 25 20 21 22 23 26 20 21 27 20 28 20 21 33 20 21 22 34 20 21 33 20 21 22 22 21 23 21 24 21 22 25 21 26 21 22 27 21 28 21 22 31 21 22 23 34 21 21 20 22 23 22 20 21 23 24 23 20 21 22 24 24 20 21 21 22 20 23 25 20 21 22 26 20 21 27 20 28 20 21 31 20 21 22 34 20 25 20 21 21 20 22 20 23 24 20 26 20 27 20 28 20 21 31 20 21 34 20 26 20 21 21 20 22 23 24 25 20 27 28 31 34 27 20 21 22 28 20 21 22 21 20 22 22 20 23 24 20 21 25 20 26 20 21 27 20 31 20 21 34 20 21 31 20 21 22 23 22 24 22 25 22 26 22 27 22 28 22 34 22 22 21 23 21 24 21 25 21 26 27 28 34 23 21 22 21 24 21 25 26 21 27 28 34 24 21 22 23 22 21 23 21 25 21 22 26 21 22 27 21 28 21 22 34 21 25 21 22 22 21 23 21 24 21 22 26 21 22 27 21 28 21 22 34 21 26 21 22 22 21 23 21 24 21 22 25 21 27 21 28 21 22 34 21 27 21 22 22 23 24 25 21 26 21 28 21 34 21 28 21 22 23 22 21 23 21 24 21 22 25 21 22 26 21 22 27 21 34 21 34 21 22 22 21 23 24 21 25 21 26 21 27 28 21 21 20 22 23 22 20 23 20 24 20 22 25 20 26 20 22 27 20 28 20 22 34 20 22 20 21 23 24 25 26 27 28 34 23 20 21 22 24 25 26 27 28 34 24 20 21 22 23 25 21 26 21 27 28 21 34 21 20 22 23 25 26 22 20 21 23 25 23 20 21 22 25 26 25 20 21 21 22 23 26 20 27 20 28 34 26 20 21 22 23 25 20 27 28 34 27 20 21 22 23 28 20 21 20 22 23 25 26 20 27 34 34 20 21 22 23 25 26 25 20 21 22 23 24 21 26 21 27 28 21 34 21 20 22 23 24 26 27 22 20 21 23 24 23 20 21 22 24 24 20 21 21 20 22 23 26 20 27 28 20 34 26 20 21 22 23 24 27 28 34 27 20 21 22 23 24 28 20 21 21 22 23 24 26 20 27 34 34 20 21 22 23 24 26 26 20 21 22 23 24 25 27 21 20 22 23 22 20 21 23 20 21 24 20 21 22 23 25 25 20 21 22 23 24 27 20 21 22 28 20 21 22 23 24 34 20 21 22 23 27 20 21 21 20 22 23 20 24 20 25 20 26 20 28 20 34 20 28 20 21 22 23 24 25 26 27 21 20 22 23 22 20 21 23 23 20 21 24 20 21 22 23 25 26 27 25 20 21 22 23 24 26 27 26 20 21 22 23 24 25 27 27 20 21 22 23 34 20 21 22 23 34 20 21 22 23 21 20 22 22 20 23 20 24 20 21 22 25 20 21 22 26 20 21 27 20 21 28 20 21 22 34 20 21 22 23 34 20 21 22 23 33 20 21 22 23 34 20 21 31 20 21 22 32 20 21 22 33 34 20 21 31 20 21 22 23 24 32 20 21 20 22 23 20 24 25 26 20 27 20 28 20 29 20 21 30 20 21 22 31 33 34 33 20 21 22 23 24 25 26 34 20 21 22 23 24 25 26 Fig. 6.16 Adaptive MCTS tree [434] Applications MCTS was introduced in 2006 [165, 166, 164] in the context of computer Go programs, following work by Chang et al. [135], Auer et al. [31], and Cazenave and Helmstetter [134]. The introduction of MCTS improved performance of Go programs considerably, from medium amateur to strong amateur. Where the heuristics-based GNU Go program played around 10 kyu, Monte Carlo programs progressed to 2-3 dan in a few years\u2019 time. Eventually, on the small 9 \u00d7 9 board, Go programs achieved very strong play. On the large 19 \u00d7 19 board, performance did not improve much beyond the 4-5 dan level, despite much efort by researchers. It was thought that perhaps the large action space of the 19\u00d719 board was too hard for MCTS. Many enhancements were considered, for the playout phase, and for the selection. As the AlphaGo results show, a crucial enhancement was the introduction of deep function approximation. After its introduction, MCTS quickly proved successful in other applications, both two agent and single agent: for video games [138], for single player applications [117], and for many other games. Beyond games, MCTS revolutionized the world of heuristic search [117]. Previously, in order to achieve best-frst search, one had to fnd a domain specifc heuristic to guide the search in a smart way. With MCTS this is no longer necessary. Now a general method exists that fnds the promising parts of the search without a domain-specifc heuristic, just by using statistics of the search itself. There is a deeper relation between UCT and reinforcement learning. Grill et al. [289] showed how the second term of P-UCT acts as a regularizer on model-free policy optimization [4]. In particular, Jacob et al. [372] showed how MCTS can be used to achieve human-like play in chess, Go, and Diplomacy, by regularizing reinforcement learning with supervised learning on human games. MCTS in AlphaGo Zero For policy improvement, AlphaGo Zero uses a version of on-policy MCTS that does not use random playouts anymore. To increase exploration, Dirichlet noise 6.2 Tabula Rasa Self-Play Agents 171 is added to the P-UCT value at the root node, to ensure that all moves may be tried. The \ud835\udc36\ud835\udc5d value of MCTS in AlphaGo is 5, heavily favoring exploration. In AlphaGo Zero the value depends on the stage in the learning; it grows during self-play. In each self-play iteration 25,000 games are played. For each move, MCTS performs 1600 simulations. In total over a three-day course of training 4.9 million games were played, after which AlphaGo Zero outperformed the previous version, AlphaGo [706]. Conclusion We have taken a look into the planning part of AlphaGo Zero\u2019s self-play architecture. MCTS consists of a move selection and a statistics backup phase, that corresponds to the behavior (trial) and learning (error) from reinforcement learning. MCTS is an important algorithm in reinforcement learning, and we have taken a detailed look at the algorithm. Move-level self-play is our frst self-play procedure; it is a procedure that plays itself to generate its counter moves. The move-level planning is only one part of the self-play picture. Just as important is the learning part. Let us have a look at how AlphaGo Zero achieves its function approximation. For this, we move to the second level of self-play: the example level. 6.2.2 Example-Level Self-Play Move-level self-play creates an environment for us that can play our counter-moves. Now we need a mechanism to learn from these actions. AlphaGo Zero follows the actor critic principle to approximate both value and policy functions. It approximates these functions using a single deep residual neural network with a value-head and a policy-head (Sect. B.2.6). The policy and the value approximations are incorporated in MCTS in the selection and backup step. In order to learn, reinforcement learning needs training examples. The training examples are generated at the self-play move level. Whenever a move is played, the \u27e8\ud835\udc60, \ud835\udc4e\u27e9 state-action pair is recorded, and whenever a full game has been played to the end, the outcome \ud835\udc67 is known, and the outcome is added to all pairs of game moves, to create \u27e8\ud835\udc60, \ud835\udc4e, \ud835\udc67\u27e9 triples. The triples are stored in the replay bufer, and sampled randomly to train the value/policy net. The actual implementation in AlphaGo Zero contains many more elements to improve the learning stability. The player is designed to become stronger than the opponent, and this occurs at the example-level. Here it uses MCTS to improve the current policy, improving it with moves that are winning against the opponent\u2019s moves. Example-level self-play is our second self-play procedure, the examples are generated in the self-play games and are used to train the network that is used to play the moves by the two players. 172 6 Two-Agent Self-Play 6.2.2.1 Policy and Value Network The frst AlphaGo program uses three separate neural networks: for the rollout policy, for the value, and for the selection policy [703]. AlphaGo Zero uses a single network, that is tightly integrated in MCTS. Let us have a closer look at this single network. The network is trained on the example triples \u27e8\ud835\udc60, \ud835\udc4e, \ud835\udc67\u27e9 from the replay bufer. These triples contain search results of the board states of the game, and the two loss function targets: \ud835\udc4e for the actions that MCTS predicts for each board states, and \ud835\udc67 for the outcome of the game (win or loss) when it came to an end. The action \ud835\udc4e is the policy loss, and the outcome \ud835\udc67 is the value loss. All triples for a game consist of the same outcome \ud835\udc67, and the diferent actions that were played at each state. AlphaGo Zero uses a dual-headed residual network (a convolutional network with extra skip-links between layers, to improve regularization, see Sect. B.2.6 [321, 132]). Policy and value loss contribute equally to the loss function [823]. The network is trained by stochastic gradient descent. L2 regularization is used to reduce overftting. The network has 19 hidden layers, and an input layer and two output layers, for policy and value. The size of the mini-batch for updates is 2048. This batch is distributed over 64 GPU workers, each with 32 data entries. The mini-batch is sampled uniformly over the last 500,000 self-play games (replay bufer). The learning rate started at 0.01 and went down to 0.0001 during self-play. More details of the AlphaGo Zero network are described in [706]. Please note the size of the replay bufer, and the long training time. Go is a complex game, with sparse rewards. Only at the end of a long game the win or loss is known, and attributing this sparse reward to the many individual moves of a game is difcult, requiring many games to even out errors. MCTS is an on-policy algorithm that makes use of guidance in two places: in the downward action selection and in the upward value backup. In AlphaGo Zero the function approximator returns both elements: a policy for the action selection and a value for the backup [706]. For tournament-level self-play to succeed, the training process must (1) cover enough of the state space, must (2) be stable, and must (3) converge. Training targets must be sufciently challenging to learn, and sufciently diverse. The purpose of MCTS is to act as a policy improver in the actor critic setting, to generate learning targets of sufcient quality and diversity for the agent to learn. Let us have a closer look at these aspects, to get a broader perspective on why it was so difcult get self-play to work in Go. 6.2.2.2 Stability and Exploration Self-play has a long history in artifcial intelligence, going back to TD-Gammon, 30 years ago. Let us look at the challenges in achieving a strong level of play. Since in AlphaGo Zero all learning is by reinforcement, the training process must now be even more stable than in AlphaGo, which also used supervised learning from 6.2 Tabula Rasa Self-Play Agents 173 grandmaster games. The slightest problem in overftting or correlation between states can throw of the coverage, correlation, and convergence. AlphaGo Zero uses various forms of exploration to achieve stable reinforcement learning. Let us summarize how stability is achieved. \u2022 Coverage of the sparse state space is improved by playing a large number of diverse games. The quality of the states is further improved by MCTS look-ahead. MCTS searches for good training samples, improving the quality and diversity of the covered states. The exploration part of MCTS should make sure that enough new and unexplored parts of the state space are covered. Dirichlet noise is added at the root node and the \ud835\udc36\ud835\udc5d parameter in the P-UCT formula, that controls the level of exploration, has been set quite high, around 5 (see also Sect. 6.2.1.2). \u2022 Correlation between subsequent states is reduced through the use of an experience replay bufer, as in DQN and Rainbow algorithms. The replay bufer breaks correlation between subsequent training examples. Furthermore, the MCTS search also breaks correlation, by searching deep in the tree to fnd better states. \u2022 Convergence of the training is improved by using on-policy MCTS, and by taking small training steps. Since the learning rate is small, training target stability is higher, and the risk of divergence is reduced. A disadvantage is that convergence is quite slow and requires many training games. By using these measures together, stable generalization and convergence are achieved. Although self-play is conceptually simple, achieving stable and highquality self-play in a game as complex and sparse as Go, required slow training with a large number of games, and quite some hyperparameter tuning. There are many hyperparameters whose values must be set correctly, for the full list, see [706]. Although the values are published [706], the reasoning behind the values is not always clear. Reproducing the AlphaGo Zero results is not easy, and much time is spent in tuning and experimenting to reproduce the AphaGo Zero results [605, 729, 768, 776, 866]. Two Views At this point it is useful to step back and refect on the self-play architecture. There are two diferent views. The one view, planning-centric, which we have followed so far, is of a searcher that is helped by a learned evaluation function (which trains on examples from games played against itself). In addition, there is move-level self-play (opponent\u2019s moves are generated with an inverted replica of itself) and there is tournament-level self-play (by the value learner). The alternative view, learning-centric, is that a policy is learned by generating game examples from self-play. In order for these examples to be of high quality, the policy-learning is helped by a policy improver, a planning function that performs lookahead to create better learning targets (and the planning is performed by making moves by a copy of the player). In addition, there is tournament-level self-play (by the policy learner) and there is move-level self-play (by the policy improver). 174 6 Two-Agent Self-Play The diference in viewpoint is who comes frst: the planning viewpoint favors the searcher, and the learner is there to help the planner; the reinforcement learning viewpoint favors the policy learner, and the planner is there to help improve the policy. Both viewpoints are equally valid, and both viewpoints are equally valuable. Knowing of the other viewpoint deepens our understanding of how these complex self-play algorithms work. This concludes our discussion of the second type of self-play, the example-level, and we move on to the third type: tournament-level self-play. 6.2.3 Tournament-Level Self-Play At the top level, a tournament of self-play games is played between the two (identical) players. The player is designed to increase in strength, learning from the examples at the second level, so that the player can achieve a higher level of play. In tabula rasa self-play, the players start from scratch. By becoming progressively stronger, they also become stronger opponents for each other, and their mutual level of play can increase. A virtuous cycle of ever increasing intelligence will emerge. For this ideal of artifcial intelligence to become reality, many stars have to line up. After TD-Gammon, many researchers have tried to achieve this goal in other games, but were unsuccessful. Tournament-level self-play is only possible when move-level self-play and example-level self-play work. For move-level self-play to work, both players need to have access to the transition function, which must be completely accurate. For example-level self-play to work, the player architecture must be such that it is able to learn a stable policy of a high quality (MCTS and the network have to mutually improve eachother). Tournament-level self-play is the third self-play procedure, where a tournament is created with games starting from easy learning tasks, changing to harder tasks, increasing all the way to world champion level training. This third procedure allows reinforcement learning to transcend the level of play (\u201cintelligence\u201d) of previous teachers. Curriculum Learning As mentioned before, the AlphaGo efort consists of three programs: AlphaGo, AlphaGo Zero, and AlphaZero. The frst AlphaGo program used supervised learning based on grandmaster games, followed by reinforcement learning on self-play games. The second program, AlphaGo Zero, used reinforcement learning only, in a selfplay architecture that starts from zero knowledge. The frst program trained many weeks, yet the second program needed only a few days to become stronger than the frst [706, 703]. 6.2 Tabula Rasa Self-Play Agents 175 Why did the self-play approach of AlphaGo Zero learn faster than the original AlphaGo that could beneft from all the knowledge of Grandmaster games? Why is self-play faster than a combination of supervised and reinforcement learning? The reason is a phenomenon called curriculum learning: self-play is faster because it creates a sequence of learning tasks that are ordered from easy to hard. Training such an ordered sequence of small tasks is quicker than one large unordered task. Curriculum learning starts the training process with easy concepts before the hard concepts are learned; this is, of course, the way in which humans learn. Before we learn to run, we learn to walk; before we learn about multiplication, we learn about addition. In curriculum learning the examples are ordered in batches from easy to hard. Learning such an ordered sequence of batches goes better since understanding the easy concepts helps understanding of the harder concepts; learning everything all at once typically takes longer and may result in lower accuracy.13 6.2.3.1 Self-Play Curriculum Learning In ordinary deep reinforcement learning the network tries to solve a fxed problem in one large step, using environment samples that are not sorted from easy to hard. With examples that are not sorted, the program has to achieve the optimization step from no knowledge to human-level play in one big, unsorted, leap, by optimizing many times over challenging samples where the error function is large. Overcoming such a large training step (from beginner to advanced) costs much training time. In contrast, in AlphaGo Zero, the network is trained in many small steps, starting against a very weak opponent, just as a human child learns to play the game by playing against a teacher that plays simple moves. As our level of play increases, so does the difculty of the moves that our teacher proposes to us. Subsequently, harder problems are generated and trained for, refning the network that has already been pretrained with the easier examples. Self-play naturally generates a curriculum with examples from easy to hard. The learning network is always in lock step with the training target\u2014errors are low throughout the training. As a consequence, training times go down and the playing strength goes up. 6.2.3.2 Supervised and Reinforcement Curriculum Learning Curriculum learning has been studied before in psychology and education science. Selfridge et al. [689] frst connected curriculum learning to machine learning, where they trained the proverbial Cartpole controller. First they trained the controller on long and light poles, while gradually moving towards shorter and heavier poles. Schmidhuber [672] proposed a related concept, to improve exploration for world 13 Such a sequence of related learning tasks corresponds to a meta-learning problem. In metalearning the aim is to learn a new task fast, by using the knowledge learned from previous, related, tasks; see Chap. 9. 176 6 Two-Agent Self-Play Fig. 6.17 Efectiveness of a Sorted Curriculum [833] models by artifcial curiosity. Curriculum learning was subsequently applied to match the order of training examples to the growth in model capacity in diferent supervised learning settings [225, 432, 77]. Another related development is in developmental robotics, where curriculum learning can help to self-organize open-ended developmental trajectories [578], related to intrinsic motivation (see Sect. 8.3.2). The AMIGo approach uses curriculum learning to generate subgoals in hierarchical reinforcement learning [125]. To order the training examples from easy to hard, we need a measure to quantify the difculty of the task. One idea is to use the minimal loss with respect to some of the upper layers of a high-quality pretrained model [833]. In a supervised learning experiment, Weinshall et al. compared the efectiveness of curriculum learning on a set of test images (5 images of mammals from CIFAR100). Figure 6.17 shows the accuracy of a curriculum ordering (green), no curriculum (blue), randomly ordered groups (yellow) and the labels sorted in reverse order (red). Both networks are regular networks with multiple convolutional layers followed by a fully connected layer. The large network has 1,208,101 parameters, the small network has 4,557 parameters. We can clearly see the efectiveness of ordered learning [833]. Procedural Content Generation Finding a good way to order the sequence of examples is often difcult. A possible method to generate a sequence of tasks that are related is by using procedural content generation (PCG) [692, 541]. Procedural content generation uses randomized algorithms to generate images and other content for computer games; the difculty of the examples can often be controlled. It is frequently used to automatically gen6.2 Tabula Rasa Self-Play Agents 177 erate diferent levels in games, so that they do not have to be all created manually by the game designers and programmers [715, 781].14 The Procgen benchmark suite has been built upon procedurally generated games [158]. Another popular benchmark is the General video game AI competition (GVGAI) [479]. Curriculum learning reduces overftting to single tasks. Justesen et al. [388] have used GVGAI to show that a policy easily overfts to specifc games, and that training over a curriculum improves its generalization to levels that were designed by humans. MiniGrid is a procedurally generated world that can be used for hierarchical reinforcement learning [143, 621]. Active Learning Curriculum learning is related also related to active learning. Active learning is a type of machine learning that is in-between supervised and reinforcement learning. Active learning is relevant when labels are in principle available (as in supervised learning) but at a cost. Active learning performs a kind of iterative supervised learning, in which the agent can choose to query which labels to reveal during the learning process. Active learning is related to reinforcement learning and to curriculum learning, and is for example of interest for studies into recommender systems, where acquiring more information may come at a cost [691, 640, 178]. Single-Agent Curriculum Learning Curriculum learning has been studied for many years. A problem is that it is difcult to fnd an ordering of tasks from easy to hard in most learning situations. In twoplayer self-play the ordering comes natural, and the successes have inspired recent work on single-agent curriculum learning. For example, Laterre et al. introduce the Ranked Reward method for solving bin packing problems [455] and Wang et al. presented a method for Morpion Solitaire [824]. Feng et al. use an AlphaZero based approach to solve hard Sokoban instances [239]. Their model is an 8 block standard residual network, with MCTS as planner. They create a curriculum by constructing simpler subproblems from hard instances, using the fact that Sokoban problems have a natural hierarchical structure. This approach was able to solve harder Sokoban instances than had been solved before. Florensa et al. [248] study the generation of goals for curriculum learning using a generator network (GAN). 14 See also generative adversarial networks and deep dreaming, for a connectionist approach to content generation, Sect. B.2.6. 178 6 Two-Agent Self-Play Name Approach Ref TD-Gammon Tabula rasa self-play, shallow network, small alpha-beta search [763] AlphaGo Supervised, self-play, 3\u00d7CNN, MCTS [703] AlphaGo Zero Tabula rasa self-play, dual-head-ResNet, MCTS [706] AlphaZero Tabula rasa self-play, dual-head-ResNet, MCTS on Go, chess, shogi [704] Table 6.2 Self-Play Approaches Conclusion Although curriculum learning has been studied in artifcial intelligence and in psychology for some time, it has not been a popular method, since it is difcult to fnd well-sorted training curricula [518, 519, 827]. Due to the self-play results, curriculum learning is now attracting more interest, see [552, 837]. Work is reported in single-agent problems [552, 239, 198, 455], and in multi-agent games, as we will see in Chap. 7. After this detailed look at self-play algorithms, it is time to look in more detail at the environments and benchmarks for self-play. 6.3 Self-Play Environments Progress in reinforcement learning is determined to a large extent by the application domains that provide the learning challenge. The domains of checkers, backgammon, and especially chess and Go, have provided highly challenging domains, for which substantial progress was achieved, inspiring many researchers. The previous section provided an overview of the planning and learning algorithms. In this section we will have a closer look at the environments and systems that are used to benchmark these algorithms. Table 6.2 lists the AlphaGo and self-play approaches that we discuss in this chapter. First we will discuss the three AlphaGo programs, and then we will list open self-play frameworks. We will start with the frst program, AlphaGo. 6.3.1 How to Design a World Class Go Program? Figure 6.18 shows the playing strength of traditional programs (right panel, in red) and diferent versions of the AlphaGo programs, in blue. We see how much stronger the 2015, 2016, and 2017 versions of AlphaGo are than the earlier heuristic minimax program GnuGo, and two MCTS-only programs Pachi and Crazy Stone. How did the AlphaGo authors design such a strong Go program? Before AlphaGo, the strongest programs used the Monte Carlo Tree Search planning algorithm, without neural networks. For some time, neural networks were considered to be 6.3 Self-Play Environments 179 Fig. 6.18 Performance of AlphaGo Zero [706] Fig. 6.19 All AlphaGo Networks [703] too slow for use as value function in MCTS, and random playouts were used, often improved with small pattern-based heuristics [269, 268, 138, 166, 227]. Around 2015 a few researchers tried to improve performance of MCTS by using deep learning evaluation functions [155, 22, 267]. These eforts were strenghthened by the strong results in Atari [523]. The AlphaGo team also tried to use neural networks. Except for backgammon, pure self-play approaches had not been shown to work well, and the AlphaGo team did the sensible thing to pretrain the network with the games of human grandmasters, using supervised learning. Next, a large number of self-play games were used to further train the networks. In total, no less than three neural networks were used: one for the MCTS playouts, one for the policy function, and one for the value function [703]. 180 6 Two-Agent Self-Play Thus, the original AlphaGo program consisted of three neural networks and used both supervised learning and reinforcement learning. The diagram in Fig. 6.19 illustrates the AlphaGo architecture. Although this design made sense at the time given the state of the art of the feld, and although it did convincingly beat the three strongest human Go players, managing and tuning such a complicated piece of software is quite difcult. The authors of AlphaGo tried to improve performance further by simplifying their design. Could TD-gammon\u2019s elegant self-play-only design be replicated in Go after all? Indeed, a year later a reinforcement learning-only version was ready that learned to play Go from zero knowledge, tabula rasa: no grandmaster games, only self-play, and just a single neural network [706]. Surprisingly, this simpler version played stronger and learned faster. The program was called AlphaGo Zero, since it learned from zero-knowledge; not a single grandmaster game was learned from, nor was there any heuristic domain knowledge hand coded into the program. This new result, tabula rasa learning in Go with a pure self-play design, inspired much further research in self-play reinforcement learning. 6.3.2 AlphaGo Zero Performance In their paper Silver et al. [706] describe that learning progressed smoothly throughout the training. AlphaGo Zero outperformed the original AlphaGo after just 36 hours. The training time for the version of AlphaGo that played Lee Sedol was several months. Furthermore, AlphaGo Zero used a single machine with 4 tensor processing units, whereas AlphaGo Lee was distributed over many machines and used 48 TPUs.15 Figure 6.18 shows the performance of AlphaGo Zero. Also shown is the performance of the raw network, without MCTS search. The importance of MCTS is large, around 2000 Elo points.16 AlphaGo Zero\u2019s reinforcement learning is truly learning Go knowledge from scratch, and, as the development team discovered, it did so in a way similar to how humans are discovering the intricacies of the game. In their paper [706] they published a picture of how this knowledge acquisition progressed (Fig. 6.20). Joseki are standard corner openings that all Go players become familiar with as they learn to play the game. There are beginner\u2019s and advanced joseki. Over the course of its learning, AlphaGo Zero did learn joseki, and it learned them from beginner to advanced. It is interesting to see how it did so, as it reveals the progression of AlphaGo Zero\u2019s Go intelligence. Figure 6.20 shows sequences from games played by the program. Not to anthropomorphize too much,17 but you can see the little program getting smarter. 15 TPU stands for tensor processing unit, a low-precision design specifcally developed for fast neural network processing. 16 The basis of the Elo rating is pairwise comparison [226]. Elo is often used to compare playing strength in board games. 17 Treat as if human 6.3 Self-Play Environments 181 Fig. 6.20 AlphaGo Zero is Learning Joseki in a Curriculum from Easy to Hard [706] The top row shows fve joseki that AlphaGo Zero discovered. The frst joseki is one of the standard beginner\u2019s openings in Go theory. As we move to the right, more difcult joseki are learned, with stones being played in looser confgurations. The bottom row shows fve joseki favored at diferent stages of the self-play training. It starts with a preference for a weak corner move. After 10 more hours of training, a better 3-3 corner sequence is favored. More training reveals more, and better, variations. AlphaGo Zero discovered a remarkable level of Go knowledge during its self-play training process. This knowledge included not only fundamental elements of human Go knowledge, but also nonstandard strategies beyond the scope of traditional Go knowledge. For a human Go player, it is remarkable to see this kind of progression in computer play, reminding them of the time when they discovered these joseki themselves. With such evidence of the computer\u2019s learning, it is hard not to anthropomorphize AlphaGo Zero. 6.3.3 AlphaZero The AlphaGo story does not end with AlphaGo Zero. A year after AlphaGo Zero, a version was created with diferent input and output layers that learned to play chess and shogi (also known as Japanese chess, Fig. 6.21). AlphaZero uses the same MCTS and deep reinforcement learning architecture as for learning to play Go (the only diferences are the input and output layers) [704]. This new program, AlphaZero, beat the strongest chess and shogi programs, Stockfsh and Elmo. Both these programs followed a conventional heuristic minimax design, optimized by hand and 182 6 Two-Agent Self-Play Fig. 6.21 A Shogi Board Fig. 6.22 Elo rating of AlphaZero in Chess, Shogi, and Go [704] machine learning, and improved with many heuristics for decades. AlphaZero used zero knowledge, zero grandmaster games, and zero hand-crafted heuristics, yet it played stronger. The AlphaZero architecture allows not only very strong play, but is also a general architecture, suitable for three diferent games.18 The Elo rating of AlphaZero in chess, shogi, and Go is shown in Fig. 6.22, [706, 703]. AlphaZero is stronger than the other programs. In chess the diference is the smallest. In this feld the program has benefted from a large community of researchers that have worked intensely on improving performance of the heuristic alpha-beta approach. For shogi the diference is larger. General Game Architecture AlphaZero can play three diferent games with the same architecture. The three games are quite diferent. Go is a static game of strategy. Stones do not move and are rarely captured. Stones, once played, are of strategic importance. In chess the pieces 18 Although an AlphaZero version that has learned to play Go, cannot play chess. It has to re-learn chess from scratch, with diferent input and output layers. 6.3 Self-Play Environments 183 Name Type URL Ref. AlphaZero General AlphaZero in Python https://github.com/suragnair/alpha-zerogeneral [768] ELF Game framework https://github.com/pytorch/ELF [776] Leela AlphaZero for Chess, Go https://github.com/LeelaChessZero/lczero [589] PhoenixGo AlphaZero-based Go prog. https://github.com/Tencent/PhoenixGo [866] PolyGames Env. for Zero learning https://github.com/facebookincubator/ Polygames [133] Table 6.3 Self-learning environments move. Chess is a dynamic game where tactics are important. Chess also features sudden death, a check-mate can occur in the middle of the game by capturing the king. Shogi is even more dynamic since captured pieces can be returned to the game, creating even more complex game dynamics. It is testament to the generality of AlphaZero\u2019s architecture, that games that difer so much in tactics and strategy can be learned so successfully. Conventional programs must be purposely developed for each game, with diferent search hyperparameters and diferent heuristics. Yet the MCTS/ResNet self-play architecture is able to learn all three from scratch. 6.3.4 Open Self-Play Frameworks Tabula rasa learning for the game of Go is a remarkable achievement that inspired many researchers. The code of AlphaGo Zero and AlphaZero, however, is not public. Fortunately, the scientifc publications [706, 704] provide many details, allowing other researchers to reproduce similar results. Table 6.3 summarizes some of the self-learning environments, which we will briefy discuss. \u2022 A0G: AlphaZero General Thakoor et al. [768] created a self-play system called AlphaZero General (A0G).19 It is implemented in Python for TensorFlow, Keras, and PyTorch, and suitably scaled down for smaller computational resources. It has implementations for 6\u00d76 Othello, tic tac toe, gobang, and connect4, all small games of signifcantly less complexity than Go. Its main network architecture is a four layer CNN followed by two fully connected layers. The code is easy to understand in an afternoon of study, and is well suited for educational purposes. The project write-up provides some documentation [768]. \u2022 Facebook ELF ELF stands for Extensible Lightweight Framework. It is a framework for game research in C++ and Python [776]. Originally developed for real-time strategy games by Facebook, it includes the Arcade Learning Environment and 19 https://github.com/suragnair/alpha-zero-general 184 6 Two-Agent Self-Play the Darkforest20 Go program [778]. ELF can be found on GitHub.21 ELF also contains the self-play program OpenGo [777], a reimplementation of AlphaGo Zero (in C++). \u2022 Leela Another reimplementation of AlphaZero is Leela. Both a chess and a Go version of Leela exist. The chess version is based on chess engine Sjeng. The Go22 version is based on Go engine Leela. Leela does not come with trained weights of the network. Part of Leela is a community efort to compute these weights. \u2022 PhoenixGo PhoenixGo is a strong self-play Go program by Tencent [866]. It is based on the AlphaGo Zero architecture.23 A trained network is available as well. \u2022 Polygames PolyGames [133] is an environment for Zero-based learning (MCTS with deep reinforcement learning) inspired by AlphaGo Zero. Relevant learning methods are implemented, and bots for hex, Othello, and Havannah have been implemented. PolyGames can be found on GitHub.24 A library of games is provided, as well as a checkpoint zoo of neural network models. 6.3.5 Hands On: Hex in Polygames Example Let us get some hands-on experience with MCTS-based self-play. We will implement the game of Hex with the PolyGames suite. Hex is a simple and fun board game invented independently by Piet Hein and John Nash in the 1940s. Its simplicity makes it easy to learn and play, and also a popular choice for mathematical analysis. The game is played on a hexagonal board, player A wins if its moves connect the right to the left side, and player B wins if top connects to bottom (see Fig. 6.23; image by Wikimedia). A simple page with resources is here;25 extensive strategy and background books have been written about hex [319, 115]. We use hex because it is simpler than Go, to get you up to speed quickly with self-play learning; we will also use PolyGames. Click on the link26 and start by reading the introduction to PolyGames on GitHub. Download the paper [133] and familiarize yourself with the concepts behind Polygames. Clone the repository and build it by following the instructions. Polygames uses PyTorch,27 so install that too (follow the instructions on the Polygames page). 20 https://github.com/facebookresearch/darkforestGo 21 https://github.com/pytorch/ELF 22 https://github.com/gcp/leela-zero 23 https://github.com/Tencent/PhoenixGo 24 https://github.com/facebookincubator/Polygames 25 https://www.maths.ed.ac.uk/~csangwin/hex/index.html 26 https://github.com/facebookincubator/Polygames 27 https://pytorch.org 6.3 Self-Play Environments 185 Fig. 6.23 A Hex Win for Blue Polygames is interfaced via the pypolygames Python package. The games, such as Hex, can be found in src/games and are coded in C++ for speed. The command pypolygames train is used to train a game and a model. The command pypolygames eval is used to use a previously trained model. The command pypolygames human allows a human to play against a trained model. Type python -m pypolygames {train,eval,traineval,human} --help for help with each of the commands train, eval, traineval, or human. A command to start training a Hex model with the default options is: python -m pypolygames train --game_name=\"Hex\" Try loading a pre-trained model from the zoo. Experiment with diferent training options, and try playing against the model that you just trained. When everything works, you can also try training diferent games. Note that more complex games may take (very) long to train. 186 6 Two-Agent Self-Play Summary and Further Reading We will now summarize the chapter, and provide pointers for further reading. Summary For two-agent zero-sum games, when the transition function is given by the rules of the game, a special kind of reinforcement learning becomes possible. Since the agent can perfectly simulate the moves of the opponent, accurate planning far into the future becomes possible, allowing strong policies to be learned. Typically, the second agent becomes the environment. Previously environments were static, but they will now evolve as the agent is learning, creating a virtuous cycle of increasing (artifcial) intelligence in agent (and environment). The promise of this self-play setup is to achieve high levels of intelligence in a specifc feld. The challenges to overcome instability, however, are large, since this kind of self-play combines diferent kinds of unstable learning methods. Both TD-Gammon and AlphaGo Zero have overcome these challenges, and we have described their approach in quite some detail. Self-play is a combination of planning, learning, and a self-play loop. The selfplay loop in AlphaGo Zero uses MCTS to generate high-quality examples, which are used to train the neural net. This new neural net is then used in a further self-play iteration to generate more difcult games, and refne the network further (and again, and again, and again). Alpha(Go) Zero thus learns starting at zero knowledge, tabula rasa. Self-play makes use of many reinforcement learning techniques. In order to ensure stable learning, exploration is important. MCTS is used for deep planning. The exploration parameter in MCTS is set high, and convergent training is achieved by a low learning rate \ud835\udefc. Because of these hyperparameter settings, and because of the sparse rewards in Go, many games have to be played. The computational demands of stable self-play are large. AlphaGo Zero uses function approximation of two functions: value and policy. Policy is used to help guide action selection in the P-UCT selection operation in MCTS, and value is used instead of random playouts to provide the value function at the leaves of the MCTS tree. MCTS has been changed signifcantly to work in the self-play setting. Gone are the random playouts that gave MCTS the name Monte Carlo, and much of the performance is due to a high-quality policy and value approximation residual network. Originally, the AlphaGo program (not AlphaGo Zero) used grandmaster games in supervised learning in addition to using reinforcement learning; it started from the knowledge of grandmaster games. Next came AlphaGo Zero, which does not use grandmaster games or any other domain specifc knowledge. All learning is based on reinforcement learning, playing itself to build up the knowledge of the game from zero knowledge. A third experiment has been published, called AlphaZero 6.3 Self-Play Environments 187 (without the \u201cGo\u201d). In this paper the same network architecure and MCTS design (and the same learning hyperparameters) were used to learn three games: chess, shogi, and Go. This presented the AlphaZero architecture as a general learning architecture, stronger than the best alpha-beta-based chess and shogi programs. Interestingly, the all-reinforcement learning AlphaGo Zero architecture was not only stronger than the supervised/reinforcement hybrid AlphaGo, but also faster: it learned world champion level play in days, not weeks. Self-play learns quickly because of curriculum learning. It is more efcient to learn a large problem in many small steps, starting with easy problems, ending with hard ones, then in one large step. Curriculum learning works both for humans and for artifcial neural networks. Further Reading One of the main interests of artifcial intelligence is the study of how intelligence emerges out of simple, basic, interactions. In self-learning systems this is happening [464]. The work on AlphaGo is a landmark achievement in artifcial intelligence. The primary sources of information for AlphaGo are the three AlphaGo/AlphaZero papers by Silver et al. [703, 706, 704]. The systems are complex, and so are the papers and their supplemental methods sections. Many blogs have been written about AlphaGo that are more accessible. A movie has been made about AlphaGo.28 There are also explanations on YouTube.29 A large literature on minimax and minimax enhancements for games exists, an overview is in [600]. A book devoted to building your own state-of-the-art self-learning Go bot is Deep Learning and the Game of Go by Pumperla and Ferguson [612], which came out before PolyGames [133]. MCTS has been a landmark algorithm by itself in artifcial intelligence [164, 117, 419]. In the contexts of MCTS, many researchers worked on combining MCTS with learned patterns, especially to improve the random rollouts of MCTS. Other developments, such as asymmetrical and continuous MCTS, are [531, 530], or parallelizations such as [516]. Supervised learning on grandmaster games was used to improve playouts and also to improve UCT selection. Gelly and Silver published notable works in this area [269, 708, 268]. Graf et al. [284] describe experiments with adaptive playouts in MCTS with deep learning. Convolutional neural nets were also used in Go by Clark and Storkey [154, 155], who had used a CNN for supervised learning from a database of human professional games, showing that it outperformed GNU Go and scored wins against Fuego, a strong open source Go program [227] based on MCTS without deep learning. 28 https://www.alphagomovie.com 29 https://www.youtube.com/watch?v=MgowR4pq3e8 188 6 Two-Agent Self-Play Tesauro\u2019s success inspired many others to try temporal diference learning. Wiering et al. and Van der Ree [840, 794] report on self-play and TD learning in Othello and Backgammon. The program Knightcap [60, 61] and Beal et al. [62] also use temporal diference learning on evaluation function features. Arenz [25] applied MCTS to chess. Heinz reported on self-play experiments in chess [327]. Since the AlphaGo results many other applications of machine learning have been shown to be successful. There is interest in theoretical physics [644, 583], chemistry [385], and pharmacology, specifcally for retrosynthetic molecular design [687] and drug design [803]. High-profle results have been achieved by AlphaFold, a program that can predict protein structures [385, 690]. To learn more about curriculum learning, see [837, 77, 502, 248]. Wang et al. [823] study the optimization target of a dual-headed self-play network in AlphaZeroGeneral. The success of self-play has led to interest in curriculum learning in singleagent problems [239, 198, 213, 455]. The relation between classical single-agent and two-agent search is studied by [664]. Exercises To review your knowledge of self-play, here are some questions and exercises. We start with questions to check your understanding of this chapter. Each question is a closed question where a simple, one sentence answer is possible. Questions 1. What are the diferences between AlphaGo, AlphaGo Zero, and AlphaZero? 2. What is MCTS? 3. What are the four steps of MCTS? 4. What does UCT do? 5. Give the UCT formula. How is P-UCT diferent? 6. Describe the function of each of the four operations of MCTS. 7. How does UCT achieve trading of exploration and exploitation? Which inputs does it use? 8. When \ud835\udc36\ud835\udc5d is small, does MCTS explore more or exploit more? 9. For small numbers of node expansions, would you prefer more exploration or more exploitation? 10. What is a double-headed network? How is it diferent from regular actor critic? 11. Which three elements make up the self-play loop? (You may draw a picture.) 12. What is tabula rasa learning? 13. How can tabula rasa learning be faster than reinforcement learning on top of supervised learning of grandmaster games? 14. What is curriculum learning? 6.3 Self-Play Environments 189 Implementation: New or Make/Undo You may have noticed that the minimax and MCTS pseudocode in the fgures lacks implementation details for performing actions, to arrive at successor states. Such board manipulation and move making details are important for creating a working program. Game playing programs typically call the search routine with the current board state, often indicated with parameter n for the new node. This board can be created and allocated anew in each search node, in a value-passing style (local variable). Another option is to pass a reference to the board, and to apply a makemove operation on the board, placing the stone on the board before the recursive call, and an undomove operation removing the stone from the board when it returns back out of the recursion (global variable). This reference-passing style may be quicker if allocating the memory for a new board is an expensive operation. It may also be more difcult to implement correctly, since the makemove/undomove protocol must be followed strictly on all relevant places in the code. If capture moves cause many changes to the board, then these must be remembered for the subsequent undo. For parallel implementations in a shared memory at least all parallel threads must have their own copy of a value-passing style board. (On a distributed memory cluster the separate machines will have their own copy of the board by virtue of the distributed memory.) Exercises For the programming exercises we use PolyGames. See the previous section on how to install PolyGames. If training takes a long time, consider using the GPU support of Polygames and Pytorch. 1. Hex Install PolyGames and train a Hex player with self-play. Experiment with diferent board sizes. Keep the training time constant, and draw a graph where you contrast playing strength against board size. Do you see a clear correlation that the program is stronger on smaller boards? 2. Visualize Install the visualization support torchviz. Visualize the training process using the draw_model script. 3. Hyperparameters Use diferent models, and try diferent hyperparameters, as specifed in the PolyGames documentation. 4. MCTS Run an evaluation tournament from the trained Hex model against a pure MCTS player. See below for tips on make and undo of moves. How many nodes can MCTS search in a reasonable search time? Compare the MCTS Hex player against the self-play player. How many games do you need to play to have statistically signifcant results? Is the random seed randomized or fxed? Which is stronger: MCTS or trained Hex? Chapter 7 Multi-Agent Reinforcement Learning On this planet, in our societies, millions of people live and work together. Each individual has their own individual set of goals and performs their actions accordingly. Some of these goals are shared. When we want to achieve shared goals we organize ourselves in teams, groups, companies, organizations and societies. In many intelligent species\u2014humans, mammals, birds, insects\u2014impressive displays of collective intelligence emerge from such organization [848, 348, 686]. We are learning as individuals, and we are learning as groups. This setting is studied in multi-agent learning: through their independent actions, agents learn to interact and to compete and cooperate with other agents, and form groups. Most research in reinforcement learning has focused on single agent problems. Progress has been made in many topics, such as path-fnding, robot locomotion, and video games. In addition, research has been performed in two-agent problems, such as competitive two-person board games. Both single-agent and two-agent problems are questions of optimization. The goal is to fnd the policy with the highest reward, the shortest path, and the best moves and counter moves. The basic setting is one of reward optimization in the face of natural adversity or competitors, modeled by the environment. As we move closer toward modeling real-world problems, we encounter another category of sequential decision making problems, and that is the category of multiagent decision problems. Multi-agent decision making is a difcult problem, Agents that share the same goal might collaborate, and fnding a policy with the highest reward for oneself or for the group may include achieving win/win solutions with other agents. Coalition forming and collusion are an integral part of the feld of multi-agent reinforcement learning. In real-world decision making both competition and cooperation are important. If we want our agents to behave realistically in settings with multiple agents, then they should understand cooperation in order to perform well. From a computational perspective, studying the emergence of group behavior and collective intelligence is challenging: the environment for the agents consists of many other agents; goals may move, many interactions have to be modeled in order to be understood, and the world to be optimized against is constantly changing. 191 192 7 Multi-Agent Reinforcement Learning A lack of compute power has held back experimental research in multi-agent reinforcement learning for some time. The recent growth in compute power and advances in deep reinforcement learning methods are making progress increasingly possible. Multi-agent decision making problems that have been studied experimentally include the games of bridge, poker, Diplomacy, StarCraft, Hide and Seek, and Capture the Flag. To a large extent, the algorithms for multi-agent reinforcement learning are still being developed. This chapter is full of challenges and development. We will start by reviewing the theoretical framework and defning multi-agent problems. We will look deeper at cooperation and competition, and introduce stochastic games, extensive-form games, and the Nash equilibrium and Pareto optimality. We will discuss population-based methods and curriculum learning in multi-player teams. Next, we will discuss environments on which multi-agent reinforcement learning methods can be tested, such as poker and StarCraft. In these games, often team structure plays an important role. The next chapter discusses hierarhical methods, which can be used to model team structures. This chapter is concluded, as usual, with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Competition \u2022 Cooperation \u2022 Team learning Core Problem \u2022 Find efcient methods for large competitive and cooperative multi-agent problems Core Algorithms \u2022 Counterfactual regret minimization (Sect. 7.2.1) \u2022 Population-based algorithms (Sect. 7.2.3.1) 7.1 Multi-Agent Problems 193 Self-Driving Car To illustrate the key components of multi-agent decision making, let us assume a self driving car is on its way to the supermarket and is approaching an intersection. What action should it take? What is the best outcome for the car? For other agents? For society? The goal of the car is to exit the intersection safely and reach the destination. Possible decisions are going straight or turning left or right into another lane. All these actions consist of sub-decisions. At each time step, the car can move by steering, accelerating and braking. The car must be able to detect objects, such as trafc lights, lane markings, and other cars. Furthermore, we aim to fnd a policy that can control the car to make a sequence of manoeuvres to achieve the goal. In a decision-making setting such as this, two additional challenges arise. The frst challenge is that we should be able to anticipate the actions of other agents. During the decision-making process, at each time step, the robot car should consider not only the immediate value of its own current action but also adapt to consequences of actions by other agents. (For example, it would not be good to choose a certain direction that appears safe early on, stick to it, and not adapt the policy as new information comes in, such as a car heading our way.) The second challenge is that these other agents, in turn, may anticipate the actions of our agent in choosing their actions. Our agent needs to take their anticipatory behavior into account in its own policy\u2014recursion at the policy level. Human drivers, for example, often predict likely movements of other cars and then take action in response (such as giving way, or accelerating while merging into another lane).1 Multi-agent reinforcement learning addresses the sequential decision making problem of multiple autonomous agents that operate in a common stochastic environment, each of which aims to maximise their own long-term reward by interacting with the environment and with other agents. Multi-agent reinforcement learning combines the felds of multi-agent systems [697, 847] and reinforcement learning. 7.1 Multi-Agent Problems We have seen impressive results of single-agent reinforcement learning in recent years. In addition, important results have been achieved in games such as Go and poker, and in autonomous driving. These application domains involve the participation of more than one agent. The study of interactions between more than one agent has a long history in felds such as economics and social choice. 1 Human drivers have a theory of mind of other drivers. Theory of mind, and the related concept of mirror neurons [258], are a psychological theory of empathy and understanding, that allows a limited amount of prediction of future behavior. Theory of mind studies how individuals simulate in their minds the actions of others, including their simulation of our actions (and of our simulations, etc.) [54, 332]. 194 7 Multi-Agent Reinforcement Learning Multi-agent problems introduce many new kinds of possibilities, such as cooperation and simultaneous actions. These phenomena invalidate some of the assumptions of the theory behind single-agent reinforcement learning, and new theoretical concepts have been developed. To begin our study of multi-agent problems, we will start with game theory. Game Theory Game theory is the study of strategic interaction among rational decision-making agents. Game theory originally addressed only the theory of two-person zero-sum games, and was later extended to cooperative games of multiple rational players, such as simultaneous action, non-zero sum, and imperfect-information games. A classic work in game theory is John von Neumann and Oskar Morgenstern\u2019s Theory of Games and Economic Behavior [818, 819], published frst in 1944. This book has laid the foundation for the mathematical study of economic behavior and social choice. Game theory has been instrumental in developing the theory of common goods and the role of government in a society of independent self-interested rational agents. In mathematics and artifcial intelligence, game theory has given a formal basis for the computation of strategies in multi-agent systems. The Markov decision process that we have used to formalize single-agent reinforcement learning assumes perfect information. Most multi-agent reinforcement learning problems, however, are imperfect information problems, where some of the information is private, or moves are simultaneous. We will have to extend our MDP model appropriately to be able to model imperfect information. Stochastic Games and Extensive-Form Games One direct generalization of MDP that captures the interaction of multiple agents is the Markov game, also known as the stochastic game [695]. Described by Littman [483], the framework of Markov games has long been used to express multi-agent reinforcement learning algorithms [452, 789]. This multi-agent version of an MDP is defned as follows [870]. At time \ud835\udc61, each agent \ud835\udc56 \u2208 \ud835\udc41 executes an action \ud835\udc4e\ud835\udc56 \ud835\udc61, for the state \ud835\udc60\ud835\udc61 of the system. The system then transitions to state \ud835\udc60\ud835\udc61+1, and rewards each agent \ud835\udc56 with reward \ud835\udc45\ud835\udc56 \ud835\udc4e\ud835\udc61 (\ud835\udc60\ud835\udc61, \ud835\udc60\ud835\udc61+1). The goal of agent \ud835\udc56 is to optimize its own long-term reward, by fnding the policy \ud835\udf0b\ud835\udc56 : \ud835\udc46 \u2192 E(\ud835\udc34\ud835\udc56) as a mapping from the state space to a distribution over the action space, such that \ud835\udc4e\ud835\udc56 \ud835\udc61 \u223c \ud835\udf0b\ud835\udc56(\u00b7|\ud835\udc60\ud835\udc61). Then, the value-function \ud835\udc49\ud835\udc56 : \ud835\udc46 \u2192 \ud835\udc45 of agent \ud835\udc56 becomes a function of the joint policy \ud835\udf0b : \ud835\udc46 \u2192 E(\ud835\udc34) defned as \ud835\udf0b(\ud835\udc4e|\ud835\udc60) = \u03a0\ud835\udc56\u2208\ud835\udc41 \ud835\udf0b\ud835\udc56(\ud835\udc4e\ud835\udc56|\ud835\udc60). To visualize imperfect information in multiple agents, new diagrams are needed. Figure 7.1 shows three schematic diagrams. First, in (a), the familiar agent/environment diagram is shown that we used in earlier chapters. Next, in (b), the multi-agent version of this diagram for Markov games is shown. Finally, in (c) an extensive-form game tree is shown [870]. Extensive form game trees are introduced to model im7.1 Multi-Agent Problems 195 Fig. 7.1 Multi-Agent Models [870] perfect information. Choices by the agent are shown as solid links, and the hidden private information of the other agent is shown dashed, as the information set of possible situations. In (a), the agent observes the state \ud835\udc60, performs action \ud835\udc4e, and receives reward \ud835\udc5f from the environment. In (b), in the Markov game, all agents choose their actions \ud835\udc4e\ud835\udc56 simultaneously and receiving their individual reward \ud835\udc5f\ud835\udc56. In (c), in a two-player extensive-form game, the agents make decisions on choosing actions \ud835\udc4e\ud835\udc56. They receive their individual reward \ud835\udc5f\ud835\udc56(\ud835\udc67) at the end of the game, where \ud835\udc67 is the score of a terminal node for the branch that resulted from the information set. The information set is indicated with a dotted line, to signal stochastic behavior, the environment (or other agent) chooses an unknown outcome amongst the dotted actions. The extensive-form notation is designed for expressing imperfect information games, where all possible (unknown) outcomes are represented in the information set. In order to calculate the value function, the agent has to regard all possible diferent choices in the information sets; the unknown choices of the opponents create a large state space. Competitive, Cooperative, and Mixed Strategies Multi-agent reinforcement learning problems fall into three groups: problems with competitive behavior, with cooperative behavior, and with mixed behavior. In the competitive setting, the reward of the agents sums up to zero. A win for one agent is a loss for another. In the cooperative setting, agents collaborate to optimize a common long-term reward. A win for one agent is a win for all agents (example: an embankment being built around an area prone to fooding; the embankment benefts all agents in the area). The mixed setting involves both cooperative and competitive agents, with so-called general-sum rewards, some action may lead to win/win, some other to win/loss. 196 7 Multi-Agent Reinforcement Learning These three behaviors are a useful guide to navigate the landscape of multi-agent algorithms, and we will do so in this chapter. Let us have a closer look at each type of behavior. 7.1.1 Competitive Behavior One of the hallmarks of the feld of game theory is a result by John Nash, who defned the conditions for a stable (and in a certain sense optimal) solution among multiple rational non-cooperative agents. The Nash equilibrium is defned as a situation in which no agent has anything to gain by changing its own strategy. For two competitive agents the Nash equilibrium is the minimax strategy. In single-agent reinforcement learning the goal is to fnd the policy that maximizes the cumulative future reward of the agent. In multi-agent reinforcement learning the goal is to fnd a combined policy of all agents that simultaneously achieves that goal: the multi-policy that for each agent maximizes their cumulative future reward. If you have a set of competitive (near-)zero exploitability strategies this is called a Nash equilibrium. The Nash equilibrium characterizes an equilibrium point \ud835\udf0b\u2605, from which none of the agents has an incentive to deviate. In other words, for any agent \ud835\udc56 \u2208 \ud835\udc41, the policy \ud835\udf0b\ud835\udc56,\u2605 is the best-response to \ud835\udf0b\u2212\ud835\udc56,\u2605, where \u2212\ud835\udc56 are all agents except \ud835\udc56 [332]. An agent that follows a Nash strategy is guaranteed to do no worse than tie, against any other opponent strategy. For games of imperfect information or chance, such as many card games, this is an expected outcome. Since the cards are randomly dealt, there is no theoretical guarantee that a Nash strategy will win or draw every single hand, although on average, it cannot do worse than tie against the other agents. If the opponents also play a Nash strategy then all will tie. If the opponents make mistakes, however, then they can lose some hands, allowing the Nash equilibrium strategy to win. Such a mistake by the opponents would be a deviation from the Nash strategy, following a hunch or other non-rational reason, despite having no theoretical incentive to do so. A Nash equilibrium plays perfect defence. It does not try to exploit the opponent strategy\u2019s faws, and instead just wins when the opponent makes mistakes [103, 451]. The Nash strategy gives the best possible outcome we can achieve against our adversaries when they work against us. In the sense that a Nash strategy is on average unbeatable, it is considered to be an optimal strategy, and solving a game is equivalent to computing a Nash equilibrium. In a few moments we will introduce a method to calculate Nash strategies, called counterfactual regret minimization, but we will frst look at cooperation. 7.1 Multi-Agent Problems 197 Fig. 7.2 Pareto Frontier of Production-Possibilties 7.1.2 Cooperative Behavior In single-agent reinforcement learning, reward functions return scalar values: an action results in a win or a loss, or a single numeric value. In multi-agent reinforcement learning the reward functions may still return a scalar value, but the functions may be diferent for each agent; the overall reward function is a vector of the individual rewards. In fully cooperative stochastic games, the individual rewards are the same, and all agents have the same goal: to maximize the common return. When choices in our problem are made in a decentralized manner by a set of individual decision makers, the problem can be modeled naturally as a decentralized partially observable Markov decision process (dec-POMDP) [569]. Large dec-POMDPs are hard to solve; in general dec-POMDPs are known to be NEXPcomplete. These problems are not solvable with polynomial-time algorithms and searching directly for an optimal solution in the policy space is intractable [82]. A central concept in cooperative problems is Pareto efciency, after the work of Vilfredo Pareto, who studied economic efciency and income distributions. The Pareto efcient solution is the situation where no cooperative agent can be better of without making at least one other agent worse of. All non-Pareto efcient combination are dominated by the Pareto efcient solution. The Pareto frontier is the set of all Pareto efcient combinations, usually drawn as a curve. Figure 7.2 shows a situation where we can choose to produce diferent quantities of two separate goods, item 1 and item 2 (image by Wikimedia). The grey items close to the origin are choices for which better choices exist. The red symbols all represent production combinations that are deemed more favorable by consumers [700]. In multi-objective reinforcement learning, diferent agents can have diferent preferences. A policy is Pareto-optimal when deviating from the policy will make 198 7 Multi-Agent Reinforcement Learning at least one agent worse of. We will compare Nash and Pareto soon by looking at how they are related in the prisoner\u2019s dilemma (Table 7.1). The Pareto optimum is the best possible outcome for us where we do not hurt others, and others do not hurt us. It is a cooperative strategy. Pareto equilibria assume communication and trust. In some sense, it is the opposite of the noncooperative Nash strategy. Pareto calculates the situation for an cooperative world, Nash for an competitive world. Multi-objective reinforcement learning Many real-world problems involve the optimization of multiple, possibly conficting objectives. In a fully cooperative setting, all agents share such a common reward function. When the reward function consists of diferent values for each agent, the problem is said to be multi-objective: diferent agents may have diferent preferences or objectives. Note that in principle single-agent problems can also have multiple ojectives, although in this book all single agent problems have had single, scalar, rewards. Multi-objective problems become especially relevant in multi-agent problems, where diferent agents may have diferent reward preferences. In decentralized decision processes, the agents must communicate their preferences to eachother. Such heterogeneity necessitates the incorporation of communication protocols into multi-agent reinforcement learning, and the analysis of communication-efcient algorithms [317, 637]. Multi-objective reinforcement learning [635, 292, 333, 618] is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, such as one for each agent. A multi-objective reinforcement learning algorithm optimizes multiple objectives at the same time, and algorithms such as Pareto Q-learning have been introduced for this purpose [802, 634]. 7.1.3 Mixed Behavior Starting as a theory for non-cooperative games, game theory has progressed to include cooperative games, where it analyzes optimal strategies for groups of individuals, assuming that they can communciate or enforce agreements between them about proper strategies. The prisoner\u2019s dilemma is a well-known example of this type of problem [433].2 The prisoner\u2019s dilemma is thought to be invented by John Nash, although in a more neutral (non cops and robbers) setting. The prisoner\u2019s dilemma is as follows (see Table 7.1). Two burglars, called Row and Column, have been arrested for breaking into a bank. Row and Column understand that they cannot trust the other person, hence they operate in a non-cooperative setting. The police ofcer is ofering the following options: If you both confess, then 2 https://plato.stanford.edu/entries/prisoner-dilemma/ 7.1 Multi-Agent Problems 199 Confess Silent Defect Cooperate Confess (\u22125, \u22125) (0, \u221210) Defect Nash Silent (\u221210, 0) (\u22122, \u22122) Cooperate Pareto Table 7.1 Prisoner\u2019s Dilemma you will both get a light sentence of 5 years. If you both keep quiet, then I will only have evidence to get you a short sentence of 2 years. However, if one of you confesses, and the other stays silent, then the confessor will walk free, and the one keeping quiet goes to prison for 10 years. Please tell me your choice tomorrow morning. This leaves our two criminals with a tough, but clear, choice. If the other stays silent, and I confess, then I walk free; if I also stay silent, then we get 2 years in prison, so confessing is better for me in this case. If the other confesses, and I also confess, then we both get 5 years; if I then stay silent, then I get 10 years in prison, so confessing is again better for me. Whatever the other chooses, confessing gives the lighter sentence, and since they can not coordinate their action, they will each independently confess. Both will get 5 years in prison, even though both would have only gotten 2 years if both would have stayed silent. The police is happy, since both confess and the case is solved. If they would have been able to communicate, or if they would have trusted eachother, then both would have stayed silent, and both would have gotten of with a 2 year sentence. The dilemma faced by the criminals is that, whatever the other does, each is better of confessing than remaining silent. However, both also know that if the other could have been trusted, or if they could have coordinated their answer, then a better outcome would have been within reach. The dilemma illustrates individual self-interest against the interest of the group. In the literature on game theory, confessing is also known as defecting, and staying silent is the cooperative choice. The confess/confess situation (both defect) is the optimal non-cooperative strategy, the Nash equilibrium, because for each agent that stays silent a strategy exists where the sentence will be made worse (10 years) when the other agent confesses. Hence the agent will not stay silent but confess, so as to limit the loss. Silent/silent (both cooperate) is Pareto optimal at 2 years for both, since going to all other cases will make at least one agent worse of. The Nash strategy is non-cooperative, non-communication, non-trust, while Pareto is the cooperative, communication, trust, outcome. 200 7 Multi-Agent Reinforcement Learning Iterated Prisoner\u2019s Dilemma The prisoner\u2019s dilemma is a one-time game. What would happen if we play this game repeatedly, being able to identify and remember the choices of our opponent? Could some kind of communication or trust arise, even if the setting is initially non-cooperative? This question is answered by the iterated version of the prisoner\u2019s dilemma. Interest in the iterated prisoner\u2019s dilemma grew after a series of publications following a computer tournament [36, 37]. The tournament was organized by political scientist Robert Axelrod, around 1980. Game theorists were invited to send in computer programs to play iterated prisoner\u2019s dilemmas. The programs were organized in a tournament and played eachother hundreds of times. The goal was to see which strategy would win, and if cooperation would emerge in this simplest of settings. A range of programs were entered, some using elaborate response strategies based on psychology, while others used advanced machine learning to try to predict the opponent\u2019s actions. Surprisingly, one of the simplest strategies won. It was submitted by Anatol Rapoport, a mathematical psychologist who specialized in the modeling of social interaction. Rapoport\u2019s program played a strategy known as tit for tat. It would start by cooperating (staying silent) in the frst round, and in the next rounds if would play whatever the opponent did in the previous round. Tit for tat thus rewards cooperation with cooperation, and punishes defecting with defection\u2014hence the name. Tit for tat wins if it is paired with a cooperative opponent, and does not loose too much by stubbornly cooperating with a non-cooperative opponent. In the long run, it either ends up in the Pareto optimum or in the Nash equilibrium. Axelrod attributes the success of tit for tat to a number of properties. First, it is nice, that is, it is never the frst to defect. In his tournament, the top eight programs played nice strategies. Second, tit for tat is also retaliatory, it is difcult to exploit the strategy by the non-cooperative strategies. Third, tit for tat is forgiving, when the opponent plays nice, it rewards the play by reverting to being nice, being willing to forget past non-cooperative behavior. Finally, the rule has the advantage of being clear and predictable. Others easily learn its behavior, and adapt to it, by being cooperative, leading to the mutual win/win Pareto optimum. 7.1.4 Challenges After this brief overview of theory, let us see how well practical algorithms are able to solve multi-agent problems. With the impressive results in single-agent and two-agent reinforcement learning, the interest (and the expectations) in multiagent problems have increased. In the next section, we will have a closer look at algorithms for multi-agent problems, but let us frst look at the challenges that these algorithms face. 7.1 Multi-Agent Problems 201 Multi-agent problems are studied in competitive, cooperative, and mixed settings. The main challenges faced by multi-agent reinforcement learning are threefold: (1) partial observability, (2) nonstationary environments, and (3) large state space. All three of these aspects increase the size of the state space. We will discuss these challenges in order. 7.1.4.1 Partial Observability Most multi-agent settings are imperfect information settings, where agents have some private information that is not revealed to other agents. The private information can be in the form of hidden cards whose value is unknowns, such as in poker, blackjack or bridge. In real time strategy games players often do not see the entire map of the game, but parts of the world are obscured. Another reason for imperfect information can be that the rules of the game allow simultaneous moves, such as in Diplomacy, where all agents are determining their next action at the same time, and agents have to act without full knowledge of the other actions. All these situations require that all possible states of the world have to considered, increasing the number of possible states greatly in comparison to perfect information games. Imperfect information is best expressed as an extensive-form game. In fact, given the size of most multi-agent systems, it quickly becomes unfeasible to communicate and keep track of all the information of all agents, even if all agents would make their state and intentions public (which they rarely do). Imperfect information increases the size of the state space, and computing the unknown outcomes quickly becomes unfeasible. 7.1.4.2 Nonstationary Environments Moreover, as all agents are improving their policies according to their own interests concurrently, the agents are faced with a nonstationary environment. The environment\u2019s dynamics are determined by the joint action space of all agents, in which the best policies of agents depend on the best policies of the other agents. This mutual dependence creates an unstable situation. In a single agent setting a single state needs to be tracked to calculate the next action. In a multi-agent setting, all states and all agents\u2019 policies need to be taken into account, and mutually so. Each agent faces a moving target problem. In multi-agent reinforcement learning the agents learn concurrently, updating their behavior policies concurrently and often simultaneously [332]. Actions taken by one agent afect the reward of other agents, and therefore of the next state. This invalidates the Markov property, which states that all information that is necessary to determine the next state is present in the current state and the agent\u2019s action. The powerful arsenal of single-agent reinforcement theory must be adapted before it can be used. 202 7 Multi-Agent Reinforcement Learning To handle nonstationarity, agents must account for the joint action space of the other agents\u2019 actions. The size of the space increases exponentially with the number of agents. In two-agent settings the agent has to consider all possible replies of a single opponent to each of its own moves, increasing the state space greatly. In multi-agent settings the number of replies increases even more, and computing solutions to these problems quickly becomes quite expensive. A large number of agents complicates convergence analysis and increases the computational demands substantially. On the other hand, agents may learn from each other, line up their goals, and collaborate. Collaboration and group forming reduce the number of independent agents that must be tracked. 7.1.4.3 Large State Space In addition to the problems caused by partial observability and nonstationarity, the size of the state space is signifcantly increased in a multi-agent setting, simply because every additional agent exponentially increases the state space. Furthermore, the action space of multi-agent reinforcement learning is a joint action space whose size increases exponentially with the number of agents. The size of the joint action space often causes scalability issues. Solving such a large state space to optimality is infeasible for all but the smallest multi-agent problems, and much work has been done to create models (abstractions) that make simplifying, but sometimes realistic, assumptions to reduce the size of the state space. In the next section we will look at some of these assumptions. 7.2 Multi-Agent Reinforcement Learning Agents In the preceding section the problem of multi-agent reinforcement learning has been introduced, as well as game theory and the links with social choice theory. We have also seen that the introduction of multiple agents has, unsurprisingly, increased the size of the state space even more than for single-agent state spaces. Recent work has introduced some approaches for solving multi-agent problems, which we will introduce here. First, we will discuss an approach based on planning, with the name Counterfactual regret minimization (CFR). This algorithm is successful for computing complex social choice problems, such as occur in the game of poker. CFR is suitable for competitive multi-agent problems. Second, we will discuss cooperative reinforcement learning methods that are also suitable for mixed multi-agent problems. Third, we discuss population-based approaches such as evolutionary strategies and swarm computing [42, 252, 346]. These approaches are inspired by computional behavior that occurs in nature, such as in focks of birds and societies of insects [95, 7.2 Multi-Agent Reinforcement Learning Agents 203 207, 97]. Such methods are well-known from single-agent optimization problems; indeed, population-based methods are successful in solving many complex and large single-agent optimization problems (including stochastic gradient descent optimization) [653]. In this section, we will see that evolutionary methods are also a natural match for mixed multi-agent problems, although they are typically used for cooperative problems with many homogeneous agents. Finally, we will discuss an approach based on multi-play self-learning, in which evolutionary and hierarchical aspects are used. Here diferent groups of reinforcement learning agents are trained against eachother. This approach has been highly successful in the games of Capture the Flag and StarCraft, and is suitable for mixed multi-agent problems. Let us start with counterfactual regret minimization. 7.2.1 Competitive Behavior The frst setting that we will discuss is the competitive setting. This setting is still close to single and two-agent reinforcement learning, that are also based on competition. 7.2.1.1 Counterfactual Regret Minimization Counterfactual regret minimization (CFR) is an iterative method for approximating the Nash equilibrium of an extensive-form game [881]. CFR is suitable for imperfect information games (such as poker) and computes a strategy that is (on average) non-exploitable, and that is therefore robust in a competitive setting. Central in counterfactual regret minimization is the notion of regret. Regret is the loss in expected reward that an agent sufers for not having selected the best strategy, with respect to fxed choices by the other players. Regret can only be known in hindsight. We can, however, statistically sample expected regret, by averaging the regret that did not happen. CFR fnds the Nash equilibrium by comparing two hypothetical players against eachother, where the opponent chooses the action that minimizes our value. CFR is a statistical algorithm that converges to a Nash equilibrium. Just like minimax, it is a self-play algorithm that fnds the optimal strategy under the assumption of optimal play by both sides. Unlike minimax, it is suitable for imperfect information games, where information sets describe a set of possible worlds that the opponent may hold. Like MCTS it samples, repeating this process for billions of games, improving its strategy each time. As it plays, it gets closer and closer towards an optimal strategy for the game: a strategy that can do no worse than tie against any opponent [379, 749, 451]. The quality of the strategy that it computes is measured by its exploitability. Exploitability is the maximum amount that a perfect counter-strategy could win (on expectation) against it. 204 7 Multi-Agent Reinforcement Learning Although the Nash equilibrium is a strategy that is theoretically proven to be not exploitable, in practice, typical human play is far from the theoretical optimum, even for top players [112]. Poker programs based on counterfactual regret minimization started beating the world\u2019s best human players in heads-up limit hold\u2019em in 2008, even though these programs programs were still very much exploitable by this worst-case measure [379]. Many papers on counterfactual regret minimization are quite technical, and the codes for algorithms are too long to explain here in detail. CFR is in important algorithm that is essential for understanding progress in poker. To make the work on poker and CFR more accessible, introductory papers and blogs have been written. Trenner [784] has written a blog3 in which CFR is used to play Kuhn poker, one of the simplest Poker variants. The CFR pseudocode and the function that calls it iteratively are shown in Fig. 7.1; for the other routines, see the blog. The CFR code works as follows. First it checks for being in a terminal state and returns the payof, just as a regular tree traversal code does. Otherwise, it retrieves the information set and the current regret-matching strategy. It uses the reach probability, which is the probability that we reach the current node according to our strategy in the current iteration. Then CFR loops over the possible actions (lines 17\u201324), computes the new reach probabilities for the next game state and calls itself recursively. As there are 2 players taking turns in Kuhn poker, the utility for one player is exactly \u22121 times the utility for the other, hence the minus sign in front of the cfr() call. What is computed here for each action is the counterfactual value. When the loop over all possible actions fnishes, the value of the node-value of the current state is computed (line 26), with our current strategy. This value is the sum of the counterfactual values per action, weighted by the likelihood of taking this action. Then the cumulative counterfactual regrets are updated by adding the node-value times the reach probability of the opponent. Finally, the node-value is returned. Another accessible blog post where the algorithm is explained step by step has been written by Kamil Czarnog\u00f2rski,4 with code on GitHub.5 7.2.1.2 Deep Counterfactual Regret Minimization Counterfactual regret minimization is a tabular algorithm that traverses the extensive-form game tree from root to terminal nodes, coming closer to the Nash equilibrium with each iteration. Tabular algorithms do not scale well to large problems, and researchers often have to use domain-specifc heuristic abstraction schemes [658, 260, 110], alternate methods for regret updates [749], or sampling variants [451] to achieve acceptable performance. 3 https://ai.plainenglish.io/building-a-poker-ai-part-6-beating-kuhn-pokerwith-cfr-using-python-1b4172a6ab2d 4 https://int8.io/counterfactual-regret-minimization-for-poker-ai/ 5 https://github.com/int8/counterfactual-regret-minimization/blob/master/games/ algorithms.py 7.2 Multi-Agent Reinforcement Learning Agents 205 1 def cfr( 2 self , 3 cards: List[str], 4 history: str , 5 reach_probabilities : np.array , 6 active_player : int) -> int: 7 if KuhnPoker. is_terminal (history): 8 return KuhnPoker. get_payoff (history , cards) 9 10 my_card = cards[ active_player ] 11 info_set = self. get_information_set (my_card + history) 12 13 strategy = info_set. get_strategy ( reach_probabilities [ active_player ]) 14 opponent = ( active_player + 1) % 2 15 counterfactual_values = np.zeros(len(Actions)) 16 17 for ix , action in enumerate(Actions): 18 action_probability = strategy[ix] 19 20 new_reach_probabilities = reach_probabilities .copy () 21 new_reach_probabilities [ active_player ] *= action_probability 22 23 counterfactual_values [ix] = -self.cfr( 24 cards , history + action , new_reach_probabilities , opponent) 25 26 node_value = counterfactual_values .dot(strategy) 27 for ix , action in enumerate(Actions): 28 counterfactual_regret [ix] = \\ 29 reach_probabilities [opponent] * ( counterfactual_values [ix] - node_value) 30 info_set. cumulative_regrets [ix] += counterfactual_regret [ ix] 31 32 return node_value 33 34 def train(self , num_iterations : int) -> int: 35 util = 0 36 kuhn_cards = [\u2019J\u2019, \u2019Q\u2019, \u2019K\u2019] 37 for _ in range( num_iterations ): 38 cards = random.sample(kuhn_cards , 2) 39 history = \u2019\u2019 40 reach_probabilities = np.ones (2) 41 util += self.cfr(cards , history , reach_probabilities , 0) 42 return util Listing 7.1 Counter-factual regret minimization [784] 206 7 Multi-Agent Reinforcement Learning For large problems a deep learning version of the algorithm has been developed [111]. The goal of deep counterfactual regret minimization is to approximate the behavior of the tabular algorithm without calculating regrets at each individual information set. It generalizes across similar infosets using approximation of the value function via a deep neural network with alternating player updates. 7.2.2 Cooperative Behavior CFR is an algorithm for the competitive setting. The Nash-equilibrium defnes the competitive win/lose multi-agent case\u2014the (\u22125, \u22125) situation of the prisoner\u2019s dilemma of Table 7.1. We will now move to the cooperative setting. As we have seen, in a cooperative setting, win/win situations are possible, with higher rewards, both for society as a whole and for the individuals. The Pareto optimum for the prisoner\u2019s dilemma example is (\u22122, \u22122), only achievable through norms, trust or cooperation by the agents (as close-knit criminal groups aim to achieve, for example, through a code of silence), see also Leibo et al. [465]. The achievements in single-agent reinforcement learning inspire researchers to achieve similar results in multi-agent. However, partial observability and nonstationarity create a computational challenge. Researchers have tried many diferent approaches, some of which we will cover, although the size of problems for which the current agorithms work is still limited. Wong et al. [846] provide a review of these approaches, open problems in cooperative reinforcement learning are listed by Dafoe et al. [175]. First we will discuss approaches based on single-agent reinforcement learning methods, next we will discuss approaches based on opponent modeling, communication, and psychology [846]. 7.2.2.1 Centralized Training/Decentralized Execution While the dec-POMDP model ofers an appropriate framework for cooperative sequential decision making under uncertainty, solving a large dec-POMDP is intractable [82], and therefore many relaxations of the problems have been developed, where some elements, such as communication, or training, are centralized, to increase tractability [846, 751]. One of the easiest approaches to train a policy for a multi-agent problem is to train the collaborating agents with a centralised controller, efectively reducing a decentralized multi-agent computation to a centralized single-agent computation. In this approach all agents send their observations and local policies to a central controller, that now has perfect information, and that decides which action to take for each agent. However, as large collaborative problems are computationally expensive, the single controller becomes overworked, and this approach does not scale. 7.2 Multi-Agent Reinforcement Learning Agents 207 On the other extreme, we can ignore communication and nonstationarity and let agents train separately. In this approach the agents learn an individual action-value function and view other agents as part of the environment. This approach simplifes the computational demands at the cost of gross oversimplifcation, ignoring multiagent interaction. An in-between approach is centralized training and decentralized execution [427]. Here agents can access extra information during training, such as other agents\u2019 observations, rewards, gradients and parameters. However, they execute their policy decentrally based on their local observations. The local computation and inter-agent communication mitigate nonstationarity while still modeling partial observability and (some) interaction. This approach stabilises the local policy learning of agents, even when other agents\u2019 policies are changing [427]. When value functions are learned centrally, how should this function then be used for decentral execution by the agents? A popular method is value-function factorization. The Value decomposition networks method (VDN) decomposes the central value function as a sum of individual value functions [735], who are executed greedily by the agents. QMIX and QTRAN are two methods that improve on VDN by allowing nonlinear combinations [628, 720]. Another approach is Multiagent variational exploration (MAVEN) which improves the inefcient exploration problem of QMIX using a latent space model [496]. Policy-based methods focus on actor critic approaches, with a centralized critic training decentralized actors. Counterfactual multi-agent (COMA) uses such a centralized critic to approximate the Q-function that has access to the actors that train the behavior policies [250]. Lowe et al. [489] introduce a multi-agent version of a popular of-policy singleagent deep policy-gradient algorithm DDPG (Sect. 4.2.7), called MADDPG. It considers action policies of other agents and their coordination. MADDPG uses an ensemble of policies for each agent. It uses a decentralized actor, centralized critic approach, with deterministic policies. MADDPG works for both competitive and collaborative multi-agent problems. An extension for collision avoidance is presented by Cai et al. [123]. A popular on-policy single-agent method is PPO. Han et al. [313] achieve sample efcient results modeling the continuous Half-cheetah task as a model-based multi-agent problem. Their model-based, multi-agent, work is inspired by MVE [238] (Sect. 5.2.2.1). Yu et al. [863] achieve good results in cooperative multi-agent games (StarCraft, Hanabi, and Particle world) with MAPPO. Modeling cooperative behavior in reinforcement learning in a way that is computationally feasible is an active area of research. Li et al. [474] use implicit coordination graphs to model the structure of interactions. They use graph neural networks to model the coordination graphs [297] for StarCraft and trafc environments [655], allowing scaling of interaction patterns that are learned by the graph convolutional network. 208 7 Multi-Agent Reinforcement Learning 7.2.2.2 Opponent Modeling The state space of multi-agent problems is large, yet the previous approaches tried to learn this large space with adaptations of single-agent algorithms. Another approach is to reduce the size of the state space, for example by explicitly modeling opponent behavior in the agents. These models can then be used to guide the agent\u2019s decision making, reducing the state space that it has to traverse. Albrecht and Stone have written a survey of approaches [14]. One approach to reduce the state space is to assume a set of stationary policies between which agents switch [232]. The Switching agent model (SAM) [877] learns an opponent model from observed trajectories with a Bayesian network. The Deep reinforcement open network (DRON) [320] uses two networks, one to learn the Q-values, and the other to learn the opponent policy representation. Opponent modeling is related to the psychological Theory of Mind [610]. According to this Theory, people attribute mental states to others, such as beliefs, intents, and emotions. Our theory of the minds of others helps us to analyze and predict their behavior. Theory of mind also holds that we assume that the other has theory of mind; it allows for a nesting of beliefs of the form: \u201cI believe that you believe that I believe\u201d [796, 795, 797]. Building on these concepts, Learning with opponent-learning awareness (LOLA) anticipates opponent\u2019s behavior [251]. Probabilistic recursive reasoning (PR2) models our own and our opponent\u2019s behavior as a hierarchy of perspectives [835]. Recursive reasoning has been shown to lead to faster convergence and better performance [535, 176]. Opponent modeling is also an active area of research. 7.2.2.3 Communication Another step towards modeling the real world is taken when we explicitly model communication between agents. A fundamental question is how language between agents emerges when no predefned communication protocol exists, and how syntax and meaning evolve out of interaction [846]. A basic approach to communication is with referential games: a sender sends two images and a message; the receiver then has to identify which of the images was the target [458]. Language also emerges in more complicated versions, or in negotiation between agents [126, 424]. Another area where multi-agent systems are frequently used is the study of coordination, social dilemmas, emergent phenomena and evolutionary processes, see, for example [750, 219, 465]. In the card game bridge [716] bidding strategies have been developed to signal to the other player in the team which cards a player has [716]. In the game of Diplomacy, an explicit negotion-phase is part of each game round [185, 430, 584, 21]. Work is ongoing to design communication-aware variants of reinforcement learning algorithms [711, 315]. 7.2 Multi-Agent Reinforcement Learning Agents 209 7.2.2.4 Psychology Many of the key ideas in reinforcement learning, such as operant conditioning and trial-and-error, originated in cognitive science. Faced with the large state space, multi-agent reinforcement learning methods are moving towards human-like agents. In addition to opponent modeling, studies focus on coordination, pro-social behavior, and intrinsic motivation. A large literature exists on emergence of social norms and cultural evolution in multi-agent systems [465, 105, 34, 181]. To deal with nonstationarity and large states spaces, humans use heuristics and approximation [275, 500]. However, heuristics can lead to biases and suboptimal decision-making [276]. It is interesting to see how multi-agent modeling is discovering concepts from psychology. More research in this area is likely to improve the human-like behavior of artifcial agents. 7.2.3 Mixed Behavior To discuss solution methods for agents in the mixed setting, we will look at one important approach that is again inspired by biology: population-based algorithms. Population-based methods such as evolutionary algorithms and swarm computing work by evolving (or optimizing) a large number of agents at the same time. We will look closer at evolutionary algorithms and at swarm computing, and then we will look at the role they play in multi-agent reinforcement learning. 7.2.3.1 Evolutionary Algorithms Evolutionary algorithms are inspired by bio-genetic processes of reproduction: mutation, recombination, and selection [40]. Evolutionary algorithms work with large populations of simulated individuals, which typically makes it easy to parallelize and run them on large computation clusters. Evolutionary algorithms often achieve good results in optimizing diverse problems. For example, in optimizing single agent problems, an evolutionary approach would model the problem as a population of individuals, in which each individual represents a candidate solution to the problem. The candidate\u2019s quality is determined by a ftness function, and the best candidates are selected for reproduction. New candidates are created through crossover and mutation of genes, and the cycle starts again, until the quality of candidates stabilizes. In this way an evolutionary algorithm iteratively approaches the optimum. Evolutionary algorithms are randomized algorithms, that can circumvent local optima. Although they are best known for solving single agent optimization problems, we will use them here to model multi-agent problems. Let us look in more detail at how an evolutionary algorithm works (see Alg. 7.1) [40]. First an initial population is generated. The ftness of each indi210 7 Multi-Agent Reinforcement Learning Algorithm 7.1 Evolutionary Framework [40] 1: Generate the initial population randomly 2: repeat 3: Evaluate the ftness of each individual of the population 4: Select the fttest individuals for reproduction 5: Through crossover and mutation generate new individuals 6: Replace the least ft individuals by the new individuals 7: until terminated vidual is computed, and the fttest individuals are selected for reproduction, using crossover and mutation to create a new generation of individuals. The least ft individuals of the old populations are replaced by the new individuals. Compared to reinforcement learning, in evolutionary algorithms the agents are typically homogeneous, in the sense that the reward (ftness) function for the individuals is the same. Individuals do have diferent genes, and thus difer in their behavior (policy). In reinforcement learning there is a single current behavior policy, where an evolutionary approach has many candidate policies (individuals). The ftness function can engender in principle both competitive and cooperative behavior between individuals, although a typical optimization scenario is to select a single individual with the genes for the highest ftness (survival of the fttest competitor).6 Changes to genes of individuals (policies) occur explicitly via crossover and (random) mutation, and implicitly via selection for ftness. In reinforcement learning the reward is used more directly as a policy goal; in evolutionary algorithms the ftness does not directly infuence the policy of an individual, only its survival. Individuals in evolutionary algorithms are passive entities that do not communicate or act, although they do combine to create new individuals. There are similarities and diferences between evolutionary and multi-agent reinforcement learning algorithms. First of all, in both approaches the goal is to fnd the optimal solution, the policy that maximizes (social) reward. In reinforcement learning this occurs by learning a policy through interaction with an environment, in evolutionary algorithms by evolving a population through survival of the fttest. Reinforcement learning deals with a limited number of agents whose policy determines their actions, evolutionary algorithms deals with many individuals whose genes determine their survival. Policies are improved using a reward function that assesses how good actions are, genes mutate and combine, and individuals are selected using a ftness function. Policies are improved \u201cin place\u201d and agents do not die, in evolutionary computation the traits (genes) of the best individuals are selected and copied to new individuals in the next generation after which the old generation does die. Although diferent at frst sight, the two approaches share many traits, including the main goal: optimizing behavior. Evolutionary algorithms are inherently multi6 Survival of the fttest cooperative group of individuals can also be achieved with an appropriate ftness function [492]. 7.2 Multi-Agent Reinforcement Learning Agents 211 Fig. 7.3 A Flock of Starlings, and One Predator agent, and may work well in fnding good solutions in large and nonstationary sequential decision problems. 7.2.3.2 Swarm Computing Swarm computing is related to evolutionary algorithms [79, 95]. Swarm computing focuses on emerging behavior in decentralized, collective, self-organized systems. Agents are typically simple, numerous, and homogeneous, and interact locally with each other and the environment. Biological examples of swarm intelligence are behavior in ant colonies, bee-hives, focks of birds, and schools of fsh, Fig. 7.3 [736]; image by Wikimedia. Behavior is typically cooperative through decentralized communication mechanisms. In artifcial swarm intelligence, individuals are sometimes able to imagine the behavior of other individuals, when they have a Theory of mind [54, 258]. Swarm intelligence, or collective intelligence in general, is a form of decentralized computing, as opposed to reinforcement learning, where external algorithms calculate optimal behavior in a single classical centralized algorithm [848]. Although both approaches work for the mixed setting, evolutionary algorithms tend to stress competition and survival of the fttest (Nash), where swarm computing stresses cooperation and survival of the group (Pareto). A well-known example of artifcial swarm intelligence is Dorigo\u2019s Ant colony optimization algorithm (ACO) which is a probabilistic optimization algorithm modeled after the pheromone-based communication of biological ants [206, 208, 209]. 212 7 Multi-Agent Reinforcement Learning Algorithm 7.2 Population Based Training [374] procedure Train(P) \u22b2 initial population P Population P, weights \ud835\udf03, hyperparameters \u210e, model evaluation \ud835\udc5d, time step \ud835\udc61 for (\ud835\udf03, \u210e, \ud835\udc5d, \ud835\udc61) \u2208 P (asynchronously in parallel) do while not end of training do \ud835\udf03 \u2190 step(\ud835\udf03 |\u210e) \u22b2 one step of optimisation using hyperparameters \u210e \ud835\udc5d \u2190 eval(\ud835\udf03) \u22b2 current model evaluation if ready( \ud835\udc5d, \ud835\udc61, P) then \u210e\u2032, \ud835\udf03\u2032 \u2190 exploit(\u210e, \ud835\udf03, \ud835\udc5d, P) \u22b2 use the rest of population for improvement if \ud835\udf03 \u2260 \ud835\udf03\u2032 then \u210e, \ud835\udf03 \u2190 explore(\u210e\u2032, \ud835\udf03\u2032, P) \u22b2 produce new hyperparameters \u210e \ud835\udc5d \u2190 eval(\ud835\udf03) \u22b2 new model evaluation end if end if update P with new (\ud835\udf03, \u210e, \ud835\udc5d, \ud835\udc61 + 1) \u22b2 update population end while end for return \ud835\udf03 with the highest \ud835\udc5d in P end procedure Emergent behavior in multi-agent reinforcement learning is specifcally studied in [487, 374, 534, 325, 50, 464]. For decentralized algorithms related to solving multi-agent problems see, for example [569, 871, 571, 570]. 7.2.3.3 Population-Based Training Translating traditional value or policy-based reinforcement learning algorithms to the multi-agent setting is non-trivial. It is interesting to see that, in contrast, evolutionary algorithms, frst designed for single-agent optimization using a population of candidate solutions, translate so naturally to the multi-agent setting, where a population of agents is used to fnd a shared solution that is optimal for society. In evolutionary algorithms agents are typically homogeneous, although they can be heterogeneous, with diferent ftness functions. The ftness functions may be competitive, or cooperative. In the latter case the increase of reward for one agent can also imply an increase for other agents, possibly for the entire group, or population. Evolutionary algorithms are quite efective optimization algorithms. Salimans et al. [653] report that evolution strategies rival the performance of standard reinforcement learning techniques on modern benchmarks, while being easy to parallelize. In particular, evolutionary strategies are simpler to implement (there is no need for backpropagation), are easier to scale in a distributed setting, do not sufer in settings with sparse rewards, and have fewer hyperparameters. Evolutionary algorithms are a form of population-based computation that can be used to compute strategies that are optimal in a game-theoretic Nash sense, or that try to fnd strategies that out-perform other agents. The evolutionary approach to optimization is efcient, robust, and easy to parallelize, and thus has some 7.2 Multi-Agent Reinforcement Learning Agents 213 advantages over the stochastic gradient approach. How does this approach relate to multi-agent reinforcement learning, and can it be used to fnd efcient solutions for multi-agent reinforcement learning problems? In recent years a number of research teams have reported successful eforts in creating reinforcement learning players for multi-agent strategy games (see Sect. 7.3). These were all large research eforts, where a range of diferent approaches was used, from self-play reinforcement learning, cooperative learning, hierarchical modeling, and evolutionary computing. Jaderberg et al. [374] report success in Capture the Flag games with a combination of ideas from evolutionary algorithms and self-play reinforcement learning. Here a population-based approach of self play is used where teams of diverse agents are trained in tournaments against each other. Algorithm 7.2 describes this Population based training (PBT) approach in more detail. Diversity is enhanced through mutation of policies, and performance is improve through culling of under-performing agents. Population based training uses two methods. The frst is exploit, which decides whether a worker should abandon the current solution and focus on a more promising one. The second is explore, which, given the current solution and hyperparameters, proposes new solutions to explore the solution space. Members of the population are trained in parallel. Their weights \ud835\udf03 are updated and eval measures their current performance. When a member of the population is deemed ready because it has reached a certain performance threshold, its weights and hyperparameters are updated by exploit and explore, to replace the current weights with the weights that have the highest recorded performance in the rest of the population, and to randomly perturb the hyperparameters with noise. After exploit and explore, iterative training continues as before until convergence. Let us have a look at this fusion approach for training leagues of players. 7.2.3.4 Self-Play Leagues Self-play league learning refers to the training of a multi-agent league of individual game playing characters. As we will see in the next section, variants have been used to play games such as StarCraft, Capture the Flag, and Hide and Seek. League learning combines population-based training with self-play training as in AlphaZero. In league learning, the agent plays against a league of diferent opponents, while being part of a larger team of agents that is being trained. The team of agents is managed to have enough diversity in order to provide a stable training goal to reduce divergence or local minima. League learning employs evolutionary concepts such as mutation of behavior policies and culling of under-performing agents from the population. The team employs cooperative strategies, in addition to competing against the other teams. Agents are trained in an explicit hierarchy. The goal of self-play league training is to fnd stable policies for all agents, that maximize their team reward. In mixed and cooperative settings the teams of agents may beneft from each others\u2019 strength increase [489]. Self-play league learning 214 7 Multi-Agent Reinforcement Learning Environment Behavior Approach Ref Poker Competitive (Deep) Counterfactual regret minimization [103, 113, 533] Hide and Seek Cooperative Self-play, Team hierarchy [49] Capture the Flag Mixed Self-play, Hierarchical, Population-based [373] StarCraft II Mixed Self-play, Population-based [813] Table 7.2 Multi-Agent Game Approaches also uses aspects of hierarchical reinforcement learning, a topic that will be covered in the next chapter. In the next section we will look deeper into how self-play league learning is implemented in specifc multi-player games. 7.3 Multi-Agent Environments Reinforcement learning has achieved quite a few imaginative results in which it has succeeded in emulating behavior that approaches human behavior in the real world. In this chapter we have made a step towards modeling more behavior that is even closer to the real world. Let us summarize in this section the results in four diferent multi-agent games, some of which have been published in prestigious scientifc journals. We will use the familiar sequence of (1) competitive, (2) cooperative, and (3) mixed environments. For each we will sketch the problem, outline the algorithmic approach, and summarize the achievements. Table 7.2 lists the multi-agent games and their dominant approach. 7.3.1 Competitive Behavior: Poker Poker is a popular imperfect-information game. It is played competitively and human championships are organized regularly. Poker has been studied for some time in artifcial intelligence, and computer poker championships have been conducted regularly [53, 89]. Poker is a competitive game. Cooperation (collusion, collaboration between two players to the detriment of a third player) is possible, but often does not occur in practice; fnding the Nash equilibrium therefore is a successful approach to playing the game in practice. The method of counterfactual regret minimization has been developed specifcally to make progress in poker. There are many variants of poker that are regularly played. No-limit Texas hold\u2019em is a popular variant; the two-player version is called Heads Up and is easier to analyse because no opponent collusion can occur. Heads-up no-limit Texas hold\u2019em (HUNL) has been the primary AI benchmark for imperfect-information game play for several years. 7.3 Multi-Agent Environments 215 Fig. 7.4 Pluribus on the Cover of Science Poker has hidden information (the face-down cards). Because of this, agents are faced with a large number of possible states; poker is a game that is far more complex than chess or checkers. The state space of the two-person HUNL version is reported to be around 10161 [112]. A further complication is that during the course of the game information is revealed by players through their bets; high bets indicate good cards, or the wish of the player to make the opponent believe that this is the case (blufng). Therefore, a player must choose between betting high on good cards, and on doing the opposite, so that the opponent does not fnd out too much, and can counter-act. In 2018, one of the top two-player poker programs, Libratus, defeated top human professionals in HUNL in a 20-day, 120,000-hand competition featuring a $200,000 prize pool. Brown et al. [112] describe the architecture of Libratus. The program consists of three main modules, one for computing a quick CFR Nash-policy using a smaller version of the game, a second module for constructing a fner-grained strategy once a later stage of the game is reached, and a third module to enhance the frst policy by flling in missing branches. In the experiment against top human players, Libratus analyzed the bet sizes that were played most often by its opponents at the end of each day of the competition. The programs would then calculate a response overnight, in order to improve as the competition proceeded. Two-agent Libratus was originally based on heuristics, abstraction, and tabular counterfactual regret minimization\u2014not on deep reinforcement learning. For multiagent poker the program Pluribus was developed. For Pluribus, the authors used deep counterfactual regret minimization [111], with a 7-layer neural network that followed the AlphaZero self-play approach. Pluribus defeated top players in sixplayer poker [113] (Fig. 7.4). Pluribus uses an AlphaZero approach of self-play in 216 7 Multi-Agent Reinforcement Learning combination with search. Another top program, DeepStack, also uses randomly generated games to train a deep value function network [533]. 7.3.2 Cooperative Behavior: Hide and Seek In addition to competition, cooperative behavior is part of real-world behavior. Indeed, cooperation is what defnes our social fabric, and much of our society consists of diferent ways in which we organize ourselves in families, groups, companies, parties, flter bubbles, and nations. The question of how voluntary cooperation can emerge between individuals has been studied extensively, and the work in tit-for-tat (Sect. 7.1.3) is just one of many studies in this fascinating feld [34, 105, 848, 465, 651]. One study into emergent cooperation has been performed with a version of the game of Hide and Seek. Baker et al.[49] report on an experiment in which they used MuJoCo to build a new multi-agent game environment. The environment was created with the specifc purpose of studying how cooperation emerged out of the combination of a few simple rules and reward maximation (see Fig. 7.5). In the Hide and Seek experiment, the game is played on a randomly generated grid-world where props are available, such as boxes. The reward function stimulates hiders to avoid the line of sight of seekers, and vice versa. There are objects scattered throughout the environment that the agents can grab and lock in place. The environment contains one to three hiders and one to three seekers, there are three to nine movable boxes, some elongated. There are also two movable ramps. Walls and rooms are static and are randomly generated. Agents can see, move, and grab objects. A good way to understand the challenge is to view the videos7 on the Hide and Seek blog.8 With only a visibility-based reward function, the agents are able to learn many diferent skills, including collaborative tool use. For example, hiders learn to create shelters by barricading doors or constructing multi-object forts, so that the seekers can never see them anymore, until, as a counter strategy, seekers learned to use ramps to jump into the shelter. In efect, out of the agents\u2019 interaction a training curriculum emerges in which the agents learn tasks, many of which require sophisticated tool use and coordination. Hide and Seek features cooperation (inside the team) and competition (between the hiders and the seekers). It uses a self-play version of PPO for policy learning. It is interesting to see how easy cooperation emerges. The game does not have explicit communication for the team to coordinate cooperation, all cooperative behavior emerges out of basic interaction between agents that are guided by their reward functions. Cooperative strategies thus emerge out of the game design: the 7 https://www.youtube.com/watch?v=kopoLzvh5jY&t=10s 8 https://openai.com/blog/emergent-tool-use/ 7.3 Multi-Agent Environments 217 Fig. 7.5 Six Strategies in Hide and Seek: Running and Chasing; Fort Building; Ramp Use; Ramp Defense; Box Surfng; Surf Defense (left-to-right, top-to-bottom) [49] homogeneous reward functions for each team and the environment in which blocks are present and follow laws of physics. During play the agents essentially construct an autocurriculum for themselves [464, 49]. Six diferent behavior strategies are reported, each more advanced than the other, increasing the competitive pressure to fnd counter strategies for the opponent (see Fig. 7.5). Baker et al. [49] report that initially, the hiders and seekers learn the basic strategy of running away and chasing. However, after much training (25 million episodes), hiders start to use boxes to construct shelters behind which they hide. Then, after another 75 million episodes, the seekers learn to move and use ramps to jump over obstacles into the shelter. A mere 10 million episodes later, the hiders learn to defend by moving the ramps to the edge and lock them in place out of range of the shelters. Then, after a long 270 million episodes of training, the seekers learned box-surfng. They moved a box to the edge of the play area next to the locked ramps. One seeker then used the ramp to climb on top of the box and the other seekers push it to the shelter, where the seeker could peek over te edge and see the hiders. Finally, in response, the hiders locked all of the boxes in place before building their shelter, and they were safe from the seekers. The Hide and Seek experiment is interesting because of the emergence of diverse behavior strategies. The strategies emerged out of a basic reward function and random exploration (see also the reward is enough argument [707]). 218 7 Multi-Agent Reinforcement Learning Fig. 7.6 Capture the Flag [373] The emergence of strategies out of basic reward and exploration suggests an evolutionary process. However, Hide and Seek does not employ population-based training or evolutionary algorithms, in contrast to the work in Capture the Flag, in the next section. 7.3.3 Mixed Behavior: Capture the Flag and StarCraft The world around us exhibits a mix of competitive and cooperative behavior. Team collaboration is an important aspect of human life, and it has been studied extensively in biology, sociology and artifcial intelligence. It emerges (evolves) out of the most basic settings\u2014the need to achieve an ambitious goal\u2014as we just saw. In recent years many research groups have studied the mixed multi-agent model in real time strategy games, such as Capture the Flag, and StarCraft. We will discuss both games. Capture the Flag First we will discuss the game Capture the Flag, which is played in a Quake III Arena (see Fig. 7.6). Jaderberg et al. have reported on an extensive experiment with this game [373], in which the agents learn from scratch to see, act, cooperate, and compete. In this experiment the agents are trained with population-based 7.3 Multi-Agent Environments 219 self-play [374], Alg. 7.2. The agents in the population are all diferent (they have diferent genes). The population learns by playing against each other, providing increased diversity of teammates and opponents, and a more stable and faster learning process than traditional single-agent deep reinforcement learning methods. In total 30 diferent bots were created and pitted against each other. Agents are part of a team, and the reward functions form a hierarchy. A two-layer optimization process optimizes the internal rewards for winning, and uses reinforcement learning on the internal rewards to learn the policies. In Capture the Flag, the bots start by acting randomly. After 450,000 games, a bot strategy was found that performed well, and they developed cooperative strategies, such as following team mates in order to outnumber opponents, and loitering near the enemy base when their team mate has the fag. Again, as in Hide and Seek, cooperative strategies emerged out of the basic rules, by combining environment feedback and survival of the fttest. The work on Capture the Flag is notable since it demonstrated that with only pixels as input an agent can learn to play competitively in a rich multi-agent environment. To do so it used a combination of population based training, internal reward optimization, and temporally hierarchical reinforcement learning (see next chapter). StarCraft The fnal game that we will discuss in this chapter is StarCraft. StarCraft is a multi-player real-time strategy game of even larger complexity. The state space has been estimated to be on the order of 101685 [573], a very large number. StarCraft features multi-agent decision making under uncertainty, spatial and temporal reasoning, competition, team-collaboration, opponent modeling, and real-time planning. Fig. 1.6 shows a picture of a StarCraft II scene. Research on StarCraft has been ongoing for some time [573], a special StarCraft multi-agent challenge has been introduced [655]. A team from DeepMind has created a program called AlphaStar. In a series of test matches held in December 2018, DeepMind\u2019s AlphaStar beat two top players in two-player single-map matches, using a diferent user interface. AlphaStar plays the full game of StarCraft II. The neural network was initially trained by supervised learning from anonymized human games, that were then further trained by playing against other AlphaStar agents, using a population-based version of self-play reinforcement learning [813, 374].9 These agents are used to seed a multi-agent reinforcement learning process. A continuous competitive league was created, with the agents of the league playing games in competition against each other. By branching from existing competitors, new competitors were added. Agents learn from games against other competitors. Population-based learning was 9 Similar to the frst approach in AlphaGo, where self-play reinforcement learning was also bootstrapped by supervised learning from human games. 220 7 Multi-Agent Reinforcement Learning taken further, creating a process that explores the very large space of StarCraft game play by pitting agents against strong opponent strategies, and retaining strong early strategies. Diversity in the league is increased by giving each agent its own learning objective, such as which competitors it should focus on and which game unit it should build. A form of prioritized league-self-play actor-critic training is used, called prioritized fctitious self-play\u2014details are in [813]. AlphaStar was trained on a custom-built scalable distributed training system using Google\u2019s tensor processing units (TPU). The AlphaStar league was run for 14 days. In this training, each agent experienced the equivalent of 200 years of real-time StarCraft play. In StarCraft players can choose to play one of three alien races: Terran, Zerg or Protoss. AlphaStar was trained to play Protoss only, to reduce training time, although the same training pipeline could be applied to any race. AlphaStar was frst tested against a human grandmaster named TLO, a top professional Zerg player and a grandmaster level Protoss player. The human player remarked: I was surprised by how strong the agent was. AlphaStar takes well-known strategies and turns them on their head. The agent demonstrated strategies I had not thought of before, which means there may still be new ways of playing the game that we haven\u2019t fully explored yet. 7.3.4 Hands On: Hide and Seek in the Gym Example Many of the research eforts reported in these fnal chapters describe signifcant eforts by research teams working on complicated and large games. These games represent the frontier of artifcial intelligence, and research teams use all available computational and software engineering power that they can acquire to achieve the best results. Also, typically a large amount of time is spent in training, and in fnding the right hyperparameters for the learning to work. Replicating results of this scale is highly challenging, and most research eforts are focused on replicating the results on a smaller, more manageable scale, with more manageable computational resources. In this section we will try to replicate some aspects with modest computational requirements. We will focus on Hide and Seek (Sect. 7.3.2). The original code for the Hide and Seek experiments is on GitHub.10 Please visit and install the code. Hide and Seek uses MuJoCo and the mujoco-worldgen package. Install them and the dependencies with pip install -r mujoco-worldgen/requirements.txt pip install -e mujoco-worldgen/ pip install -e multi-agent-emergence-environments/ 10 https://github.com/openai/multi-agent-emergence-environments 7.3 Multi-Agent Environments 221 Examples of environments can be found in the mae_envs/envs folder. You can also build your own environments, starting from the Base environment, in mae_envs/envs/base.py, and then adding boxes, ramps, as well as the appropriate wrappers. Look in the other environments for how to do this. Try out environments by using the bin/examine script; example usage: bin/examine.py base See further the instructions in the GitHub repository. Multiplayer Environments To conclude this hands-on section, we mention a multiplayer implementation of the Arcade Learning Environment. It is presented by Terry et al. [760], who also present baseline performance results for a multiplayer version of DQN, Ape-X DQN, which performed well elsewhere [350, 52]. The environment is also presented as part of PettingZoo, a multi-agent version of Gym [761, 791, 414]. Another multi-agent research environment is the Google Football Research Environment [438]. A physics-based simulator is provided, as well as three baseline implementations (DQN, IMPALA, and PPO). Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary Multi-agent reinforcement learning learns optimal policies for environments that consist of multiple agents. The optimal policy of the agents is infuenced by the policies of the other agents, whose policy is also being optimized. This gives rise to the problem of nonstationarity, and agent behavior violates the Markov property. Multi-agent reinforcement learning adds the element of cooperative behavior to the repertoire of reinforcement learning, which now consists of competition, cooperation, and mixed behavior. The feld is closely related to game theory\u2014the basis of the study of rational behavior in economics. A famous problem of game theory is the prisoner\u2019s dilemma. A famous result of non-cooperative game theory is the Nash equilibrium, which is defned as the joint strategy where no player has anything to gain by changing their own strategy. A famous result from cooperative game-theory is the Pareto optimum, the situation where no individual can be better of without making someone else worse of. 222 7 Multi-Agent Reinforcement Learning When agents have private information, a multi-agent problem is partially observable. Multi-agent problems can be modeled by stochastic games or as extensive form games. The behavior of agents is ultimately determined by the reward functions, that can be homogeneous, or heterogeneous. When agents have diferent reward functions, multi-agent becomes multi-objective reinforcement learning. The regret of an action is the amount of reward that is missed by the agent for not choosing the actions with the highest payof. A regret minimization algorithm is the stochastic and multi-agent equivalent of minimax. Counterfactual-regret minimization is an approach for fnding Nash strategies in competitive multi-agent games, such as poker. Variants of single-agent algorithms are often used for cooperative multi-agent situations. The large state space due to nonstationarity and partial observability precludes solving large problems. Other promising approaches are opponent modeling and explicit communication modeling. Population-based methods such as evolutionary algorithms and swarm intelligence are used frequently in multi-agent systems. These approaches are suitable for homogeneous reward functions and competitive, cooperative, and mixed problems. Evolutionary methods evolve a population of agents, combining behaviors, and selecting the best according to some ftness function. Evolutionary methods are a natural ft for parallel computers and are among the most popular and successful optimization algorithms. Swarm intelligence often introduces (rudimentary) forms of communication between agents, such as in Ant colony optimization where agents communicate through artifcial pheromones to indicate which part of the solution space they have traveled. For some of the most complicated problems that have recently been tackled, such as StarCraft, Capture the Flag, and Hide and Seek, hierarchical and evolutionary principles are often combined in league training, where leagues of teams of agents are trained in a self-play fashion, and where the fttest agents survive. Current achievements require large amounts of computational power, future work is trying to reduce these requirements. Further Reading Multi-agent learning is a widely studied feld. Surveys\u2014both on early multi-agent reinforcement learning and on deep multi-agent reinforcement learning\u2014can be found in [292, 122, 858, 13, 14, 750, 790, 333, 332, 846]. After Littman [483], Shoham et al. [698] look deeper into MDP modeling. The classic work on game theory is Von Neumann and Morgenstern [818]. Modern introductions are [545, 180, 259]. Game theory underlies much of the theory of rational behavior in classical economics. Seminal works of John Nash are [555, 556, 554]. In 1950 he introduced the Nash equilibrium in his dissertation of 28 pages, which won him the Nobel prize in Economics in 1994. A biography and flm have been made about the life of John Nash [553]. 7.3 Multi-Agent Environments 223 The game of rock-paper-scissors plays an important role in game theory, and the study of computer poker [822, 89, 641, 113]. Prospect theory [390], introduced in 1979, studies human behavior in the face of uncertainty, a topic that evolved into the feld of behavioral economics [843, 539, 128]. Gigerenzer introduced fast and frugal heuristics to explain human decision making [275]. For more intriguing works on the feld of evolution of cooperation and the emergence of social norms, see, for example [105, 37, 34, 33, 35, 331, 336]. More recently multi-objective reinforcement learning has been studied, a survey is [484]. In this feld the more realistic assumption is adopted that agents have diferent rewards functions, leading to diferent Pareto optima [802, 537, 841]. Oliehoek et al. have written a concise introduction to decentralized multi-agent modelling [569, 568]. Counterfactual regret minimization has been fundamental for the success in computer poker [112, 113, 379, 881]. An often-used Monte Carlo version is published in [451]. A combination with function approximation is studied in [111]. Evolutionary algorithms have delivered highly successful optimization algorithms. Some entries to this vast feld are [224, 40, 42, 41, 43]. A related feld is swarm intelligence, where communication between homogeneous agents is taking place [406, 217, 207, 78]. For further research in multi-agent systems refer to [847, 793]. For collective intelligence, see, for example [406, 208, 258, 848, 599]. Many other works report on evolutionary algorithms in a reinforcement learning setting [731, 409, 536, 838, 148, 163, 653, 842]. Most of these approaches concern single agent approaches, although some are specifcally applied to multi agent approaches [498, 408, 713, 487, 374, 414]. Research into benchmarks is active. Among interesting approaches are Procedural content generation [781], MuJoCo Soccer [487], and the Obstacle Tower Challenge [384]. There is an extensive literature on computer poker. See, for example [89, 90, 277, 641, 53, 103, 90, 277, 104, 657, 533]. StarCraft research can be found in [815, 655, 813, 655, 573]. Other games studies are [759, 783]. Approaches inspired by results in poker and Go are now also being applied with success in no-press Diplomacy [584, 21]. Exercises Below are a few quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. Questions 1. Why is there so much interest in multi-agent reinforcement learning? 2. What is one of the main challenges of multi-agent reinforcement learning? 3. What is a Nash strategy? 224 7 Multi-Agent Reinforcement Learning 4. What is a Pareto Optimum? 5. In a competitive multi-agent system, what algorithm can be used to calculate a Nash strategy? 6. What makes it difcult to calculate the solution for a game of imperfect information? 7. Describe the Prisoner\u2019s dilemma. 8. Describe the iterated Prisoner\u2019s dilemma. 9. Name two multi-agent card games of imperfect information. 10. What is the setting with a heterogeneous reward function usually called? 11. Name three kinds of strategies that can occur a multi-agent reinforcement learning. 12. Name two solution methods that are appropriate for solving mixed strategy games. 13. What AI method is named after ant colonies, bee swarms, bird focks, or fsh schools? How does it work in general terms? 14. Describe the main steps of an evolutionary algorithm. 15. Describe the general form of Hide and Seek and three strategies that emerged from the interactions of the hiders or seekers. Exercises Here are some programming exercises to become more familiar with the methods that we have covered in this chapter. 1. CFR Implement counterfactual regret minimization for a Kuhn poker player. Play against the program, and see if you can win. Do you see possibilities to extend it to a more challenging version of poker? 2. Hide and Seek Implement Hide and Seek with cooperation and competition. Add more types of objects. See if other cooperation behavior emerges. 3. Ant Colony Use the DeepMind Control Suite and setup a collaborative level and a competitive level, and implement Ant Colony Optimization. Find problem instances on the web, or in the original paper [208]. Can you implement more swarm algorithms? 4. Football Go to the Google Football blog11 and implement algorithms for football agents. Consider using a population-based approach. 5. StarCraft Go to the StarCraft Python interface,12 and implement a StarCraft player (highly challenging) [655]. 11 https://ai.googleblog.com/2019/06/introducing-google-research-football.html 12 https://github.com/deepmind/pysc2 Chapter 8 Hierarchical Reinforcement Learning The goal of artifcial intelligence is to understand and create intelligent behavior; the goal of deep reinforcement learning is to fnd a behavior policy for ever larger sequential decision problems. But how does real intelligence fnd these policies? One of the things that humans are good at, is dividing a complex task into simpler subproblems, and then solving those tasks, one by one, and combining them as the solution to the larger problem. These subtasks are of diferent scales, or granularity, than the original problem. For example, when planning a trip from your house to a hotel room in a far-away city, you typically only plan the start and the end in terms of footsteps taken in a certain direction. The in-between part may contain diferent modes of transportation that get you to your destination quicker, with macro steps, such as taking a trainride or a fight. During this macro, you do not try out footsteps in diferent directions. Our trip\u2014our policy\u2014is a combination of fne-grain primitive actions and coarse-grain macro actions. Hierarchical reinforcement learning studies this real-world-inspired approach to problem solving. It provides formalisms and algorithms to divide problems into larger subproblems, and then plans with these subpolicies, as if they were subroutines. In principle the hierarchical approach can exploit structure in all sequential decision problems, although some problems are easier than others. Some environments can be subdivided into smaller problems in a natural way, such as navigational tasks on maps, or path-fnding tasks in mazes. Multi-agent problems also naturally divide into hierarchical teams, and have large state spaces where hierarchical methods may help. For other problems, however, it can be hard to fnd efcient macros, or it can be computationally intensive to fnd good combinations of macro steps and primitive steps. Another aspect of hierarchical methods is that since macro-actions take large steps, they may miss the global minimum. The best policies found by hierarchical methods may be less optimal than those found by \u201cfat\u201d approaches (although they may get there much quicker). 225 226 8 Hierarchical Reinforcement Learning In this chapter we will start with an example to capture the favor of hierarchical problem solving. Next, we will look at a theoretical framework that is used to model hierarchical algorithms, and at a few examples of algorithms. Finally, we will look deeper at hierarchical environments. The chapter ends with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Solve large, structured, problems by divide and conquer \u2022 Temporal abstraction of actions with options Core Problem \u2022 Find subgoals and subpolicies efciently, to perform hierarchical abstraction Core Algorithms \u2022 Options framework (Sect. 8.2.1) \u2022 Option critic (Sect. 8.2.3.2) \u2022 Hierarchical actor critic (Sect. 8.2.3.2) Planning a Trip Let us see how we plan a major trip to visit a friend that lives in another city, with a hierarchical method. The method would break up the trip in diferent parts. The frst part would be to walk to your closet and get your things, and then to get out and get your bike. You would go to the train station, and park your bike. You would then take the train to the other city, possibly changing trains enroute if that would be necessary to get you there faster. Arriving in the city your friend would meet you at the station and would drive you to their house. A \u201cfat\u201d reinforcement learning method would have at its disposal actions consisting of footsteps in certain directions. This would make for a large space of possible policies, although the fne grain at which the policy would be planned\u2014individual footsteps\u2014would sure be able to fnd the optimal shortest route. The hierarchical method has at its disposal a wider variety of actions\u2014macro actions: it can plan a bike ride, a train trip, and getting a ride by your friend. The route may not be the shortest possible (who knows if the train follows the shortest 8.1 Granularity of the Structure of Problems 227 route between the two cities) but planning will be much faster than painstakingly optimizing footstep by footstep. 8.1 Granularity of the Structure of Problems In hierarchical reinforcement learning the granularity of abstractions is larger than the fne grain of the primitive actions of the environment. When we are preparing a meal, we reason in large action chunks: chop onion, cook spaghetti, instead of reasoning about the actuation of individual muscles in our hands and arms. Infants learn to use their muscles to performs certain tasks as they grow up until it becomes second nature. We generate subgoals that act as temporal abstractions, and subpolicies that are macro\u2019s of multiple ordinary actions [673]. Temporal abstraction allows us to reason about actions of diferent time scales, sometimes with course grain actions\u2014taking a train\u2014sometimes with fne grain actions\u2014opening a door\u2014mixing macro actions with primitive actions. Let us look at advantages and disadvantages of the hierarchical approach. 8.1.1 Advantages We will start with the advantages of hierarchical methods [247]. First of all, hierarchical reinforcement learning simplifes problems through abstraction. Problems are abstracted into a higher level of aggregation. Agents create subgoals and solve fne grain subtasks frst. Actions are abstracted into larger macro actions to solve these subgoals; agents use temporal abstraction. Second, the temporal abstractions increase sample efciency. The number of interactions with the environment is reduced because subpolicies are learned to solve subtasks, reducing the environment interactions. Since subtasks are learned, they can be transfered to other problems, supporting transfer learning. Third, subtasks reduce brittleness due to overspecialization of policies. Policies become more general, and are able to adapt to changes in the environment more easily. Fourth, and most importantly, the higher level of abstraction allows agents to solve larger, more complex problems. This is a reason why for complex multi-agent games such as StarCraft, where teams of agents must be managed, hierarchical approaches are used. Multi-agent reinforcement learning often exhibits a hierarchical structure; problems can be organized such that each agent is assigned its own subproblem, or the agents themselves may be structured or organized in teams or groups. There can be cooperation within the teams or competition between the teams, or the behavior can be fully cooperative or fully competitive. 228 8 Hierarchical Reinforcement Learning Flet-Berliac [247], in a recent overview, summarizes the promise of hierarchical reinforcement learning as follows: (1) achieve long-term credit assignment through faster learning and better generalization, (2) allow structured exploration by exploring with sub-policies rather than with primitive actions, and (3) perform transfer learning because diferent levels of hierarchy can encompass diferent knowledge. 8.1.2 Disadvantages There are also disadvantages and challenges associated with hierarchical reinforcement learning. First of all, it works better when there is domain knowledge available about structure in the domain. Many hierarchical methods assume that domain knowledge is available to subdivide the environment so that hierarchical reinforcement learning can be applied. Second, there is algorithmic complexity to be solved. Subgoals must be identifed in the problem environment, subpolicies must be learned, and termination conditions are needed. These algorithms must be designed, which costs programmer efort. Third, hierarchical approaches introduce a new type of actions, macro-actions. Macros are combinations of primitive actions, and their use can greatly improve the performance of the policy. On the other hand, the number of possible combinations of actions is exponentially large in their length [44]. For larger problems enumeration of all possible macros is out of the question, and the overall-policy function has to be approximated. Furthermore, at each decision point in a hierarchical planning or learning algorithm we now have the option to consider if any of the macro actions improves the current policy. The computational complexity of the planning and learning choices increases by the introduction of the macro actions [44], and approximation methods must be used. The efciency gains of the hierarchical behavioral policy must outweigh the higher cost of fnding this policy. Fourth, the quality of a behavioral policy that includes macro-actions may be less than that of a policy consisting only of primitive actions. The macro-actions may skip over possible shorter routes, that the primitive actions would have found. Conclusion There are advantages and disadvantages to hierarchical reinforcement learning. Whether an efcient policy can be constructed and whether its accuracy is good enough depends on the problem at hand, and also on the quality of the algorithms that are used to fnd this policy. For a long time, fnding good subgoals has been a major challenge. With recent algorithmic advances, especially in function approximation, important progress has been made. We will discuss these advances in the next section. 8.2 Divide and Conquer for Agents 229 8.2 Divide and Conquer for Agents To discuss hierarchical reinforcement learning, frst we will discuss a model, the options framework, that formalizes the concepts of subgoals and subpolicies. Next, we will describe the main challenge of hierarchical reinforcement learning, which is sample efciency. Then we will discuss the main part of this chapter: algorithms for fnding subgoals and subpolicies, and fnally we will provide an overview of algorithms that have been developed in the feld. 8.2.1 The Options Framework A hierarchical reinforcement learning algorithm tries to solve sequential decision problems more efciently by identifying common substructures and re-using subpolicies to solve them. The hierarchical approach has three challenges [619, 435]: fnd subgoals, fnd a meta-policy over these subgoals, and fnd subpolicies for these subgoals. Normally, in reinforcement learning, the agent follows in each state the action that is indicated by the policy. In 1999, Sutton, Precup and Singh [745] introduced the options framework. This framework introduces formal constructs with which subgoals and subpolicies can be incorporated elegantly into the reinforcement learning setting. The idea of options is simple. Whenever a state is reached that is a subgoal, then, in addition to following a primitive action suggested by the main policy, the option can be taken. This means that not the main action policy is followed, but the option policy, a macro action consisting of a diferent subpolicy specially aimed at satisfying the subgoal in one large step. In this way macros are incorporated into the reinforcement learning framework. We have been using the terms macro and option somewhat loosely until now; there is, however, a diference between macros and options. A macro is any group of actions, possibly open-ended. An option is a group of actions with a termination condition. Options take in environment observations and output actions until a termination condition is met. Formally, an option \ud835\udf14 has three elements [16]. Each option \ud835\udf14 = \u27e8\ud835\udc3c, \ud835\udf0b, \ud835\udefd\u27e9 has the following triple: \ud835\udc3c\ud835\udf14 The initiation set \ud835\udc3c \u2286 \ud835\udc46 are the states that the option can start from \ud835\udf0b\ud835\udf14(\ud835\udc4e|\ud835\udc60) The subpolicy \ud835\udf0b : \ud835\udc46 \u00d7 \ud835\udc34 \u2192 [0, 1] internal to this particular option \ud835\udefd\ud835\udf14(\ud835\udc60) The termination condition \ud835\udefd : \ud835\udc46 \u2192 [0, 1] tells us if \ud835\udf14 terminates in \ud835\udc60 The set of all options is denoted as \u03a9. In the options framework, there are thus two types of policies: the (meta-)policy over options \ud835\udf0b\u03a9(\ud835\udf14|\ud835\udc60) and the subpolicies \ud835\udf0b\ud835\udf14(\ud835\udc4e|\ud835\udc60). The subpolicies \ud835\udf0b\ud835\udf14 are short macros to get from \ud835\udc3c\ud835\udf14 to \ud835\udefd\ud835\udf14 quickly, using the previously learned macro (subpolicy). Temporal abstractions mix actions of diferent granularity, short and long, primitive action and subpolicy. They allow 230 8 Hierarchical Reinforcement Learning Fig. 8.1 Multi-Room Grid [745] traveling from \ud835\udc3c to \ud835\udefd without additional learning, using a previously provided or learned subpolicy. One of the problems for which the options framework works well, is room navigation in a Grid world (Fig. 8.1). In a regular reinforcement learning problem the agent would learn to move step by step. In hierarchical reinforcement learning the doors between rooms are bottleneck states, and are natural subgoals. Macro actions (subpolicies) are to move to a door in one multi-step action (without considering alternative actions along the way). Then we can go to a diferent room, if we choose the appropriate option, using another macro, closer to where the main goal is located. The four-room problem from the fgure is used in many research works in hierarchical reinforcement learning. Universal Value Function In the original options framework the process of identifying the subgoals (the hallways, doors) is external. The subgoals have to be provided manually, or by other methods [609, 316, 444, 727]. Subsequently, methods have been published to learn these subgoals. Options are goal-conditioned subpolicies. More recently a generalization to parameterized options has been presented in the universal value function, by Schaul et al. [665]. Universal value functions provide a unifed theory for goal-conditioned parameterized value approximators \ud835\udc49(\ud835\udc60, \ud835\udc54, \ud835\udf03). 8.2.2 Finding Subgoals Whether the hierarchical method improves over a traditional fat method depends on a number of factors. First, there should be enough repeating structure in the domain to be exploited (are there many rooms?), second, the algorithm must fnd 8.2 Divide and Conquer for Agents 231 Find Find Name Agent Environment Subg Subpol Ref STRIPS Macro-actions STRIPS planner [241] Abstraction Hier. State abstraction Scheduling/plan. + + [415] HAM Abstract machines MDP/maze [588] MAXQ Value function decomposition Taxi [196] HTN Task networks Block world [171, 272] Bottleneck Randomized search Four room + + [727] Feudal manager/worker, RNN Atari + + [811, 182] Self p. goal emb. self play subgoal Mazebase, AntG + + [732] Deep Skill Netw. deep skill array, policy distillation Minecraft + + [766] STRAW end-to-end implicit plans Atari + + [810] HIRO of-policy Ant maze + + [546] Option-critic policy-gradient Four room + + [45] HAC actor critic, hindsight exper. repl. Four room ant + + [471, 19] Modul. pol. hier. bit-vector, intrinsic motivation FetchPush + + [590] h-DQN intrinsic motivation Montezuma\u2019s R. + [435] Meta l. sh. hier. shared primitives, strength metric Walk, crawl + + [256] CSRL model-based transition dynamics Robot tasks + [477] Learning Repr. unsup. subg. disc., intrinsic motiv. Montezuma\u2019s R. + + [619] AMIGo Adversarially intrinsic goals MiniGrid PCG + + [125] Table 8.1 Hierarchical Reinforcement Learning Approaches (Tabular and Deep) appropriate subgoals (can it fnd the doors?), third, the options that are found must repeat many times (is the puzzle played frequently enough for the optionfnding cost to be ofset?), and, fnally, subpolicies must be found that give enough improvement (are the rooms large enough that options outweigh actions?). The original options framework assumes that the structure of the domain is obvious, and that the subgoals are given. When this is not the case, then the subgoals must be found by the algorithm. Let us look at an overview of approaches, both tabular and with deep function approximation. 8.2.3 Overview of Hierarchical Algorithms The options framework provides a convenient formalism for temporal abstraction. In addition to the algorithms that can construct policies consisting of individual actions, we need algorithms that fnd the subgoals, and learn the subpolicies. Finding efcient algorithms for the three tasks is important in order to be able to achieve an efciency advantage over ordinary \u201cfat\u201d reinforcement learning. Hierarchical reinforcement learning is based on subgoals. It implements a toplevel policy over these subgoals, and subpolicies to solve the subgoals. The landscape of subgoals determines to a great extent the efciency of the algorithm [216]. In recent years new algorithms have been developed to fnd sub-policies for options, and the feld has received a renewed interest [592]. Table 8.1 shows a list of approaches. 232 8 Hierarchical Reinforcement Learning The table starts with classic tabular approaches (above the line). It continues with more recent deep learning approaches. We will now look at some of the algorithms. 8.2.3.1 Tabular Methods Divide and conquer is a natural method to exploit hierarchical problem structures. A famous early planning system is STRIPS, the Stanford Research Insititute Problem Solver, designed by Richard Fikes and Nils Nilsson in the 1970s [241]. STRIPS created an extensive language for expressing planning problems, and was quite infuential. Concepts from STRIPS are at the basis of most modern planning systems, action languages, and knowledge representation systems [266, 51, 798]. The concept of macros as open-ended groups of actions was used in STRIPS to create higher-level primitives, or subroutines. Later planning-based approaches are Parr and Russell\u2019s hierarchical abstract machines [588] and Dietterich\u2019s MAXQ [196]. Typical applications of these systems are the blocks world, in which a robot arm has to manipulate blocks, stacking them on top of each other, and the taxi world, which we have seen in earlier chapters. An overview of these and other early approaches can be found in Barto et al. [56]. Many of these early approaches focused on macros (the subpolicies), and require that the experimenters identify the subgoals in a planning language. For problems where no such obvious subgoals are available, Knoblock [415] showed how abstraction hierarchies can be generated, although Backstrom et al. [44] found that doing so can be exponentially less efcient. For small room problems, however, Stolle and Precup [727] showed that subgoals can be found in a more efcient way, using a short randomized search to fnd bottleneck states that can be used as subgoals. This approach fnds subgoals automatically, and efciently, in a rooms-grid world. Tabular hierarchical methods were mostly applied to small and low-dimensional problems, and have difculty fnding subgoals, especially for large problems. The advent of deep function approximation methods attracted renewed interest in hierarchical methods. 8.2.3.2 Deep Learning Function approximation can potentially reduce the problem of exponentially exploding search spaces that plague tabular methods, especially for subgoal discovery. Deep learning exploits similarities between states using commonalities between features, and allows larger problems to be solved. Many new methods were developed. The deep learning approaches in hierarchical reinforcement learning typically are end-to-end: they generate both appropriate subgoals and their policies. Feudal networks is an older idea from Dayan and Hinton in which an explicit control hierarchy is built of managers and workers that work on tasks and subtasks, organized as in a feudal fefdom [182]. This idea was used 15 years later as a model for hierarchical deep reinforcement learning by Vezhnevets et al. [811], out8.2 Divide and Conquer for Agents 233 performing non-hierarchical A3C on Montezuma\u2019s Revenge, and performing well on other Atari games, achieving a similar score as Option-critic [45]. The approach uses a manager that sets abstract goals (in latent space) for workers. The feudal idea was also used as inspiration for a multi-agent cooperative reinforcement learning design [9], on proof of concept cooperative multi-agent problems on pre-specifed hierarchies. Other deep learning approaches include deep skill networks [766], of-policy approaches [546], and self-play [732]. The latter uses an intrinsic motivation approach to learn both a low level actor and the representation of the state space [597]. Subgoals are learned at the higher level, after which policies are trained at the lower level. Application environments for deep learning have become more challenging, and now include Minecraft, and robotic tasks such as ant navigation in multiple rooms, and maze navigation. The approaches outperform basic non-hierarchical approaches such as DQN. In STRAW Vezhnevets et al. [810] learns a model of macros-actions, and is evaluated on text recognition tasks and on Atari games such as PacMan and Frostbite, showing promising results. Zhang et al. [873] use world models to learn latent landmarks (subgoals) for graph-based planning (see also Sect. 5.2.1.2). Almost two decades after the options framework was introduced, Bacon et al. [45] introduced the option-critic approch. Option-critic extends the options framework with methods to learn the option subgoal and subpolicy, so that it does not have to be provided externally anymore. The options are learned similar to actor critic using a gradient-based approach. The intra-option policies and termination functions, as well as the policy over options are learned simultaneously. The user of the Option-critic approach has to specify how many options have to be learned. The Option-critic paper reports good results for experiments in a four-room environment with 4 and with 8 options (Fig. 8.2). Option critic learns options in an end-to-end fashion that scales to larger domains, outperforming DQN in four ALE games (Asterix, Seaquest, Ms. Pacman, Zaxxon) [45]. Levy et al. [471] presented an approach based on Option critic, called Hierarchical actor critic. This approach can learn the goal-conditioned policies at diferent levels concurrently, where previous approaches had to learn them in a bottom up fashion. In addition, Hierarchical actor critic uses a method for learning the multiple levels of policies for sparse rewards, using Hindsight experience replay [19]. In typical robotics tasks, the reinforcement learning algorithm learns more from a successful outcome (bat hits the ball) than from an unsuccessful outcome (bat misses the ball, low). In this failure case a human learner would draw the conclusion that we can now reach another goal, being bat misses the ball if we aim low. Hindsight experience replay allows learning to take place by incorporating such adjustments of the goal using the beneft of hindsight, so that the algorithm can now also learn from failures, by pretending that they were the goal that you wanted to reach, and learn from them as if they were. Hierarchical actor critic has been evaluated on grid world tasks and more complex simulated robotics environments, using a 3-level hierarchy. 234 8 Hierarchical Reinforcement Learning Fig. 8.2 Termination Probabilities Learned with 4 Options by Option-Critic [45]; Options Tend to Favor Squares Close to Doors A fnal approach that we mention is AMIGo [125], which is related to intrinsic motivation. It uses a teacher to adversarially generate goals for a student. The student is trained with increasingly challenging goals to learn general skills. The system efectively builds up an automatic curriculum of goals. It is evaluated on MiniGrid, a parameterized world that is generated by procedural content generation [621, 143]. Conclusion Looking back at the list of advantages and disadvantages at the start of this chapter, we see a range of interesting and creative ideas that achieve the advantages (Sect. 8.1.1) by providing methods to address the disadvantages (Sect. 8.1.2). In general, the tabular methods are restricted to smaller problems, and often need to be provided with subgoals. Most of the newer deep learning methods fnd subgoals by themselves, for which then subpolicies are found. Many promising methods have been discussed, and most report to outperform one or more fat baseline algorithms. The promising results stimulate further research in deep hierarchical methods, and more benchmark studies of large problems are needed. Let us have a closer look at the environments that have been used so far. 8.3 Hierarchical Environments 235 Fig. 8.3 Four Rooms, and One Room with Subpolicy and Subgoal [745] 8.3 Hierarchical Environments Many environments for hierarchical reinforcement learning exist, starting with mazes and the four-room environment from the options paper. Environments have evolved with the rest of the feld of reinforcement learning; for hierarchical reinforcement learning no clear favorite benchmark has emerged, although Atari en MuJoCo tasks are often used. In the following we will review some of the environments that are used in algorithmic studies. Most hierarchical environments are smaller than typically used for model-free fat reinforcement learning, although some studies do use complex environments, such as StarCraft. 8.3.1 Four Rooms and Robot Tasks Sutton et al. [745] presented the four rooms problems to illustrate how the options model worked (Fig. 8.3; left panel). This environment has been used frequently in subsequent papers on reinforcement learning. The rooms are connected by hallways. Options point the way to these hallways, which lead to the goal \ud835\udc3a2 of the environment. A hierarchical algorithm should identify the hallways as the subgoals, and create subpolicies for each room to go to the hallway subgoal (Fig. 8.3; right panel). The four-room environment is a toy environment with which algorithms can be explained. More complex versions can be created by increasing the dimensions of the grids and by increasing the number of rooms. The Hierarchical actor critic paper uses the four-room environment as a basis for a robot to crawl through. The agent has to learn both the locomotion task and solving the four-room problem (Fig. 8.4). Other environments that are used for hierarchical reinforcement learning are robot tasks, such as shown in Fig. 8.5 [633]. 236 8 Hierarchical Reinforcement Learning Fig. 8.4 Ant in Four Rooms [471] Fig. 8.5 Six Robot Tasks [633] 8.3.2 Montezuma\u2019s Revenge One of the most difcult situations for reinforcement learning is when there is little reward signal, and when it is delayed. The game of Montezuma\u2019s Revenge consists of long stretches in which the agent has to walk without the reward changing. Without smart exploration methods this game cannot be solved. Indeed, the game has long been a test bed for research into goal-conditioned and exploration methods. For the state in Fig. 8.6, the player has to go through several rooms while collecting items. However, to pass through doors (top right and top left corners), the player needs the key. To pick up the key, the player has to climb down the ladders 8.3 Hierarchical Environments 237 Fig. 8.6 Montezuma\u2019s Revenge [71] Fig. 8.7 Intrinsic Motivation in Reinforcement Learning [712] and move towards the key. This is a long and complex sequence before receiving the reward increments for collecting the key. Next, the player has to go to the door to collect another increase in reward. Flat reinforcement learning algorithms struggle with this environment. For hierarchical reinforcement the long stretches without a reward can be an opportunity to show the usefulness of the option, jumping through the space from states where the reward changes to another reward change. To do so, the algorithm has to be able to identify the key as a subgoal. Rafati and Noelle [619] learn subgoals in Montezuma\u2019s Revenge, and so do Kulkarni et al. [435]. Learning to choose promising subgoals is a challenging problem by itself. Once subgoals are found, the subpolicies can be learned by introducing a reward signal for achieving the subgoals. Such intrinsic rewards are related to intrinsic motivation and the psychological concept of curiosity [29, 577]. Figure 8.7 illustrates the idea behind intrinsic motivation. In ordinary reinforcement learning, a critic in the environment provides rewards to the agent. When the agent has an internal environment where an internal critic provides rewards, these internal rewards provide an intrinsic motivation to the agent. This mechanism aims to more closely model exploration behavior in animals and humans [712]. For example, during curiosity-driven activities, children use knowledge to generate intrinsic goals while playing, building block structures, etc. While doing this, they 238 8 Hierarchical Reinforcement Learning construct subgoals such as putting a lighter entity on top of a heavier entity in order to build a tower [435, 712]. Intrinsic motivation is an active feld of research. A recent survey is [29]. Montezuma\u2019s Revenge has also been used as benchmark for the Go-Explore algorithm, that has achieved good results in sparse reward problems, using a goalconditioned policy with cell aggregation [222]. Go-Explore performs a planning-like form of backtracking, combining elements of planning and learning in a diferent way than AlphaZero. 8.3.3 Multi-Agent Environments Many multi-agent problems are a natural match for hierarchical reinforcement learning, since agents often work together in teams or other hierarchical structure. For multi-agent hierarchical problems a multitude of diferent environments are used. Makar et al. [499, 273] study cooperative multi-agent learning, and use small tasks such as a two-agent cooperative trash collection task, the dynamic rescheduling of automated guided vehicles in a factory, as well as an environment in which agents communicate amongst eachother. Han et al. [312] use a multi-agent Taxi environment. Tang et al. [753] also use robotic trash collection. Due to the computational complexity of multi-agent and hierarchical environments, many of the environments are of lower dimensionality than what we see in model-free and model-based single-agent reinforcement learning. There are a few exceptions, as we saw in the previous chapter (Capture the Flag and StarCraft). However, the algorithms that were used for these environments were based on population-based self-play algorithms that are well-suited for parallelization; hierarchical reinforcement algorithms of the type that we have discussed in this chapter were of less importance [813]. 8.3.4 Hands On: Hierarchical Actor Citic Example The research reported in this chapter is of a more manageable scale than in some other chapters. Environments are smaller, computational demands are more reasonable. Four-room experiments and experiments with movement of single robot arms invites experimentation and tweaking. Again, as in the other chapters, the code of most papers can be found online on GitHub. Hierarchical reinforcement learning is well suited for experimentation because the environments are small, and the concepts of hierarchy, team, and subgoal, are intuitively appealing. Debgugging one\u2019s implementation should be just that bit easier when the desired behavior of the diferent pieces of code is clear. 8.3 Hierarchical Environments 239 Algorithm 8.1 Hierarchical Actor Critic [471] Input: Key agent parameters: number of levels in hierarchy \ud835\udc58, maximum subgoal horizon \ud835\udc3b, and subgoal testing frequency \ud835\udf06 Output: \ud835\udc58 trained actor and critic functions \ud835\udf0b0, ..., \ud835\udf0b\ud835\udc58\u22121, \ud835\udc440, ..., \ud835\udc44\ud835\udc58\u22121 for \ud835\udc40 episodes do \u22b2 Train for M episodes \ud835\udc60 \u2190 \ud835\udc46init, \ud835\udc54 \u2190 \ud835\udc3a\ud835\udc58\u22121 \u22b2 Sample initial state and task goal train-level(\ud835\udc58 \u2212 1, \ud835\udc60, \ud835\udc54) \u22b2 Begin training Update all actor and critic networks end for function train-level(\ud835\udc56 :: level, \ud835\udc60 :: state, \ud835\udc54 :: goal) \ud835\udc60\ud835\udc56 \u2190 \ud835\udc60, \ud835\udc54\ud835\udc56 \u2190 \ud835\udc54 \u22b2 Set current state and goal for level \ud835\udc56 for \ud835\udc3b attempts or until \ud835\udc54\ud835\udc5b, \ud835\udc56 \u2264 \ud835\udc5b < \ud835\udc58 achieved do \ud835\udc4e\ud835\udc56 \u2190 \ud835\udf0b\ud835\udc56 (\ud835\udc60\ud835\udc56, \ud835\udc54\ud835\udc56) + noise (if not subgoal testing) \u22b2 Sample (noisy) action from policy if \ud835\udc56 > 0 then Determine whether to test subgoal \ud835\udc4e\ud835\udc56 \ud835\udc60\u2032 \ud835\udc56 \u2190 train-level(\ud835\udc56 \u2212 1, \ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56) \u22b2 Train level \ud835\udc56 \u2212 1 using subgoal \ud835\udc4e\ud835\udc56 else Execute primitive action \ud835\udc4e0 and observe next state \ud835\udc60\u2032 0 end if \u22b2 Create replay transitions if \ud835\udc56 > 0 and \ud835\udc4e\ud835\udc56 missed then if \ud835\udc4e\ud835\udc56 was tested then \u22b2 Penalize subgoal \ud835\udc4e\ud835\udc56 Replay_Bufer\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f = Penalty, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 = \ud835\udc54\ud835\udc56, \ud835\udefe = 0] end if \ud835\udc4e\ud835\udc56 \u2190 \ud835\udc60\u2032 \ud835\udc56 \u22b2 Replace original action with action executed in hindsight end if \u22b2 Evaluate executed action on current goal and hindsight goals Replay_Bufer\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f \u2208 {\u22121, 0}, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 = \ud835\udc54\ud835\udc56, \ud835\udefe \u2208 {\ud835\udefe, 0}] HER_Storage\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f =TBD, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 =TBD, \ud835\udefe =TBD] \ud835\udc60\ud835\udc56 \u2190 \ud835\udc60\u2032 \ud835\udc56 end for Replay_Bufer\ud835\udc56 \u2190 Perform HER using HER_Storage\ud835\udc56 transitions return \ud835\udc60\u2032 \ud835\udc56 \u22b2 Output current state end function To get you started with hierarchical reinforcement learning we will go to HAC: Hierarchical actor critic [471]. Algorithm 8.1 shows the pseudocode, where TBD is the subgoal in hindsight [471]. A blog1 with animations has been written, a video2 has been made of the results, and the code can be found in GitHub.3 To run the hierarchical actor critic experiments, you need MuJoCo and the required Python wrappers. The code is TensorFlow 2 compatible. When you have cloned the repository, run the experiment with 1 http://bigai.cs.brown.edu/2019/09/03/hac.html 2 https://www.youtube.com/watch?v=DYcVTveeNK0 3 https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC240 8 Hierarchical Reinforcement Learning python3 initialize_HAC.py --retrain which will train a UR5 reacher agent with a 3-level hierarchy. Here is a video4 that shows how it should look like after 450 training episodes. You can watch your trained agent with the command python3 initialize_HAC.py --test --show The README at the GitHub repository contains more suggestions on what to try. You can try diferent hyperparameters, and you can modify the designs, if you feel like it. Happy experimenting! Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary A typical reinforcement learning algorithm moves in small steps. For a state, it picks an action, gives it to the environment for a new state and a reward, and processes the reward to pick a new action. Reinforcement learning works step by small step. In contrast, consider the following problem: in the real world, when we plan a trip from A to B, we use abstraction to reduce the state space, to be able to reason at a higher level. We do not reason at the level of footsteps to take, but we frst decide on the mode of transportation to get close to our goal, and then we fll in the diferent parts of the journey with small steps. Hierarchical reinforcement learning tries to mimic this idea: conventional reinforcement learning works at the level of a single state; hierarchical reinforcement learning performs abstraction, solving subproblems in sequence. Temporal abstraction is described in a paper by Sutton et al. [745]. Hierarchical reinforcement learning uses the principles of divide and conquer to make solving large problems feasible. It fnds subgoals in the space that it solves with subpolicies (macros or options). Despite the appealing intuition, progress in hierarchical reinforcement learning was initially slow. Finding these new subgoals and subpolicies is a computationally intensive problems that is exponential in the number of actions, and in some situations it is quicker to use conventional \u201cfat\u201d reinforcement learning methods, unless domain knowledge can be exploited. The advent of deep learning provided a 4 https://www.youtube.com/watch?v=R86Vs9Vb6Bc 8.3 Hierarchical Environments 241 boost to hierarchical reinforcement learning, and much progress is being reported in important tasks such as learning subgoals automatically, and fnding subpolicies. Although popular for single-agent reinforcement learning, hierarchical methods are also used in multi-agent problems. Multi-agent problems often feature agents that work in teams, that cooperate within, and compete between the teams. Such an agent-hierarchy is a natural ft for hierachical solution methods. Hierarchical reinforcment learning remains a promising technique. Further Reading Hierarchical reinforcement learning, and subgoal fnding, have a rich and long history [248, 588, 745, 609, 196, 316, 444, 56, 592]; see also Table 8.1. Macro-actions are a basic approach [316, 626]. Others, using macros, are [855, 852, 214]. The options framework has provided a boost to the development of the feld [745]. Other approaches are MAXQ [195] and Feudal networks [811]. Earlier tabular approaches are [248, 588, 745, 609, 196, 316, 444]. Recent method are Option-critic [45] and Hierarchical actor-critic [471]. There are many deep learning methods for fnding subgoals and subpolicies [471, 582, 248, 598, 546, 256, 806, 665, 735, 177]. Andrychowicz et al. [19] introduce Hindsight experience replay, which can improve performance for hierarchical methods. Instrinsic motivation is a concept from developmental neuroscience that has come to reinforcement learning with the purpose of providing learning signals in large spaces. It is related to curiosity. Botvinick et al. [101] have written an overview of hierarchical reinforcement learning and neuroscience. Aubret et al. [29] provide a survey of intrinsic motivation for reinforcement learning. Instrinsic motivation is used by [435, 619]. Intrinsic motivation is closely related to goaldriven reinforcement learning [665, 650, 576, 577, 578]. Exercises It is time for the Exercises to test your knowledge. Questions Below are some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer should be sufcient. 1. Why can hierarchical reinforcement learning be faster? 2. Why can hierarchical reinforcement learning be slower? 3. Why may hierarchical reinforcement learning give an answer of lesser quality? 4. Is hierachical reinforcement more general or less general? 242 8 Hierarchical Reinforcement Learning 5. What is an option? 6. What are the three elements that an option consists of? 7. What is a macro? 8. What is intrinsic motivation? 9. How do multi agent and hierarchical reinforcement learning ft together? 10. What is so special about Montezuma\u2019s Revenge? Exercises Let us go to the programming exercises to become more familiar with the methods that we have covered in this chapter. 1. Four Rooms Implement a hierarchical solver for the four-rooms environment. You can code the hallway subgoals using domain knowledge. Use a simple tabular, planning, approach. How will you implement the subpolicies? 2. Flat Implement a fat planning or Q-learning-based solver for 4-rooms. Compare this program to the tabular hierarchical solver. Which is quicker? Which of the two does fewer environment actions? 3. Sokoban Implement a Sokoban solver using a hierarchical approach (challenging). The challenge in Sokoban is that there can be dead-ends in the game that you create rendering the game unsolvable (also see the literature [696, 291]). Recognizing these dead-end moves is important. What are the subgoals? Rooms, or each box-task is one subgoal, or can you fnd a way to code dead-ends as subgoal? How far can you get? Find Sokoban levels.567 4. Petting Zoo Choose one of the easier multi-agent problems from the Petting Zoo [761], introduce teams, and write a hierarchical solver. First try a tabular planning approach, then look at hierarchical actor critic (challenging). 5. StarCraft The same as the previous exercise, only now with StarCraft (very challenging). 5 http://sneezingtiger.com/sokoban/levels.html 6 http://www.sokobano.de/wiki/index.php?title=Level_format 7 https://www.sourcecode.se/sokoban/levels Chapter 9 Meta-Learning Although current deep reinforcement learning methods have obtained great successes, training times for most interesting problems are high; they are often measured in weeks or months, consuming time and resources\u2014as you may have noticed while doing some of the exercises at the end of the chapters. Model-based methods aim to reduce the sample complexity in order to speed up learning\u2014but still, for each new task a new network has to be trained from scratch. In this chapter we turn to another approach, that aims to re-use information learned in earlier training tasks from a closely related problem. When humans learn a new task, they do not learn from a blank slate. Children learn to walk and then they learn to run; they follow a training curriculum, and they remember. Human learning builds on existing knowledge, using knowledge from previously learned tasks to facilitate the learning of new tasks. In machine learning such transfer of previously learned knowledge from one task to another is called transfer learning. We will study it in this chapter. Humans learn continuously. When learning a new task, we do not start from scratch, zapping our minds frst to emptiness. Previously learned task-representations allow us to learn new representations for new tasks quickly; in efect, we have learned to learn. Understanding how we (learn to) learn has intrigued artifcial intelligence researchers since the early days, and it is the topic of this chapter. The felds of transfer learning and meta-learning are tightly related. For both, the goal is to speed up learning a new task, using previous knowledge. In transfer learning, we pretrain our parameter network with knowledge from a single task. In meta-learning, we use multiple related tasks. In this chapter, we frst discuss the concept of lifelong learning, something that is quite familiar to human beings. Then we discuss transfer learning, followed by meta-learning. Next, we discuss some of the benchmarks that are used to test transfer learning and meta-learning. The chapter is concluded with exercises, a summary, and pointers to further reading. 243 244 9 Meta-Learning Core Concepts \u2022 Knowledge transfer \u2022 Learning to learn Core Problem \u2022 Speed-up learning with knowledge from related tasks Core Algorithms \u2022 Pretraining (Listing 9.1) \u2022 Model-Agnostic Meta-Learning (Alg. 9.1) Foundation Models Humans are good at meta-learning. We learn new tasks more easily after we have learned other tasks. Teach us to walk, and we learn how to run. Teach us to play the violin, the viola, and the cello, and we more easily learn to play the double bass (Fig. 9.1). Current deep learning networks are large, with many layers of neurons and millions of parameters. For new problems, training large networks on large datasets or environments takes time, up to weeks, or months\u2014both for supervised and for reinforcement learning. In order to shorten training times for subsequent networks, these are often pretrained, using foundation models [96]. With pretraining, some of the exisiting weights of another network are used as starting point for fnetuning a network on a new dataset, instead of using a randomly initialized network. Pretraining works especially well on deeply layered architectures. The reason is that the \u201cknowledge\u201d in the layers goes from generic to specifc: lower layers contain generic flters such as lines and curves, and upper layers contain more specifc flters such as ears, noses, and mouths (for a face recognition application) [459, 497]. These lower layers contain more generic information that is well suited for transfer to other tasks. Foundation models are large models in a certain feld, such as image recognition, or natural language processing, that are trained extensively on large datasets. Foundation models contain general knowledge, that can be specialized for a certain purpose. The world of applied deep learning has moved from training a net from scratch for a certain problem, to taking a part of an existing net that is trained for a related problem and then fnetuning it on the new task. Nearly all state-of-the-art 9 Meta-Learning 245 Fig. 9.1 Violin, viola, cello and double bass Fig. 9.2 Imagenet Thumbnails [646] visual perception approaches rely on the same approach: (1) pretrain a convolutional network on a large, manually annotated image classifcation dataset and (2) fnetune the network on a smaller, task-specifc dataset [278, 200, 865, 8, 360], see Fig. 9.2 for some thumbnails of Imagenet [646]. For natural language recognition, pretraining is also the norm\u2014for example, for large-scale pretrained language models such as Word2vec [513, 514], BERT [193] and GPT-3 [617]. In this chapter, we will study pretraining, and more. 246 9 Meta-Learning Name Dataset Task Single-task Learning \ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \u2286 \ud835\udc37, \ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \u2286 \ud835\udc37 \ud835\udc47 = \ud835\udc47\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b = \ud835\udc47\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 Transfer Learning \ud835\udc371 \u226b \ud835\udc372 \ud835\udc471 \u2260 \ud835\udc472 Multi-task Learning \ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \u2286 \ud835\udc37, \ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \u2286 \ud835\udc37 \ud835\udc471 \u2260 \ud835\udc472 Domain Adaptation \ud835\udc371 \u2260 \ud835\udc372 \ud835\udc471 = \ud835\udc472 Meta-Learning {\ud835\udc371, . . . , \ud835\udc37\ud835\udc41\u22121} \u226b \ud835\udc37\ud835\udc41 \ud835\udc471, . . . , \ud835\udc47\ud835\udc5b\u22121 \u2260 \ud835\udc47\ud835\udc41 Table 9.1 Diferent Kinds of Supervised Learning. A typical single learning task is to classify pictures of animals into diferent classes. The dataset is split into a train and a testset, and the task (loss function) is the same at train and test time. A transfer learning task uses part of the knowledge (network parameters) that are learned on a large dataset to initialize a second network, that is trained subsequently (fne tuned) on a diferent dataset. This second dataset/learning task is related to the frst task, so the learning the second task goes faster. For example, having learned to recognize cars, may be useful to speedup recognizing trucks. In multi-task learning several related tasks are trained at the same time, possibly beneftting from better regularization. An example could be training spam flters for diferent users at the same time. Domain adaptation tries to adapt the network to a new dataset of related examples, such as images of pedestrians in diferent light conditions. Meta-learning tries to learn meta knowledge, such as hyperparameters over a sequence of related (larger) learning tasks, so that a new learning task goes faster. In deep meta-learning these hyperparameters include the initial network parameters, in this sense meta-learning can be considered to be multi-job transfer learning. 9.1 Learning to Learn Related Problems Training times for modern deep networks are large. Training AlexNet for ImageNet took 5-6 days on 2 GPUs in 2012 [431], see Sect. B.3.1. In reinforcement learning, training AlphaGo took weeks [703, 706], in natural language processing, training also takes a long time [193], even excessively long as in the case of GPT-3 [114]. Clearly, some solution is needed. Before we look closer at transfer learning, let us have a look at the bigger picture: lifelong learning. When humans learn a new task, learning is based on previous experience. Initial learning by infants of elementary skills in vision, speech, and locomotion takes years. Subsequent learning of new skills builds on the previously acquired skills. Existing knowledge is adapted, and new skills are learned based on previous skills. Lifelong learning remains a long-standing challenge for machine learning; in current methods the continuous acquisition of information often leads to interference of concepts or catastrophic forgetting [701]. This limitation represents a major drawback for deep networks that typically learn representations from stationary batches of training data. Although some advances have been made in narrow domains, signifcant advances are necessary to approach generally-applicable lifelong learning. Diferent approaches have been developed. Among the methods are metalearning, domain adaptation, multi-task learning, and pretraining. Table 9.1 lists these approaches, together with regular single task learning. The learning tasks are formulated using datasets, as in a regular supervised setting. The table shows 9.2 Transfer Learning and Meta-Learning Agents 247 how the lifelong learning methods difer in their training and test dataset, and the diferent learning tasks. The frst line shows regular single-task learning. For single-task learning, the training and test dataset are both drawn from the same distribution (the datasets do not contain the same examples, but they are drawn from the same original dataset and the data distribution is expected to be the same), and the task to perform is the same for training and test. In the next line, for transfer learning, networks trained on one dataset are used to speedup training for a diferent task, possibly using a much smaller dataset [581]. Since the datasets are not drawn from the same master dataset, their distribution will difer, and there typically is only an informal notion of how \u201crelated\u201d the datasets are. However, in practice transfer learning often provides signifcant speedups, and transfer learning, pretraining and fnetuning are currently used in many real-world training tasks, sometimes using large foundation models as a basis. In multi-task learning, more than one task is learned from one dataset [129]. The tasks are often related, such as classifcation tasks of diferent, but related, classes of images, or learning spam flters for diferent email-users. Regularization may be improved when a neural network is trained on related tasks at the same time [59, 151]. So far, our learning tasks were trying to speedup learning diferent tasks with related data. Domain adaptation switches this around: the task remains the same, but the data changes. In domain adaptation, a diferent dataset is used to perform the same task, such as recognizing pedestrians in diferent light conditions [782]. In meta-learning, both datasets and tasks are diferent, although not too diferent. In meta-learning, a sequence of datasets and learning tasks is generalized to learn a new (related) task quickly [106, 351, 363, 667]. The goal of meta-learning is to learn hyperparameters over a sequence of learning tasks. 9.2 Transfer Learning and Meta-Learning Agents We will now introduce transfer learning and meta-learning algorithms. Where in normal learning we would intialize our parameters randomly, in transfer learning we initialize them with (part of) the training results of another training task. This other task is related in some way to the new task, for example, a task to recognize images of dogs playing in a forest is initalized on a dataset of dogs playing in a park. The parameter transfer of the old task is called pretraining, the second phase, where the network learns the new task on the new dataset, is called fnetuning. The pretraining will hopefully allow the new task to train faster. Where transfer learning transfers knowledge from a single previous task, metalearning aims to generalize knowledge from multiple previous learning tasks. Metalearning tries to learn hyperparameters over these related learning tasks, that tell the algorithm how to learn the new task. Meta-learning thus aims to learn to learn. In deep meta-learning approaches, the initial network parameters are typically part 248 9 Meta-Learning of the hyperparameters. Note that in transfer learning we also use (part of) the parameters to speed up learning (fnetuning) a new, related task. We can say that deep meta-learning generalizes transfer learning by learning the initial parameters over not one but a sequence of related tasks [351, 364]. (Defnitions are still in fux, however, and diferent authors and diferent felds have diferent defnitions.) Transfer learning has become part of the standard approach in machine learning, meta-learning is still an area of active research. We will look into meta-learning shortly, after we have looked into transfer learning, multi-task learning, and domain adaptation. 9.2.1 Transfer Learning Transfer learning aims to improve the process of learning new tasks using the experience gained by solving similar problems [606, 774, 773, 585]. Transfer learning aims to transfer past experience of source tasks and use it to boost learning in a related target task [581, 879]. In transfer learning, we frst train a base network on a base dataset and task, and then we repurpose some of the learned features to a second target network to be trained on a target dataset and task. This process works better if the features are general, meaning suitable to both base and target tasks, instead of specifc to the base task. This form of transfer learning is called inductive transfer. The scope of possible models (model bias) is narrowed in a benefcial way by using a model ft on a diferent but related task. First we will look at task similarity, then at transfer learning, multi-task learning, and domain adaptation. 9.2.1.1 Task Similarity Clearly, pretraining works better when the tasks are similar [129]. Learning to play the viola based on the violin is more similar than learning the tables of multiplication based on tennis. Diferent measures can be used to measure the similarity of examples and features in datasets, from linear one-dimensional measures to nonlinear multi-dimensional measures. Common measures are the cosine similarity for real-valued vectors and the radial basis function kernel [752, 809], but many more elaborate measures have been devised. Similarity measures are also used to devise meta-learning algorithms, as we will see later. 9.2 Transfer Learning and Meta-Learning Agents 249 9.2.1.2 Pretraining and Finetuning When we want to transfer knowledge, we can transfer the weights of the network, and then start re-training with the new dataset. Please refer back to Table 9.1. In pretraining the new dataset is smaller than the old dataset \ud835\udc371 \u226b \ud835\udc372, and we train for a new task \ud835\udc471 \u2260 \ud835\udc472, which we want to train faster. This works when the new task is diferent, but similar, so that the old dataset \ud835\udc371 contains useful information for the new task \ud835\udc472. To learn new image recognition problems, it is common to use a deep learning model pre-trained for a large and challenging image classifcation task such as the ImageNet 1000-class photograph classifcation competition. Three examples of pretrained models include: the Oxford VGG Model, Google\u2019s Inception Model, Microsoft\u2019s ResNet Model. For more examples, see the Cafe Model Zoo,1 or other zoos2 where more pre-trained models are shared. Transfer learning is efective because the images were trained on a corpus that requires the model to make predictions on a large number of classes, requiring the model to be general, and since it efciently learns to extract features in order to perform well. Convolutional neural network features are more generic in lower layers, such as color blobs or Gabor flters, and more specifc to the original dataset in higher layers. Features must eventually transition from general to specifc in the last layers of the network [862]. Pretraining copies some of the layers to the new task. Care should be taken how much of the old task network to copy. It is relatively safe to copy the more general lower layers. Copying the more specifc higher layers may be detrimental to performance. In natural language processing a similar situation occurs. In natural language processing, a word embedding is used that is a mapping of words to a high-dimensional continuous vector where diferent words with a similar meaning have a similar vector representation. Efcient algorithms exist to learn these word representations. Two examples of common pre-trained word models trained on very large datasets of text documents include Google\u2019s Word2vec model [513] and Stanford\u2019s GloVe model [596]. 9.2.1.3 Hands-on: Pretraining Example Transfer learning and pretraining have become a standard approach to learning new tasks, especially when only a small dataset is present, or when we wish to limit training time. Let us have a look at a hands-on example that is part of the Keras distribution (Sect. B.3.3). The Keras transfer learning example provides a basic Imagenet-based approach, getting data from TensorFlow DataSets (TFDS). The example follows a supervised learning approach, although the learning and 1 https://caffe.berkeleyvision.org/model_zoo.html 2 https://modelzoo.co 250 9 Meta-Learning 1 base_model = keras. applications .Xception( 2 weights=\u2019imagenet \u2019, # Load weights pre -trained on ImageNet. 3 input_shape =(150 , 150, 3), 4 include_top=False) # Do not include the ImageNet classifier at the top. 5 6 base_model.trainable = False Listing 9.1 Pretraining in Keras (1): instantiate model fne-tuning phase can easily be substituted by a reinforcement learning setup. The Keras transfer learning example is at the Keras site,3 and can also be run in a Google Colab. The most common incarnation of transfer learning in the context of deep learning is the following worfklow: 1. Take layers from a previously trained model. 2. Freeze them, so as to avoid destroying any of the information they contain during future training rounds. 3. Add some new, trainable layers on top of the frozen layers. They will train on the new dataset using the old features as predictions. 4. Train the new layers on your new (small) dataset. 5. A last, optional, step, is fne-tuning of the frozen layers, which consists of unfreezing the entire model you obtained above, and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data. Let us look at how this workfow works in practice in Keras. At the Keras site we fnd an accessible example code for pretraining (see Listing 9.1). First, we instantiate a base model with pretrained weights. (We do not include the classifer on top.) Then, we freeze the base model (see Listing 9.2). Next, we create a new model on top, and train it. The Keras example contains this example and others, including fne-tuning. Please go to the Keras site and improve your experience with pretraining in practice. 9.2.1.4 Multi-task Learning Multi-task learning is related to transfer learning. In multi-task learning a single network is trained at the same time on multiple related tasks [772, 129]. In multi-task learning the learning process of one task benefts from the simultaneous learning of the related task. This approach is efective when the tasks have some commonality, such as learning to recognize breeds of dogs and breeds of cats. 3 https://keras.io/guides/transfer_learning/ 9.2 Transfer Learning and Meta-Learning Agents 251 1 inputs = keras.Input(shape =(150 , 150, 3)) 2 # The base_model is running in inference mode here , 3 # by passing \u2018training=False \u2018. This is important for fine -tuning 4 x = base_model(inputs , training=False) 5 # Convert features of shape \u2018base_model . output_shape [1:] \u2018 to vectors 6 x = keras.layers. GlobalAveragePooling2D ()(x) 7 # A Dense classifier with a single unit (binary classification ) 8 outputs = keras.layers.Dense (1)(x) 9 model = keras.Model(inputs , outputs) 10 11 12 model.compile(optimizer=keras. optimizers .Adam (), 13 loss=keras.losses. BinaryCrossentropy ( from_logits = True), 14 metrics =[ keras.metrics. BinaryAccuracy ()]) 15 model.fit(new_dataset , epochs =20, callbacks =... , validation_data =...) Listing 9.2 Pretraining in Keras (2): create new model and train In multi-task learning, related learning tasks are learned at the same time, whereas in transfer learning they are learned in sequence by diferent networks. Multi-task learning improves regularization by requiring the algorithm to perform well on a related learning task instead of penalizing all overftting uniformly [233, 26]. The two-headed AlphaGo Zero network optimizes for value and for policy at the same time in the same network [581, 129]. A multi-headed architecture is often used in multi-task learning, although in AlphaGo Zero the two heads are trained for two related aspects (policy and value) of the same task (playing Go games). Multi-task learning has been applied with success to Atari games [405, 404]. 9.2.1.5 Domain Adaptation Domain adaptation is necessary when there is a change in the data distribution between the training dataset and the test dataset (domain shift). This problem is related to out-of-distribution learning [470]. Domain shifts are common in practical applications of artifcial intelligence, such as when items must be recognized in diferent light conditions, or when the background changes. Conventional machinelearning algorithms often have difculty adapting to such changes. The goal of domain adaptation is to compensate for the variation among two data distributions, to be able to reuse information from a source domain on a diferent target domain [782], see Fig. 9.3 for a backpack in diferent circumstances. As was indicated in Table 9.1, domain adaptation applies to the situation where the tasks are the same \ud835\udc471 = \ud835\udc472 but the datasets are diferent \ud835\udc371 \u2260 \ud835\udc372, although still somewhat similar. For example, the task may be to recognize a backpack, but in a diferent orientation, or to recognize a pedestrian, but in diferent lighting. 252 9 Meta-Learning Fig. 9.3 Domain Adaptation: Recognizing Items in Diferent Circumstances is Difcult [301] Domain adaptation can be seen as the opposite of pretraining. Pretraining uses the same dataset for a diferent task, while domain adaptation adapts to a new dataset for the same task [127]. In natural language processing, examples of domain shift are an algorithm that has been trained on news items that is then applied to a dataset of biomedical documents [179, 733], or a spam flter that is trained on a certain group of email users, which is deployed to a new target user [74]. Sudden changes in the environment (pandemics, severe weather) can also upset machine learning algorithms. There are diferent techniques to overcome domain shift [168, 872, 856, 834]. In visual applications, adaptation can be achieved by re-weighting the samples of the frst dataset, or clustering them for visually coherent sub-domains. Other approaches try to fnd transformations that map the source distribution to the target, or learn a classifcation model and a feature transformation jointly [782]. Adversarial techniques where feature representations are encouraged to be difcult to distinguish can be used to achieve adaptation [792, 850, 211], see also Sect. B.2.6. 9.2.2 Meta-Learning Related to transfer learning is meta-learning. Where the focus in transfer learning is on transferring parameters from a single donor task to a receiver task for further fnetuning, in meta-learning the focus is on using the knowledge of a number of tasks to learn how to learn a new task faster and better. Regular machine learning 9.2 Transfer Learning and Meta-Learning Agents 253 learns examples for one task, meta-learning aims to learn across tasks. Machine learning learns parameters that approximate the function, meta-learning learns hyperparameters about the learning-function.4 This is often expressed as learning to learn, a phrase that has defned the feld ever since its introduction [669, 772, 774]. The term meta-learning has been used for many diferent contexts, not only for deep learning, but also for tasks ranging from hyperparameter optimization, algorithm selection, to automated machine learning. (We will briefy look at these in Sect. 9.2.2.5.) Deep meta reinforcement learning is an active area of research. Many algorithms are being developed, and much progress is being made. Table 9.2 lists nine algorithms that have been proposed for deep meta reinforcement learning (see the surveys [351, 364]). 9.2.2.1 Evaluating Few-Shot Learning Problems One of the challenges of lifelong machine learning is to judge the performance of an algorithm. Regular train-test generalization does not capture the speed of adaptation of a meta-learning algorithm. For this reason, meta-learning tasks are typically evaluated on their few-shot learning ability. In few-shot learning, we test if a learning algorithm can be made to recognize examples from classes from which it has seen only few examples in training. In few-shot learning prior knowledge is available in the network. To translate few-shot learning to a human setting, we can think of a situation where a human plays the double bass after only a few minutes of training on the double bass, but after years on the violin, viola, or cello. Meta-learning algorithms are often evaluated with few shot learning tasks, in which the algorithm must recognize items of which it has only seen a few examples. This is formalized in the \ud835\udc41-way-\ud835\udc58-shot approach [141, 445, 829]. Figure 9.4 illustrates this process. Given a large dataset D, a smaller training dataset \ud835\udc37 is sampled from this dataset. The \ud835\udc41-way-\ud835\udc58-shot classifcation problem constructs training dataset \ud835\udc37 such that it consists of \ud835\udc41 classes, of which, for each class, \ud835\udc58 examples are present in the dataset. Thus, the cardinality of |\ud835\udc37| = \ud835\udc41 \u00b7 \ud835\udc58. A full \ud835\udc41-way-\ud835\udc58-shot few-shot learning meta task T consists of many episodes in which base tasks T\ud835\udc56 are performed. A base task consists of a training set and a test set, to test generalization. In few-shot terminology, the training set is called the support set, and the test set is called the query set. The support set has size \ud835\udc41 \u00b7 \ud835\udc58, the query set consists of a small number of examples. The meta-learning algorithm can learn from the episodes of \ud835\udc41-way-\ud835\udc58-shot query/support base tasks, until at meta-test time the generalization of the meta-learning algorithm is tested with another query, as is illustrated in the fgure. 4 Associating base learning with parameter learning, and meta-learning wth hyperparameter learning appears to give us a clear distinction; however, in practice the distinction is not so clear cut: in deep meta-learning the initialization of the regular parameters is considered to be an important \u201chyper\u201dparameter. 254 9 Meta-Learning Fig. 9.4 \ud835\udc41 -way-\ud835\udc58-shot learning [98] Name Approach Environment Ref Recurr. ML Deploy recurrent networks on RL problems [213, 826] Meta Netw. Fast reparam. of base-learner by distinct meta-learner O-glot, miniIm. [542] SNAIL Attention mechanism coupled with temporal conv. O-glot, miniIm. [517] LSTM ML Embed base-learner parameters in cell state of LSTM miniImageNet [629] MAML Learn initialization weights \ud835\udf03 for fast adaptation O-glot, miniIm. [243] iMAML Approx. higher-order gradients, indep. of optim. path O-glot, miniIm. [623] Meta SGD Learn both the initialization and updates O-glot, miniIm. [476] Reptile Move init. towards task-specifc updated weights O-glot, miniIm. [561] BayesMAML Learn multiple initializations \u0398, jointly optim. SVGD miniImagaNet [861] Table 9.2 Meta Reinforcement Learning Approaches [364] 9.2.2.2 Deep Meta-Learning Algorithms We will now turn our attention to deep meta-learning algorithms. The meta-learning feld is still young and active, nevertheless, the feld is converging on a set of defnitions that we will present here. We start our explanation in a supervised setting. Meta-learning is concerned with learning a task T from a set of base-learning tasks {T1, T2, T3, . . . } so that a new (related) meta-test-task will reach a high accuracy quicker. Each base-learning task T\ud835\udc56 consists of a dataset \ud835\udc37\ud835\udc56 and a learning objective, the loss function L\ud835\udc56. Thus we get T\ud835\udc56 = (\ud835\udc37\ud835\udc56, L\ud835\udc56). Each dataset consists of pairs of inputs and labels \ud835\udc37\ud835\udc56 = {(\ud835\udc65 \ud835\udc57, \ud835\udc66 \ud835\udc57)}, and is split into a training and a testset \ud835\udc37\ud835\udc56 = {\ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b, \ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61}. On each training dataset a parameterized model \u02c6\ud835\udc53\ud835\udf03\ud835\udc56 (\ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) is approximated, with a loss function L\ud835\udc56(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b). The model \u02c6\ud835\udc53 is approximated with a deep learning algorithm, that is governed by a set of hyperparameters \ud835\udf14. The particular hyperparameters vary from algorithm to algorithm, 9.2 Transfer Learning and Meta-Learning Agents 255 but frequently encountered hyperparameters are the learning rate \ud835\udefc, the initial parameters \ud835\udf030, and algorithm constants. This conventional machine learning algorithm is called the base learner. Each base learner task approximates a model \u02c6\ud835\udc53\ud835\udc56 by fnding the optimal parameters \ud835\udf03\u2605 \ud835\udc56 to minimize the loss function on its data set T\ud835\udc56 = \u02c6\ud835\udc53\ud835\udf03\u2605 \ud835\udc56 = arg min \ud835\udf03\ud835\udc56 L\ud835\udc56,\ud835\udf14(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) while the learning algorithm is governed by hyperparameters \ud835\udf14. Inner and Outer Loop Optimization One of the most popular deep meta-learning approaches of the last few years is optimization-based meta-learning [364]. This approach optimizes the initial parameters \ud835\udf03 of the network for fast learning of new tasks. Most optimization-based techniques do so by approaching meta-learning as a two-level optimization problem. At the inner level, a base learner makes task-specifc updates to \ud835\udf03 for the diferent observations in the training set. At the outer level, the meta-learner optimizes hyperparameters \ud835\udf14 across a sequence of base tasks where the loss of each task is evaluated using the test data from the base tasks \ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 [629, 351, 463]. The inner loop optimizes the parameters \ud835\udf03, and the outer loop optimizes the hyperparameters \ud835\udf14 to fnd the best performance on the set of base tasks \ud835\udc56 = 0, . . . , \ud835\udc40 with the appropriate test data: \ud835\udf14\u2605 = arg min \ud835\udf14 Lmeta \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd outer loop (arg min \ud835\udf03\ud835\udc56 Lbase \ud835\udf14 (\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd inner loop , \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61). The inner loop optimizes \ud835\udf03\ud835\udc56 within the datasets \ud835\udc37\ud835\udc56 of the tasks T\ud835\udc56 , and the outer loop optimizes \ud835\udf14 across the tasks and datasets. The meta loss function optimizes for the meta objective, which can be accuracy, speed, or another goal over the set of base tasks (and datasets). The outcome of the meta optimization is a set of optimal hyperparameters \ud835\udf14\u2605. In optimization-based meta-learning the most important hyperparameters \ud835\udf14 are the optimal initial parameters \ud835\udf03\u2605 0. When the meta-learner only optimizes the initial parameters as hyperparameters (\ud835\udf14 = \ud835\udf030) then the inner/outer formula simplifes as follows: \ud835\udf03\u2605 0 = arg min \ud835\udf030 L \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd outer loop (arg min \ud835\udf03\ud835\udc56 L(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd inner loop , \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61). In this approach we meta-optimize the initial parameters \ud835\udf030, such that the loss function performs well on the test data of the base tasks. Section 9.2.2.4 describes MAML, a well-known example of this approach. 256 9 Meta-Learning Fig. 9.5 Workfow of recurrent meta-learners in reinforcement learning contexts. State, action, reward, and termination fag at time step \ud835\udc61 are denotednby \ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, and \ud835\udc51\ud835\udc61, \u210e\ud835\udc61 refers to the hidden state [213]. Deep meta-learning approaches are sometimes categorized as (1) similaritymetric-based, (2) model-based, and (3) optimization-based [364]. We will now have a closer look at two of the nine meta reinforcement learning algorithms from Table 9.2. We will look at Recurrent meta-learning and MAML; the former is a model-based approach, the latter optimization-based. 9.2.2.3 Recurrent Meta-Learning For meta reinforcement learning approaches to be able to learn to learn, they must be able to remember what they have learned across subtasks. Let us see how Recurrent meta-learning learns across tasks. Recurrent meta-learning uses recurrent neural networks to remember this knowledge [213, 826]. The recurrent network serves as dynamic storage for the learned task embedding (weight vector). The recurrence can be implemented by an LSTM [826] or by gated recurrent units [213]. The choice of recurrent neural meta-network (meta-RNN) determines how well it adapts to the subtasks, as it gradually accumulates knowledge about the base-task structure. Recurrent meta-learning tracks variables \ud835\udc60, \ud835\udc4e, \ud835\udc5f, \ud835\udc51 which denote state, action, reward, and termination of the episode. For each task T\ud835\udc56, Recurrent meta-learning inputs the set of environment variables {\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc51\ud835\udc61} into a meta-RNN at each time step \ud835\udc61. The meta-RNN outputs an action and a hidden state \u210e\ud835\udc61. Conditioned on the hidden state \u210e\ud835\udc61, the meta network outputs action \ud835\udc4e\ud835\udc61. The goal is to maximize the expected reward in each trial (Fig. 9.5). Since Recurrent meta-learning embeds information from previously seen inputs in hidden state, it is regarded as a modelbased meta-learner [364]. Recurrent meta-learners performed almost as well as model-free baselines on simple \ud835\udc41-way-\ud835\udc58-shot reinforcement learning tasks [826, 213]. However, the performance degrades in more complex problems, when dependencies span a longer horizon. 9.2 Transfer Learning and Meta-Learning Agents 257 Fig. 9.6 The Optimization approach aims to learn parameters from which other tasks can be learned quickly. The intuition behind Optimization approaches such as MAML is that when our meta-training set consists of tasks A, B, C, and D, then if our meta-learning algorithm adjusts parameters \ud835\udc4e and \ud835\udc4f to (2, 2), then they are close to either of the four tasks, and can be adjustly quickly to them, with few examples (after [364, 243]). 9.2.2.4 Model-Agnostic Meta-Learning The Model-agnostic meta-learning approach (MAML) [243] is an optimization approach that is model-agnostic: it can be used for diferent learning problems, such as classifcation, regression, and reinforcement learning. As mentioned, the optimization view of meta-learning is especially focused on optimizing the initial parameters \ud835\udf03. The intuition behind the optimization view can be illustrated with a simple regression example (Fig. 9.6). Let us assume that we are faced with multiple linear regression problems \ud835\udc53\ud835\udc56(\ud835\udc65). The model has two parameters: \ud835\udc4e and \ud835\udc4f, \u02c6\ud835\udc53 (\ud835\udc65) = \ud835\udc4e \u00b7 \ud835\udc65 + \ud835\udc4f. When the meta-training set consists of four tasks, A, B, C, and D, then we wish to optimize to a single set of parameters {\ud835\udc4e, \ud835\udc4f} from which we can quickly learn the optimal parameters for each of the four tasks. In Fig. 9.6 the point in the middle represents this combination of parameters. The point is the closest to the four diferent tasks. This is how Model-agnostic meta-learning works [243]: by exposing our model to various base tasks, we update the parameters \ud835\udf03 = {\ud835\udc4e, \ud835\udc4f} to good initial parameters \ud835\udf030 that facilitate quick meta-adaptation. Let us look at the process of training a deep learning model\u2019s parameters from a feature learning standpoint [243, 364], where the goal is that a few gradient steps can produce good results on a new task. We build a feature representation that is broadly suitable for many tasks, and by then fne-tuning the parameters slightly (primarily updating the top layer weights) we achieve good results\u2014not unlike transfer learning. MAML fnds parameters \ud835\udf03 that are easy and fast to fnetune, allowing the adaptation to happen in an embedding space that is well suited for 258 9 Meta-Learning Algorithm 9.1 MAML for Reinforcement Learning [243] Require: \ud835\udc5d(T): distribution over tasks Require: \ud835\udefc, \ud835\udefd: step size hyperparameters randomly initialize \ud835\udf03 while not done do Sample batch of tasks T\ud835\udc56 \u223c \ud835\udc5d(T) for all T\ud835\udc56 do Sample \ud835\udc58 trajectories D = {(\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ) } using \ud835\udc53\ud835\udf03 in T\ud835\udc56 Evaluate \u2207\ud835\udf03 LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03) using D and LT\ud835\udc56 in Equation 9.1 Compute adapted parameters with gradient descent: \ud835\udf03\u2032 \ud835\udc56 = \ud835\udf03 \u2212 \ud835\udefc\u2207\ud835\udf03 LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03) Sample trajectories D\u2032 \ud835\udc56 = {(\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ) } using \ud835\udc53\ud835\udf03\u2032 \ud835\udc56 in T\ud835\udc56 end for Update \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udefd\u2207\ud835\udf03 \ufffd T\ud835\udc56\u223c\ud835\udc5d(T) LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03\u2032 \ud835\udc56) using each D\u2032 \ud835\udc56 and LT\ud835\udc56 in Eq. 9.1 end while fast learning. To put it another way, MAML\u2019s goal is to fnd the point in the middle of Fig. 9.6, from where the other tasks are easily reachable. How does MAML work [363, 620, 243]? Please refer to the pseudocode for MAML in Alg. 9.1. The learning task is an episodic Markov decision process with horizon \ud835\udc47, where the learner is allowed to query a limited number of sample trajectories for few-shot learning. Each reinforcement learning task T\ud835\udc56 contains an initial state distribution \ud835\udc5d\ud835\udc56(\ud835\udc601) and a transition distribution \ud835\udc5d\ud835\udc56(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). The loss LT\ud835\udc56 corresponds to the (negative) reward function \ud835\udc45. The model being learned, \ud835\udf0b\ud835\udf03, is a policy from states \ud835\udc60\ud835\udc61 to a distribution over actions \ud835\udc4e\ud835\udc61 at each timestep \ud835\udc61 \u2208 {1, ...,\ud835\udc47}. The loss for task T\ud835\udc56 and policy \ud835\udf0b\ud835\udf03 takes the familiar form of the objective (Eq. 2.5): LT\ud835\udc56 (\ud835\udf0b\ud835\udf03) = \u2212E\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u223c\ud835\udf0b\ud835\udf03 ,\ud835\udc5dT\ud835\udc56 \ufffd \ud835\udc47 \u2211\ufe01 \ud835\udc61=1 \ud835\udc45\ud835\udc56(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61+1) \ufffd . (9.1) In \ud835\udc58-shot reinforcement learning, \ud835\udc58 rollouts from \ud835\udf0b\ud835\udf03 and task T\ud835\udc56, (\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ), and the rewards \ud835\udc45(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61), may be used for adaptation on a new task T\ud835\udc56. MAML uses TRPO to estimate the gradient both for the policy gradient update(s) and the meta optimization [681]. The goal is to quickly learn new concepts, which is equivalent to achieving a minimal loss in few gradient update steps. The number of gradient steps has to be specifed in advance. For a single gradient update step gradient descent produces updated parameters \ud835\udf03\u2032 \ud835\udc56 = \ud835\udf03 \u2212 \ud835\udefc\u2207\ud835\udf03LT\ud835\udc56 (\ud835\udf0b\ud835\udf03) specifc to task \ud835\udc56. The meta loss of one gradient step across tasks is \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udefd\u2207\ud835\udf03 \u2211\ufe01 T\ud835\udc56\u223c\ud835\udc5d(T) LT\ud835\udc56 (\ud835\udf0b\ud835\udf03\u2032 \ud835\udc56) (9.2) 9.2 Transfer Learning and Meta-Learning Agents 259 where \ud835\udc5d(T) is a probability distribution over tasks. This expression contains an inner gradient \u2207\ud835\udf3dLT\ud835\udc56 (\ud835\udf0b\ud835\udf03\u2032 \ud835\udc56). Optimizing this meta loss requires computing secondorder gradients when backpropagating the meta gradient through the gradient operator in the meta objective (Eq. 9.2), which is computationally expensive [243]. Various algorithms have been inspired by MAML and aim to improve optimizationbased meta-learning further [561, 363]. In meta reinforcement learning the goal is to quickly fnd a policy for a new environment using only a small amount of experience. MAML has gained attention within the feld of deep meta-learning, due to its simplicity (it requires two hyperparameters), its general applicability, and its strong performance. 9.2.2.5 Hyperparameter Optimization Meta-learning has been around for a long time, long before deep learning became popular. It has been applied to classic machine learning tasks, such as regression, decision trees, support vector machines, clustering algorithms, Bayesian networks, evolutionary algorithms, and local search [93, 106, 812]. The hyperparameter view on meta-learning originated here. Although this is a book about deep learning, it is interesting to briefy discuss this non-deep background, also because hyperparameter optimization is an important technology to fnd a good set of hyperparameters in reinforcement learning experiments. Machine learning algorithms have hyperparameters that govern their behavior, and fnding the optimal setting for these hyperparameters has long been called meta-learning. A naive approach is to enumerate all combinations and run the machine learning problem for them. For all but the smallest hyperparameter spaces such a grid search will be prohibitively slow. Among the smarter meta-optimization approaches are random search, Bayesian optimization, gradient-based optimization, and evolutionary optimization. This meta-algorithm approach has given rise to algorithm confguration research, such as SMAC [366],5 ParamILS [367],6 irace [488],7 and algorithm selection research [630, 407], such as SATzilla [854].8 Well-known hyperparameter optimization packages include scikit-learn [595],9 scikit-optimize,10 nevergrad [627],11 and optuna [11].12 Hyperparameter optimization and algorithm confguration have 5 https://github.com/automl/SMAC3 6 http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/ 7 http://iridia.ulb.ac.be/irace/ 8 http://www.cs.ubc.ca/labs/beta/Projects/SATzilla/ 9 https://scikit-learn.org/stable/ 10 https://scikit-optimize.github.io/stable/index.html 11 https://code.fb.com/ai-research/nevergrad/ 12 https://optuna.org 260 9 Meta-Learning grown into the feld of automated machine learning, or AutoML.13 The AutoML feld is a large and active feld of meta-learning, an overview book is [368]. All machine learning algorithms have a bias, diferent algorithms perform better on diferent types of problems. Hyperparameters constrain this algorithm bias. This so-called inductive bias refects the set of assumptions about the data on which the algorithms are based. Learning algorithms perform better when this bias matches the learning problem (for example: CNNs work well on image problems, RNNs on language problems). Meta-learning changes this inductive bias, either by choosing a diferent learning algorithm, by changing the network initialization, or by other means, allowing the algorithm to be adjusted to work well on diferent problems. 9.2.2.6 Meta-Learning and Curriculum Learning There is in interesting connection between meta-learning and curriculum learning. Both approaches aim to improve the speed and accuracy of learning, by learning from a set of subtasks. In meta-learning, knowledge is gained from subtasks, so that the learning of a new, related, task, can be quick. In curriculum learning (Sect. 6.2.3.1) we aim to learn quicker by dividing a large and difcult learning task into a set of subtasks, ordered from easy to hard. Thus we can conclude that curriculum learning is a form of meta-learning where the subtasks are ordered from easy to hard, or, equivalently, that meta-learning is unordered curriculum learning. 9.2.2.7 From Few-Shot to Zero-Shot Learning Meta-learning uses information from previous learning tasks to learn new tasks quicker [774]. Meta-learning algorithms are often evaluated in a few-shot setting, to see how well they do in image classifcation problems when they are shown only few training examples. This few-shot learning problem aims to correctly classify queries with little previous support for the new class. In the previous sections we have discussed how meta-learning algorithms aim to achieve few-shot learning. A discussion of meta-learning would not be complete without mentioning zero-shot learning. Zero-shot learning (ZSL) goes a step further than few-shot learning. In zero-shot learning an example has to be recognized as belonging to a class without ever having been trained on an example of this class [454, 580]. In zero-shot learning the classes covered by training instances and the classes we aim to classify are disjoint. This may sound like an impossibility\u2014how can you recognize something you have never seen before?\u2014yet it is something that we, humans, do all the time: having learned to pour cofee in a cup, we can also pour tea, even if we never have seen 13 https://www.automl.org 9.3 Meta-Learning Environments 261 tea before. (Or, having learned to play the violin, the viola, and the cello, we can play the double bass, to some degree, even if we have never played the double bass before.) Whenever we recognize something that we have not seen before, we are actually using extra information (or features). If we recognize a red beak in a picture of a bird-species that we have never seen before, then the concepts \u201cred\u201d and \u201cbeak\u201d are known to us, because we have learned them in other contexts. Zero-shot learning recognizes new categories of instances without training examples. Attribute-based zero-shot learning uses separate high-level attribute descriptions of the new categories, based on categories previously learned in the dataset. Attributes are an intermediate representation that enables parameter sharing between classes [10]. The extra information can be in the form of textual description of the class\u2014red, or beak\u2014in addition to the visual information [449]. The learner must be able to match text with image information. Zero-shot learning approaches are designed to learn this intermediate semantic layer, the attributes, and apply them at inference time to predict new classes, when they are provided with descriptions in terms of these attributes. Attributes correspond to high-level properties of the objects which are shared across multiple classes (which can be detected by machines and which can be understood by humans). Attribute-based image classifcation is a label-embedding problem where each class is embedded in the space of attribute vectors. As an example, if the classes correspond to animals, possible attributes include has paws, has stripes or is black. 9.3 Meta-Learning Environments Now that we have seen how transfer learning and meta-learning can be implemented, it is time to look at some of the environments that are used to evaluate the algorithms. We will list important datasets, environments, and foundation models for images, behavior, and text, expanding our scope beyond pure reinforcement learning. We will look at how well the approaches succeed in generalizing quickly to new machine learning tasks. Many benchmarks have been introduced to test transfer and meta-learning algorithms. Benchmarks for conventional machine learning algorithms aim to ofer a variety of challenging learning tasks. Benchmarks for meta-learning, in contrast, aim to ofer related learning tasks. Some benchmarks are parameterized, where the diference between tasks can be controlled. Meta-learning aims to learn new and related tasks quicker, trading of speed versus accuracy. This raises the question how fast and how accurate meta-learning algorithms are under diferent circumstances. In answering these questions we must keep in mind that the closer the learning tasks are, the easier the task is, and the quicker and the more accurate results will be. Hence, we should carefully look at which dataset a benchmark uses when we compare results. 262 9 Meta-Learning Name Type Domain Ref ALE single games [71] MuJoCo single robot [780] DeepMind Control single robot [755] BERT transfer text [193] GPT-3 transfer text [617] ImageNet transfer image [237] Omniglot meta image [445] Mini-ImageNet meta image [814] Meta-Dataset meta image [785] Meta-World meta robot [864] Alchemy meta unity [825] Table 9.3 Datasets, Environments, and Models for Meta-Learning in Images, Behavior, and Text Table 9.3 lists some of the environments that are often used for meta-learning experiments. Some are regular deep learning environments designed for single-task learning (\u201csingle\u201d), some are transfer learning and pretraining datasets (\u201ctransfer\u201d), and some datasets and environments are specifcally designed for meta-learning experiments (\u201cmeta\u201d). We will now describe them in more detail. ALE (Sect. 3.1.1), MuJoCo (Sect. 4.1.3) and the DeepMind control suite (Sect. 4.3.2) are originally single task deep learning environments. They are also being used in meta-learning experiments and few-shot learning, often with moderate results, since the tasks are typically not very similar (Pong is not like Pac-Man). 9.3.1 Image Processing Traditionally, two datasets have emerged as de facto benchmarks for few-shot image learning: Omniglot [446], and mini-ImageNet [646, 814]. Omniglot is a dataset for one-shot learning. This dataset contains 1623 diferent handwritten characters from 50 diferent alphabets and contains 20 examples per class (character) [446, 447]. Most recent methods obtain very high accuracy on Omniglot, rendering comparisons between them mostly uninformative. Mini-ImageNet uses the same setup as Omniglot for testing, consisting of 60,000 colour images of size 84 \u00d7 84 with 100 classes (64/16/20 for train/validation/test) and contains 600 examples per class [814]. Albeit harder than Omniglot, most recent methods achieve similar accuracy when controlling for model capacity. Meta-learning algorithms such as Bayesian Program Learning and MAML achieved accuracies comparable to human performance on Omniglot and ImageNet, with accuracies in the high nineties and error rates as low as a few percent [447]. Models 9.3 Meta-Learning Environments 263 trained on the largest datasets, such as ImageNet, are used as foundation models [96]. Pre-trained models can be downloaded from the Model zoo.14 These benchmarks may be too homogeneous for testing meta-learning. In contrast, real-life learning experiences are heterogeneous: they vary in terms of the number of classes and examples per class, and are unbalanced. Furthermore, the Omniglot and Mini-ImageNet benchmarks measure within-dataset generalization. For meta-learning, we are eventually after models that can generalize to entirely new distributions. For this reason, new datasets are being developed specifcally for meta-learning. 9.3.2 Natural Language Processing In natural language processing BERT is a well-known pretrained model. BERT stands for Bidirectional encoder representations from transformers [193]. It is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on two contexts. BERT has shown that transfer learning can work well in natural language tasks. BERT can be used for classifcation tasks such as sentiment analysis, question answering tasks, and named entity recognition. BERT is a large model, with 345 million parameters [639]. An even larger pretrained transformer model is the Generative pretrained transformer 3, or GPT-3 [617], with 175 billion parameters. The quality of the text generated by GPT-3 is exceptionally good, it is difcult to distinguish from that written by humans. In this case, it appears that size matters. OpenAI provides a public interface where you can see for yourself how well it performs.15 BERT and GPT-3 are large models, that are used more and more as a foundation model as a basis for other experiments to pretrain on. 9.3.3 Meta-Dataset A recent dataset specifcally designed for meta-learning is Meta-dataset [785].16 Meta-dataset is a set of datasets, consisting of: Imagenet, Omniglot, Aircraft, Birds, Textures, Quick Draw, Fungi, Flower, Trafc Signs, and MSCOCO. Thus, the datasets provide a more heterogeneous challenge than earlier single-dataset experiments. Triantafllou et al. report results with Matching networks [814], Prototypical networks [717], frst-order MAML [243], and Relation networks [737]. As can be expected, the accuracy on the larger (meta) dataset is much lower than on previous homogeneous datasets. They fnd that a variant of MAML performs best, although for classifers that are trained on other datasets most accuracies are between 40% 14 https://modelzoo.co 15 https://openai.com/blog/openai-api/ 16 https://github.com/google-research/meta-dataset 264 9 Meta-Learning Fig. 9.7 Multi Task and Meta Reinforcement Learning [864] Fig. 9.8 Meta-World Tasks [864] and 60%, except for Birds and Flowers, which scores in the 70s and 80s, closer to the single dataset results for Omniglot and Imagenet. Meta-learning for heterogeneous datasets remains a challenging task (see also [141, 775]). 9.3.4 Meta-World For deep reinforcement learning two traditionally popular environments are ALE and MuJoCo. The games in the ALE benchmarks typically difer considerably, which makes the ALE test set challenging for meta-learning, and little success has been reported. (There are a few exceptions that apply transfer learning (pretraining) to DQN [586, 520, 718] succeeding in multitask learning in a set of Atari games that all move a ball.) Robotics tasks, on the other hand, are more easily parameterizable. Test tasks can be generated with the desired level of similarity, making robotic tasks amenable to meta-learning testing. Typical tasks such as reaching and learning diferent walking 9.3 Meta-Learning Environments 265 gaits, are more related than two Atari games such as, for example, Breakout and Space Invaders. To provide a better benchmark that is more challenging for meta reinforcement learning, Yu et al. introduced Meta-World [864],17 a benchmark for multi-task and meta reinforcement learning (see Fig. 9.7 for their pictorial explanation of the diference between multi-task and meta-learning). Meta-World consists of 50 distinct manipulation tasks with a robotic arm (Fig. 9.8). The tasks are designed to be diferent, and contain structure, which can be leveraged for transfer to new tasks. When the authors of Meta-World evaluated six state-of-the-art meta-reinforcement and multi-task learning algorithms on these tasks, they found that generalization of existing algorithms to heterogeneous tasks is limited. They tried PPO, TRPO, SAC, RL2[213], MAML, and PEARL [624]. Small variations of tasks such as diferent object positions can be learned with reasonable success, but the algorithms struggled to learn multiple tasks at the same time, even with as few as ten distinct training tasks. In contrast to more limited meta-learning benchmarks, Meta-World emphasizes generalization to new tasks and interaction scenarios, not just a parametric variation in goals. 9.3.5 Alchemy A fnal meta reinforcement learning benchmark that we will discuss is Alchemy [825]. The Alchemy benchmark is a procedurally generated 3D video game [692], implemented in Unity [383]. Task generation is parameterized, and varying degrees of similarity and hidden structure can be chosen. The process by which Alchemy levels are created is accessible to researchers, and a perfect Bayesian ideal observer can be implemented. Experiments with two agents are reported, VMPO and IMPALA. The VMPO [722, 587] agent is based on a gated transformer network. The IMPALA [230, 374] agent is based on population-based training with an LSTM core network. Both agents are strong deep learning methods, although not necessarily for meta-learning. Again, in both agents meta-learning became more difcult as learning tasks became more diverse. The reported performance on meta-learning was weak. The Alchemy platform can be found on GitHub.18 A human-playable interface is part of the environment. 17 https://meta-world.github.io 18 https://github.com/deepmind/dm_alchemy 266 9 Meta-Learning 1 import metaworld 2 import random 3 4 print(metaworld.ML1.ENV_NAMES) # Try available environments 5 6 ml1 = metaworld.ML1(\u2019pick -place -v1\u2019) # Construct the benchmark 7 8 env = ml1. train_classes [\u2019pick -place -v1\u2019]() 9 task = random.choice(ml1. train_tasks ) 10 env.set_task(task) # Set task 11 12 obs = env.reset () # Reset environment 13 a = env. action_space .sample () # Sample an action 14 obs , reward , done , info = env.step(a) # Step the environoment Listing 9.3 Using Meta-World 9.3.6 Hands-on: Meta-World Example Let us get experience with running Meta-World benchmark environments. We will run a few popular agent algorithms on them, such as PPO and TRPO. Meta-World is as easy to use as Gym. The code can be found on GitHub,19 and the accompanying implementations of a suite of agent algorithms is called Garage [261].20 Garage runs on PyTorch and on TensorFlow. Standard Meta-World benchmarks include multi-task and meta-learning setups, named MT1, MT10, MT50, and ML1, ML10, ML50. The Meta-World benchmark can be installed with pip pip install git+https://github.com/rlworkgroup/ metaworld.git@master\\#egg=metaworld Note that Meta-World is a robotics benchmark and needs MuJoCo, so you have to install that too.21 The GitHub site contains brief example instructions on the usage of the benchmark, please refer to Listing 9.3. The benchmark can be used to test the meta-learning performance of your favorite algorithm, or you can use one of the baselines provided in Garage. Conclusion In this chapter we have seen diferent approaches to learning new and diferent tasks with few or even zero examples. Impressive results have been achieved, although 19 https://github.com/rlworkgroup/metaworld 20 https://github.com/rlworkgroup/garage 21 https://github.com/openai/mujoco-py#install-mujoco 9.3 Meta-Learning Environments 267 major challenges remain to learn general adaptation when tasks are more diverse. As is often the case in new felds, many diferent approaches have been tried. The feld of meta-learning is an active feld of research, aiming to reduce one of the main problems of machine learning, and many new methods will continue to be developed. Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary This chapter is concerned with learning new tasks faster and with smaller datasets or lower sample complexity. Transfer learning is concerned with transferring knowledge that has been learned to solve a taks, to another task, to allow quicker learning. A popular transfer learning approach is pretraining, where some network layers are copied to intialize a network for a new task, followed by fne tuning, to improve performance on the new task, but with a smaller dataset. Another approach is meta-learning, or learning to learn. Here knowledge of how a sequence of previous tasks is learned is used to learn a new task quicker. Metalearning learns hyperparameters of the diferent tasks. In deep meta-learning, the set of initial network parameters is usually considered to be such a hyperparameter. Meta-learning aims to learn hyperparameters that can learn a new task with only a few training examples, often using \ud835\udc41-way-\ud835\udc58-shot learning. For deep few-shot learning the Model-Agnostic Meta-Learning (MAML) approach is well-known, and has inspired follow-up work. Meta-learning is of great importance in machine learning. For tasks that are related, good results are reported. For more challenging benchmarks, where tasks are less related (such as pictures of animals from very diferent species), results are reported that are weaker. Further Reading Meta-learning is a highly active feld of research. Good entry points are [882, 351, 363, 364, 100]. Meta-learning has attracted much attention in artifcial intelligence, both in supervised learning and in reinforcement learning. Many books and surveys have been written about the feld of meta-learning, see, for example [106, 684, 804, 667, 836]. 268 9 Meta-Learning There has been active research interest in meta-learning algorithms for some time, see, for example [669, 75, 676, 812]. Research into transfer learning and metalearning has a long history, starting with Pratt and Thrun [606, 774]. Early surveys into the feld are [581, 758, 834], more recent surveys are [872, 879]. Huh et al. focus on ImageNet [360, 622]. Yang et al. study the relation between transfer learning and curriculum learning with Sokoban [859]. Early principles of meta-learning are described by Schmidhuber [669, 676]. Metalearning surveys are [667, 812, 106, 684, 804, 836, 302, 351, 363, 364]. Papers on similarity-metric meta-learning are [418, 814, 717, 737, 263, 699]. Papers on modelbased meta-learning are [213, 826, 659, 542, 517, 223, 264]. Papers on optimizationbased meta-learning are many [629, 473, 243, 23, 623, 476, 561, 649, 245, 285, 246, 861, 83]. Domain adaptation is studied in [17, 200, 168, 872, 541]. Zero-shot learning is an active and promising feld. Interesting papers are [87, 356, 454, 580, 728, 10, 851, 449, 636, 197, 622]. Like zero-shot learning, few-shot learning is also a popular area of meta-learning research [337, 437, 719, 565, 728]. Benchmark papers are [785, 864, 141, 775, 825]. Exercises It is time to test our understanding of transfer learning and meta-learning with some exercises and questions. Questions Below are some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. 1. What is the reason for the interest in meta-learning and transfer learning? 2. What is transfer learning? 3. What is meta-learning? 4. How is meta-learning diferent from multi task learning? 5. Zero-shot learning aims to identify classes that it has not seen before. How is that possible? 6. Is pretraining a form of transfer learning? 7. Can you explain learning to learn? 8. Are the initial network parameters also hyperparameters? Explain. 9. What is an approach for zero-shot learning? 10. As the diversity of tasks increases, does meta-learning achieve good results? 9.3 Meta-Learning Environments 269 Exercises Let us go to the programming exercises to become more familiar with the methods that we have covered in this chapter. Meta-learning and transfer learning experiments are often very computationally expensive. You may need to scale down dataset sizes, or skip some exercises as a last resort. 1. Pretraining Implement pretraining and fne tuning in the Keras pretraining example from Sect. 9.2.1.3.22 Do the exercises as suggested, including fnetuning on the cats and dogs training set. Note the uses of preprocessing, data augmentation and regularization (dropout and batch normalization). See the efects of increasing the number of layers that you transfer on training performance and speed. 2. MAML Reptile [561] is a meta-learning approach inpspired by MAML, but frst order, and faster, specifcally designed for few-shot learning. The Keras website contains a segment on Reptile.23 At the start a number of hyperparameters are defned: learning rate, step size, batch size, number of meta-learning iterations, number of evaluation iterations, how many shots, classes, etcetera. Study the efect of tuning diferent hyperparameters, especially the ones related to few-shot learning: the classes, the shots, and the number of iterations. To delve deeper into few-shot learning, also have a look at the MAML code, which has a section on reinforcement learning.24 Try diferent environments. 3. Meta World As we have seen in Sect. 9.3.4, Meta World [864] is an elaborate benchmark suite for meta reinforcement learning. Re-read the section, go to GitHub, and install the benchmark.25 Also go to Garage to install the agent algorithms so that you are able to test their performance.26 See that they work with your PyTorch or TensorFlow setup. First try running the ML1 meta benchmark for PPO. Then try MAML, and RL2. Next, try the more elaborate meta-learning benchmarks. Read the Meta World paper, and see if you can reproduce their results. 4. ZSL We go from few-shot learning to zero-shot learning. One of the ways in which zero-shot learning works is by learning attributes that are shared by classes. Read the papers Label-Embedding for Image Classifcation [10], and An embarrassingly simple approach to zero-shot learning [636], and go to the code [87].27 Implement it, and try to understand how attribute learning works. Print the attributes for the classes, and use the diferent datasets. Does MAML work for few-shot learning? (challenging) 22 https://keras.io/guides/transfer_learning/ 23 https://keras.io/examples/vision/reptile/ 24 https://github.com/cbfinn/maml_rl/tree/master/rllab 25 https://github.com/rlworkgroup/metaworld 26 https://github.com/rlworkgroup/garage 27 https://github.com/sbharadwajj/embarrassingly-simple-zero-shot-learning Chapter 10 Further Developments We have come to the end of this book. We will refect on what we have learned. In this chapter we will review the main themes and essential lessons, and we will look to the future. Why do we study deep reinforcement learning? Our inspiration is the dream of artifcial intelligence; to understand human intelligence and to create intelligent behavior that can supplement our own, so that together we can grow. For reinforcement learning our goal is to learn from the world, to learn increasingly complex behaviors for increasingly complex sequential decision problems. The preceding chapters have shown us that many successful algorithms were inspired by how humans learn. Currently many environments consist of games and simulated robots, in the future this may include human-computer interactions and collaborations in teams with real humans. 10.1 Development of Deep Reinforcement Learning Reinforcement learning has made a remarkable transition, from a method that was used to learn small tabular toy problems, to learning simulated robots how to walk, to playing the largest multi-agent real time strategy games, and beating the best humans in Go and poker. The reinforcement learning paradigm is a framework in which many learning algorithms have been developed. The framework is able to incorporate powerful ideas from other felds, such as deep learning, and autoencoders. To appreciate the versatility of reinforcement learning, and now that we have studied the feld in great detail, let us have a closer look at how the developments in the feld have proceeded over time. 271 272 10 Further Developments 10.1.1 Tabular Methods Reinforcement learning starts with a simple agent/environment loop, where an environment performs the agent\u2019s actions, and returns a reward (the model-free approach). We use a Markov decision process to formalize reinforcement learning. The value and policy functions are initially implemented in a tabular fashion, limiting the method to small environments, since the agent has to ft the function representations in memory. Typical environments are Grid world, Cartpole and Mountain car; a typical algorithm to fnd the optimal policy function is tabular Q-learning. Basic principles in the design of these algorithms are exploration, exploitation and on-policy/of-policy learning. Furthermore, imagination, as a form of model-based reinforcement learning, was developed. This part of the feld forms a well-established and stable basis, that is, however, only suitable for learning small single-agent problems. The advent of deep learning caused the feld to shift into a higher gear. 10.1.2 Model-free Deep Learning Inspired by breakthroughs in supervised image recognition, deep learning was also applied to Q-learning, causing the Atari breakthrough for deep reinforcement learning. The basis of the success of deep learning in reinforcement learning are methods to break correlations and improve convergence (replay bufer and a separate target network). The DQN algorithm [523] has become quite well known. Policy-based and actor critic approaches work well with deep learning, and are also applicable to continuous action spaces. Many model-free actor critic variants have been developed [683, 480, 306, 521], they are often tested on simulated robot applications. Algorithms often reach good quality optima, but model-free algorithms have a high sample complexity. Tables 3.1 and 4.1 list these algorithms. This part of the feld\u2014deep model-free value and policy-based algorithms\u2014can now be considered as well-established, with mature algorithms whose behavior is well understood, and with good results for high-dimensional single-agent environments. Typical high-dimensional environments are the Arcade Learning Environment, and MuJoCo simulated physics locomotion tasks. 10.1.3 Multi-Agent Methods Next, more advanced methods are covered. In Chap. 5 model-based algorithms combine planning and learning to improve sample efciency. For high-dimensional visual environments they use uncertainty modeling and latent models or world models, to reduce the dimensionality for planning. The algorithms are listed in Table 5.2. 10.1 Development of Deep Reinforcement Learning 273 Furthermore, the step from single-agent to multi-agent is made, enlarging the type of problems that can be modeled, getting closer to real-world problems. The strongest human Go players are beaten with a model-based self-play combination of MCTS and a deep actor critic algorithm. The self-play setup performs curriculum learning, learning from previous learning tasks ordered from easy to hard, and is in that sense a form of meta-learning. Variants are shown in Table 6.2. For multi-agent and imperfect information problems, deep reinforcement learning is used to study competition, emergent collaboration, and hierarchical team learning. In these areas reinforcement learning comes close to multi-agent systems and population based methods, such as swarm computing. Parallel population-based methods may be able to learn the policy quicker than gradient-based methods, and they may be a good ft for multi-agent problems. Also, imperfect information multi-agent problems are studied, such as poker; for competitive games counterfactual regret minimization was developed. Research into cooperation is continuing. Early strong results are reported in StarCraft using team collaboration and team competition. In addition, research is being performed into emergent social behavior, connecting the felds of reinforcement learning to swarm computing and multi-agent systems. Algorithms and experiments are listed in Table 7.2. A list of hierarchical approaches is shown in Table 8.1. In human learning, new concepts are learned based on old concepts. Transfer learning from foundation models, and meta-learning, aim to re-use existing knowledge, or even learn to learn. Meta-learning, curriculum learning, and hierarchical learning are emerging as techniques to conquer ever larger state spaces. Table 9.2 shows meta-learning approaches. All these areas should be considered as advanced reinforcement learning, where active research is still very much occurring. New algorithms are being developed, and experiments typically require large amounts of compute power. Furthermore, results are less robust, and require much hyperparameter tuning. More advances are needed, and expected. 10.1.4 Evolution of Reinforcement Learning In contrast to supervised learning, which learns from a fxed dataset, reinforcement learning is a mechanism for learning by doing, just as children learn. The agent/environment framework has turned out to be a versatile approach, that can be augmented and enhanced when we try new problem domains, such as highdimensions, multi-agent or imperfect information. Reinforcement learning has encompassed methods from supervised learning (deep learning) and unsupervised learning (autoencoders), as well as from population-based optimization. In this sense reinforcement learning has evolved from being a single-agent Markov decision process to a framework for learning, based on agent and environment. Other approaches can be hooked into this framework, to learn new felds, and to improve performance. These additions can interpret high dimensional states 274 10 Further Developments (as in DQN), or shrink a state space (as with latent models). When accomodating self-play, the framework provided us with a curriculum learning sequence, yielding world class levels of play in two-agent games. 10.2 Main Challenges Deep reinforcement learning is being used to understand more real world sequential decision making situations. Among the applications that motivate these developments are self driving cars and other autonomous operations, image and speech recognition, decision making, and, in general, acting naturally. What will the future bring for deep reinforcement learning? The main challenge for deep reinforcement learning is to manage the combinatorial explosion that occurs when a sequence of decisions is chained together. Finding the right kind of inductive bias can exploit structure in this state space. We list three major challenges for current and future research in deep reinforcement learning: 1. Solving larger problems faster 2. Solving problems with more agents 3. Interacting with people The following techniques address these challenges: 1. Solving larger problems faster \u2022 Reducing sample complexity with latent-models \u2022 Curriculum learning in self-play methods \u2022 Hierarchical reinforcement learning \u2022 Learn from previous tasks with transfer learning and meta-learning \u2022 Better exploration through intrinsic motivation 2. Solving problems with more agents \u2022 Hierarchical reinforcement learning \u2022 Population-based self-play league methods 3. Interacting with people \u2022 Explainable AI \u2022 Generalization Let us have a closer look at these techniques, to see what future developments can be expected for them. 10.2 Main Challenges 275 10.2.1 Latent Models Chapter 5 discussed model-based deep reinforcement learning methods. In modelbased methods a transition model is learned that is then used with planning to augment the policy function, reducing sample complexity. A problem for modelbased methods in high-dimensional problems is that high-capacity networks need many observations in order to prevent overftting, negating the potential reduction in sample complexity. One of the most promising model-based methods is the use of autoencoders to create latent models, that compress or abstract from irrelevant observations, yielding a lower-dimensional latent state model that can be used for planning in a reduced state space. The reduction in sample complexity of model-based approaches can thus be maintained. Latent models create compact world representations, that are also used in hierarchical and multi-agent problems, and further work is ongoing. A second development in model-based deep reinforcement learning is the use of end-to-end planning and learning of the transition model. Especially for elaborate self-play designs such as AlphaZero, where an MCTS planner is integrated in selflearning, the use of end-to-end learning is advantageous, as the work on MuZero has shown. Research is ongoing in this feld where planning and learning are combined [679, 215, 186, 680, 359, 334, 525]. 10.2.2 Self-Play Chapter 6 discussed learning by self-play in two-agent games. In many two-agent games the transition function is given. When the environment of an agent is played by the opponent with the exact same transition function, a self-learning self-play system can be constructed in which both agent and environment improve eachother. We have discussed examples of cycles of continuous improvement, from tabula-rasa to world-champion level. After earlier results in backgammon [763], AlphaZero achieved landmark results in Go, chess, and shogi [706, 704]. The AlphaZero design includes an MCTS planner in a self-play loop that improves a dual-headed deep residual network [704]. The design has spawned much further research, as well as inspired general interest in artifcial intelligence and reinforcement learning [213, 464, 687, 552, 455, 239, 679, 385]. 10.2.3 Hierarchical Reinforcement Learning Team play is important in multi-agent problems, and hierarchical approaches can structure the environment in a hierarchy of agents. Hierarchical reinforcement learning methods are also applied to single agent problems, using principles of ",
    "Methods": "",
    "Approach": "",
    "Approaches": "Approaches also uses aspects of hierarchical reinforcement learning, a topic that will be covered in the next chapter. In the next section we will look deeper into how self-play league learning is implemented in specifc multi-player games. 7.3 Multi-Agent Environments Reinforcement learning has achieved quite a few imaginative results in which it has succeeded in emulating behavior that approaches human behavior in the real world. In this chapter we have made a step towards modeling more behavior that is even closer to the real world. Let us summarize in this section the results in four diferent multi-agent games, some of which have been published in prestigious scientifc journals. We will use the familiar sequence of (1) competitive, (2) cooperative, and (3) mixed environments. For each we will sketch the problem, outline the algorithmic approach, and summarize the achievements. Table 7.2 lists the multi-agent games and their dominant approach. 7.3.1 Competitive Behavior: Poker Poker is a popular imperfect-information game. It is played competitively and human championships are organized regularly. Poker has been studied for some time in artifcial intelligence, and computer poker championships have been conducted regularly [53, 89]. Poker is a competitive game. Cooperation (collusion, collaboration between two players to the detriment of a third player) is possible, but often does not occur in practice; fnding the Nash equilibrium therefore is a successful approach to playing the game in practice. The method of counterfactual regret minimization has been developed specifcally to make progress in poker. There are many variants of poker that are regularly played. No-limit Texas hold\u2019em is a popular variant; the two-player version is called Heads Up and is easier to analyse because no opponent collusion can occur. Heads-up no-limit Texas hold\u2019em (HUNL) has been the primary AI benchmark for imperfect-information game play for several years. 7.3 Multi-Agent Environments 215 Fig. 7.4 Pluribus on the Cover of Science Poker has hidden information (the face-down cards). Because of this, agents are faced with a large number of possible states; poker is a game that is far more complex than chess or checkers. The state space of the two-person HUNL version is reported to be around 10161 [112]. A further complication is that during the course of the game information is revealed by players through their bets; high bets indicate good cards, or the wish of the player to make the opponent believe that this is the case (blufng). Therefore, a player must choose between betting high on good cards, and on doing the opposite, so that the opponent does not fnd out too much, and can counter-act. In 2018, one of the top two-player poker programs, Libratus, defeated top human professionals in HUNL in a 20-day, 120,000-hand competition featuring a $200,000 prize pool. Brown et al. [112] describe the architecture of Libratus. The program consists of three main modules, one for computing a quick CFR Nash-policy using a smaller version of the game, a second module for constructing a fner-grained strategy once a later stage of the game is reached, and a third module to enhance the frst policy by flling in missing branches. In the experiment against top human players, Libratus analyzed the bet sizes that were played most often by its opponents at the end of each day of the competition. The programs would then calculate a response overnight, in order to improve as the competition proceeded. Two-agent Libratus was originally based on heuristics, abstraction, and tabular counterfactual regret minimization\u2014not on deep reinforcement learning. For multiagent poker the program Pluribus was developed. For Pluribus, the authors used deep counterfactual regret minimization [111], with a 7-layer neural network that followed the AlphaZero self-play approach. Pluribus defeated top players in sixplayer poker [113] (Fig. 7.4). Pluribus uses an AlphaZero approach of self-play in 216 7 Multi-Agent Reinforcement Learning combination with search. Another top program, DeepStack, also uses randomly generated games to train a deep value function network [533]. 7.3.2 Cooperative Behavior: Hide and Seek In addition to competition, cooperative behavior is part of real-world behavior. Indeed, cooperation is what defnes our social fabric, and much of our society consists of diferent ways in which we organize ourselves in families, groups, companies, parties, flter bubbles, and nations. The question of how voluntary cooperation can emerge between individuals has been studied extensively, and the work in tit-for-tat (Sect. 7.1.3) is just one of many studies in this fascinating feld [34, 105, 848, 465, 651]. One study into emergent cooperation has been performed with a version of the game of Hide and Seek. Baker et al.[49] report on an experiment in which they used MuJoCo to build a new multi-agent game environment. The environment was created with the specifc purpose of studying how cooperation emerged out of the combination of a few simple rules and reward maximation (see Fig. 7.5). In the Hide and Seek experiment, the game is played on a randomly generated grid-world where props are available, such as boxes. The reward function stimulates hiders to avoid the line of sight of seekers, and vice versa. There are objects scattered throughout the environment that the agents can grab and lock in place. The environment contains one to three hiders and one to three seekers, there are three to nine movable boxes, some elongated. There are also two movable ramps. Walls and rooms are static and are randomly generated. Agents can see, move, and grab objects. A good way to understand the challenge is to view the videos7 on the Hide and Seek blog.8 With only a visibility-based reward function, the agents are able to learn many diferent skills, including collaborative tool use. For example, hiders learn to create shelters by barricading doors or constructing multi-object forts, so that the seekers can never see them anymore, until, as a counter strategy, seekers learned to use ramps to jump into the shelter. In efect, out of the agents\u2019 interaction a training curriculum emerges in which the agents learn tasks, many of which require sophisticated tool use and coordination. Hide and Seek features cooperation (inside the team) and competition (between the hiders and the seekers). It uses a self-play version of PPO for policy learning. It is interesting to see how easy cooperation emerges. The game does not have explicit communication for the team to coordinate cooperation, all cooperative behavior emerges out of basic interaction between agents that are guided by their reward functions. Cooperative strategies thus emerge out of the game design: the 7 https://www.youtube.com/watch?v=kopoLzvh5jY&t=10s 8 https://openai.com/blog/emergent-tool-use/ 7.3 Multi-Agent Environments 217 Fig. 7.5 Six Strategies in Hide and Seek: Running and Chasing; Fort Building; Ramp Use; Ramp Defense; Box Surfng; Surf Defense (left-to-right, top-to-bottom) [49] homogeneous reward functions for each team and the environment in which blocks are present and follow laws of physics. During play the agents essentially construct an autocurriculum for themselves [464, 49]. Six diferent behavior strategies are reported, each more advanced than the other, increasing the competitive pressure to fnd counter strategies for the opponent (see Fig. 7.5). Baker et al. [49] report that initially, the hiders and seekers learn the basic strategy of running away and chasing. However, after much training (25 million episodes), hiders start to use boxes to construct shelters behind which they hide. Then, after another 75 million episodes, the seekers learn to move and use ramps to jump over obstacles into the shelter. A mere 10 million episodes later, the hiders learn to defend by moving the ramps to the edge and lock them in place out of range of the shelters. Then, after a long 270 million episodes of training, the seekers learned box-surfng. They moved a box to the edge of the play area next to the locked ramps. One seeker then used the ramp to climb on top of the box and the other seekers push it to the shelter, where the seeker could peek over te edge and see the hiders. Finally, in response, the hiders locked all of the boxes in place before building their shelter, and they were safe from the seekers. The Hide and Seek experiment is interesting because of the emergence of diverse behavior strategies. The strategies emerged out of a basic reward function and random exploration (see also the reward is enough argument [707]). 218 7 Multi-Agent Reinforcement Learning Fig. 7.6 Capture the Flag [373] The emergence of strategies out of basic reward and exploration suggests an evolutionary process. However, Hide and Seek does not employ population-based training or evolutionary algorithms, in contrast to the work in Capture the Flag, in the next section. 7.3.3 Mixed Behavior: Capture the Flag and StarCraft The world around us exhibits a mix of competitive and cooperative behavior. Team collaboration is an important aspect of human life, and it has been studied extensively in biology, sociology and artifcial intelligence. It emerges (evolves) out of the most basic settings\u2014the need to achieve an ambitious goal\u2014as we just saw. In recent years many research groups have studied the mixed multi-agent model in real time strategy games, such as Capture the Flag, and StarCraft. We will discuss both games. Capture the Flag First we will discuss the game Capture the Flag, which is played in a Quake III Arena (see Fig. 7.6). Jaderberg et al. have reported on an extensive experiment with this game [373], in which the agents learn from scratch to see, act, cooperate, and compete. In this experiment the agents are trained with population-based 7.3 Multi-Agent Environments 219 self-play [374], Alg. 7.2. The agents in the population are all diferent (they have diferent genes). The population learns by playing against each other, providing increased diversity of teammates and opponents, and a more stable and faster learning process than traditional single-agent deep reinforcement learning methods. In total 30 diferent bots were created and pitted against each other. Agents are part of a team, and the reward functions form a hierarchy. A two-layer optimization process optimizes the internal rewards for winning, and uses reinforcement learning on the internal rewards to learn the policies. In Capture the Flag, the bots start by acting randomly. After 450,000 games, a bot strategy was found that performed well, and they developed cooperative strategies, such as following team mates in order to outnumber opponents, and loitering near the enemy base when their team mate has the fag. Again, as in Hide and Seek, cooperative strategies emerged out of the basic rules, by combining environment feedback and survival of the fttest. The work on Capture the Flag is notable since it demonstrated that with only pixels as input an agent can learn to play competitively in a rich multi-agent environment. To do so it used a combination of population based training, internal reward optimization, and temporally hierarchical reinforcement learning (see next chapter). StarCraft The fnal game that we will discuss in this chapter is StarCraft. StarCraft is a multi-player real-time strategy game of even larger complexity. The state space has been estimated to be on the order of 101685 [573], a very large number. StarCraft features multi-agent decision making under uncertainty, spatial and temporal reasoning, competition, team-collaboration, opponent modeling, and real-time planning. Fig. 1.6 shows a picture of a StarCraft II scene. Research on StarCraft has been ongoing for some time [573], a special StarCraft multi-agent challenge has been introduced [655]. A team from DeepMind has created a program called AlphaStar. In a series of test matches held in December 2018, DeepMind\u2019s AlphaStar beat two top players in two-player single-map matches, using a diferent user interface. AlphaStar plays the full game of StarCraft II. The neural network was initially trained by supervised learning from anonymized human games, that were then further trained by playing against other AlphaStar agents, using a population-based version of self-play reinforcement learning [813, 374].9 These agents are used to seed a multi-agent reinforcement learning process. A continuous competitive league was created, with the agents of the league playing games in competition against each other. By branching from existing competitors, new competitors were added. Agents learn from games against other competitors. Population-based learning was 9 Similar to the frst approach in AlphaGo, where self-play reinforcement learning was also bootstrapped by supervised learning from human games. 220 7 Multi-Agent Reinforcement Learning taken further, creating a process that explores the very large space of StarCraft game play by pitting agents against strong opponent strategies, and retaining strong early strategies. Diversity in the league is increased by giving each agent its own learning objective, such as which competitors it should focus on and which game unit it should build. A form of prioritized league-self-play actor-critic training is used, called prioritized fctitious self-play\u2014details are in [813]. AlphaStar was trained on a custom-built scalable distributed training system using Google\u2019s tensor processing units (TPU). The AlphaStar league was run for 14 days. In this training, each agent experienced the equivalent of 200 years of real-time StarCraft play. In StarCraft players can choose to play one of three alien races: Terran, Zerg or Protoss. AlphaStar was trained to play Protoss only, to reduce training time, although the same training pipeline could be applied to any race. AlphaStar was frst tested against a human grandmaster named TLO, a top professional Zerg player and a grandmaster level Protoss player. The human player remarked: I was surprised by how strong the agent was. AlphaStar takes well-known strategies and turns them on their head. The agent demonstrated strategies I had not thought of before, which means there may still be new ways of playing the game that we haven\u2019t fully explored yet. 7.3.4 Hands On: Hide and Seek in the Gym Example Many of the research eforts reported in these fnal chapters describe signifcant eforts by research teams working on complicated and large games. These games represent the frontier of artifcial intelligence, and research teams use all available computational and software engineering power that they can acquire to achieve the best results. Also, typically a large amount of time is spent in training, and in fnding the right hyperparameters for the learning to work. Replicating results of this scale is highly challenging, and most research eforts are focused on replicating the results on a smaller, more manageable scale, with more manageable computational resources. In this section we will try to replicate some aspects with modest computational requirements. We will focus on Hide and Seek (Sect. 7.3.2). The original code for the Hide and Seek experiments is on GitHub.10 Please visit and install the code. Hide and Seek uses MuJoCo and the mujoco-worldgen package. Install them and the dependencies with pip install -r mujoco-worldgen/requirements.txt pip install -e mujoco-worldgen/ pip install -e multi-agent-emergence-environments/ 10 https://github.com/openai/multi-agent-emergence-environments 7.3 Multi-Agent Environments 221 Examples of environments can be found in the mae_envs/envs folder. You can also build your own environments, starting from the Base environment, in mae_envs/envs/base.py, and then adding boxes, ramps, as well as the appropriate wrappers. Look in the other environments for how to do this. Try out environments by using the bin/examine script; example usage: bin/examine.py base See further the instructions in the GitHub repository. Multiplayer Environments To conclude this hands-on section, we mention a multiplayer implementation of the Arcade Learning Environment. It is presented by Terry et al. [760], who also present baseline performance results for a multiplayer version of DQN, Ape-X DQN, which performed well elsewhere [350, 52]. The environment is also presented as part of PettingZoo, a multi-agent version of Gym [761, 791, 414]. Another multi-agent research environment is the Google Football Research Environment [438]. A physics-based simulator is provided, as well as three baseline implementations (DQN, IMPALA, and PPO). Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary Multi-agent reinforcement learning learns optimal policies for environments that consist of multiple agents. The optimal policy of the agents is infuenced by the policies of the other agents, whose policy is also being optimized. This gives rise to the problem of nonstationarity, and agent behavior violates the Markov property. Multi-agent reinforcement learning adds the element of cooperative behavior to the repertoire of reinforcement learning, which now consists of competition, cooperation, and mixed behavior. The feld is closely related to game theory\u2014the basis of the study of rational behavior in economics. A famous problem of game theory is the prisoner\u2019s dilemma. A famous result of non-cooperative game theory is the Nash equilibrium, which is defned as the joint strategy where no player has anything to gain by changing their own strategy. A famous result from cooperative game-theory is the Pareto optimum, the situation where no individual can be better of without making someone else worse of. 222 7 Multi-Agent Reinforcement Learning When agents have private information, a multi-agent problem is partially observable. Multi-agent problems can be modeled by stochastic games or as extensive form games. The behavior of agents is ultimately determined by the reward functions, that can be homogeneous, or heterogeneous. When agents have diferent reward functions, multi-agent becomes multi-objective reinforcement learning. The regret of an action is the amount of reward that is missed by the agent for not choosing the actions with the highest payof. A regret minimization algorithm is the stochastic and multi-agent equivalent of minimax. Counterfactual-regret minimization is an approach for fnding Nash strategies in competitive multi-agent games, such as poker. Variants of single-agent algorithms are often used for cooperative multi-agent situations. The large state space due to nonstationarity and partial observability precludes solving large problems. Other promising approaches are opponent modeling and explicit communication modeling. Population-based methods such as evolutionary algorithms and swarm intelligence are used frequently in multi-agent systems. These approaches are suitable for homogeneous reward functions and competitive, cooperative, and mixed problems. Evolutionary methods evolve a population of agents, combining behaviors, and selecting the best according to some ftness function. Evolutionary methods are a natural ft for parallel computers and are among the most popular and successful optimization algorithms. Swarm intelligence often introduces (rudimentary) forms of communication between agents, such as in Ant colony optimization where agents communicate through artifcial pheromones to indicate which part of the solution space they have traveled. For some of the most complicated problems that have recently been tackled, such as StarCraft, Capture the Flag, and Hide and Seek, hierarchical and evolutionary principles are often combined in league training, where leagues of teams of agents are trained in a self-play fashion, and where the fttest agents survive. Current achievements require large amounts of computational power, future work is trying to reduce these requirements. Further Reading Multi-agent learning is a widely studied feld. Surveys\u2014both on early multi-agent reinforcement learning and on deep multi-agent reinforcement learning\u2014can be found in [292, 122, 858, 13, 14, 750, 790, 333, 332, 846]. After Littman [483], Shoham et al. [698] look deeper into MDP modeling. The classic work on game theory is Von Neumann and Morgenstern [818]. Modern introductions are [545, 180, 259]. Game theory underlies much of the theory of rational behavior in classical economics. Seminal works of John Nash are [555, 556, 554]. In 1950 he introduced the Nash equilibrium in his dissertation of 28 pages, which won him the Nobel prize in Economics in 1994. A biography and flm have been made about the life of John Nash [553]. 7.3 Multi-Agent Environments 223 The game of rock-paper-scissors plays an important role in game theory, and the study of computer poker [822, 89, 641, 113]. Prospect theory [390], introduced in 1979, studies human behavior in the face of uncertainty, a topic that evolved into the feld of behavioral economics [843, 539, 128]. Gigerenzer introduced fast and frugal heuristics to explain human decision making [275]. For more intriguing works on the feld of evolution of cooperation and the emergence of social norms, see, for example [105, 37, 34, 33, 35, 331, 336]. More recently multi-objective reinforcement learning has been studied, a survey is [484]. In this feld the more realistic assumption is adopted that agents have diferent rewards functions, leading to diferent Pareto optima [802, 537, 841]. Oliehoek et al. have written a concise introduction to decentralized multi-agent modelling [569, 568]. Counterfactual regret minimization has been fundamental for the success in computer poker [112, 113, 379, 881]. An often-used Monte Carlo version is published in [451]. A combination with function approximation is studied in [111]. Evolutionary algorithms have delivered highly successful optimization algorithms. Some entries to this vast feld are [224, 40, 42, 41, 43]. A related feld is swarm intelligence, where communication between homogeneous agents is taking place [406, 217, 207, 78]. For further research in multi-agent systems refer to [847, 793]. For collective intelligence, see, for example [406, 208, 258, 848, 599]. Many other works report on evolutionary algorithms in a reinforcement learning setting [731, 409, 536, 838, 148, 163, 653, 842]. Most of these approaches concern single agent approaches, although some are specifcally applied to multi agent approaches [498, 408, 713, 487, 374, 414]. Research into benchmarks is active. Among interesting approaches are Procedural content generation [781], MuJoCo Soccer [487], and the Obstacle Tower Challenge [384]. There is an extensive literature on computer poker. See, for example [89, 90, 277, 641, 53, 103, 90, 277, 104, 657, 533]. StarCraft research can be found in [815, 655, 813, 655, 573]. Other games studies are [759, 783]. Approaches inspired by results in poker and Go are now also being applied with success in no-press Diplomacy [584, 21]. Exercises Below are a few quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. Questions 1. Why is there so much interest in multi-agent reinforcement learning? 2. What is one of the main challenges of multi-agent reinforcement learning? 3. What is a Nash strategy? 224 7 Multi-Agent Reinforcement Learning 4. What is a Pareto Optimum? 5. In a competitive multi-agent system, what algorithm can be used to calculate a Nash strategy? 6. What makes it difcult to calculate the solution for a game of imperfect information? 7. Describe the Prisoner\u2019s dilemma. 8. Describe the iterated Prisoner\u2019s dilemma. 9. Name two multi-agent card games of imperfect information. 10. What is the setting with a heterogeneous reward function usually called? 11. Name three kinds of strategies that can occur a multi-agent reinforcement learning. 12. Name two solution methods that are appropriate for solving mixed strategy games. 13. What AI method is named after ant colonies, bee swarms, bird focks, or fsh schools? How does it work in general terms? 14. Describe the main steps of an evolutionary algorithm. 15. Describe the general form of Hide and Seek and three strategies that emerged from the interactions of the hiders or seekers. Exercises Here are some programming exercises to become more familiar with the methods that we have covered in this chapter. 1. CFR Implement counterfactual regret minimization for a Kuhn poker player. Play against the program, and see if you can win. Do you see possibilities to extend it to a more challenging version of poker? 2. Hide and Seek Implement Hide and Seek with cooperation and competition. Add more types of objects. See if other cooperation behavior emerges. 3. Ant Colony Use the DeepMind Control Suite and setup a collaborative level and a competitive level, and implement Ant Colony Optimization. Find problem instances on the web, or in the original paper [208]. Can you implement more swarm algorithms? 4. Football Go to the Google Football blog11 and implement algorithms for football agents. Consider using a population-based approach. 5. StarCraft Go to the StarCraft Python interface,12 and implement a StarCraft player (highly challenging) [655]. 11 https://ai.googleblog.com/2019/06/introducing-google-research-football.html 12 https://github.com/deepmind/pysc2 Chapter 8 Hierarchical Reinforcement Learning The goal of artifcial intelligence is to understand and create intelligent behavior; the goal of deep reinforcement learning is to fnd a behavior policy for ever larger sequential decision problems. But how does real intelligence fnd these policies? One of the things that humans are good at, is dividing a complex task into simpler subproblems, and then solving those tasks, one by one, and combining them as the solution to the larger problem. These subtasks are of diferent scales, or granularity, than the original problem. For example, when planning a trip from your house to a hotel room in a far-away city, you typically only plan the start and the end in terms of footsteps taken in a certain direction. The in-between part may contain diferent modes of transportation that get you to your destination quicker, with macro steps, such as taking a trainride or a fight. During this macro, you do not try out footsteps in diferent directions. Our trip\u2014our policy\u2014is a combination of fne-grain primitive actions and coarse-grain macro actions. Hierarchical reinforcement learning studies this real-world-inspired approach to problem solving. It provides formalisms and algorithms to divide problems into larger subproblems, and then plans with these subpolicies, as if they were subroutines. In principle the hierarchical approach can exploit structure in all sequential decision problems, although some problems are easier than others. Some environments can be subdivided into smaller problems in a natural way, such as navigational tasks on maps, or path-fnding tasks in mazes. Multi-agent problems also naturally divide into hierarchical teams, and have large state spaces where hierarchical methods may help. For other problems, however, it can be hard to fnd efcient macros, or it can be computationally intensive to fnd good combinations of macro steps and primitive steps. Another aspect of hierarchical methods is that since macro-actions take large steps, they may miss the global minimum. The best policies found by hierarchical methods may be less optimal than those found by \u201cfat\u201d approaches (although they may get there much quicker). 225 226 8 Hierarchical Reinforcement Learning In this chapter we will start with an example to capture the favor of hierarchical problem solving. Next, we will look at a theoretical framework that is used to model hierarchical algorithms, and at a few examples of algorithms. Finally, we will look deeper at hierarchical environments. The chapter ends with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Solve large, structured, problems by divide and conquer \u2022 Temporal abstraction of actions with options Core Problem \u2022 Find subgoals and subpolicies efciently, to perform hierarchical abstraction Core Algorithms \u2022 Options framework (Sect. 8.2.1) \u2022 Option critic (Sect. 8.2.3.2) \u2022 Hierarchical actor critic (Sect. 8.2.3.2) Planning a Trip Let us see how we plan a major trip to visit a friend that lives in another city, with a hierarchical method. The method would break up the trip in diferent parts. The frst part would be to walk to your closet and get your things, and then to get out and get your bike. You would go to the train station, and park your bike. You would then take the train to the other city, possibly changing trains enroute if that would be necessary to get you there faster. Arriving in the city your friend would meet you at the station and would drive you to their house. A \u201cfat\u201d reinforcement learning method would have at its disposal actions consisting of footsteps in certain directions. This would make for a large space of possible policies, although the fne grain at which the policy would be planned\u2014individual footsteps\u2014would sure be able to fnd the optimal shortest route. The hierarchical method has at its disposal a wider variety of actions\u2014macro actions: it can plan a bike ride, a train trip, and getting a ride by your friend. The route may not be the shortest possible (who knows if the train follows the shortest 8.1 Granularity of the Structure of Problems 227 route between the two cities) but planning will be much faster than painstakingly optimizing footstep by footstep. 8.1 Granularity of the Structure of Problems In hierarchical reinforcement learning the granularity of abstractions is larger than the fne grain of the primitive actions of the environment. When we are preparing a meal, we reason in large action chunks: chop onion, cook spaghetti, instead of reasoning about the actuation of individual muscles in our hands and arms. Infants learn to use their muscles to performs certain tasks as they grow up until it becomes second nature. We generate subgoals that act as temporal abstractions, and subpolicies that are macro\u2019s of multiple ordinary actions [673]. Temporal abstraction allows us to reason about actions of diferent time scales, sometimes with course grain actions\u2014taking a train\u2014sometimes with fne grain actions\u2014opening a door\u2014mixing macro actions with primitive actions. Let us look at advantages and disadvantages of the hierarchical approach. 8.1.1 Advantages We will start with the advantages of hierarchical methods [247]. First of all, hierarchical reinforcement learning simplifes problems through abstraction. Problems are abstracted into a higher level of aggregation. Agents create subgoals and solve fne grain subtasks frst. Actions are abstracted into larger macro actions to solve these subgoals; agents use temporal abstraction. Second, the temporal abstractions increase sample efciency. The number of interactions with the environment is reduced because subpolicies are learned to solve subtasks, reducing the environment interactions. Since subtasks are learned, they can be transfered to other problems, supporting transfer learning. Third, subtasks reduce brittleness due to overspecialization of policies. Policies become more general, and are able to adapt to changes in the environment more easily. Fourth, and most importantly, the higher level of abstraction allows agents to solve larger, more complex problems. This is a reason why for complex multi-agent games such as StarCraft, where teams of agents must be managed, hierarchical approaches are used. Multi-agent reinforcement learning often exhibits a hierarchical structure; problems can be organized such that each agent is assigned its own subproblem, or the agents themselves may be structured or organized in teams or groups. There can be cooperation within the teams or competition between the teams, or the behavior can be fully cooperative or fully competitive. 228 8 Hierarchical Reinforcement Learning Flet-Berliac [247], in a recent overview, summarizes the promise of hierarchical reinforcement learning as follows: (1) achieve long-term credit assignment through faster learning and better generalization, (2) allow structured exploration by exploring with sub-policies rather than with primitive actions, and (3) perform transfer learning because diferent levels of hierarchy can encompass diferent knowledge. 8.1.2 Disadvantages There are also disadvantages and challenges associated with hierarchical reinforcement learning. First of all, it works better when there is domain knowledge available about structure in the domain. Many hierarchical methods assume that domain knowledge is available to subdivide the environment so that hierarchical reinforcement learning can be applied. Second, there is algorithmic complexity to be solved. Subgoals must be identifed in the problem environment, subpolicies must be learned, and termination conditions are needed. These algorithms must be designed, which costs programmer efort. Third, hierarchical approaches introduce a new type of actions, macro-actions. Macros are combinations of primitive actions, and their use can greatly improve the performance of the policy. On the other hand, the number of possible combinations of actions is exponentially large in their length [44]. For larger problems enumeration of all possible macros is out of the question, and the overall-policy function has to be approximated. Furthermore, at each decision point in a hierarchical planning or learning algorithm we now have the option to consider if any of the macro actions improves the current policy. The computational complexity of the planning and learning choices increases by the introduction of the macro actions [44], and approximation methods must be used. The efciency gains of the hierarchical behavioral policy must outweigh the higher cost of fnding this policy. Fourth, the quality of a behavioral policy that includes macro-actions may be less than that of a policy consisting only of primitive actions. The macro-actions may skip over possible shorter routes, that the primitive actions would have found. Conclusion There are advantages and disadvantages to hierarchical reinforcement learning. Whether an efcient policy can be constructed and whether its accuracy is good enough depends on the problem at hand, and also on the quality of the algorithms that are used to fnd this policy. For a long time, fnding good subgoals has been a major challenge. With recent algorithmic advances, especially in function approximation, important progress has been made. We will discuss these advances in the next section. 8.2 Divide and Conquer for Agents 229 8.2 Divide and Conquer for Agents To discuss hierarchical reinforcement learning, frst we will discuss a model, the options framework, that formalizes the concepts of subgoals and subpolicies. Next, we will describe the main challenge of hierarchical reinforcement learning, which is sample efciency. Then we will discuss the main part of this chapter: algorithms for fnding subgoals and subpolicies, and fnally we will provide an overview of algorithms that have been developed in the feld. 8.2.1 The Options Framework A hierarchical reinforcement learning algorithm tries to solve sequential decision problems more efciently by identifying common substructures and re-using subpolicies to solve them. The hierarchical approach has three challenges [619, 435]: fnd subgoals, fnd a meta-policy over these subgoals, and fnd subpolicies for these subgoals. Normally, in reinforcement learning, the agent follows in each state the action that is indicated by the policy. In 1999, Sutton, Precup and Singh [745] introduced the options framework. This framework introduces formal constructs with which subgoals and subpolicies can be incorporated elegantly into the reinforcement learning setting. The idea of options is simple. Whenever a state is reached that is a subgoal, then, in addition to following a primitive action suggested by the main policy, the option can be taken. This means that not the main action policy is followed, but the option policy, a macro action consisting of a diferent subpolicy specially aimed at satisfying the subgoal in one large step. In this way macros are incorporated into the reinforcement learning framework. We have been using the terms macro and option somewhat loosely until now; there is, however, a diference between macros and options. A macro is any group of actions, possibly open-ended. An option is a group of actions with a termination condition. Options take in environment observations and output actions until a termination condition is met. Formally, an option \ud835\udf14 has three elements [16]. Each option \ud835\udf14 = \u27e8\ud835\udc3c, \ud835\udf0b, \ud835\udefd\u27e9 has the following triple: \ud835\udc3c\ud835\udf14 The initiation set \ud835\udc3c \u2286 \ud835\udc46 are the states that the option can start from \ud835\udf0b\ud835\udf14(\ud835\udc4e|\ud835\udc60) The subpolicy \ud835\udf0b : \ud835\udc46 \u00d7 \ud835\udc34 \u2192 [0, 1] internal to this particular option \ud835\udefd\ud835\udf14(\ud835\udc60) The termination condition \ud835\udefd : \ud835\udc46 \u2192 [0, 1] tells us if \ud835\udf14 terminates in \ud835\udc60 The set of all options is denoted as \u03a9. In the options framework, there are thus two types of policies: the (meta-)policy over options \ud835\udf0b\u03a9(\ud835\udf14|\ud835\udc60) and the subpolicies \ud835\udf0b\ud835\udf14(\ud835\udc4e|\ud835\udc60). The subpolicies \ud835\udf0b\ud835\udf14 are short macros to get from \ud835\udc3c\ud835\udf14 to \ud835\udefd\ud835\udf14 quickly, using the previously learned macro (subpolicy). Temporal abstractions mix actions of diferent granularity, short and long, primitive action and subpolicy. They allow 230 8 Hierarchical Reinforcement Learning Fig. 8.1 Multi-Room Grid [745] traveling from \ud835\udc3c to \ud835\udefd without additional learning, using a previously provided or learned subpolicy. One of the problems for which the options framework works well, is room navigation in a Grid world (Fig. 8.1). In a regular reinforcement learning problem the agent would learn to move step by step. In hierarchical reinforcement learning the doors between rooms are bottleneck states, and are natural subgoals. Macro actions (subpolicies) are to move to a door in one multi-step action (without considering alternative actions along the way). Then we can go to a diferent room, if we choose the appropriate option, using another macro, closer to where the main goal is located. The four-room problem from the fgure is used in many research works in hierarchical reinforcement learning. Universal Value Function In the original options framework the process of identifying the subgoals (the hallways, doors) is external. The subgoals have to be provided manually, or by other methods [609, 316, 444, 727]. Subsequently, methods have been published to learn these subgoals. Options are goal-conditioned subpolicies. More recently a generalization to parameterized options has been presented in the universal value function, by Schaul et al. [665]. Universal value functions provide a unifed theory for goal-conditioned parameterized value approximators \ud835\udc49(\ud835\udc60, \ud835\udc54, \ud835\udf03). 8.2.2 Finding Subgoals Whether the hierarchical method improves over a traditional fat method depends on a number of factors. First, there should be enough repeating structure in the domain to be exploited (are there many rooms?), second, the algorithm must fnd 8.2 Divide and Conquer for Agents 231 Find Find Name Agent Environment Subg Subpol Ref STRIPS Macro-actions STRIPS planner [241] Abstraction Hier. State abstraction Scheduling/plan. + + [415] HAM Abstract machines MDP/maze [588] MAXQ Value function decomposition Taxi [196] HTN Task networks Block world [171, 272] Bottleneck Randomized search Four room + + [727] Feudal manager/worker, RNN Atari + + [811, 182] Self p. goal emb. self play subgoal Mazebase, AntG + + [732] Deep Skill Netw. deep skill array, policy distillation Minecraft + + [766] STRAW end-to-end implicit plans Atari + + [810] HIRO of-policy Ant maze + + [546] Option-critic policy-gradient Four room + + [45] HAC actor critic, hindsight exper. repl. Four room ant + + [471, 19] Modul. pol. hier. bit-vector, intrinsic motivation FetchPush + + [590] h-DQN intrinsic motivation Montezuma\u2019s R. + [435] Meta l. sh. hier. shared primitives, strength metric Walk, crawl + + [256] CSRL model-based transition dynamics Robot tasks + [477] Learning Repr. unsup. subg. disc., intrinsic motiv. Montezuma\u2019s R. + + [619] AMIGo Adversarially intrinsic goals MiniGrid PCG + + [125] Table 8.1 Hierarchical Reinforcement Learning Approaches (Tabular and Deep) appropriate subgoals (can it fnd the doors?), third, the options that are found must repeat many times (is the puzzle played frequently enough for the optionfnding cost to be ofset?), and, fnally, subpolicies must be found that give enough improvement (are the rooms large enough that options outweigh actions?). The original options framework assumes that the structure of the domain is obvious, and that the subgoals are given. When this is not the case, then the subgoals must be found by the algorithm. Let us look at an overview of approaches, both tabular and with deep function approximation. 8.2.3 Overview of Hierarchical Algorithms The options framework provides a convenient formalism for temporal abstraction. In addition to the algorithms that can construct policies consisting of individual actions, we need algorithms that fnd the subgoals, and learn the subpolicies. Finding efcient algorithms for the three tasks is important in order to be able to achieve an efciency advantage over ordinary \u201cfat\u201d reinforcement learning. Hierarchical reinforcement learning is based on subgoals. It implements a toplevel policy over these subgoals, and subpolicies to solve the subgoals. The landscape of subgoals determines to a great extent the efciency of the algorithm [216]. In recent years new algorithms have been developed to fnd sub-policies for options, and the feld has received a renewed interest [592]. Table 8.1 shows a list of approaches. 232 8 Hierarchical Reinforcement Learning The table starts with classic tabular approaches (above the line). It continues with more recent deep learning approaches. We will now look at some of the algorithms. 8.2.3.1 Tabular Methods Divide and conquer is a natural method to exploit hierarchical problem structures. A famous early planning system is STRIPS, the Stanford Research Insititute Problem Solver, designed by Richard Fikes and Nils Nilsson in the 1970s [241]. STRIPS created an extensive language for expressing planning problems, and was quite infuential. Concepts from STRIPS are at the basis of most modern planning systems, action languages, and knowledge representation systems [266, 51, 798]. The concept of macros as open-ended groups of actions was used in STRIPS to create higher-level primitives, or subroutines. Later planning-based approaches are Parr and Russell\u2019s hierarchical abstract machines [588] and Dietterich\u2019s MAXQ [196]. Typical applications of these systems are the blocks world, in which a robot arm has to manipulate blocks, stacking them on top of each other, and the taxi world, which we have seen in earlier chapters. An overview of these and other early approaches can be found in Barto et al. [56]. Many of these early approaches focused on macros (the subpolicies), and require that the experimenters identify the subgoals in a planning language. For problems where no such obvious subgoals are available, Knoblock [415] showed how abstraction hierarchies can be generated, although Backstrom et al. [44] found that doing so can be exponentially less efcient. For small room problems, however, Stolle and Precup [727] showed that subgoals can be found in a more efcient way, using a short randomized search to fnd bottleneck states that can be used as subgoals. This approach fnds subgoals automatically, and efciently, in a rooms-grid world. Tabular hierarchical methods were mostly applied to small and low-dimensional problems, and have difculty fnding subgoals, especially for large problems. The advent of deep function approximation methods attracted renewed interest in hierarchical methods. 8.2.3.2 Deep Learning Function approximation can potentially reduce the problem of exponentially exploding search spaces that plague tabular methods, especially for subgoal discovery. Deep learning exploits similarities between states using commonalities between features, and allows larger problems to be solved. Many new methods were developed. The deep learning approaches in hierarchical reinforcement learning typically are end-to-end: they generate both appropriate subgoals and their policies. Feudal networks is an older idea from Dayan and Hinton in which an explicit control hierarchy is built of managers and workers that work on tasks and subtasks, organized as in a feudal fefdom [182]. This idea was used 15 years later as a model for hierarchical deep reinforcement learning by Vezhnevets et al. [811], out8.2 Divide and Conquer for Agents 233 performing non-hierarchical A3C on Montezuma\u2019s Revenge, and performing well on other Atari games, achieving a similar score as Option-critic [45]. The approach uses a manager that sets abstract goals (in latent space) for workers. The feudal idea was also used as inspiration for a multi-agent cooperative reinforcement learning design [9], on proof of concept cooperative multi-agent problems on pre-specifed hierarchies. Other deep learning approaches include deep skill networks [766], of-policy approaches [546], and self-play [732]. The latter uses an intrinsic motivation approach to learn both a low level actor and the representation of the state space [597]. Subgoals are learned at the higher level, after which policies are trained at the lower level. Application environments for deep learning have become more challenging, and now include Minecraft, and robotic tasks such as ant navigation in multiple rooms, and maze navigation. The approaches outperform basic non-hierarchical approaches such as DQN. In STRAW Vezhnevets et al. [810] learns a model of macros-actions, and is evaluated on text recognition tasks and on Atari games such as PacMan and Frostbite, showing promising results. Zhang et al. [873] use world models to learn latent landmarks (subgoals) for graph-based planning (see also Sect. 5.2.1.2). Almost two decades after the options framework was introduced, Bacon et al. [45] introduced the option-critic approch. Option-critic extends the options framework with methods to learn the option subgoal and subpolicy, so that it does not have to be provided externally anymore. The options are learned similar to actor critic using a gradient-based approach. The intra-option policies and termination functions, as well as the policy over options are learned simultaneously. The user of the Option-critic approach has to specify how many options have to be learned. The Option-critic paper reports good results for experiments in a four-room environment with 4 and with 8 options (Fig. 8.2). Option critic learns options in an end-to-end fashion that scales to larger domains, outperforming DQN in four ALE games (Asterix, Seaquest, Ms. Pacman, Zaxxon) [45]. Levy et al. [471] presented an approach based on Option critic, called Hierarchical actor critic. This approach can learn the goal-conditioned policies at diferent levels concurrently, where previous approaches had to learn them in a bottom up fashion. In addition, Hierarchical actor critic uses a method for learning the multiple levels of policies for sparse rewards, using Hindsight experience replay [19]. In typical robotics tasks, the reinforcement learning algorithm learns more from a successful outcome (bat hits the ball) than from an unsuccessful outcome (bat misses the ball, low). In this failure case a human learner would draw the conclusion that we can now reach another goal, being bat misses the ball if we aim low. Hindsight experience replay allows learning to take place by incorporating such adjustments of the goal using the beneft of hindsight, so that the algorithm can now also learn from failures, by pretending that they were the goal that you wanted to reach, and learn from them as if they were. Hierarchical actor critic has been evaluated on grid world tasks and more complex simulated robotics environments, using a 3-level hierarchy. 234 8 Hierarchical Reinforcement Learning Fig. 8.2 Termination Probabilities Learned with 4 Options by Option-Critic [45]; Options Tend to Favor Squares Close to Doors A fnal approach that we mention is AMIGo [125], which is related to intrinsic motivation. It uses a teacher to adversarially generate goals for a student. The student is trained with increasingly challenging goals to learn general skills. The system efectively builds up an automatic curriculum of goals. It is evaluated on MiniGrid, a parameterized world that is generated by procedural content generation [621, 143]. Conclusion Looking back at the list of advantages and disadvantages at the start of this chapter, we see a range of interesting and creative ideas that achieve the advantages (Sect. 8.1.1) by providing methods to address the disadvantages (Sect. 8.1.2). In general, the tabular methods are restricted to smaller problems, and often need to be provided with subgoals. Most of the newer deep learning methods fnd subgoals by themselves, for which then subpolicies are found. Many promising methods have been discussed, and most report to outperform one or more fat baseline algorithms. The promising results stimulate further research in deep hierarchical methods, and more benchmark studies of large problems are needed. Let us have a closer look at the environments that have been used so far. 8.3 Hierarchical Environments 235 Fig. 8.3 Four Rooms, and One Room with Subpolicy and Subgoal [745] 8.3 Hierarchical Environments Many environments for hierarchical reinforcement learning exist, starting with mazes and the four-room environment from the options paper. Environments have evolved with the rest of the feld of reinforcement learning; for hierarchical reinforcement learning no clear favorite benchmark has emerged, although Atari en MuJoCo tasks are often used. In the following we will review some of the environments that are used in algorithmic studies. Most hierarchical environments are smaller than typically used for model-free fat reinforcement learning, although some studies do use complex environments, such as StarCraft. 8.3.1 Four Rooms and Robot Tasks Sutton et al. [745] presented the four rooms problems to illustrate how the options model worked (Fig. 8.3; left panel). This environment has been used frequently in subsequent papers on reinforcement learning. The rooms are connected by hallways. Options point the way to these hallways, which lead to the goal \ud835\udc3a2 of the environment. A hierarchical algorithm should identify the hallways as the subgoals, and create subpolicies for each room to go to the hallway subgoal (Fig. 8.3; right panel). The four-room environment is a toy environment with which algorithms can be explained. More complex versions can be created by increasing the dimensions of the grids and by increasing the number of rooms. The Hierarchical actor critic paper uses the four-room environment as a basis for a robot to crawl through. The agent has to learn both the locomotion task and solving the four-room problem (Fig. 8.4). Other environments that are used for hierarchical reinforcement learning are robot tasks, such as shown in Fig. 8.5 [633]. 236 8 Hierarchical Reinforcement Learning Fig. 8.4 Ant in Four Rooms [471] Fig. 8.5 Six Robot Tasks [633] 8.3.2 Montezuma\u2019s Revenge One of the most difcult situations for reinforcement learning is when there is little reward signal, and when it is delayed. The game of Montezuma\u2019s Revenge consists of long stretches in which the agent has to walk without the reward changing. Without smart exploration methods this game cannot be solved. Indeed, the game has long been a test bed for research into goal-conditioned and exploration methods. For the state in Fig. 8.6, the player has to go through several rooms while collecting items. However, to pass through doors (top right and top left corners), the player needs the key. To pick up the key, the player has to climb down the ladders 8.3 Hierarchical Environments 237 Fig. 8.6 Montezuma\u2019s Revenge [71] Fig. 8.7 Intrinsic Motivation in Reinforcement Learning [712] and move towards the key. This is a long and complex sequence before receiving the reward increments for collecting the key. Next, the player has to go to the door to collect another increase in reward. Flat reinforcement learning algorithms struggle with this environment. For hierarchical reinforcement the long stretches without a reward can be an opportunity to show the usefulness of the option, jumping through the space from states where the reward changes to another reward change. To do so, the algorithm has to be able to identify the key as a subgoal. Rafati and Noelle [619] learn subgoals in Montezuma\u2019s Revenge, and so do Kulkarni et al. [435]. Learning to choose promising subgoals is a challenging problem by itself. Once subgoals are found, the subpolicies can be learned by introducing a reward signal for achieving the subgoals. Such intrinsic rewards are related to intrinsic motivation and the psychological concept of curiosity [29, 577]. Figure 8.7 illustrates the idea behind intrinsic motivation. In ordinary reinforcement learning, a critic in the environment provides rewards to the agent. When the agent has an internal environment where an internal critic provides rewards, these internal rewards provide an intrinsic motivation to the agent. This mechanism aims to more closely model exploration behavior in animals and humans [712]. For example, during curiosity-driven activities, children use knowledge to generate intrinsic goals while playing, building block structures, etc. While doing this, they 238 8 Hierarchical Reinforcement Learning construct subgoals such as putting a lighter entity on top of a heavier entity in order to build a tower [435, 712]. Intrinsic motivation is an active feld of research. A recent survey is [29]. Montezuma\u2019s Revenge has also been used as benchmark for the Go-Explore algorithm, that has achieved good results in sparse reward problems, using a goalconditioned policy with cell aggregation [222]. Go-Explore performs a planning-like form of backtracking, combining elements of planning and learning in a diferent way than AlphaZero. 8.3.3 Multi-Agent Environments Many multi-agent problems are a natural match for hierarchical reinforcement learning, since agents often work together in teams or other hierarchical structure. For multi-agent hierarchical problems a multitude of diferent environments are used. Makar et al. [499, 273] study cooperative multi-agent learning, and use small tasks such as a two-agent cooperative trash collection task, the dynamic rescheduling of automated guided vehicles in a factory, as well as an environment in which agents communicate amongst eachother. Han et al. [312] use a multi-agent Taxi environment. Tang et al. [753] also use robotic trash collection. Due to the computational complexity of multi-agent and hierarchical environments, many of the environments are of lower dimensionality than what we see in model-free and model-based single-agent reinforcement learning. There are a few exceptions, as we saw in the previous chapter (Capture the Flag and StarCraft). However, the algorithms that were used for these environments were based on population-based self-play algorithms that are well-suited for parallelization; hierarchical reinforcement algorithms of the type that we have discussed in this chapter were of less importance [813]. 8.3.4 Hands On: Hierarchical Actor Citic Example The research reported in this chapter is of a more manageable scale than in some other chapters. Environments are smaller, computational demands are more reasonable. Four-room experiments and experiments with movement of single robot arms invites experimentation and tweaking. Again, as in the other chapters, the code of most papers can be found online on GitHub. Hierarchical reinforcement learning is well suited for experimentation because the environments are small, and the concepts of hierarchy, team, and subgoal, are intuitively appealing. Debgugging one\u2019s implementation should be just that bit easier when the desired behavior of the diferent pieces of code is clear. 8.3 Hierarchical Environments 239 Algorithm 8.1 Hierarchical Actor Critic [471] Input: Key agent parameters: number of levels in hierarchy \ud835\udc58, maximum subgoal horizon \ud835\udc3b, and subgoal testing frequency \ud835\udf06 Output: \ud835\udc58 trained actor and critic functions \ud835\udf0b0, ..., \ud835\udf0b\ud835\udc58\u22121, \ud835\udc440, ..., \ud835\udc44\ud835\udc58\u22121 for \ud835\udc40 episodes do \u22b2 Train for M episodes \ud835\udc60 \u2190 \ud835\udc46init, \ud835\udc54 \u2190 \ud835\udc3a\ud835\udc58\u22121 \u22b2 Sample initial state and task goal train-level(\ud835\udc58 \u2212 1, \ud835\udc60, \ud835\udc54) \u22b2 Begin training Update all actor and critic networks end for function train-level(\ud835\udc56 :: level, \ud835\udc60 :: state, \ud835\udc54 :: goal) \ud835\udc60\ud835\udc56 \u2190 \ud835\udc60, \ud835\udc54\ud835\udc56 \u2190 \ud835\udc54 \u22b2 Set current state and goal for level \ud835\udc56 for \ud835\udc3b attempts or until \ud835\udc54\ud835\udc5b, \ud835\udc56 \u2264 \ud835\udc5b < \ud835\udc58 achieved do \ud835\udc4e\ud835\udc56 \u2190 \ud835\udf0b\ud835\udc56 (\ud835\udc60\ud835\udc56, \ud835\udc54\ud835\udc56) + noise (if not subgoal testing) \u22b2 Sample (noisy) action from policy if \ud835\udc56 > 0 then Determine whether to test subgoal \ud835\udc4e\ud835\udc56 \ud835\udc60\u2032 \ud835\udc56 \u2190 train-level(\ud835\udc56 \u2212 1, \ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56) \u22b2 Train level \ud835\udc56 \u2212 1 using subgoal \ud835\udc4e\ud835\udc56 else Execute primitive action \ud835\udc4e0 and observe next state \ud835\udc60\u2032 0 end if \u22b2 Create replay transitions if \ud835\udc56 > 0 and \ud835\udc4e\ud835\udc56 missed then if \ud835\udc4e\ud835\udc56 was tested then \u22b2 Penalize subgoal \ud835\udc4e\ud835\udc56 Replay_Bufer\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f = Penalty, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 = \ud835\udc54\ud835\udc56, \ud835\udefe = 0] end if \ud835\udc4e\ud835\udc56 \u2190 \ud835\udc60\u2032 \ud835\udc56 \u22b2 Replace original action with action executed in hindsight end if \u22b2 Evaluate executed action on current goal and hindsight goals Replay_Bufer\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f \u2208 {\u22121, 0}, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 = \ud835\udc54\ud835\udc56, \ud835\udefe \u2208 {\ud835\udefe, 0}] HER_Storage\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f =TBD, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 =TBD, \ud835\udefe =TBD] \ud835\udc60\ud835\udc56 \u2190 \ud835\udc60\u2032 \ud835\udc56 end for Replay_Bufer\ud835\udc56 \u2190 Perform HER using HER_Storage\ud835\udc56 transitions return \ud835\udc60\u2032 \ud835\udc56 \u22b2 Output current state end function To get you started with hierarchical reinforcement learning we will go to HAC: Hierarchical actor critic [471]. Algorithm 8.1 shows the pseudocode, where TBD is the subgoal in hindsight [471]. A blog1 with animations has been written, a video2 has been made of the results, and the code can be found in GitHub.3 To run the hierarchical actor critic experiments, you need MuJoCo and the required Python wrappers. The code is TensorFlow 2 compatible. When you have cloned the repository, run the experiment with 1 http://bigai.cs.brown.edu/2019/09/03/hac.html 2 https://www.youtube.com/watch?v=DYcVTveeNK0 3 https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC240 8 Hierarchical Reinforcement Learning python3 initialize_HAC.py --retrain which will train a UR5 reacher agent with a 3-level hierarchy. Here is a video4 that shows how it should look like after 450 training episodes. You can watch your trained agent with the command python3 initialize_HAC.py --test --show The README at the GitHub repository contains more suggestions on what to try. You can try diferent hyperparameters, and you can modify the designs, if you feel like it. Happy experimenting! Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary A typical reinforcement learning algorithm moves in small steps. For a state, it picks an action, gives it to the environment for a new state and a reward, and processes the reward to pick a new action. Reinforcement learning works step by small step. In contrast, consider the following problem: in the real world, when we plan a trip from A to B, we use abstraction to reduce the state space, to be able to reason at a higher level. We do not reason at the level of footsteps to take, but we frst decide on the mode of transportation to get close to our goal, and then we fll in the diferent parts of the journey with small steps. Hierarchical reinforcement learning tries to mimic this idea: conventional reinforcement learning works at the level of a single state; hierarchical reinforcement learning performs abstraction, solving subproblems in sequence. Temporal abstraction is described in a paper by Sutton et al. [745]. Hierarchical reinforcement learning uses the principles of divide and conquer to make solving large problems feasible. It fnds subgoals in the space that it solves with subpolicies (macros or options). Despite the appealing intuition, progress in hierarchical reinforcement learning was initially slow. Finding these new subgoals and subpolicies is a computationally intensive problems that is exponential in the number of actions, and in some situations it is quicker to use conventional \u201cfat\u201d reinforcement learning methods, unless domain knowledge can be exploited. The advent of deep learning provided a 4 https://www.youtube.com/watch?v=R86Vs9Vb6Bc 8.3 Hierarchical Environments 241 boost to hierarchical reinforcement learning, and much progress is being reported in important tasks such as learning subgoals automatically, and fnding subpolicies. Although popular for single-agent reinforcement learning, hierarchical methods are also used in multi-agent problems. Multi-agent problems often feature agents that work in teams, that cooperate within, and compete between the teams. Such an agent-hierarchy is a natural ft for hierachical solution methods. Hierarchical reinforcment learning remains a promising technique. Further Reading Hierarchical reinforcement learning, and subgoal fnding, have a rich and long history [248, 588, 745, 609, 196, 316, 444, 56, 592]; see also Table 8.1. Macro-actions are a basic approach [316, 626]. Others, using macros, are [855, 852, 214]. The options framework has provided a boost to the development of the feld [745]. Other approaches are MAXQ [195] and Feudal networks [811]. Earlier tabular approaches are [248, 588, 745, 609, 196, 316, 444]. Recent method are Option-critic [45] and Hierarchical actor-critic [471]. There are many deep learning methods for fnding subgoals and subpolicies [471, 582, 248, 598, 546, 256, 806, 665, 735, 177]. Andrychowicz et al. [19] introduce Hindsight experience replay, which can improve performance for hierarchical methods. Instrinsic motivation is a concept from developmental neuroscience that has come to reinforcement learning with the purpose of providing learning signals in large spaces. It is related to curiosity. Botvinick et al. [101] have written an overview of hierarchical reinforcement learning and neuroscience. Aubret et al. [29] provide a survey of intrinsic motivation for reinforcement learning. Instrinsic motivation is used by [435, 619]. Intrinsic motivation is closely related to goaldriven reinforcement learning [665, 650, 576, 577, 578]. Exercises It is time for the Exercises to test your knowledge. Questions Below are some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer should be sufcient. 1. Why can hierarchical reinforcement learning be faster? 2. Why can hierarchical reinforcement learning be slower? 3. Why may hierarchical reinforcement learning give an answer of lesser quality? 4. Is hierachical reinforcement more general or less general? 242 8 Hierarchical Reinforcement Learning 5. What is an option? 6. What are the three elements that an option consists of? 7. What is a macro? 8. What is intrinsic motivation? 9. How do multi agent and hierarchical reinforcement learning ft together? 10. What is so special about Montezuma\u2019s Revenge? Exercises Let us go to the programming exercises to become more familiar with the methods that we have covered in this chapter. 1. Four Rooms Implement a hierarchical solver for the four-rooms environment. You can code the hallway subgoals using domain knowledge. Use a simple tabular, planning, approach. How will you implement the subpolicies? 2. Flat Implement a fat planning or Q-learning-based solver for 4-rooms. Compare this program to the tabular hierarchical solver. Which is quicker? Which of the two does fewer environment actions? 3. Sokoban Implement a Sokoban solver using a hierarchical approach (challenging). The challenge in Sokoban is that there can be dead-ends in the game that you create rendering the game unsolvable (also see the literature [696, 291]). Recognizing these dead-end moves is important. What are the subgoals? Rooms, or each box-task is one subgoal, or can you fnd a way to code dead-ends as subgoal? How far can you get? Find Sokoban levels.567 4. Petting Zoo Choose one of the easier multi-agent problems from the Petting Zoo [761], introduce teams, and write a hierarchical solver. First try a tabular planning approach, then look at hierarchical actor critic (challenging). 5. StarCraft The same as the previous exercise, only now with StarCraft (very challenging). 5 http://sneezingtiger.com/sokoban/levels.html 6 http://www.sokobano.de/wiki/index.php?title=Level_format 7 https://www.sourcecode.se/sokoban/levels Chapter 9 Meta-Learning Although current deep reinforcement learning methods have obtained great successes, training times for most interesting problems are high; they are often measured in weeks or months, consuming time and resources\u2014as you may have noticed while doing some of the exercises at the end of the chapters. Model-based methods aim to reduce the sample complexity in order to speed up learning\u2014but still, for each new task a new network has to be trained from scratch. In this chapter we turn to another approach, that aims to re-use information learned in earlier training tasks from a closely related problem. When humans learn a new task, they do not learn from a blank slate. Children learn to walk and then they learn to run; they follow a training curriculum, and they remember. Human learning builds on existing knowledge, using knowledge from previously learned tasks to facilitate the learning of new tasks. In machine learning such transfer of previously learned knowledge from one task to another is called transfer learning. We will study it in this chapter. Humans learn continuously. When learning a new task, we do not start from scratch, zapping our minds frst to emptiness. Previously learned task-representations allow us to learn new representations for new tasks quickly; in efect, we have learned to learn. Understanding how we (learn to) learn has intrigued artifcial intelligence researchers since the early days, and it is the topic of this chapter. The felds of transfer learning and meta-learning are tightly related. For both, the goal is to speed up learning a new task, using previous knowledge. In transfer learning, we pretrain our parameter network with knowledge from a single task. In meta-learning, we use multiple related tasks. In this chapter, we frst discuss the concept of lifelong learning, something that is quite familiar to human beings. Then we discuss transfer learning, followed by meta-learning. Next, we discuss some of the benchmarks that are used to test transfer learning and meta-learning. The chapter is concluded with exercises, a summary, and pointers to further reading. 243 244 9 Meta-Learning Core Concepts \u2022 Knowledge transfer \u2022 Learning to learn Core Problem \u2022 Speed-up learning with knowledge from related tasks Core Algorithms \u2022 Pretraining (Listing 9.1) \u2022 Model-Agnostic Meta-Learning (Alg. 9.1) Foundation Models Humans are good at meta-learning. We learn new tasks more easily after we have learned other tasks. Teach us to walk, and we learn how to run. Teach us to play the violin, the viola, and the cello, and we more easily learn to play the double bass (Fig. 9.1). Current deep learning networks are large, with many layers of neurons and millions of parameters. For new problems, training large networks on large datasets or environments takes time, up to weeks, or months\u2014both for supervised and for reinforcement learning. In order to shorten training times for subsequent networks, these are often pretrained, using foundation models [96]. With pretraining, some of the exisiting weights of another network are used as starting point for fnetuning a network on a new dataset, instead of using a randomly initialized network. Pretraining works especially well on deeply layered architectures. The reason is that the \u201cknowledge\u201d in the layers goes from generic to specifc: lower layers contain generic flters such as lines and curves, and upper layers contain more specifc flters such as ears, noses, and mouths (for a face recognition application) [459, 497]. These lower layers contain more generic information that is well suited for transfer to other tasks. Foundation models are large models in a certain feld, such as image recognition, or natural language processing, that are trained extensively on large datasets. Foundation models contain general knowledge, that can be specialized for a certain purpose. The world of applied deep learning has moved from training a net from scratch for a certain problem, to taking a part of an existing net that is trained for a related problem and then fnetuning it on the new task. Nearly all state-of-the-art 9 Meta-Learning 245 Fig. 9.1 Violin, viola, cello and double bass Fig. 9.2 Imagenet Thumbnails [646] visual perception approaches rely on the same approach: (1) pretrain a convolutional network on a large, manually annotated image classifcation dataset and (2) fnetune the network on a smaller, task-specifc dataset [278, 200, 865, 8, 360], see Fig. 9.2 for some thumbnails of Imagenet [646]. For natural language recognition, pretraining is also the norm\u2014for example, for large-scale pretrained language models such as Word2vec [513, 514], BERT [193] and GPT-3 [617]. In this chapter, we will study pretraining, and more. 246 9 Meta-Learning Name Dataset Task Single-task Learning \ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \u2286 \ud835\udc37, \ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \u2286 \ud835\udc37 \ud835\udc47 = \ud835\udc47\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b = \ud835\udc47\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 Transfer Learning \ud835\udc371 \u226b \ud835\udc372 \ud835\udc471 \u2260 \ud835\udc472 Multi-task Learning \ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \u2286 \ud835\udc37, \ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \u2286 \ud835\udc37 \ud835\udc471 \u2260 \ud835\udc472 Domain Adaptation \ud835\udc371 \u2260 \ud835\udc372 \ud835\udc471 = \ud835\udc472 Meta-Learning {\ud835\udc371, . . . , \ud835\udc37\ud835\udc41\u22121} \u226b \ud835\udc37\ud835\udc41 \ud835\udc471, . . . , \ud835\udc47\ud835\udc5b\u22121 \u2260 \ud835\udc47\ud835\udc41 Table 9.1 Diferent Kinds of Supervised Learning. A typical single learning task is to classify pictures of animals into diferent classes. The dataset is split into a train and a testset, and the task (loss function) is the same at train and test time. A transfer learning task uses part of the knowledge (network parameters) that are learned on a large dataset to initialize a second network, that is trained subsequently (fne tuned) on a diferent dataset. This second dataset/learning task is related to the frst task, so the learning the second task goes faster. For example, having learned to recognize cars, may be useful to speedup recognizing trucks. In multi-task learning several related tasks are trained at the same time, possibly beneftting from better regularization. An example could be training spam flters for diferent users at the same time. Domain adaptation tries to adapt the network to a new dataset of related examples, such as images of pedestrians in diferent light conditions. Meta-learning tries to learn meta knowledge, such as hyperparameters over a sequence of related (larger) learning tasks, so that a new learning task goes faster. In deep meta-learning these hyperparameters include the initial network parameters, in this sense meta-learning can be considered to be multi-job transfer learning. 9.1 Learning to Learn Related Problems Training times for modern deep networks are large. Training AlexNet for ImageNet took 5-6 days on 2 GPUs in 2012 [431], see Sect. B.3.1. In reinforcement learning, training AlphaGo took weeks [703, 706], in natural language processing, training also takes a long time [193], even excessively long as in the case of GPT-3 [114]. Clearly, some solution is needed. Before we look closer at transfer learning, let us have a look at the bigger picture: lifelong learning. When humans learn a new task, learning is based on previous experience. Initial learning by infants of elementary skills in vision, speech, and locomotion takes years. Subsequent learning of new skills builds on the previously acquired skills. Existing knowledge is adapted, and new skills are learned based on previous skills. Lifelong learning remains a long-standing challenge for machine learning; in current methods the continuous acquisition of information often leads to interference of concepts or catastrophic forgetting [701]. This limitation represents a major drawback for deep networks that typically learn representations from stationary batches of training data. Although some advances have been made in narrow domains, signifcant advances are necessary to approach generally-applicable lifelong learning. Diferent approaches have been developed. Among the methods are metalearning, domain adaptation, multi-task learning, and pretraining. Table 9.1 lists these approaches, together with regular single task learning. The learning tasks are formulated using datasets, as in a regular supervised setting. The table shows 9.2 Transfer Learning and Meta-Learning Agents 247 how the lifelong learning methods difer in their training and test dataset, and the diferent learning tasks. The frst line shows regular single-task learning. For single-task learning, the training and test dataset are both drawn from the same distribution (the datasets do not contain the same examples, but they are drawn from the same original dataset and the data distribution is expected to be the same), and the task to perform is the same for training and test. In the next line, for transfer learning, networks trained on one dataset are used to speedup training for a diferent task, possibly using a much smaller dataset [581]. Since the datasets are not drawn from the same master dataset, their distribution will difer, and there typically is only an informal notion of how \u201crelated\u201d the datasets are. However, in practice transfer learning often provides signifcant speedups, and transfer learning, pretraining and fnetuning are currently used in many real-world training tasks, sometimes using large foundation models as a basis. In multi-task learning, more than one task is learned from one dataset [129]. The tasks are often related, such as classifcation tasks of diferent, but related, classes of images, or learning spam flters for diferent email-users. Regularization may be improved when a neural network is trained on related tasks at the same time [59, 151]. So far, our learning tasks were trying to speedup learning diferent tasks with related data. Domain adaptation switches this around: the task remains the same, but the data changes. In domain adaptation, a diferent dataset is used to perform the same task, such as recognizing pedestrians in diferent light conditions [782]. In meta-learning, both datasets and tasks are diferent, although not too diferent. In meta-learning, a sequence of datasets and learning tasks is generalized to learn a new (related) task quickly [106, 351, 363, 667]. The goal of meta-learning is to learn hyperparameters over a sequence of learning tasks. 9.2 Transfer Learning and Meta-Learning Agents We will now introduce transfer learning and meta-learning algorithms. Where in normal learning we would intialize our parameters randomly, in transfer learning we initialize them with (part of) the training results of another training task. This other task is related in some way to the new task, for example, a task to recognize images of dogs playing in a forest is initalized on a dataset of dogs playing in a park. The parameter transfer of the old task is called pretraining, the second phase, where the network learns the new task on the new dataset, is called fnetuning. The pretraining will hopefully allow the new task to train faster. Where transfer learning transfers knowledge from a single previous task, metalearning aims to generalize knowledge from multiple previous learning tasks. Metalearning tries to learn hyperparameters over these related learning tasks, that tell the algorithm how to learn the new task. Meta-learning thus aims to learn to learn. In deep meta-learning approaches, the initial network parameters are typically part 248 9 Meta-Learning of the hyperparameters. Note that in transfer learning we also use (part of) the parameters to speed up learning (fnetuning) a new, related task. We can say that deep meta-learning generalizes transfer learning by learning the initial parameters over not one but a sequence of related tasks [351, 364]. (Defnitions are still in fux, however, and diferent authors and diferent felds have diferent defnitions.) Transfer learning has become part of the standard approach in machine learning, meta-learning is still an area of active research. We will look into meta-learning shortly, after we have looked into transfer learning, multi-task learning, and domain adaptation. 9.2.1 Transfer Learning Transfer learning aims to improve the process of learning new tasks using the experience gained by solving similar problems [606, 774, 773, 585]. Transfer learning aims to transfer past experience of source tasks and use it to boost learning in a related target task [581, 879]. In transfer learning, we frst train a base network on a base dataset and task, and then we repurpose some of the learned features to a second target network to be trained on a target dataset and task. This process works better if the features are general, meaning suitable to both base and target tasks, instead of specifc to the base task. This form of transfer learning is called inductive transfer. The scope of possible models (model bias) is narrowed in a benefcial way by using a model ft on a diferent but related task. First we will look at task similarity, then at transfer learning, multi-task learning, and domain adaptation. 9.2.1.1 Task Similarity Clearly, pretraining works better when the tasks are similar [129]. Learning to play the viola based on the violin is more similar than learning the tables of multiplication based on tennis. Diferent measures can be used to measure the similarity of examples and features in datasets, from linear one-dimensional measures to nonlinear multi-dimensional measures. Common measures are the cosine similarity for real-valued vectors and the radial basis function kernel [752, 809], but many more elaborate measures have been devised. Similarity measures are also used to devise meta-learning algorithms, as we will see later. 9.2 Transfer Learning and Meta-Learning Agents 249 9.2.1.2 Pretraining and Finetuning When we want to transfer knowledge, we can transfer the weights of the network, and then start re-training with the new dataset. Please refer back to Table 9.1. In pretraining the new dataset is smaller than the old dataset \ud835\udc371 \u226b \ud835\udc372, and we train for a new task \ud835\udc471 \u2260 \ud835\udc472, which we want to train faster. This works when the new task is diferent, but similar, so that the old dataset \ud835\udc371 contains useful information for the new task \ud835\udc472. To learn new image recognition problems, it is common to use a deep learning model pre-trained for a large and challenging image classifcation task such as the ImageNet 1000-class photograph classifcation competition. Three examples of pretrained models include: the Oxford VGG Model, Google\u2019s Inception Model, Microsoft\u2019s ResNet Model. For more examples, see the Cafe Model Zoo,1 or other zoos2 where more pre-trained models are shared. Transfer learning is efective because the images were trained on a corpus that requires the model to make predictions on a large number of classes, requiring the model to be general, and since it efciently learns to extract features in order to perform well. Convolutional neural network features are more generic in lower layers, such as color blobs or Gabor flters, and more specifc to the original dataset in higher layers. Features must eventually transition from general to specifc in the last layers of the network [862]. Pretraining copies some of the layers to the new task. Care should be taken how much of the old task network to copy. It is relatively safe to copy the more general lower layers. Copying the more specifc higher layers may be detrimental to performance. In natural language processing a similar situation occurs. In natural language processing, a word embedding is used that is a mapping of words to a high-dimensional continuous vector where diferent words with a similar meaning have a similar vector representation. Efcient algorithms exist to learn these word representations. Two examples of common pre-trained word models trained on very large datasets of text documents include Google\u2019s Word2vec model [513] and Stanford\u2019s GloVe model [596]. 9.2.1.3 Hands-on: Pretraining Example Transfer learning and pretraining have become a standard approach to learning new tasks, especially when only a small dataset is present, or when we wish to limit training time. Let us have a look at a hands-on example that is part of the Keras distribution (Sect. B.3.3). The Keras transfer learning example provides a basic Imagenet-based approach, getting data from TensorFlow DataSets (TFDS). The example follows a supervised learning approach, although the learning and 1 https://caffe.berkeleyvision.org/model_zoo.html 2 https://modelzoo.co 250 9 Meta-Learning 1 base_model = keras. applications .Xception( 2 weights=\u2019imagenet \u2019, # Load weights pre -trained on ImageNet. 3 input_shape =(150 , 150, 3), 4 include_top=False) # Do not include the ImageNet classifier at the top. 5 6 base_model.trainable = False Listing 9.1 Pretraining in Keras (1): instantiate model fne-tuning phase can easily be substituted by a reinforcement learning setup. The Keras transfer learning example is at the Keras site,3 and can also be run in a Google Colab. The most common incarnation of transfer learning in the context of deep learning is the following worfklow: 1. Take layers from a previously trained model. 2. Freeze them, so as to avoid destroying any of the information they contain during future training rounds. 3. Add some new, trainable layers on top of the frozen layers. They will train on the new dataset using the old features as predictions. 4. Train the new layers on your new (small) dataset. 5. A last, optional, step, is fne-tuning of the frozen layers, which consists of unfreezing the entire model you obtained above, and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data. Let us look at how this workfow works in practice in Keras. At the Keras site we fnd an accessible example code for pretraining (see Listing 9.1). First, we instantiate a base model with pretrained weights. (We do not include the classifer on top.) Then, we freeze the base model (see Listing 9.2). Next, we create a new model on top, and train it. The Keras example contains this example and others, including fne-tuning. Please go to the Keras site and improve your experience with pretraining in practice. 9.2.1.4 Multi-task Learning Multi-task learning is related to transfer learning. In multi-task learning a single network is trained at the same time on multiple related tasks [772, 129]. In multi-task learning the learning process of one task benefts from the simultaneous learning of the related task. This approach is efective when the tasks have some commonality, such as learning to recognize breeds of dogs and breeds of cats. 3 https://keras.io/guides/transfer_learning/ 9.2 Transfer Learning and Meta-Learning Agents 251 1 inputs = keras.Input(shape =(150 , 150, 3)) 2 # The base_model is running in inference mode here , 3 # by passing \u2018training=False \u2018. This is important for fine -tuning 4 x = base_model(inputs , training=False) 5 # Convert features of shape \u2018base_model . output_shape [1:] \u2018 to vectors 6 x = keras.layers. GlobalAveragePooling2D ()(x) 7 # A Dense classifier with a single unit (binary classification ) 8 outputs = keras.layers.Dense (1)(x) 9 model = keras.Model(inputs , outputs) 10 11 12 model.compile(optimizer=keras. optimizers .Adam (), 13 loss=keras.losses. BinaryCrossentropy ( from_logits = True), 14 metrics =[ keras.metrics. BinaryAccuracy ()]) 15 model.fit(new_dataset , epochs =20, callbacks =... , validation_data =...) Listing 9.2 Pretraining in Keras (2): create new model and train In multi-task learning, related learning tasks are learned at the same time, whereas in transfer learning they are learned in sequence by diferent networks. Multi-task learning improves regularization by requiring the algorithm to perform well on a related learning task instead of penalizing all overftting uniformly [233, 26]. The two-headed AlphaGo Zero network optimizes for value and for policy at the same time in the same network [581, 129]. A multi-headed architecture is often used in multi-task learning, although in AlphaGo Zero the two heads are trained for two related aspects (policy and value) of the same task (playing Go games). Multi-task learning has been applied with success to Atari games [405, 404]. 9.2.1.5 Domain Adaptation Domain adaptation is necessary when there is a change in the data distribution between the training dataset and the test dataset (domain shift). This problem is related to out-of-distribution learning [470]. Domain shifts are common in practical applications of artifcial intelligence, such as when items must be recognized in diferent light conditions, or when the background changes. Conventional machinelearning algorithms often have difculty adapting to such changes. The goal of domain adaptation is to compensate for the variation among two data distributions, to be able to reuse information from a source domain on a diferent target domain [782], see Fig. 9.3 for a backpack in diferent circumstances. As was indicated in Table 9.1, domain adaptation applies to the situation where the tasks are the same \ud835\udc471 = \ud835\udc472 but the datasets are diferent \ud835\udc371 \u2260 \ud835\udc372, although still somewhat similar. For example, the task may be to recognize a backpack, but in a diferent orientation, or to recognize a pedestrian, but in diferent lighting. 252 9 Meta-Learning Fig. 9.3 Domain Adaptation: Recognizing Items in Diferent Circumstances is Difcult [301] Domain adaptation can be seen as the opposite of pretraining. Pretraining uses the same dataset for a diferent task, while domain adaptation adapts to a new dataset for the same task [127]. In natural language processing, examples of domain shift are an algorithm that has been trained on news items that is then applied to a dataset of biomedical documents [179, 733], or a spam flter that is trained on a certain group of email users, which is deployed to a new target user [74]. Sudden changes in the environment (pandemics, severe weather) can also upset machine learning algorithms. There are diferent techniques to overcome domain shift [168, 872, 856, 834]. In visual applications, adaptation can be achieved by re-weighting the samples of the frst dataset, or clustering them for visually coherent sub-domains. Other approaches try to fnd transformations that map the source distribution to the target, or learn a classifcation model and a feature transformation jointly [782]. Adversarial techniques where feature representations are encouraged to be difcult to distinguish can be used to achieve adaptation [792, 850, 211], see also Sect. B.2.6. 9.2.2 Meta-Learning Related to transfer learning is meta-learning. Where the focus in transfer learning is on transferring parameters from a single donor task to a receiver task for further fnetuning, in meta-learning the focus is on using the knowledge of a number of tasks to learn how to learn a new task faster and better. Regular machine learning 9.2 Transfer Learning and Meta-Learning Agents 253 learns examples for one task, meta-learning aims to learn across tasks. Machine learning learns parameters that approximate the function, meta-learning learns hyperparameters about the learning-function.4 This is often expressed as learning to learn, a phrase that has defned the feld ever since its introduction [669, 772, 774]. The term meta-learning has been used for many diferent contexts, not only for deep learning, but also for tasks ranging from hyperparameter optimization, algorithm selection, to automated machine learning. (We will briefy look at these in Sect. 9.2.2.5.) Deep meta reinforcement learning is an active area of research. Many algorithms are being developed, and much progress is being made. Table 9.2 lists nine algorithms that have been proposed for deep meta reinforcement learning (see the surveys [351, 364]). 9.2.2.1 Evaluating Few-Shot Learning Problems One of the challenges of lifelong machine learning is to judge the performance of an algorithm. Regular train-test generalization does not capture the speed of adaptation of a meta-learning algorithm. For this reason, meta-learning tasks are typically evaluated on their few-shot learning ability. In few-shot learning, we test if a learning algorithm can be made to recognize examples from classes from which it has seen only few examples in training. In few-shot learning prior knowledge is available in the network. To translate few-shot learning to a human setting, we can think of a situation where a human plays the double bass after only a few minutes of training on the double bass, but after years on the violin, viola, or cello. Meta-learning algorithms are often evaluated with few shot learning tasks, in which the algorithm must recognize items of which it has only seen a few examples. This is formalized in the \ud835\udc41-way-\ud835\udc58-shot approach [141, 445, 829]. Figure 9.4 illustrates this process. Given a large dataset D, a smaller training dataset \ud835\udc37 is sampled from this dataset. The \ud835\udc41-way-\ud835\udc58-shot classifcation problem constructs training dataset \ud835\udc37 such that it consists of \ud835\udc41 classes, of which, for each class, \ud835\udc58 examples are present in the dataset. Thus, the cardinality of |\ud835\udc37| = \ud835\udc41 \u00b7 \ud835\udc58. A full \ud835\udc41-way-\ud835\udc58-shot few-shot learning meta task T consists of many episodes in which base tasks T\ud835\udc56 are performed. A base task consists of a training set and a test set, to test generalization. In few-shot terminology, the training set is called the support set, and the test set is called the query set. The support set has size \ud835\udc41 \u00b7 \ud835\udc58, the query set consists of a small number of examples. The meta-learning algorithm can learn from the episodes of \ud835\udc41-way-\ud835\udc58-shot query/support base tasks, until at meta-test time the generalization of the meta-learning algorithm is tested with another query, as is illustrated in the fgure. 4 Associating base learning with parameter learning, and meta-learning wth hyperparameter learning appears to give us a clear distinction; however, in practice the distinction is not so clear cut: in deep meta-learning the initialization of the regular parameters is considered to be an important \u201chyper\u201dparameter. 254 9 Meta-Learning Fig. 9.4 \ud835\udc41 -way-\ud835\udc58-shot learning [98] Name Approach Environment Ref Recurr. ML Deploy recurrent networks on RL problems [213, 826] Meta Netw. Fast reparam. of base-learner by distinct meta-learner O-glot, miniIm. [542] SNAIL Attention mechanism coupled with temporal conv. O-glot, miniIm. [517] LSTM ML Embed base-learner parameters in cell state of LSTM miniImageNet [629] MAML Learn initialization weights \ud835\udf03 for fast adaptation O-glot, miniIm. [243] iMAML Approx. higher-order gradients, indep. of optim. path O-glot, miniIm. [623] Meta SGD Learn both the initialization and updates O-glot, miniIm. [476] Reptile Move init. towards task-specifc updated weights O-glot, miniIm. [561] BayesMAML Learn multiple initializations \u0398, jointly optim. SVGD miniImagaNet [861] Table 9.2 Meta Reinforcement Learning Approaches [364] 9.2.2.2 Deep Meta-Learning Algorithms We will now turn our attention to deep meta-learning algorithms. The meta-learning feld is still young and active, nevertheless, the feld is converging on a set of defnitions that we will present here. We start our explanation in a supervised setting. Meta-learning is concerned with learning a task T from a set of base-learning tasks {T1, T2, T3, . . . } so that a new (related) meta-test-task will reach a high accuracy quicker. Each base-learning task T\ud835\udc56 consists of a dataset \ud835\udc37\ud835\udc56 and a learning objective, the loss function L\ud835\udc56. Thus we get T\ud835\udc56 = (\ud835\udc37\ud835\udc56, L\ud835\udc56). Each dataset consists of pairs of inputs and labels \ud835\udc37\ud835\udc56 = {(\ud835\udc65 \ud835\udc57, \ud835\udc66 \ud835\udc57)}, and is split into a training and a testset \ud835\udc37\ud835\udc56 = {\ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b, \ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61}. On each training dataset a parameterized model \u02c6\ud835\udc53\ud835\udf03\ud835\udc56 (\ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) is approximated, with a loss function L\ud835\udc56(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b). The model \u02c6\ud835\udc53 is approximated with a deep learning algorithm, that is governed by a set of hyperparameters \ud835\udf14. The particular hyperparameters vary from algorithm to algorithm, 9.2 Transfer Learning and Meta-Learning Agents 255 but frequently encountered hyperparameters are the learning rate \ud835\udefc, the initial parameters \ud835\udf030, and algorithm constants. This conventional machine learning algorithm is called the base learner. Each base learner task approximates a model \u02c6\ud835\udc53\ud835\udc56 by fnding the optimal parameters \ud835\udf03\u2605 \ud835\udc56 to minimize the loss function on its data set T\ud835\udc56 = \u02c6\ud835\udc53\ud835\udf03\u2605 \ud835\udc56 = arg min \ud835\udf03\ud835\udc56 L\ud835\udc56,\ud835\udf14(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) while the learning algorithm is governed by hyperparameters \ud835\udf14. Inner and Outer Loop Optimization One of the most popular deep meta-learning approaches of the last few years is optimization-based meta-learning [364]. This approach optimizes the initial parameters \ud835\udf03 of the network for fast learning of new tasks. Most optimization-based techniques do so by approaching meta-learning as a two-level optimization problem. At the inner level, a base learner makes task-specifc updates to \ud835\udf03 for the diferent observations in the training set. At the outer level, the meta-learner optimizes hyperparameters \ud835\udf14 across a sequence of base tasks where the loss of each task is evaluated using the test data from the base tasks \ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 [629, 351, 463]. The inner loop optimizes the parameters \ud835\udf03, and the outer loop optimizes the hyperparameters \ud835\udf14 to fnd the best performance on the set of base tasks \ud835\udc56 = 0, . . . , \ud835\udc40 with the appropriate test data: \ud835\udf14\u2605 = arg min \ud835\udf14 Lmeta \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd outer loop (arg min \ud835\udf03\ud835\udc56 Lbase \ud835\udf14 (\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd inner loop , \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61). The inner loop optimizes \ud835\udf03\ud835\udc56 within the datasets \ud835\udc37\ud835\udc56 of the tasks T\ud835\udc56 , and the outer loop optimizes \ud835\udf14 across the tasks and datasets. The meta loss function optimizes for the meta objective, which can be accuracy, speed, or another goal over the set of base tasks (and datasets). The outcome of the meta optimization is a set of optimal hyperparameters \ud835\udf14\u2605. In optimization-based meta-learning the most important hyperparameters \ud835\udf14 are the optimal initial parameters \ud835\udf03\u2605 0. When the meta-learner only optimizes the initial parameters as hyperparameters (\ud835\udf14 = \ud835\udf030) then the inner/outer formula simplifes as follows: \ud835\udf03\u2605 0 = arg min \ud835\udf030 L \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd outer loop (arg min \ud835\udf03\ud835\udc56 L(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd inner loop , \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61). In this approach we meta-optimize the initial parameters \ud835\udf030, such that the loss function performs well on the test data of the base tasks. Section 9.2.2.4 describes MAML, a well-known example of this approach. 256 9 Meta-Learning Fig. 9.5 Workfow of recurrent meta-learners in reinforcement learning contexts. State, action, reward, and termination fag at time step \ud835\udc61 are denotednby \ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, and \ud835\udc51\ud835\udc61, \u210e\ud835\udc61 refers to the hidden state [213]. Deep meta-learning approaches are sometimes categorized as (1) similaritymetric-based, (2) model-based, and (3) optimization-based [364]. We will now have a closer look at two of the nine meta reinforcement learning algorithms from Table 9.2. We will look at Recurrent meta-learning and MAML; the former is a model-based approach, the latter optimization-based. 9.2.2.3 Recurrent Meta-Learning For meta reinforcement learning approaches to be able to learn to learn, they must be able to remember what they have learned across subtasks. Let us see how Recurrent meta-learning learns across tasks. Recurrent meta-learning uses recurrent neural networks to remember this knowledge [213, 826]. The recurrent network serves as dynamic storage for the learned task embedding (weight vector). The recurrence can be implemented by an LSTM [826] or by gated recurrent units [213]. The choice of recurrent neural meta-network (meta-RNN) determines how well it adapts to the subtasks, as it gradually accumulates knowledge about the base-task structure. Recurrent meta-learning tracks variables \ud835\udc60, \ud835\udc4e, \ud835\udc5f, \ud835\udc51 which denote state, action, reward, and termination of the episode. For each task T\ud835\udc56, Recurrent meta-learning inputs the set of environment variables {\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc51\ud835\udc61} into a meta-RNN at each time step \ud835\udc61. The meta-RNN outputs an action and a hidden state \u210e\ud835\udc61. Conditioned on the hidden state \u210e\ud835\udc61, the meta network outputs action \ud835\udc4e\ud835\udc61. The goal is to maximize the expected reward in each trial (Fig. 9.5). Since Recurrent meta-learning embeds information from previously seen inputs in hidden state, it is regarded as a modelbased meta-learner [364]. Recurrent meta-learners performed almost as well as model-free baselines on simple \ud835\udc41-way-\ud835\udc58-shot reinforcement learning tasks [826, 213]. However, the performance degrades in more complex problems, when dependencies span a longer horizon. 9.2 Transfer Learning and Meta-Learning Agents 257 Fig. 9.6 The Optimization approach aims to learn parameters from which other tasks can be learned quickly. The intuition behind Optimization approaches such as MAML is that when our meta-training set consists of tasks A, B, C, and D, then if our meta-learning algorithm adjusts parameters \ud835\udc4e and \ud835\udc4f to (2, 2), then they are close to either of the four tasks, and can be adjustly quickly to them, with few examples (after [364, 243]). 9.2.2.4 Model-Agnostic Meta-Learning The Model-agnostic meta-learning approach (MAML) [243] is an optimization approach that is model-agnostic: it can be used for diferent learning problems, such as classifcation, regression, and reinforcement learning. As mentioned, the optimization view of meta-learning is especially focused on optimizing the initial parameters \ud835\udf03. The intuition behind the optimization view can be illustrated with a simple regression example (Fig. 9.6). Let us assume that we are faced with multiple linear regression problems \ud835\udc53\ud835\udc56(\ud835\udc65). The model has two parameters: \ud835\udc4e and \ud835\udc4f, \u02c6\ud835\udc53 (\ud835\udc65) = \ud835\udc4e \u00b7 \ud835\udc65 + \ud835\udc4f. When the meta-training set consists of four tasks, A, B, C, and D, then we wish to optimize to a single set of parameters {\ud835\udc4e, \ud835\udc4f} from which we can quickly learn the optimal parameters for each of the four tasks. In Fig. 9.6 the point in the middle represents this combination of parameters. The point is the closest to the four diferent tasks. This is how Model-agnostic meta-learning works [243]: by exposing our model to various base tasks, we update the parameters \ud835\udf03 = {\ud835\udc4e, \ud835\udc4f} to good initial parameters \ud835\udf030 that facilitate quick meta-adaptation. Let us look at the process of training a deep learning model\u2019s parameters from a feature learning standpoint [243, 364], where the goal is that a few gradient steps can produce good results on a new task. We build a feature representation that is broadly suitable for many tasks, and by then fne-tuning the parameters slightly (primarily updating the top layer weights) we achieve good results\u2014not unlike transfer learning. MAML fnds parameters \ud835\udf03 that are easy and fast to fnetune, allowing the adaptation to happen in an embedding space that is well suited for 258 9 Meta-Learning Algorithm 9.1 MAML for Reinforcement Learning [243] Require: \ud835\udc5d(T): distribution over tasks Require: \ud835\udefc, \ud835\udefd: step size hyperparameters randomly initialize \ud835\udf03 while not done do Sample batch of tasks T\ud835\udc56 \u223c \ud835\udc5d(T) for all T\ud835\udc56 do Sample \ud835\udc58 trajectories D = {(\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ) } using \ud835\udc53\ud835\udf03 in T\ud835\udc56 Evaluate \u2207\ud835\udf03 LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03) using D and LT\ud835\udc56 in Equation 9.1 Compute adapted parameters with gradient descent: \ud835\udf03\u2032 \ud835\udc56 = \ud835\udf03 \u2212 \ud835\udefc\u2207\ud835\udf03 LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03) Sample trajectories D\u2032 \ud835\udc56 = {(\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ) } using \ud835\udc53\ud835\udf03\u2032 \ud835\udc56 in T\ud835\udc56 end for Update \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udefd\u2207\ud835\udf03 \ufffd T\ud835\udc56\u223c\ud835\udc5d(T) LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03\u2032 \ud835\udc56) using each D\u2032 \ud835\udc56 and LT\ud835\udc56 in Eq. 9.1 end while fast learning. To put it another way, MAML\u2019s goal is to fnd the point in the middle of Fig. 9.6, from where the other tasks are easily reachable. How does MAML work [363, 620, 243]? Please refer to the pseudocode for MAML in Alg. 9.1. The learning task is an episodic Markov decision process with horizon \ud835\udc47, where the learner is allowed to query a limited number of sample trajectories for few-shot learning. Each reinforcement learning task T\ud835\udc56 contains an initial state distribution \ud835\udc5d\ud835\udc56(\ud835\udc601) and a transition distribution \ud835\udc5d\ud835\udc56(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). The loss LT\ud835\udc56 corresponds to the (negative) reward function \ud835\udc45. The model being learned, \ud835\udf0b\ud835\udf03, is a policy from states \ud835\udc60\ud835\udc61 to a distribution over actions \ud835\udc4e\ud835\udc61 at each timestep \ud835\udc61 \u2208 {1, ...,\ud835\udc47}. The loss for task T\ud835\udc56 and policy \ud835\udf0b\ud835\udf03 takes the familiar form of the objective (Eq. 2.5): LT\ud835\udc56 (\ud835\udf0b\ud835\udf03) = \u2212E\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u223c\ud835\udf0b\ud835\udf03 ,\ud835\udc5dT\ud835\udc56 \ufffd \ud835\udc47 \u2211\ufe01 \ud835\udc61=1 \ud835\udc45\ud835\udc56(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61+1) \ufffd . (9.1) In \ud835\udc58-shot reinforcement learning, \ud835\udc58 rollouts from \ud835\udf0b\ud835\udf03 and task T\ud835\udc56, (\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ), and the rewards \ud835\udc45(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61), may be used for adaptation on a new task T\ud835\udc56. MAML uses TRPO to estimate the gradient both for the policy gradient update(s) and the meta optimization [681]. The goal is to quickly learn new concepts, which is equivalent to achieving a minimal loss in few gradient update steps. The number of gradient steps has to be specifed in advance. For a single gradient update step gradient descent produces updated parameters \ud835\udf03\u2032 \ud835\udc56 = \ud835\udf03 \u2212 \ud835\udefc\u2207\ud835\udf03LT\ud835\udc56 (\ud835\udf0b\ud835\udf03) specifc to task \ud835\udc56. The meta loss of one gradient step across tasks is \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udefd\u2207\ud835\udf03 \u2211\ufe01 T\ud835\udc56\u223c\ud835\udc5d(T) LT\ud835\udc56 (\ud835\udf0b\ud835\udf03\u2032 \ud835\udc56) (9.2) 9.2 Transfer Learning and Meta-Learning Agents 259 where \ud835\udc5d(T) is a probability distribution over tasks. This expression contains an inner gradient \u2207\ud835\udf3dLT\ud835\udc56 (\ud835\udf0b\ud835\udf03\u2032 \ud835\udc56). Optimizing this meta loss requires computing secondorder gradients when backpropagating the meta gradient through the gradient operator in the meta objective (Eq. 9.2), which is computationally expensive [243]. Various algorithms have been inspired by MAML and aim to improve optimizationbased meta-learning further [561, 363]. In meta reinforcement learning the goal is to quickly fnd a policy for a new environment using only a small amount of experience. MAML has gained attention within the feld of deep meta-learning, due to its simplicity (it requires two hyperparameters), its general applicability, and its strong performance. 9.2.2.5 Hyperparameter Optimization Meta-learning has been around for a long time, long before deep learning became popular. It has been applied to classic machine learning tasks, such as regression, decision trees, support vector machines, clustering algorithms, Bayesian networks, evolutionary algorithms, and local search [93, 106, 812]. The hyperparameter view on meta-learning originated here. Although this is a book about deep learning, it is interesting to briefy discuss this non-deep background, also because hyperparameter optimization is an important technology to fnd a good set of hyperparameters in reinforcement learning experiments. Machine learning algorithms have hyperparameters that govern their behavior, and fnding the optimal setting for these hyperparameters has long been called meta-learning. A naive approach is to enumerate all combinations and run the machine learning problem for them. For all but the smallest hyperparameter spaces such a grid search will be prohibitively slow. Among the smarter meta-optimization approaches are random search, Bayesian optimization, gradient-based optimization, and evolutionary optimization. This meta-algorithm approach has given rise to algorithm confguration research, such as SMAC [366],5 ParamILS [367],6 irace [488],7 and algorithm selection research [630, 407], such as SATzilla [854].8 Well-known hyperparameter optimization packages include scikit-learn [595],9 scikit-optimize,10 nevergrad [627],11 and optuna [11].12 Hyperparameter optimization and algorithm confguration have 5 https://github.com/automl/SMAC3 6 http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/ 7 http://iridia.ulb.ac.be/irace/ 8 http://www.cs.ubc.ca/labs/beta/Projects/SATzilla/ 9 https://scikit-learn.org/stable/ 10 https://scikit-optimize.github.io/stable/index.html 11 https://code.fb.com/ai-research/nevergrad/ 12 https://optuna.org 260 9 Meta-Learning grown into the feld of automated machine learning, or AutoML.13 The AutoML feld is a large and active feld of meta-learning, an overview book is [368]. All machine learning algorithms have a bias, diferent algorithms perform better on diferent types of problems. Hyperparameters constrain this algorithm bias. This so-called inductive bias refects the set of assumptions about the data on which the algorithms are based. Learning algorithms perform better when this bias matches the learning problem (for example: CNNs work well on image problems, RNNs on language problems). Meta-learning changes this inductive bias, either by choosing a diferent learning algorithm, by changing the network initialization, or by other means, allowing the algorithm to be adjusted to work well on diferent problems. 9.2.2.6 Meta-Learning and Curriculum Learning There is in interesting connection between meta-learning and curriculum learning. Both approaches aim to improve the speed and accuracy of learning, by learning from a set of subtasks. In meta-learning, knowledge is gained from subtasks, so that the learning of a new, related, task, can be quick. In curriculum learning (Sect. 6.2.3.1) we aim to learn quicker by dividing a large and difcult learning task into a set of subtasks, ordered from easy to hard. Thus we can conclude that curriculum learning is a form of meta-learning where the subtasks are ordered from easy to hard, or, equivalently, that meta-learning is unordered curriculum learning. 9.2.2.7 From Few-Shot to Zero-Shot Learning Meta-learning uses information from previous learning tasks to learn new tasks quicker [774]. Meta-learning algorithms are often evaluated in a few-shot setting, to see how well they do in image classifcation problems when they are shown only few training examples. This few-shot learning problem aims to correctly classify queries with little previous support for the new class. In the previous sections we have discussed how meta-learning algorithms aim to achieve few-shot learning. A discussion of meta-learning would not be complete without mentioning zero-shot learning. Zero-shot learning (ZSL) goes a step further than few-shot learning. In zero-shot learning an example has to be recognized as belonging to a class without ever having been trained on an example of this class [454, 580]. In zero-shot learning the classes covered by training instances and the classes we aim to classify are disjoint. This may sound like an impossibility\u2014how can you recognize something you have never seen before?\u2014yet it is something that we, humans, do all the time: having learned to pour cofee in a cup, we can also pour tea, even if we never have seen 13 https://www.automl.org 9.3 Meta-Learning Environments 261 tea before. (Or, having learned to play the violin, the viola, and the cello, we can play the double bass, to some degree, even if we have never played the double bass before.) Whenever we recognize something that we have not seen before, we are actually using extra information (or features). If we recognize a red beak in a picture of a bird-species that we have never seen before, then the concepts \u201cred\u201d and \u201cbeak\u201d are known to us, because we have learned them in other contexts. Zero-shot learning recognizes new categories of instances without training examples. Attribute-based zero-shot learning uses separate high-level attribute descriptions of the new categories, based on categories previously learned in the dataset. Attributes are an intermediate representation that enables parameter sharing between classes [10]. The extra information can be in the form of textual description of the class\u2014red, or beak\u2014in addition to the visual information [449]. The learner must be able to match text with image information. Zero-shot learning approaches are designed to learn this intermediate semantic layer, the attributes, and apply them at inference time to predict new classes, when they are provided with descriptions in terms of these attributes. Attributes correspond to high-level properties of the objects which are shared across multiple classes (which can be detected by machines and which can be understood by humans). Attribute-based image classifcation is a label-embedding problem where each class is embedded in the space of attribute vectors. As an example, if the classes correspond to animals, possible attributes include has paws, has stripes or is black. 9.3 Meta-Learning Environments Now that we have seen how transfer learning and meta-learning can be implemented, it is time to look at some of the environments that are used to evaluate the algorithms. We will list important datasets, environments, and foundation models for images, behavior, and text, expanding our scope beyond pure reinforcement learning. We will look at how well the approaches succeed in generalizing quickly to new machine learning tasks. Many benchmarks have been introduced to test transfer and meta-learning algorithms. Benchmarks for conventional machine learning algorithms aim to ofer a variety of challenging learning tasks. Benchmarks for meta-learning, in contrast, aim to ofer related learning tasks. Some benchmarks are parameterized, where the diference between tasks can be controlled. Meta-learning aims to learn new and related tasks quicker, trading of speed versus accuracy. This raises the question how fast and how accurate meta-learning algorithms are under diferent circumstances. In answering these questions we must keep in mind that the closer the learning tasks are, the easier the task is, and the quicker and the more accurate results will be. Hence, we should carefully look at which dataset a benchmark uses when we compare results. 262 9 Meta-Learning Name Type Domain Ref ALE single games [71] MuJoCo single robot [780] DeepMind Control single robot [755] BERT transfer text [193] GPT-3 transfer text [617] ImageNet transfer image [237] Omniglot meta image [445] Mini-ImageNet meta image [814] Meta-Dataset meta image [785] Meta-World meta robot [864] Alchemy meta unity [825] Table 9.3 Datasets, Environments, and Models for Meta-Learning in Images, Behavior, and Text Table 9.3 lists some of the environments that are often used for meta-learning experiments. Some are regular deep learning environments designed for single-task learning (\u201csingle\u201d), some are transfer learning and pretraining datasets (\u201ctransfer\u201d), and some datasets and environments are specifcally designed for meta-learning experiments (\u201cmeta\u201d). We will now describe them in more detail. ALE (Sect. 3.1.1), MuJoCo (Sect. 4.1.3) and the DeepMind control suite (Sect. 4.3.2) are originally single task deep learning environments. They are also being used in meta-learning experiments and few-shot learning, often with moderate results, since the tasks are typically not very similar (Pong is not like Pac-Man). 9.3.1 Image Processing Traditionally, two datasets have emerged as de facto benchmarks for few-shot image learning: Omniglot [446], and mini-ImageNet [646, 814]. Omniglot is a dataset for one-shot learning. This dataset contains 1623 diferent handwritten characters from 50 diferent alphabets and contains 20 examples per class (character) [446, 447]. Most recent methods obtain very high accuracy on Omniglot, rendering comparisons between them mostly uninformative. Mini-ImageNet uses the same setup as Omniglot for testing, consisting of 60,000 colour images of size 84 \u00d7 84 with 100 classes (64/16/20 for train/validation/test) and contains 600 examples per class [814]. Albeit harder than Omniglot, most recent methods achieve similar accuracy when controlling for model capacity. Meta-learning algorithms such as Bayesian Program Learning and MAML achieved accuracies comparable to human performance on Omniglot and ImageNet, with accuracies in the high nineties and error rates as low as a few percent [447]. Models 9.3 Meta-Learning Environments 263 trained on the largest datasets, such as ImageNet, are used as foundation models [96]. Pre-trained models can be downloaded from the Model zoo.14 These benchmarks may be too homogeneous for testing meta-learning. In contrast, real-life learning experiences are heterogeneous: they vary in terms of the number of classes and examples per class, and are unbalanced. Furthermore, the Omniglot and Mini-ImageNet benchmarks measure within-dataset generalization. For meta-learning, we are eventually after models that can generalize to entirely new distributions. For this reason, new datasets are being developed specifcally for meta-learning. 9.3.2 Natural Language Processing In natural language processing BERT is a well-known pretrained model. BERT stands for Bidirectional encoder representations from transformers [193]. It is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on two contexts. BERT has shown that transfer learning can work well in natural language tasks. BERT can be used for classifcation tasks such as sentiment analysis, question answering tasks, and named entity recognition. BERT is a large model, with 345 million parameters [639]. An even larger pretrained transformer model is the Generative pretrained transformer 3, or GPT-3 [617], with 175 billion parameters. The quality of the text generated by GPT-3 is exceptionally good, it is difcult to distinguish from that written by humans. In this case, it appears that size matters. OpenAI provides a public interface where you can see for yourself how well it performs.15 BERT and GPT-3 are large models, that are used more and more as a foundation model as a basis for other experiments to pretrain on. 9.3.3 Meta-Dataset A recent dataset specifcally designed for meta-learning is Meta-dataset [785].16 Meta-dataset is a set of datasets, consisting of: Imagenet, Omniglot, Aircraft, Birds, Textures, Quick Draw, Fungi, Flower, Trafc Signs, and MSCOCO. Thus, the datasets provide a more heterogeneous challenge than earlier single-dataset experiments. Triantafllou et al. report results with Matching networks [814], Prototypical networks [717], frst-order MAML [243], and Relation networks [737]. As can be expected, the accuracy on the larger (meta) dataset is much lower than on previous homogeneous datasets. They fnd that a variant of MAML performs best, although for classifers that are trained on other datasets most accuracies are between 40% 14 https://modelzoo.co 15 https://openai.com/blog/openai-api/ 16 https://github.com/google-research/meta-dataset 264 9 Meta-Learning Fig. 9.7 Multi Task and Meta Reinforcement Learning [864] Fig. 9.8 Meta-World Tasks [864] and 60%, except for Birds and Flowers, which scores in the 70s and 80s, closer to the single dataset results for Omniglot and Imagenet. Meta-learning for heterogeneous datasets remains a challenging task (see also [141, 775]). 9.3.4 Meta-World For deep reinforcement learning two traditionally popular environments are ALE and MuJoCo. The games in the ALE benchmarks typically difer considerably, which makes the ALE test set challenging for meta-learning, and little success has been reported. (There are a few exceptions that apply transfer learning (pretraining) to DQN [586, 520, 718] succeeding in multitask learning in a set of Atari games that all move a ball.) Robotics tasks, on the other hand, are more easily parameterizable. Test tasks can be generated with the desired level of similarity, making robotic tasks amenable to meta-learning testing. Typical tasks such as reaching and learning diferent walking 9.3 Meta-Learning Environments 265 gaits, are more related than two Atari games such as, for example, Breakout and Space Invaders. To provide a better benchmark that is more challenging for meta reinforcement learning, Yu et al. introduced Meta-World [864],17 a benchmark for multi-task and meta reinforcement learning (see Fig. 9.7 for their pictorial explanation of the diference between multi-task and meta-learning). Meta-World consists of 50 distinct manipulation tasks with a robotic arm (Fig. 9.8). The tasks are designed to be diferent, and contain structure, which can be leveraged for transfer to new tasks. When the authors of Meta-World evaluated six state-of-the-art meta-reinforcement and multi-task learning algorithms on these tasks, they found that generalization of existing algorithms to heterogeneous tasks is limited. They tried PPO, TRPO, SAC, RL2[213], MAML, and PEARL [624]. Small variations of tasks such as diferent object positions can be learned with reasonable success, but the algorithms struggled to learn multiple tasks at the same time, even with as few as ten distinct training tasks. In contrast to more limited meta-learning benchmarks, Meta-World emphasizes generalization to new tasks and interaction scenarios, not just a parametric variation in goals. 9.3.5 Alchemy A fnal meta reinforcement learning benchmark that we will discuss is Alchemy [825]. The Alchemy benchmark is a procedurally generated 3D video game [692], implemented in Unity [383]. Task generation is parameterized, and varying degrees of similarity and hidden structure can be chosen. The process by which Alchemy levels are created is accessible to researchers, and a perfect Bayesian ideal observer can be implemented. Experiments with two agents are reported, VMPO and IMPALA. The VMPO [722, 587] agent is based on a gated transformer network. The IMPALA [230, 374] agent is based on population-based training with an LSTM core network. Both agents are strong deep learning methods, although not necessarily for meta-learning. Again, in both agents meta-learning became more difcult as learning tasks became more diverse. The reported performance on meta-learning was weak. The Alchemy platform can be found on GitHub.18 A human-playable interface is part of the environment. 17 https://meta-world.github.io 18 https://github.com/deepmind/dm_alchemy ",
    "Conclusion": "",
    "Experiments": "Experiments The main goal of model-based reinforcement learning is to learn the transition model accurately\u2014not just the policy function that fnds the best action, but the function that fnds the next state. By learning the full essence of the environment a substantial reduction of sample complexity can be achieved. Also, the hope is that the model allows us to solve new classes of problems. In this section we will try to answer the question if these approaches have succeeded. The answer to this question can be measured in training time and in run-time performance. For performance, most benchmark domains provide easily measurable quantities, such as the score in an Atari game. For model-based approaches, the scores achieved by state-of-the-art model-free algorithms such as DQN, DDPG, PPO, SAC and A3C are a useful baseline. For training time, the reduction in sample complexity is an obvious choice. However, many model-based approaches use a fxed hyperparameter to determine the relation between external environment samples and internal model samples (such as 1 : 1000). Then the number of time steps needed for high performance to be reached becomes an important measure, and this is indeed published by most authors. For model-free methods, we often see time steps in the millions per training run, and sometimes even billions. With so many time steps it becomes quite important how much processing each time step takes. For model-based methods, individual time steps may take longer than for model-free, since more processing for learning and planning has to be performed. In the end, wall-clock time is important, and this is also often published. There are two additional questions. First we are interested in knowing whether a model-based approach allows new types of problems to be solved, that could not be solved by model-free methods. Second is the question of brittleness. In many experiments the numerical results are quite sensitive to diferent settings of hyperparameters (including the random seeds). This is the case in many model-free 138 5 Model-Based Reinforcement Learning Name Learning Planning Environment Ref PILCO Uncertainty Trajectory Pendulum [188] iLQG Uncertainty MPC Small [756] GPS Uncertainty Trajectory Small [468] SVG Uncertainty Trajectory Small [326] VIN CNN e2e Mazes [748] VProp CNN e2e Mazes [551] Planning CNN/LSTM e2e Mazes [298] TreeQN Latent e2e Mazes [236] I2A Latent e2e Mazes [615] Predictron Latent e2e Mazes [710] World Model Latent e2e Car Racing [304] Local Model Uncertainty Trajectory MuJoCo [296] Visual Foresight Video Prediction MPC Manipulation [244] PETS Ensemble MPC MuJoCo [149] MVE Ensemble Trajectory MuJoCo [238] Meta Policy Ensemble Trajectory MuJoCo [156] Policy Optim Ensemble Trajectory MuJoCo [376] PlaNet Latent MPC MuJoCo [309] Dreamer Latent Trajectory MuJoCo [308] Plan2Explore Latent Trajectory MuJoCo [688] L3P Latent Trajectory MuJoCo [873] Video-prediction Latent Trajectory Atari [563] VPN Latent Trajectory Atari [564] SimPLe Latent Trajectory Atari [391] Dreamer-v2 Latent Trajectory Atari [310] MuZero Latent e2e/MCTS Atari/Go [679] Table 5.2 Model-Based Reinforcement Learning Approaches [601] and model-based results [328]. However, when the transition model is accurate, the variance may diminish, and some model-based approaches might be more robust. Table 5.2 lists 26 experiments with model-based methods [601]. In addition to the name, the table provides an indication of the type of model learning that the agent uses, of the type of planning, and of the application environment in which it was used. The categories in the table are described in the previous section, where e2e means end-to-end. In the table, the approaches are grouped by environment. At the top are smaller applications such as mazes and navigation tasks. In the middle are larger MuJoCo tasks. At the bottom are high-dimensional Atari tasks. Let us look in more depth at the three groups of environments: small navigation, robotics, and Atari games. 5.3 High-Dimensional Environments 139 5.3.2 Small Navigation Tasks We see that a few approaches use smaller 2D Grid world navigation tasks such as mazes, or block puzzles, such as Sokoban, and Pacman. Grid world tasks are some of the oldest problems in reinforcement learning, and they are used frequently to test out new ideas. Tabular imagination approaches such as Dyna, and some latent model and end-to-end learning and planning, have been evaluated with these environments. They typically achieve good results, since the problems are of moderate complexity. Grid world navigation problems are quintessential sequential decision problems. Navigation problems are typically low-dimensional, and no visual recognition is involved; transition functions are easy to learn. Navigation tasks are also used for latent model and end-to-end learning. Three latent model approaches in Table 5.2 use navigation problems. I2A deals with model imperfections by introducing a latent model, based on Chiappa et al. and Buesing et al. [144, 121]. I2A is applied to Sokoban and Mini-Pacman by [615, 121]. Performance compares favorably to model-free learning and to planning algorithms (MCTS). Value iteration networks introduced the concept of end-to-end diferentiable learning and planning [748, 562], after [403, 671, 369]. Through backpropagation the model learns to perform value iteration. The aim to learn a general model that can navigate in unseen environments was achieved, although diferent extensions were needed for more complex environments. 5.3.3 Robotic Applications Next, we look at papers that use MuJoCo to model continuous robotic problems. Robotic problems are high-dimensional problems with continuous action spaces. MuJoCo is used by most experiments in this category to simulate the physical behavior of robotic movement and the environment. Uncertainty modeling with ensembles and MPC re-planning try to reduce or contain inaccuracies. The combination of ensemble methods with MPC is well suited for robotic problems, as we have seen in individual approaches such as PILCO and PETS. Robotic applications are more complex than Grid worlds; model-free methods can take many time steps to fnd good policies. It is important to know if modelbased methods succeed in reducing sample complexity in these problems. When we have a closer look at how well uncertainty modeling and MPC succeed at achieving our frst goal, we fnd a mixed picture. A benchmark study by Wang et al. [828] looked into the performance of ensemble methods and Model-predictive control on MuJoCo tasks. It fnds that these methods mostly fnd good policies, and do so in signifcantly fewer time steps than modelfree methods, typcially in 200k time steps versus 1 million for model-free. So, it would appear that the lower sample complexity is achieved. However, they also 140 5 Model-Based Reinforcement Learning note that per time step, the more complex model-based methods perform more processing than the simpler model-free methods. Although the sample complexity may be lower, the wall-clock time is not, and model-free methods such as PPO ans SAC are still much faster for many problems. Furthermore, the score that the policy achieves varies greatly for diferent problems, and is sensitive to diferent hyperparameter values. Another fnding is that in some experiments with a large number of time steps, the performance of model-based methods plateaus well below model-free performance, and the performance of the model-based methods themselves difers substantially. There is a need for further research in deep model-based methods, especially into robustness of results. More benchmarking studies are needed that compare diferent methods. 5.3.4 Atari Games Applications Some experiments in Table 5.2 use the Arcade learning environment (ALE). ALE features high-dimensional inputs, and provides one of the most challenging environments of the table. Especially latent models choose Atari games to showcase their performance, and some do indeed achieve impressive results, in that they are able to solve new problems, such as playing all 57 Atari games well (Dreamer-v2) [310] and learning the rules of Atari and chess (MuZero) [679]. Hafner et al. published the papers Dream to control: learning behaviors by latent imagination, and Dreamer v2 [308, 310]. Their work extends the work on VPN and PlaNet with more advanced latent models and reinforcement learning methods [564, 309]. Dreamer uses an actor-critic approach to learn behaviors that consider rewards beyond the horizon. Values are backpropagated through the value model, similar to DDPG [480] and Soft actor critic [306]. An important advantage of model-based reinforcement learning is that it can generalize to unseen environments with similar dynamics [688]. The Dreamer experiments showed that latent models are indeed more robust to unseen environments than model-free methods. Dreamer is tested with applications from the DeepMind control suite (Sect. 4.3.2). Value prediction networks are another latent approach. They outperform modelfree DQN on mazes and Atari games such as Seaquest, QBert, Krull, and Crazy Climber. Taking the development of end-to-end learner/planners such as VPN and Predictron further is the work on MuZero [679, 290, 359]. In MuZero a new architecture is used to learn the transition functions for a range of diferent games, from Atari to the board games chess, shogi and Go. MuZero learns the transition model for all games from interaction with the environment.11 The MuZero model includes diferent modules: a representation, dynamics, and prediction function. Like 11 This is a somewhat unusual usage of board games. Most researchers use board games because the transition function is given (see next chapter). MuZero instead does not know the rules of chess, but starts from scratch learning the rules from interaction with the environment. 5.3 High-Dimensional Environments 141 AlphaZero, MuZero uses a refned version of MCTS for planning (see Sect. 6.2.1.2 in the next chapter). This MCTS planner is used in a self-play training loop for policy improvement. MuZero\u2019s achievements are impressive: it is able to learn the rules of Atari games as well as board games, learning to play the games from scratch, in conjunction with learning the rules of the games. The MuZero achievements have created follow up work to provide more insight into the relationship between actual and latent representations, and to reduce the computational demands [290, 186, 39, 334, 18, 680, 289, 860]. Latent models reduce observational dimensionality to a smaller model to perform planning in latent space. End-to-end learning and planning is able to learn new problems\u2014the second of our two goals: it is able to learn to generalize navigation tasks, and to learn the rules of chess and Atari. These are new problems, that are out of reach for model-free methods (although the sample complexity of MuZero is quite large). Conclusion In deep model-based reinforcement learning benchmarks drive progress. We have seen good results with ensembles and local re-planning in continuous problems, and with latent models in discrete problems. In some applications, both the frst goal, of better sample complexity, and the other goals, of learning new applications and reducing brittleness, are achieved. The experiments used many diferent environments within the ALE and MuJoCo suites, from hard to harder. In the next two chapters we will study multi-agent problems, where we encounter a new set of benchmarks, with a state space of many combinations, including hidden information and simultaneous actions. These provide even more complex challenges for deep reinforcement learning methods. 5.3.5 Hands On: PlaNet Example Before we go to the next chaper, let us take a closer look at how one of these methods achieves efcient learning of a complex high-dimensional task. We will look at PlaNet, a well-documented project by Hafner et al. [309]. Code is available,12 scripts are available, videos are available, and a blog is available13 inviting us to take the experiments further. The name of the work is Learning latent dynamics from pixels, which describes what the algorithm does: use high dimensional visual input, convert it to latent space, and plan in latent space to learn robot locomotion dynamics. 12 https://github.com/google-research/planet 13 https://planetrl.github.io 142 5 Model-Based Reinforcement Learning Fig. 5.6 Locomotion tasks of PlaNet [309] PlaNet solves continuous control tasks that include contact dynamics, partial observability, and sparse rewards. The applications used in the PlaNet experiments are: (a) Cartpole (b) Reacher (c) Cheetah (d) Finger (e) Cup and (f) Walker (see Fig. 5.6). The Cartpole task is a swing-up task, with a fxed viewpoint. The cart can be out of sight, requiring the agent to remember information from previous frames. The Finger spin task requires predicting the location of two separate objects and their interactions. The Cheetah task involves learning to run. It includes contacts of the feet with the ground that requires a model to predict multiple futures. The Cup task must catch a ball in a cup. It provides a sparse reward signal once the ball is caught, requiring accurate predictions far into the future. The Walker task involves a simulated robot that begins lying on the ground, and must learn to stand up and then walk. PlaNet performs well on these tasks. On DeepMind control tasks it achieves higher accuracy than an A3C or an D4PG agent. It reportedly does so using 5000% fewer interactions with the environment on average. It is instructive to experiment with PlaNet. The code can be found on GitHub.14 Scripts are available to run the experiments with simple one line commands: python3 -m planet.scripts.train --logdir /path/to/logdir --params \u2019{tasks: [cheetah_run]}\u2019 As usual, this does require having the right versions of the right libraries installed, which may be a challenge and may require some creativity on your part. The required versions are listed on the GitHub page. The blog also contains videos and pictures of what to expect, including comparisons to model-free baselines from the DeepMind Control Suite (A3C, D4PG). The experiments show the viability of the idea to use rewards and values to compress actual states into lower dimensional latent states, and then plan with these latent states. Value-based compression reduces details in the high-dimensional actual states as noise that is not relevant to improve the value function [290]. To help understand how the actual state map to the latent states, see, for example [186, 472, 401]. 14 https://github.com/google-research/planet 5.3 High-Dimensional Environments 143 Summary and Further Reading This has been a diverse chapter. We will summarize the chapter, and provide references for further reading. Summary Model-free methods sample the environment using the rewards to learn the policy function, providing actions for all states for an environment. Model-based methods use the rewards to learn the transition function, and then use planning methods to sample the policy from this internal model. Metaphorically speaking: modelfree learns how to act in the environment, model-based learns how to be the environment. The learned transition model acts as a multiplier on the amount of information that is used from each environment sample. A consequence is that model-based methods have a lower sample complexity, although, when the agent\u2019s transition model does not perfectly refect the environment\u2019s transition function, the performance of the policy may be worse than a model-free policy (since that always uses the environment to sample from). Another, and perhaps more important aspect of the model-based approach, is generalization. Model-based reinforcement learning builds a dynamics model of the domain. This model can be used multiple times, for new problem instances, but also for related problem classes. By learning the transition and reward model, model-based reinforcement learning may be better at capturing the essence of a domain than model-free methods, and thus be able to generalize to variations of the problem. Imagination showed how to learn a model and use it to fll in extra samples based on the model (not the environment). For problems where tabular methods work, imagination can be many times more efcient than model-free methods. When the agent has access to the transition model, it can apply reversible planning algorithms, in additon to one-way learning with samples. There is a large literature on backtracking and tree-traversal algorithms. Using a look-ahead of more than one step can increase the quality of the reward even more. When the problem size increases, or when we perform deep multi-step look-ahead, the accuracy of the model becomes critical. For high-dimensional problems high capacity networks are used that require many samples to prevent overftting. Thus a trade-of exists, to keep sample complexity low. Methods such as PETS aim to take the uncertainty of the model into account in order to increase modeling accuracy. Model-predictive control methods re-plan at each environment step to prevent over-reliance on the accuracy of the model. Classical tabular approaches and Gaussian Process approaches have been quite succesful in achieving low sample complexity for small problems [743, 190, 417]. Latent models observe that in many high-dimensional problems the factors that infuence changes in the value function are often lower-dimensional. For example, 144 5 Model-Based Reinforcement Learning the background scenery in an image may be irrelevant for the quality of play in a game, and has no efect on the value. Latent models use an encoder to translate the high-dimensional actual state space into a lower-dimensional latent state space. Subsequent planning and value functions work on the (much smaller) latent space. Finally, we considered end-to-end model-based algorithms. These fully diferentiable algorithms not only learn the dynamics model, but also learn the planning algorithm that uses the model. The work on Value iteration networks [748] inspired recent work on end-to-end learning, where both the transition model and the planning algorithm are learned, end-to-end. Combined with latent models (or World models [304]) impressive results were achieved [710], and the model and planning accuracy was improved to the extent that tabula rasa self-learning of game-rules was achieved, in Muzero [679] for both chess, shogi, Go, and Atari games. Further Reading Model-based reinforcement learning promises more sample efcient learning. The feld has a long history. For exact tabular methods Sutton\u2019s Dyna-Q is a classical approach that illustrates the basic concept of model-based learning [741, 742]. There is an extensive literature on the approaches that were discussed in this chapter. For uncertainty modeling see [188, 189, 756], and for ensembles [574, 149, 156, 376, 439, 391, 469, 468, 326, 244]. For Model-predictive control see [549, 262, 505, 426, 296, 238, 38]. Latent models is an active feld of research. Two of the earlier works are [563, 564], although the ideas go back to World models [304, 403, 671, 369]. Later, a sequence of PlaNet and Dreamer papers was infuential [309, 308, 688, 310, 874]. The literature on end-to-end learning and planning is also extensive, starting with VIN [748], see [22, 704, 706, 710, 551, 298, 236, 679, 239]. As applications became more challenging, notably in robotics, other methods were developed, mostly based on uncertainty, see for surveys [190, 417]. Later, as high-dimensional problems became prevalent, latent and end-to-end methods were developed. The basis for the section on environments is an overview of recent modelbased approaches [601]. Another survey is [529], a comprehensive benchmark study is [828]. Exercises Let us go to the Exercises. 5.3 High-Dimensional Environments 145 Questions Below are frst some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. 1. What is the advantage of model-based over model-free methods? 2. Why may the sample complexity of model-based methods sufer in highdimensional problems? 3. Which functions are part of the dynamics model? 4. Mention four deep model-based approaches. 5. Do model-based methods achieve better sample complexity than model-free? 6. Do model-based methods achieve better performance than model-free? 7. In Dyna-Q the policy is updated by two mechanisms: learning by sampling the environment and what other mechanism? 8. Why is the variance of ensemble methods lower than of the individual machine learning approaches that are used in the ensemble? 9. What does model-predictive control do and why is this approach suited for models with lower accuracy? 10. What is the advantage of planning with latent models over planning with actual models? 11. How are latent models trained? 12. Mention four typical modules that constitute the latent model. 13. What is the advantage of end-to-end planning and learning? 14. Mention two end-to-end planning and learning methods. Exercises It is now time to introduce a few programming exercises. The main purpose of the exercises is to become more familiar with the methods that we have covered in this chapter. By playing around with the algorithms and trying out diferent hyperparameter settings you will develop some intuition for the efect on performance and run time of the diferent methods. The experiments may become computationally expensive. You may want to consider running them in the cloud, with Google Colab, Amazon AWS, or Microsoft Azure. They may have student discounts, and they will have the latest GPUs or TPUs for use with TensorFlow or PyTorch. 1. Dyna Implement tabular Dyna-Q for the Gym Taxi environment. Vary the amount of planning \ud835\udc41 and see how performance is infuenced. 2. Keras Make a function approximation version of Dyna-Q and Taxi, with Keras. Vary the capacity of the network and the amount of planning. Compare against a pure model-free version, and note the diference in performance for diferent tasks and in computational demands. 146 5 Model-Based Reinforcement Learning 3. Planning In Dyna-Q, planning has so far been with single step model samples. Implement a simple depth-limited multi-step look-ahead planner, and see how performance is infuenced for the diferent look-ahead depths. 4. MPC Read the paper by Nagabandi et al. [549] and download the code.15 Acquire the right versions of the libraries, and run the code with the supplied scripts, just for the MB (model-based) versions. Note that plotting is also supported by the scripts. Run with diferent MPC horizons. Run with diferent ensemble sizes. What are the efects on performance and run time for the diferent applications? 5. PlaNet Go to the PlaNet blog and read it (see previous section).16 Go to the PlaNet GitHub site and download and install the code.17 Install the DeepMind control suite,18 and all necessary versions of the support libraries. Run Reacher and Walker in PlaNet, and compare against the model-free methods D4PG and A3C. Vary the size of the encoding network and note the efect on performance and run time. Now turn of the encoder, and run with planning on actual states (you may have to change network sizes to achieve this). Vary the capacity of the latent model, and of the value and reward functions. Also vary the amount of planning, and note its efect. 6. End-to-end As you have seen, these experiments are computationally expensive. We will now turn to end-to-end planning and learning (VIN and MuZero). This exercise is also computationally expensive. Use small applications, such as small mazes, and Cartpole. Find and download a MuZero implementation from GitHub and explore using the experience that you have gained from the previous exercises. Focus on gaining insight into the shape of the latent space. Try MuZero-General [215],19 or a MuZero visualization [186] to get insight into latent space.20 (This is a challenging exercise, suitable for a term project or thesis.) 15 https://github.com/anagabandi/nn_dynamics 16 https://planetrl.github.io 17 https://github.com/google-research/planet 18 https://github.com/deepmind/dm_control 19 https://github.com/werner-duvaud/muzero-general 20 https://github.com/kaesve/muzero Chapter 6 Two-Agent Self-Play Previous chapters were concerned with how a single agent can learn optimal behavior for its environment. This chapter is diferent. We turn to problems where two agents operate whose behavior will both be modeled (and, in the next chapter, more than two). Two-agent problems are interesting for two reasons. First, the world around us is full of active entities that interact, and modeling two agents and their interaction is a step closer to understanding the real world than modeling a single agent. Second, in two-agent problems exceptional results were achieved\u2014reinforcement learning agents teaching themselves to become stronger than human world champions\u2014and by studying these methods we may fnd a way to achieve similar results in other problems. The kind of interaction that we model in this chapter is zero-sum: my win is your loss and vice versa. These two-agent zero-sum dynamics are fundamentally diferent from single-agent dynamics. In single agent problems the environment lets you probe it, lets you learn how it works, and lets you fnd good actions. Although the environment may not be your friend, it is also not working against you. In two-agent zero-sum problems the environment does try to win from you, it actively changes its replies to minimize your reward, based on what it learns from your behavior. When learning our optimal policy we should take all possible counter-actions into account. A popular way to do so is to implement the environment\u2019s actions with self-play: we replace the environment by a copy of ourselves. In this way we let ourselves play against an opponent that has all the knowledge that we currently have, and agents learn from eachother. We start with a short review of two-agent problems, after which we dive into selflearning. We look at the situation when both agents know the transition function perfectly, so that model accuracy is no longer a problem. This is the case, for example, in games such as chess and Go, where the rules of the game determine how we can go from one state to another. In self-learning the environment is used to generate training examples for the agent to train a better policy, after which the better agent policy is used in this 147 148 6 Two-Agent Self-Play environment to train the agent, and again, and again, creating a virtuous cycle of self-learning and mutual improvement. It is possible for an agent to teach itself to play a game without any prior knowledge at all, so-called tabula rasa learning, learning from a blank slate. The self-play systems that we describe in this chapter use model-based methods, and combine planning and learning approaches. There is a planning algorithm that we have mentioned a few times, but have not yet explained in detail. In this chapter we will discuss Monte Carlo Tree Search, or MCTS, a highly popular planning algorithm. MCTS can be used in single agent and in two-agent situations, and is the core of many successful applications, including MuZero and the self-learning AlphaZero series of programs. We will explain how self-learning and self-play work in AlphaGo Zero, and why they work so well. We will then discuss the concept of curriculum learning, which is behind the success of self-learning. The chapter is concluded with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Self-play \u2022 Curriculum learning Core Problem \u2022 Use a given transition model for self-play, in order to become stronger than the current best players Core Algorithms \u2022 Minimax (Listing 6.2) \u2022 Monte Carlo Tree Search (Listing 6.3) \u2022 AlphaZero tabula rasa learning (Listing 6.1) Self-Play in Games We have seen in Chap. 5 that when the agent has a transition model of the environment, it can achieve greater performance, especially when the model has high accuracy. What if the accuracy of our model were perfect, if the agent\u2019s transition 6 Two-Agent Self-Play 149 Fig. 6.1 Backgammon and Tesauro function is the same as the environment\u2019s, how far would that bring us? And what if we could improve our environment as part of our learning process, can we then transcend our teacher, can the sorcerer\u2019s apprentice outsmart the wizard? To set the scene for this chapter, let us describe the frst game where this has happened: backgammon. Learning to Play Backgammon In Sect. 3.2.3 we briefy discussed research into backgammon. Already in the early 1990s, the program TD-Gammon achieved stable reinforcement learning with a shallow network. This work was started at the end of the 1980s by Gerald Tesauro, a researcher at IBM laboratories. Tesauro was faced with the problem of getting a program to learn beyond the capabilities of any existing entity. (In Fig. 6.1 we see Tesauro in front of his program; image by IBM Watson Media.) In the 1980s computing was diferent. Computers were slow, datasets were small, and neural networks were shallow. Against this background, the success of Tesauro is quite remarkable. His programs were based on neural networks that learned good patterns of play. His frst program, Neurogammon, was trained using supervised learning, based on games of human experts. In supervised learning the model cannot become stronger than the human games it is trained on. Neurogammon achieved an intermediate level of play [762]. His second program, TD-Gammon, was based on reinforcement learning, using temporal diference learning and self-play. Combined with handcrafted heuristics and some planning, in 1992 it played at human championship level, becoming the frst computer program to do so in a game of skill [765]. TD-Gammon is named after temporal diference learning because it updates its neural net after each move, reducing the diference between the evaluation of previous and current positions. The neural network used a single hidden layer with up to 80 units. TD-Gammon initially learned from a state of zero knowledge, tabula rasa. Tesauro describes TD-Gammon\u2019s self-play as follows: The move that is selected is the move with maximum expected outcome for the side making the move. In other 150 6 Two-Agent Self-Play words, the neural network is learning from the results of playing against itself. This self-play training paradigm is used even at the start of learning, when the network\u2019s weights are random, and hence its initial strategy is a random strategy [764]. TD-Gammon performed tabula rasa learning, its neural network weights initiliazed to small random numbers. It reached world-champion level purely by playing against itself, learning the game as it played along. Such autonomous self-learning is one of the main goals of artifcial intelligence. TD-Gammon\u2019s success inspired many researchers to try neural networks and selfplay approaches, culminating eventually, many years later, in high-profle results in Atari [522] and AlphaGo [703, 706], which we will describe in this chapter.1 In Sect. 6.1 two-agent zero-sum environments will be described. Next, in Sect. 6.2 the tabula rasa self-play method is described in detail. In Sect. 6.3 we focus on the achievements of the self-play methods. Let us now start with two-agent zero-sum problems. 6.1 Two-Agent Zero-Sum Problems Before we look into self-play algorithms, let us look for a moment at the two-agent games that have fascinated artifcial intelligence researchers for such a long time. Games come in many shapes and sizes. Some are easy, some are hard. The characteristics of games are described in a fairly standard taxonomy. Important characteristics of games are: the number of players, whether the game is zero-sum or non-zero-sum, whether it is perfect or imperfect information, what the complexity of taking decisions is, and what the state space complexity is. We will look at these characteristics in more detail. \u2022 Number of Players One of the most important elements of a game is the number of players. One-player games are normally called puzzles, and are modeled as a standard MDP. The goal of a puzzle is to fnd a solution. Two-player games are \u201creal\u201d games. Quite a number of two-player games exist that provide a nice balance between being too easy and being too hard for players (and for computer programmers) [172]. Examples of two-player games that are popular in AI are chess, checkers, Go, Othello, and shogi. Multi-player games are played by three or more players. Well-known examples of multiplayer games are the card games bridge and poker, and strategy games such as Risk, Diplomacy, and StarCraft. \u2022 Zero Sum versus Non Zero Sum An important aspect of a game is whether it is competitive or cooperative. Most two-player games are competitive: the win (+1) of player A is the loss (\u22121) of player B. These games are called zero sum because the sum of the wins for the players remains a constant zero. Competition is an 1 A modern reimplementation of TD-Gammon in TensorFlow is available on GitHub at TDGammon https://github.com/fomorians/td-gammon 6.1 Two-Agent Zero-Sum Problems 151 important element in the real world, and these games provide a useful model for the study of confict and strategic behavior. In contrast, in cooperative games the players win if they can fnd win/win situations. Examples of cooperative games are Hanabi, bridge, Diplomacy [429, 185], poker and Risk. The next chapter will discuss multi-agent and cooperative games. \u2022 Perfect versus Imperfect Information In perfect information games all relevant information is known to all players. This is the case in typical board games such as chess and checkers. In imperfect information games some information may be hidden from some players. This is the case in card games such as bridge and poker, where not all cards are known to all players. Imperfect information games can be modeled as partially observable Markov processes, POMDP [569, 693]. A special form of (im)perfect information games are games of chance, such as backgammon and Monopoly, in which dice play an important role. There is no hidden information in these games, and these games are sometimes considered to be perfect information games, despite the uncertainty present at move time. Stochasticity is not the same as imperfect information. \u2022 Decision Complexity The difculty of playing a game depends on the complexity of the game. The decision complexity is the number of end positions that defne the value (win, draw, or loss) of the initial game position (also known as the critical tree or proof tree [416]). The larger the number of actions in a position, the larger the decision complexity. Games with small board sizes such as tic tac toe (3 \u00d7 3) have a smaller complexity than games with larger boards, such as gomoku (19 \u00d7 19). When the action space is very large, it can often be treated as a continuous action space. In poker, for example, the monetary bets can be of any size, defning an action size that is practically continuous. \u2022 State Space Complexity The state space complexity of a game is the number of legal positions reachable from the initial position of a game. State space and decision complexity are normally positively correlated, since games with high decision complexity typically have high state space complexity. Determining the exact state space complexity of a game is a nontrivial task, since positions may be illegal or unreachable.2 For many games approximations of the state space have been calculated. In general, games with a larger state space complexity are harder to play (\u201crequire more intelligence\u201d) for humans and computers. Note that the dimensionality of the states may not correlate with the size of the state space, for example, the rules of some of the simpler Atari games limit the number of reachable states, although the states themselves are high-dimensional (they consist of many video pixels). 2 For example, the maximal state space of tic tac toe is 39 = 19683 positions (9 squares of \u2019X\u2019, \u2019O\u2019, or blank), where only 765 positions remain if we remove symmetrical and illegal positions [661]. 152 6 Two-Agent Self-Play Name board state space zero-sum information Chess 8 \u00d7 8 1047 zero-sum perfect Checkers 8 \u00d7 8 1018 zero-sum perfect Othello 8 \u00d7 8 1028 zero-sum perfect Backgammon 24 1020 zero-sum chance Go 19 \u00d7 19 10170 zero-sum perfect Shogi 9 \u00d7 9 1071 zero-sum perfect Poker card 10161 non-zero imperfect Table 6.1 Characteristics of games Fig. 6.2 Deep Blue and Garry Kasparov in May 1997 in New York Zero-Sum Perfect-Information Games Two-person zero-sum games of perfect information, such as chess, checkers, and Go, are among the oldest applications of artifcial intelligence. Turing and Shannon published the frst ideas on how to write a program to play chess more than 70 years ago [788, 694]. To study strategic reasoning in artifcial intelligence, these games are frequently used. Strategies, or policies, determine the outcome. Table 6.1 summarizes some of the games that have played an important role in artifcial intelligence research. 6.1.1 The Difculty of Playing Go After the 1997 defeat of chess world champion Garry Kasparov by IBM\u2019s Deep Blue computer (Fig. 6.2; image by Chessbase), the game of Go (Fig. 1.4) became the next 6.1 Two-Agent Zero-Sum Problems 153 benchmark game, the Drosophila3 of AI, and research activity in Go intensifed signifcantly. The game of Go is more difcult than chess. It is played on a larger board (19 \u00d7 19 vs. 8 \u00d7 8), the action space is larger (around 250 moves available in a position versus some 25 in chess), the game takes longer (typically 300 moves versus 70) and the state space complexity is much larger: 10170 for Go, versus 1047 for chess. Furthermore, rewards in Go are sparse. Only at the end of a long game, after many moves have been played, is the outcome (win/loss) known. Captures are not so frequent in Go, and no good efciently computable heuristic has been found. In chess, in contrast, the material balance in chess can be calculated efciently, and gives a good indication of how far ahead we are. For the computer, much of the playing in Go happens in the dark. In contrast, for humans, it can be argued that the visual patterns of Go may be somewhat easier to interpret than the deep combinatorial lines of chess. For reinforcement learning, credit assignment in Go is challenging. Rewards only occur after a long sequence of moves, and it is unclear which moves contributed the most to such an outcome, or whether all moves contributed equally. Many games will have to be played to acquire enough outcomes. In conclusion, Go is more difcult to master with a computer than chess. Traditionally, computer Go programs followed the conventional chess design of a minimax search with a heuristic evaluation function, that, in the case of Go, was based on the infuence of stones (see Sect. 6.2.1 and Fig. 6.4) [515]. This chess approach, however, did not work for Go, or at least not well enough. The level of play was stuck at mid-amateur level for many years. The main problems were the large branching factor, and the absence of an efcient and good evaluation function. Subsequently, Monte Carlo Tree Search was developed, in 2006. MCTS is a variable depth adaptive search algorithm, that did not need a heuristic function, but instead used random playouts to estimate board strength. MCTS programs caused the level of play to improve from 10 kyu to 2-3 dan, and even stronger on the small 9 \u00d7 9 board.4 However, again, at that point, performance stagnated, and researchers expected that world champion level play was still many years into the future. Neural networks had been tried, but were slow, and did not improve performance much. Playing Strength in Go Let us compare the three programming paradigms of a few diferent Go programs that have been written over the years (Fig. 6.3). The programs fall into three categories. First are the programs that use heuristic planning, the minimax-style 3 Drosophila Melanogaster is also known as the fruitfy, a favorite species of genetics researchers to test their theories, because experiments produce quick and clear answers. 4 Absolute beginners in Go start at 30 kyu, progressing to 10 kyu, and advancing to 1 kyu (30k\u20131k). Stronger amateur players then achieve 1 dan, progressing to 7 dan, the highest amateur rating for Go (1d\u20137d). Professional Go players have a rating from 1 dan to 9 dan, written as 1p\u20139p. 154 6 Two-Agent Self-Play Fig. 6.3 Go Playing Strength of Top Programs over the Years [20] programs. GNU Go is a well-known example of this group of programs. The heuristics in these programs are hand-coded. The level of play of these programs was at medium amateur level. Next come the MCTS-based programs. They reached strong amateur level. Finally come the AlphaGo programs, in which MCTS is combined with deep self-play. These reached super-human performance. The fgure also shows other programs that follow a related approach. Thus, Go provided a large and sparse state space, providing a highly challenging test, to see how far self-play with a perfect transition function can come. Let us have a closer look at the achievements of AlphaGo. 6.1.2 AlphaGo Achievements In 2016, after decades of research, the efort in Go paid of. In the years 2015\u20132017 the DeepMind AlphaGo team played three matches in which it beat all human champions that it played, Fan Hui, Lee Sedol, and Ke Jie. The breakthrough performance of AlphaGo came as a surprise. Experts in computer games had expected grandmaster level play to be at least ten years away. The techniques used in AlphaGo are the result of many years of research, and cover a wide range of topics. The game of Go worked very well as Drosophila. Important new algorithms were developed, most notably Monte Carlo Tree Search (MCTS), as well as major progress was made in deep reinforcement learning. We will provide a high-level overview of the research that culminated in AlphaGo (that 6.1 Two-Agent Zero-Sum Problems 155 Fig. 6.4 Infuence in the game of Go. Empty intersections are marked as being part of Black\u2019s or White\u2019s Territory Fig. 6.5 AlphaGo versus Lee Sedol in 2016 in Seoul beat the champions), and its successor, AlphaGo Zero (that learns Go tabula rasa). First we will describe the Go matches. The games against Fan Hui were played in October 2015 in London as part of the development efort of AlphaGo. Fan Hui is the 2013, 2014, and 2015 European Go Champion, then rated at 2p dan. The games against Lee Sedol were played in May 2016 in Seoul, and were widely covered by the media (see Fig. 6.5; image by 156 6 Two-Agent Self-Play Fig. 6.6 AlphaGo on the Cover of Nature DeepMind). Although there is no ofcial worldwide ranking in international Go, in 2016 Lee Sedol was widely considered one of the four best players in the world. A year later another match was played, this time in China, against the Chinese champion Ke Jie, who was ranked number one in the Korean, Japanese, and Chinese ranking systems at the time of the match. All three matches were won convincingly by AlphaGo. Beating the best Go players appeared on the cover of the journal Nature, see Fig. 6.6. The AlphaGo series of programs actually consists of three programs: AlphaGo, AlphaGo Zero, and AlphaZero. AlphaGo is the program that beat the human Go champions. It consists of a combination of supervised learning from grandmaster games and from self-play games. The second program, AlphaGo Zero, is a full re-design, based solely on self-play. It performs tabula rasa learning of Go, and plays stronger than AlphaGo. AlphaZero is a generalization of this program that also plays chess and shogi. Section 6.3 will describe the programs in more detail. Let us now have an in-depth look at the self-play algorithms as featured in AlphaGo Zero and AlphaZero. 6.2 Tabula Rasa Self-Play Agents 157 Agent2 Agent1 \ud835\udc5f\u2032 1 \ud835\udc60\u2032 1 \ud835\udc4e2 \ud835\udc5f\u2032 2 \ud835\udc60\u2032 2 \ud835\udc4e1 Fig. 6.7 Agent-Agent World 6.2 Tabula Rasa Self-Play Agents Model-based reinforcement learning showed us that by learning a local transition model, good sample efciency can be achieved when the accuracy of the model is sufcient. When we have perfect knowledge of the transitions, as we have in this chapter, then we can plan far into the future, without error. In regular agent-environment reinforcement learning the complexity of the environment does not change as the agent learns, and as a consequence, the intelligence of the agent\u2019s policy is limited by the complexity of the environment. However, in self-play a cycle of mutual improvement occurs; the intelligence of the environment improves because the agent is learning. With self-play, we can create a system that can transcend the original environment, and keep growing, and growing, in a virtuous cycle of mutual learning. Intelligence emerging out of nothing. This is the kind of system that is needed when we wish beat the best known entity in a certain domain, since copying from a teacher will not help us to transcend it. Studying how such a high level of play is achieved is interesting, for three reasons: (1) it is exciting to follow an AI success story, (2) it is interesting to see which techniques were used and how it is possible to achieve beyond-human intelligence, and (3) it is interesting to see if we can learn a few techniques that can be used in other domains, beyond two-agent zero-sum games, to see if we can achieve super-intelligence there as well. Let us have a closer look at the self-learning agent architecture that is used by AlphaGo Zero. We will see that two-agent self-play actually consists of three levels of self-play: move-level self-play, example-level self-play, and tournament-level self-play. First, we will discuss the general architecture, and how it creates a cycle of virtuous improvement. Next, we will describe the levels in detail. 158 6 Two-Agent Self-Play Transition Rules Opponent playing Policy/Value learning/planning acting Fig. 6.8 Playing with Known Transition Rules Cycle of Virtuous Improvement In contrast to the agent/environment model, we now have two agents (Fig. 6.7). In comparison with the model-based world of Chap. 5 (Fig. 6.8) our learned model has been replaced by perfect knowledge of the transition rules, and the environment is now called opponent: the negative version of the same agent playing the role of agent2. The goal in this chapter is to reach the highest possible performance in terms of level of play, without using any hand-coded domain knowledge. In applications such as chess and Go a perfect transition model is present. Together with a learned reward function and a learned policy function, we can create a self-learning system in which a virtuous cycle of ever improving performance occurs. Figure 6.9 illustrates such a system: (1) the searcher uses the evaluation network to estimate reward values and policy actions, and the search results are used in games against the opponent in self-play, (2) the game results are then collected in a bufer, which is used to train the evaluation network in self-learning, and (3) by playing a tournament against a copy of ourselves a virtuous cycle of ever-increasing function improvement is created. AlphaGo Zero Self-Play in Detail Let us look in more detail at how self-learning works in AlphaGo Zero. AlphaGo Zero uses a model-based actor critic approach with a planner that improves a single value/policy network. For policy improvement it uses MCTS, for learning a single deep residual network with a policy head and a value head (Sect. B.2.6), see Fig. 6.9. MCTS improves the quality of the training examples in each iteration (left panel), and the net is trained with these better examples, improving its quality (right panel). The output of MCTS is used to train the evaluation network, whose output is then used as evalution function in that same MCTS. A loop is wrapped around the search-eval functions to keep training the network with the game results, creating a learning curriculum. Let us put these ideas into pseudocode. 6.2 Tabula Rasa Self-Play Agents 159 search eval train game examples new net reward opponent games net0 net1 net2 net3 . . . MCTS MCTS MCTS MCTS Fig. 6.9 Self-play loop improving quality of net 1 for tourn in range (1, max_tourns ): # curric. of tournaments 2 for game in range (1, max_games): # play a tourn. of games 3 trim(triples) # if buffer full: replace old entries 4 while not game_over (): # generate the states of one game 5 move = mcts(board , eval(net)) # move is (s,a) pair 6 game_pairs += move 7 make_move_and_switch_side (board , move) 8 triples += add(game_pairs , game_outcome ( game_pairs )) # add to buf 9 net = train(net , triples) # retrain with (s,a,outc) triples Listing 6.1 Self-play pseudocode The Cycle in Pseudocode Conceptually self-play is as ingenious as it is elegant: a double training loop around an MCTS player with a neural network as evaluation and policy function that help MCTS. Figure 6.10 and Listing 6.1 show the self-play loop in detail. The numbers in the fgure correspond to the line numbers in the pseudocode. Let us perform an outside-in walk-through of this system. Line 1 is the main self-play loop. It controls how long the execution of the curriculum of self-play tournaments will continue. Line 2 executes the training episodes, the tournaments of self-play games after which the network is retrained. Line 4 plays such a game to create (state, action) pairs for each move, and the outcome of the game. Line 5 calls MCTS to generate an action in each state. MCTS performs the simulations where it uses the policy head of the net in P-UCT selection, and the value head of the net at the MCTS leaves. Line 6 appends the state/action pair to the list of game 160 6 Two-Agent Self-Play 5 game_pairs \u2190 mcts 5 pol/val \u2190 eval(net(state)) 9 net \u2190 train(net, triples) 8 triples 1 tourn: iterate with new net policy/value state 2/4 game_pairs Fig. 6.10 A diagram of self-play with line-numbers moves. Line 7 performs the move on the board, and switches color to the other player, for the next move in the while loop. At line 8 a full game has ended, and the outcome is known. Line 8 adds the outcome of each game to the (state, action)-pairs, to make the (state, action, outcome)-triples for the network to train on. Note that since the network is a two-headed policy/value net, both an action and an outcome are needed for network training. On the last line this triples-bufer is then used to train the network. The newly trained network is used in the next self-play iteration as the evaluation function by the searcher. With this net another tournament is played, using the searcher\u2019s look-ahead to generate a next batch of higher-quality examples, resulting in a sequence of stronger and stronger networks (Fig. 6.9 right panel). In the pseudocode we see the three self-play loops where the principle of playing against a copy of yourself is used: 1. Move-level: in the MCTS playouts, our opponent actually is a copy of ourselves (line 5)\u2014hence, self-play at the level of game moves 2. Example-level: the input for self-training the approximator for the policy and the reward functions is generated by our own games (line 2)\u2014hence, self-play at the level of the value/policy network. 3. Tournament-level: the self-play loop creates a training curriculum that starts tabula rasa and ends at world champion level. The system trains at the level of the player against itself (line 1)\u2014hence, self-play, of the third kind. All three of these levels use their own kind of self-play, of which we will describe the details in the following sections. We start with move-level self-play. 6.2.1 Move-Level Self-Play At the innermost level, we use the agent to play against itself, as its own opponent. Whenever it is my opponent\u2019s turn to move, I play its move, trying to fnd the best move for my opponent (which will be the worst possible move for me). This scheme uses the same knowledge for player and opponent. This is diferent from the real world, where the agents are diferent, with diferent brains, diferent reasoning 6.2 Tabula Rasa Self-Play Agents 161 eval search value action/state Fig. 6.11 Search-Eval Architecture of Games skills, and diferent experience. Our scheme is symmetrical: when we assume that our agent plays a strong game, then the opponent is also assumed to play strongly, and we can hope to learn from the strong counter play. (We thus assume that our agent plays with the same knowledge as we have; we are not trying to consciously exploit opponent weaknesses.)5 6.2.1.1 Minimax This principle of generating the counter play by playing yourself while switching perspectives has been used since the start of artifcial intelligence. It is known as minimax. The games of chess, checkers and Go are challenging games. The architecture that has been used to program chess and checkers players has been the same since the earliest paper designs of Turing [788]: a search routine based on minimax which searches to a certain depth, and an evaluation function to estimate the score of board positions using heuristic rules of thumb when this depth is reached. In chess and checkers, for example, the number of pieces on the board of a player is a crude but efective approximation of the strength of a state for that player. Figure 6.11 shows a diagram of this classic search-eval architecture.6 Based on this principle many successful search algorithms have been developed, of which alpha-beta is the best known [416, 593]. Since the size of the state space is exponential in the depth of lookahead, however, many enhancements had to be developed to manage the size of the state space and to allow deep lookahead to occur [600]. The word minimax is a contraction of maximizing/minimizing (and then reversed for easy pronunciation). It means that in zero-sum games the two players alternate making moves, and that on even moves, when player A is to choose a move, the 5 There is also research into opponent modeling, where we try to exploit our opponent\u2019s weaknesses [320, 91, 259]. Here, we assume an identical opponent, which often works best in chess and Go. 6 Because the agent knows the transition function \ud835\udc47 , it can calculate the new state \ud835\udc60\u2032 for each action \ud835\udc4e. The reward \ud835\udc5f is calculated at terminal states, where it is equal to the value \ud835\udc63. Hence, in this diagram the search function provides the state to the eval function. See [788, 600] for an explanation of the search-eval architecture. 162 6 Two-Agent Self-Play 2 1 6 1 3 2 3 4 2 1 1 6 5 Fig. 6.12 Minimax tree best move is the one that maximizes the score for player A, while on odd moves the best move for player B is the move that minimizes the score for player A. Figure 6.12 depicts this situation in a tree. The score values in the nodes are chosen to show how minimax works. At the top is the root of the tree, level 0, a square node where player A is to move. Since we assume that all players rationally choose the best move, the value of the root node is determined by the value of the best move, the maximum of its children. Each child, at level 1, is a circle node where player B chooses its best move, in order to minimize the score for player A. The leaves of this tree, at level 2, are again max squares (even though there is no child to choose from anymore). Note how for each circle node the value is the minimum of its children, and for the square node, the value is the maximum of the tree circle nodes. Python pseudocode for a recursive minimax procedure is shown in Listing 6.2. Note the extra hyperparameter d. This is the search depth counting upwards from the leaves. At depth 0 are the leaves, where the heuristic evaluation function is called to score the board.7 Also note that the code for making moves on the board\u2014 transitioning actions into the new states\u2014is not shown in the listing. It is assumed to happen inside the children dictionary. We frivolously mix actions and states in these sections, since an action fully determines which state will follow. (At the end of this chapter, the exercises provide more detail about move making and unmaking.) AlphaGo Zero uses MCTS, a more advanced search algorithm than minimax, that we will discuss shortly. 7 The heuristic evaluation function is originally a linear combination of hand-crafted heuristic rules, such as material balance (which side has more pieces) or center control. At frst, the linear combinations (coefcients) were not only hand-coded, but also hand-tuned. Later they were trained by supervised learning [61, 613, 771, 253]. More recently, NNUE was introduced as a non-linear neural network to use as evaluation function in an alpha-beta framework [557]. 6.2 Tabula Rasa Self-Play Agents 163 1 INF = 99999 # a value assumed to be larger than eval ever returns 2 3 def minimax(n, d): 4 if d <= 0: 5 return heuristic_eval (n) 6 elif n[\u2019type \u2019] == \u2019MAX \u2019: 7 g = -INF 8 for c in n[\u2019children \u2019]: 9 g = max(g, minimax(c, d -1)) 10 elif n[\u2019type \u2019] == \u2019MIN \u2019: 11 g = INF 12 for c in n[\u2019children \u2019]: 13 g = min(g, minimax(c, d -1)) 14 return g 15 16 print(\"Minimax\u2423value:\u2423\", minimax(root , 2)) Listing 6.2 Minimax code [600] Fig. 6.13 Three Lines of Play [65] Beyond Heuristics Minimax-based procedures traverse the state space by recursively following all actions in each state that they visit [788]. Minimax works just like a standard depth-frst search procedure, such as we have been taught in our undergraduate algorithms and data structures courses. It is a straightforward, rigid, approach, that searches all branches of the node to the same search depth. To focus the search efort on promising parts of the tree, researchers have subsequently introduced many algorithmic enhancements, such as alpha-beta cutofs, iterative deepening, transposition tables, null windows, and null moves [416, 602, 422, 714, 203]. In the early 1990s experiments with a diferent approach started, based on random playouts of a single line of play [6, 102, 118] (Fig. 6.13 and 6.14). In Fig. 6.14 164 6 Two-Agent Self-Play Fig. 6.14 Searching a Tree versus Searching a Path this diferent approach is illustrated. We see a search of a single line of play versus a search of a full subtree. It turned out that averaging many such playouts could also be used to approximate the value of the root, in addition to the classic recursive tree search approach. In 2006, a tree version of this approach was introduced that proved successful in Go. This algorithms was called Monte Carlo Tree Search [164, 117]. Also in that year Kocsis and Szepesv\u00e1ri created a selection rule for the exploration/exploitation trade-of that performed well and converged to the minimax value [419]. Their rule is called UCT, for upper confdence bounds applied to trees. 6.2.1.2 Monte Carlo Tree Search Monte Carlo Tree Search has two main advantages over minimax and alpha-beta. First, MCTS is based on averaging single lines of play, instead of recursively traversing subtrees. The computational complexity of a path from the root to a leaf is polynomial in the search depth. The computational complexity of a tree is exponential in the search depth. Especially in applications with many actions per state it is much easier to manage the search time with an algorithm that expands one path at a time.8 Second, MCTS does not need a heuristic evaluation function. It plays out a line of play in the game from the root to an end position. In end-positions the score of the game, a win or a loss, is known. By averaging many of these playouts the value of the root is approximated. Minimax has to cope with an exponential search tree, which it cuts of after a certain search depth, at which point it uses the heuristic to estimate the scores at the leaves. There are, however, games where no efcient 8 Compare chess and Go: in chess the typical number of moves in a position is 25, for Go this number is 250. A chess-tree of depth 5 has 255 = 9765625 leaves. A Go-tree of depth 5 has 2505 = 976562500000 leaves. A depth-5 minimax search in Go would take prohibitively long; an MCTS search of 1000 expansions expands the same number of paths from root to leaf in both games. 6.2 Tabula Rasa Self-Play Agents 165 Fig. 6.15 Monte Carlo Tree Search [117] heuristic evaluation function can be found. In this case MCTS has a clear advantage, since it works without a heuristic score function. MCTS has proven to be successful in many diferent applications. Since its introduction in 2006 MCTS has transformed the feld of heuristic search. Let us see in more detail how it works. Monte Carlo Tree Search consists of four operations: select, expand, playout, and backpropagate (Fig. 6.15). The third operation (playout) is also called rollout, simulation, and sampling. Backpropagation is sometimes called backup. Select is the downward policy action trial part, backup is the upward error/learning part of the algorithm. We will discuss the operations in more detail in a short while. MCTS is a succesful planning-based reinforcement learning algorithm, with an advanced exploration/exploitation selection rule. MCTS starts from the initial state \ud835\udc600, using the transition function to generate successor states. In MCTS the state space is traversed iteratively, and the tree data structure is built in a step by step fashion, node by node, playout by playout. A typical size of an MCTS search is to do 1000\u201310,000 iterations. In MCTS each iteration starts at the root \ud835\udc600, traversing a path in the tree down to the leaves using a selection rule, expanding a new node, and performing a random playout. The result of the playout is then propagated back to the root. During the backpropagation, statistics at all internal nodes are updated. These statistics are then used in future iterations by the selection rule to go to the currently most interesting part of the tree. The statistics consist of two counters: the win count \ud835\udc64 and the visit count \ud835\udc63. During backpropagation, the visit count \ud835\udc63 at all nodes that are on the path back from the leaf to the root are incremented. When the result of the playout was a win, then the win count \ud835\udc64 of those nodes is also incremented. If the result was a loss, then the win count is left unchanged. The selection rule uses the win rate \ud835\udc64/\ud835\udc63 and the visit count \ud835\udc63 to decide whether to exploit high-win-rate parts of the tree or to explore low-visit-count parts. An 166 6 Two-Agent Self-Play 1 def monte_carlo_tree_search (root): 2 while resources_left (time , computational power): 3 leaf = select(root) # leaf = unvisited node 4 simulation_result = rollout(leaf) 5 backpropagate (leaf , simulation_result ) 6 return best_child(root) # or: child with highest visit count 7 8 def select(node): 9 while fully_expanded (node): 10 node = best_child (node) # traverse down path of best UCT nodes 11 return expand(node.children) or node # no children/node is terminal 12 13 def rollout(node): 14 while non_terminal (node): 15 node = rollout_policy (node) 16 return result(node) 17 18 def rollout_policy (node): 19 return pick_random (node.children) 20 21 def backpropagate (node , result): 22 if is_root(node) return 23 node.stats = update_stats (node , result) 24 backpropagate (node.parent) 25 26 def best_child(node , c_param =1.0): 27 choices_weights = [ 28 (c.q / c.n) + c_param * np.sqrt ((np.log(node.n) / c.n)) # UCT 29 for c in node.children 30 ] 31 return node.children[np.argmax( choices_weights )] Listing 6.3 MCTS pseudo-Python [117, 173] often-used selection rule is UCT (Sect. 6.2.1.2). It is this selection rule that governs the exploration/exploitation trade-of in MCTS. The Four MCTS Operations Let us look in more detail at the four operations. Please refer to Listing 6.3 and Fig. 6.15 [117]. As we see in the fgure and the listing, the main steps are repeated as long as there is time left. Per step, the activities are as follows. 1. Select In the selection step the tree is traversed from the root node down until a leaf of the MCTS search tree is reached where a new child is selected that is not part of the tree yet. At each internal state the selection rule is followed to 6.2 Tabula Rasa Self-Play Agents 167 determine which action to take and thus which state to go to next. The UCT rule works well in many applications [419]. The selections at these states are part of the policy \ud835\udf0b(\ud835\udc60) of actions of the state. 2. Expand Then, in the expansion step, a child is added to the tree. In most cases only one child is added. In some MCTS versions all successors of a leaf are added to the tree [117]. 3. Playout Subsequently, during the playout step random moves are played in a form of self-play until the end of the game is reached. (These nodes are not added to the MCTS tree, but their search result is, in the backpropagation step.) The reward \ud835\udc5f of this simulated game is +1 in case of a win for the frst player, 0 in case of a draw, and \u22121 in case of a win for the opponent.9 4. Backpropagation In the backpropagation step, reward \ud835\udc5f is propagated back upwards in the tree, through the nodes that were traversed down previously. Two counts are updated: the visit count, for all nodes, and the win count, depending on the reward value. Note that in a two-agent game, nodes in the MCTS tree alternate color. If white has won, then only white-to-play nodes are incremented; if black has won, then only the black-to-play nodes. MCTS is on-policy: the values that are backed up are those of the nodes that were selected. Pseudocode Many websites contain useful resources on MCTS, including example code (see Listing 6.3).10 The pseudocode in the listing is from an example program for game play. The MCTS algorithm can be coded in many diferent ways. For implementation details, see [173] and the comprehensive survey [117]. MCTS is a popular algorithm. An easy way to use it in Python is by installing it from a pip package (pip install mcts). Policies At the end of the search, after the predetermined iterations have been performed, or when time is up, MCTS returns the value and the action with the highest visit count. An alternative would be to return the action with the highest win rate. However, the visit count takes into account the win rate (through UCT) and the number of simulations on which it is based. A high win rate may be based on a low number of simulations, and can thus be high variance. High visit counts will be low variance. 9 Originally, playouts were random (the Monte Carlo part in the name of MCTS) following Br\u00fcgmann\u2019s [118] and Bouzy and Helmstetter\u2019s [102] original approach. In practice, most Go playing programs improve on the random playouts by using databases of small 3 \u00d7 3 patterns with best replies and other fast heuristics [269, 165, 137, 702, 170]. Small amounts of domain knowledge are used after all, albeit not in the form of a heuristic evaluation function. 10 https://int8.io/monte-carlo-tree-search-beginners-guide/ 168 6 Two-Agent Self-Play Due to selection rule, high visit count implies high win-rate with high confdence, while high win rate may be low confdence [117]. The action of this initial state \ud835\udc600 constitutes the deterministic policy \ud835\udf0b(\ud835\udc600). UCT Selection The adaptive exploration/exploitation behavior of MCTS is governed by the selection rule, for which often UCT is chosen. UCT is an adaptive exploration/exploitation rule that achieves high performance in many diferent domains. UCT was introduced in 2006 by Kocsis and Szepesv\u00e1ri [419]. The paper provides a theoretical guarantee of eventual convergence to the minimax value. The selection rule was named UCT, for upper confdence bounds for multi-armed bandits applied to trees. (Bandit theory was also mentioned in Sect. 2.2.4.3). The selection rule determines the way in which the current values of the children infuence which part of the tree will be explored more. The UCT formula is UCT(\ud835\udc4e) = \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e + \ud835\udc36\ud835\udc5d \u221a\ufe04 ln \ud835\udc5b \ud835\udc5b\ud835\udc4e (6.1) where \ud835\udc64\ud835\udc4e is the number of wins in child \ud835\udc4e, \ud835\udc5b\ud835\udc4e is the number of times child \ud835\udc4e has been visited, \ud835\udc5b is the number of times the parent node has been visited, and \ud835\udc36\ud835\udc5d \u2265 0 is a constant, the tunable exploration/exploitation hyperparameter. The frst term in the UCT equation, the win rate \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e , is the exploitation term. A child with a high win rate receives through this term an exploitation bonus. The second term \u221a\ufe03 ln \ud835\udc5b \ud835\udc5b\ud835\udc4e is for exploration. A child that has been visited infrequently has a higher exploration term. The level of exploration can be adjusted by the \ud835\udc36\ud835\udc5d constant. A low \ud835\udc36\ud835\udc5d does little exploration; a high \ud835\udc36\ud835\udc5d has more exploration. The selection rule then is to select the child with the highest UCT sum (the familiar arg max function of value-based methods). The UCT formula balances win rate \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e and \u201cnewness\u201d \u221a\ufe03 ln \ud835\udc5b \ud835\udc5b\ud835\udc4e in the selection of nodes to expand.11 Alternative selection rules have been proposed, such as Auer\u2019s UCB1 [30, 31, 32] and P-UCT [638, 504]. P-UCT We should note that the MCTS that is used in the AlphaGo Zero program is a little diferent. MCTS is used inside the training loop, as an integral part of the 11 The square-root term is a measure of the variance (uncertainty) of the action value. The use of the natural logarithm ensures that, since increases get smaller over time, old actions are selected less frequently. However, since logarithm values are unbounded, eventually all actions will be selected [743]. 6.2 Tabula Rasa Self-Play Agents 169 self-generation of training examples, to enhance the quality of the examples for every self-play iteration, using both value and policy inputs to guide the search. Also, in the AlphaGo Zero program MCTS backups rely fully on the value function approximator; no playout is performed anymore. The MC part in the name of MCTS, which stands for the Monte Carlo playouts, really has become a misnomer for this neural network-guided tree searcher. Furthermore, selection in self-play MCTS is diferent. UCT-based node selection now also uses the input from the policy head of the trained function approximators, in addition to the win rate and newness. What remains is that through the UCT mechanism MCTS can focus its search efort greedily on the part with the highest win rate, while at the same time balancing exploration of parts of the tree that are underexplored. The formula that is used to incorporate input from the policy head of the deep network is a variant of P-UCT [706, 530, 638, 504] (for predictor-UCT). Let us compare P-UCT with UCT. The P-UCT formula adds the policy head \ud835\udf0b(\ud835\udc4e|\ud835\udc60) to Eq. 6.1 P-UCT(\ud835\udc4e) = \ud835\udc64\ud835\udc4e \ud835\udc5b\ud835\udc4e + \ud835\udc36\ud835\udc5d\ud835\udf0b(\ud835\udc4e|\ud835\udc60) \u221a\ud835\udc5b 1 + \ud835\udc5b\ud835\udc4e . P-UCT adds the \ud835\udf0b(\ud835\udc4e|\ud835\udc60) term specifying the probability of the action \ud835\udc4e to the exploration part of the UCT formula.12 Exploration/Exploitation The search process of MCTS is guided by the statistics values in the tree. MCTS discovers during the search where the promising parts of the tree are. The tree expansion of MCTS is inherently variable-depth and variable-width (in contrast to minimax-based algorithms such as alpha-beta, which are inherently fxed-depth and fxed-width). In Fig. 6.16 we see a snapshot of the search tree of an actual MCTS optimization. Some parts of the tree are searched more deeply than others [808]. An important element of MCTS is the exploration/exploitation trade-of, that can be tuned with the \ud835\udc36\ud835\udc5d hyperparameter. The efectiveness of MCTS in diferent applications depends on the value of this hyperparameter. Typical initial choices for Go programs are \ud835\udc36\ud835\udc5d = 1 or \ud835\udc36\ud835\udc5d = 0.1 [117], although in AlphaGo we see highly explorative choices such as \ud835\udc36\ud835\udc5d = 5. In general, experience has learned that when compute power is low, \ud835\udc36\ud835\udc5d should be low, and when more compute power is available, more exploration (higher \ud835\udc36\ud835\udc5d) is advisable [117, 434]. 12 Note further the small diferences under the square root (no logarithm, and the 1 in the denominator) also change the UCT function profle somewhat, ensuring correct behavior at unvisited actions [530]. 170 6 Two-Agent Self-Play 0 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 21 20 22 23 22 20 21 23 20 21 24 20 21 22 25 20 21 22 23 24 20 26 20 27 20 28 20 21 29 30 31 20 32 33 34 20 26 20 21 22 23 27 20 21 22 23 24 25 28 20 21 22 23 24 25 26 27 29 30 31 32 33 29 20 21 21 22 23 20 24 20 25 26 27 20 28 20 30 20 31 32 33 34 30 20 21 21 20 22 20 23 20 24 20 25 20 26 20 27 20 21 22 28 29 20 21 22 23 21 20 22 23 22 23 20 24 20 21 22 23 25 20 21 22 26 20 21 27 20 28 20 21 31 20 21 22 32 20 21 22 23 24 21 22 25 26 21 27 21 28 21 31 21 22 33 21 34 21 21 20 22 23 22 20 21 23 20 21 24 20 21 22 21 22 20 23 20 25 20 21 22 26 20 21 27 20 28 20 21 22 31 20 21 22 33 20 21 34 20 21 22 25 20 21 21 20 22 23 20 24 20 21 26 20 27 20 28 20 31 20 21 22 33 20 21 22 34 20 26 20 21 22 23 24 25 27 27 20 21 22 23 28 20 21 22 23 31 20 21 22 23 21 20 22 22 20 23 20 24 20 21 22 25 20 21 22 23 26 20 21 27 20 28 20 21 33 20 21 22 34 20 21 33 20 21 22 22 21 23 21 24 21 22 25 21 26 21 22 27 21 28 21 22 31 21 22 23 34 21 21 20 22 23 22 20 21 23 24 23 20 21 22 24 24 20 21 21 22 20 23 25 20 21 22 26 20 21 27 20 28 20 21 31 20 21 22 34 20 25 20 21 21 20 22 20 23 24 20 26 20 27 20 28 20 21 31 20 21 34 20 26 20 21 21 20 22 23 24 25 20 27 28 31 34 27 20 21 22 28 20 21 22 21 20 22 22 20 23 24 20 21 25 20 26 20 21 27 20 31 20 21 34 20 21 31 20 21 22 23 22 24 22 25 22 26 22 27 22 28 22 34 22 22 21 23 21 24 21 25 21 26 27 28 34 23 21 22 21 24 21 25 26 21 27 28 34 24 21 22 23 22 21 23 21 25 21 22 26 21 22 27 21 28 21 22 34 21 25 21 22 22 21 23 21 24 21 22 26 21 22 27 21 28 21 22 34 21 26 21 22 22 21 23 21 24 21 22 25 21 27 21 28 21 22 34 21 27 21 22 22 23 24 25 21 26 21 28 21 34 21 28 21 22 23 22 21 23 21 24 21 22 25 21 22 26 21 22 27 21 34 21 34 21 22 22 21 23 24 21 25 21 26 21 27 28 21 21 20 22 23 22 20 23 20 24 20 22 25 20 26 20 22 27 20 28 20 22 34 20 22 20 21 23 24 25 26 27 28 34 23 20 21 22 24 25 26 27 28 34 24 20 21 22 23 25 21 26 21 27 28 21 34 21 20 22 23 25 26 22 20 21 23 25 23 20 21 22 25 26 25 20 21 21 22 23 26 20 27 20 28 34 26 20 21 22 23 25 20 27 28 34 27 20 21 22 23 28 20 21 20 22 23 25 26 20 27 34 34 20 21 22 23 25 26 25 20 21 22 23 24 21 26 21 27 28 21 34 21 20 22 23 24 26 27 22 20 21 23 24 23 20 21 22 24 24 20 21 21 20 22 23 26 20 27 28 20 34 26 20 21 22 23 24 27 28 34 27 20 21 22 23 24 28 20 21 21 22 23 24 26 20 27 34 34 20 21 22 23 24 26 26 20 21 22 23 24 25 27 21 20 22 23 22 20 21 23 20 21 24 20 21 22 23 25 25 20 21 22 23 24 27 20 21 22 28 20 21 22 23 24 34 20 21 22 23 27 20 21 21 20 22 23 20 24 20 25 20 26 20 28 20 34 20 28 20 21 22 23 24 25 26 27 21 20 22 23 22 20 21 23 23 20 21 24 20 21 22 23 25 26 27 25 20 21 22 23 24 26 27 26 20 21 22 23 24 25 27 27 20 21 22 23 34 20 21 22 23 34 20 21 22 23 21 20 22 22 20 23 20 24 20 21 22 25 20 21 22 26 20 21 27 20 21 28 20 21 22 34 20 21 22 23 34 20 21 22 23 33 20 21 22 23 34 20 21 31 20 21 22 32 20 21 22 33 34 20 21 31 20 21 22 23 24 32 20 21 20 22 23 20 24 25 26 20 27 20 28 20 29 20 21 30 20 21 22 31 33 34 33 20 21 22 23 24 25 26 34 20 21 22 23 24 25 26 Fig. 6.16 Adaptive MCTS tree [434] Applications MCTS was introduced in 2006 [165, 166, 164] in the context of computer Go programs, following work by Chang et al. [135], Auer et al. [31], and Cazenave and Helmstetter [134]. The introduction of MCTS improved performance of Go programs considerably, from medium amateur to strong amateur. Where the heuristics-based GNU Go program played around 10 kyu, Monte Carlo programs progressed to 2-3 dan in a few years\u2019 time. Eventually, on the small 9 \u00d7 9 board, Go programs achieved very strong play. On the large 19 \u00d7 19 board, performance did not improve much beyond the 4-5 dan level, despite much efort by researchers. It was thought that perhaps the large action space of the 19\u00d719 board was too hard for MCTS. Many enhancements were considered, for the playout phase, and for the selection. As the AlphaGo results show, a crucial enhancement was the introduction of deep function approximation. After its introduction, MCTS quickly proved successful in other applications, both two agent and single agent: for video games [138], for single player applications [117], and for many other games. Beyond games, MCTS revolutionized the world of heuristic search [117]. Previously, in order to achieve best-frst search, one had to fnd a domain specifc heuristic to guide the search in a smart way. With MCTS this is no longer necessary. Now a general method exists that fnds the promising parts of the search without a domain-specifc heuristic, just by using statistics of the search itself. There is a deeper relation between UCT and reinforcement learning. Grill et al. [289] showed how the second term of P-UCT acts as a regularizer on model-free policy optimization [4]. In particular, Jacob et al. [372] showed how MCTS can be used to achieve human-like play in chess, Go, and Diplomacy, by regularizing reinforcement learning with supervised learning on human games. MCTS in AlphaGo Zero For policy improvement, AlphaGo Zero uses a version of on-policy MCTS that does not use random playouts anymore. To increase exploration, Dirichlet noise 6.2 Tabula Rasa Self-Play Agents 171 is added to the P-UCT value at the root node, to ensure that all moves may be tried. The \ud835\udc36\ud835\udc5d value of MCTS in AlphaGo is 5, heavily favoring exploration. In AlphaGo Zero the value depends on the stage in the learning; it grows during self-play. In each self-play iteration 25,000 games are played. For each move, MCTS performs 1600 simulations. In total over a three-day course of training 4.9 million games were played, after which AlphaGo Zero outperformed the previous version, AlphaGo [706]. Conclusion We have taken a look into the planning part of AlphaGo Zero\u2019s self-play architecture. MCTS consists of a move selection and a statistics backup phase, that corresponds to the behavior (trial) and learning (error) from reinforcement learning. MCTS is an important algorithm in reinforcement learning, and we have taken a detailed look at the algorithm. Move-level self-play is our frst self-play procedure; it is a procedure that plays itself to generate its counter moves. The move-level planning is only one part of the self-play picture. Just as important is the learning part. Let us have a look at how AlphaGo Zero achieves its function approximation. For this, we move to the second level of self-play: the example level. 6.2.2 Example-Level Self-Play Move-level self-play creates an environment for us that can play our counter-moves. Now we need a mechanism to learn from these actions. AlphaGo Zero follows the actor critic principle to approximate both value and policy functions. It approximates these functions using a single deep residual neural network with a value-head and a policy-head (Sect. B.2.6). The policy and the value approximations are incorporated in MCTS in the selection and backup step. In order to learn, reinforcement learning needs training examples. The training examples are generated at the self-play move level. Whenever a move is played, the \u27e8\ud835\udc60, \ud835\udc4e\u27e9 state-action pair is recorded, and whenever a full game has been played to the end, the outcome \ud835\udc67 is known, and the outcome is added to all pairs of game moves, to create \u27e8\ud835\udc60, \ud835\udc4e, \ud835\udc67\u27e9 triples. The triples are stored in the replay bufer, and sampled randomly to train the value/policy net. The actual implementation in AlphaGo Zero contains many more elements to improve the learning stability. The player is designed to become stronger than the opponent, and this occurs at the example-level. Here it uses MCTS to improve the current policy, improving it with moves that are winning against the opponent\u2019s moves. Example-level self-play is our second self-play procedure, the examples are generated in the self-play games and are used to train the network that is used to play the moves by the two players. 172 6 Two-Agent Self-Play 6.2.2.1 Policy and Value Network The frst AlphaGo program uses three separate neural networks: for the rollout policy, for the value, and for the selection policy [703]. AlphaGo Zero uses a single network, that is tightly integrated in MCTS. Let us have a closer look at this single network. The network is trained on the example triples \u27e8\ud835\udc60, \ud835\udc4e, \ud835\udc67\u27e9 from the replay bufer. These triples contain search results of the board states of the game, and the two loss function targets: \ud835\udc4e for the actions that MCTS predicts for each board states, and \ud835\udc67 for the outcome of the game (win or loss) when it came to an end. The action \ud835\udc4e is the policy loss, and the outcome \ud835\udc67 is the value loss. All triples for a game consist of the same outcome \ud835\udc67, and the diferent actions that were played at each state. AlphaGo Zero uses a dual-headed residual network (a convolutional network with extra skip-links between layers, to improve regularization, see Sect. B.2.6 [321, 132]). Policy and value loss contribute equally to the loss function [823]. The network is trained by stochastic gradient descent. L2 regularization is used to reduce overftting. The network has 19 hidden layers, and an input layer and two output layers, for policy and value. The size of the mini-batch for updates is 2048. This batch is distributed over 64 GPU workers, each with 32 data entries. The mini-batch is sampled uniformly over the last 500,000 self-play games (replay bufer). The learning rate started at 0.01 and went down to 0.0001 during self-play. More details of the AlphaGo Zero network are described in [706]. Please note the size of the replay bufer, and the long training time. Go is a complex game, with sparse rewards. Only at the end of a long game the win or loss is known, and attributing this sparse reward to the many individual moves of a game is difcult, requiring many games to even out errors. MCTS is an on-policy algorithm that makes use of guidance in two places: in the downward action selection and in the upward value backup. In AlphaGo Zero the function approximator returns both elements: a policy for the action selection and a value for the backup [706]. For tournament-level self-play to succeed, the training process must (1) cover enough of the state space, must (2) be stable, and must (3) converge. Training targets must be sufciently challenging to learn, and sufciently diverse. The purpose of MCTS is to act as a policy improver in the actor critic setting, to generate learning targets of sufcient quality and diversity for the agent to learn. Let us have a closer look at these aspects, to get a broader perspective on why it was so difcult get self-play to work in Go. 6.2.2.2 Stability and Exploration Self-play has a long history in artifcial intelligence, going back to TD-Gammon, 30 years ago. Let us look at the challenges in achieving a strong level of play. Since in AlphaGo Zero all learning is by reinforcement, the training process must now be even more stable than in AlphaGo, which also used supervised learning from 6.2 Tabula Rasa Self-Play Agents 173 grandmaster games. The slightest problem in overftting or correlation between states can throw of the coverage, correlation, and convergence. AlphaGo Zero uses various forms of exploration to achieve stable reinforcement learning. Let us summarize how stability is achieved. \u2022 Coverage of the sparse state space is improved by playing a large number of diverse games. The quality of the states is further improved by MCTS look-ahead. MCTS searches for good training samples, improving the quality and diversity of the covered states. The exploration part of MCTS should make sure that enough new and unexplored parts of the state space are covered. Dirichlet noise is added at the root node and the \ud835\udc36\ud835\udc5d parameter in the P-UCT formula, that controls the level of exploration, has been set quite high, around 5 (see also Sect. 6.2.1.2). \u2022 Correlation between subsequent states is reduced through the use of an experience replay bufer, as in DQN and Rainbow algorithms. The replay bufer breaks correlation between subsequent training examples. Furthermore, the MCTS search also breaks correlation, by searching deep in the tree to fnd better states. \u2022 Convergence of the training is improved by using on-policy MCTS, and by taking small training steps. Since the learning rate is small, training target stability is higher, and the risk of divergence is reduced. A disadvantage is that convergence is quite slow and requires many training games. By using these measures together, stable generalization and convergence are achieved. Although self-play is conceptually simple, achieving stable and highquality self-play in a game as complex and sparse as Go, required slow training with a large number of games, and quite some hyperparameter tuning. There are many hyperparameters whose values must be set correctly, for the full list, see [706]. Although the values are published [706], the reasoning behind the values is not always clear. Reproducing the AlphaGo Zero results is not easy, and much time is spent in tuning and experimenting to reproduce the AphaGo Zero results [605, 729, 768, 776, 866]. Two Views At this point it is useful to step back and refect on the self-play architecture. There are two diferent views. The one view, planning-centric, which we have followed so far, is of a searcher that is helped by a learned evaluation function (which trains on examples from games played against itself). In addition, there is move-level self-play (opponent\u2019s moves are generated with an inverted replica of itself) and there is tournament-level self-play (by the value learner). The alternative view, learning-centric, is that a policy is learned by generating game examples from self-play. In order for these examples to be of high quality, the policy-learning is helped by a policy improver, a planning function that performs lookahead to create better learning targets (and the planning is performed by making moves by a copy of the player). In addition, there is tournament-level self-play (by the policy learner) and there is move-level self-play (by the policy improver). 174 6 Two-Agent Self-Play The diference in viewpoint is who comes frst: the planning viewpoint favors the searcher, and the learner is there to help the planner; the reinforcement learning viewpoint favors the policy learner, and the planner is there to help improve the policy. Both viewpoints are equally valid, and both viewpoints are equally valuable. Knowing of the other viewpoint deepens our understanding of how these complex self-play algorithms work. This concludes our discussion of the second type of self-play, the example-level, and we move on to the third type: tournament-level self-play. 6.2.3 Tournament-Level Self-Play At the top level, a tournament of self-play games is played between the two (identical) players. The player is designed to increase in strength, learning from the examples at the second level, so that the player can achieve a higher level of play. In tabula rasa self-play, the players start from scratch. By becoming progressively stronger, they also become stronger opponents for each other, and their mutual level of play can increase. A virtuous cycle of ever increasing intelligence will emerge. For this ideal of artifcial intelligence to become reality, many stars have to line up. After TD-Gammon, many researchers have tried to achieve this goal in other games, but were unsuccessful. Tournament-level self-play is only possible when move-level self-play and example-level self-play work. For move-level self-play to work, both players need to have access to the transition function, which must be completely accurate. For example-level self-play to work, the player architecture must be such that it is able to learn a stable policy of a high quality (MCTS and the network have to mutually improve eachother). Tournament-level self-play is the third self-play procedure, where a tournament is created with games starting from easy learning tasks, changing to harder tasks, increasing all the way to world champion level training. This third procedure allows reinforcement learning to transcend the level of play (\u201cintelligence\u201d) of previous teachers. Curriculum Learning As mentioned before, the AlphaGo efort consists of three programs: AlphaGo, AlphaGo Zero, and AlphaZero. The frst AlphaGo program used supervised learning based on grandmaster games, followed by reinforcement learning on self-play games. The second program, AlphaGo Zero, used reinforcement learning only, in a selfplay architecture that starts from zero knowledge. The frst program trained many weeks, yet the second program needed only a few days to become stronger than the frst [706, 703]. 6.2 Tabula Rasa Self-Play Agents 175 Why did the self-play approach of AlphaGo Zero learn faster than the original AlphaGo that could beneft from all the knowledge of Grandmaster games? Why is self-play faster than a combination of supervised and reinforcement learning? The reason is a phenomenon called curriculum learning: self-play is faster because it creates a sequence of learning tasks that are ordered from easy to hard. Training such an ordered sequence of small tasks is quicker than one large unordered task. Curriculum learning starts the training process with easy concepts before the hard concepts are learned; this is, of course, the way in which humans learn. Before we learn to run, we learn to walk; before we learn about multiplication, we learn about addition. In curriculum learning the examples are ordered in batches from easy to hard. Learning such an ordered sequence of batches goes better since understanding the easy concepts helps understanding of the harder concepts; learning everything all at once typically takes longer and may result in lower accuracy.13 6.2.3.1 Self-Play Curriculum Learning In ordinary deep reinforcement learning the network tries to solve a fxed problem in one large step, using environment samples that are not sorted from easy to hard. With examples that are not sorted, the program has to achieve the optimization step from no knowledge to human-level play in one big, unsorted, leap, by optimizing many times over challenging samples where the error function is large. Overcoming such a large training step (from beginner to advanced) costs much training time. In contrast, in AlphaGo Zero, the network is trained in many small steps, starting against a very weak opponent, just as a human child learns to play the game by playing against a teacher that plays simple moves. As our level of play increases, so does the difculty of the moves that our teacher proposes to us. Subsequently, harder problems are generated and trained for, refning the network that has already been pretrained with the easier examples. Self-play naturally generates a curriculum with examples from easy to hard. The learning network is always in lock step with the training target\u2014errors are low throughout the training. As a consequence, training times go down and the playing strength goes up. 6.2.3.2 Supervised and Reinforcement Curriculum Learning Curriculum learning has been studied before in psychology and education science. Selfridge et al. [689] frst connected curriculum learning to machine learning, where they trained the proverbial Cartpole controller. First they trained the controller on long and light poles, while gradually moving towards shorter and heavier poles. Schmidhuber [672] proposed a related concept, to improve exploration for world 13 Such a sequence of related learning tasks corresponds to a meta-learning problem. In metalearning the aim is to learn a new task fast, by using the knowledge learned from previous, related, tasks; see Chap. 9. 176 6 Two-Agent Self-Play Fig. 6.17 Efectiveness of a Sorted Curriculum [833] models by artifcial curiosity. Curriculum learning was subsequently applied to match the order of training examples to the growth in model capacity in diferent supervised learning settings [225, 432, 77]. Another related development is in developmental robotics, where curriculum learning can help to self-organize open-ended developmental trajectories [578], related to intrinsic motivation (see Sect. 8.3.2). The AMIGo approach uses curriculum learning to generate subgoals in hierarchical reinforcement learning [125]. To order the training examples from easy to hard, we need a measure to quantify the difculty of the task. One idea is to use the minimal loss with respect to some of the upper layers of a high-quality pretrained model [833]. In a supervised learning experiment, Weinshall et al. compared the efectiveness of curriculum learning on a set of test images (5 images of mammals from CIFAR100). Figure 6.17 shows the accuracy of a curriculum ordering (green), no curriculum (blue), randomly ordered groups (yellow) and the labels sorted in reverse order (red). Both networks are regular networks with multiple convolutional layers followed by a fully connected layer. The large network has 1,208,101 parameters, the small network has 4,557 parameters. We can clearly see the efectiveness of ordered learning [833]. Procedural Content Generation Finding a good way to order the sequence of examples is often difcult. A possible method to generate a sequence of tasks that are related is by using procedural content generation (PCG) [692, 541]. Procedural content generation uses randomized algorithms to generate images and other content for computer games; the difculty of the examples can often be controlled. It is frequently used to automatically gen6.2 Tabula Rasa Self-Play Agents 177 erate diferent levels in games, so that they do not have to be all created manually by the game designers and programmers [715, 781].14 The Procgen benchmark suite has been built upon procedurally generated games [158]. Another popular benchmark is the General video game AI competition (GVGAI) [479]. Curriculum learning reduces overftting to single tasks. Justesen et al. [388] have used GVGAI to show that a policy easily overfts to specifc games, and that training over a curriculum improves its generalization to levels that were designed by humans. MiniGrid is a procedurally generated world that can be used for hierarchical reinforcement learning [143, 621]. Active Learning Curriculum learning is related also related to active learning. Active learning is a type of machine learning that is in-between supervised and reinforcement learning. Active learning is relevant when labels are in principle available (as in supervised learning) but at a cost. Active learning performs a kind of iterative supervised learning, in which the agent can choose to query which labels to reveal during the learning process. Active learning is related to reinforcement learning and to curriculum learning, and is for example of interest for studies into recommender systems, where acquiring more information may come at a cost [691, 640, 178]. Single-Agent Curriculum Learning Curriculum learning has been studied for many years. A problem is that it is difcult to fnd an ordering of tasks from easy to hard in most learning situations. In twoplayer self-play the ordering comes natural, and the successes have inspired recent work on single-agent curriculum learning. For example, Laterre et al. introduce the Ranked Reward method for solving bin packing problems [455] and Wang et al. presented a method for Morpion Solitaire [824]. Feng et al. use an AlphaZero based approach to solve hard Sokoban instances [239]. Their model is an 8 block standard residual network, with MCTS as planner. They create a curriculum by constructing simpler subproblems from hard instances, using the fact that Sokoban problems have a natural hierarchical structure. This approach was able to solve harder Sokoban instances than had been solved before. Florensa et al. [248] study the generation of goals for curriculum learning using a generator network (GAN). 14 See also generative adversarial networks and deep dreaming, for a connectionist approach to content generation, Sect. B.2.6. 178 6 Two-Agent Self-Play Name Approach Ref TD-Gammon Tabula rasa self-play, shallow network, small alpha-beta search [763] AlphaGo Supervised, self-play, 3\u00d7CNN, MCTS [703] AlphaGo Zero Tabula rasa self-play, dual-head-ResNet, MCTS [706] AlphaZero Tabula rasa self-play, dual-head-ResNet, MCTS on Go, chess, shogi [704] Table 6.2 Self-Play Approaches Conclusion Although curriculum learning has been studied in artifcial intelligence and in psychology for some time, it has not been a popular method, since it is difcult to fnd well-sorted training curricula [518, 519, 827]. Due to the self-play results, curriculum learning is now attracting more interest, see [552, 837]. Work is reported in single-agent problems [552, 239, 198, 455], and in multi-agent games, as we will see in Chap. 7. After this detailed look at self-play algorithms, it is time to look in more detail at the environments and benchmarks for self-play. 6.3 Self-Play Environments Progress in reinforcement learning is determined to a large extent by the application domains that provide the learning challenge. The domains of checkers, backgammon, and especially chess and Go, have provided highly challenging domains, for which substantial progress was achieved, inspiring many researchers. The previous section provided an overview of the planning and learning algorithms. In this section we will have a closer look at the environments and systems that are used to benchmark these algorithms. Table 6.2 lists the AlphaGo and self-play approaches that we discuss in this chapter. First we will discuss the three AlphaGo programs, and then we will list open self-play frameworks. We will start with the frst program, AlphaGo. 6.3.1 How to Design a World Class Go Program? Figure 6.18 shows the playing strength of traditional programs (right panel, in red) and diferent versions of the AlphaGo programs, in blue. We see how much stronger the 2015, 2016, and 2017 versions of AlphaGo are than the earlier heuristic minimax program GnuGo, and two MCTS-only programs Pachi and Crazy Stone. How did the AlphaGo authors design such a strong Go program? Before AlphaGo, the strongest programs used the Monte Carlo Tree Search planning algorithm, without neural networks. For some time, neural networks were considered to be 6.3 Self-Play Environments 179 Fig. 6.18 Performance of AlphaGo Zero [706] Fig. 6.19 All AlphaGo Networks [703] too slow for use as value function in MCTS, and random playouts were used, often improved with small pattern-based heuristics [269, 268, 138, 166, 227]. Around 2015 a few researchers tried to improve performance of MCTS by using deep learning evaluation functions [155, 22, 267]. These eforts were strenghthened by the strong results in Atari [523]. The AlphaGo team also tried to use neural networks. Except for backgammon, pure self-play approaches had not been shown to work well, and the AlphaGo team did the sensible thing to pretrain the network with the games of human grandmasters, using supervised learning. Next, a large number of self-play games were used to further train the networks. In total, no less than three neural networks were used: one for the MCTS playouts, one for the policy function, and one for the value function [703]. 180 6 Two-Agent Self-Play Thus, the original AlphaGo program consisted of three neural networks and used both supervised learning and reinforcement learning. The diagram in Fig. 6.19 illustrates the AlphaGo architecture. Although this design made sense at the time given the state of the art of the feld, and although it did convincingly beat the three strongest human Go players, managing and tuning such a complicated piece of software is quite difcult. The authors of AlphaGo tried to improve performance further by simplifying their design. Could TD-gammon\u2019s elegant self-play-only design be replicated in Go after all? Indeed, a year later a reinforcement learning-only version was ready that learned to play Go from zero knowledge, tabula rasa: no grandmaster games, only self-play, and just a single neural network [706]. Surprisingly, this simpler version played stronger and learned faster. The program was called AlphaGo Zero, since it learned from zero-knowledge; not a single grandmaster game was learned from, nor was there any heuristic domain knowledge hand coded into the program. This new result, tabula rasa learning in Go with a pure self-play design, inspired much further research in self-play reinforcement learning. 6.3.2 AlphaGo Zero Performance In their paper Silver et al. [706] describe that learning progressed smoothly throughout the training. AlphaGo Zero outperformed the original AlphaGo after just 36 hours. The training time for the version of AlphaGo that played Lee Sedol was several months. Furthermore, AlphaGo Zero used a single machine with 4 tensor processing units, whereas AlphaGo Lee was distributed over many machines and used 48 TPUs.15 Figure 6.18 shows the performance of AlphaGo Zero. Also shown is the performance of the raw network, without MCTS search. The importance of MCTS is large, around 2000 Elo points.16 AlphaGo Zero\u2019s reinforcement learning is truly learning Go knowledge from scratch, and, as the development team discovered, it did so in a way similar to how humans are discovering the intricacies of the game. In their paper [706] they published a picture of how this knowledge acquisition progressed (Fig. 6.20). Joseki are standard corner openings that all Go players become familiar with as they learn to play the game. There are beginner\u2019s and advanced joseki. Over the course of its learning, AlphaGo Zero did learn joseki, and it learned them from beginner to advanced. It is interesting to see how it did so, as it reveals the progression of AlphaGo Zero\u2019s Go intelligence. Figure 6.20 shows sequences from games played by the program. Not to anthropomorphize too much,17 but you can see the little program getting smarter. 15 TPU stands for tensor processing unit, a low-precision design specifcally developed for fast neural network processing. 16 The basis of the Elo rating is pairwise comparison [226]. Elo is often used to compare playing strength in board games. 17 Treat as if human 6.3 Self-Play Environments 181 Fig. 6.20 AlphaGo Zero is Learning Joseki in a Curriculum from Easy to Hard [706] The top row shows fve joseki that AlphaGo Zero discovered. The frst joseki is one of the standard beginner\u2019s openings in Go theory. As we move to the right, more difcult joseki are learned, with stones being played in looser confgurations. The bottom row shows fve joseki favored at diferent stages of the self-play training. It starts with a preference for a weak corner move. After 10 more hours of training, a better 3-3 corner sequence is favored. More training reveals more, and better, variations. AlphaGo Zero discovered a remarkable level of Go knowledge during its self-play training process. This knowledge included not only fundamental elements of human Go knowledge, but also nonstandard strategies beyond the scope of traditional Go knowledge. For a human Go player, it is remarkable to see this kind of progression in computer play, reminding them of the time when they discovered these joseki themselves. With such evidence of the computer\u2019s learning, it is hard not to anthropomorphize AlphaGo Zero. 6.3.3 AlphaZero The AlphaGo story does not end with AlphaGo Zero. A year after AlphaGo Zero, a version was created with diferent input and output layers that learned to play chess and shogi (also known as Japanese chess, Fig. 6.21). AlphaZero uses the same MCTS and deep reinforcement learning architecture as for learning to play Go (the only diferences are the input and output layers) [704]. This new program, AlphaZero, beat the strongest chess and shogi programs, Stockfsh and Elmo. Both these programs followed a conventional heuristic minimax design, optimized by hand and 182 6 Two-Agent Self-Play Fig. 6.21 A Shogi Board Fig. 6.22 Elo rating of AlphaZero in Chess, Shogi, and Go [704] machine learning, and improved with many heuristics for decades. AlphaZero used zero knowledge, zero grandmaster games, and zero hand-crafted heuristics, yet it played stronger. The AlphaZero architecture allows not only very strong play, but is also a general architecture, suitable for three diferent games.18 The Elo rating of AlphaZero in chess, shogi, and Go is shown in Fig. 6.22, [706, 703]. AlphaZero is stronger than the other programs. In chess the diference is the smallest. In this feld the program has benefted from a large community of researchers that have worked intensely on improving performance of the heuristic alpha-beta approach. For shogi the diference is larger. General Game Architecture AlphaZero can play three diferent games with the same architecture. The three games are quite diferent. Go is a static game of strategy. Stones do not move and are rarely captured. Stones, once played, are of strategic importance. In chess the pieces 18 Although an AlphaZero version that has learned to play Go, cannot play chess. It has to re-learn chess from scratch, with diferent input and output layers. 6.3 Self-Play Environments 183 Name Type URL Ref. AlphaZero General AlphaZero in Python https://github.com/suragnair/alpha-zerogeneral [768] ELF Game framework https://github.com/pytorch/ELF [776] Leela AlphaZero for Chess, Go https://github.com/LeelaChessZero/lczero [589] PhoenixGo AlphaZero-based Go prog. https://github.com/Tencent/PhoenixGo [866] PolyGames Env. for Zero learning https://github.com/facebookincubator/ Polygames [133] Table 6.3 Self-learning environments move. Chess is a dynamic game where tactics are important. Chess also features sudden death, a check-mate can occur in the middle of the game by capturing the king. Shogi is even more dynamic since captured pieces can be returned to the game, creating even more complex game dynamics. It is testament to the generality of AlphaZero\u2019s architecture, that games that difer so much in tactics and strategy can be learned so successfully. Conventional programs must be purposely developed for each game, with diferent search hyperparameters and diferent heuristics. Yet the MCTS/ResNet self-play architecture is able to learn all three from scratch. 6.3.4 Open Self-Play Frameworks Tabula rasa learning for the game of Go is a remarkable achievement that inspired many researchers. The code of AlphaGo Zero and AlphaZero, however, is not public. Fortunately, the scientifc publications [706, 704] provide many details, allowing other researchers to reproduce similar results. Table 6.3 summarizes some of the self-learning environments, which we will briefy discuss. \u2022 A0G: AlphaZero General Thakoor et al. [768] created a self-play system called AlphaZero General (A0G).19 It is implemented in Python for TensorFlow, Keras, and PyTorch, and suitably scaled down for smaller computational resources. It has implementations for 6\u00d76 Othello, tic tac toe, gobang, and connect4, all small games of signifcantly less complexity than Go. Its main network architecture is a four layer CNN followed by two fully connected layers. The code is easy to understand in an afternoon of study, and is well suited for educational purposes. The project write-up provides some documentation [768]. \u2022 Facebook ELF ELF stands for Extensible Lightweight Framework. It is a framework for game research in C++ and Python [776]. Originally developed for real-time strategy games by Facebook, it includes the Arcade Learning Environment and 19 https://github.com/suragnair/alpha-zero-general 184 6 Two-Agent Self-Play the Darkforest20 Go program [778]. ELF can be found on GitHub.21 ELF also contains the self-play program OpenGo [777], a reimplementation of AlphaGo Zero (in C++). \u2022 Leela Another reimplementation of AlphaZero is Leela. Both a chess and a Go version of Leela exist. The chess version is based on chess engine Sjeng. The Go22 version is based on Go engine Leela. Leela does not come with trained weights of the network. Part of Leela is a community efort to compute these weights. \u2022 PhoenixGo PhoenixGo is a strong self-play Go program by Tencent [866]. It is based on the AlphaGo Zero architecture.23 A trained network is available as well. \u2022 Polygames PolyGames [133] is an environment for Zero-based learning (MCTS with deep reinforcement learning) inspired by AlphaGo Zero. Relevant learning methods are implemented, and bots for hex, Othello, and Havannah have been implemented. PolyGames can be found on GitHub.24 A library of games is provided, as well as a checkpoint zoo of neural network models. 6.3.5 Hands On: Hex in Polygames Example Let us get some hands-on experience with MCTS-based self-play. We will implement the game of Hex with the PolyGames suite. Hex is a simple and fun board game invented independently by Piet Hein and John Nash in the 1940s. Its simplicity makes it easy to learn and play, and also a popular choice for mathematical analysis. The game is played on a hexagonal board, player A wins if its moves connect the right to the left side, and player B wins if top connects to bottom (see Fig. 6.23; image by Wikimedia). A simple page with resources is here;25 extensive strategy and background books have been written about hex [319, 115]. We use hex because it is simpler than Go, to get you up to speed quickly with self-play learning; we will also use PolyGames. Click on the link26 and start by reading the introduction to PolyGames on GitHub. Download the paper [133] and familiarize yourself with the concepts behind Polygames. Clone the repository and build it by following the instructions. Polygames uses PyTorch,27 so install that too (follow the instructions on the Polygames page). 20 https://github.com/facebookresearch/darkforestGo 21 https://github.com/pytorch/ELF 22 https://github.com/gcp/leela-zero 23 https://github.com/Tencent/PhoenixGo 24 https://github.com/facebookincubator/Polygames 25 https://www.maths.ed.ac.uk/~csangwin/hex/index.html 26 https://github.com/facebookincubator/Polygames 27 https://pytorch.org 6.3 Self-Play Environments 185 Fig. 6.23 A Hex Win for Blue Polygames is interfaced via the pypolygames Python package. The games, such as Hex, can be found in src/games and are coded in C++ for speed. The command pypolygames train is used to train a game and a model. The command pypolygames eval is used to use a previously trained model. The command pypolygames human allows a human to play against a trained model. Type python -m pypolygames {train,eval,traineval,human} --help for help with each of the commands train, eval, traineval, or human. A command to start training a Hex model with the default options is: python -m pypolygames train --game_name=\"Hex\" Try loading a pre-trained model from the zoo. Experiment with diferent training options, and try playing against the model that you just trained. When everything works, you can also try training diferent games. Note that more complex games may take (very) long to train. 186 6 Two-Agent Self-Play Summary and Further Reading We will now summarize the chapter, and provide pointers for further reading. Summary For two-agent zero-sum games, when the transition function is given by the rules of the game, a special kind of reinforcement learning becomes possible. Since the agent can perfectly simulate the moves of the opponent, accurate planning far into the future becomes possible, allowing strong policies to be learned. Typically, the second agent becomes the environment. Previously environments were static, but they will now evolve as the agent is learning, creating a virtuous cycle of increasing (artifcial) intelligence in agent (and environment). The promise of this self-play setup is to achieve high levels of intelligence in a specifc feld. The challenges to overcome instability, however, are large, since this kind of self-play combines diferent kinds of unstable learning methods. Both TD-Gammon and AlphaGo Zero have overcome these challenges, and we have described their approach in quite some detail. Self-play is a combination of planning, learning, and a self-play loop. The selfplay loop in AlphaGo Zero uses MCTS to generate high-quality examples, which are used to train the neural net. This new neural net is then used in a further self-play iteration to generate more difcult games, and refne the network further (and again, and again, and again). Alpha(Go) Zero thus learns starting at zero knowledge, tabula rasa. Self-play makes use of many reinforcement learning techniques. In order to ensure stable learning, exploration is important. MCTS is used for deep planning. The exploration parameter in MCTS is set high, and convergent training is achieved by a low learning rate \ud835\udefc. Because of these hyperparameter settings, and because of the sparse rewards in Go, many games have to be played. The computational demands of stable self-play are large. AlphaGo Zero uses function approximation of two functions: value and policy. Policy is used to help guide action selection in the P-UCT selection operation in MCTS, and value is used instead of random playouts to provide the value function at the leaves of the MCTS tree. MCTS has been changed signifcantly to work in the self-play setting. Gone are the random playouts that gave MCTS the name Monte Carlo, and much of the performance is due to a high-quality policy and value approximation residual network. Originally, the AlphaGo program (not AlphaGo Zero) used grandmaster games in supervised learning in addition to using reinforcement learning; it started from the knowledge of grandmaster games. Next came AlphaGo Zero, which does not use grandmaster games or any other domain specifc knowledge. All learning is based on reinforcement learning, playing itself to build up the knowledge of the game from zero knowledge. A third experiment has been published, called AlphaZero 6.3 Self-Play Environments 187 (without the \u201cGo\u201d). In this paper the same network architecure and MCTS design (and the same learning hyperparameters) were used to learn three games: chess, shogi, and Go. This presented the AlphaZero architecture as a general learning architecture, stronger than the best alpha-beta-based chess and shogi programs. Interestingly, the all-reinforcement learning AlphaGo Zero architecture was not only stronger than the supervised/reinforcement hybrid AlphaGo, but also faster: it learned world champion level play in days, not weeks. Self-play learns quickly because of curriculum learning. It is more efcient to learn a large problem in many small steps, starting with easy problems, ending with hard ones, then in one large step. Curriculum learning works both for humans and for artifcial neural networks. Further Reading One of the main interests of artifcial intelligence is the study of how intelligence emerges out of simple, basic, interactions. In self-learning systems this is happening [464]. The work on AlphaGo is a landmark achievement in artifcial intelligence. The primary sources of information for AlphaGo are the three AlphaGo/AlphaZero papers by Silver et al. [703, 706, 704]. The systems are complex, and so are the papers and their supplemental methods sections. Many blogs have been written about AlphaGo that are more accessible. A movie has been made about AlphaGo.28 There are also explanations on YouTube.29 A large literature on minimax and minimax enhancements for games exists, an overview is in [600]. A book devoted to building your own state-of-the-art self-learning Go bot is Deep Learning and the Game of Go by Pumperla and Ferguson [612], which came out before PolyGames [133]. MCTS has been a landmark algorithm by itself in artifcial intelligence [164, 117, 419]. In the contexts of MCTS, many researchers worked on combining MCTS with learned patterns, especially to improve the random rollouts of MCTS. Other developments, such as asymmetrical and continuous MCTS, are [531, 530], or parallelizations such as [516]. Supervised learning on grandmaster games was used to improve playouts and also to improve UCT selection. Gelly and Silver published notable works in this area [269, 708, 268]. Graf et al. [284] describe experiments with adaptive playouts in MCTS with deep learning. Convolutional neural nets were also used in Go by Clark and Storkey [154, 155], who had used a CNN for supervised learning from a database of human professional games, showing that it outperformed GNU Go and scored wins against Fuego, a strong open source Go program [227] based on MCTS without deep learning. 28 https://www.alphagomovie.com 29 https://www.youtube.com/watch?v=MgowR4pq3e8 188 6 Two-Agent Self-Play Tesauro\u2019s success inspired many others to try temporal diference learning. Wiering et al. and Van der Ree [840, 794] report on self-play and TD learning in Othello and Backgammon. The program Knightcap [60, 61] and Beal et al. [62] also use temporal diference learning on evaluation function features. Arenz [25] applied MCTS to chess. Heinz reported on self-play experiments in chess [327]. Since the AlphaGo results many other applications of machine learning have been shown to be successful. There is interest in theoretical physics [644, 583], chemistry [385], and pharmacology, specifcally for retrosynthetic molecular design [687] and drug design [803]. High-profle results have been achieved by AlphaFold, a program that can predict protein structures [385, 690]. To learn more about curriculum learning, see [837, 77, 502, 248]. Wang et al. [823] study the optimization target of a dual-headed self-play network in AlphaZeroGeneral. The success of self-play has led to interest in curriculum learning in singleagent problems [239, 198, 213, 455]. The relation between classical single-agent and two-agent search is studied by [664]. Exercises To review your knowledge of self-play, here are some questions and exercises. We start with questions to check your understanding of this chapter. Each question is a closed question where a simple, one sentence answer is possible. Questions 1. What are the diferences between AlphaGo, AlphaGo Zero, and AlphaZero? 2. What is MCTS? 3. What are the four steps of MCTS? 4. What does UCT do? 5. Give the UCT formula. How is P-UCT diferent? 6. Describe the function of each of the four operations of MCTS. 7. How does UCT achieve trading of exploration and exploitation? Which inputs does it use? 8. When \ud835\udc36\ud835\udc5d is small, does MCTS explore more or exploit more? 9. For small numbers of node expansions, would you prefer more exploration or more exploitation? 10. What is a double-headed network? How is it diferent from regular actor critic? 11. Which three elements make up the self-play loop? (You may draw a picture.) 12. What is tabula rasa learning? 13. How can tabula rasa learning be faster than reinforcement learning on top of supervised learning of grandmaster games? 14. What is curriculum learning? 6.3 Self-Play Environments 189 Implementation: New or Make/Undo You may have noticed that the minimax and MCTS pseudocode in the fgures lacks implementation details for performing actions, to arrive at successor states. Such board manipulation and move making details are important for creating a working program. Game playing programs typically call the search routine with the current board state, often indicated with parameter n for the new node. This board can be created and allocated anew in each search node, in a value-passing style (local variable). Another option is to pass a reference to the board, and to apply a makemove operation on the board, placing the stone on the board before the recursive call, and an undomove operation removing the stone from the board when it returns back out of the recursion (global variable). This reference-passing style may be quicker if allocating the memory for a new board is an expensive operation. It may also be more difcult to implement correctly, since the makemove/undomove protocol must be followed strictly on all relevant places in the code. If capture moves cause many changes to the board, then these must be remembered for the subsequent undo. For parallel implementations in a shared memory at least all parallel threads must have their own copy of a value-passing style board. (On a distributed memory cluster the separate machines will have their own copy of the board by virtue of the distributed memory.) Exercises For the programming exercises we use PolyGames. See the previous section on how to install PolyGames. If training takes a long time, consider using the GPU support of Polygames and Pytorch. 1. Hex Install PolyGames and train a Hex player with self-play. Experiment with diferent board sizes. Keep the training time constant, and draw a graph where you contrast playing strength against board size. Do you see a clear correlation that the program is stronger on smaller boards? 2. Visualize Install the visualization support torchviz. Visualize the training process using the draw_model script. 3. Hyperparameters Use diferent models, and try diferent hyperparameters, as specifed in the PolyGames documentation. 4. MCTS Run an evaluation tournament from the trained Hex model against a pure MCTS player. See below for tips on make and undo of moves. How many nodes can MCTS search in a reasonable search time? Compare the MCTS Hex player against the self-play player. How many games do you need to play to have statistically signifcant results? Is the random seed randomized or fxed? Which is stronger: MCTS or trained Hex? Chapter 7 Multi-Agent Reinforcement Learning On this planet, in our societies, millions of people live and work together. Each individual has their own individual set of goals and performs their actions accordingly. Some of these goals are shared. When we want to achieve shared goals we organize ourselves in teams, groups, companies, organizations and societies. In many intelligent species\u2014humans, mammals, birds, insects\u2014impressive displays of collective intelligence emerge from such organization [848, 348, 686]. We are learning as individuals, and we are learning as groups. This setting is studied in multi-agent learning: through their independent actions, agents learn to interact and to compete and cooperate with other agents, and form groups. Most research in reinforcement learning has focused on single agent problems. Progress has been made in many topics, such as path-fnding, robot locomotion, and video games. In addition, research has been performed in two-agent problems, such as competitive two-person board games. Both single-agent and two-agent problems are questions of optimization. The goal is to fnd the policy with the highest reward, the shortest path, and the best moves and counter moves. The basic setting is one of reward optimization in the face of natural adversity or competitors, modeled by the environment. As we move closer toward modeling real-world problems, we encounter another category of sequential decision making problems, and that is the category of multiagent decision problems. Multi-agent decision making is a difcult problem, Agents that share the same goal might collaborate, and fnding a policy with the highest reward for oneself or for the group may include achieving win/win solutions with other agents. Coalition forming and collusion are an integral part of the feld of multi-agent reinforcement learning. In real-world decision making both competition and cooperation are important. If we want our agents to behave realistically in settings with multiple agents, then they should understand cooperation in order to perform well. From a computational perspective, studying the emergence of group behavior and collective intelligence is challenging: the environment for the agents consists of many other agents; goals may move, many interactions have to be modeled in order to be understood, and the world to be optimized against is constantly changing. 191 192 7 Multi-Agent Reinforcement Learning A lack of compute power has held back experimental research in multi-agent reinforcement learning for some time. The recent growth in compute power and advances in deep reinforcement learning methods are making progress increasingly possible. Multi-agent decision making problems that have been studied experimentally include the games of bridge, poker, Diplomacy, StarCraft, Hide and Seek, and Capture the Flag. To a large extent, the algorithms for multi-agent reinforcement learning are still being developed. This chapter is full of challenges and development. We will start by reviewing the theoretical framework and defning multi-agent problems. We will look deeper at cooperation and competition, and introduce stochastic games, extensive-form games, and the Nash equilibrium and Pareto optimality. We will discuss population-based methods and curriculum learning in multi-player teams. Next, we will discuss environments on which multi-agent reinforcement learning methods can be tested, such as poker and StarCraft. In these games, often team structure plays an important role. The next chapter discusses hierarhical methods, which can be used to model team structures. This chapter is concluded, as usual, with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Competition \u2022 Cooperation \u2022 Team learning Core Problem \u2022 Find efcient methods for large competitive and cooperative multi-agent problems Core Algorithms \u2022 Counterfactual regret minimization (Sect. 7.2.1) \u2022 Population-based algorithms (Sect. 7.2.3.1) 7.1 Multi-Agent Problems 193 Self-Driving Car To illustrate the key components of multi-agent decision making, let us assume a self driving car is on its way to the supermarket and is approaching an intersection. What action should it take? What is the best outcome for the car? For other agents? For society? The goal of the car is to exit the intersection safely and reach the destination. Possible decisions are going straight or turning left or right into another lane. All these actions consist of sub-decisions. At each time step, the car can move by steering, accelerating and braking. The car must be able to detect objects, such as trafc lights, lane markings, and other cars. Furthermore, we aim to fnd a policy that can control the car to make a sequence of manoeuvres to achieve the goal. In a decision-making setting such as this, two additional challenges arise. The frst challenge is that we should be able to anticipate the actions of other agents. During the decision-making process, at each time step, the robot car should consider not only the immediate value of its own current action but also adapt to consequences of actions by other agents. (For example, it would not be good to choose a certain direction that appears safe early on, stick to it, and not adapt the policy as new information comes in, such as a car heading our way.) The second challenge is that these other agents, in turn, may anticipate the actions of our agent in choosing their actions. Our agent needs to take their anticipatory behavior into account in its own policy\u2014recursion at the policy level. Human drivers, for example, often predict likely movements of other cars and then take action in response (such as giving way, or accelerating while merging into another lane).1 Multi-agent reinforcement learning addresses the sequential decision making problem of multiple autonomous agents that operate in a common stochastic environment, each of which aims to maximise their own long-term reward by interacting with the environment and with other agents. Multi-agent reinforcement learning combines the felds of multi-agent systems [697, 847] and reinforcement learning. 7.1 Multi-Agent Problems We have seen impressive results of single-agent reinforcement learning in recent years. In addition, important results have been achieved in games such as Go and poker, and in autonomous driving. These application domains involve the participation of more than one agent. The study of interactions between more than one agent has a long history in felds such as economics and social choice. 1 Human drivers have a theory of mind of other drivers. Theory of mind, and the related concept of mirror neurons [258], are a psychological theory of empathy and understanding, that allows a limited amount of prediction of future behavior. Theory of mind studies how individuals simulate in their minds the actions of others, including their simulation of our actions (and of our simulations, etc.) [54, 332]. 194 7 Multi-Agent Reinforcement Learning Multi-agent problems introduce many new kinds of possibilities, such as cooperation and simultaneous actions. These phenomena invalidate some of the assumptions of the theory behind single-agent reinforcement learning, and new theoretical concepts have been developed. To begin our study of multi-agent problems, we will start with game theory. Game Theory Game theory is the study of strategic interaction among rational decision-making agents. Game theory originally addressed only the theory of two-person zero-sum games, and was later extended to cooperative games of multiple rational players, such as simultaneous action, non-zero sum, and imperfect-information games. A classic work in game theory is John von Neumann and Oskar Morgenstern\u2019s Theory of Games and Economic Behavior [818, 819], published frst in 1944. This book has laid the foundation for the mathematical study of economic behavior and social choice. Game theory has been instrumental in developing the theory of common goods and the role of government in a society of independent self-interested rational agents. In mathematics and artifcial intelligence, game theory has given a formal basis for the computation of strategies in multi-agent systems. The Markov decision process that we have used to formalize single-agent reinforcement learning assumes perfect information. Most multi-agent reinforcement learning problems, however, are imperfect information problems, where some of the information is private, or moves are simultaneous. We will have to extend our MDP model appropriately to be able to model imperfect information. Stochastic Games and Extensive-Form Games One direct generalization of MDP that captures the interaction of multiple agents is the Markov game, also known as the stochastic game [695]. Described by Littman [483], the framework of Markov games has long been used to express multi-agent reinforcement learning algorithms [452, 789]. This multi-agent version of an MDP is defned as follows [870]. At time \ud835\udc61, each agent \ud835\udc56 \u2208 \ud835\udc41 executes an action \ud835\udc4e\ud835\udc56 \ud835\udc61, for the state \ud835\udc60\ud835\udc61 of the system. The system then transitions to state \ud835\udc60\ud835\udc61+1, and rewards each agent \ud835\udc56 with reward \ud835\udc45\ud835\udc56 \ud835\udc4e\ud835\udc61 (\ud835\udc60\ud835\udc61, \ud835\udc60\ud835\udc61+1). The goal of agent \ud835\udc56 is to optimize its own long-term reward, by fnding the policy \ud835\udf0b\ud835\udc56 : \ud835\udc46 \u2192 E(\ud835\udc34\ud835\udc56) as a mapping from the state space to a distribution over the action space, such that \ud835\udc4e\ud835\udc56 \ud835\udc61 \u223c \ud835\udf0b\ud835\udc56(\u00b7|\ud835\udc60\ud835\udc61). Then, the value-function \ud835\udc49\ud835\udc56 : \ud835\udc46 \u2192 \ud835\udc45 of agent \ud835\udc56 becomes a function of the joint policy \ud835\udf0b : \ud835\udc46 \u2192 E(\ud835\udc34) defned as \ud835\udf0b(\ud835\udc4e|\ud835\udc60) = \u03a0\ud835\udc56\u2208\ud835\udc41 \ud835\udf0b\ud835\udc56(\ud835\udc4e\ud835\udc56|\ud835\udc60). To visualize imperfect information in multiple agents, new diagrams are needed. Figure 7.1 shows three schematic diagrams. First, in (a), the familiar agent/environment diagram is shown that we used in earlier chapters. Next, in (b), the multi-agent version of this diagram for Markov games is shown. Finally, in (c) an extensive-form game tree is shown [870]. Extensive form game trees are introduced to model im7.1 Multi-Agent Problems 195 Fig. 7.1 Multi-Agent Models [870] perfect information. Choices by the agent are shown as solid links, and the hidden private information of the other agent is shown dashed, as the information set of possible situations. In (a), the agent observes the state \ud835\udc60, performs action \ud835\udc4e, and receives reward \ud835\udc5f from the environment. In (b), in the Markov game, all agents choose their actions \ud835\udc4e\ud835\udc56 simultaneously and receiving their individual reward \ud835\udc5f\ud835\udc56. In (c), in a two-player extensive-form game, the agents make decisions on choosing actions \ud835\udc4e\ud835\udc56. They receive their individual reward \ud835\udc5f\ud835\udc56(\ud835\udc67) at the end of the game, where \ud835\udc67 is the score of a terminal node for the branch that resulted from the information set. The information set is indicated with a dotted line, to signal stochastic behavior, the environment (or other agent) chooses an unknown outcome amongst the dotted actions. The extensive-form notation is designed for expressing imperfect information games, where all possible (unknown) outcomes are represented in the information set. In order to calculate the value function, the agent has to regard all possible diferent choices in the information sets; the unknown choices of the opponents create a large state space. Competitive, Cooperative, and Mixed Strategies Multi-agent reinforcement learning problems fall into three groups: problems with competitive behavior, with cooperative behavior, and with mixed behavior. In the competitive setting, the reward of the agents sums up to zero. A win for one agent is a loss for another. In the cooperative setting, agents collaborate to optimize a common long-term reward. A win for one agent is a win for all agents (example: an embankment being built around an area prone to fooding; the embankment benefts all agents in the area). The mixed setting involves both cooperative and competitive agents, with so-called general-sum rewards, some action may lead to win/win, some other to win/loss. 196 7 Multi-Agent Reinforcement Learning These three behaviors are a useful guide to navigate the landscape of multi-agent algorithms, and we will do so in this chapter. Let us have a closer look at each type of behavior. 7.1.1 Competitive Behavior One of the hallmarks of the feld of game theory is a result by John Nash, who defned the conditions for a stable (and in a certain sense optimal) solution among multiple rational non-cooperative agents. The Nash equilibrium is defned as a situation in which no agent has anything to gain by changing its own strategy. For two competitive agents the Nash equilibrium is the minimax strategy. In single-agent reinforcement learning the goal is to fnd the policy that maximizes the cumulative future reward of the agent. In multi-agent reinforcement learning the goal is to fnd a combined policy of all agents that simultaneously achieves that goal: the multi-policy that for each agent maximizes their cumulative future reward. If you have a set of competitive (near-)zero exploitability strategies this is called a Nash equilibrium. The Nash equilibrium characterizes an equilibrium point \ud835\udf0b\u2605, from which none of the agents has an incentive to deviate. In other words, for any agent \ud835\udc56 \u2208 \ud835\udc41, the policy \ud835\udf0b\ud835\udc56,\u2605 is the best-response to \ud835\udf0b\u2212\ud835\udc56,\u2605, where \u2212\ud835\udc56 are all agents except \ud835\udc56 [332]. An agent that follows a Nash strategy is guaranteed to do no worse than tie, against any other opponent strategy. For games of imperfect information or chance, such as many card games, this is an expected outcome. Since the cards are randomly dealt, there is no theoretical guarantee that a Nash strategy will win or draw every single hand, although on average, it cannot do worse than tie against the other agents. If the opponents also play a Nash strategy then all will tie. If the opponents make mistakes, however, then they can lose some hands, allowing the Nash equilibrium strategy to win. Such a mistake by the opponents would be a deviation from the Nash strategy, following a hunch or other non-rational reason, despite having no theoretical incentive to do so. A Nash equilibrium plays perfect defence. It does not try to exploit the opponent strategy\u2019s faws, and instead just wins when the opponent makes mistakes [103, 451]. The Nash strategy gives the best possible outcome we can achieve against our adversaries when they work against us. In the sense that a Nash strategy is on average unbeatable, it is considered to be an optimal strategy, and solving a game is equivalent to computing a Nash equilibrium. In a few moments we will introduce a method to calculate Nash strategies, called counterfactual regret minimization, but we will frst look at cooperation. 7.1 Multi-Agent Problems 197 Fig. 7.2 Pareto Frontier of Production-Possibilties 7.1.2 Cooperative Behavior In single-agent reinforcement learning, reward functions return scalar values: an action results in a win or a loss, or a single numeric value. In multi-agent reinforcement learning the reward functions may still return a scalar value, but the functions may be diferent for each agent; the overall reward function is a vector of the individual rewards. In fully cooperative stochastic games, the individual rewards are the same, and all agents have the same goal: to maximize the common return. When choices in our problem are made in a decentralized manner by a set of individual decision makers, the problem can be modeled naturally as a decentralized partially observable Markov decision process (dec-POMDP) [569]. Large dec-POMDPs are hard to solve; in general dec-POMDPs are known to be NEXPcomplete. These problems are not solvable with polynomial-time algorithms and searching directly for an optimal solution in the policy space is intractable [82]. A central concept in cooperative problems is Pareto efciency, after the work of Vilfredo Pareto, who studied economic efciency and income distributions. The Pareto efcient solution is the situation where no cooperative agent can be better of without making at least one other agent worse of. All non-Pareto efcient combination are dominated by the Pareto efcient solution. The Pareto frontier is the set of all Pareto efcient combinations, usually drawn as a curve. Figure 7.2 shows a situation where we can choose to produce diferent quantities of two separate goods, item 1 and item 2 (image by Wikimedia). The grey items close to the origin are choices for which better choices exist. The red symbols all represent production combinations that are deemed more favorable by consumers [700]. In multi-objective reinforcement learning, diferent agents can have diferent preferences. A policy is Pareto-optimal when deviating from the policy will make 198 7 Multi-Agent Reinforcement Learning at least one agent worse of. We will compare Nash and Pareto soon by looking at how they are related in the prisoner\u2019s dilemma (Table 7.1). The Pareto optimum is the best possible outcome for us where we do not hurt others, and others do not hurt us. It is a cooperative strategy. Pareto equilibria assume communication and trust. In some sense, it is the opposite of the noncooperative Nash strategy. Pareto calculates the situation for an cooperative world, Nash for an competitive world. Multi-objective reinforcement learning Many real-world problems involve the optimization of multiple, possibly conficting objectives. In a fully cooperative setting, all agents share such a common reward function. When the reward function consists of diferent values for each agent, the problem is said to be multi-objective: diferent agents may have diferent preferences or objectives. Note that in principle single-agent problems can also have multiple ojectives, although in this book all single agent problems have had single, scalar, rewards. Multi-objective problems become especially relevant in multi-agent problems, where diferent agents may have diferent reward preferences. In decentralized decision processes, the agents must communicate their preferences to eachother. Such heterogeneity necessitates the incorporation of communication protocols into multi-agent reinforcement learning, and the analysis of communication-efcient algorithms [317, 637]. Multi-objective reinforcement learning [635, 292, 333, 618] is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, such as one for each agent. A multi-objective reinforcement learning algorithm optimizes multiple objectives at the same time, and algorithms such as Pareto Q-learning have been introduced for this purpose [802, 634]. 7.1.3 Mixed Behavior Starting as a theory for non-cooperative games, game theory has progressed to include cooperative games, where it analyzes optimal strategies for groups of individuals, assuming that they can communciate or enforce agreements between them about proper strategies. The prisoner\u2019s dilemma is a well-known example of this type of problem [433].2 The prisoner\u2019s dilemma is thought to be invented by John Nash, although in a more neutral (non cops and robbers) setting. The prisoner\u2019s dilemma is as follows (see Table 7.1). Two burglars, called Row and Column, have been arrested for breaking into a bank. Row and Column understand that they cannot trust the other person, hence they operate in a non-cooperative setting. The police ofcer is ofering the following options: If you both confess, then 2 https://plato.stanford.edu/entries/prisoner-dilemma/ 7.1 Multi-Agent Problems 199 Confess Silent Defect Cooperate Confess (\u22125, \u22125) (0, \u221210) Defect Nash Silent (\u221210, 0) (\u22122, \u22122) Cooperate Pareto Table 7.1 Prisoner\u2019s Dilemma you will both get a light sentence of 5 years. If you both keep quiet, then I will only have evidence to get you a short sentence of 2 years. However, if one of you confesses, and the other stays silent, then the confessor will walk free, and the one keeping quiet goes to prison for 10 years. Please tell me your choice tomorrow morning. This leaves our two criminals with a tough, but clear, choice. If the other stays silent, and I confess, then I walk free; if I also stay silent, then we get 2 years in prison, so confessing is better for me in this case. If the other confesses, and I also confess, then we both get 5 years; if I then stay silent, then I get 10 years in prison, so confessing is again better for me. Whatever the other chooses, confessing gives the lighter sentence, and since they can not coordinate their action, they will each independently confess. Both will get 5 years in prison, even though both would have only gotten 2 years if both would have stayed silent. The police is happy, since both confess and the case is solved. If they would have been able to communicate, or if they would have trusted eachother, then both would have stayed silent, and both would have gotten of with a 2 year sentence. The dilemma faced by the criminals is that, whatever the other does, each is better of confessing than remaining silent. However, both also know that if the other could have been trusted, or if they could have coordinated their answer, then a better outcome would have been within reach. The dilemma illustrates individual self-interest against the interest of the group. In the literature on game theory, confessing is also known as defecting, and staying silent is the cooperative choice. The confess/confess situation (both defect) is the optimal non-cooperative strategy, the Nash equilibrium, because for each agent that stays silent a strategy exists where the sentence will be made worse (10 years) when the other agent confesses. Hence the agent will not stay silent but confess, so as to limit the loss. Silent/silent (both cooperate) is Pareto optimal at 2 years for both, since going to all other cases will make at least one agent worse of. The Nash strategy is non-cooperative, non-communication, non-trust, while Pareto is the cooperative, communication, trust, outcome. 200 7 Multi-Agent Reinforcement Learning Iterated Prisoner\u2019s Dilemma The prisoner\u2019s dilemma is a one-time game. What would happen if we play this game repeatedly, being able to identify and remember the choices of our opponent? Could some kind of communication or trust arise, even if the setting is initially non-cooperative? This question is answered by the iterated version of the prisoner\u2019s dilemma. Interest in the iterated prisoner\u2019s dilemma grew after a series of publications following a computer tournament [36, 37]. The tournament was organized by political scientist Robert Axelrod, around 1980. Game theorists were invited to send in computer programs to play iterated prisoner\u2019s dilemmas. The programs were organized in a tournament and played eachother hundreds of times. The goal was to see which strategy would win, and if cooperation would emerge in this simplest of settings. A range of programs were entered, some using elaborate response strategies based on psychology, while others used advanced machine learning to try to predict the opponent\u2019s actions. Surprisingly, one of the simplest strategies won. It was submitted by Anatol Rapoport, a mathematical psychologist who specialized in the modeling of social interaction. Rapoport\u2019s program played a strategy known as tit for tat. It would start by cooperating (staying silent) in the frst round, and in the next rounds if would play whatever the opponent did in the previous round. Tit for tat thus rewards cooperation with cooperation, and punishes defecting with defection\u2014hence the name. Tit for tat wins if it is paired with a cooperative opponent, and does not loose too much by stubbornly cooperating with a non-cooperative opponent. In the long run, it either ends up in the Pareto optimum or in the Nash equilibrium. Axelrod attributes the success of tit for tat to a number of properties. First, it is nice, that is, it is never the frst to defect. In his tournament, the top eight programs played nice strategies. Second, tit for tat is also retaliatory, it is difcult to exploit the strategy by the non-cooperative strategies. Third, tit for tat is forgiving, when the opponent plays nice, it rewards the play by reverting to being nice, being willing to forget past non-cooperative behavior. Finally, the rule has the advantage of being clear and predictable. Others easily learn its behavior, and adapt to it, by being cooperative, leading to the mutual win/win Pareto optimum. 7.1.4 Challenges After this brief overview of theory, let us see how well practical algorithms are able to solve multi-agent problems. With the impressive results in single-agent and two-agent reinforcement learning, the interest (and the expectations) in multiagent problems have increased. In the next section, we will have a closer look at algorithms for multi-agent problems, but let us frst look at the challenges that these algorithms face. 7.1 Multi-Agent Problems 201 Multi-agent problems are studied in competitive, cooperative, and mixed settings. The main challenges faced by multi-agent reinforcement learning are threefold: (1) partial observability, (2) nonstationary environments, and (3) large state space. All three of these aspects increase the size of the state space. We will discuss these challenges in order. 7.1.4.1 Partial Observability Most multi-agent settings are imperfect information settings, where agents have some private information that is not revealed to other agents. The private information can be in the form of hidden cards whose value is unknowns, such as in poker, blackjack or bridge. In real time strategy games players often do not see the entire map of the game, but parts of the world are obscured. Another reason for imperfect information can be that the rules of the game allow simultaneous moves, such as in Diplomacy, where all agents are determining their next action at the same time, and agents have to act without full knowledge of the other actions. All these situations require that all possible states of the world have to considered, increasing the number of possible states greatly in comparison to perfect information games. Imperfect information is best expressed as an extensive-form game. In fact, given the size of most multi-agent systems, it quickly becomes unfeasible to communicate and keep track of all the information of all agents, even if all agents would make their state and intentions public (which they rarely do). Imperfect information increases the size of the state space, and computing the unknown outcomes quickly becomes unfeasible. 7.1.4.2 Nonstationary Environments Moreover, as all agents are improving their policies according to their own interests concurrently, the agents are faced with a nonstationary environment. The environment\u2019s dynamics are determined by the joint action space of all agents, in which the best policies of agents depend on the best policies of the other agents. This mutual dependence creates an unstable situation. In a single agent setting a single state needs to be tracked to calculate the next action. In a multi-agent setting, all states and all agents\u2019 policies need to be taken into account, and mutually so. Each agent faces a moving target problem. In multi-agent reinforcement learning the agents learn concurrently, updating their behavior policies concurrently and often simultaneously [332]. Actions taken by one agent afect the reward of other agents, and therefore of the next state. This invalidates the Markov property, which states that all information that is necessary to determine the next state is present in the current state and the agent\u2019s action. The powerful arsenal of single-agent reinforcement theory must be adapted before it can be used. 202 7 Multi-Agent Reinforcement Learning To handle nonstationarity, agents must account for the joint action space of the other agents\u2019 actions. The size of the space increases exponentially with the number of agents. In two-agent settings the agent has to consider all possible replies of a single opponent to each of its own moves, increasing the state space greatly. In multi-agent settings the number of replies increases even more, and computing solutions to these problems quickly becomes quite expensive. A large number of agents complicates convergence analysis and increases the computational demands substantially. On the other hand, agents may learn from each other, line up their goals, and collaborate. Collaboration and group forming reduce the number of independent agents that must be tracked. 7.1.4.3 Large State Space In addition to the problems caused by partial observability and nonstationarity, the size of the state space is signifcantly increased in a multi-agent setting, simply because every additional agent exponentially increases the state space. Furthermore, the action space of multi-agent reinforcement learning is a joint action space whose size increases exponentially with the number of agents. The size of the joint action space often causes scalability issues. Solving such a large state space to optimality is infeasible for all but the smallest multi-agent problems, and much work has been done to create models (abstractions) that make simplifying, but sometimes realistic, assumptions to reduce the size of the state space. In the next section we will look at some of these assumptions. 7.2 Multi-Agent Reinforcement Learning Agents In the preceding section the problem of multi-agent reinforcement learning has been introduced, as well as game theory and the links with social choice theory. We have also seen that the introduction of multiple agents has, unsurprisingly, increased the size of the state space even more than for single-agent state spaces. Recent work has introduced some approaches for solving multi-agent problems, which we will introduce here. First, we will discuss an approach based on planning, with the name Counterfactual regret minimization (CFR). This algorithm is successful for computing complex social choice problems, such as occur in the game of poker. CFR is suitable for competitive multi-agent problems. Second, we will discuss cooperative reinforcement learning methods that are also suitable for mixed multi-agent problems. Third, we discuss population-based approaches such as evolutionary strategies and swarm computing [42, 252, 346]. These approaches are inspired by computional behavior that occurs in nature, such as in focks of birds and societies of insects [95, 7.2 Multi-Agent Reinforcement Learning Agents 203 207, 97]. Such methods are well-known from single-agent optimization problems; indeed, population-based methods are successful in solving many complex and large single-agent optimization problems (including stochastic gradient descent optimization) [653]. In this section, we will see that evolutionary methods are also a natural match for mixed multi-agent problems, although they are typically used for cooperative problems with many homogeneous agents. Finally, we will discuss an approach based on multi-play self-learning, in which evolutionary and hierarchical aspects are used. Here diferent groups of reinforcement learning agents are trained against eachother. This approach has been highly successful in the games of Capture the Flag and StarCraft, and is suitable for mixed multi-agent problems. Let us start with counterfactual regret minimization. 7.2.1 Competitive Behavior The frst setting that we will discuss is the competitive setting. This setting is still close to single and two-agent reinforcement learning, that are also based on competition. 7.2.1.1 Counterfactual Regret Minimization Counterfactual regret minimization (CFR) is an iterative method for approximating the Nash equilibrium of an extensive-form game [881]. CFR is suitable for imperfect information games (such as poker) and computes a strategy that is (on average) non-exploitable, and that is therefore robust in a competitive setting. Central in counterfactual regret minimization is the notion of regret. Regret is the loss in expected reward that an agent sufers for not having selected the best strategy, with respect to fxed choices by the other players. Regret can only be known in hindsight. We can, however, statistically sample expected regret, by averaging the regret that did not happen. CFR fnds the Nash equilibrium by comparing two hypothetical players against eachother, where the opponent chooses the action that minimizes our value. CFR is a statistical algorithm that converges to a Nash equilibrium. Just like minimax, it is a self-play algorithm that fnds the optimal strategy under the assumption of optimal play by both sides. Unlike minimax, it is suitable for imperfect information games, where information sets describe a set of possible worlds that the opponent may hold. Like MCTS it samples, repeating this process for billions of games, improving its strategy each time. As it plays, it gets closer and closer towards an optimal strategy for the game: a strategy that can do no worse than tie against any opponent [379, 749, 451]. The quality of the strategy that it computes is measured by its exploitability. Exploitability is the maximum amount that a perfect counter-strategy could win (on expectation) against it. 204 7 Multi-Agent Reinforcement Learning Although the Nash equilibrium is a strategy that is theoretically proven to be not exploitable, in practice, typical human play is far from the theoretical optimum, even for top players [112]. Poker programs based on counterfactual regret minimization started beating the world\u2019s best human players in heads-up limit hold\u2019em in 2008, even though these programs programs were still very much exploitable by this worst-case measure [379]. Many papers on counterfactual regret minimization are quite technical, and the codes for algorithms are too long to explain here in detail. CFR is in important algorithm that is essential for understanding progress in poker. To make the work on poker and CFR more accessible, introductory papers and blogs have been written. Trenner [784] has written a blog3 in which CFR is used to play Kuhn poker, one of the simplest Poker variants. The CFR pseudocode and the function that calls it iteratively are shown in Fig. 7.1; for the other routines, see the blog. The CFR code works as follows. First it checks for being in a terminal state and returns the payof, just as a regular tree traversal code does. Otherwise, it retrieves the information set and the current regret-matching strategy. It uses the reach probability, which is the probability that we reach the current node according to our strategy in the current iteration. Then CFR loops over the possible actions (lines 17\u201324), computes the new reach probabilities for the next game state and calls itself recursively. As there are 2 players taking turns in Kuhn poker, the utility for one player is exactly \u22121 times the utility for the other, hence the minus sign in front of the cfr() call. What is computed here for each action is the counterfactual value. When the loop over all possible actions fnishes, the value of the node-value of the current state is computed (line 26), with our current strategy. This value is the sum of the counterfactual values per action, weighted by the likelihood of taking this action. Then the cumulative counterfactual regrets are updated by adding the node-value times the reach probability of the opponent. Finally, the node-value is returned. Another accessible blog post where the algorithm is explained step by step has been written by Kamil Czarnog\u00f2rski,4 with code on GitHub.5 7.2.1.2 Deep Counterfactual Regret Minimization Counterfactual regret minimization is a tabular algorithm that traverses the extensive-form game tree from root to terminal nodes, coming closer to the Nash equilibrium with each iteration. Tabular algorithms do not scale well to large problems, and researchers often have to use domain-specifc heuristic abstraction schemes [658, 260, 110], alternate methods for regret updates [749], or sampling variants [451] to achieve acceptable performance. 3 https://ai.plainenglish.io/building-a-poker-ai-part-6-beating-kuhn-pokerwith-cfr-using-python-1b4172a6ab2d 4 https://int8.io/counterfactual-regret-minimization-for-poker-ai/ 5 https://github.com/int8/counterfactual-regret-minimization/blob/master/games/ algorithms.py 7.2 Multi-Agent Reinforcement Learning Agents 205 1 def cfr( 2 self , 3 cards: List[str], 4 history: str , 5 reach_probabilities : np.array , 6 active_player : int) -> int: 7 if KuhnPoker. is_terminal (history): 8 return KuhnPoker. get_payoff (history , cards) 9 10 my_card = cards[ active_player ] 11 info_set = self. get_information_set (my_card + history) 12 13 strategy = info_set. get_strategy ( reach_probabilities [ active_player ]) 14 opponent = ( active_player + 1) % 2 15 counterfactual_values = np.zeros(len(Actions)) 16 17 for ix , action in enumerate(Actions): 18 action_probability = strategy[ix] 19 20 new_reach_probabilities = reach_probabilities .copy () 21 new_reach_probabilities [ active_player ] *= action_probability 22 23 counterfactual_values [ix] = -self.cfr( 24 cards , history + action , new_reach_probabilities , opponent) 25 26 node_value = counterfactual_values .dot(strategy) 27 for ix , action in enumerate(Actions): 28 counterfactual_regret [ix] = \\ 29 reach_probabilities [opponent] * ( counterfactual_values [ix] - node_value) 30 info_set. cumulative_regrets [ix] += counterfactual_regret [ ix] 31 32 return node_value 33 34 def train(self , num_iterations : int) -> int: 35 util = 0 36 kuhn_cards = [\u2019J\u2019, \u2019Q\u2019, \u2019K\u2019] 37 for _ in range( num_iterations ): 38 cards = random.sample(kuhn_cards , 2) 39 history = \u2019\u2019 40 reach_probabilities = np.ones (2) 41 util += self.cfr(cards , history , reach_probabilities , 0) 42 return util Listing 7.1 Counter-factual regret minimization [784] 206 7 Multi-Agent Reinforcement Learning For large problems a deep learning version of the algorithm has been developed [111]. The goal of deep counterfactual regret minimization is to approximate the behavior of the tabular algorithm without calculating regrets at each individual information set. It generalizes across similar infosets using approximation of the value function via a deep neural network with alternating player updates. 7.2.2 Cooperative Behavior CFR is an algorithm for the competitive setting. The Nash-equilibrium defnes the competitive win/lose multi-agent case\u2014the (\u22125, \u22125) situation of the prisoner\u2019s dilemma of Table 7.1. We will now move to the cooperative setting. As we have seen, in a cooperative setting, win/win situations are possible, with higher rewards, both for society as a whole and for the individuals. The Pareto optimum for the prisoner\u2019s dilemma example is (\u22122, \u22122), only achievable through norms, trust or cooperation by the agents (as close-knit criminal groups aim to achieve, for example, through a code of silence), see also Leibo et al. [465]. The achievements in single-agent reinforcement learning inspire researchers to achieve similar results in multi-agent. However, partial observability and nonstationarity create a computational challenge. Researchers have tried many diferent approaches, some of which we will cover, although the size of problems for which the current agorithms work is still limited. Wong et al. [846] provide a review of these approaches, open problems in cooperative reinforcement learning are listed by Dafoe et al. [175]. First we will discuss approaches based on single-agent reinforcement learning methods, next we will discuss approaches based on opponent modeling, communication, and psychology [846]. 7.2.2.1 Centralized Training/Decentralized Execution While the dec-POMDP model ofers an appropriate framework for cooperative sequential decision making under uncertainty, solving a large dec-POMDP is intractable [82], and therefore many relaxations of the problems have been developed, where some elements, such as communication, or training, are centralized, to increase tractability [846, 751]. One of the easiest approaches to train a policy for a multi-agent problem is to train the collaborating agents with a centralised controller, efectively reducing a decentralized multi-agent computation to a centralized single-agent computation. In this approach all agents send their observations and local policies to a central controller, that now has perfect information, and that decides which action to take for each agent. However, as large collaborative problems are computationally expensive, the single controller becomes overworked, and this approach does not scale. 7.2 Multi-Agent Reinforcement Learning Agents 207 On the other extreme, we can ignore communication and nonstationarity and let agents train separately. In this approach the agents learn an individual action-value function and view other agents as part of the environment. This approach simplifes the computational demands at the cost of gross oversimplifcation, ignoring multiagent interaction. An in-between approach is centralized training and decentralized execution [427]. Here agents can access extra information during training, such as other agents\u2019 observations, rewards, gradients and parameters. However, they execute their policy decentrally based on their local observations. The local computation and inter-agent communication mitigate nonstationarity while still modeling partial observability and (some) interaction. This approach stabilises the local policy learning of agents, even when other agents\u2019 policies are changing [427]. When value functions are learned centrally, how should this function then be used for decentral execution by the agents? A popular method is value-function factorization. The Value decomposition networks method (VDN) decomposes the central value function as a sum of individual value functions [735], who are executed greedily by the agents. QMIX and QTRAN are two methods that improve on VDN by allowing nonlinear combinations [628, 720]. Another approach is Multiagent variational exploration (MAVEN) which improves the inefcient exploration problem of QMIX using a latent space model [496]. Policy-based methods focus on actor critic approaches, with a centralized critic training decentralized actors. Counterfactual multi-agent (COMA) uses such a centralized critic to approximate the Q-function that has access to the actors that train the behavior policies [250]. Lowe et al. [489] introduce a multi-agent version of a popular of-policy singleagent deep policy-gradient algorithm DDPG (Sect. 4.2.7), called MADDPG. It considers action policies of other agents and their coordination. MADDPG uses an ensemble of policies for each agent. It uses a decentralized actor, centralized critic approach, with deterministic policies. MADDPG works for both competitive and collaborative multi-agent problems. An extension for collision avoidance is presented by Cai et al. [123]. A popular on-policy single-agent method is PPO. Han et al. [313] achieve sample efcient results modeling the continuous Half-cheetah task as a model-based multi-agent problem. Their model-based, multi-agent, work is inspired by MVE [238] (Sect. 5.2.2.1). Yu et al. [863] achieve good results in cooperative multi-agent games (StarCraft, Hanabi, and Particle world) with MAPPO. Modeling cooperative behavior in reinforcement learning in a way that is computationally feasible is an active area of research. Li et al. [474] use implicit coordination graphs to model the structure of interactions. They use graph neural networks to model the coordination graphs [297] for StarCraft and trafc environments [655], allowing scaling of interaction patterns that are learned by the graph convolutional network. 208 7 Multi-Agent Reinforcement Learning 7.2.2.2 Opponent Modeling The state space of multi-agent problems is large, yet the previous approaches tried to learn this large space with adaptations of single-agent algorithms. Another approach is to reduce the size of the state space, for example by explicitly modeling opponent behavior in the agents. These models can then be used to guide the agent\u2019s decision making, reducing the state space that it has to traverse. Albrecht and Stone have written a survey of approaches [14]. One approach to reduce the state space is to assume a set of stationary policies between which agents switch [232]. The Switching agent model (SAM) [877] learns an opponent model from observed trajectories with a Bayesian network. The Deep reinforcement open network (DRON) [320] uses two networks, one to learn the Q-values, and the other to learn the opponent policy representation. Opponent modeling is related to the psychological Theory of Mind [610]. According to this Theory, people attribute mental states to others, such as beliefs, intents, and emotions. Our theory of the minds of others helps us to analyze and predict their behavior. Theory of mind also holds that we assume that the other has theory of mind; it allows for a nesting of beliefs of the form: \u201cI believe that you believe that I believe\u201d [796, 795, 797]. Building on these concepts, Learning with opponent-learning awareness (LOLA) anticipates opponent\u2019s behavior [251]. Probabilistic recursive reasoning (PR2) models our own and our opponent\u2019s behavior as a hierarchy of perspectives [835]. Recursive reasoning has been shown to lead to faster convergence and better performance [535, 176]. Opponent modeling is also an active area of research. 7.2.2.3 Communication Another step towards modeling the real world is taken when we explicitly model communication between agents. A fundamental question is how language between agents emerges when no predefned communication protocol exists, and how syntax and meaning evolve out of interaction [846]. A basic approach to communication is with referential games: a sender sends two images and a message; the receiver then has to identify which of the images was the target [458]. Language also emerges in more complicated versions, or in negotiation between agents [126, 424]. Another area where multi-agent systems are frequently used is the study of coordination, social dilemmas, emergent phenomena and evolutionary processes, see, for example [750, 219, 465]. In the card game bridge [716] bidding strategies have been developed to signal to the other player in the team which cards a player has [716]. In the game of Diplomacy, an explicit negotion-phase is part of each game round [185, 430, 584, 21]. Work is ongoing to design communication-aware variants of reinforcement learning algorithms [711, 315]. 7.2 Multi-Agent Reinforcement Learning Agents 209 7.2.2.4 Psychology Many of the key ideas in reinforcement learning, such as operant conditioning and trial-and-error, originated in cognitive science. Faced with the large state space, multi-agent reinforcement learning methods are moving towards human-like agents. In addition to opponent modeling, studies focus on coordination, pro-social behavior, and intrinsic motivation. A large literature exists on emergence of social norms and cultural evolution in multi-agent systems [465, 105, 34, 181]. To deal with nonstationarity and large states spaces, humans use heuristics and approximation [275, 500]. However, heuristics can lead to biases and suboptimal decision-making [276]. It is interesting to see how multi-agent modeling is discovering concepts from psychology. More research in this area is likely to improve the human-like behavior of artifcial agents. 7.2.3 Mixed Behavior To discuss solution methods for agents in the mixed setting, we will look at one important approach that is again inspired by biology: population-based algorithms. Population-based methods such as evolutionary algorithms and swarm computing work by evolving (or optimizing) a large number of agents at the same time. We will look closer at evolutionary algorithms and at swarm computing, and then we will look at the role they play in multi-agent reinforcement learning. 7.2.3.1 Evolutionary Algorithms Evolutionary algorithms are inspired by bio-genetic processes of reproduction: mutation, recombination, and selection [40]. Evolutionary algorithms work with large populations of simulated individuals, which typically makes it easy to parallelize and run them on large computation clusters. Evolutionary algorithms often achieve good results in optimizing diverse problems. For example, in optimizing single agent problems, an evolutionary approach would model the problem as a population of individuals, in which each individual represents a candidate solution to the problem. The candidate\u2019s quality is determined by a ftness function, and the best candidates are selected for reproduction. New candidates are created through crossover and mutation of genes, and the cycle starts again, until the quality of candidates stabilizes. In this way an evolutionary algorithm iteratively approaches the optimum. Evolutionary algorithms are randomized algorithms, that can circumvent local optima. Although they are best known for solving single agent optimization problems, we will use them here to model multi-agent problems. Let us look in more detail at how an evolutionary algorithm works (see Alg. 7.1) [40]. First an initial population is generated. The ftness of each indi210 7 Multi-Agent Reinforcement Learning Algorithm 7.1 Evolutionary Framework [40] 1: Generate the initial population randomly 2: repeat 3: Evaluate the ftness of each individual of the population 4: Select the fttest individuals for reproduction 5: Through crossover and mutation generate new individuals 6: Replace the least ft individuals by the new individuals 7: until terminated vidual is computed, and the fttest individuals are selected for reproduction, using crossover and mutation to create a new generation of individuals. The least ft individuals of the old populations are replaced by the new individuals. Compared to reinforcement learning, in evolutionary algorithms the agents are typically homogeneous, in the sense that the reward (ftness) function for the individuals is the same. Individuals do have diferent genes, and thus difer in their behavior (policy). In reinforcement learning there is a single current behavior policy, where an evolutionary approach has many candidate policies (individuals). The ftness function can engender in principle both competitive and cooperative behavior between individuals, although a typical optimization scenario is to select a single individual with the genes for the highest ftness (survival of the fttest competitor).6 Changes to genes of individuals (policies) occur explicitly via crossover and (random) mutation, and implicitly via selection for ftness. In reinforcement learning the reward is used more directly as a policy goal; in evolutionary algorithms the ftness does not directly infuence the policy of an individual, only its survival. Individuals in evolutionary algorithms are passive entities that do not communicate or act, although they do combine to create new individuals. There are similarities and diferences between evolutionary and multi-agent reinforcement learning algorithms. First of all, in both approaches the goal is to fnd the optimal solution, the policy that maximizes (social) reward. In reinforcement learning this occurs by learning a policy through interaction with an environment, in evolutionary algorithms by evolving a population through survival of the fttest. Reinforcement learning deals with a limited number of agents whose policy determines their actions, evolutionary algorithms deals with many individuals whose genes determine their survival. Policies are improved using a reward function that assesses how good actions are, genes mutate and combine, and individuals are selected using a ftness function. Policies are improved \u201cin place\u201d and agents do not die, in evolutionary computation the traits (genes) of the best individuals are selected and copied to new individuals in the next generation after which the old generation does die. Although diferent at frst sight, the two approaches share many traits, including the main goal: optimizing behavior. Evolutionary algorithms are inherently multi6 Survival of the fttest cooperative group of individuals can also be achieved with an appropriate ftness function [492]. 7.2 Multi-Agent Reinforcement Learning Agents 211 Fig. 7.3 A Flock of Starlings, and One Predator agent, and may work well in fnding good solutions in large and nonstationary sequential decision problems. 7.2.3.2 Swarm Computing Swarm computing is related to evolutionary algorithms [79, 95]. Swarm computing focuses on emerging behavior in decentralized, collective, self-organized systems. Agents are typically simple, numerous, and homogeneous, and interact locally with each other and the environment. Biological examples of swarm intelligence are behavior in ant colonies, bee-hives, focks of birds, and schools of fsh, Fig. 7.3 [736]; image by Wikimedia. Behavior is typically cooperative through decentralized communication mechanisms. In artifcial swarm intelligence, individuals are sometimes able to imagine the behavior of other individuals, when they have a Theory of mind [54, 258]. Swarm intelligence, or collective intelligence in general, is a form of decentralized computing, as opposed to reinforcement learning, where external algorithms calculate optimal behavior in a single classical centralized algorithm [848]. Although both approaches work for the mixed setting, evolutionary algorithms tend to stress competition and survival of the fttest (Nash), where swarm computing stresses cooperation and survival of the group (Pareto). A well-known example of artifcial swarm intelligence is Dorigo\u2019s Ant colony optimization algorithm (ACO) which is a probabilistic optimization algorithm modeled after the pheromone-based communication of biological ants [206, 208, 209]. 212 7 Multi-Agent Reinforcement Learning Algorithm 7.2 Population Based Training [374] procedure Train(P) \u22b2 initial population P Population P, weights \ud835\udf03, hyperparameters \u210e, model evaluation \ud835\udc5d, time step \ud835\udc61 for (\ud835\udf03, \u210e, \ud835\udc5d, \ud835\udc61) \u2208 P (asynchronously in parallel) do while not end of training do \ud835\udf03 \u2190 step(\ud835\udf03 |\u210e) \u22b2 one step of optimisation using hyperparameters \u210e \ud835\udc5d \u2190 eval(\ud835\udf03) \u22b2 current model evaluation if ready( \ud835\udc5d, \ud835\udc61, P) then \u210e\u2032, \ud835\udf03\u2032 \u2190 exploit(\u210e, \ud835\udf03, \ud835\udc5d, P) \u22b2 use the rest of population for improvement if \ud835\udf03 \u2260 \ud835\udf03\u2032 then \u210e, \ud835\udf03 \u2190 explore(\u210e\u2032, \ud835\udf03\u2032, P) \u22b2 produce new hyperparameters \u210e \ud835\udc5d \u2190 eval(\ud835\udf03) \u22b2 new model evaluation end if end if update P with new (\ud835\udf03, \u210e, \ud835\udc5d, \ud835\udc61 + 1) \u22b2 update population end while end for return \ud835\udf03 with the highest \ud835\udc5d in P end procedure Emergent behavior in multi-agent reinforcement learning is specifcally studied in [487, 374, 534, 325, 50, 464]. For decentralized algorithms related to solving multi-agent problems see, for example [569, 871, 571, 570]. 7.2.3.3 Population-Based Training Translating traditional value or policy-based reinforcement learning algorithms to the multi-agent setting is non-trivial. It is interesting to see that, in contrast, evolutionary algorithms, frst designed for single-agent optimization using a population of candidate solutions, translate so naturally to the multi-agent setting, where a population of agents is used to fnd a shared solution that is optimal for society. In evolutionary algorithms agents are typically homogeneous, although they can be heterogeneous, with diferent ftness functions. The ftness functions may be competitive, or cooperative. In the latter case the increase of reward for one agent can also imply an increase for other agents, possibly for the entire group, or population. Evolutionary algorithms are quite efective optimization algorithms. Salimans et al. [653] report that evolution strategies rival the performance of standard reinforcement learning techniques on modern benchmarks, while being easy to parallelize. In particular, evolutionary strategies are simpler to implement (there is no need for backpropagation), are easier to scale in a distributed setting, do not sufer in settings with sparse rewards, and have fewer hyperparameters. Evolutionary algorithms are a form of population-based computation that can be used to compute strategies that are optimal in a game-theoretic Nash sense, or that try to fnd strategies that out-perform other agents. The evolutionary approach to optimization is efcient, robust, and easy to parallelize, and thus has some 7.2 Multi-Agent Reinforcement Learning Agents 213 advantages over the stochastic gradient approach. How does this approach relate to multi-agent reinforcement learning, and can it be used to fnd efcient solutions for multi-agent reinforcement learning problems? In recent years a number of research teams have reported successful eforts in creating reinforcement learning players for multi-agent strategy games (see Sect. 7.3). These were all large research eforts, where a range of diferent approaches was used, from self-play reinforcement learning, cooperative learning, hierarchical modeling, and evolutionary computing. Jaderberg et al. [374] report success in Capture the Flag games with a combination of ideas from evolutionary algorithms and self-play reinforcement learning. Here a population-based approach of self play is used where teams of diverse agents are trained in tournaments against each other. Algorithm 7.2 describes this Population based training (PBT) approach in more detail. Diversity is enhanced through mutation of policies, and performance is improve through culling of under-performing agents. Population based training uses two methods. The frst is exploit, which decides whether a worker should abandon the current solution and focus on a more promising one. The second is explore, which, given the current solution and hyperparameters, proposes new solutions to explore the solution space. Members of the population are trained in parallel. Their weights \ud835\udf03 are updated and eval measures their current performance. When a member of the population is deemed ready because it has reached a certain performance threshold, its weights and hyperparameters are updated by exploit and explore, to replace the current weights with the weights that have the highest recorded performance in the rest of the population, and to randomly perturb the hyperparameters with noise. After exploit and explore, iterative training continues as before until convergence. Let us have a look at this fusion approach for training leagues of players. 7.2.3.4 Self-Play Leagues Self-play league learning refers to the training of a multi-agent league of individual game playing characters. As we will see in the next section, variants have been used to play games such as StarCraft, Capture the Flag, and Hide and Seek. League learning combines population-based training with self-play training as in AlphaZero. In league learning, the agent plays against a league of diferent opponents, while being part of a larger team of agents that is being trained. The team of agents is managed to have enough diversity in order to provide a stable training goal to reduce divergence or local minima. League learning employs evolutionary concepts such as mutation of behavior policies and culling of under-performing agents from the population. The team employs cooperative strategies, in addition to competing against the other teams. Agents are trained in an explicit hierarchy. The goal of self-play league training is to fnd stable policies for all agents, that maximize their team reward. In mixed and cooperative settings the teams of agents may beneft from each others\u2019 strength increase [489]. Self-play league learning 214 7 Multi-Agent Reinforcement Learning Environment Behavior Approach Ref Poker Competitive (Deep) Counterfactual regret minimization [103, 113, 533] Hide and Seek Cooperative Self-play, Team hierarchy [49] Capture the Flag Mixed Self-play, Hierarchical, Population-based [373] StarCraft II Mixed Self-play, Population-based [813] Table 7.2 Multi-Agent Game Approaches also uses aspects of hierarchical reinforcement learning, a topic that will be covered in the next chapter. In the next section we will look deeper into how self-play league learning is implemented in specifc multi-player games. 7.3 Multi-Agent Environments Reinforcement learning has achieved quite a few imaginative results in which it has succeeded in emulating behavior that approaches human behavior in the real world. In this chapter we have made a step towards modeling more behavior that is even closer to the real world. Let us summarize in this section the results in four diferent multi-agent games, some of which have been published in prestigious scientifc journals. We will use the familiar sequence of (1) competitive, (2) cooperative, and (3) mixed environments. For each we will sketch the problem, outline the algorithmic approach, and summarize the achievements. Table 7.2 lists the multi-agent games and their dominant approach. 7.3.1 Competitive Behavior: Poker Poker is a popular imperfect-information game. It is played competitively and human championships are organized regularly. Poker has been studied for some time in artifcial intelligence, and computer poker championships have been conducted regularly [53, 89]. Poker is a competitive game. Cooperation (collusion, collaboration between two players to the detriment of a third player) is possible, but often does not occur in practice; fnding the Nash equilibrium therefore is a successful approach to playing the game in practice. The method of counterfactual regret minimization has been developed specifcally to make progress in poker. There are many variants of poker that are regularly played. No-limit Texas hold\u2019em is a popular variant; the two-player version is called Heads Up and is easier to analyse because no opponent collusion can occur. Heads-up no-limit Texas hold\u2019em (HUNL) has been the primary AI benchmark for imperfect-information game play for several years. 7.3 Multi-Agent Environments 215 Fig. 7.4 Pluribus on the Cover of Science Poker has hidden information (the face-down cards). Because of this, agents are faced with a large number of possible states; poker is a game that is far more complex than chess or checkers. The state space of the two-person HUNL version is reported to be around 10161 [112]. A further complication is that during the course of the game information is revealed by players through their bets; high bets indicate good cards, or the wish of the player to make the opponent believe that this is the case (blufng). Therefore, a player must choose between betting high on good cards, and on doing the opposite, so that the opponent does not fnd out too much, and can counter-act. In 2018, one of the top two-player poker programs, Libratus, defeated top human professionals in HUNL in a 20-day, 120,000-hand competition featuring a $200,000 prize pool. Brown et al. [112] describe the architecture of Libratus. The program consists of three main modules, one for computing a quick CFR Nash-policy using a smaller version of the game, a second module for constructing a fner-grained strategy once a later stage of the game is reached, and a third module to enhance the frst policy by flling in missing branches. In the experiment against top human players, Libratus analyzed the bet sizes that were played most often by its opponents at the end of each day of the competition. The programs would then calculate a response overnight, in order to improve as the competition proceeded. Two-agent Libratus was originally based on heuristics, abstraction, and tabular counterfactual regret minimization\u2014not on deep reinforcement learning. For multiagent poker the program Pluribus was developed. For Pluribus, the authors used deep counterfactual regret minimization [111], with a 7-layer neural network that followed the AlphaZero self-play approach. Pluribus defeated top players in sixplayer poker [113] (Fig. 7.4). Pluribus uses an AlphaZero approach of self-play in 216 7 Multi-Agent Reinforcement Learning combination with search. Another top program, DeepStack, also uses randomly generated games to train a deep value function network [533]. 7.3.2 Cooperative Behavior: Hide and Seek In addition to competition, cooperative behavior is part of real-world behavior. Indeed, cooperation is what defnes our social fabric, and much of our society consists of diferent ways in which we organize ourselves in families, groups, companies, parties, flter bubbles, and nations. The question of how voluntary cooperation can emerge between individuals has been studied extensively, and the work in tit-for-tat (Sect. 7.1.3) is just one of many studies in this fascinating feld [34, 105, 848, 465, 651]. One study into emergent cooperation has been performed with a version of the game of Hide and Seek. Baker et al.[49] report on an experiment in which they used MuJoCo to build a new multi-agent game environment. The environment was created with the specifc purpose of studying how cooperation emerged out of the combination of a few simple rules and reward maximation (see Fig. 7.5). In the Hide and Seek experiment, the game is played on a randomly generated grid-world where props are available, such as boxes. The reward function stimulates hiders to avoid the line of sight of seekers, and vice versa. There are objects scattered throughout the environment that the agents can grab and lock in place. The environment contains one to three hiders and one to three seekers, there are three to nine movable boxes, some elongated. There are also two movable ramps. Walls and rooms are static and are randomly generated. Agents can see, move, and grab objects. A good way to understand the challenge is to view the videos7 on the Hide and Seek blog.8 With only a visibility-based reward function, the agents are able to learn many diferent skills, including collaborative tool use. For example, hiders learn to create shelters by barricading doors or constructing multi-object forts, so that the seekers can never see them anymore, until, as a counter strategy, seekers learned to use ramps to jump into the shelter. In efect, out of the agents\u2019 interaction a training curriculum emerges in which the agents learn tasks, many of which require sophisticated tool use and coordination. Hide and Seek features cooperation (inside the team) and competition (between the hiders and the seekers). It uses a self-play version of PPO for policy learning. It is interesting to see how easy cooperation emerges. The game does not have explicit communication for the team to coordinate cooperation, all cooperative behavior emerges out of basic interaction between agents that are guided by their reward functions. Cooperative strategies thus emerge out of the game design: the 7 https://www.youtube.com/watch?v=kopoLzvh5jY&t=10s 8 https://openai.com/blog/emergent-tool-use/ 7.3 Multi-Agent Environments 217 Fig. 7.5 Six Strategies in Hide and Seek: Running and Chasing; Fort Building; Ramp Use; Ramp Defense; Box Surfng; Surf Defense (left-to-right, top-to-bottom) [49] homogeneous reward functions for each team and the environment in which blocks are present and follow laws of physics. During play the agents essentially construct an autocurriculum for themselves [464, 49]. Six diferent behavior strategies are reported, each more advanced than the other, increasing the competitive pressure to fnd counter strategies for the opponent (see Fig. 7.5). Baker et al. [49] report that initially, the hiders and seekers learn the basic strategy of running away and chasing. However, after much training (25 million episodes), hiders start to use boxes to construct shelters behind which they hide. Then, after another 75 million episodes, the seekers learn to move and use ramps to jump over obstacles into the shelter. A mere 10 million episodes later, the hiders learn to defend by moving the ramps to the edge and lock them in place out of range of the shelters. Then, after a long 270 million episodes of training, the seekers learned box-surfng. They moved a box to the edge of the play area next to the locked ramps. One seeker then used the ramp to climb on top of the box and the other seekers push it to the shelter, where the seeker could peek over te edge and see the hiders. Finally, in response, the hiders locked all of the boxes in place before building their shelter, and they were safe from the seekers. The Hide and Seek experiment is interesting because of the emergence of diverse behavior strategies. The strategies emerged out of a basic reward function and random exploration (see also the reward is enough argument [707]). 218 7 Multi-Agent Reinforcement Learning Fig. 7.6 Capture the Flag [373] The emergence of strategies out of basic reward and exploration suggests an evolutionary process. However, Hide and Seek does not employ population-based training or evolutionary algorithms, in contrast to the work in Capture the Flag, in the next section. 7.3.3 Mixed Behavior: Capture the Flag and StarCraft The world around us exhibits a mix of competitive and cooperative behavior. Team collaboration is an important aspect of human life, and it has been studied extensively in biology, sociology and artifcial intelligence. It emerges (evolves) out of the most basic settings\u2014the need to achieve an ambitious goal\u2014as we just saw. In recent years many research groups have studied the mixed multi-agent model in real time strategy games, such as Capture the Flag, and StarCraft. We will discuss both games. Capture the Flag First we will discuss the game Capture the Flag, which is played in a Quake III Arena (see Fig. 7.6). Jaderberg et al. have reported on an extensive experiment with this game [373], in which the agents learn from scratch to see, act, cooperate, and compete. In this experiment the agents are trained with population-based 7.3 Multi-Agent Environments 219 self-play [374], Alg. 7.2. The agents in the population are all diferent (they have diferent genes). The population learns by playing against each other, providing increased diversity of teammates and opponents, and a more stable and faster learning process than traditional single-agent deep reinforcement learning methods. In total 30 diferent bots were created and pitted against each other. Agents are part of a team, and the reward functions form a hierarchy. A two-layer optimization process optimizes the internal rewards for winning, and uses reinforcement learning on the internal rewards to learn the policies. In Capture the Flag, the bots start by acting randomly. After 450,000 games, a bot strategy was found that performed well, and they developed cooperative strategies, such as following team mates in order to outnumber opponents, and loitering near the enemy base when their team mate has the fag. Again, as in Hide and Seek, cooperative strategies emerged out of the basic rules, by combining environment feedback and survival of the fttest. The work on Capture the Flag is notable since it demonstrated that with only pixels as input an agent can learn to play competitively in a rich multi-agent environment. To do so it used a combination of population based training, internal reward optimization, and temporally hierarchical reinforcement learning (see next chapter). StarCraft The fnal game that we will discuss in this chapter is StarCraft. StarCraft is a multi-player real-time strategy game of even larger complexity. The state space has been estimated to be on the order of 101685 [573], a very large number. StarCraft features multi-agent decision making under uncertainty, spatial and temporal reasoning, competition, team-collaboration, opponent modeling, and real-time planning. Fig. 1.6 shows a picture of a StarCraft II scene. Research on StarCraft has been ongoing for some time [573], a special StarCraft multi-agent challenge has been introduced [655]. A team from DeepMind has created a program called AlphaStar. In a series of test matches held in December 2018, DeepMind\u2019s AlphaStar beat two top players in two-player single-map matches, using a diferent user interface. AlphaStar plays the full game of StarCraft II. The neural network was initially trained by supervised learning from anonymized human games, that were then further trained by playing against other AlphaStar agents, using a population-based version of self-play reinforcement learning [813, 374].9 These agents are used to seed a multi-agent reinforcement learning process. A continuous competitive league was created, with the agents of the league playing games in competition against each other. By branching from existing competitors, new competitors were added. Agents learn from games against other competitors. Population-based learning was 9 Similar to the frst approach in AlphaGo, where self-play reinforcement learning was also bootstrapped by supervised learning from human games. 220 7 Multi-Agent Reinforcement Learning taken further, creating a process that explores the very large space of StarCraft game play by pitting agents against strong opponent strategies, and retaining strong early strategies. Diversity in the league is increased by giving each agent its own learning objective, such as which competitors it should focus on and which game unit it should build. A form of prioritized league-self-play actor-critic training is used, called prioritized fctitious self-play\u2014details are in [813]. AlphaStar was trained on a custom-built scalable distributed training system using Google\u2019s tensor processing units (TPU). The AlphaStar league was run for 14 days. In this training, each agent experienced the equivalent of 200 years of real-time StarCraft play. In StarCraft players can choose to play one of three alien races: Terran, Zerg or Protoss. AlphaStar was trained to play Protoss only, to reduce training time, although the same training pipeline could be applied to any race. AlphaStar was frst tested against a human grandmaster named TLO, a top professional Zerg player and a grandmaster level Protoss player. The human player remarked: I was surprised by how strong the agent was. AlphaStar takes well-known strategies and turns them on their head. The agent demonstrated strategies I had not thought of before, which means there may still be new ways of playing the game that we haven\u2019t fully explored yet. 7.3.4 Hands On: Hide and Seek in the Gym Example Many of the research eforts reported in these fnal chapters describe signifcant eforts by research teams working on complicated and large games. These games represent the frontier of artifcial intelligence, and research teams use all available computational and software engineering power that they can acquire to achieve the best results. Also, typically a large amount of time is spent in training, and in fnding the right hyperparameters for the learning to work. Replicating results of this scale is highly challenging, and most research eforts are focused on replicating the results on a smaller, more manageable scale, with more manageable computational resources. In this section we will try to replicate some aspects with modest computational requirements. We will focus on Hide and Seek (Sect. 7.3.2). The original code for the Hide and Seek experiments is on GitHub.10 Please visit and install the code. Hide and Seek uses MuJoCo and the mujoco-worldgen package. Install them and the dependencies with pip install -r mujoco-worldgen/requirements.txt pip install -e mujoco-worldgen/ pip install -e multi-agent-emergence-environments/ 10 https://github.com/openai/multi-agent-emergence-environments 7.3 Multi-Agent Environments 221 Examples of environments can be found in the mae_envs/envs folder. You can also build your own environments, starting from the Base environment, in mae_envs/envs/base.py, and then adding boxes, ramps, as well as the appropriate wrappers. Look in the other environments for how to do this. Try out environments by using the bin/examine script; example usage: bin/examine.py base See further the instructions in the GitHub repository. Multiplayer Environments To conclude this hands-on section, we mention a multiplayer implementation of the Arcade Learning Environment. It is presented by Terry et al. [760], who also present baseline performance results for a multiplayer version of DQN, Ape-X DQN, which performed well elsewhere [350, 52]. The environment is also presented as part of PettingZoo, a multi-agent version of Gym [761, 791, 414]. Another multi-agent research environment is the Google Football Research Environment [438]. A physics-based simulator is provided, as well as three baseline implementations (DQN, IMPALA, and PPO). Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary Multi-agent reinforcement learning learns optimal policies for environments that consist of multiple agents. The optimal policy of the agents is infuenced by the policies of the other agents, whose policy is also being optimized. This gives rise to the problem of nonstationarity, and agent behavior violates the Markov property. Multi-agent reinforcement learning adds the element of cooperative behavior to the repertoire of reinforcement learning, which now consists of competition, cooperation, and mixed behavior. The feld is closely related to game theory\u2014the basis of the study of rational behavior in economics. A famous problem of game theory is the prisoner\u2019s dilemma. A famous result of non-cooperative game theory is the Nash equilibrium, which is defned as the joint strategy where no player has anything to gain by changing their own strategy. A famous result from cooperative game-theory is the Pareto optimum, the situation where no individual can be better of without making someone else worse of. 222 7 Multi-Agent Reinforcement Learning When agents have private information, a multi-agent problem is partially observable. Multi-agent problems can be modeled by stochastic games or as extensive form games. The behavior of agents is ultimately determined by the reward functions, that can be homogeneous, or heterogeneous. When agents have diferent reward functions, multi-agent becomes multi-objective reinforcement learning. The regret of an action is the amount of reward that is missed by the agent for not choosing the actions with the highest payof. A regret minimization algorithm is the stochastic and multi-agent equivalent of minimax. Counterfactual-regret minimization is an approach for fnding Nash strategies in competitive multi-agent games, such as poker. Variants of single-agent algorithms are often used for cooperative multi-agent situations. The large state space due to nonstationarity and partial observability precludes solving large problems. Other promising approaches are opponent modeling and explicit communication modeling. Population-based methods such as evolutionary algorithms and swarm intelligence are used frequently in multi-agent systems. These approaches are suitable for homogeneous reward functions and competitive, cooperative, and mixed problems. Evolutionary methods evolve a population of agents, combining behaviors, and selecting the best according to some ftness function. Evolutionary methods are a natural ft for parallel computers and are among the most popular and successful optimization algorithms. Swarm intelligence often introduces (rudimentary) forms of communication between agents, such as in Ant colony optimization where agents communicate through artifcial pheromones to indicate which part of the solution space they have traveled. For some of the most complicated problems that have recently been tackled, such as StarCraft, Capture the Flag, and Hide and Seek, hierarchical and evolutionary principles are often combined in league training, where leagues of teams of agents are trained in a self-play fashion, and where the fttest agents survive. Current achievements require large amounts of computational power, future work is trying to reduce these requirements. Further Reading Multi-agent learning is a widely studied feld. Surveys\u2014both on early multi-agent reinforcement learning and on deep multi-agent reinforcement learning\u2014can be found in [292, 122, 858, 13, 14, 750, 790, 333, 332, 846]. After Littman [483], Shoham et al. [698] look deeper into MDP modeling. The classic work on game theory is Von Neumann and Morgenstern [818]. Modern introductions are [545, 180, 259]. Game theory underlies much of the theory of rational behavior in classical economics. Seminal works of John Nash are [555, 556, 554]. In 1950 he introduced the Nash equilibrium in his dissertation of 28 pages, which won him the Nobel prize in Economics in 1994. A biography and flm have been made about the life of John Nash [553]. 7.3 Multi-Agent Environments 223 The game of rock-paper-scissors plays an important role in game theory, and the study of computer poker [822, 89, 641, 113]. Prospect theory [390], introduced in 1979, studies human behavior in the face of uncertainty, a topic that evolved into the feld of behavioral economics [843, 539, 128]. Gigerenzer introduced fast and frugal heuristics to explain human decision making [275]. For more intriguing works on the feld of evolution of cooperation and the emergence of social norms, see, for example [105, 37, 34, 33, 35, 331, 336]. More recently multi-objective reinforcement learning has been studied, a survey is [484]. In this feld the more realistic assumption is adopted that agents have diferent rewards functions, leading to diferent Pareto optima [802, 537, 841]. Oliehoek et al. have written a concise introduction to decentralized multi-agent modelling [569, 568]. Counterfactual regret minimization has been fundamental for the success in computer poker [112, 113, 379, 881]. An often-used Monte Carlo version is published in [451]. A combination with function approximation is studied in [111]. Evolutionary algorithms have delivered highly successful optimization algorithms. Some entries to this vast feld are [224, 40, 42, 41, 43]. A related feld is swarm intelligence, where communication between homogeneous agents is taking place [406, 217, 207, 78]. For further research in multi-agent systems refer to [847, 793]. For collective intelligence, see, for example [406, 208, 258, 848, 599]. Many other works report on evolutionary algorithms in a reinforcement learning setting [731, 409, 536, 838, 148, 163, 653, 842]. Most of these approaches concern single agent approaches, although some are specifcally applied to multi agent approaches [498, 408, 713, 487, 374, 414]. Research into benchmarks is active. Among interesting approaches are Procedural content generation [781], MuJoCo Soccer [487], and the Obstacle Tower Challenge [384]. There is an extensive literature on computer poker. See, for example [89, 90, 277, 641, 53, 103, 90, 277, 104, 657, 533]. StarCraft research can be found in [815, 655, 813, 655, 573]. Other games studies are [759, 783]. Approaches inspired by results in poker and Go are now also being applied with success in no-press Diplomacy [584, 21]. Exercises Below are a few quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. Questions 1. Why is there so much interest in multi-agent reinforcement learning? 2. What is one of the main challenges of multi-agent reinforcement learning? 3. What is a Nash strategy? 224 7 Multi-Agent Reinforcement Learning 4. What is a Pareto Optimum? 5. In a competitive multi-agent system, what algorithm can be used to calculate a Nash strategy? 6. What makes it difcult to calculate the solution for a game of imperfect information? 7. Describe the Prisoner\u2019s dilemma. 8. Describe the iterated Prisoner\u2019s dilemma. 9. Name two multi-agent card games of imperfect information. 10. What is the setting with a heterogeneous reward function usually called? 11. Name three kinds of strategies that can occur a multi-agent reinforcement learning. 12. Name two solution methods that are appropriate for solving mixed strategy games. 13. What AI method is named after ant colonies, bee swarms, bird focks, or fsh schools? How does it work in general terms? 14. Describe the main steps of an evolutionary algorithm. 15. Describe the general form of Hide and Seek and three strategies that emerged from the interactions of the hiders or seekers. Exercises Here are some programming exercises to become more familiar with the methods that we have covered in this chapter. 1. CFR Implement counterfactual regret minimization for a Kuhn poker player. Play against the program, and see if you can win. Do you see possibilities to extend it to a more challenging version of poker? 2. Hide and Seek Implement Hide and Seek with cooperation and competition. Add more types of objects. See if other cooperation behavior emerges. 3. Ant Colony Use the DeepMind Control Suite and setup a collaborative level and a competitive level, and implement Ant Colony Optimization. Find problem instances on the web, or in the original paper [208]. Can you implement more swarm algorithms? 4. Football Go to the Google Football blog11 and implement algorithms for football agents. Consider using a population-based approach. 5. StarCraft Go to the StarCraft Python interface,12 and implement a StarCraft player (highly challenging) [655]. 11 https://ai.googleblog.com/2019/06/introducing-google-research-football.html 12 https://github.com/deepmind/pysc2 Chapter 8 Hierarchical Reinforcement Learning The goal of artifcial intelligence is to understand and create intelligent behavior; the goal of deep reinforcement learning is to fnd a behavior policy for ever larger sequential decision problems. But how does real intelligence fnd these policies? One of the things that humans are good at, is dividing a complex task into simpler subproblems, and then solving those tasks, one by one, and combining them as the solution to the larger problem. These subtasks are of diferent scales, or granularity, than the original problem. For example, when planning a trip from your house to a hotel room in a far-away city, you typically only plan the start and the end in terms of footsteps taken in a certain direction. The in-between part may contain diferent modes of transportation that get you to your destination quicker, with macro steps, such as taking a trainride or a fight. During this macro, you do not try out footsteps in diferent directions. Our trip\u2014our policy\u2014is a combination of fne-grain primitive actions and coarse-grain macro actions. Hierarchical reinforcement learning studies this real-world-inspired approach to problem solving. It provides formalisms and algorithms to divide problems into larger subproblems, and then plans with these subpolicies, as if they were subroutines. In principle the hierarchical approach can exploit structure in all sequential decision problems, although some problems are easier than others. Some environments can be subdivided into smaller problems in a natural way, such as navigational tasks on maps, or path-fnding tasks in mazes. Multi-agent problems also naturally divide into hierarchical teams, and have large state spaces where hierarchical methods may help. For other problems, however, it can be hard to fnd efcient macros, or it can be computationally intensive to fnd good combinations of macro steps and primitive steps. Another aspect of hierarchical methods is that since macro-actions take large steps, they may miss the global minimum. The best policies found by hierarchical methods may be less optimal than those found by \u201cfat\u201d approaches (although they may get there much quicker). 225 226 8 Hierarchical Reinforcement Learning In this chapter we will start with an example to capture the favor of hierarchical problem solving. Next, we will look at a theoretical framework that is used to model hierarchical algorithms, and at a few examples of algorithms. Finally, we will look deeper at hierarchical environments. The chapter ends with exercises, a summary, and pointers to further reading. Core Concepts \u2022 Solve large, structured, problems by divide and conquer \u2022 Temporal abstraction of actions with options Core Problem \u2022 Find subgoals and subpolicies efciently, to perform hierarchical abstraction Core Algorithms \u2022 Options framework (Sect. 8.2.1) \u2022 Option critic (Sect. 8.2.3.2) \u2022 Hierarchical actor critic (Sect. 8.2.3.2) Planning a Trip Let us see how we plan a major trip to visit a friend that lives in another city, with a hierarchical method. The method would break up the trip in diferent parts. The frst part would be to walk to your closet and get your things, and then to get out and get your bike. You would go to the train station, and park your bike. You would then take the train to the other city, possibly changing trains enroute if that would be necessary to get you there faster. Arriving in the city your friend would meet you at the station and would drive you to their house. A \u201cfat\u201d reinforcement learning method would have at its disposal actions consisting of footsteps in certain directions. This would make for a large space of possible policies, although the fne grain at which the policy would be planned\u2014individual footsteps\u2014would sure be able to fnd the optimal shortest route. The hierarchical method has at its disposal a wider variety of actions\u2014macro actions: it can plan a bike ride, a train trip, and getting a ride by your friend. The route may not be the shortest possible (who knows if the train follows the shortest 8.1 Granularity of the Structure of Problems 227 route between the two cities) but planning will be much faster than painstakingly optimizing footstep by footstep. 8.1 Granularity of the Structure of Problems In hierarchical reinforcement learning the granularity of abstractions is larger than the fne grain of the primitive actions of the environment. When we are preparing a meal, we reason in large action chunks: chop onion, cook spaghetti, instead of reasoning about the actuation of individual muscles in our hands and arms. Infants learn to use their muscles to performs certain tasks as they grow up until it becomes second nature. We generate subgoals that act as temporal abstractions, and subpolicies that are macro\u2019s of multiple ordinary actions [673]. Temporal abstraction allows us to reason about actions of diferent time scales, sometimes with course grain actions\u2014taking a train\u2014sometimes with fne grain actions\u2014opening a door\u2014mixing macro actions with primitive actions. Let us look at advantages and disadvantages of the hierarchical approach. 8.1.1 Advantages We will start with the advantages of hierarchical methods [247]. First of all, hierarchical reinforcement learning simplifes problems through abstraction. Problems are abstracted into a higher level of aggregation. Agents create subgoals and solve fne grain subtasks frst. Actions are abstracted into larger macro actions to solve these subgoals; agents use temporal abstraction. Second, the temporal abstractions increase sample efciency. The number of interactions with the environment is reduced because subpolicies are learned to solve subtasks, reducing the environment interactions. Since subtasks are learned, they can be transfered to other problems, supporting transfer learning. Third, subtasks reduce brittleness due to overspecialization of policies. Policies become more general, and are able to adapt to changes in the environment more easily. Fourth, and most importantly, the higher level of abstraction allows agents to solve larger, more complex problems. This is a reason why for complex multi-agent games such as StarCraft, where teams of agents must be managed, hierarchical approaches are used. Multi-agent reinforcement learning often exhibits a hierarchical structure; problems can be organized such that each agent is assigned its own subproblem, or the agents themselves may be structured or organized in teams or groups. There can be cooperation within the teams or competition between the teams, or the behavior can be fully cooperative or fully competitive. 228 8 Hierarchical Reinforcement Learning Flet-Berliac [247], in a recent overview, summarizes the promise of hierarchical reinforcement learning as follows: (1) achieve long-term credit assignment through faster learning and better generalization, (2) allow structured exploration by exploring with sub-policies rather than with primitive actions, and (3) perform transfer learning because diferent levels of hierarchy can encompass diferent knowledge. 8.1.2 Disadvantages There are also disadvantages and challenges associated with hierarchical reinforcement learning. First of all, it works better when there is domain knowledge available about structure in the domain. Many hierarchical methods assume that domain knowledge is available to subdivide the environment so that hierarchical reinforcement learning can be applied. Second, there is algorithmic complexity to be solved. Subgoals must be identifed in the problem environment, subpolicies must be learned, and termination conditions are needed. These algorithms must be designed, which costs programmer efort. Third, hierarchical approaches introduce a new type of actions, macro-actions. Macros are combinations of primitive actions, and their use can greatly improve the performance of the policy. On the other hand, the number of possible combinations of actions is exponentially large in their length [44]. For larger problems enumeration of all possible macros is out of the question, and the overall-policy function has to be approximated. Furthermore, at each decision point in a hierarchical planning or learning algorithm we now have the option to consider if any of the macro actions improves the current policy. The computational complexity of the planning and learning choices increases by the introduction of the macro actions [44], and approximation methods must be used. The efciency gains of the hierarchical behavioral policy must outweigh the higher cost of fnding this policy. Fourth, the quality of a behavioral policy that includes macro-actions may be less than that of a policy consisting only of primitive actions. The macro-actions may skip over possible shorter routes, that the primitive actions would have found. Conclusion There are advantages and disadvantages to hierarchical reinforcement learning. Whether an efcient policy can be constructed and whether its accuracy is good enough depends on the problem at hand, and also on the quality of the algorithms that are used to fnd this policy. For a long time, fnding good subgoals has been a major challenge. With recent algorithmic advances, especially in function approximation, important progress has been made. We will discuss these advances in the next section. 8.2 Divide and Conquer for Agents 229 8.2 Divide and Conquer for Agents To discuss hierarchical reinforcement learning, frst we will discuss a model, the options framework, that formalizes the concepts of subgoals and subpolicies. Next, we will describe the main challenge of hierarchical reinforcement learning, which is sample efciency. Then we will discuss the main part of this chapter: algorithms for fnding subgoals and subpolicies, and fnally we will provide an overview of algorithms that have been developed in the feld. 8.2.1 The Options Framework A hierarchical reinforcement learning algorithm tries to solve sequential decision problems more efciently by identifying common substructures and re-using subpolicies to solve them. The hierarchical approach has three challenges [619, 435]: fnd subgoals, fnd a meta-policy over these subgoals, and fnd subpolicies for these subgoals. Normally, in reinforcement learning, the agent follows in each state the action that is indicated by the policy. In 1999, Sutton, Precup and Singh [745] introduced the options framework. This framework introduces formal constructs with which subgoals and subpolicies can be incorporated elegantly into the reinforcement learning setting. The idea of options is simple. Whenever a state is reached that is a subgoal, then, in addition to following a primitive action suggested by the main policy, the option can be taken. This means that not the main action policy is followed, but the option policy, a macro action consisting of a diferent subpolicy specially aimed at satisfying the subgoal in one large step. In this way macros are incorporated into the reinforcement learning framework. We have been using the terms macro and option somewhat loosely until now; there is, however, a diference between macros and options. A macro is any group of actions, possibly open-ended. An option is a group of actions with a termination condition. Options take in environment observations and output actions until a termination condition is met. Formally, an option \ud835\udf14 has three elements [16]. Each option \ud835\udf14 = \u27e8\ud835\udc3c, \ud835\udf0b, \ud835\udefd\u27e9 has the following triple: \ud835\udc3c\ud835\udf14 The initiation set \ud835\udc3c \u2286 \ud835\udc46 are the states that the option can start from \ud835\udf0b\ud835\udf14(\ud835\udc4e|\ud835\udc60) The subpolicy \ud835\udf0b : \ud835\udc46 \u00d7 \ud835\udc34 \u2192 [0, 1] internal to this particular option \ud835\udefd\ud835\udf14(\ud835\udc60) The termination condition \ud835\udefd : \ud835\udc46 \u2192 [0, 1] tells us if \ud835\udf14 terminates in \ud835\udc60 The set of all options is denoted as \u03a9. In the options framework, there are thus two types of policies: the (meta-)policy over options \ud835\udf0b\u03a9(\ud835\udf14|\ud835\udc60) and the subpolicies \ud835\udf0b\ud835\udf14(\ud835\udc4e|\ud835\udc60). The subpolicies \ud835\udf0b\ud835\udf14 are short macros to get from \ud835\udc3c\ud835\udf14 to \ud835\udefd\ud835\udf14 quickly, using the previously learned macro (subpolicy). Temporal abstractions mix actions of diferent granularity, short and long, primitive action and subpolicy. They allow 230 8 Hierarchical Reinforcement Learning Fig. 8.1 Multi-Room Grid [745] traveling from \ud835\udc3c to \ud835\udefd without additional learning, using a previously provided or learned subpolicy. One of the problems for which the options framework works well, is room navigation in a Grid world (Fig. 8.1). In a regular reinforcement learning problem the agent would learn to move step by step. In hierarchical reinforcement learning the doors between rooms are bottleneck states, and are natural subgoals. Macro actions (subpolicies) are to move to a door in one multi-step action (without considering alternative actions along the way). Then we can go to a diferent room, if we choose the appropriate option, using another macro, closer to where the main goal is located. The four-room problem from the fgure is used in many research works in hierarchical reinforcement learning. Universal Value Function In the original options framework the process of identifying the subgoals (the hallways, doors) is external. The subgoals have to be provided manually, or by other methods [609, 316, 444, 727]. Subsequently, methods have been published to learn these subgoals. Options are goal-conditioned subpolicies. More recently a generalization to parameterized options has been presented in the universal value function, by Schaul et al. [665]. Universal value functions provide a unifed theory for goal-conditioned parameterized value approximators \ud835\udc49(\ud835\udc60, \ud835\udc54, \ud835\udf03). 8.2.2 Finding Subgoals Whether the hierarchical method improves over a traditional fat method depends on a number of factors. First, there should be enough repeating structure in the domain to be exploited (are there many rooms?), second, the algorithm must fnd 8.2 Divide and Conquer for Agents 231 Find Find Name Agent Environment Subg Subpol Ref STRIPS Macro-actions STRIPS planner [241] Abstraction Hier. State abstraction Scheduling/plan. + + [415] HAM Abstract machines MDP/maze [588] MAXQ Value function decomposition Taxi [196] HTN Task networks Block world [171, 272] Bottleneck Randomized search Four room + + [727] Feudal manager/worker, RNN Atari + + [811, 182] Self p. goal emb. self play subgoal Mazebase, AntG + + [732] Deep Skill Netw. deep skill array, policy distillation Minecraft + + [766] STRAW end-to-end implicit plans Atari + + [810] HIRO of-policy Ant maze + + [546] Option-critic policy-gradient Four room + + [45] HAC actor critic, hindsight exper. repl. Four room ant + + [471, 19] Modul. pol. hier. bit-vector, intrinsic motivation FetchPush + + [590] h-DQN intrinsic motivation Montezuma\u2019s R. + [435] Meta l. sh. hier. shared primitives, strength metric Walk, crawl + + [256] CSRL model-based transition dynamics Robot tasks + [477] Learning Repr. unsup. subg. disc., intrinsic motiv. Montezuma\u2019s R. + + [619] AMIGo Adversarially intrinsic goals MiniGrid PCG + + [125] Table 8.1 Hierarchical Reinforcement Learning Approaches (Tabular and Deep) appropriate subgoals (can it fnd the doors?), third, the options that are found must repeat many times (is the puzzle played frequently enough for the optionfnding cost to be ofset?), and, fnally, subpolicies must be found that give enough improvement (are the rooms large enough that options outweigh actions?). The original options framework assumes that the structure of the domain is obvious, and that the subgoals are given. When this is not the case, then the subgoals must be found by the algorithm. Let us look at an overview of approaches, both tabular and with deep function approximation. 8.2.3 Overview of Hierarchical Algorithms The options framework provides a convenient formalism for temporal abstraction. In addition to the algorithms that can construct policies consisting of individual actions, we need algorithms that fnd the subgoals, and learn the subpolicies. Finding efcient algorithms for the three tasks is important in order to be able to achieve an efciency advantage over ordinary \u201cfat\u201d reinforcement learning. Hierarchical reinforcement learning is based on subgoals. It implements a toplevel policy over these subgoals, and subpolicies to solve the subgoals. The landscape of subgoals determines to a great extent the efciency of the algorithm [216]. In recent years new algorithms have been developed to fnd sub-policies for options, and the feld has received a renewed interest [592]. Table 8.1 shows a list of approaches. 232 8 Hierarchical Reinforcement Learning The table starts with classic tabular approaches (above the line). It continues with more recent deep learning approaches. We will now look at some of the algorithms. 8.2.3.1 Tabular Methods Divide and conquer is a natural method to exploit hierarchical problem structures. A famous early planning system is STRIPS, the Stanford Research Insititute Problem Solver, designed by Richard Fikes and Nils Nilsson in the 1970s [241]. STRIPS created an extensive language for expressing planning problems, and was quite infuential. Concepts from STRIPS are at the basis of most modern planning systems, action languages, and knowledge representation systems [266, 51, 798]. The concept of macros as open-ended groups of actions was used in STRIPS to create higher-level primitives, or subroutines. Later planning-based approaches are Parr and Russell\u2019s hierarchical abstract machines [588] and Dietterich\u2019s MAXQ [196]. Typical applications of these systems are the blocks world, in which a robot arm has to manipulate blocks, stacking them on top of each other, and the taxi world, which we have seen in earlier chapters. An overview of these and other early approaches can be found in Barto et al. [56]. Many of these early approaches focused on macros (the subpolicies), and require that the experimenters identify the subgoals in a planning language. For problems where no such obvious subgoals are available, Knoblock [415] showed how abstraction hierarchies can be generated, although Backstrom et al. [44] found that doing so can be exponentially less efcient. For small room problems, however, Stolle and Precup [727] showed that subgoals can be found in a more efcient way, using a short randomized search to fnd bottleneck states that can be used as subgoals. This approach fnds subgoals automatically, and efciently, in a rooms-grid world. Tabular hierarchical methods were mostly applied to small and low-dimensional problems, and have difculty fnding subgoals, especially for large problems. The advent of deep function approximation methods attracted renewed interest in hierarchical methods. 8.2.3.2 Deep Learning Function approximation can potentially reduce the problem of exponentially exploding search spaces that plague tabular methods, especially for subgoal discovery. Deep learning exploits similarities between states using commonalities between features, and allows larger problems to be solved. Many new methods were developed. The deep learning approaches in hierarchical reinforcement learning typically are end-to-end: they generate both appropriate subgoals and their policies. Feudal networks is an older idea from Dayan and Hinton in which an explicit control hierarchy is built of managers and workers that work on tasks and subtasks, organized as in a feudal fefdom [182]. This idea was used 15 years later as a model for hierarchical deep reinforcement learning by Vezhnevets et al. [811], out8.2 Divide and Conquer for Agents 233 performing non-hierarchical A3C on Montezuma\u2019s Revenge, and performing well on other Atari games, achieving a similar score as Option-critic [45]. The approach uses a manager that sets abstract goals (in latent space) for workers. The feudal idea was also used as inspiration for a multi-agent cooperative reinforcement learning design [9], on proof of concept cooperative multi-agent problems on pre-specifed hierarchies. Other deep learning approaches include deep skill networks [766], of-policy approaches [546], and self-play [732]. The latter uses an intrinsic motivation approach to learn both a low level actor and the representation of the state space [597]. Subgoals are learned at the higher level, after which policies are trained at the lower level. Application environments for deep learning have become more challenging, and now include Minecraft, and robotic tasks such as ant navigation in multiple rooms, and maze navigation. The approaches outperform basic non-hierarchical approaches such as DQN. In STRAW Vezhnevets et al. [810] learns a model of macros-actions, and is evaluated on text recognition tasks and on Atari games such as PacMan and Frostbite, showing promising results. Zhang et al. [873] use world models to learn latent landmarks (subgoals) for graph-based planning (see also Sect. 5.2.1.2). Almost two decades after the options framework was introduced, Bacon et al. [45] introduced the option-critic approch. Option-critic extends the options framework with methods to learn the option subgoal and subpolicy, so that it does not have to be provided externally anymore. The options are learned similar to actor critic using a gradient-based approach. The intra-option policies and termination functions, as well as the policy over options are learned simultaneously. The user of the Option-critic approach has to specify how many options have to be learned. The Option-critic paper reports good results for experiments in a four-room environment with 4 and with 8 options (Fig. 8.2). Option critic learns options in an end-to-end fashion that scales to larger domains, outperforming DQN in four ALE games (Asterix, Seaquest, Ms. Pacman, Zaxxon) [45]. Levy et al. [471] presented an approach based on Option critic, called Hierarchical actor critic. This approach can learn the goal-conditioned policies at diferent levels concurrently, where previous approaches had to learn them in a bottom up fashion. In addition, Hierarchical actor critic uses a method for learning the multiple levels of policies for sparse rewards, using Hindsight experience replay [19]. In typical robotics tasks, the reinforcement learning algorithm learns more from a successful outcome (bat hits the ball) than from an unsuccessful outcome (bat misses the ball, low). In this failure case a human learner would draw the conclusion that we can now reach another goal, being bat misses the ball if we aim low. Hindsight experience replay allows learning to take place by incorporating such adjustments of the goal using the beneft of hindsight, so that the algorithm can now also learn from failures, by pretending that they were the goal that you wanted to reach, and learn from them as if they were. Hierarchical actor critic has been evaluated on grid world tasks and more complex simulated robotics environments, using a 3-level hierarchy. 234 8 Hierarchical Reinforcement Learning Fig. 8.2 Termination Probabilities Learned with 4 Options by Option-Critic [45]; Options Tend to Favor Squares Close to Doors A fnal approach that we mention is AMIGo [125], which is related to intrinsic motivation. It uses a teacher to adversarially generate goals for a student. The student is trained with increasingly challenging goals to learn general skills. The system efectively builds up an automatic curriculum of goals. It is evaluated on MiniGrid, a parameterized world that is generated by procedural content generation [621, 143]. Conclusion Looking back at the list of advantages and disadvantages at the start of this chapter, we see a range of interesting and creative ideas that achieve the advantages (Sect. 8.1.1) by providing methods to address the disadvantages (Sect. 8.1.2). In general, the tabular methods are restricted to smaller problems, and often need to be provided with subgoals. Most of the newer deep learning methods fnd subgoals by themselves, for which then subpolicies are found. Many promising methods have been discussed, and most report to outperform one or more fat baseline algorithms. The promising results stimulate further research in deep hierarchical methods, and more benchmark studies of large problems are needed. Let us have a closer look at the environments that have been used so far. 8.3 Hierarchical Environments 235 Fig. 8.3 Four Rooms, and One Room with Subpolicy and Subgoal [745] 8.3 Hierarchical Environments Many environments for hierarchical reinforcement learning exist, starting with mazes and the four-room environment from the options paper. Environments have evolved with the rest of the feld of reinforcement learning; for hierarchical reinforcement learning no clear favorite benchmark has emerged, although Atari en MuJoCo tasks are often used. In the following we will review some of the environments that are used in algorithmic studies. Most hierarchical environments are smaller than typically used for model-free fat reinforcement learning, although some studies do use complex environments, such as StarCraft. 8.3.1 Four Rooms and Robot Tasks Sutton et al. [745] presented the four rooms problems to illustrate how the options model worked (Fig. 8.3; left panel). This environment has been used frequently in subsequent papers on reinforcement learning. The rooms are connected by hallways. Options point the way to these hallways, which lead to the goal \ud835\udc3a2 of the environment. A hierarchical algorithm should identify the hallways as the subgoals, and create subpolicies for each room to go to the hallway subgoal (Fig. 8.3; right panel). The four-room environment is a toy environment with which algorithms can be explained. More complex versions can be created by increasing the dimensions of the grids and by increasing the number of rooms. The Hierarchical actor critic paper uses the four-room environment as a basis for a robot to crawl through. The agent has to learn both the locomotion task and solving the four-room problem (Fig. 8.4). Other environments that are used for hierarchical reinforcement learning are robot tasks, such as shown in Fig. 8.5 [633]. 236 8 Hierarchical Reinforcement Learning Fig. 8.4 Ant in Four Rooms [471] Fig. 8.5 Six Robot Tasks [633] 8.3.2 Montezuma\u2019s Revenge One of the most difcult situations for reinforcement learning is when there is little reward signal, and when it is delayed. The game of Montezuma\u2019s Revenge consists of long stretches in which the agent has to walk without the reward changing. Without smart exploration methods this game cannot be solved. Indeed, the game has long been a test bed for research into goal-conditioned and exploration methods. For the state in Fig. 8.6, the player has to go through several rooms while collecting items. However, to pass through doors (top right and top left corners), the player needs the key. To pick up the key, the player has to climb down the ladders 8.3 Hierarchical Environments 237 Fig. 8.6 Montezuma\u2019s Revenge [71] Fig. 8.7 Intrinsic Motivation in Reinforcement Learning [712] and move towards the key. This is a long and complex sequence before receiving the reward increments for collecting the key. Next, the player has to go to the door to collect another increase in reward. Flat reinforcement learning algorithms struggle with this environment. For hierarchical reinforcement the long stretches without a reward can be an opportunity to show the usefulness of the option, jumping through the space from states where the reward changes to another reward change. To do so, the algorithm has to be able to identify the key as a subgoal. Rafati and Noelle [619] learn subgoals in Montezuma\u2019s Revenge, and so do Kulkarni et al. [435]. Learning to choose promising subgoals is a challenging problem by itself. Once subgoals are found, the subpolicies can be learned by introducing a reward signal for achieving the subgoals. Such intrinsic rewards are related to intrinsic motivation and the psychological concept of curiosity [29, 577]. Figure 8.7 illustrates the idea behind intrinsic motivation. In ordinary reinforcement learning, a critic in the environment provides rewards to the agent. When the agent has an internal environment where an internal critic provides rewards, these internal rewards provide an intrinsic motivation to the agent. This mechanism aims to more closely model exploration behavior in animals and humans [712]. For example, during curiosity-driven activities, children use knowledge to generate intrinsic goals while playing, building block structures, etc. While doing this, they 238 8 Hierarchical Reinforcement Learning construct subgoals such as putting a lighter entity on top of a heavier entity in order to build a tower [435, 712]. Intrinsic motivation is an active feld of research. A recent survey is [29]. Montezuma\u2019s Revenge has also been used as benchmark for the Go-Explore algorithm, that has achieved good results in sparse reward problems, using a goalconditioned policy with cell aggregation [222]. Go-Explore performs a planning-like form of backtracking, combining elements of planning and learning in a diferent way than AlphaZero. 8.3.3 Multi-Agent Environments Many multi-agent problems are a natural match for hierarchical reinforcement learning, since agents often work together in teams or other hierarchical structure. For multi-agent hierarchical problems a multitude of diferent environments are used. Makar et al. [499, 273] study cooperative multi-agent learning, and use small tasks such as a two-agent cooperative trash collection task, the dynamic rescheduling of automated guided vehicles in a factory, as well as an environment in which agents communicate amongst eachother. Han et al. [312] use a multi-agent Taxi environment. Tang et al. [753] also use robotic trash collection. Due to the computational complexity of multi-agent and hierarchical environments, many of the environments are of lower dimensionality than what we see in model-free and model-based single-agent reinforcement learning. There are a few exceptions, as we saw in the previous chapter (Capture the Flag and StarCraft). However, the algorithms that were used for these environments were based on population-based self-play algorithms that are well-suited for parallelization; hierarchical reinforcement algorithms of the type that we have discussed in this chapter were of less importance [813]. 8.3.4 Hands On: Hierarchical Actor Citic Example The research reported in this chapter is of a more manageable scale than in some other chapters. Environments are smaller, computational demands are more reasonable. Four-room experiments and experiments with movement of single robot arms invites experimentation and tweaking. Again, as in the other chapters, the code of most papers can be found online on GitHub. Hierarchical reinforcement learning is well suited for experimentation because the environments are small, and the concepts of hierarchy, team, and subgoal, are intuitively appealing. Debgugging one\u2019s implementation should be just that bit easier when the desired behavior of the diferent pieces of code is clear. 8.3 Hierarchical Environments 239 Algorithm 8.1 Hierarchical Actor Critic [471] Input: Key agent parameters: number of levels in hierarchy \ud835\udc58, maximum subgoal horizon \ud835\udc3b, and subgoal testing frequency \ud835\udf06 Output: \ud835\udc58 trained actor and critic functions \ud835\udf0b0, ..., \ud835\udf0b\ud835\udc58\u22121, \ud835\udc440, ..., \ud835\udc44\ud835\udc58\u22121 for \ud835\udc40 episodes do \u22b2 Train for M episodes \ud835\udc60 \u2190 \ud835\udc46init, \ud835\udc54 \u2190 \ud835\udc3a\ud835\udc58\u22121 \u22b2 Sample initial state and task goal train-level(\ud835\udc58 \u2212 1, \ud835\udc60, \ud835\udc54) \u22b2 Begin training Update all actor and critic networks end for function train-level(\ud835\udc56 :: level, \ud835\udc60 :: state, \ud835\udc54 :: goal) \ud835\udc60\ud835\udc56 \u2190 \ud835\udc60, \ud835\udc54\ud835\udc56 \u2190 \ud835\udc54 \u22b2 Set current state and goal for level \ud835\udc56 for \ud835\udc3b attempts or until \ud835\udc54\ud835\udc5b, \ud835\udc56 \u2264 \ud835\udc5b < \ud835\udc58 achieved do \ud835\udc4e\ud835\udc56 \u2190 \ud835\udf0b\ud835\udc56 (\ud835\udc60\ud835\udc56, \ud835\udc54\ud835\udc56) + noise (if not subgoal testing) \u22b2 Sample (noisy) action from policy if \ud835\udc56 > 0 then Determine whether to test subgoal \ud835\udc4e\ud835\udc56 \ud835\udc60\u2032 \ud835\udc56 \u2190 train-level(\ud835\udc56 \u2212 1, \ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56) \u22b2 Train level \ud835\udc56 \u2212 1 using subgoal \ud835\udc4e\ud835\udc56 else Execute primitive action \ud835\udc4e0 and observe next state \ud835\udc60\u2032 0 end if \u22b2 Create replay transitions if \ud835\udc56 > 0 and \ud835\udc4e\ud835\udc56 missed then if \ud835\udc4e\ud835\udc56 was tested then \u22b2 Penalize subgoal \ud835\udc4e\ud835\udc56 Replay_Bufer\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f = Penalty, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 = \ud835\udc54\ud835\udc56, \ud835\udefe = 0] end if \ud835\udc4e\ud835\udc56 \u2190 \ud835\udc60\u2032 \ud835\udc56 \u22b2 Replace original action with action executed in hindsight end if \u22b2 Evaluate executed action on current goal and hindsight goals Replay_Bufer\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f \u2208 {\u22121, 0}, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 = \ud835\udc54\ud835\udc56, \ud835\udefe \u2208 {\ud835\udefe, 0}] HER_Storage\ud835\udc56 \u2190 [\ud835\udc60 = \ud835\udc60\ud835\udc56, \ud835\udc4e = \ud835\udc4e\ud835\udc56, \ud835\udc5f =TBD, \ud835\udc60\u2032 = \ud835\udc60\u2032 \ud835\udc56, \ud835\udc54 =TBD, \ud835\udefe =TBD] \ud835\udc60\ud835\udc56 \u2190 \ud835\udc60\u2032 \ud835\udc56 end for Replay_Bufer\ud835\udc56 \u2190 Perform HER using HER_Storage\ud835\udc56 transitions return \ud835\udc60\u2032 \ud835\udc56 \u22b2 Output current state end function To get you started with hierarchical reinforcement learning we will go to HAC: Hierarchical actor critic [471]. Algorithm 8.1 shows the pseudocode, where TBD is the subgoal in hindsight [471]. A blog1 with animations has been written, a video2 has been made of the results, and the code can be found in GitHub.3 To run the hierarchical actor critic experiments, you need MuJoCo and the required Python wrappers. The code is TensorFlow 2 compatible. When you have cloned the repository, run the experiment with 1 http://bigai.cs.brown.edu/2019/09/03/hac.html 2 https://www.youtube.com/watch?v=DYcVTveeNK0 3 https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC240 8 Hierarchical Reinforcement Learning python3 initialize_HAC.py --retrain which will train a UR5 reacher agent with a 3-level hierarchy. Here is a video4 that shows how it should look like after 450 training episodes. You can watch your trained agent with the command python3 initialize_HAC.py --test --show The README at the GitHub repository contains more suggestions on what to try. You can try diferent hyperparameters, and you can modify the designs, if you feel like it. Happy experimenting! Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary A typical reinforcement learning algorithm moves in small steps. For a state, it picks an action, gives it to the environment for a new state and a reward, and processes the reward to pick a new action. Reinforcement learning works step by small step. In contrast, consider the following problem: in the real world, when we plan a trip from A to B, we use abstraction to reduce the state space, to be able to reason at a higher level. We do not reason at the level of footsteps to take, but we frst decide on the mode of transportation to get close to our goal, and then we fll in the diferent parts of the journey with small steps. Hierarchical reinforcement learning tries to mimic this idea: conventional reinforcement learning works at the level of a single state; hierarchical reinforcement learning performs abstraction, solving subproblems in sequence. Temporal abstraction is described in a paper by Sutton et al. [745]. Hierarchical reinforcement learning uses the principles of divide and conquer to make solving large problems feasible. It fnds subgoals in the space that it solves with subpolicies (macros or options). Despite the appealing intuition, progress in hierarchical reinforcement learning was initially slow. Finding these new subgoals and subpolicies is a computationally intensive problems that is exponential in the number of actions, and in some situations it is quicker to use conventional \u201cfat\u201d reinforcement learning methods, unless domain knowledge can be exploited. The advent of deep learning provided a 4 https://www.youtube.com/watch?v=R86Vs9Vb6Bc 8.3 Hierarchical Environments 241 boost to hierarchical reinforcement learning, and much progress is being reported in important tasks such as learning subgoals automatically, and fnding subpolicies. Although popular for single-agent reinforcement learning, hierarchical methods are also used in multi-agent problems. Multi-agent problems often feature agents that work in teams, that cooperate within, and compete between the teams. Such an agent-hierarchy is a natural ft for hierachical solution methods. Hierarchical reinforcment learning remains a promising technique. Further Reading Hierarchical reinforcement learning, and subgoal fnding, have a rich and long history [248, 588, 745, 609, 196, 316, 444, 56, 592]; see also Table 8.1. Macro-actions are a basic approach [316, 626]. Others, using macros, are [855, 852, 214]. The options framework has provided a boost to the development of the feld [745]. Other approaches are MAXQ [195] and Feudal networks [811]. Earlier tabular approaches are [248, 588, 745, 609, 196, 316, 444]. Recent method are Option-critic [45] and Hierarchical actor-critic [471]. There are many deep learning methods for fnding subgoals and subpolicies [471, 582, 248, 598, 546, 256, 806, 665, 735, 177]. Andrychowicz et al. [19] introduce Hindsight experience replay, which can improve performance for hierarchical methods. Instrinsic motivation is a concept from developmental neuroscience that has come to reinforcement learning with the purpose of providing learning signals in large spaces. It is related to curiosity. Botvinick et al. [101] have written an overview of hierarchical reinforcement learning and neuroscience. Aubret et al. [29] provide a survey of intrinsic motivation for reinforcement learning. Instrinsic motivation is used by [435, 619]. Intrinsic motivation is closely related to goaldriven reinforcement learning [665, 650, 576, 577, 578]. Exercises It is time for the Exercises to test your knowledge. Questions Below are some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer should be sufcient. 1. Why can hierarchical reinforcement learning be faster? 2. Why can hierarchical reinforcement learning be slower? 3. Why may hierarchical reinforcement learning give an answer of lesser quality? 4. Is hierachical reinforcement more general or less general? 242 8 Hierarchical Reinforcement Learning 5. What is an option? 6. What are the three elements that an option consists of? 7. What is a macro? 8. What is intrinsic motivation? 9. How do multi agent and hierarchical reinforcement learning ft together? 10. What is so special about Montezuma\u2019s Revenge? Exercises Let us go to the programming exercises to become more familiar with the methods that we have covered in this chapter. 1. Four Rooms Implement a hierarchical solver for the four-rooms environment. You can code the hallway subgoals using domain knowledge. Use a simple tabular, planning, approach. How will you implement the subpolicies? 2. Flat Implement a fat planning or Q-learning-based solver for 4-rooms. Compare this program to the tabular hierarchical solver. Which is quicker? Which of the two does fewer environment actions? 3. Sokoban Implement a Sokoban solver using a hierarchical approach (challenging). The challenge in Sokoban is that there can be dead-ends in the game that you create rendering the game unsolvable (also see the literature [696, 291]). Recognizing these dead-end moves is important. What are the subgoals? Rooms, or each box-task is one subgoal, or can you fnd a way to code dead-ends as subgoal? How far can you get? Find Sokoban levels.567 4. Petting Zoo Choose one of the easier multi-agent problems from the Petting Zoo [761], introduce teams, and write a hierarchical solver. First try a tabular planning approach, then look at hierarchical actor critic (challenging). 5. StarCraft The same as the previous exercise, only now with StarCraft (very challenging). 5 http://sneezingtiger.com/sokoban/levels.html 6 http://www.sokobano.de/wiki/index.php?title=Level_format 7 https://www.sourcecode.se/sokoban/levels Chapter 9 Meta-Learning Although current deep reinforcement learning methods have obtained great successes, training times for most interesting problems are high; they are often measured in weeks or months, consuming time and resources\u2014as you may have noticed while doing some of the exercises at the end of the chapters. Model-based methods aim to reduce the sample complexity in order to speed up learning\u2014but still, for each new task a new network has to be trained from scratch. In this chapter we turn to another approach, that aims to re-use information learned in earlier training tasks from a closely related problem. When humans learn a new task, they do not learn from a blank slate. Children learn to walk and then they learn to run; they follow a training curriculum, and they remember. Human learning builds on existing knowledge, using knowledge from previously learned tasks to facilitate the learning of new tasks. In machine learning such transfer of previously learned knowledge from one task to another is called transfer learning. We will study it in this chapter. Humans learn continuously. When learning a new task, we do not start from scratch, zapping our minds frst to emptiness. Previously learned task-representations allow us to learn new representations for new tasks quickly; in efect, we have learned to learn. Understanding how we (learn to) learn has intrigued artifcial intelligence researchers since the early days, and it is the topic of this chapter. The felds of transfer learning and meta-learning are tightly related. For both, the goal is to speed up learning a new task, using previous knowledge. In transfer learning, we pretrain our parameter network with knowledge from a single task. In meta-learning, we use multiple related tasks. In this chapter, we frst discuss the concept of lifelong learning, something that is quite familiar to human beings. Then we discuss transfer learning, followed by meta-learning. Next, we discuss some of the benchmarks that are used to test transfer learning and meta-learning. The chapter is concluded with exercises, a summary, and pointers to further reading. 243 244 9 Meta-Learning Core Concepts \u2022 Knowledge transfer \u2022 Learning to learn Core Problem \u2022 Speed-up learning with knowledge from related tasks Core Algorithms \u2022 Pretraining (Listing 9.1) \u2022 Model-Agnostic Meta-Learning (Alg. 9.1) Foundation Models Humans are good at meta-learning. We learn new tasks more easily after we have learned other tasks. Teach us to walk, and we learn how to run. Teach us to play the violin, the viola, and the cello, and we more easily learn to play the double bass (Fig. 9.1). Current deep learning networks are large, with many layers of neurons and millions of parameters. For new problems, training large networks on large datasets or environments takes time, up to weeks, or months\u2014both for supervised and for reinforcement learning. In order to shorten training times for subsequent networks, these are often pretrained, using foundation models [96]. With pretraining, some of the exisiting weights of another network are used as starting point for fnetuning a network on a new dataset, instead of using a randomly initialized network. Pretraining works especially well on deeply layered architectures. The reason is that the \u201cknowledge\u201d in the layers goes from generic to specifc: lower layers contain generic flters such as lines and curves, and upper layers contain more specifc flters such as ears, noses, and mouths (for a face recognition application) [459, 497]. These lower layers contain more generic information that is well suited for transfer to other tasks. Foundation models are large models in a certain feld, such as image recognition, or natural language processing, that are trained extensively on large datasets. Foundation models contain general knowledge, that can be specialized for a certain purpose. The world of applied deep learning has moved from training a net from scratch for a certain problem, to taking a part of an existing net that is trained for a related problem and then fnetuning it on the new task. Nearly all state-of-the-art 9 Meta-Learning 245 Fig. 9.1 Violin, viola, cello and double bass Fig. 9.2 Imagenet Thumbnails [646] visual perception approaches rely on the same approach: (1) pretrain a convolutional network on a large, manually annotated image classifcation dataset and (2) fnetune the network on a smaller, task-specifc dataset [278, 200, 865, 8, 360], see Fig. 9.2 for some thumbnails of Imagenet [646]. For natural language recognition, pretraining is also the norm\u2014for example, for large-scale pretrained language models such as Word2vec [513, 514], BERT [193] and GPT-3 [617]. In this chapter, we will study pretraining, and more. 246 9 Meta-Learning Name Dataset Task Single-task Learning \ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \u2286 \ud835\udc37, \ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \u2286 \ud835\udc37 \ud835\udc47 = \ud835\udc47\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b = \ud835\udc47\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 Transfer Learning \ud835\udc371 \u226b \ud835\udc372 \ud835\udc471 \u2260 \ud835\udc472 Multi-task Learning \ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b \u2286 \ud835\udc37, \ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 \u2286 \ud835\udc37 \ud835\udc471 \u2260 \ud835\udc472 Domain Adaptation \ud835\udc371 \u2260 \ud835\udc372 \ud835\udc471 = \ud835\udc472 Meta-Learning {\ud835\udc371, . . . , \ud835\udc37\ud835\udc41\u22121} \u226b \ud835\udc37\ud835\udc41 \ud835\udc471, . . . , \ud835\udc47\ud835\udc5b\u22121 \u2260 \ud835\udc47\ud835\udc41 Table 9.1 Diferent Kinds of Supervised Learning. A typical single learning task is to classify pictures of animals into diferent classes. The dataset is split into a train and a testset, and the task (loss function) is the same at train and test time. A transfer learning task uses part of the knowledge (network parameters) that are learned on a large dataset to initialize a second network, that is trained subsequently (fne tuned) on a diferent dataset. This second dataset/learning task is related to the frst task, so the learning the second task goes faster. For example, having learned to recognize cars, may be useful to speedup recognizing trucks. In multi-task learning several related tasks are trained at the same time, possibly beneftting from better regularization. An example could be training spam flters for diferent users at the same time. Domain adaptation tries to adapt the network to a new dataset of related examples, such as images of pedestrians in diferent light conditions. Meta-learning tries to learn meta knowledge, such as hyperparameters over a sequence of related (larger) learning tasks, so that a new learning task goes faster. In deep meta-learning these hyperparameters include the initial network parameters, in this sense meta-learning can be considered to be multi-job transfer learning. 9.1 Learning to Learn Related Problems Training times for modern deep networks are large. Training AlexNet for ImageNet took 5-6 days on 2 GPUs in 2012 [431], see Sect. B.3.1. In reinforcement learning, training AlphaGo took weeks [703, 706], in natural language processing, training also takes a long time [193], even excessively long as in the case of GPT-3 [114]. Clearly, some solution is needed. Before we look closer at transfer learning, let us have a look at the bigger picture: lifelong learning. When humans learn a new task, learning is based on previous experience. Initial learning by infants of elementary skills in vision, speech, and locomotion takes years. Subsequent learning of new skills builds on the previously acquired skills. Existing knowledge is adapted, and new skills are learned based on previous skills. Lifelong learning remains a long-standing challenge for machine learning; in current methods the continuous acquisition of information often leads to interference of concepts or catastrophic forgetting [701]. This limitation represents a major drawback for deep networks that typically learn representations from stationary batches of training data. Although some advances have been made in narrow domains, signifcant advances are necessary to approach generally-applicable lifelong learning. Diferent approaches have been developed. Among the methods are metalearning, domain adaptation, multi-task learning, and pretraining. Table 9.1 lists these approaches, together with regular single task learning. The learning tasks are formulated using datasets, as in a regular supervised setting. The table shows 9.2 Transfer Learning and Meta-Learning Agents 247 how the lifelong learning methods difer in their training and test dataset, and the diferent learning tasks. The frst line shows regular single-task learning. For single-task learning, the training and test dataset are both drawn from the same distribution (the datasets do not contain the same examples, but they are drawn from the same original dataset and the data distribution is expected to be the same), and the task to perform is the same for training and test. In the next line, for transfer learning, networks trained on one dataset are used to speedup training for a diferent task, possibly using a much smaller dataset [581]. Since the datasets are not drawn from the same master dataset, their distribution will difer, and there typically is only an informal notion of how \u201crelated\u201d the datasets are. However, in practice transfer learning often provides signifcant speedups, and transfer learning, pretraining and fnetuning are currently used in many real-world training tasks, sometimes using large foundation models as a basis. In multi-task learning, more than one task is learned from one dataset [129]. The tasks are often related, such as classifcation tasks of diferent, but related, classes of images, or learning spam flters for diferent email-users. Regularization may be improved when a neural network is trained on related tasks at the same time [59, 151]. So far, our learning tasks were trying to speedup learning diferent tasks with related data. Domain adaptation switches this around: the task remains the same, but the data changes. In domain adaptation, a diferent dataset is used to perform the same task, such as recognizing pedestrians in diferent light conditions [782]. In meta-learning, both datasets and tasks are diferent, although not too diferent. In meta-learning, a sequence of datasets and learning tasks is generalized to learn a new (related) task quickly [106, 351, 363, 667]. The goal of meta-learning is to learn hyperparameters over a sequence of learning tasks. 9.2 Transfer Learning and Meta-Learning Agents We will now introduce transfer learning and meta-learning algorithms. Where in normal learning we would intialize our parameters randomly, in transfer learning we initialize them with (part of) the training results of another training task. This other task is related in some way to the new task, for example, a task to recognize images of dogs playing in a forest is initalized on a dataset of dogs playing in a park. The parameter transfer of the old task is called pretraining, the second phase, where the network learns the new task on the new dataset, is called fnetuning. The pretraining will hopefully allow the new task to train faster. Where transfer learning transfers knowledge from a single previous task, metalearning aims to generalize knowledge from multiple previous learning tasks. Metalearning tries to learn hyperparameters over these related learning tasks, that tell the algorithm how to learn the new task. Meta-learning thus aims to learn to learn. In deep meta-learning approaches, the initial network parameters are typically part 248 9 Meta-Learning of the hyperparameters. Note that in transfer learning we also use (part of) the parameters to speed up learning (fnetuning) a new, related task. We can say that deep meta-learning generalizes transfer learning by learning the initial parameters over not one but a sequence of related tasks [351, 364]. (Defnitions are still in fux, however, and diferent authors and diferent felds have diferent defnitions.) Transfer learning has become part of the standard approach in machine learning, meta-learning is still an area of active research. We will look into meta-learning shortly, after we have looked into transfer learning, multi-task learning, and domain adaptation. 9.2.1 Transfer Learning Transfer learning aims to improve the process of learning new tasks using the experience gained by solving similar problems [606, 774, 773, 585]. Transfer learning aims to transfer past experience of source tasks and use it to boost learning in a related target task [581, 879]. In transfer learning, we frst train a base network on a base dataset and task, and then we repurpose some of the learned features to a second target network to be trained on a target dataset and task. This process works better if the features are general, meaning suitable to both base and target tasks, instead of specifc to the base task. This form of transfer learning is called inductive transfer. The scope of possible models (model bias) is narrowed in a benefcial way by using a model ft on a diferent but related task. First we will look at task similarity, then at transfer learning, multi-task learning, and domain adaptation. 9.2.1.1 Task Similarity Clearly, pretraining works better when the tasks are similar [129]. Learning to play the viola based on the violin is more similar than learning the tables of multiplication based on tennis. Diferent measures can be used to measure the similarity of examples and features in datasets, from linear one-dimensional measures to nonlinear multi-dimensional measures. Common measures are the cosine similarity for real-valued vectors and the radial basis function kernel [752, 809], but many more elaborate measures have been devised. Similarity measures are also used to devise meta-learning algorithms, as we will see later. 9.2 Transfer Learning and Meta-Learning Agents 249 9.2.1.2 Pretraining and Finetuning When we want to transfer knowledge, we can transfer the weights of the network, and then start re-training with the new dataset. Please refer back to Table 9.1. In pretraining the new dataset is smaller than the old dataset \ud835\udc371 \u226b \ud835\udc372, and we train for a new task \ud835\udc471 \u2260 \ud835\udc472, which we want to train faster. This works when the new task is diferent, but similar, so that the old dataset \ud835\udc371 contains useful information for the new task \ud835\udc472. To learn new image recognition problems, it is common to use a deep learning model pre-trained for a large and challenging image classifcation task such as the ImageNet 1000-class photograph classifcation competition. Three examples of pretrained models include: the Oxford VGG Model, Google\u2019s Inception Model, Microsoft\u2019s ResNet Model. For more examples, see the Cafe Model Zoo,1 or other zoos2 where more pre-trained models are shared. Transfer learning is efective because the images were trained on a corpus that requires the model to make predictions on a large number of classes, requiring the model to be general, and since it efciently learns to extract features in order to perform well. Convolutional neural network features are more generic in lower layers, such as color blobs or Gabor flters, and more specifc to the original dataset in higher layers. Features must eventually transition from general to specifc in the last layers of the network [862]. Pretraining copies some of the layers to the new task. Care should be taken how much of the old task network to copy. It is relatively safe to copy the more general lower layers. Copying the more specifc higher layers may be detrimental to performance. In natural language processing a similar situation occurs. In natural language processing, a word embedding is used that is a mapping of words to a high-dimensional continuous vector where diferent words with a similar meaning have a similar vector representation. Efcient algorithms exist to learn these word representations. Two examples of common pre-trained word models trained on very large datasets of text documents include Google\u2019s Word2vec model [513] and Stanford\u2019s GloVe model [596]. 9.2.1.3 Hands-on: Pretraining Example Transfer learning and pretraining have become a standard approach to learning new tasks, especially when only a small dataset is present, or when we wish to limit training time. Let us have a look at a hands-on example that is part of the Keras distribution (Sect. B.3.3). The Keras transfer learning example provides a basic Imagenet-based approach, getting data from TensorFlow DataSets (TFDS). The example follows a supervised learning approach, although the learning and 1 https://caffe.berkeleyvision.org/model_zoo.html 2 https://modelzoo.co 250 9 Meta-Learning 1 base_model = keras. applications .Xception( 2 weights=\u2019imagenet \u2019, # Load weights pre -trained on ImageNet. 3 input_shape =(150 , 150, 3), 4 include_top=False) # Do not include the ImageNet classifier at the top. 5 6 base_model.trainable = False Listing 9.1 Pretraining in Keras (1): instantiate model fne-tuning phase can easily be substituted by a reinforcement learning setup. The Keras transfer learning example is at the Keras site,3 and can also be run in a Google Colab. The most common incarnation of transfer learning in the context of deep learning is the following worfklow: 1. Take layers from a previously trained model. 2. Freeze them, so as to avoid destroying any of the information they contain during future training rounds. 3. Add some new, trainable layers on top of the frozen layers. They will train on the new dataset using the old features as predictions. 4. Train the new layers on your new (small) dataset. 5. A last, optional, step, is fne-tuning of the frozen layers, which consists of unfreezing the entire model you obtained above, and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data. Let us look at how this workfow works in practice in Keras. At the Keras site we fnd an accessible example code for pretraining (see Listing 9.1). First, we instantiate a base model with pretrained weights. (We do not include the classifer on top.) Then, we freeze the base model (see Listing 9.2). Next, we create a new model on top, and train it. The Keras example contains this example and others, including fne-tuning. Please go to the Keras site and improve your experience with pretraining in practice. 9.2.1.4 Multi-task Learning Multi-task learning is related to transfer learning. In multi-task learning a single network is trained at the same time on multiple related tasks [772, 129]. In multi-task learning the learning process of one task benefts from the simultaneous learning of the related task. This approach is efective when the tasks have some commonality, such as learning to recognize breeds of dogs and breeds of cats. 3 https://keras.io/guides/transfer_learning/ 9.2 Transfer Learning and Meta-Learning Agents 251 1 inputs = keras.Input(shape =(150 , 150, 3)) 2 # The base_model is running in inference mode here , 3 # by passing \u2018training=False \u2018. This is important for fine -tuning 4 x = base_model(inputs , training=False) 5 # Convert features of shape \u2018base_model . output_shape [1:] \u2018 to vectors 6 x = keras.layers. GlobalAveragePooling2D ()(x) 7 # A Dense classifier with a single unit (binary classification ) 8 outputs = keras.layers.Dense (1)(x) 9 model = keras.Model(inputs , outputs) 10 11 12 model.compile(optimizer=keras. optimizers .Adam (), 13 loss=keras.losses. BinaryCrossentropy ( from_logits = True), 14 metrics =[ keras.metrics. BinaryAccuracy ()]) 15 model.fit(new_dataset , epochs =20, callbacks =... , validation_data =...) Listing 9.2 Pretraining in Keras (2): create new model and train In multi-task learning, related learning tasks are learned at the same time, whereas in transfer learning they are learned in sequence by diferent networks. Multi-task learning improves regularization by requiring the algorithm to perform well on a related learning task instead of penalizing all overftting uniformly [233, 26]. The two-headed AlphaGo Zero network optimizes for value and for policy at the same time in the same network [581, 129]. A multi-headed architecture is often used in multi-task learning, although in AlphaGo Zero the two heads are trained for two related aspects (policy and value) of the same task (playing Go games). Multi-task learning has been applied with success to Atari games [405, 404]. 9.2.1.5 Domain Adaptation Domain adaptation is necessary when there is a change in the data distribution between the training dataset and the test dataset (domain shift). This problem is related to out-of-distribution learning [470]. Domain shifts are common in practical applications of artifcial intelligence, such as when items must be recognized in diferent light conditions, or when the background changes. Conventional machinelearning algorithms often have difculty adapting to such changes. The goal of domain adaptation is to compensate for the variation among two data distributions, to be able to reuse information from a source domain on a diferent target domain [782], see Fig. 9.3 for a backpack in diferent circumstances. As was indicated in Table 9.1, domain adaptation applies to the situation where the tasks are the same \ud835\udc471 = \ud835\udc472 but the datasets are diferent \ud835\udc371 \u2260 \ud835\udc372, although still somewhat similar. For example, the task may be to recognize a backpack, but in a diferent orientation, or to recognize a pedestrian, but in diferent lighting. 252 9 Meta-Learning Fig. 9.3 Domain Adaptation: Recognizing Items in Diferent Circumstances is Difcult [301] Domain adaptation can be seen as the opposite of pretraining. Pretraining uses the same dataset for a diferent task, while domain adaptation adapts to a new dataset for the same task [127]. In natural language processing, examples of domain shift are an algorithm that has been trained on news items that is then applied to a dataset of biomedical documents [179, 733], or a spam flter that is trained on a certain group of email users, which is deployed to a new target user [74]. Sudden changes in the environment (pandemics, severe weather) can also upset machine learning algorithms. There are diferent techniques to overcome domain shift [168, 872, 856, 834]. In visual applications, adaptation can be achieved by re-weighting the samples of the frst dataset, or clustering them for visually coherent sub-domains. Other approaches try to fnd transformations that map the source distribution to the target, or learn a classifcation model and a feature transformation jointly [782]. Adversarial techniques where feature representations are encouraged to be difcult to distinguish can be used to achieve adaptation [792, 850, 211], see also Sect. B.2.6. 9.2.2 Meta-Learning Related to transfer learning is meta-learning. Where the focus in transfer learning is on transferring parameters from a single donor task to a receiver task for further fnetuning, in meta-learning the focus is on using the knowledge of a number of tasks to learn how to learn a new task faster and better. Regular machine learning 9.2 Transfer Learning and Meta-Learning Agents 253 learns examples for one task, meta-learning aims to learn across tasks. Machine learning learns parameters that approximate the function, meta-learning learns hyperparameters about the learning-function.4 This is often expressed as learning to learn, a phrase that has defned the feld ever since its introduction [669, 772, 774]. The term meta-learning has been used for many diferent contexts, not only for deep learning, but also for tasks ranging from hyperparameter optimization, algorithm selection, to automated machine learning. (We will briefy look at these in Sect. 9.2.2.5.) Deep meta reinforcement learning is an active area of research. Many algorithms are being developed, and much progress is being made. Table 9.2 lists nine algorithms that have been proposed for deep meta reinforcement learning (see the surveys [351, 364]). 9.2.2.1 Evaluating Few-Shot Learning Problems One of the challenges of lifelong machine learning is to judge the performance of an algorithm. Regular train-test generalization does not capture the speed of adaptation of a meta-learning algorithm. For this reason, meta-learning tasks are typically evaluated on their few-shot learning ability. In few-shot learning, we test if a learning algorithm can be made to recognize examples from classes from which it has seen only few examples in training. In few-shot learning prior knowledge is available in the network. To translate few-shot learning to a human setting, we can think of a situation where a human plays the double bass after only a few minutes of training on the double bass, but after years on the violin, viola, or cello. Meta-learning algorithms are often evaluated with few shot learning tasks, in which the algorithm must recognize items of which it has only seen a few examples. This is formalized in the \ud835\udc41-way-\ud835\udc58-shot approach [141, 445, 829]. Figure 9.4 illustrates this process. Given a large dataset D, a smaller training dataset \ud835\udc37 is sampled from this dataset. The \ud835\udc41-way-\ud835\udc58-shot classifcation problem constructs training dataset \ud835\udc37 such that it consists of \ud835\udc41 classes, of which, for each class, \ud835\udc58 examples are present in the dataset. Thus, the cardinality of |\ud835\udc37| = \ud835\udc41 \u00b7 \ud835\udc58. A full \ud835\udc41-way-\ud835\udc58-shot few-shot learning meta task T consists of many episodes in which base tasks T\ud835\udc56 are performed. A base task consists of a training set and a test set, to test generalization. In few-shot terminology, the training set is called the support set, and the test set is called the query set. The support set has size \ud835\udc41 \u00b7 \ud835\udc58, the query set consists of a small number of examples. The meta-learning algorithm can learn from the episodes of \ud835\udc41-way-\ud835\udc58-shot query/support base tasks, until at meta-test time the generalization of the meta-learning algorithm is tested with another query, as is illustrated in the fgure. 4 Associating base learning with parameter learning, and meta-learning wth hyperparameter learning appears to give us a clear distinction; however, in practice the distinction is not so clear cut: in deep meta-learning the initialization of the regular parameters is considered to be an important \u201chyper\u201dparameter. 254 9 Meta-Learning Fig. 9.4 \ud835\udc41 -way-\ud835\udc58-shot learning [98] Name Approach Environment Ref Recurr. ML Deploy recurrent networks on RL problems [213, 826] Meta Netw. Fast reparam. of base-learner by distinct meta-learner O-glot, miniIm. [542] SNAIL Attention mechanism coupled with temporal conv. O-glot, miniIm. [517] LSTM ML Embed base-learner parameters in cell state of LSTM miniImageNet [629] MAML Learn initialization weights \ud835\udf03 for fast adaptation O-glot, miniIm. [243] iMAML Approx. higher-order gradients, indep. of optim. path O-glot, miniIm. [623] Meta SGD Learn both the initialization and updates O-glot, miniIm. [476] Reptile Move init. towards task-specifc updated weights O-glot, miniIm. [561] BayesMAML Learn multiple initializations \u0398, jointly optim. SVGD miniImagaNet [861] Table 9.2 Meta Reinforcement Learning Approaches [364] 9.2.2.2 Deep Meta-Learning Algorithms We will now turn our attention to deep meta-learning algorithms. The meta-learning feld is still young and active, nevertheless, the feld is converging on a set of defnitions that we will present here. We start our explanation in a supervised setting. Meta-learning is concerned with learning a task T from a set of base-learning tasks {T1, T2, T3, . . . } so that a new (related) meta-test-task will reach a high accuracy quicker. Each base-learning task T\ud835\udc56 consists of a dataset \ud835\udc37\ud835\udc56 and a learning objective, the loss function L\ud835\udc56. Thus we get T\ud835\udc56 = (\ud835\udc37\ud835\udc56, L\ud835\udc56). Each dataset consists of pairs of inputs and labels \ud835\udc37\ud835\udc56 = {(\ud835\udc65 \ud835\udc57, \ud835\udc66 \ud835\udc57)}, and is split into a training and a testset \ud835\udc37\ud835\udc56 = {\ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b, \ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61}. On each training dataset a parameterized model \u02c6\ud835\udc53\ud835\udf03\ud835\udc56 (\ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) is approximated, with a loss function L\ud835\udc56(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b). The model \u02c6\ud835\udc53 is approximated with a deep learning algorithm, that is governed by a set of hyperparameters \ud835\udf14. The particular hyperparameters vary from algorithm to algorithm, 9.2 Transfer Learning and Meta-Learning Agents 255 but frequently encountered hyperparameters are the learning rate \ud835\udefc, the initial parameters \ud835\udf030, and algorithm constants. This conventional machine learning algorithm is called the base learner. Each base learner task approximates a model \u02c6\ud835\udc53\ud835\udc56 by fnding the optimal parameters \ud835\udf03\u2605 \ud835\udc56 to minimize the loss function on its data set T\ud835\udc56 = \u02c6\ud835\udc53\ud835\udf03\u2605 \ud835\udc56 = arg min \ud835\udf03\ud835\udc56 L\ud835\udc56,\ud835\udf14(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) while the learning algorithm is governed by hyperparameters \ud835\udf14. Inner and Outer Loop Optimization One of the most popular deep meta-learning approaches of the last few years is optimization-based meta-learning [364]. This approach optimizes the initial parameters \ud835\udf03 of the network for fast learning of new tasks. Most optimization-based techniques do so by approaching meta-learning as a two-level optimization problem. At the inner level, a base learner makes task-specifc updates to \ud835\udf03 for the diferent observations in the training set. At the outer level, the meta-learner optimizes hyperparameters \ud835\udf14 across a sequence of base tasks where the loss of each task is evaluated using the test data from the base tasks \ud835\udc37 T\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 [629, 351, 463]. The inner loop optimizes the parameters \ud835\udf03, and the outer loop optimizes the hyperparameters \ud835\udf14 to fnd the best performance on the set of base tasks \ud835\udc56 = 0, . . . , \ud835\udc40 with the appropriate test data: \ud835\udf14\u2605 = arg min \ud835\udf14 Lmeta \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd outer loop (arg min \ud835\udf03\ud835\udc56 Lbase \ud835\udf14 (\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd inner loop , \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61). The inner loop optimizes \ud835\udf03\ud835\udc56 within the datasets \ud835\udc37\ud835\udc56 of the tasks T\ud835\udc56 , and the outer loop optimizes \ud835\udf14 across the tasks and datasets. The meta loss function optimizes for the meta objective, which can be accuracy, speed, or another goal over the set of base tasks (and datasets). The outcome of the meta optimization is a set of optimal hyperparameters \ud835\udf14\u2605. In optimization-based meta-learning the most important hyperparameters \ud835\udf14 are the optimal initial parameters \ud835\udf03\u2605 0. When the meta-learner only optimizes the initial parameters as hyperparameters (\ud835\udf14 = \ud835\udf030) then the inner/outer formula simplifes as follows: \ud835\udf03\u2605 0 = arg min \ud835\udf030 L \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd outer loop (arg min \ud835\udf03\ud835\udc56 L(\ud835\udf03\ud835\udc56, \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd inner loop , \ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61). In this approach we meta-optimize the initial parameters \ud835\udf030, such that the loss function performs well on the test data of the base tasks. Section 9.2.2.4 describes MAML, a well-known example of this approach. 256 9 Meta-Learning Fig. 9.5 Workfow of recurrent meta-learners in reinforcement learning contexts. State, action, reward, and termination fag at time step \ud835\udc61 are denotednby \ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, and \ud835\udc51\ud835\udc61, \u210e\ud835\udc61 refers to the hidden state [213]. Deep meta-learning approaches are sometimes categorized as (1) similaritymetric-based, (2) model-based, and (3) optimization-based [364]. We will now have a closer look at two of the nine meta reinforcement learning algorithms from Table 9.2. We will look at Recurrent meta-learning and MAML; the former is a model-based approach, the latter optimization-based. 9.2.2.3 Recurrent Meta-Learning For meta reinforcement learning approaches to be able to learn to learn, they must be able to remember what they have learned across subtasks. Let us see how Recurrent meta-learning learns across tasks. Recurrent meta-learning uses recurrent neural networks to remember this knowledge [213, 826]. The recurrent network serves as dynamic storage for the learned task embedding (weight vector). The recurrence can be implemented by an LSTM [826] or by gated recurrent units [213]. The choice of recurrent neural meta-network (meta-RNN) determines how well it adapts to the subtasks, as it gradually accumulates knowledge about the base-task structure. Recurrent meta-learning tracks variables \ud835\udc60, \ud835\udc4e, \ud835\udc5f, \ud835\udc51 which denote state, action, reward, and termination of the episode. For each task T\ud835\udc56, Recurrent meta-learning inputs the set of environment variables {\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc51\ud835\udc61} into a meta-RNN at each time step \ud835\udc61. The meta-RNN outputs an action and a hidden state \u210e\ud835\udc61. Conditioned on the hidden state \u210e\ud835\udc61, the meta network outputs action \ud835\udc4e\ud835\udc61. The goal is to maximize the expected reward in each trial (Fig. 9.5). Since Recurrent meta-learning embeds information from previously seen inputs in hidden state, it is regarded as a modelbased meta-learner [364]. Recurrent meta-learners performed almost as well as model-free baselines on simple \ud835\udc41-way-\ud835\udc58-shot reinforcement learning tasks [826, 213]. However, the performance degrades in more complex problems, when dependencies span a longer horizon. 9.2 Transfer Learning and Meta-Learning Agents 257 Fig. 9.6 The Optimization approach aims to learn parameters from which other tasks can be learned quickly. The intuition behind Optimization approaches such as MAML is that when our meta-training set consists of tasks A, B, C, and D, then if our meta-learning algorithm adjusts parameters \ud835\udc4e and \ud835\udc4f to (2, 2), then they are close to either of the four tasks, and can be adjustly quickly to them, with few examples (after [364, 243]). 9.2.2.4 Model-Agnostic Meta-Learning The Model-agnostic meta-learning approach (MAML) [243] is an optimization approach that is model-agnostic: it can be used for diferent learning problems, such as classifcation, regression, and reinforcement learning. As mentioned, the optimization view of meta-learning is especially focused on optimizing the initial parameters \ud835\udf03. The intuition behind the optimization view can be illustrated with a simple regression example (Fig. 9.6). Let us assume that we are faced with multiple linear regression problems \ud835\udc53\ud835\udc56(\ud835\udc65). The model has two parameters: \ud835\udc4e and \ud835\udc4f, \u02c6\ud835\udc53 (\ud835\udc65) = \ud835\udc4e \u00b7 \ud835\udc65 + \ud835\udc4f. When the meta-training set consists of four tasks, A, B, C, and D, then we wish to optimize to a single set of parameters {\ud835\udc4e, \ud835\udc4f} from which we can quickly learn the optimal parameters for each of the four tasks. In Fig. 9.6 the point in the middle represents this combination of parameters. The point is the closest to the four diferent tasks. This is how Model-agnostic meta-learning works [243]: by exposing our model to various base tasks, we update the parameters \ud835\udf03 = {\ud835\udc4e, \ud835\udc4f} to good initial parameters \ud835\udf030 that facilitate quick meta-adaptation. Let us look at the process of training a deep learning model\u2019s parameters from a feature learning standpoint [243, 364], where the goal is that a few gradient steps can produce good results on a new task. We build a feature representation that is broadly suitable for many tasks, and by then fne-tuning the parameters slightly (primarily updating the top layer weights) we achieve good results\u2014not unlike transfer learning. MAML fnds parameters \ud835\udf03 that are easy and fast to fnetune, allowing the adaptation to happen in an embedding space that is well suited for 258 9 Meta-Learning Algorithm 9.1 MAML for Reinforcement Learning [243] Require: \ud835\udc5d(T): distribution over tasks Require: \ud835\udefc, \ud835\udefd: step size hyperparameters randomly initialize \ud835\udf03 while not done do Sample batch of tasks T\ud835\udc56 \u223c \ud835\udc5d(T) for all T\ud835\udc56 do Sample \ud835\udc58 trajectories D = {(\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ) } using \ud835\udc53\ud835\udf03 in T\ud835\udc56 Evaluate \u2207\ud835\udf03 LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03) using D and LT\ud835\udc56 in Equation 9.1 Compute adapted parameters with gradient descent: \ud835\udf03\u2032 \ud835\udc56 = \ud835\udf03 \u2212 \ud835\udefc\u2207\ud835\udf03 LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03) Sample trajectories D\u2032 \ud835\udc56 = {(\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ) } using \ud835\udc53\ud835\udf03\u2032 \ud835\udc56 in T\ud835\udc56 end for Update \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udefd\u2207\ud835\udf03 \ufffd T\ud835\udc56\u223c\ud835\udc5d(T) LT\ud835\udc56 ( \ud835\udf0b\ud835\udf03\u2032 \ud835\udc56) using each D\u2032 \ud835\udc56 and LT\ud835\udc56 in Eq. 9.1 end while fast learning. To put it another way, MAML\u2019s goal is to fnd the point in the middle of Fig. 9.6, from where the other tasks are easily reachable. How does MAML work [363, 620, 243]? Please refer to the pseudocode for MAML in Alg. 9.1. The learning task is an episodic Markov decision process with horizon \ud835\udc47, where the learner is allowed to query a limited number of sample trajectories for few-shot learning. Each reinforcement learning task T\ud835\udc56 contains an initial state distribution \ud835\udc5d\ud835\udc56(\ud835\udc601) and a transition distribution \ud835\udc5d\ud835\udc56(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). The loss LT\ud835\udc56 corresponds to the (negative) reward function \ud835\udc45. The model being learned, \ud835\udf0b\ud835\udf03, is a policy from states \ud835\udc60\ud835\udc61 to a distribution over actions \ud835\udc4e\ud835\udc61 at each timestep \ud835\udc61 \u2208 {1, ...,\ud835\udc47}. The loss for task T\ud835\udc56 and policy \ud835\udf0b\ud835\udf03 takes the familiar form of the objective (Eq. 2.5): LT\ud835\udc56 (\ud835\udf0b\ud835\udf03) = \u2212E\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u223c\ud835\udf0b\ud835\udf03 ,\ud835\udc5dT\ud835\udc56 \ufffd \ud835\udc47 \u2211\ufe01 \ud835\udc61=1 \ud835\udc45\ud835\udc56(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61+1) \ufffd . (9.1) In \ud835\udc58-shot reinforcement learning, \ud835\udc58 rollouts from \ud835\udf0b\ud835\udf03 and task T\ud835\udc56, (\ud835\udc601, \ud835\udc4e1, ...\ud835\udc60\ud835\udc47 ), and the rewards \ud835\udc45(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61), may be used for adaptation on a new task T\ud835\udc56. MAML uses TRPO to estimate the gradient both for the policy gradient update(s) and the meta optimization [681]. The goal is to quickly learn new concepts, which is equivalent to achieving a minimal loss in few gradient update steps. The number of gradient steps has to be specifed in advance. For a single gradient update step gradient descent produces updated parameters \ud835\udf03\u2032 \ud835\udc56 = \ud835\udf03 \u2212 \ud835\udefc\u2207\ud835\udf03LT\ud835\udc56 (\ud835\udf0b\ud835\udf03) specifc to task \ud835\udc56. The meta loss of one gradient step across tasks is \ud835\udf03 \u2190 \ud835\udf03 \u2212 \ud835\udefd\u2207\ud835\udf03 \u2211\ufe01 T\ud835\udc56\u223c\ud835\udc5d(T) LT\ud835\udc56 (\ud835\udf0b\ud835\udf03\u2032 \ud835\udc56) (9.2) 9.2 Transfer Learning and Meta-Learning Agents 259 where \ud835\udc5d(T) is a probability distribution over tasks. This expression contains an inner gradient \u2207\ud835\udf3dLT\ud835\udc56 (\ud835\udf0b\ud835\udf03\u2032 \ud835\udc56). Optimizing this meta loss requires computing secondorder gradients when backpropagating the meta gradient through the gradient operator in the meta objective (Eq. 9.2), which is computationally expensive [243]. Various algorithms have been inspired by MAML and aim to improve optimizationbased meta-learning further [561, 363]. In meta reinforcement learning the goal is to quickly fnd a policy for a new environment using only a small amount of experience. MAML has gained attention within the feld of deep meta-learning, due to its simplicity (it requires two hyperparameters), its general applicability, and its strong performance. 9.2.2.5 Hyperparameter Optimization Meta-learning has been around for a long time, long before deep learning became popular. It has been applied to classic machine learning tasks, such as regression, decision trees, support vector machines, clustering algorithms, Bayesian networks, evolutionary algorithms, and local search [93, 106, 812]. The hyperparameter view on meta-learning originated here. Although this is a book about deep learning, it is interesting to briefy discuss this non-deep background, also because hyperparameter optimization is an important technology to fnd a good set of hyperparameters in reinforcement learning experiments. Machine learning algorithms have hyperparameters that govern their behavior, and fnding the optimal setting for these hyperparameters has long been called meta-learning. A naive approach is to enumerate all combinations and run the machine learning problem for them. For all but the smallest hyperparameter spaces such a grid search will be prohibitively slow. Among the smarter meta-optimization approaches are random search, Bayesian optimization, gradient-based optimization, and evolutionary optimization. This meta-algorithm approach has given rise to algorithm confguration research, such as SMAC [366],5 ParamILS [367],6 irace [488],7 and algorithm selection research [630, 407], such as SATzilla [854].8 Well-known hyperparameter optimization packages include scikit-learn [595],9 scikit-optimize,10 nevergrad [627],11 and optuna [11].12 Hyperparameter optimization and algorithm confguration have 5 https://github.com/automl/SMAC3 6 http://www.cs.ubc.ca/labs/beta/Projects/ParamILS/ 7 http://iridia.ulb.ac.be/irace/ 8 http://www.cs.ubc.ca/labs/beta/Projects/SATzilla/ 9 https://scikit-learn.org/stable/ 10 https://scikit-optimize.github.io/stable/index.html 11 https://code.fb.com/ai-research/nevergrad/ 12 https://optuna.org 260 9 Meta-Learning grown into the feld of automated machine learning, or AutoML.13 The AutoML feld is a large and active feld of meta-learning, an overview book is [368]. All machine learning algorithms have a bias, diferent algorithms perform better on diferent types of problems. Hyperparameters constrain this algorithm bias. This so-called inductive bias refects the set of assumptions about the data on which the algorithms are based. Learning algorithms perform better when this bias matches the learning problem (for example: CNNs work well on image problems, RNNs on language problems). Meta-learning changes this inductive bias, either by choosing a diferent learning algorithm, by changing the network initialization, or by other means, allowing the algorithm to be adjusted to work well on diferent problems. 9.2.2.6 Meta-Learning and Curriculum Learning There is in interesting connection between meta-learning and curriculum learning. Both approaches aim to improve the speed and accuracy of learning, by learning from a set of subtasks. In meta-learning, knowledge is gained from subtasks, so that the learning of a new, related, task, can be quick. In curriculum learning (Sect. 6.2.3.1) we aim to learn quicker by dividing a large and difcult learning task into a set of subtasks, ordered from easy to hard. Thus we can conclude that curriculum learning is a form of meta-learning where the subtasks are ordered from easy to hard, or, equivalently, that meta-learning is unordered curriculum learning. 9.2.2.7 From Few-Shot to Zero-Shot Learning Meta-learning uses information from previous learning tasks to learn new tasks quicker [774]. Meta-learning algorithms are often evaluated in a few-shot setting, to see how well they do in image classifcation problems when they are shown only few training examples. This few-shot learning problem aims to correctly classify queries with little previous support for the new class. In the previous sections we have discussed how meta-learning algorithms aim to achieve few-shot learning. A discussion of meta-learning would not be complete without mentioning zero-shot learning. Zero-shot learning (ZSL) goes a step further than few-shot learning. In zero-shot learning an example has to be recognized as belonging to a class without ever having been trained on an example of this class [454, 580]. In zero-shot learning the classes covered by training instances and the classes we aim to classify are disjoint. This may sound like an impossibility\u2014how can you recognize something you have never seen before?\u2014yet it is something that we, humans, do all the time: having learned to pour cofee in a cup, we can also pour tea, even if we never have seen 13 https://www.automl.org 9.3 Meta-Learning Environments 261 tea before. (Or, having learned to play the violin, the viola, and the cello, we can play the double bass, to some degree, even if we have never played the double bass before.) Whenever we recognize something that we have not seen before, we are actually using extra information (or features). If we recognize a red beak in a picture of a bird-species that we have never seen before, then the concepts \u201cred\u201d and \u201cbeak\u201d are known to us, because we have learned them in other contexts. Zero-shot learning recognizes new categories of instances without training examples. Attribute-based zero-shot learning uses separate high-level attribute descriptions of the new categories, based on categories previously learned in the dataset. Attributes are an intermediate representation that enables parameter sharing between classes [10]. The extra information can be in the form of textual description of the class\u2014red, or beak\u2014in addition to the visual information [449]. The learner must be able to match text with image information. Zero-shot learning approaches are designed to learn this intermediate semantic layer, the attributes, and apply them at inference time to predict new classes, when they are provided with descriptions in terms of these attributes. Attributes correspond to high-level properties of the objects which are shared across multiple classes (which can be detected by machines and which can be understood by humans). Attribute-based image classifcation is a label-embedding problem where each class is embedded in the space of attribute vectors. As an example, if the classes correspond to animals, possible attributes include has paws, has stripes or is black. 9.3 Meta-Learning Environments Now that we have seen how transfer learning and meta-learning can be implemented, it is time to look at some of the environments that are used to evaluate the algorithms. We will list important datasets, environments, and foundation models for images, behavior, and text, expanding our scope beyond pure reinforcement learning. We will look at how well the approaches succeed in generalizing quickly to new machine learning tasks. Many benchmarks have been introduced to test transfer and meta-learning algorithms. Benchmarks for conventional machine learning algorithms aim to ofer a variety of challenging learning tasks. Benchmarks for meta-learning, in contrast, aim to ofer related learning tasks. Some benchmarks are parameterized, where the diference between tasks can be controlled. Meta-learning aims to learn new and related tasks quicker, trading of speed versus accuracy. This raises the question how fast and how accurate meta-learning algorithms are under diferent circumstances. In answering these questions we must keep in mind that the closer the learning tasks are, the easier the task is, and the quicker and the more accurate results will be. Hence, we should carefully look at which dataset a benchmark uses when we compare results. 262 9 Meta-Learning Name Type Domain Ref ALE single games [71] MuJoCo single robot [780] DeepMind Control single robot [755] BERT transfer text [193] GPT-3 transfer text [617] ImageNet transfer image [237] Omniglot meta image [445] Mini-ImageNet meta image [814] Meta-Dataset meta image [785] Meta-World meta robot [864] Alchemy meta unity [825] Table 9.3 Datasets, Environments, and Models for Meta-Learning in Images, Behavior, and Text Table 9.3 lists some of the environments that are often used for meta-learning experiments. Some are regular deep learning environments designed for single-task learning (\u201csingle\u201d), some are transfer learning and pretraining datasets (\u201ctransfer\u201d), and some datasets and environments are specifcally designed for meta-learning experiments (\u201cmeta\u201d). We will now describe them in more detail. ALE (Sect. 3.1.1), MuJoCo (Sect. 4.1.3) and the DeepMind control suite (Sect. 4.3.2) are originally single task deep learning environments. They are also being used in meta-learning experiments and few-shot learning, often with moderate results, since the tasks are typically not very similar (Pong is not like Pac-Man). 9.3.1 Image Processing Traditionally, two datasets have emerged as de facto benchmarks for few-shot image learning: Omniglot [446], and mini-ImageNet [646, 814]. Omniglot is a dataset for one-shot learning. This dataset contains 1623 diferent handwritten characters from 50 diferent alphabets and contains 20 examples per class (character) [446, 447]. Most recent methods obtain very high accuracy on Omniglot, rendering comparisons between them mostly uninformative. Mini-ImageNet uses the same setup as Omniglot for testing, consisting of 60,000 colour images of size 84 \u00d7 84 with 100 classes (64/16/20 for train/validation/test) and contains 600 examples per class [814]. Albeit harder than Omniglot, most recent methods achieve similar accuracy when controlling for model capacity. Meta-learning algorithms such as Bayesian Program Learning and MAML achieved accuracies comparable to human performance on Omniglot and ImageNet, with accuracies in the high nineties and error rates as low as a few percent [447]. Models 9.3 Meta-Learning Environments 263 trained on the largest datasets, such as ImageNet, are used as foundation models [96]. Pre-trained models can be downloaded from the Model zoo.14 These benchmarks may be too homogeneous for testing meta-learning. In contrast, real-life learning experiences are heterogeneous: they vary in terms of the number of classes and examples per class, and are unbalanced. Furthermore, the Omniglot and Mini-ImageNet benchmarks measure within-dataset generalization. For meta-learning, we are eventually after models that can generalize to entirely new distributions. For this reason, new datasets are being developed specifcally for meta-learning. 9.3.2 Natural Language Processing In natural language processing BERT is a well-known pretrained model. BERT stands for Bidirectional encoder representations from transformers [193]. It is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on two contexts. BERT has shown that transfer learning can work well in natural language tasks. BERT can be used for classifcation tasks such as sentiment analysis, question answering tasks, and named entity recognition. BERT is a large model, with 345 million parameters [639]. An even larger pretrained transformer model is the Generative pretrained transformer 3, or GPT-3 [617], with 175 billion parameters. The quality of the text generated by GPT-3 is exceptionally good, it is difcult to distinguish from that written by humans. In this case, it appears that size matters. OpenAI provides a public interface where you can see for yourself how well it performs.15 BERT and GPT-3 are large models, that are used more and more as a foundation model as a basis for other experiments to pretrain on. 9.3.3 Meta-Dataset A recent dataset specifcally designed for meta-learning is Meta-dataset [785].16 Meta-dataset is a set of datasets, consisting of: Imagenet, Omniglot, Aircraft, Birds, Textures, Quick Draw, Fungi, Flower, Trafc Signs, and MSCOCO. Thus, the datasets provide a more heterogeneous challenge than earlier single-dataset experiments. Triantafllou et al. report results with Matching networks [814], Prototypical networks [717], frst-order MAML [243], and Relation networks [737]. As can be expected, the accuracy on the larger (meta) dataset is much lower than on previous homogeneous datasets. They fnd that a variant of MAML performs best, although for classifers that are trained on other datasets most accuracies are between 40% 14 https://modelzoo.co 15 https://openai.com/blog/openai-api/ 16 https://github.com/google-research/meta-dataset 264 9 Meta-Learning Fig. 9.7 Multi Task and Meta Reinforcement Learning [864] Fig. 9.8 Meta-World Tasks [864] and 60%, except for Birds and Flowers, which scores in the 70s and 80s, closer to the single dataset results for Omniglot and Imagenet. Meta-learning for heterogeneous datasets remains a challenging task (see also [141, 775]). 9.3.4 Meta-World For deep reinforcement learning two traditionally popular environments are ALE and MuJoCo. The games in the ALE benchmarks typically difer considerably, which makes the ALE test set challenging for meta-learning, and little success has been reported. (There are a few exceptions that apply transfer learning (pretraining) to DQN [586, 520, 718] succeeding in multitask learning in a set of Atari games that all move a ball.) Robotics tasks, on the other hand, are more easily parameterizable. Test tasks can be generated with the desired level of similarity, making robotic tasks amenable to meta-learning testing. Typical tasks such as reaching and learning diferent walking 9.3 Meta-Learning Environments 265 gaits, are more related than two Atari games such as, for example, Breakout and Space Invaders. To provide a better benchmark that is more challenging for meta reinforcement learning, Yu et al. introduced Meta-World [864],17 a benchmark for multi-task and meta reinforcement learning (see Fig. 9.7 for their pictorial explanation of the diference between multi-task and meta-learning). Meta-World consists of 50 distinct manipulation tasks with a robotic arm (Fig. 9.8). The tasks are designed to be diferent, and contain structure, which can be leveraged for transfer to new tasks. When the authors of Meta-World evaluated six state-of-the-art meta-reinforcement and multi-task learning algorithms on these tasks, they found that generalization of existing algorithms to heterogeneous tasks is limited. They tried PPO, TRPO, SAC, RL2[213], MAML, and PEARL [624]. Small variations of tasks such as diferent object positions can be learned with reasonable success, but the algorithms struggled to learn multiple tasks at the same time, even with as few as ten distinct training tasks. In contrast to more limited meta-learning benchmarks, Meta-World emphasizes generalization to new tasks and interaction scenarios, not just a parametric variation in goals. 9.3.5 Alchemy A fnal meta reinforcement learning benchmark that we will discuss is Alchemy [825]. The Alchemy benchmark is a procedurally generated 3D video game [692], implemented in Unity [383]. Task generation is parameterized, and varying degrees of similarity and hidden structure can be chosen. The process by which Alchemy levels are created is accessible to researchers, and a perfect Bayesian ideal observer can be implemented. Experiments with two agents are reported, VMPO and IMPALA. The VMPO [722, 587] agent is based on a gated transformer network. The IMPALA [230, 374] agent is based on population-based training with an LSTM core network. Both agents are strong deep learning methods, although not necessarily for meta-learning. Again, in both agents meta-learning became more difcult as learning tasks became more diverse. The reported performance on meta-learning was weak. The Alchemy platform can be found on GitHub.18 A human-playable interface is part of the environment. 17 https://meta-world.github.io 18 https://github.com/deepmind/dm_alchemy 266 9 Meta-Learning 1 import metaworld 2 import random 3 4 print(metaworld.ML1.ENV_NAMES) # Try available environments 5 6 ml1 = metaworld.ML1(\u2019pick -place -v1\u2019) # Construct the benchmark 7 8 env = ml1. train_classes [\u2019pick -place -v1\u2019]() 9 task = random.choice(ml1. train_tasks ) 10 env.set_task(task) # Set task 11 12 obs = env.reset () # Reset environment 13 a = env. action_space .sample () # Sample an action 14 obs , reward , done , info = env.step(a) # Step the environoment Listing 9.3 Using Meta-World 9.3.6 Hands-on: Meta-World Example Let us get experience with running Meta-World benchmark environments. We will run a few popular agent algorithms on them, such as PPO and TRPO. Meta-World is as easy to use as Gym. The code can be found on GitHub,19 and the accompanying implementations of a suite of agent algorithms is called Garage [261].20 Garage runs on PyTorch and on TensorFlow. Standard Meta-World benchmarks include multi-task and meta-learning setups, named MT1, MT10, MT50, and ML1, ML10, ML50. The Meta-World benchmark can be installed with pip pip install git+https://github.com/rlworkgroup/ metaworld.git@master\\#egg=metaworld Note that Meta-World is a robotics benchmark and needs MuJoCo, so you have to install that too.21 The GitHub site contains brief example instructions on the usage of the benchmark, please refer to Listing 9.3. The benchmark can be used to test the meta-learning performance of your favorite algorithm, or you can use one of the baselines provided in Garage. Conclusion In this chapter we have seen diferent approaches to learning new and diferent tasks with few or even zero examples. Impressive results have been achieved, although 19 https://github.com/rlworkgroup/metaworld 20 https://github.com/rlworkgroup/garage 21 https://github.com/openai/mujoco-py#install-mujoco 9.3 Meta-Learning Environments 267 major challenges remain to learn general adaptation when tasks are more diverse. As is often the case in new felds, many diferent approaches have been tried. The feld of meta-learning is an active feld of research, aiming to reduce one of the main problems of machine learning, and many new methods will continue to be developed. Summary and Further Reading We will now summarize the chapter and provide pointers to further reading. Summary This chapter is concerned with learning new tasks faster and with smaller datasets or lower sample complexity. Transfer learning is concerned with transferring knowledge that has been learned to solve a taks, to another task, to allow quicker learning. A popular transfer learning approach is pretraining, where some network layers are copied to intialize a network for a new task, followed by fne tuning, to improve performance on the new task, but with a smaller dataset. Another approach is meta-learning, or learning to learn. Here knowledge of how a sequence of previous tasks is learned is used to learn a new task quicker. Metalearning learns hyperparameters of the diferent tasks. In deep meta-learning, the set of initial network parameters is usually considered to be such a hyperparameter. Meta-learning aims to learn hyperparameters that can learn a new task with only a few training examples, often using \ud835\udc41-way-\ud835\udc58-shot learning. For deep few-shot learning the Model-Agnostic Meta-Learning (MAML) approach is well-known, and has inspired follow-up work. Meta-learning is of great importance in machine learning. For tasks that are related, good results are reported. For more challenging benchmarks, where tasks are less related (such as pictures of animals from very diferent species), results are reported that are weaker. Further Reading Meta-learning is a highly active feld of research. Good entry points are [882, 351, 363, 364, 100]. Meta-learning has attracted much attention in artifcial intelligence, both in supervised learning and in reinforcement learning. Many books and surveys have been written about the feld of meta-learning, see, for example [106, 684, 804, 667, 836]. 268 9 Meta-Learning There has been active research interest in meta-learning algorithms for some time, see, for example [669, 75, 676, 812]. Research into transfer learning and metalearning has a long history, starting with Pratt and Thrun [606, 774]. Early surveys into the feld are [581, 758, 834], more recent surveys are [872, 879]. Huh et al. focus on ImageNet [360, 622]. Yang et al. study the relation between transfer learning and curriculum learning with Sokoban [859]. Early principles of meta-learning are described by Schmidhuber [669, 676]. Metalearning surveys are [667, 812, 106, 684, 804, 836, 302, 351, 363, 364]. Papers on similarity-metric meta-learning are [418, 814, 717, 737, 263, 699]. Papers on modelbased meta-learning are [213, 826, 659, 542, 517, 223, 264]. Papers on optimizationbased meta-learning are many [629, 473, 243, 23, 623, 476, 561, 649, 245, 285, 246, 861, 83]. Domain adaptation is studied in [17, 200, 168, 872, 541]. Zero-shot learning is an active and promising feld. Interesting papers are [87, 356, 454, 580, 728, 10, 851, 449, 636, 197, 622]. Like zero-shot learning, few-shot learning is also a popular area of meta-learning research [337, 437, 719, 565, 728]. Benchmark papers are [785, 864, 141, 775, 825]. Exercises It is time to test our understanding of transfer learning and meta-learning with some exercises and questions. Questions Below are some quick questions to check your understanding of this chapter. For each question a simple, single sentence answer is sufcient. 1. What is the reason for the interest in meta-learning and transfer learning? 2. What is transfer learning? 3. What is meta-learning? 4. How is meta-learning diferent from multi task learning? 5. Zero-shot learning aims to identify classes that it has not seen before. How is that possible? 6. Is pretraining a form of transfer learning? 7. Can you explain learning to learn? 8. Are the initial network parameters also hyperparameters? Explain. 9. What is an approach for zero-shot learning? 10. As the diversity of tasks increases, does meta-learning achieve good results? 9.3 Meta-Learning Environments 269 Exercises Let us go to the programming exercises to become more familiar with the methods that we have covered in this chapter. Meta-learning and transfer learning experiments are often very computationally expensive. You may need to scale down dataset sizes, or skip some exercises as a last resort. 1. Pretraining Implement pretraining and fne tuning in the Keras pretraining example from Sect. 9.2.1.3.22 Do the exercises as suggested, including fnetuning on the cats and dogs training set. Note the uses of preprocessing, data augmentation and regularization (dropout and batch normalization). See the efects of increasing the number of layers that you transfer on training performance and speed. 2. MAML Reptile [561] is a meta-learning approach inpspired by MAML, but frst order, and faster, specifcally designed for few-shot learning. The Keras website contains a segment on Reptile.23 At the start a number of hyperparameters are defned: learning rate, step size, batch size, number of meta-learning iterations, number of evaluation iterations, how many shots, classes, etcetera. Study the efect of tuning diferent hyperparameters, especially the ones related to few-shot learning: the classes, the shots, and the number of iterations. To delve deeper into few-shot learning, also have a look at the MAML code, which has a section on reinforcement learning.24 Try diferent environments. 3. Meta World As we have seen in Sect. 9.3.4, Meta World [864] is an elaborate benchmark suite for meta reinforcement learning. Re-read the section, go to GitHub, and install the benchmark.25 Also go to Garage to install the agent algorithms so that you are able to test their performance.26 See that they work with your PyTorch or TensorFlow setup. First try running the ML1 meta benchmark for PPO. Then try MAML, and RL2. Next, try the more elaborate meta-learning benchmarks. Read the Meta World paper, and see if you can reproduce their results. 4. ZSL We go from few-shot learning to zero-shot learning. One of the ways in which zero-shot learning works is by learning attributes that are shared by classes. Read the papers Label-Embedding for Image Classifcation [10], and An embarrassingly simple approach to zero-shot learning [636], and go to the code [87].27 Implement it, and try to understand how attribute learning works. Print the attributes for the classes, and use the diferent datasets. Does MAML work for few-shot learning? (challenging) 22 https://keras.io/guides/transfer_learning/ 23 https://keras.io/examples/vision/reptile/ 24 https://github.com/cbfinn/maml_rl/tree/master/rllab 25 https://github.com/rlworkgroup/metaworld 26 https://github.com/rlworkgroup/garage 27 https://github.com/sbharadwajj/embarrassingly-simple-zero-shot-learning Chapter 10 Further Developments We have come to the end of this book. We will refect on what we have learned. In this chapter we will review the main themes and essential lessons, and we will look to the future. Why do we study deep reinforcement learning? Our inspiration is the dream of artifcial intelligence; to understand human intelligence and to create intelligent behavior that can supplement our own, so that together we can grow. For reinforcement learning our goal is to learn from the world, to learn increasingly complex behaviors for increasingly complex sequential decision problems. The preceding chapters have shown us that many successful algorithms were inspired by how humans learn. Currently many environments consist of games and simulated robots, in the future this may include human-computer interactions and collaborations in teams with real humans. 10.1 Development of Deep Reinforcement Learning Reinforcement learning has made a remarkable transition, from a method that was used to learn small tabular toy problems, to learning simulated robots how to walk, to playing the largest multi-agent real time strategy games, and beating the best humans in Go and poker. The reinforcement learning paradigm is a framework in which many learning algorithms have been developed. The framework is able to incorporate powerful ideas from other felds, such as deep learning, and autoencoders. To appreciate the versatility of reinforcement learning, and now that we have studied the feld in great detail, let us have a closer look at how the developments in the feld have proceeded over time. 271 272 10 Further Developments 10.1.1 Tabular Methods Reinforcement learning starts with a simple agent/environment loop, where an environment performs the agent\u2019s actions, and returns a reward (the model-free approach). We use a Markov decision process to formalize reinforcement learning. The value and policy functions are initially implemented in a tabular fashion, limiting the method to small environments, since the agent has to ft the function representations in memory. Typical environments are Grid world, Cartpole and Mountain car; a typical algorithm to fnd the optimal policy function is tabular Q-learning. Basic principles in the design of these algorithms are exploration, exploitation and on-policy/of-policy learning. Furthermore, imagination, as a form of model-based reinforcement learning, was developed. This part of the feld forms a well-established and stable basis, that is, however, only suitable for learning small single-agent problems. The advent of deep learning caused the feld to shift into a higher gear. 10.1.2 Model-free Deep Learning Inspired by breakthroughs in supervised image recognition, deep learning was also applied to Q-learning, causing the Atari breakthrough for deep reinforcement learning. The basis of the success of deep learning in reinforcement learning are methods to break correlations and improve convergence (replay bufer and a separate target network). The DQN algorithm [523] has become quite well known. Policy-based and actor critic approaches work well with deep learning, and are also applicable to continuous action spaces. Many model-free actor critic variants have been developed [683, 480, 306, 521], they are often tested on simulated robot applications. Algorithms often reach good quality optima, but model-free algorithms have a high sample complexity. Tables 3.1 and 4.1 list these algorithms. This part of the feld\u2014deep model-free value and policy-based algorithms\u2014can now be considered as well-established, with mature algorithms whose behavior is well understood, and with good results for high-dimensional single-agent environments. Typical high-dimensional environments are the Arcade Learning Environment, and MuJoCo simulated physics locomotion tasks. 10.1.3 Multi-Agent Methods Next, more advanced methods are covered. In Chap. 5 model-based algorithms combine planning and learning to improve sample efciency. For high-dimensional visual environments they use uncertainty modeling and latent models or world models, to reduce the dimensionality for planning. The algorithms are listed in Table 5.2. 10.1 Development of Deep Reinforcement Learning 273 Furthermore, the step from single-agent to multi-agent is made, enlarging the type of problems that can be modeled, getting closer to real-world problems. The strongest human Go players are beaten with a model-based self-play combination of MCTS and a deep actor critic algorithm. The self-play setup performs curriculum learning, learning from previous learning tasks ordered from easy to hard, and is in that sense a form of meta-learning. Variants are shown in Table 6.2. For multi-agent and imperfect information problems, deep reinforcement learning is used to study competition, emergent collaboration, and hierarchical team learning. In these areas reinforcement learning comes close to multi-agent systems and population based methods, such as swarm computing. Parallel population-based methods may be able to learn the policy quicker than gradient-based methods, and they may be a good ft for multi-agent problems. Also, imperfect information multi-agent problems are studied, such as poker; for competitive games counterfactual regret minimization was developed. Research into cooperation is continuing. Early strong results are reported in StarCraft using team collaboration and team competition. In addition, research is being performed into emergent social behavior, connecting the felds of reinforcement learning to swarm computing and multi-agent systems. Algorithms and experiments are listed in Table 7.2. A list of hierarchical approaches is shown in Table 8.1. In human learning, new concepts are learned based on old concepts. Transfer learning from foundation models, and meta-learning, aim to re-use existing knowledge, or even learn to learn. Meta-learning, curriculum learning, and hierarchical learning are emerging as techniques to conquer ever larger state spaces. Table 9.2 shows meta-learning approaches. All these areas should be considered as advanced reinforcement learning, where active research is still very much occurring. New algorithms are being developed, and experiments typically require large amounts of compute power. Furthermore, results are less robust, and require much hyperparameter tuning. More advances are needed, and expected. 10.1.4 Evolution of Reinforcement Learning In contrast to supervised learning, which learns from a fxed dataset, reinforcement learning is a mechanism for learning by doing, just as children learn. The agent/environment framework has turned out to be a versatile approach, that can be augmented and enhanced when we try new problem domains, such as highdimensions, multi-agent or imperfect information. Reinforcement learning has encompassed methods from supervised learning (deep learning) and unsupervised learning (autoencoders), as well as from population-based optimization. In this sense reinforcement learning has evolved from being a single-agent Markov decision process to a framework for learning, based on agent and environment. Other approaches can be hooked into this framework, to learn new felds, and to improve performance. These additions can interpret high dimensional states 274 10 Further Developments (as in DQN), or shrink a state space (as with latent models). When accomodating self-play, the framework provided us with a curriculum learning sequence, yielding world class levels of play in two-agent games. 10.2 Main Challenges Deep reinforcement learning is being used to understand more real world sequential decision making situations. Among the applications that motivate these developments are self driving cars and other autonomous operations, image and speech recognition, decision making, and, in general, acting naturally. What will the future bring for deep reinforcement learning? The main challenge for deep reinforcement learning is to manage the combinatorial explosion that occurs when a sequence of decisions is chained together. Finding the right kind of inductive bias can exploit structure in this state space. We list three major challenges for current and future research in deep reinforcement learning: 1. Solving larger problems faster 2. Solving problems with more agents 3. Interacting with people The following techniques address these challenges: 1. Solving larger problems faster \u2022 Reducing sample complexity with latent-models \u2022 Curriculum learning in self-play methods \u2022 Hierarchical reinforcement learning \u2022 Learn from previous tasks with transfer learning and meta-learning \u2022 Better exploration through intrinsic motivation 2. Solving problems with more agents \u2022 Hierarchical reinforcement learning \u2022 Population-based self-play league methods 3. Interacting with people \u2022 Explainable AI \u2022 Generalization Let us have a closer look at these techniques, to see what future developments can be expected for them. 10.2 Main Challenges 275 10.2.1 Latent Models Chapter 5 discussed model-based deep reinforcement learning methods. In modelbased methods a transition model is learned that is then used with planning to augment the policy function, reducing sample complexity. A problem for modelbased methods in high-dimensional problems is that high-capacity networks need many observations in order to prevent overftting, negating the potential reduction in sample complexity. One of the most promising model-based methods is the use of autoencoders to create latent models, that compress or abstract from irrelevant observations, yielding a lower-dimensional latent state model that can be used for planning in a reduced state space. The reduction in sample complexity of model-based approaches can thus be maintained. Latent models create compact world representations, that are also used in hierarchical and multi-agent problems, and further work is ongoing. A second development in model-based deep reinforcement learning is the use of end-to-end planning and learning of the transition model. Especially for elaborate self-play designs such as AlphaZero, where an MCTS planner is integrated in selflearning, the use of end-to-end learning is advantageous, as the work on MuZero has shown. Research is ongoing in this feld where planning and learning are combined [679, 215, 186, 680, 359, 334, 525]. 10.2.2 Self-Play Chapter 6 discussed learning by self-play in two-agent games. In many two-agent games the transition function is given. When the environment of an agent is played by the opponent with the exact same transition function, a self-learning self-play system can be constructed in which both agent and environment improve eachother. We have discussed examples of cycles of continuous improvement, from tabula-rasa to world-champion level. After earlier results in backgammon [763], AlphaZero achieved landmark results in Go, chess, and shogi [706, 704]. The AlphaZero design includes an MCTS planner in a self-play loop that improves a dual-headed deep residual network [704]. The design has spawned much further research, as well as inspired general interest in artifcial intelligence and reinforcement learning [213, 464, 687, 552, 455, 239, 679, 385]. 10.2.3 Hierarchical Reinforcement Learning Team play is important in multi-agent problems, and hierarchical approaches can structure the environment in a hierarchy of agents. Hierarchical reinforcement learning methods are also applied to single agent problems, using principles of 276 10 Further Developments divide and conquer. Many large single-agent problems are hierarchically structured. Hierarchical methods aim to make use of this structure by dividing large problems into smaller subproblems; they group primitive actions into macro actions. When a policy has been found with a solution for a certain subproblem, then this can be re-used when the subproblem surfaces again. Note that for some problems it is difcult to fnd a hierarchical structure that can be exploited efciently. Hierarchical reinforcement learning has been studied for some time [150, 247, 745]. Recent work has been reported on successful methods for deep hierarchical learning and population-based training [475, 471, 546, 633], and more is to be expected. 10.2.4 Transfer Learning and Meta-Learning Among the major challenges of deep reinforcement learning is the long training time. Transfer learning and meta-learning aim to reduce the long training times, by transferring learned knowledge from existing to new (but related) tasks, and by learning to learn from the training of previous tasks, to speedup learning new (but related) tasks. In the felds of image recognition and natural language processing it has become common practice to use networks that are pretrained on ImageNet [191, 237] or BERT [193] or other large pretrained networks [548, 92]. Optimization-based methods such as MAML learn better initial network parameters for new tasks, and have spawned much further research. Zero-shot learning is a meta-learning approach where outside information is learned, such as attributes or a textual description for image content, that is then used to recognize individuals from a new class [636, 719, 851, 10]. Meta-learning is a highly active feld where more results can be expected. Foundation models are large models, such as ImageNet for image recognition, and GPT-3 in natural language processing, that are trained extensively on large datasets. They contain general knowledge, that can be specialized for a certain more specialized task. They can also be used for multi-modal tasks, where text and image information is combined. The DALL-E project is able to create images that go with textual descriptions. See Fig. 10.1 for amusing or beautiful examples (\u201can armchair in the shape of an avocado\u201d) [616]. GPT-3 has also been used to study zero-shot learning, with success, in the CLIP project [625]. 10.2.5 Population-Based Methods Most reinforcement learning has focused on one or two-agent problems. However, the world around us is full of many-agent problems. The major problem of multiagent reinforcement learning is to model the large, nonstationary, problem space. 10.2 Main Challenges 277 Fig. 10.1 DALL-E, an Algorithm that draws Pictures based on Textual Commands [616] Recent work applies self-play methods in a multi-agent setting, where entire populations of agents are trained against eachother. This approach combines aspects of evolutionary algorithms (combining and mutating policies, as well as culling of underperforming agent policies) and hierarchical methods (modeling of team collaboration). Population-based training of leagues of agents has achieved success in highly complex multi-agent games: StarCraft [813] and Capture the Flag [373]. Populationbased methods, combining evolutionary principles with self-play, curriculum learning, and hierarchical methods are active areas of research [653, 374, 512, 817, 408]. 10.2.6 Exploration and Intrinsic Motivation The prime motivator for learning in reinforcement learning is reward. However, reward is sparse in many sequential decision problems. Reward shaping tries to augment the reward function with heuristc knowledge. Developemental psychology argues that learning is (also) based on curiosity, or intrinsically motivated. Intrinsic motivation is a basic curiosity drive to explore for exploration\u2019s sake, deriving satisfaction from the exploration process itself. The feld of intrinsic motivation is relatively new in reinforcement learning. Links with hierarchical reinforcement learning and the options framework are being explored, as well as with models of curiosity [674, 660, 161]. Intrinsic motivation in reinforcement learning can be used for exploring openended environments [29, 712] (see also Sect. 8.3.2). Intrinsically motivated goalconditioned algorithms can train agents to learn to represent, generate and pursue their own goals [162]. The success of the Go-Explore algorithm in domains with sparse rewards also stresses the importance of exploration in reinforcement learning [222]. 278 10 Further Developments 10.2.7 Explainable AI Explainable AI (XAI) is closely related to the topics of planning and learning that we discuss in this book, and to natural language processing. When a human expert suggests an answer, this expert can be questioned to explain the reasoning behind the answer. This is a desirable property, and enhances how much we trust the answer. Most clients receiving advice, be it fnancial or medical, put greater trust in a well-reasoned explanation than in a yes or no answer without any explanation. Decision support systems that are based on classic symbolic AI can often be made to provide such reasoning easily. For example, interpretable models [611], decision trees [614], graphical models [381, 457], and search trees [160] can be traversed and the choices at decision points can be recorded and used to translate in a human-understandable argument. Connectionist approaches such as deep learning, in contrast, are less interpretable. Their accuracy, however, is typically much higher than the classical approaches. Explainable AI aims to combine the ease of interpreting symbolic AI with the accuracy of connectionist approaches [300, 205, 116]. The work on soft decision trees [257, 341] and adaptive neural trees [754] has shown how hybrid approaches of planning and learning can try to build an explanatory decision tree based on a neural network. These works build in part on model compression [142, 120] and belief networks [323, 558, 685, 184, 69, 767, 107, 28]. Unsupervised methods can be used to fnd interpretable models [611, 643, 807]. Model-based reinforcement learning methods aim to perform deep sequential planning in learned world models [304]. 10.2.8 Generalization Benchmarks drive algorithmic progress in artifcial intelligence. Chess and poker have given us deep heuristic search; ImageNet has driven deep supervised learning; ALE has given us deep Q-learning; MuJoCo and the DeepMind control suite have driven actor critic methods; Omniglot, MiniImagenet and Meta-World drive meta-learning; and StarCraft and other multi-agent games drive hierarchical and population-based methods. As the work on explainable AI indicates, there is a trend in reinforcement learning to study problems that are closer to the real world, through model-based methods, multi-agent methods, meta-learning and hierarchical methods. As deep reinforcement learning will be more widely applied to real-world problems, generalization becomes important, as is argued by Zhang et al. [868]. Where supervised learning experiments have a clear training set/test set separation to measure generalization, in reinforcement learning agents often fail to generalize beyond the environment they were trained in [579, 159]. Reinforcement learning agents often overft on their training environment [867, 869, 839, 235, 274]. This 10.3 The Future of Artifcial Intelligence 279 becomes especially difcult in sim-to-real transfer [876]. One benchmark specifcally designed to increase generalization is Procgen. It aims to increase environment diversity through procedural content generation, providing 16 parameterizable environments [158]. Benchmarks will continue to drive progress in artifcial intelligence, especially for generalization [413]. 10.3 The Future of Artifcial Intelligence This book has covered the stable basis of deep reinforcement learning, as well as active areas of research. Deep reinforcement learning is a highly active feld, and many more developments will follow. We have seen complex methods for solving sequential decision problems, some of which are easily solved on a daily basis by humans in the world around us. In certain problems, such as backgammon, chess, checkers, poker, and Go, computational methods have now surpassed human ability. In most other endeavours, such as pouring water from a bottle in a cup, writing poetry, or falling in love, humans still reign supreme. Reinforcement learning is inspired by biological learning, yet computational and biological methods for learning are still far apart. Human intelligence is general and broad\u2014we know much about many diferent topics, and we use our general knowledge of previous tasks when learning new things. Artifcial intelligence is specialized and deep\u2014computers can be good at certain tasks, but their intelligence is narrow, and learning from other tasks is a challenge. Two conclusion are clear. First, for humans, hybrid intelligence, where human general intelligence is augmented by specialized artifcial intelligence, can be highly benefcial. Second, for AI, the feld of deep reinforcement learning is taking cues from human learning in hierarchical methods, curriculum learning, learning to learn, and multi-agent cooperation. The future of artifcial intelligence is human. Appendices Appendix A Mathematical Background This appendix provides essential mathematical background and establishes the notation that we use in this book. We start with notation of sets and functions, then we discuss probability distributions, expectations, and information theory. You will most likely have seen some of these in previous courses. We will also discuss how to diferentiate through an expectation, which frequently appears in machine learning. This appendix is based on Moerland [524]. A.1 Sets and Functions We start at the beginning, with sets and functions. A.1.1 Sets Discrete set A discrete set is a set of countable elements. Examples: \u2022 \ud835\udc4b = {1, 2, .., \ud835\udc5b} (integers) \u2022 \ud835\udc4b = {up, down, left, right} (arbitrary elements) \u2022 \ud835\udc4b = {0, 1}\ud835\udc51 (d-dimensional binary space) 283 284 A Mathematical Background Continuous set A continuous set is a set of connected elements. Examples: \u2022 \ud835\udc4b = [2, 11] (bounded interval) \u2022 \ud835\udc4b = R (real line) \u2022 \ud835\udc4b = [0, 1]\ud835\udc51 (\ud835\udc51-dimensional hypercube) Conditioning a set We can also condition within a set, by using : or |. For example, the discrete probability \ud835\udc58-simplex, which is what we actually use to defne a discrete probability distribution over \ud835\udc58 categories, is given by: \ud835\udc4b = {\ud835\udc65 \u2208 [0, 1]\ud835\udc58 : \u2211\ufe01 \ud835\udc58 \ud835\udc65\ud835\udc58 = 1}. This means that \ud835\udc65 is a vector of length \ud835\udc58, consisting of entries between 0 and 1, with the restriction that the vector sums to 1. Cardinality and dimensionality It is important to distinguish the cardinality and dimensionality of a set: \u2022 The cardinality (size) counts the number of elements in a vector space, for which we write |\ud835\udc4b|. \u2022 The dimensionality counts the number of dimensions in the vector space \ud835\udc4b, for which we write Dim(\ud835\udc4b). Examples: \u2022 The discrete space \ud835\udc4b = {0, 1, 2} has cardinality |\ud835\udc4b| = 3 and dimensionality Dim(\ud835\udc4b) = 1. \u2022 The discrete vector space \ud835\udc4b = {0, 1}4 has cardinality |\ud835\udc4b| = 24 = 16 and dimensionality Dim(\ud835\udc4b) = 4. A.1 Sets and Functions 285 Fig. A.1 \ud835\udc66 = \ud835\udc652 Cartesian product We can combine two spaces by taking the Cartesian product, denoted by \u00d7, which consists of all the possible combinations of elements in the frst and second set: \ud835\udc4b \u00d7 \ud835\udc4d = {(\ud835\udc65, \ud835\udc67) : \ud835\udc65 \u2208 \ud835\udc4b, \ud835\udc67 \u2208 \ud835\udc4d} We can also combine discrete and continuous spaces through Cartesian products. Example: Assume \ud835\udc4b = {20, 30} and \ud835\udc4d = {0, 1}. Then \ud835\udc4b \u00d7 \ud835\udc4d = {(20, 0), (20, 1), (30, 0), (30, 1)} Assume \ud835\udc4b = R and \ud835\udc4d = R. Then \ud835\udc4b \u00d7 \ud835\udc4d = R2. A.1.2 Functions \u2022 A function \ud835\udc53 maps a value in the function\u2019s domain \ud835\udc4b to a (unique) value in the function\u2019s co-domain/range \ud835\udc4c, where \ud835\udc4b and \ud835\udc4c can be discrete or continuous sets. \u2022 We write the statement that \ud835\udc53 is a function from \ud835\udc4b to \ud835\udc4c as \ud835\udc53 : \ud835\udc4b \u2192 \ud835\udc4c Examples: \u2022 \ud835\udc66 = \ud835\udc652 maps every value in domain \ud835\udc4b \u2208 R to range \ud835\udc4c \u2208 R+ (see Fig. A.1) 286 A Mathematical Background A.2 Probability Distributions A probability distribution is a mathematical function that gives the probability of the occurrence of a set of possible outcomes. The set of possible outcomes is called the sample space, which can be discrete or continuous, and is denoted by \ud835\udc4b. For example, for fipping a coin \ud835\udc4b = {heads, tails}. When we actually sample the variable, we get a particular value \ud835\udc65 \u2208 \ud835\udc4b. For example, for the frst coin fip \ud835\udc651 = heads. Before we actually sample the outcome, the particular outcome value is still unknown. We say that it is a random variable, denoted by \ud835\udc4b, which always has an associated probability distribution \ud835\udc5d(\ud835\udc4b). Sample space (a set) \ud835\udc4b Random variable \ud835\udc4b Particular value \ud835\udc65 Depending on whether the sample space is a discrete or continuous set, the distribution \ud835\udc5d(\ud835\udc4b) and the way to represent it difer. We detail both below, see also Fig. A.2. A.2.1 Discrete Probability Distributions \u2022 A discrete variable \ud835\udc4b can take values in a discrete set \ud835\udc4b = {1, 2, .., \ud835\udc5b}. A particular value that \ud835\udc4b takes is denoted by \ud835\udc65. \u2022 Discrete variable \ud835\udc4b has an associated probability mass function: \ud835\udc5d(\ud835\udc4b), where \ud835\udc5d : \ud835\udc4b \u2192 [0, 1]. Each possible value \ud835\udc65 that the variable can take is associated with a probability \ud835\udc5d(\ud835\udc4b = \ud835\udc65) \u2208 [0, 1]. (For example, \ud835\udc5d(\ud835\udc4b = 1) = 0.2, the probability that \ud835\udc4b is equal to 1 is 20%.) \u2022 Probability distributions always sum to 1: \ufffd \ud835\udc65\u2208\ud835\udc4b \ud835\udc5d(\ud835\udc65) = 1. Parameters We represent a probability distribution with parameters. For a discrete distribution of size \ud835\udc5b, we need \ud835\udc5b\u22121 parameters, {\ud835\udc5d\ud835\udc65=1, .., \ud835\udc5d\ud835\udc65=\ud835\udc5b\u22121}, where \ud835\udc5d\ud835\udc65=1 = \ud835\udc5d(\ud835\udc65=1). The probability of the last category follows from the sum to one constraint, \ud835\udc5d\ud835\udc65=\ud835\udc5b = 1 \u2212 \ufffd\ud835\udc5b\u22121 \ud835\udc56=1 \ud835\udc5d\ud835\udc65=\ud835\udc56. A.2 Probability Distributions 287 Example: A discrete variable \ud835\udc4b that can take three values (\ud835\udc4b = {1, 2, 3}), with associated probability distribution \ud835\udc5d(\ud835\udc4b = \ud835\udc65): \ud835\udc5d(\ud835\udc4b = 1) \ud835\udc5d(\ud835\udc4b = 2) \ud835\udc5d(\ud835\udc4b = 3) 0.2 0.4 0.4 Representing discrete random variables It is important to realize that we always represent a discrete variable as a vector of probabilities. Therefore, the above variable \ud835\udc4b does not really take values \ud835\udc4b = {1, 2, 3}, because 1, 2 and 3 are arbitrary categories (category two is not twice as much as the frst category). We could just as well have written \ud835\udc4b = {\ud835\udc4e, \ud835\udc4f, \ud835\udc50}. Always think of the possible values of a discrete variable as separate entries. Therefore, we should represent the value of a discrete variable as a vector of probabilities. In the data, when we observe the ground truth, this becomes a one-hot encoding, where we put all mass on the observed class. Example: In the above example, we had (\ud835\udc4b = {1, 2, 3}). Imagine we sample \ud835\udc4b three times and observe 1, 2 and 3, respectively. We would actually represent these observations as Observed category Representation 1 (1, 0, 0) 2 (0, 1, 0) 3 (0, 0, 1) A.2.2 Continuous Probability Distributions \u2022 A continuous variable \ud835\udc4b can take values in a continuous set, \ud835\udc4b = R (the real line), or \ud835\udc4b = [0, 1] (a bounded interval). \u2022 Continuous variable \ud835\udc4b has an associated probability density function: \ud835\udc5d(\ud835\udc4b), where \ud835\udc5d : \ud835\udc4b \u2192 R+ (a positive real number). \u2022 In a continuous set, there are infnitely many values that the random value can take. Therefore, the absolute probability of any particular value is 0. \u2022 We can only defne absolute probability on an interval, \ud835\udc5d(\ud835\udc4e < \ud835\udc4b \u2264 \ud835\udc4f) = \u222b \ud835\udc4f \ud835\udc4e \ud835\udc5d(\ud835\udc65). (For example, \ud835\udc5d(2 < \ud835\udc4b \u2264 3) = 0.2, the probability that \ud835\udc4b will fall between 2 and 3 is equal to 20%.) 288 A Mathematical Background Fig. A.2 Examples of discrete (left) versus continuous (right) probability distibution [378] \u2022 The interpretation of an individual value of the density, like \ud835\udc5d(\ud835\udc4b = 3) = 4, is only a relative probability. The higher the probability \ud835\udc5d(\ud835\udc4b = \ud835\udc65), the higher the relative chance that we would observe \ud835\udc65. \u2022 Probability distributions always sum to 1: \u222b \ud835\udc65 \u2208 \ud835\udc4b \ud835\udc5d(\ud835\udc65) = 1 (note that this time we integrate instead of sum). Parameters We need to represent a continuous distribution with a parameterized function, that for every possible value in the sample space predicts a relative probability. Moreover, we need to obey the sum to one constraint. Therefore, there are many parameterized continuous probability densities. An example is the Normal distribution. A continuous density is a function \ud835\udc5d : \ud835\udc4b \u2192 R+ that depends on some parameters. Scaling the parameters allows variation in the location where we put probability mass. Example: A variable \ud835\udc4b that can take values on the real line with distribution \ud835\udc5d(\ud835\udc65; \ud835\udf07, \ud835\udf0e) = 1 \ud835\udf0e \u221a 2\ud835\udf0b exp \ufffd \u2212 (\ud835\udc65 \u2212 \ud835\udf07)2 2\ud835\udf0e2 \ufffd . Here, the mean parameter \ud835\udf07 and standard deviation \ud835\udf0e are the parameters. We can change them to change the shape of the distribution, while always ensuring that it still sums to one. We draw an example normal distribution in Fig. A.2, right. The diferences between discrete and continuous probability distributions are summarized in Table A.1. A.2 Probability Distributions 289 Table A.1 Comparison of discrete and continuous probability distributions Discrete distribution Continuous distribution Input/sample space Discrete set, \ud835\udc4b = {0, 1}, with size \ud835\udc5b = |\ud835\udc4b | Continuous set, \ud835\udc4b = R Probability function Probability mass function (pmf) \ud835\udc5d : \ud835\udc4b \u2192 [0, 1] such that \ufffd \ud835\udc65\u2208\ud835\udc4b \ud835\udc5d(\ud835\udc65) = 1 Probability density function (pdf) \ud835\udc5d : \ud835\udc4b \u2192 R such that \u222b \ud835\udc65\u2208\ud835\udc4b \ud835\udc5d(\ud835\udc65) = 1 Possible parametrized distributions Various, but only need simple Discrete Various, Normal, Logistic, Beta, etc. Parameters {\ud835\udc5d\ud835\udc65=1, .., \ud835\udc5d\ud835\udc65=\ud835\udc5b\u22121} Depends on distribution, for normal: {\ud835\udf07, \ud835\udf0e} Number of parameters \ud835\udc5b\u22121 = |\ud835\udc4b |\u22121 (Due to sum to 1 constraint)a Depends on distribution, for normal: 2 Example distribution function \ud835\udc5d(\ud835\udc65 = 1) = 0.2 \ud835\udc5d(\ud835\udc65 = 2) = 0.4 \ud835\udc5d(\ud835\udc65 = 3) = 0.4 e.g. for normal \ud835\udc5d(\ud835\udc65 |\ud835\udf07, \ud835\udf0e) = 1 \ud835\udf0e \u221a 2\ud835\udf0b exp \ufffd \u2212 (\ud835\udc65\u2212\ud835\udf07)2 2\ud835\udf0e2 \ufffd Absolute probability \ud835\udc5d(\ud835\udc65 = 1) = 0.2 \ud835\udc5d(3 \u2264 \ud835\udc65 < 4) = \u222b 4 3 \ud835\udc5d(\ud835\udc65) = 0.3 (on interval)b Relative probability \ud835\udc5d(\ud835\udc65 = 3) = 7.4 a Due to the sum to 1 constraint, we need one parameter less than the size of the sample space, since the last probability is 1 minus all the others: \ud835\udc5d\ud835\udc5b = 1 \u2212 \ufffd\ud835\udc5b\u22121 \ud835\udc56=1 \ud835\udc5d\ud835\udc56. b Note that for continuous distributions, probabilities are only defned on intervals. The density function \ud835\udc5d(\ud835\udc65) only gives relative probabilities, and therefore we may have \ud835\udc5d(\ud835\udc65) > 1, like \ud835\udc5d(\ud835\udc65 = 3) = 5.6, which is of course not possible (one should not interpret it as an absolute probability). However, \ud835\udc5d(\ud835\udc4e \u2264 \ud835\udc65 < \ud835\udc4f) = \u222b \ud835\udc4f \ud835\udc4e \ud835\udc5d(\ud835\udc65) < 1 by defnition. A.2.3 Conditional Distributions \u2022 A conditional distribution means that the distribution of one variable depends on the value that another variable takes. \u2022 We write \ud835\udc5d(\ud835\udc4b|\ud835\udc4c) to indicate that the value of \ud835\udc4b depends on the value of \ud835\udc4c. 290 A Mathematical Background Example: For discrete random variables, we may store a conditional distribution as a table of size |\ud835\udc4b| \u00d7 |\ud835\udc4c|. A variable \ud835\udc4b that can take three values (\ud835\udc4b = {1, 2, 3}) and variable \ud835\udc4c that can take two values (\ud835\udc4c = {1, 2}). The conditional distribution may for example be: \ud835\udc5d(\ud835\udc4b = 1|\ud835\udc4c) \ud835\udc5d(\ud835\udc4b = 2|\ud835\udc4c) \ud835\udc5d(\ud835\udc4b = 3|\ud835\udc4c) \ud835\udc4c = 1 0.2 0.4 0.4 \ud835\udc4c = 2 0.1 0.9 0.0 Note that for each value \ud835\udc4c, \ud835\udc5d(\ud835\udc4b|\ud835\udc4c) should still sum to 1, it is a valid probability distribution. In the table above, each row therefore sums to 1. Example: We can similarly store conditional distributions for continuous random variables, this time mapping the input space to the parameters of a continuous probability distribution. For example, for \ud835\udc5d(\ud835\udc4c|\ud835\udc4b) we can assume a Gaussian distribution \ud835\udc41(\ud835\udf07(\ud835\udc65), \ud835\udf0e(\ud835\udc60)), where the mean and standard deviation depend on \ud835\udc65 \u2208 R. Then we can for example specify: \ud835\udf07(\ud835\udc65) = 2\ud835\udc65, \ud835\udf0e(\ud835\udc65) = \ud835\udc652 and therefore have \ud835\udc5d(\ud835\udc66|\ud835\udc65) = \ud835\udc41(2\ud835\udc65, \ud835\udc652) Note that for each value of \ud835\udc4b, \ud835\udc5d(\ud835\udc4c|\ud835\udc4b) still integrates to 1, it is a valid probability distribution. A.2.4 Expectation We also need the notion of an expectation. A.2.4.1 Expectation of a Random Variable The expectation of a random variable is essentially an average. For a discrete variable, it is defned as: E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [ \ud835\udc53 (\ud835\udc4b)] = \u2211\ufe01 \ud835\udc65\u2208\ud835\udc4b [\ud835\udc65 \u00b7 \ud835\udc5d(\ud835\udc65)] (A.1) For a continuous variable, the summation becomes integration. A.2 Probability Distributions 291 Example: Assume a given \ud835\udc5d(\ud835\udc4b) for a binary variable: \ud835\udc65 \ud835\udc5d(\ud835\udc4b = \ud835\udc65) 0 0.8 1 0.2 The expectation is E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [ \ud835\udc53 (\ud835\udc4b)] = 0.8 \u00b7 0 + 0.2 \u00b7 1 = 0.2 (A.2) A.2.4.2 Expectation of a Function of a Random Variable More often, and also in the context of reinforcement learning, we will need the expectation of a function of the random variable, denoted by \ud835\udc53 (\ud835\udc4b). Often, this function maps to a continuous output. \u2022 Assume a function \ud835\udc53 : \ud835\udc4b \u2192 R, which, for every value \ud835\udc65 \u2208 \ud835\udc4b maps to a continuous value \ud835\udc53 (\ud835\udc65) \u2208 R. \u2022 The expectation is then defned as follows: E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [ \ud835\udc53 (\ud835\udc4b)] = \u2211\ufe01 \ud835\udc65\u2208\ud835\udc4b [ \ud835\udc53 (\ud835\udc65) \u00b7 \ud835\udc5d(\ud835\udc65)] (A.3) For a continuous variable the summation again becomes integration. The formula may look complicated, but it essentially reweights each function outcome by the probability that this output occurs, see the example below. Example: Assume a given density \ud835\udc5d(\ud835\udc4b) and function \ud835\udc53 (\ud835\udc65): \ud835\udc65 \ud835\udc5d(\ud835\udc4b = \ud835\udc65) \ud835\udc53 (\ud835\udc65) 1 0.2 22.0 2 0.3 13.0 3 0.5 7.4 The expectation of the function can be computed as E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [ \ud835\udc53 (\ud835\udc4b)] = 22.0 \u00b7 0.2 + 13.0 \u00b7 0.3 + 7.4 \u00b7 0.5 = 12.0 The same principle applies when \ud835\udc5d(\ud835\udc65) is a continuous density, only with the summation replaced by integration. 292 A Mathematical Background Fig. A.3 Entropy of a binary discrete variable. Horizontal axis shows the probability that the variable takes value 1, the vertical axis shows the associated entropy of the distribution. High entropy implies high spread in the distribution, while low entropy implies little spread. A.2.5 Information Theory Information theory studies the amount of information that is present in distributions, and the way that we can compare distributions. A.2.5.1 Information The information \ud835\udc3c of an event \ud835\udc65 observed from distribution \ud835\udc5d(\ud835\udc4b) is defned as: \ud835\udc3c(\ud835\udc65) = \u2212 log \ud835\udc5d(\ud835\udc65). In words, the more likely an observation is (the higher \ud835\udc5d(\ud835\udc65)), the less information we get when we actually observe the event. In other words, the information of an event is the (potential) reduction of uncertainty. On the two extremes we have: \u2022 \ud835\udc5d(\ud835\udc65) = 0 : \ud835\udc3c(\ud835\udc65) = \u2212 log 0 = \u221e \u2022 \ud835\udc5d(\ud835\udc65) = 1 : \ud835\udc3c(\ud835\udc65) = \u2212 log 1 = 0 A.2.5.2 Entropy We defne the entropy \ud835\udc3b of a discrete distribution \ud835\udc5d(\ud835\udc4b) as \ud835\udc3b[\ud835\udc5d] = E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [\ud835\udc3c(\ud835\udc4b)] = E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [\u2212 log \ud835\udc5d(\ud835\udc4b)] = \u2212 \u2211\ufe01 \ud835\udc65 \ud835\udc5d(\ud835\udc65) log \ud835\udc5d(\ud835\udc65) (A.4) A.2 Probability Distributions 293 If the base of the logarithm is 2, then we measure it in bits. When the base of the logarithm is \ud835\udc52, then we measure the entropy in nats. The continuous version of the above equation is called the continuous entropy or diferential entropy. Informally, the entropy of a distribution is a measure of the amount of \u201cuncertainty\u201d in a distribution, i.e., a measure of its \u201cspread.\u201d We can nicely illustrate this with a binary variable (0/1), where we plot the probability of a 1 against the entropy of the distribution (Fig. A.3). We see that on the two extremes, the entropy of the distribution is 0 (no spread at all), while the entropy is maximal for \ud835\udc5d(\ud835\udc65 = 1) = 0.5 (and therefore \ud835\udc5d(\ud835\udc65 = 0) = 0.5), which gives maximal spread to the distribution. Example: The entropy of the distribution in the previous example is: \ud835\udc3b[\ud835\udc5d] = \u2212 \u2211\ufe01 \ud835\udc65 \ud835\udc5d(\ud835\udc65) log \ud835\udc5d(\ud835\udc65) = \u22120.2 \u00b7 ln 0.2 \u2212 0.3 \u00b7 ln 0.3 \u2212 0.5 \u00b7 ln 0.5 = 1.03 nats (A.5) A.2.5.3 Cross-entropy The cross-entropy is defned between two distributions \ud835\udc5d(\ud835\udc4b) and \ud835\udc5e(\ud835\udc4b) defned over the same support (sample space). The cross-entropy is given by: \ud835\udc3b[\ud835\udc5d, \ud835\udc5e] = E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) [\u2212 log \ud835\udc5e(\ud835\udc4b)] = \u2212 \u2211\ufe01 \ud835\udc65 \ud835\udc5d(\ud835\udc65) log \ud835\udc5e(\ud835\udc65) (A.6) When we do maximum likelihood estimation in supervised learning, then we actually minimize the cross-entropy between the data distribution and the model distribution. A.2.5.4 Kullback-Leibler Divergence For two distributions \ud835\udc5d(\ud835\udc4b) and \ud835\udc5e(\ud835\udc4b) we can also defne the relative entropy, better known as the Kullback-Leibler (KL) divergence \ud835\udc37KL: 294 A Mathematical Background \ud835\udc37KL[\ud835\udc5d||\ud835\udc5e] = E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) \ufffd \u2212 log \ud835\udc5e(\ud835\udc4b) \ud835\udc5d(\ud835\udc4b) \ufffd = \u2212 \u2211\ufe01 \ud835\udc65 \ud835\udc5d(\ud835\udc65) log \ud835\udc5e(\ud835\udc65) \ud835\udc5d(\ud835\udc65) (A.7) The Kullback-Leibler divergence is a measure of the distance between two distributions. The more two distributions depart from eachother, the higher the KLdivergence will be. Note that the KL-divergence is not symmetrical, \ud835\udc37KL[\ud835\udc5d||\ud835\udc5e] \u2260 \ud835\udc37KL[\ud835\udc5e||\ud835\udc5d] in general. Finally, we can also rewrite the KL-divergence as an entropy and cross-entropy, relating the previously introduced quantities: \ud835\udc37KL[\ud835\udc5d||\ud835\udc5e] = E\ud835\udc4b\u223c\ud835\udc5d(\ud835\udc4b) \ufffd \u2212 log \ud835\udc5e(\ud835\udc4b) \ud835\udc5d(\ud835\udc4b) \ufffd = \u2211\ufe01 \ud835\udc65 \ud835\udc5d(\ud835\udc65) log \ud835\udc5d(\ud835\udc65) \u2212 \u2211\ufe01 \ud835\udc65 \ud835\udc5d(\ud835\udc65) log \ud835\udc5e(\ud835\udc65) = \ud835\udc3b[\ud835\udc5d] + \ud835\udc3b[\ud835\udc5d, \ud835\udc5e] (A.8) Entropy, cross-entropy and KL-divergence are common in many machine learning domains, especially to construct loss functions. A.3 Derivative of an Expectation A key problem in gradient-based optimization, which appears in parts of machine learning, is getting the gradient of an expectation. We will here discuss one wellknown method:1 the REINFORCE estimator (in reinforcement learning), which is in other felds also know as the score function estimator, likelihood ratio method, and automated variational inference. Assume that we are interested in the gradient of an expectation, where the parameters appear in the distribution of the expectation:2 \u2207\ud835\udf03E\ud835\udc65\u223c\ud835\udc5d\ud835\udf03 (\ud835\udc65) [ \ud835\udc53 (\ud835\udc65)] (A.9) We cannot sample the above quantity, because we have to somehow move the gradient inside the expectation (and then we can sample the expectation to evaluate 1 Other methods to diferentiate through an expectation is through the reparametrization trick, as for example used in variational auto-encoders, but we will not further treat this topic here. 2 If the parameters only appear in the function \ud835\udc53 (\ud835\udc65) and not in \ud835\udc5d(\ud835\udc65), then we can simply push the gradient through the expectation A.4 Bellman Equations 295 it). To achieve this, we will use a simple rule regarding the gradient of the log of some function \ud835\udc54(\ud835\udc65): \u2207\ud835\udc65 log \ud835\udc54(\ud835\udc65) = \u2207\ud835\udc65\ud835\udc54(\ud835\udc65) \ud835\udc54(\ud835\udc65) (A.10) This results from simple application of the chain-rule. We will now expand Eq. A.9, where we midway apply the above log-derivative trick. \u2207\ud835\udf03E\ud835\udc65\u223c\ud835\udc5d\ud835\udf03 (\ud835\udc65) [ \ud835\udc53 (\ud835\udc65)] = \u2207\ud835\udf03 \u2211\ufe01 \ud835\udc65 \ud835\udc53 (\ud835\udc65) \u00b7 \ud835\udc5d\ud835\udf03 (\ud835\udc65) defnition of expectation = \u2211\ufe01 \ud835\udc65 \ud835\udc53 (\ud835\udc65) \u00b7 \u2207\ud835\udf03 \ud835\udc5d\ud835\udf03 (\ud835\udc65) push gradient through sum = \u2211\ufe01 \ud835\udc65 \ud835\udc53 (\ud835\udc65) \u00b7 \ud835\udc5d\ud835\udf03 (\ud835\udc65) \u00b7 \u2207\ud835\udf03 \ud835\udc5d\ud835\udf03 (\ud835\udc65) \ud835\udc5d\ud835\udf03 (\ud835\udc65) multiply/divide by \ud835\udc5d\ud835\udf03 (\ud835\udc65) = \u2211\ufe01 \ud835\udc65 \ud835\udc53 (\ud835\udc65) \u00b7 \ud835\udc5d\ud835\udf03 (\ud835\udc65) \u00b7 \u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03 (\ud835\udc65) log-der. rule (Eq. A.10) = E\ud835\udc65\u223c\ud835\udc5d\ud835\udf03 (\ud835\udc65) [ \ud835\udc53 (\ud835\udc65) \u00b7 \u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03 (\ud835\udc65)] rewrite into expectation What the above derivation essential does is pushing the derivative inside of the sum. This equally applies when we change the sum into an integral. Therefore, for any \ud835\udc5d\ud835\udf03 (\ud835\udc65), we have: \u2207\ud835\udf03E\ud835\udc65\u223c\ud835\udc5d\ud835\udf03 (\ud835\udc65) [ \ud835\udc53 (\ud835\udc65)] = E\ud835\udc65\u223c\ud835\udc5d\ud835\udf03 (\ud835\udc65) [ \ud835\udc53 (\ud835\udc65) \u00b7 \u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03 (\ud835\udc65)] (A.11) This is known as the log-derivative trick, score function estimator, or REINFORCE trick. Although the formula may look complicated, the interpretation is actually simple. We explain this idea in Fig. A.4. In Sect. 4.2.1 we apply this idea to reinforcement learning. A.4 Bellman Equations Bellman noted that the value function can be written in recursive form, because the value is also defned at the next states. In his work on dynamic programming [72], he derived recursive equations for \ud835\udc49 and \ud835\udc44. The Bellman equations for state-values and state-action values are: \ud835\udc49 \ud835\udf0b(\ud835\udc60) = E\ud835\udc4e\u223c\ud835\udf0b(\u00b7|\ud835\udc60)E\ud835\udc60\u2032\u223c\ud835\udc47\ud835\udc4e (\ud835\udc60) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \ud835\udc49 \ud835\udf0b(\ud835\udc60\u2032) \ufffd \ud835\udc44 \ud835\udf0b(\ud835\udc60, \ud835\udc4e) = E\ud835\udc60\u2032\u223c\ud835\udc47\ud835\udc4e (\ud835\udc60) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 E\ud835\udc4e\u2032\u223c\ud835\udf0b(\u00b7|\ud835\udc60\u2032) [\ud835\udc44 \ud835\udf0b(\ud835\udc60\u2032, \ud835\udc4e\u2032)] \ufffd Depending on whether the state and action space are discrete or continuous respectively, we write out these equations diferently. For a discrete state space and 296 A Mathematical Background Fig. A.4 Graphical illustration of REINFORCE estimator. Left: Example distribution \ud835\udc5d\ud835\udf03 (\ud835\udc65) and function \ud835\udc53 (\ud835\udc65). When we evaluate the expectation of Eq. A.11, we take \ud835\udc5a samples, indicated by the blue dots (in this case \ud835\udc5a = 8). The magnitude of \ud835\udc53 (\ud835\udc65) is shown with the red vertical arrows. Right: When we apply the gradient update, each sample pushes up the density at that location, but the magnitude of the push is multiplied by \ud835\udc53 (\ud835\udc65). Therefore, the higher \ud835\udc53 (\ud835\udc65), the harder we push. Since a density needs to integrate to 1, we will increase the density where we push hardest (in the example on the rightmost sample). The distribution will therefore shift to the right on this update. discrete action space, we write the expectations as summations: \ud835\udc49(\ud835\udc60) = \u2211\ufe01 \ud835\udc4e\u2208\ud835\udc34 \ud835\udf0b(\ud835\udc4e|\ud835\udc60) \ufffd \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \ud835\udc49(\ud835\udc60\u2032) \ufffd\ufffd \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \u2211\ufe01 \ud835\udc4e\u2208\ud835\udc34 \ud835\udf0b(\ud835\udc4e|\ud835\udc60)[\ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032)] \ufffd For continuous state and action spaces, the summations over policy and transition are replaced by integration: \ud835\udc49(\ud835\udc60) = \u222b \ud835\udc4e \ud835\udf0b(\ud835\udc4e|\ud835\udc60) \ufffd \u222b \ud835\udc60\u2032 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \ud835\udc49(\ud835\udc60\u2032) \ufffd d\ud835\udc60\u2032\ufffd d\ud835\udc4e The same principle applies to the Bellman equation for state-action values: \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \u222b \ud835\udc60\u2032 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \u222b \ud835\udc4e\u2032[\ud835\udf0b(\ud835\udc4e\u2032|\ud835\udc60\u2032) \u00b7 \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) \ufffd d\ud835\udc4e\u2032\ufffd d\ud835\udc60\u2032 We may also have a continuous state space (such as visual input) with a discrete action space (such as pressing buttons in a game): \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \u222b \ud835\udc60\u2032 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \u2211\ufe01 \ud835\udc4e\u2032 [\ud835\udf0b(\ud835\udc4e\u2032|\ud835\udc60\u2032) \u00b7 \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) \ufffd\ufffd d\ud835\udc60\u2032 Appendix B Deep Supervised Learning This appendix provides a chapter-length overview of essentials of machine learning and deep learning. Deep reinforcement learning uses much of the machinery of deep supervised learning, and a good understanding of deep supervised learning is essential. This appendix should provide you with the basics, in case your knowledge of basic machine learning and deep learning is rusty. In doubt? Try to answer the questions on page 328. We will start with machine learning basics. We will discuss training and testing, accuracy, the confusion matrix, generalization, and overftting. Next, we will provide an overview of deep learning. We will look into neural networks, error functions, training by gradient descent, end-to-end feature learning, and the curse of dimensionality. For neural networks we will discuss convolutional networks, recurrent networks, LSTM, and measures against overftting. Finally, on the practical side, we will discuss TensorFlow, Keras, and PyTorch. We will start with methods for solving large and complex problems. B.1 Machine Learning The goal of machine learning is generalization: to create an accurate, predictive, model of the world. Such an accurate model is said to generalize well to the world.1 In machine learning we operationalize this goal with two datasets, a training set and a test set. The feld of machine learning aims to ft a function to approximate an input/output relation. This can be, for example, a regression function or a classifcation function. The most basic form of machine learning is when a dataset \ud835\udc37 of input/output pairs is given. We call this form supervised learning, since the learning process is supervised by the output values. 1 Generalization is closely related to the concepts of overftting, regularization, and smoothness, as we will see in Sect. B.1.3. 297 298 B Deep Supervised Learning Predicted class P N Actual P TP FN class N FP TN Predicted class Cat Dog Actual Cat 122 8 class Dog 3 9 Table B.1 Confusion Matrix In machine learning we often deal with large problem domains. We are interested in a function that works not just on the particular values on which it was trained, but also in the rest of the problem domain from which the data items were taken. In this section we will frst see how such a learning process for generalization works; for notation and examples we follow from [524]. Next, we will discuss the specifc problems of large domains. Finally, we will discuss the phenomenon of overftting and how it relates to the bias-variance trade-of. B.1.1 Training Set and Test Set A machine learning algorithm must learn a function \u02c6\ud835\udc53 (\ud835\udc65) \u2192 \ud835\udc66 on a training set from input values \ud835\udc65 (such as images) to approximate corresponding output values \ud835\udc66 (such as image labels). The goal of the machine learning algorithm is to learn this function \u02c6\ud835\udc53 , such that it performs well on the training data and generalizes well to the test data. Let us see how we can measure how well the machine learning methods generalize. The notions that are used to assess the quality of the approximation are as follows. Let us assume that our problem is a classifcation task. The elements that our function correctly predicts are called the true positives (TP). Elements that are correctly identifed as not belonging to a class are the true negatives (TN). Elements that are mis-identifed as belonging to a class are the false positives (FP), and elements that belong to the class but that the predictor misclassifes are the false negatives (FN). Confusion Matrix: The number of true positives divided by the total number of positives (true and false, TP/(TP+FP)) is called the precision of the classifer. The number of true positives divided by the size of the class (TP/(TP+FN)) is the recall, or the number of relevant elements that the classifer could fnd. The term accuracy is defned as the total number of true positives and negatives divided by the total number of predictions (TP+TN)/(TP+TN+FP+FN): how well correct elements are predicted [93]. These numbers are often shown in the form of a confusion matrix (Table B.1). The numbers in the cells represent the number of true positives, etc., of the experiment. B.1 Machine Learning 299 In machine learning, we are often interested in the accuracy of a method on the training set, and whether the accuracy on the test set is the same as on the training set (generalization). In regression and classifcation problems the term error value is used to indicate the diference between the true output value \ud835\udc66 and the predicted output value \u02c6\ud835\udc53 (\ud835\udc65). Measuring how well a function generalizes is typically performed using a method called \ud835\udc58-fold cross-validation. This works as follows. When a dataset of input-output examples (\ud835\udc65, \ud835\udc66) is present, this set is split into a large training set and a smaller hold-out test set, typically 80/20 or 90/10. The approximator is trained on the training set until a certain level of accuracy is achieved. For example, the approximator may be a neural network whose parameters \ud835\udf03 are iteratively adjusted by gradient descent so that the error value on the training set is reduced to a suitably low value. The approximator function is said to generalize well, when the accuracy of the approximator is about the same on the test set as it is on the training set. (Sometimes a third dataset is used, the validation set, to allow stopping before overftting occurs, see Sect. B.2.7.) Since the test set and the training set contain examples that are drawn from the same original dataset, approximators can be expected to be able to generalize well. The testing is said to be in-distribution when training and test set are from the same distribution. Out-of-distribution generalization to diferent problems (or transfer learning) is more difcult, see Sect. 9.2.1. B.1.2 Curse of Dimensionality The state space of a problem is the space of all possible diferent states (combinations of values of variables). State spaces grow exponentially with the number of dimensions (variables); high-dimensional problems have large state spaces, and modern machine learning algorithms have to be able to learn functions in such large state spaces. Example: A classic problem in AI is image classifcation: predicting what type of object is pictured in an image. Imagine that we have low-resolution greyscale images of 100\u00d7100 pixels, where each pixel takes a discrete value between 0 and 255 (a byte). Then, the input space \ud835\udc4b \u2208 {0, 1, . . . , 255}100\u00b7100, the machine learning problem has dimensionality 100 \u00b7 100 = 10000, and the state space has size 25610000. When the input space \ud835\udc4b is high dimensional, we can never store the entire state space (all possible pixels with all possible values) as a table. The efect of an exponential need for observation data as the dimensionality grows, has been called the curse of dimensionality by Richard Bellman [72]. The curse of dimensionality states that the cardinality (number of unique points) of a space scales exponentially in the dimensionality of the problem. In a formula, we have that 300 B Deep Supervised Learning |\ud835\udc4b| \u223c exp(Dim(\ud835\udc4b)). Due to the curse of dimensionality, the size of a table to represent a function increases quickly when the size of the input space increases. Example: Imagine we have a discrete input space \ud835\udc4b = {0, 1}\ud835\udc37 that maps to a real number, \ud835\udc4c = R, and we want to store this function fully as a table. We will show the required size of the table and the required memory when we use 32-bit (4 byte) foating point numbers: Dim(\ud835\udc4b) |\ud835\udc4b| Memory 1 2 8 Byte 5 25 = 32 128 Byte 10 210 = 1024 4KB 20 220 \u2248 106 4MB 50 250 \u2248 1015 4.5 million TB 100 2100 \u2248 1030 5 \u00b7 1021 TB 265 2265 \u2248 2 \u00b7 1080 The table shows how quickly exponential growth develops. At a discrete space of size 20, it appears that we are doing alright, storing 4 Megabyte of information. However, at a size of 50, we suddenly need to store 4.5 million Terabyte. We can hardly imagine the numbers that follow. At an input dimensionality of size 265, our table would have grown to size 2\u00b71080, which is close to the estimated number of atoms in the universe. Since the size of the state space grows exponentially, but the number of observations typically does not, most state spaces in large machine learning problems are sparsely populated with observations. An important challenge for machine learning algorithms is to ft good predictive models on sparse data. To reliably estimate a function, each variable needs a certain number of observations. The number of samples that are needed to maintain statistical signifcance increases exponentially as the number of dimensions grows. A large amount of training data is required to ensure that there are several samples for each combination of values;2 however, datasets rarely grow exponentially. B.1.3 Overftting and the Bias-Variance Trade-Of Basic statistics tells us that, according to the law of large numbers, the more observations we have of an experiment, the more reliable the estimate of their value is (that is, the average will be close to the expected value) [93]. This has important 2 Unless we introduce some bias into the problem, by assuming smoothness, implying that there are dependencies between variables, and that the \u201ctrue\u201d number of independent variables is smaller than the number of pixels. B.1 Machine Learning 301 Fig. B.1 Curve ftting: does the curvy red line or the straight dashed blue line best generalize the information in the data points? implications for the study of large problems where we would like to have confdence in the estimated values of our parameters. In small toy problems the number of variables of the model is also small. Singlevariable linear regression problems model the function as a straight line \ud835\udc66 = \ud835\udc4e \u00b7\ud835\udc65 +\ud835\udc4f, with only one independent variable \ud835\udc65 and two parameters \ud835\udc4e and \ud835\udc4f. Typically, when regression is performed with a small number of independent variables, then a relatively large number of observations is available per variable, giving confdence in the estimated parameter values. A rule of thumb is that there should be 5 or more training examples for each variable [769]. In machine learning dimensionality typically means the number of variables of a model. In statistics, dimensionality can be a relative concept: the ratio of the number of variables compared to the number of observations. In practice, the size of the observation dataset is limited, and then absolute and relative dimensionality do not difer much. In this book we follow the machine learning defnition of dimensions meaning variables. Modeling high-dimensional problems well typically requires a model with many parameters, the so-called high-capacity models. Let us look deeper into the consequences of working with high-dimensional problems. To see how to best ft our model, let us consider machine learning as a curve ftting problem, see Fig. B.1. In many problems, the observations are measurements of an underlying natural process. The observations therefore contain some measurement noise. The goal is (1) that the approximating curve fts the (noisy) observations as accurately as possible, but, (2) in order to generalize well, to aim to ft the signal, not the noise. How complex should the approximator be to faithfully capture the essence of a natural, noisy, process? You can think of this question as: how many parameters \ud835\udf03 the network should have, or, if the approximator is a polynomial, what the degree 302 B Deep Supervised Learning Environment Model Relative Chance of Observations Variables Dimensionality \ud835\udc5b > \ud835\udc51 low underftting, high bias \ud835\udc5b < \ud835\udc51 high overftting, high variance Table B.2 Observations, Variables, Relative Dimensionality, Overftting Fig. B.2 Bias-variance trade-of; few-parameter model: high bias; many-parameter model: high variance \ud835\udc51 of the polynomial should be? The complexity of the approximator is also called the capacity of the model, for the amount of information that it can contain (see Table B.2). When the capacity of the approximator \ud835\udc51 is lower than the number of observations \ud835\udc5b, then the model is likely to underft: the curve is too simple and cannot reach all observations well, the error is large and the accuracy is low.3 Conversely, when the number of coefcients \ud835\udc51 is as high or higher than the number of observations \ud835\udc5b, then many machine learning procedures wil be able to fnd a good ft. The error on the training set will be zero, and training accuracy reaches 100%. Will this high-capacity approximator generalize well to unseen states? Most likely it will not, since the training and test observations are from a real-world process and observations contain noise from the training set. The training noise will be modeled perfectly, and the trained function will have low accuracy on the test set and other datasets. A high-capacity model (\ud835\udc51 > \ud835\udc5b) that trains well but tests poorly is said to overft on the training data. Underftting and overftting are related to the so-called bias-variance trade-of, see Fig. B.2. High-capacity models ft \u201cpeaky\u201d high-variance curves that can ft signal and noise of the observations, and tend to overft. Low-capacity models ft \u201cstraighter\u201d low-variance curves that have higher bias and tend to underft the observations. Models with a capacity of \ud835\udc51 \u2248 \ud835\udc5b can have both good bias and variance [93]. 3 Overftting can be reduced in the loss function and training procedure, see Sect. B.2.7. B.2 Deep Learning 303 Fig. B.3 A Single Biological neuron [821] Preventing underftting and overftting is a matter of matching the capacity of our model so that \ud835\udc51 matches \ud835\udc5b. This is a delicate trade-of, since reducing the capacity also reduces expressive power of the models. To reduce overftting, we can use regularization, and many regularization methods have been devised. Regularization has the efect of dynamically adjusting capacity to the number of observations. Regularization\u2014the World is Smooth Generalization is closely related to regularization. Most functions in the real world\u2014 the functions that we wish to learn in machine learning\u2014are smooth: near similar input leads to near similar output. Few real-world functions are jagged, or locally random (when they are, they are often contrived examples). This has lead to the introduction of regularization methods, to restrict or smooth the behavior of models. Regularization methods allow us to use high capacity models, learn the complex function, and then later introduce a smoothing procedure, to reduce too much of the randomness and jaggedness. Regularization may for example restrict the weights of variables by moving the values closer to zero, but many other methods exist, as we will see. Diferent techniques for regularization have been developed for high-capacity deep neural models, and we discuss them in Sect. B.2.7. The goal is to flter out random noise, but at the same time to allow meaningful trends to be recognized, to smooth the function without restricting complex shapes [280]. Before we delve into regularization methods, let us have a look in more detail at how to implement parameterized functions, for which we will use neural networks and deep learning. B.2 Deep Learning The architecture of artifcial neural networks is inspired by the architecture of biological neural networks, such as the human brain. Neural networks consist of 304 B Deep Supervised Learning \ud835\udc651 \ud835\udc652 \ud835\udc661 \ud835\udc662 Fig. B.4 A Fully Connected Shallow Neural Network with 9 Neurons neural core cells that are connected by nerve cells [63]. Figure B.3 shows a drawing of a biological neuron, with a nucleus, an axon, and dendrites [821]. Figure B.4 shows a simple fully connected artifcial neural network, with an input layer of two neurons, an output layer of two neurons, and a single hidden layer of fve neurons. A neural network with a single hidden layer is called shallow. When the network has more hidden layers it is called deep (Fig. B.5). In this section, we provide an overview of artifcial neural networks and their training algorithms. We provide enough detail to understand the deep reinforcement learning concepts in this book. Space does provide a limit to how deep we can go. Please refer to specialized deep learning literature for more information, such as [280]. We provide a conceptual overview, which should be enough to successfully use existing high quality deep learning packages, such as TensorFlowa [1] or PyTorch [591].b a https://www.tensorflow.org b https://pytorch.org B.2.1 Weights, Neurons Neural networks consist of neurons and connections, typically organized in layers. The neurons process their input signals as a weighted combination, producing an output signal. This calculation is called the activation function, or squashing function, since it is non-linear. Popular activation functions are the rectifed linear unit (ReLU: partly linear, partly zero), the hyperbolic tangent, and the sigmoid or logistic function 1 1+\ud835\udc52\u2212\ud835\udc4e for neuron activation \ud835\udc4e. The neurons are connected B.2 Deep Learning 305 Fig. B.5 Four fully connected hidden layers [652] by weights. At each neuron \ud835\udc57 the incoming weights \ud835\udc56\ud835\udc57 are summed \ufffd and then processed by the activation function \ud835\udf0e. The output \ud835\udc5c of neuron \ud835\udc57 is therefore: \ud835\udc5c \ud835\udc57 = \ud835\udf0e( \u2211\ufe01 \ud835\udc56 \ud835\udc5c\ud835\udc56\ud835\udc64\ud835\udc56 \ud835\udc57) for weight \ud835\udc56\ud835\udc57 of predecessor neuron \ud835\udc5c\ud835\udc56. The outputs of this layer of neurons are fed to the inputs for the weights of the next layer. B.2.2 Backpropagation The neural network as a whole is a parameterized function \ud835\udc53\ud835\udf03 (\ud835\udc65) \u2192 \u02c6\ud835\udc66 that converts input to an output approximation. The behavior depends on the parameters \ud835\udf03, also known as the network weights. The parameters are adjusted such that the required input-output relation is achieved (training the network). This is done by minimizing the error (or loss) function that calculates the diference between the network output \u02c6\ud835\udc66 and the training target \ud835\udc66. The training process consists of training epochs, individual passes in which the network weights are optimized towards the target, using a method called gradient descent (since the goal is to minimize the error function). An epoch is one complete pass over the training data. Epochs are usually done in batches. Since training is an iterative optimization process, it is typical to train for multiple epochs. Listing B.1 shows simplifed pseudocode for the gradient descent training algorithm (based on [280]). When training starts, the weights of the network are initialized to 306 B Deep Supervised Learning 1 def train_sl(data , net , alpha =0.001): # train classifier 2 for epoch in range(max_epochs ): # an epoch is one pass 3 sum_sq = 0 # reset to zero for each pass 4 for (image , label) in data: 5 output = net. forward_pass (image) # predict 6 sum_sq += (output - label)**2 # compute error 7 grad = net.gradient(sum_sq) # derivative of error 8 net. backward_pass (grad , alpha) # adjust weights 9 return net Listing B.1 Network training pseudocode for supervised learning small random numbers. Each epoch consists of a forward pass (recognition, usage) and a backward pass (training, adjustment). The forward pass is just the regular recognition operation for which the network is designed. The input layer is exposed to the input (the image), which is then propagated through the network to the output layers, using the weights and activation functions. The output layer provides the answer, by having a high value at the neuron corresponding to the right label (such as Cat or Dog, or the correct number), so that an error can be calculated to be used to adjust the weights in the backward pass. The listing shows a basic version of gradient descent, that calculates the gradient over all examples in the dataset, and then updates the weights. Batch versions of gradient descent update the weights after smaller subsets, and are typically quicker. Loss Function At the output layer the propagated value \u02c6\ud835\udc66 is compared with the other part of the example pair, the label \ud835\udc66. The diference with the label is calculated, yielding the error. The error function is also known as the loss function L. Two common error functions are the mean squared error 1 \ud835\udc5b \ufffd\ud835\udc5b \ud835\udc56 (\ud835\udc66\ud835\udc56 \u2212 \u02c6\ud835\udc66\ud835\udc56)2 (for regression) and the cross-entropy error \u2212 \ufffd\ud835\udc40 \ud835\udc56 \ud835\udc66\ud835\udc56 log \u02c6\ud835\udc66\ud835\udc56 (for classifcation of \ud835\udc40 classes). The backward pass uses the diference between the forward recognition outcome and the true label to adjust the weights, so that the error becomes smaller. This method uses the gradient of the error function over the weights, and is called gradient descent. The parameters are adjusted as follows: \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udefc\u2207\ud835\udf03\ud835\udc61 L\ud835\udc37( \ud835\udc53\ud835\udf03\ud835\udc61) where \ud835\udf03 are the network parameters, \ud835\udc61 is the optimization time step, \ud835\udefc is the learning rate, \u2207\ud835\udf03\ud835\udc61 are the current gradient of the loss function of data L\ud835\udc37, and \ud835\udc53\ud835\udf03 is the parameterized objective function. The training process can be stopped when the error has been reduced below a certain threshold for a single example, or when the loss on an entire validation set B.2 Deep Learning 307 has dropped sufciently. More elaborate stopping criteria can be used in relation to overftting (see Sect. B.2.7). Most neural nets are trained using a stochastic version of gradient descent, or SGD [723]. SGD samples a minibatch of size smaller than the total dataset, and thereby computes a noisy estimate of the true gradient. This is faster per update step, and does not afect the direction of the gradient too much. See Goodfellow et al. [280] for details. B.2.3 End-to-end Feature Learning Let us now look in more detail at how neural networks can be used to implement end-to-end feature learning. We can approximate a function through the discovery of common features in states. Let us, again, concentrate on image recognition. Traditionally, feature discovery was a manual process. Image-specialists would painstakingly pour over images to identify common features in a dataset, such as lines, squares, circles, and angles, by hand. They would write small pre-processing algorithms to recognize the features that were then used with classical machine learning methods such as decision trees, support vector machines, or principal component analysis to construct recognizers to classify an image. This hand-crafted method is a laborintensive and error prone process, and researchers have worked to fnd algorithms for the full image recognition process, end-to-end. For this to work, also the features must be learned. For example, if the function approximator consists of the sum of \ud835\udc5b features, then with hand-crafted features, only the coefcients \ud835\udc50\ud835\udc56 of the features in the function \u210e(\ud835\udc60) = \ud835\udc501 \u00d7 \ud835\udc531(\ud835\udc60) + \ud835\udc502 \u00d7 \ud835\udc532(\ud835\udc60) + \ud835\udc503 \u00d7 \ud835\udc533(\ud835\udc60) + . . . + \ud835\udc50\ud835\udc5b \u00d7 \ud835\udc53\ud835\udc5b(\ud835\udc60) are learned. In end-to-end learning, the coefcients \ud835\udc50\ud835\udc56 and the features \ud835\udc53\ud835\udc56(\ud835\udc60) are learned. Deep end-to-end learning has achieved great success in image recognition, speech recognition, and natural language processing [431, 287, 853, 193]. Endto-end learning is the learning of a classifer directly from high-dimensional, raw, un-pre-processed, pixel data, all the way to the classifcation layer, as opposed to learning pre-processed data from intermediate (lower dimensional) hand-crafted features. We will now see in more detail how neural networks can perform automated feature discovery. The hierarchy of network layers together can recognize a hierarchy of low-to-high level concepts [459, 462]. For example, in face recognition (Fig. B.6) the frst hidden layer may encode edges; the second layer then composes and encodes simple structures of edges; the third layer may encode higher-level concepts such as noses or eyes; and the fourth layer may work at the abstraction level of a face. Deep feature learning fnds what to abstract at which level on its 308 B Deep Supervised Learning Fig. B.6 Layers of features of increasing complexity [462] Function Input Output Dataset Regression continuous (number) continuous (number) Classifcation discrete (image) discrete (class label) Environment Value \ud835\udc49 state continuous (number) Action-value \ud835\udc44 state \u00d7 action continuous (number) Policy \ud835\udf0b state action(-distribution) Table B.3 Functions that are Frequently Approximated own [76], and can come up with classes of intermediate concepts, that work, but look counterintuitive upon inspection by humans. Towards the end of the 1990s the work on neural networks moved into deep learning, a term coined by Dechter in [187]. LeCun et al. [461] published an infuential paper on deep convolutional nets. The paper introduced the architecture LeNet-5, a seven-layer convolutional neural net trained to classify handwritten MNIST digits from 32 \u00d7 32 pixel images. Listing B.2 shows a modern rendering of LeNet in Keras. The code straightforwardly lists the layer defnitions. End-to-end learning is computationally quite demanding. After the turn of the century, methods, datasets, and compute power had improved to such an extent that full raw, un-pre-processed pictures could be learned, without the intermediate step of hand-crafting features. End-to-end learning proved very powerful, achieving higher accuracy in image recognition than previous methods, and even higher than human test subjects [431]. In natural language processing, deep transformer models such as BERT and GPT-2 and 3 have reached equally impressive results [193, 114]. B.2 Deep Learning 309 1 2 def lenet_model(img_shape =(28 , 28, 1), n_classes =10, l2_reg =0., 3 weights=None): 4 5 # Initialize model 6 lenet = Sequential () 7 8 # 2 sets of CRP (Convolution , RELU , Pooling) 9 lenet.add(Conv2D (20, (5, 5), padding=\"same\", 10 input_shape =img_shape , kernel_regularizer =l2( l2_reg))) 11 lenet.add(Activation (\"relu\")) 12 lenet.add( MaxPooling2D (pool_size =(2, 2), strides =(2, 2))) 13 14 lenet.add(Conv2D (50, (5, 5), padding=\"same\", 15 kernel_regularizer =l2(l2_reg))) 16 lenet.add(Activation (\"relu\")) 17 lenet.add( MaxPooling2D (pool_size =(2, 2), strides =(2, 2))) 18 19 # Fully connected layers (w/ RELU) 20 lenet.add(Flatten ()) 21 lenet.add(Dense (500 , kernel_regularizer =l2(l2_reg))) 22 lenet.add(Activation (\"relu\")) 23 24 # Softmax (for classification ) 25 lenet.add(Dense(n_classes , kernel_regularizer =l2(l2_reg)) ) 26 lenet.add(Activation (\"softmax\")) 27 28 if weights is not None: 29 lenet. load_weights (weights) 30 31 # Return the constructed network 32 return lenet Listing B.2 LeNet-5 code in Keras [461, 832] Function Approximation Let us have a look at the diferent kinds of functions that we wish to approximate in machine learning. The most basic function establishing an input/output relation is regression, which outputs a continuous number. Another important function is classifcation, which outputs a discrete number. Regression and classifcation are often learned through supervision, with a dataset of examples (observations) and labels. In reinforcement learning, three functions are typically approximated: the value function \ud835\udc49(\ud835\udc60), that relates states to their expected cumulative future rewards, the action-value function \ud835\udc44(\ud835\udc60, \ud835\udc4e) that relate actions to their values, and the policy 310 B Deep Supervised Learning function \ud835\udf0b(\ud835\udc60) that relates states to an action (or \ud835\udf0b(\ud835\udc4e|\ud835\udc60) to an action distribution).4 In reinforcement learning, the functions \ud835\udc49, \ud835\udc44, \ud835\udf0b are learned through reinforcement by the environment. Table B.3 summarizes these functions. B.2.4 Convolutional Networks The frst neural networks consisted of fully connected layers (Fig. B.5). In image recognition, the input layer of a neural network is typically connected directly to the input image. Higher resolution images therefore need a higher number of input neurons. If all layers would have more neurons, then the width of the network grows quickly. Unfortunately, growing a fully connected network (see Fig. B.4) by increasing its width (number of neurons per layer) will increase the number of parameters quadratically. The naive solution of high-resolution problem learning is to increase the capacity of the model \ud835\udc5a. However, because of the problem of overftting, as \ud835\udc5a grows, so must the number of examples, \ud835\udc5b. The solution lies in using a sparse interconnection structure instead of a fully connected network. Convolutional neural nets (CNNs) take their inspiration from biology. The visual cortex in animals and humans is not fully connected, but locally connected [357, 358, 503]. Convolutions efciently exploit prior knowledge about the structure of the data: patterns reoccur at diferent locations in the data (translation invariance), and therefore we can share parameters by moving a convolutional window over the image. A CNN consists of convolutional operators or flters. A typical convolution operator has a small receptive feld (it only connects to a limited number of neurons, say 5 \u00d7 5), whereas a fully connected neuron connects to all neurons in the layer below. Convolutional flters detect the presence of local patterns. The next layer thus acts as a feature map. A CNN layer can be seen as a set of learnable flters, invariant for local transformations [280]. Filters can be used to identify features. Features are basic elements such as edges, straight lines, round lines, curves, and colors. To work as a curve detector, for example, the flter should have a pixel structure with high values indicating a shape of a curve. By then multiplying and adding these flter values with the pixel values, we can detect whether the shape is present. The sum of the multiplications in the input image will be large if there is a shape that resembles the curve in the flter. This flter can only detect a certain shape of curve. Other flters can detect other shapes. Larger activation maps can recognize more elements in the input image. Adding more flters increases the size of the network, which efectively enlarges the activation map. The flters in the frst network layer process (\u201cconvolve\u201d) the input image and fre (have high values) when a specifc feature that it is built to 4 In Chap. 5, on model-based learning, we also approximate the transition function \ud835\udc47\ud835\udc4e (\u00b7) and the reward function \ud835\udc45\ud835\udc4e (\u00b7). B.2 Deep Learning 311 detect is in the input image. Training a convolutional net is training a flter that consists of layers of subflters. By going through the convolutional layers of the network, increasingly complex features can be represented in the activation maps. Once they are trained, they can be used for as many recognition tasks as needed. A recognition task consists of a single quick forward pass through the network. Let us spend some more time on understanding these flters. Shared Weights In CNNs the flter parameters are shared in a layer. Each layer thus defnes a flter operation. A flter is defned by few parameters but is applied to many pixels of the image; each flter is replicated across the entire visual feld. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specifc response feld. Replicating units in this way allows for features to be detected regardless of their position in the visual feld, thus constituting the property of translation invariance. This weight sharing is also important to prevent an increase in the number of weights in deep and wide nets, and to prevent overftting, as we shall see later. Real-world images consist of repetitions of many smaller elements. Due to this so-called translation invariance, the same patterns reappear throughout an image. CNNs can take advantage of this. The weights of the links are shared, resulting in a large reduction in the number of weights that have to be trained. Mathematically CNNs put constraints on what the weight values can be. This is a signifcant advantage of CNNs, since the computational requirements of training the weights of fully connected layers are prohibitive. In addition, statistical strength is gained, since the efective data per weight increases. Deep CNNs work well in image recognition tasks, for visual fltering operations in spatial dependencies, and for feature recognition (edges, shapes) [460].5 CNN Architecture Convolutions recognize features\u2014the deeper the network, the more complex the features. A typical CNN architecture consists of a number of stacked convolutional layers. In the fnal layers, fully connected layers are used to then classify the inputs. In the convolutional layers, by connecting only locally, the number of weights is dramatically reduced in comparison with a fully connected net. The ability of a single neuron to recognize diferent features, however, is less than that of a fully connected neuron. 5 Interestingly, this paper was already published in 1989. The deep learning revolution happened twenty years later, when publicly available datasets, more efcient algorithms, and more compute power in the form of GPUs were available. 312 B Deep Supervised Learning Fig. B.7 Convolutional network example architecture [652] Fig. B.8 Max and average 2 \u00d7 2 pooling [280] By stacking many such locally connected layers on top of each other we can achieve the desired nonlinear flters whose joint efect becomes increasingly global.6 The neurons become responsive to a larger region of pixel space, so that the network frst creates representations of small parts of the input, and from these representations create larger areas. The network can recognize and represent increasingly complex concepts without an explosion of weights. A typical CNN architecture consists of an architecture of multiple layers of convolution, max pooling, and ReLU layers, topped of by a fully connected layer (Fig. B.7).7 Max Pooling A further method for reducing the number of weights is weight pooling. Pooling is a kind of nonlinear downsampling (expressing the information in lower resolution 6 Nonlinearity is essential. If all neurons performed linearly, then there would be no need for layers. Linear recognition functions cannot discriminate between cats and dogs. 7 Often with a softmax function. The softmax function normalizes an input vector of real numbers to a probability distribution [0, 1]; \ud835\udc5d\ud835\udf03 (\ud835\udc66|\ud835\udc65) = softmax( \ud835\udc53\ud835\udf03 (\ud835\udc65)) = \ud835\udc52 \ud835\udc53\ud835\udf03 (\ud835\udc65) \ufffd \ud835\udc58 \ud835\udc52 \ud835\udc53\ud835\udf03,\ud835\udc58 (\ud835\udc65) B.2 Deep Learning 313 Fig. B.9 RNN \ud835\udc65\ud835\udc61 is an input vector, \u210e\ud835\udc61 is the output/prediction, and \ud835\udc34 is the RNN [567] Fig. B.10 RNN unrolled in time [567] with fewer bits). Typically, a 2 \u00d7 2 block is sampled down to a scalar value (Fig. B.8). Pooling reduces the dimension of the network. The most frequently used form is max pooling. It is an important component for object detection [153] and is an integral part of most CNN architectures. Max pooling also allows small translations, such as shifting the object by a few pixels, or scaling, such as putting the object closer to the camera. B.2.5 Recurrent Networks Image recognition has had a large impact on network architectures, leading to innovations such as convolutional nets for spatial data. Speech recognition and time series analysis have also caused new architectures to be created, for sequential data. Such sequences can be modeled by recurrent neural nets (RNN) [84, 240]. Some of the better known RNNs are Hopfeld networks [349], and long short-term memory (LSTM) [345]. Figure B.9 shows a basic recurrent neural network. An RNN neuron is the same as a normal neuron, with input, output, and activation function. However, RNN neurons have an extra pair of looping input/output connections. Through this structure, the values of the parameters in an RNN can evolve. In efect, RNNs have a variable-like state. To understand how RNNs work, it helps to unroll the network, as has been done in Fig. B.10. The recurrent neuron loops have been drawn as a straight line to show 314 B Deep Supervised Learning Fig. B.11 RNN confgurations [399] the network in a deeply layered style, with connections between the layers. In reality the layers are time steps in the processing of the recurrent connections. In a sense, an RNN is a deeply layered neural net folded into a single layer of recurrent neurons. Where deep convolutional networks are successful in image classifcation, RNNs are used for tasks with a sequential nature, such as captioning challenges. In a captioning task the network is shown a picture, and then has to come up with a textual description that makes sense [816]. The main innovation of recurrent nets is that they allow us to work with sequences of vectors. Figure B.11 shows diferent combinations of sequences that we will discuss now, following an accessible and well-illustrated blog on the diferent RNN confgurations written by Karpathy [399]. There can be sequences in the input, in the output, or in both. The fgure shows diferent rectangles. Each rectangle is a vector. Arrows represent computations, such as matrix multiply. Input vectors are in red, output vectors are in blue, and green vectors hold the state. From left to right we see: 1. One to one, the standard network without RNN. This network maps a fxed-sized input to fxed-sized output, such as an image classifcation task (picture in/class out). 2. One to many adds a sequence in the output. This can be an image captioning task that takes an image and outputs a sentence of words. 3. Many to one is the opposite, with a sequence in the input. Think for example of sentiment analysis (a sentence is classifed for words with negative or positive emotional meaning). 4. Many to many has both a sequence for input and a sequence for output. This can be the case in machine translation, where a sentence in English is read and then a sentence in Fran\u00e7ais is produced. 5. Many to many is a related but diferent situation, with synchronized input and output sequences. This can be the case in video classifcation where each frame of the video should be labeled. B.2 Deep Learning 315 Fig. B.12 LSTM [399] Long Short-Term Memory Time series prediction is more complex than conventional regression or classifcation. It adds the complexity of a sequence dependence among the input variables. LSTM (long short-term memory) is a more powerful type of neuron designed to handle sequences. Figure B.12 shows the LSTM module, allowing comparison with a simple RNN. LSTMs are designed for sequential problems, such as time series, and planning. LSTMs were introduces by Hochreiter and Schmidhuber [345]. RNN training sufers from the vanishing gradient problem. For short-term sequences this problem may be controllable by the same methods as for deep CNN [280]. For long-term remembering LSTM are better suited. LSTMs are frequently used to solve diverse problems [675, 286, 288, 271], and we encounter them at many places throughout this book. B.2.6 More Network Architectures Deep learning is a highly active feld of research, in which many advanced network architectures have been developed. We will describe some of the better known architectures. Residual Networks An important innovation on CNNs is the residual network architecture, or ResNets. This idea was introduced in the 2015 ImageNet challenge, which He et al. [321] won with a very low error rate of 3.57%. This error rate is actually lower than what most humans achieve: 5\u201310%. ResNet has no fewer than 152 layers. Residual nets introduce skip links. Skip links are connections skipping one or more layers, allowing the training to go directly to other layers, reducing the efective depth of the network (Fig. B.13). Skip links create a mixture of a shallow and a deep network, preventing the accuracy degradation and vanishing gradients of deep networks [280]. 316 B Deep Supervised Learning Fig. B.13 Residual Net with Skip Links [321] Generative Adversarial Networks Normally neural networks are used in forward mode, to discriminate input images into classes, going from high dimensional to low dimensional. Networks can also be run backwards, to generate an image that goes with a certain class, going from low dimensional to high dimensional.8 Going from small to large implies many possibilities for the image to be instantiated. Extra input is needed to fll in the degrees of freedom. Running the recognizers backwards, in generative mode, has created an active research area called deep generative modeling. An important type of generative model that has made quite an impact is the Generative adversarial network, or GAN [281]. Deep networks are susceptible to adversarial attacks. A well-known problem of the image recognition process is that it is brittle. It was found that if an image is slightly perturbed, and imperceptibly to the human eye, deep networks can easily be fooled to characterize an image as the wrong category [747]. This brittleness is known as the one-pixel problem: changing a single unimportant pixel in an image could cause classifers to switch from classifying an image from cat to dog [730, 747]. GANs are used to generate input images that are slightly diferent from the original input. GANs generate adversarial examples whose purpose it is to fool the discriminator (recognizer). The frst network, the generator, generates an image. The second network, the discriminator, tries to recognize the image. The goal for the generator is to mislead the discriminator, in order to improve the robustness of the discriminator. In this way, GANs can be used to make image recognition more robust. The one-pixel problem has spawned an active area of research to understand this problem, and to make deep networks more robust. Another use of generative networks is to generate artifcial photo-realistic images known as deep fake images [857] and deep dreaming [402], see Fig. B.14 and 8 Just like the decoding phase of autoencoders. B.2 Deep Learning 317 Fig. B.14 Deep fakes [857] Fig. B.15.9 GANs have signifcantly increased our theoretical understanding of deep learning. Autoencoders Autoencoders and variational autoencoders (VAE) are used in deep learning for dimensionality reduction, in unsupervised learning [428, 411, 412]. An autoencoder network has a butterfy-like architecture, with the same number of neurons in the input and the output layers, but a decreasing layer-size as we go to the center (Fig. B.16). The input (contracting) side is said to perform a discriminative action, such as image classifcation, and the other (expanding) side is generative [281]. When an image is fed to both the input and the output of the autoencoder, results in the center layers are exposed to a compression/decompression process, resulting in the same image, only smoothed. The discriminative/generative process performed by autoencoders reduces the dimensionality, and is generally thought of as going to the \u201cessence\u201d of a problem, de-noising it [342]. 9 Deep Dream Generator at https://deepdreamgenerator.com 318 B Deep Supervised Learning Fig. B.15 Deep dream [402] Fig. B.16 Autoencoder, fnding the \u201cessence\u201d of the data in the middle [538] Autoencoding illustrates a deep relation between supervised and unsupervised learning. The architecture of an autoencoders consists of an encoder and a decoder. The encoder is a regular discriminative network, as is common in supervised learning. The decoder is a generative network, creating high dimensional output from low dimensional input. Together, the encoder/decoder butterfy perform dimensionality reduction, or compression, a form of unsupervised learning[294]. Autoencoders and generative networks are an active area of research. B.2 Deep Learning 319 Fig. B.17 Attention Architecture [47, 490] Fig. B.18 Example for Attention Mechanism [490] Attention Mechanism Another important architecture is the attention mechanism. Sequence to sequence learning occurs in many applications of machine learning, for example, in machine translation. The dimensionality of the output of basic RNNs is the same as the input. The attention architecture allows a fexible mapping between input and output dimensionality [739]. It does so by augmenting the RNNs with an extra network that focuses attention over the sequence of encoder RNN states [47], see Fig. B.17. The extra network focuses perception and memory access. It has been shown to achieve state of the art results in machine translation and natural language tasks [47], see Fig. B.18. The attention mechanism is especially useful for time series forecasting and translation. 320 B Deep Supervised Learning Transformers Finally, we discuss the transformer architecture. The transformer architecture is introduced by Vaswani et al. [805]. Transformers are based on the concept of selfattention: they use attention encoder-decoder models, but weigh the infuence of diferent parts of the data. They are central to the highly successful BERT [193] and GPT-3 [617, 114] natural language models, but have also been successful in imaginative text-to-image creation. Transformer-based foundation models play an important role in multi-modal (text/image) learning [625, 616], such as the one that we saw in Fig. 10.1. Treating deep reinforcement learning as a sequence problem, transformers are also being applied to reinforcement learning, with some early success [140, 377]. Transformers are an active area of research. For a detailed explanation of how they work, see, for example [12, 94, 805]. B.2.7 Overftting Now that we have discussed network architectures, it is time to discuss an important problem of deep neural networks: overftting, and how we can reduce it. Overftting in neural networks can be reduced in a number of ways. Some of the methods are aimed at restoring the balance between the number of network parameters and the number of training examples, others on data augmentation and capacity reduction. Another approach is to look at the training process. Let us list the most popular approaches [280]. \u2022 Data Augmentation Overftting occurs when there are more parameters in the network than examples to train on. The training dataset is increased through manipulations such as rotations, refections, noise, rescaling, etc. A disadvantage of this method is that the computational cost of training increases. \u2022 Capacity Reduction Another solution to overftting lies in the realization that overftting is a result of the network having too large a capacity; the network has too many parameters. A cheap way of preventing this situation is to reduce the capacity of the network, by reducing the width and depth of the network. \u2022 Dropout A popular method to reduce overftting is to introduce dropout layers into the networks. Dropout reduces the efective capacity of the network by stochastically dropping a certain percentage of neurons from the backpropagation process [344, 725]. Dropout is an efective and computationally efcient method to reduce overftting [280]. \u2022 L1 and L2 Regularization Regularization involves adding an extra term to the loss function that forces the network to not be too complex. The term penalizes the model for using too high weight values. This limits fexibility, but also encourages building solutions based on multiple features. Two popular versions of this method are L1 and L2 regularization [559, 280]. B.3 Datasets and Software 321 \u2022 Early Stopping Early stopping is based on the observation that overftting can be regarded as a consequence of so-called overtraining (training that progresses beyond the signal, into the noise). By terminating the training process earlier, for example by using a higher stopping threshold for the error function, we can prevent overftting from occurring [130, 607, 608]. A convenient and popular way is to add a third set to the training set/test set duo which then becomes a training set, a test set, and a holdout validation set. After each training epoch, the network is evaluated against the holdout validation set, to see if under- or overftting occurs, and if we should stop training. In this way, overftting can be prevented dynamically during training [608, 93, 280]. \u2022 Batch Normalization Another method is batch normalization. Batch normalization periodically normalizes the input to the layers [370]. This has many benefts, including a reduction of overftting. Overftting and regularization are an important topic of research. In fact, a basic question is why large neural networks perform so well at all. The network capacity is in the millions to billions of parameters, much larger than the number of observations, yet networks perform well. There appears to be a regime, beyond where performance sufers due to overftting, where performance increases as we continue to increase the capacity of our networks [68, 66]. Belkin et al. have performed studies on interpolation in SGD. Their studies suggests an implicit regularization regime of many parameters beyond overftting, where SGD generalizes well to test data, explaining in part the good results in practice of deep learning. Nakkiran et al. report similar experimental results, termed the double descent phenomenon [550, 392]. Research into the nature of overftting is active [68, 491, 66, 67, 867]. B.3 Datasets and Software We have discussed in depth background concepts in machine learning, and important aspects of the theory of neural networks. It is time to turn our attention to practical matters. Here we encounter a rich feld of data, environments, software and blogs on how to use deep learning in practice. The feld of deep reinforcement learning is an open feld. Researchers release their algorithms and code allowing replication of results. Datasets and trained networks are shared. The barrier to entry is low: high-quality software is available on GitHub for you to download and start doing research in the feld. We point to code bases and open software suites at GitHub throughout the book, Appendix C has pointers to software environments and open-source code frameworks. The most popular deep learning packages are PyTorch [591] and TensorFlow [1], and its top-level language Keras [147]. Some machine learning and mathematical packages also ofer deep learning tools, such as scikit-learn, MATLAB and R. In this section we will start with some easy classifcation and behavior examples, using the Keras library. We will use Python as our programming language. Python has become the language of choice for machine learning packages; not just for 322 B Deep Supervised Learning Fig. B.19 Some MNIST images [461] PyTorch and TensorFlow, but also for numpy, scipy, scikit-learn, matplotlib, and many other mature and high-quality machine learning libraries. If Python is not present on your computer, or if you want to download a newer version, please go to https://www.python.org to install it. Note that due to some unfortunate version issues of the Stable Baselines and TensorFlow, you may have to install diferent versions of Python to get these examples to work, and to use virtual environments to manage your software versions. Since deep learning is very computationally intensive, these packages typically support GPU parallelism, which can speedup your training tenfold or more, if you have the right GPU card in your system. Cloud providers of computing, such as AWS, Google, Azure, and others, also typically provide modern GPU hardware to support machine learning, often with student discounts. B.3.1 MNIST and ImageNet One of the most important elements for the success in image recognition was the availability of good datasets. In the early days of deep learning, the feld benefted greatly from eforts in handwriting recognition. This application was of great value to the postal service, where accurate recognition of handwritten zip codes or postal codes allowed great improvements to efcient sorting and delivery of the mail. A standard test set for handwriting recognition was MNIST (for Modifed National Institute of Standards and Technology) [461]. Standard MNIST images are low-resolution 32 \u00d7 32 pixel images of single handwritten digits (Fig. B.19). Of course, researchers wanted to process more complex scenes than single digits, and higher-resolution images. To achieve higher accuracy, and to process more complex scenes, networks (and datasets) needed to grow in size and complexity. B.3 Datasets and Software 323 ImageNet A major dataset in deep learning is ImageNet [237, 191]. It is a collection of more than 14 million URLs of images that have been hand annotated with the objects that are in the picture. It contains more than 20,000 categories. A typical category contains several hundred training images. The importance of ImageNet for the progress in deep learning is large. The availability of an accepted standard set of labeled images allowed learning algorithms to be tested and improved, and new algorithms to be created. ImageNet was conceived by Fei-Fei Li et al. in 2006, and in later years she developed it further with her group. Since 2010 an annual software contest has been organized, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [191]. Since 2012 ILSVRC has been won by deep networks, starting the deep learning boom. The network architecture that won this challenge in that year has become known as AlexNet, after one of its authors [431]. The 2012 ImageNet database as used by AlexNet has 14 million labeled images. The network featured a highly optimized 2D two-GPU implementation of 5 convolutional layers and 3 fully connected layers. The flters in the convolutional layers are 11\u00d711 in size. The neurons use a ReLU activation function. In AlexNet images were scaled to 256 \u00d7 256 RGB pixels. The size of the network was large, with 60 million parameters. This causes considerable overftting. AlexNet used data augmentation and dropouts to reduce the impact of overftting. Krizhevsky et al. won the 2012 ImageNet competition with an error rate of 15%, signifcantly better than the number two, who achieved 26%. Although there were earlier reports of CNNs that were successful in applications such as bioinformatics and Chinese handwriting recognition, it was this win of the 2012 ImageNet competition for which AlexNet has become well known. B.3.2 GPU Implementations The deep learning breakthrough around 2012 was caused by the co-occurrence of three major developments: (1) algorithmic advances that solved key problems in deep learning, (2) the availability of large datasets of labeled training data, and (3) the availability of computational power in the form of graphical processing units, GPUs. The most expensive operations in image processing and neural network training are operations on matrices. Matrix operations are some of the most well-studied problems in computer science. Their algorithmic structure is well understood, and for basic linear algebra operations high-performance parallel implementations for CPU exist, such as the BLAS [202, 145]. GPUs were originally designed for smooth graphics performance in video games. Graphical processing requires fast linear algebra computations such as matrix multiply. These are precisely the kind of operations that are at the core of deep 324 B Deep Supervised Learning Fig. B.20 SIMD: Connection Machine 1 and GPU learning training algorithms. Modern GPUs consist of thousands of small arithmetic units that are capable of performing linear algebra matrix operations very fast in parallel. This kind of data parallel processing is based on SIMD computing, for singleinstruction-multiple-data [249, 330]. SIMD data parallism goes back to designs from 1960s and 1970s vector supercomputers such as the Connection Machine series from Thinking Machines [339, 340, 466]. Figure B.20 shows a picture of the historic CM-1, and of a modern GPU. Modern GPUs consist of thousands of processing units optimized to process linear algebra matrix operations in parallel [656, 486, 721], ofering matrix performance that is orders of magnitude faster than CPUs [566, 152, 746]. B.3.3 Hands On: Classifcation Example It is high time to try out some of the material in practice. Let us see if we can do some image recognition ourselves. Installing TensorFlow and Keras We will frst install TensorFlow. It is possible to run TensorFlow in the cloud, in Colab, or in a Docker container. Links to ready to run Colab environments are on the TensorFlow website. We will, however, asssume a traditional local installation on your own computer. All major operating systems are supported: Linux/Ubuntu, macOS, Windows. The programming model of TensorFlow is complex, and not very user friendly. Fortunately, an easy to use language has been built on top of TensorFlow: Keras. B.3 Datasets and Software 325 The Keras language is easy to use, well-documented, and many examples exist to get you started. When you install TensorFlow, Keras is installed automatically as well. To install TensorFlow and Keras go to the TensorFlow page on https://www. tensorflow.org. It is recommended to make a virtual environment to isolate the package installation from the rest of your system. This is achieved by typing python3 -m venv --system-site-packages ./venv (or equivalent) to create the virtual environment. Using the virtual environment requires activation:10 source ./venv/bin/activate You will most likely run into version issues when installing packages. Note that for deep reinforcement learning we will be making extensive use of reinforcement learning agent algorithms from the so-called Stable Baselines.11 The stable baselines work with version 1 of TensorFlow and with PyTorch, but, as of this writing, not with version 2 of TensorFlow. TensorFlow version 1.14 has been tested to work.12 Installing is easy with Python\u2019s pip package manager: just type pip install tensorflow==1.14 or, for GPU support, pip install tensorflow-gpu==1.14 This should now download and install TensorFlow and Keras. We will check if everything is working by executing the MNIST training example with the default training dataset. 10 More installation guidance can be found on the TensorFlow page at https://www.tensorflow. org/install/pip. 11 https://github.com/hill-a/stable-baselines 12 You may be surprised that so many version numbers were mentioned. Unfortunately not all versions are compatible; some care is necessary to get the software to work: Python 3.7.9, TensorFlow 1.14.0, Stable Baselines 2, pyglet 1.5.11 worked at the time of writing. This slightly embarrassing situation is because the feld of deep reinforcement learning is driven by a community of researchers, who collaborate to make code bases work. When new insights trigger a rewrite that loses backward compatibility, as happened with TensorFlow 2.0, frantic rewriting of dependent software occurs. The feld is still a new feld, and some instability of software packages will remain with us for the foreseeable future. 326 B Deep Supervised Learning 1 from tensorflow.keras.models import Sequential 2 from tensorflow.keras.layers import Dense 3 4 model = Sequential () 5 6 model.add(Dense(units =64, activation =\u2019relu \u2019)) 7 model.add(Dense(units =10, activation =\u2019softmax \u2019)) 8 9 model.compile(loss=\u2019categorical_crossentropy \u2019, 10 optimizer=\u2019sgd \u2019, 11 metrics =[\u2019accuracy \u2019]) 12 13 # x_train and y_train are Numpy arrays --just like in the Scikit Learn API. 14 model.fit(x_train , y_train , epochs =5, batch_size =32) 15 16 # evaluation on test data is simple 17 loss_and_metrics = model.evaluate(x_test , y_test , batch_size =128) 18 19 # as is predicting output 20 classes = model.predict(x_test , batch_size =128) Listing B.3 Sequential Model in Keras Keras MNIST Example Keras is built on top of TensorFlow, and is installed with TensorFlow. Basic Keras mirrors the familiar Scikit-learn interface [595]. Each Keras program specifes a model that is to be learned. The model is the neural network, that consists of weights and layers (of neurons). You will specify the architecure of the model in Keras, and then ft the model on the training data. When the model is trained, you can evaluate the loss function on test data, or perform predictions of the outcome based on some test input example. Keras has two main programming paradigms: sequential and functional. Listing B.3 shows the most basic Sequential Keras model, from the Keras documentation, with a two-layer model, a ReLU layer, and a softmax layer, using simple SGD for backpropagation. The Sequential model in Keras has an object-oriented syntax. The Keras documentation is at https://keras.io/getting_started/intro_ to_keras_for_researchers/. It is quite accessible, and you are encouraged to learn Keras by working through the online tutorials. A slightly more useful example is ftting a model on MNIST, see Listing B.4. This example uses a more fexible Keras syntax, the functional API, in which transformations are chained on top of the previous layers. This example of Keras code loads MNIST images in a training set and a test set, and creates a model of dense ReLU layers using the Functional API. It then creates a Keras model of these layers, and prints a summary of the model. Then the model is trained from numpy data, with B.3 Datasets and Software 327 1 # Get the data as Numpy arrays 2 (x_train , y_train), (x_test , y_test) = keras.datasets.mnist. load_data () 3 4 # Build a simple model 5 inputs = keras.Input(shape =(28 , 28)) 6 x = layers. experimental . preprocessing .Rescaling (1.0 / 255)(inputs ) 7 x = layers.Flatten ()(x) 8 x = layers.Dense (128 , activation=\"relu\")(x) 9 x = layers.Dense (128 , activation=\"relu\")(x) 10 outputs = layers.Dense (10, activation =\"softmax\")(x) 11 model = keras.Model(inputs , outputs) 12 model.summary () 13 14 # Compile the model 15 model.compile(optimizer=\"adam\", loss=\" sparse_categorical_crossentropy \") 16 17 # Train the model for 1 epoch from Numpy data 18 batch_size = 64 19 print(\"Fit\u2423on\u2423NumPy\u2423data\") 20 history = model.fit(x_train , y_train , batch_size =batch_size , epochs =1) 21 22 # Train the model for 1 epoch using a dataset 23 dataset = tf.data.Dataset. from_tensor_slices (( x_train , y_train)). batch(batch_size) 24 print(\"Fit\u2423on\u2423Dataset\") 25 history = model.fit(dataset , epochs =1) Listing B.4 Functional MNIST Model in Keras the ft function, for a single epoch, and also from a dataset, and the loss history is printed. The code of the example shows how close Keras is to the way in which we think and reason about neural networks. The examples only show the briefest of glimpses to what is possible in Keras. Keras has options for performance monitoring, for checkpointing of long training runs, and for interfacing with TensorBoard, to visualize the training process. TensorBoard is an indispensible tool, allowing you to debug your intuition of what should be going on in your network, and what is going on. Deep reinforcement learning is still very much a feld with more degrees of freedom in experimentation than established practices, and being able to plot the progress of training processes is essential for a better understanding of how the model behaves. The more you explore Keras, the better you will be able to progress in deep reinforcement learning [270]. 328 B Deep Supervised Learning Exercises Below are some questions to check your understanding of deep learning. Each question is a closed question where a simple, single sentence answer is expected. Questions 1. Datasets are often split into two sub-datasets in machine learning. Which are those two, and why are they split? 2. What is generalization? 3. Sometimes a third sub-dataset is used. What is it called and what is it used for? 4. If we consider observations and model parameters, when is a problem highdimensional and when is it low-dimensional? 5. How do we measure the capacity of a machine learning model? 6. What is a danger with low capacity models? 7. What is a danger with high capacity models? 8. What is the efect of overftting on generalization? 9. What is the diference between supervised learning and reinforcement learning? 10. What is the diference between a shallow network and a deep network? 11. Supervised learning has a model and a dataset, reinforcement learning has which two central concepts? 12. What phases does a learning epoch have? What happens in each phase? 13. Name three factors that were essential for the deep learning breakthrough, and why. 14. What is end-to-end learning? Do you know an alternative? What are the advantages of each? 15. What is underftting, and what causes it? What is overftting, and what causes it? How can you see if you have overftting? 16. Name three ways to prevent overftting. 17. Which three types of layers does a neural network have? 18. How many hidden layers does a shallow neural network have? 19. Describe how adjusting weights works in a neural network. Hint: think of a examples, labels, forward phase, a backward phase, error functions, gradients. 20. What is the diference between a fully connected network and a convolutional neural network? 21. What is max pooling? 22. Why are shared weights advantageous? 23. What is feature learning? 24. What is representation learning? 25. What is deep learning? 26. What is an advantage of convolutional neural networks of fully connected neural networks? 27. Name two well-known image recognition data sets. B.3 Datasets and Software 329 Exercises Let us now start with some exercises. If you have not done so already, install PyTorch13 or TensorFlow and Keras (see Sect. 2.2.4.1 or go to the TensorFlow page).14 Be sure to check the right versions of Python, TensorFlow, and the Stable Baselines to make sure they work well together. The exercises below are meant to be done in Keras. 1. Generalization Install Keras. Go to the Keras MNIST example. Perform a classifcation task. Note how many epochs the training takes, and in testing, how well it generalizes. Perform the classifcation on a smaller training set, how does learning rate change, how does generalization change. Vary other elements: try a diferent optimizer than adam, try a diferent learning rate, try a diferent (deeper) architecture, try wider hidden layers. Does it learn faster? Does it generalize better? 2. Overftting Use Keras again, but this time on ImageNet. Now try diferent overftting solutions. Does the training speed change? Does generalization change? Now try the hold-out validation set. Do training and generalization change? 3. Confdence How many runs did you do in the previous exercises, just a single run to see how long training took and how well generalization worked? Try to run it again. Do you get the same results? How large is the diference? Can you change the random seeds of Keras or TensorFlow? Can you calculate the confdence interval, how much does the confdence improve when you do 10 randomized runs? How about 100 runs? Make graphs with error bars. 4. GPU It might be that you have access to a GPU machine that is capable of running PyTorch or TensorFlow in parallel to speed up the training. Install the GPU version and check that it recognizes the GPU and is indeed using it. 5. Parallelism It might be that you have access to a multicore CPU machine. When you are running multiple runs in order to improve confedence, then an easy way to speed up your experiment is to spawn multiple jobs at the shell, assigning the output to diferent log fles, and write a script to combine results and draw graphs. Write the scripts necessary to achieve this, test them, and do a large-confdence experiment. 13 https://pytorch.org 14 https://www.tensorflow.org Appendix C Deep Reinforcement Learning Suites Deep reinforcement learning is a highly active feld of research. One reason for the progress is the availability of high quality algorithms and code: high quality environments, algorithms and deep learning suites are all being made available by researchers along with their research papers. This appendix provides pointers to these codes. 331 332 C Deep Reinforcement Learning Suites C.1 Environments Progress has benefted greatly from the availability of high quality environments, on which the algorithms can be tested. We provide pointers to some of the environments (Table C.1). Environment Type URL Ref Gym ALE, MuJoCo https://gym.openai.com [108] ALE Atari https://github.com/mgbellemare/ArcadeLearning-Environment [71] MuJoCo Simulated Robot http://www.mujoco.org [780] DeepMind Lab 3D navigation https://github.com/deepmind/lab [64] Control Suite Physics tasks https://github.com/deepmind/dm_control [755] Behavior Suite Core RL https://github.com/deepmind/bsuite [575] StarCraft Python interface https://github.com/deepmind/pysc2 [815] Omniglot Images https://github.com/brendenlake/omniglot [445] Mini ImageNet Images https://github.com/yaoyao-liu/mini-imagenettools [814] ProcGen Procedurally Gen. https://openai.com/blog/procgen-benchmark/ [158] OpenSpiel Board Games https://github.com/deepmind/open_spiel [450] RLlib Scalable RL https://github.com/ray-project/ray [478] Meta Dataset dataset of datas. https://github.com/google-research/metadataset [785] Meta World meta RL https://meta-world.github.io [864] Alchemy meta RL https://github.com/deepmind/dm_alchemy [825] Garage reproducible RL https://github.com/rlworkgroup/garage [261] Football multi agent https://github.com/google-research/football [438] Emergence Hide and Seek https://github.com/openai/multi-agentemergence-environments [49] Unplugged Ofine RL https://github.com/deepmind/deepmindresearch/tree/master/rl_unplugged [299] Unity 3D https://github.com/Unity-Technologies/mlagents [383] PolyGames board games https://github.com/teytaud/Polygames [133] Dopamine RL framework https://github.com/google/dopamine [131] Table C.1 Reinforcement Learning Environments C.2 Agent Algorithms 333 C.2 Agent Algorithms For value-based and policy-based methods most mainstream algorithms have been collected and are freely available. For two-agent, multi-agent, hierarchical, and meta learning, the agent algorithms are also on GitHub, but not always in the same place as the basic algorithms. Table C.2 provides pointers. Repo URL Algorithms Ref Spinning Up https://spinningup.openai.com Tutorial on DDPG, PPO, etc OpenAI Baselines https://github.com/openai/ baselines DQN, PPO, etc [194] Stable Basel. https://stable-baselines. readthedocs.io/en/master/ Refactored baselines [338] PlaNet https://planetrl.github.io Latent Model [309] Dreamer https://github.com/danijar/ dreamerv2 Latent Model [310] VPN https://github.com/junhyukoh/ value-prediction-network Value Prediction Network [564] MuZero https://github.com/kaesve/muzero Reimplementation of MuZero [186] MCTS pip install mcts MCTS [117] AlphaZ. Gen https://github.com/suragnair/ alpha-zero-general AZ in Python [768] ELF https://github.com/ facebookresearch/ELF Framework for Game research [776] PolyGames https://github.com/teytaud/ Polygames Zero Learning [133] CFR https://github.com/bakanaouji/ cpp-cfr CFR [881] DeepCFR https://github.com/ EricSteinberger/Deep-CFR Deep CFR [726] MADDPG https://github.com/openai/maddpg Multi-agent DDPG [489] PBT https://github.com/voiler/ PopulationBasedTraining Population Based Training [374] Go-Explore https://github.com/uberresearch/go-explore Go Explore [222] MAML https://github.com/cbfinn/maml MAML [243] Table C.2 Agent Algorithms 334 C Deep Reinforcement Learning Suites C.3 Deep Learning Suites The two most well-known deep learning suites are TensorFlow and PyTorch. BaseTensorFlow has a complicated programming model. Keras has been developed as an easy to use layer on top of TensorFlow. When you use TensorFlow, start with Keras. Or use PyTorch. TensorFlow and Keras are at https://www.tensorflow.org. PyTorch is at https://pytorch.org. References 1. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jefrey Dean, Matthieu Devin, Sanjay Ghemawat, Geofrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorfow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265\u2013283, 2016. 304, 321 2. Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608\u20131639, 2010. 6 3. Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of reinforcement learning to aerobatic helicopter fight. In Advances in Neural Information Processing Systems, pages 1\u20138, 2007. 5, 6 4. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. 170 5. Abhishek. Multi-arm bandits: a potential alternative to a/b tests https://medium.com/ brillio-data-science/multi-arm-bandits-a-potential-alternative-to-a-btests-a647d9bf2a7e, 2019. 48 6. Bruce Abramson. Expected-outcome: A general model of static evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(2):182\u2013193, 1990. 163 7. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. arXiv preprint arXiv:2108.13264, 2021. 115 8. Pulkit Agrawal, Ross Girshick, and Jitendra Malik. Analyzing the performance of multilayer neural networks for object recognition. In European Conference on Computer Vision, pages 329\u2013344. Springer, 2014. 245 9. Sanjeevan Ahilan and Peter Dayan. Feudal multi-agent hierarchies for cooperative reinforcement learning. arXiv preprint arXiv:1901.08492, 2019. 233 10. Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-based classifcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 819\u2013826, 2013. 261, 268, 269, 276 11. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2623\u20132631, 2019. 259 12. Jay Alammer. The illustrated transformer. https://jalammar.github.io/illustratedtransformer/. 320 335 336 References 13. Stefano Albrecht and Peter Stone. Multiagent learning: foundations and recent trends. In Tutorial at IJCAI-17 conference, 2017. 222 14. Stefano Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artifcial Intelligence, 258:66\u201395, 2018. 208, 222 15. Ethem Alpaydin. Introduction to Machine Learning. MIT press, 2009. 40, 45 16. Safa Alver. The option-critic architecture. https://alversafa.github.io/blog/2018/ 11/28/optncrtc.html, 2018. 229 17. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016. 268 18. Ankesh Anand, Jacob Walker, Yazhe Li, Eszter V\u00e9rtes, Julian Schrittwieser, Sherjil Ozair, Th\u00e9ophane Weber, and Jessica B Hamrick. Procedural generalization by planning with self-supervised world models. arXiv preprint arXiv:2111.01587, 2021. 136, 141 19. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048\u20135058, 2017. 231, 233, 241 20. Anonymous. Go AI strength vs. time. Reddit post, 2017. 154 21. Thomas Anthony, Tom Eccles, Andrea Tacchetti, J\u00e1nos Kram\u00e1r, Ian M. Gemp, Thomas C. Hudson, Nicolas Porcel, Marc Lanctot, Julien P\u00e9rolat, Richard Everett, Satinder Singh, Thore Graepel, and Yoram Bachrach. Learning to play no-press diplomacy with best response policy iteration. In Advances in Neural Information Processing Systems, 2020. 208, 223 22. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Advances in Neural Information Processing Systems, pages 5360\u20135370, 2017. 144, 179 23. Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. arXiv preprint arXiv:1810.09502, 2018. 268 24. Grigoris Antoniou and Frank Van Harmelen. A Semantic Web Primer. MIT press Cambridge, MA, 2008. 9 25. Oleg Arenz. Monte Carlo Chess. Master\u2019s thesis, Universit\u00e4t Darmstadt, 2012. 188 26. Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems, pages 41\u201348, 2007. 251 27. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26\u201338, 2017. 25, 28, 59, 86 28. John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A Bayesian sampling approach to exploration in reinforcement learning. In Proceedings of the TwentyFifth Conference on Uncertainty in Artifcial Intelligence, pages 19\u201326. AUAI Press, 2009. 278 29. Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in reinforcement learning. arXiv preprint arXiv:1908.06976, 2019. 237, 238, 241, 277 30. Peter Auer. Using confdence bounds for exploitation-exploration trade-ofs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002. 47, 168 31. Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235\u2013256, 2002. 168, 170 32. Peter Auer and Ronald Ortner. UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2):55\u201365, 2010. 168 33. Robert Axelrod. An evolutionary approach to norms. The American Political Science Review, pages 1095\u20131111, 1986. 223 34. Robert Axelrod. The complexity of cooperation: Agent-based models of competition and collaboration, volume 3. Princeton university press, 1997. 209, 216, 223 35. Robert Axelrod. The dissemination of culture: A model with local convergence and global polarization. Journal of Confict Resolution, 41(2):203\u2013226, 1997. 223 36. Robert Axelrod and Douglas Dion. The further evolution of cooperation. Science, 242(4884):1385\u20131390, 1988. 200 37. Robert Axelrod and William D Hamilton. The evolution of cooperation. Science, 211(4489):1390\u20131396, 1981. 200, 223 References 337 38. Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary C Lipton, and Animashree Anandkumar. Surprising negative results for generative adversarial tree search. arXiv preprint arXiv:1806.05780, 2018. 144 39. Mohammad Babaeizadeh, Mohammad Taghi Safar, Danijar Hafner, Harini Kannan, Chelsea Finn, Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design trade-ofs in visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603, 2020. 141 40. Thomas B\u00e4ck. Evolutionary Algorithms in Theory and Practice: Evolutionary Strategies, Evolutionary Programming, Genetic Algorithms. Oxford University Press, 1996. 209, 210, 223 41. Thomas B\u00e4ck, David B Fogel, and Zbigniew Michalewicz. Handbook of evolutionary computation. Release, 97(1):B1, 1997. 223 42. Thomas B\u00e4ck, Frank Hofmeister, and Hans-Paul Schwefel. A survey of evolution strategies. In Proceedings of the fourth International Conference on Genetic Algorithms, 1991. 202, 223 43. Thomas B\u00e4ck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter optimization. Evolutionary Computation, 1(1):1\u201323, 1993. 11, 223 44. Christer Backstrom and Peter Jonsson. Planning with abstraction hierarchies can be exponentially less efcient. In Proceedings of the 14th International Joint Conference on Artifcial Intelligence, volume 2, pages 1599\u20131604, 1995. 228, 232 45. Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 31, 2017. 231, 233, 234, 241 46. Adri\u00e0 Puigdom\u00e8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari human benchmark. arXiv preprint arXiv:2003.13350, 2020. 85 47. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 319 48. Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pages 30\u201337. Elsevier, 1995. 66, 73 49. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019. 214, 216, 217, 332 50. Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017. 212 51. Chitta Baral. Knowledge Representation, Reasoning and Declarative Problem Solving. Cambridge university press, 2003. 232 52. Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare, and Michael Bowling. The Hanabi challenge: A new frontier for AI research. Artifcial Intelligence, 280:103216, 2020. 221 53. Nolan Bard, John Hawkin, Jonathan Rubin, and Martin Zinkevich. The annual computer poker competition. AI Magazine, 34(2):112, 2013. 214, 223 54. Simon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a \u201ctheory of mind\u201d? Cognition, 21(1):37\u201346, 1985. 193, 211 55. Andrew Barron, Jorma Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743\u20132760, 1998. 15 56. Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(1-2):41\u201377, 2003. 232, 241 57. Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, (5):834\u2013846, 1983. 56, 99, 116 58. OpenAI Baselines. DQN https://openai.com/blog/openai-baselines-dqn/, 2017. 80, 84 59. Jonathan Baxter. A model of inductive bias learning. Journal of Artifcial Intelligence Research, 12:149\u2013198, 2000. 247 60. Jonathan Baxter, Andrew Tridgell, and Lex Weaver. Knightcap: a chess program that learns by combining TD (\ud835\udf06) with game-tree search. arXiv preprint cs/9901002, 1999. 188 338 References 61. Jonathan Baxter, Andrew Tridgell, and Lex Weaver. Learning to play chess using temporal diferences. Machine Learning, 40(3):243\u2013263, 2000. 162, 188 62. Don Beal and Martin C. Smith. Temporal diference learning for heuristic search and game playing. Information Sciences, 122(1):3\u201321, 2000. 188 63. Mark F Bear, Barry W Connors, and Michael A Paradiso. Neuroscience, volume 2. Lippincott Williams & Wilkins, 2007. 304 64. Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fcttler, Andrew Lefrancq, Simon Green, V\u00edctor Vald\u00e9s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gafney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. 332 65. Laurens Beljaards. Ai agents for the abstract strategy game tak. Master\u2019s thesis, Leiden University, 2017. 163 66. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-of. arXiv preprint arXiv:1812.11118, 2018. 321 67. Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv preprint arXiv:1903.07571, 2019. 321 68. Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overftting or perfect ftting? Risk bounds for classifcation and regression rules that interpolate. In Advances in Neural Information Processing Systems, pages 2300\u20132311, 2018. 321 69. Marc Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively factored environments. In International Conference on Machine Learning, pages 1211\u20131219, 2013. 278 70. Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449\u2013458, 2017. 80, 83 71. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artifcial Intelligence Research, 47:253\u2013279, 2013. 7, 41, 65, 67, 85, 86, 237, 262, 332 72. Richard Bellman. Dynamic Programming. Courier Corporation, 1957, 2013. 39, 45, 59, 295, 299 73. Richard Bellman. On the application of dynamic programing to the determination of optimal play in chess and checkers. Proceedings of the National Academy of Sciences, 53(2):244\u2013247, 1965. 59 74. Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in Neural Information Processing Systems, 19:137, 2007. 252 75. Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Technical report, Montreal, 1990. 268 76. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u2013 1828, 2013. 308 77. Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 41\u201348, 2009. 176, 188 78. Gerardo Beni. Swarm intelligence. Complex Social and Behavioral Systems: Game Theory and Agent-Based Models, pages 791\u2013818, 2020. 223 79. Gerardo Beni and Jing Wang. Swarm intelligence in cellular robotic systems. In Robots and Biological Systems: Towards a New Bionics?, pages 703\u2013712. Springer, 1993. 211 80. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond\u00e9 de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. 69 References 339 81. Tim Berners-Lee, James Hendler, and Ora Lassila. The semantic web. Scientifc American, 284(5):28\u201337, 2001. 9 82. Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of markov decision processes. Mathematics of Operations Research, 27(4):819\u2013840, 2002. 197, 206 83. Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with diferentiable closed-form solvers. In International Conference on Learning Representations, 2018. 268 84. R Bertolami, H Bunke, S Fernandez, A Graves, M Liwicki, and J Schmidhuber. A novel connectionist system for improved unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5), 2009. 313 85. Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena scientifc Belmont, MA, 1995. 10 86. Dimitri P Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. MIT Press Cambridge, 1996. 7, 59 87. Shrisha Bharadwaj. Embarrsingly simple zero shot learning. https://github.com/ chichilicious/embarrsingly-simple-zero-shot-learning, 2018. 268, 269 88. Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba Szepesv\u00e1ri. Convergent temporal-diference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, pages 1204\u20131212, 2009. 74, 86 89. Darse Billings, Aaron Davidson, Jonathan Schaefer, and Duane Szafron. The challenge of poker. Artifcial Intelligence, 134(1-2):201\u2013240, 2002. 214, 223 90. Darse Billings, Aaron Davidson, Terence Schauenberg, Neil Burch, Michael Bowling, Robert Holte, Jonathan Schaefer, and Duane Szafron. Game-tree search with adaptation in stochastic imperfect-information games. In International Conference on Computers and Games, pages 21\u201334. Springer, 2004. 223 91. Darse Billings, Denis Papp, Jonathan Schaefer, and Duane Szafron. Opponent modeling in poker. AAAI/IAAI, 493:499, 1998. 161 92. Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. O\u2019Reilly Media, Inc., 2009. 276 93. Christopher M Bishop. Pattern Recognition and Machine Learning. Information science and statistics. Springer Verlag, Heidelberg, 2006. 7, 14, 59, 105, 128, 129, 259, 298, 300, 302, 321 94. Peter Bloem. Transformers http://peterbloem.nl/blog/transformers. 320 95. Christian Blum and Daniel Merkle. Swarm Intelligence: Introduction and Applications. Springer Science & Business Media, 2008. 203, 211 96. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geof Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, and Rohith Kuditipudi. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 244, 263 97. Eric Bonabeau, Marco Dorigo, and Guy Theraulaz. Swarm Intelligence: From Natural to Artifcial Systems. Oxford University Press, 1999. 11, 203 98. Borealis. Few shot learning tutorial https://www.borealisai.com/en/blog/tutorial2-few-shot-learning-and-meta-learning-i/. 254 99. Zdravko I Botev, Dirk P Kroese, Reuven Y Rubinstein, and Pierre L\u2019Ecuyer. The cross-entropy method for optimization. In Handbook of Statistics, volume 31, pages 35\u201359. Elsevier, 2013. 133 340 References 100. Matthew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and Demis Hassabis. Reinforcement learning, fast and slow. Trends in Cognitive Sciences, 23(5):408\u2013422, 2019. 267 101. Matthew M Botvinick, Yael Niv, and Andew G Barto. Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective. Cognition, 113(3):262\u2013280, 2009. 241 102. Bruno Bouzy and Bernard Helmstetter. Monte Carlo Go developments. In Advances in Computer Games, pages 159\u2013174. Springer, 2004. 163, 167 103. Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up Limit Hold\u2019em poker is solved. Science, 347(6218):145\u2013149, 2015. 196, 214, 223 104. Michael H. Bowling, Nicholas Abou Risk, Nolan Bard, Darse Billings, Neil Burch, Joshua Davidson, John Alexander Hawkin, Robert Holte, Michael Johanson, Morgan Kan, Bryce Paradis, Jonathan Schaefer, David Schnizlein, Duane Szafron, Kevin Waugh, and Martin Zinkevich. A demonstration of the polaris poker system. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems, volume 2, pages 1391\u20131392, 2009. 223 105. Robert Boyd and Peter J Richerson. Culture and the Evolutionary Process. University of Chicago press, 1988. 209, 216, 223 106. Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning: Applications to data mining. Springer Science & Business Media, 2008. 247, 259, 267, 268 107. Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. 278 108. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. 41, 85, 86, 94, 116, 332 109. Rodney A Brooks. Intelligence without representation. Artifcial Intelligence, 47(1-3):139\u2013159, 1991. 11 110. Noam Brown, Sam Ganzfried, and Tuomas Sandholm. Hierarchical abstraction, distributed equilibrium computation, and post-processing, with application to a champion No-Limit Texas Hold\u2019em agent. In AAAI Workshop: Computer Poker and Imperfect Information, 2015. 204 111. Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization. In International Conference on Machine Learning, pages 793\u2013802. PMLR, 2019. 206, 215, 223 112. Noam Brown and Tuomas Sandholm. Superhuman AI for Heads-up No-limit poker: Libratus beats top professionals. Science, 359(6374):418\u2013424, 2018. 204, 215, 223 113. Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):885\u2013890, 2019. 214, 215, 223 114. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jefrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. 246, 308, 320 115. Cameron Browne. Hex Strategy. AK Peters/CRC Press, 2000. 184 116. Cameron Browne, Dennis JNJ Soemers, and Eric Piette. Strategic features for general games. In KEG@ AAAI, pages 70\u201375, 2019. 278 117. Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of Monte Carlo Tree Search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1\u201343, 2012. 164, 165, 166, 167, 168, 169, 170, 187, 333 118. Bernd Br\u00fcgmann. Monte Carlo Go. Technical report, Syracuse University, 1993. 163, 167 References 341 119. Bruno Buchberger, George E Collins, R\u00fcdiger Loos, and Rudolph Albrecht. Computer algebra symbolic and algebraic computation. ACM SIGSAM Bulletin, 16(4):5\u20135, 1982. 9 120. Cristian Bucilu\u02c7a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 535\u2013541, 2006. 278 121. Lars Buesing, Theophane Weber, S\u00e9bastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, and Daan Wierstra. Learning and querying fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018. 131, 139 122. Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156\u2013172, 2008. 222 123. Zhiyuan Cai, Huanhui Cao, Wenjie Lu, Lin Zhang, and Hao Xiong. Safe multi-agent reinforcement learning through decentralized multiple control barrier functions. arXiv preprint arXiv:2103.12553, 2021. 207 124. Murray Campbell, A Joseph Hoane Jr, and Feng-Hsiung Hsu. Deep Blue. Artifcial Intelligence, 134(1-2):57\u201383, 2002. 57, 64 125. Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with AMIGo: Adversarially motivated intrinsic goals. In International Conference on Learning Representations, 2020. 176, 231, 234 126. Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark. Emergent communication through negotiation. In International Conference on Learning Representations, 2018. 208 127. Thomas Carr, Maria Chli, and George Vogiatzis. Domain adaptation for reinforcement learning on the atari. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201919, Montreal, pages 1859\u20131861, 2018. 252 128. Edward Cartwright. Behavioral Economics. Routledge, 2018. 223 129. Rich Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997. 247, 248, 250, 251 130. Rich Caruana, Steve Lawrence, and C Lee Giles. Overftting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in Neural Information Processing Systems, pages 402\u2013408, 2001. 321 131. Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018. 332 132. Tristan Cazenave. Residual networks for computer Go. IEEE Transactions on Games, 10(1):107\u2013 110, 2018. 172 133. Tristan Cazenave, Yen-Chi Chen, Guan-Wei Chen, Shi-Yu Chen, Xian-Dong Chiu, Julien Dehos, Maria Elsa, Qucheng Gong, Hengyuan Hu, Vasil Khalidov, Cheng-Ling Li, Hsin-I Lin, Yu-Jin Lin, Xavier Martinet, Vegard Mella, J\u00e9r\u00e9my Rapin, Baptiste Rozi\u00e8re, Gabriel Synnaeve, Fabien Teytaud, Olivier Teytaud, Shi-Cheng Ye, Yi-Jun Ye, Shi-Jim Yen, and Sergey Zagoruyko. Polygames: Improved zero learning. arXiv preprint arXiv:2001.09832, 2020. 183, 184, 187, 332, 333 134. Tristan Cazenave and Bernard Helmstetter. Combining tactical search and Monte-Carlo in the game of Go. In Proceedings of the 2005 IEEE Symposium on Computational Intelligence and Games (CIG05), Essex University, volume 5, pages 171\u2013175, 2005. 170 135. Hyeong Soo Chang, Michael C Fu, Jiaqiao Hu, and Steven I Marcus. An adaptive sampling algorithm for solving Markov decision processes. Operations Research, 53(1):126\u2013139, 2005. 170 136. Yang Chao. Share and play new sokoban levels. http://Sokoban.org, 2013. 26 137. Guillaume Chaslot. Monte-Carlo tree search. PhD thesis, Maastricht University, 2010. 167 138. Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-Carlo tree search: A new framework for game AI. In AIIDE, 2008. 170, 179 139. Kumar Chellapilla and David B Fogel. Evolving neural networks to play checkers without relying on expert knowledge. IEEE Transactions on Neural Networks, 10(6):1382\u20131391, 1999. 75 342 References 140. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv preprint arXiv:2106.01345, 2021. 320 141. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classifcation. In International Conference on Learning Representations, 2019. 253, 264, 268 142. Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. 278 143. Maxime Chevalier-Boisvert, Lucas Willems, and Sumans Pal. Minimalistic gridworld environment for OpenAI Gym https://github.com/maximecb/gym-minigrid, 2018. 177, 234 144. Silvia Chiappa, S\u00e9bastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In International Conference on Learning Representations, 2017. 139 145. Jaeyoung Choi, Jack J Dongarra, and David W Walker. PB-BLAS: a set of parallel block basic linear algebra subprograms. Concurrency: Practice and Experience, 8(7):517\u2013535, 1996. 323 146. Fran\u00e7ois Chollet. Keras. https://keras.io, 2015. 87 147. Fran\u00e7ois Chollet. Deep learning with Python. Manning Publications Co., 2017. 321 148. Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. Back to basics: Benchmarking canonical evolution strategies for playing Atari. In Proceedings of the Twenty-Seventh International Joint Conference on Artifcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, pages 1419\u20131426, 2018. 223 149. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754\u20134765, 2018. 129, 133, 138, 144 150. Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In International Conference on Learning Representations, 2016. 276 151. Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, and Lorenzo Rosasco. Convex learning of multiple tasks and their structure. In International Conference on Machine Learning, pages 1548\u20131557. PMLR, 2015. 247 152. Dan Cire\u015fan, Ueli Meier, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber. Deep, big, simple neural nets for handwritten digit recognition. Neural Computation, 22(12):3207\u20133220, 2010. 324 153. Dan Cire\u015fan, Ueli Meier, and J\u00fcrgen Schmidhuber. Multi-column deep neural networks for image classifcation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, US, pages 3642\u20133649, 2012. 313 154. Christopher Clark and Amos Storkey. Teaching deep convolutional neural networks to play Go. arxiv preprint. arXiv preprint arXiv:1412.3409, 1, 2014. 75, 187 155. Christopher Clark and Amos Storkey. Training deep convolutional neural networks to play Go. In International Conference on Machine Learning, pages 1766\u20131774, 2015. 179, 187 156. Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. In 2nd Annual Conference on Robot Learning, CoRL 2018, Z\u00fcrich, Switzerland, pages 617\u2013629, 2018. 129, 138, 144 157. William F Clocksin and Christopher S Mellish. Programming in Prolog: Using the ISO standard. Springer Science & Business Media, 1981. 9 158. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International Conference on Machine Learning, pages 2048\u20132056. PMLR, 2020. 177, 279, 332 159. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In International Conference on Machine Learning, pages 1282\u20131289, 2018. 278 160. Helder Coelho and Luis Moniz Pereira. Automated reasoning in geometry theorem proving with prolog. Journal of Automated Reasoning, 2(4):329\u2013390, 1986. 278 References 343 161. C\u00e9dric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer. Curious: intrinsically motivated modular multi-goal reinforcement learning. In International Conference on Machine Learning, pages 1331\u20131340. PMLR, 2019. 277 162. C\u00e9dric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Intrinsically motivated goal-conditioned reinforcement learning: a short survey. arXiv preprint arXiv:2012.09830, 2020. 277 163. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O Stanley, and Jef Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Advances in Neural Information Processing Systems, pages 5032\u20135043, 2018. 223 164. R\u00e9mi Coulom. Efcient selectivity and backup operators in Monte-Carlo Tree Search. In International Conference on Computers and Games, pages 72\u201383. Springer, 2006. 164, 170, 187 165. R\u00e9mi Coulom. Monte-Carlo tree search in Crazy Stone. In Proceedings Game Programming Workshop, Tokyo, Japan, pages 74\u201375, 2007. 167, 170 166. R\u00e9mi Coulom. The Monte-Carlo revolution in Go. In The Japanese-French Frontiers of Science Symposium (JFFoS 2008), Roscof, France, 2009. 170, 179 167. Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016\u20132019. 92, 93, 116 168. Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. In Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition, pages 1\u201335. Springer, 2017. 252, 268 169. Joseph Culberson. Sokoban is PSPACE-complete. Technical report, University of Alberta, 1997. 27, 56 170. Joseph C Culberson and Jonathan Schaefer. Pattern databases. Computational Intelligence, 14(3):318\u2013334, 1998. 167 171. Ken Currie and Austin Tate. O-plan: the open planning architecture. Artifcial Intelligence, 52(1):49\u201386, 1991. 231 172. Wojciech Marian Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omidshafei, David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. In Advances in Neural Information Processing Systems, 2020. 150 173. Kamil Czarnog\u00f3rski. Monte Carlo Tree Search beginners guide https://int8.io/montecarlo-tree-search-beginners-guide/, 2018. 166, 167 174. Will Dabney, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis Hassabis, R\u00e9mi Munos, and Matthew Botvinick. A distributional code for value in dopamine-based reinforcement learning. Nature, pages 1\u20135, 2020. 83 175. Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R McKee, Joel Z Leibo, Kate Larson, and Thore Graepel. Open problems in cooperative AI. arXiv preprint arXiv:2012.08630, 2020. 206 176. Zhongxiang Dai, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet, and Teck-Hua Ho. R2-B2: recursive reasoning-based Bayesian optimization for no-regret learning in games. In International Conference on Machine Learning, pages 2291\u20132301. PMLR, 2020. 208 177. Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2):337\u2013357, 2016. 241 178. Shubhomoy Das, Weng-Keen Wong, Thomas Dietterich, Alan Fern, and Andrew Emmott. Incorporating expert feedback into active anomaly discovery. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pages 853\u2013858. IEEE, 2016. 177 179. Hal Daum\u00e9 III. Frustratingly easy domain adaptation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, 2007. 252 180. Morton D Davis. Game Theory: a Nontechnical Introduction. Courier Corporation, 2012. 222 181. Richard Dawkins and Nicola Davis. The Selfsh Gene. Macat Library, 2017. 209 182. Peter Dayan and Geofrey E Hinton. Feudal reinforcement learning. In Advances in Neural Information Processing Systems, pages 271\u2013278, 1993. 231, 232 344 References 183. Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of Operations Research, 134(1):19\u201367, 2005. 133 184. Luis M De Campos, Juan M Fernandez-Luna, Jos\u00e9 A G\u00e1mez, and Jos\u00e9 M Puerta. Ant colony optimization for learning Bayesian networks. International Journal of Approximate Reasoning, 31(3):291\u2013311, 2002. 278 185. Dave De Jonge, Tim Baarslag, Reyhan Aydo\u011fan, Catholijn Jonker, Katsuhide Fujita, and Takayuki Ito. The challenge of negotiation in the game of diplomacy. In International Conference on Agreement Technologies, pages 100\u2013114. Springer, 2018. 151, 208 186. Joery A. de Vries, Ken S. Voskuil, Thomas M. Moerland, and Aske Plaat. Visualizing MuZero models. arXiv preprint arXiv:2102.12924, 2021. 141, 142, 146, 275, 333 187. Rina Dechter. Learning while searching in constraint-satisfaction problems. AAAI, 1986. 308 188. Marc Deisenroth and Carl E Rasmussen. PILCO: a model-based and data-efcient approach to policy search. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 465\u2013472, 2011. 128, 138, 144 189. Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen. Gaussian processes for data-efcient learning in robotics and control. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):408\u2013423, 2013. 128, 144 190. Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. In Foundations and Trends in Robotics 2, pages 1\u2013142. Now publishers, 2013. 143, 144 191. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. Ieee, 2009. 65, 276, 323 192. Mohit Deshpande. Deep RL policy methods https://mohitd.github.io/2019/01/20/ deep-rl-policy-methods.html. 116 193. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, 2018. 245, 246, 262, 263, 276, 307, 308, 320 194. Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI baselines. https://github.com/openai/baselines, 2017. 333 195. Thomas G Dietterich. The MAXQ method for hierarchical reinforcement learning. In International Conference on Machine Learning, volume 98, pages 118\u2013126, 1998. 241 196. Thomas G Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artifcial Intelligence Research, 13:227\u2013303, 2000. 27, 56, 231, 232, 241 197. Chuong B Do and Andrew Y Ng. Transfer learning for text classifcation. Advances in Neural Information Processing Systems, 18:299\u2013306, 2005. 268 198. Thang Doan, Joao Monteiro, Isabela Albuquerque, Bogdan Mazoure, Audrey Durand, Joelle Pineau, and R Devon Hjelm. On-line adaptative curriculum learning for GANs. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 33, pages 3470\u20133477, 2019. 178, 188 199. Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint, and Sebastian Trimpe. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018. 131 200. Jef Donahue, Yangqing Jia, Oriol Vinyals, Judy Hofman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, pages 647\u2013655. PMLR, 2014. 245, 268 201. Hao Dong, Zihan Ding, and Shanghang Zhang. Deep Reinforcement Learning. Springer, 2020. 86 202. Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J Hanson. An extended set of FORTRAN basic linear algebra subprograms. ACM Transactions on Mathematical Software, 14(1):1\u201317, 1988. 323 References 345 203. Christian Donninger. Null move and deep search. ICGA Journal, 16(3):137\u2013143, 1993. 163 204. Dorit Dor and Uri Zwick. Sokoban and other motion planning problems. Computational Geometry, 13(4):215\u2013228, 1999. 27, 56 205. Derek Doran, Sarah Schulz, and Tarek R Besold. What does explainable AI really mean? A new conceptualization of perspectives. arXiv preprint arXiv:1710.00794, 2017. 278 206. Marco Dorigo. Optimization, learning and natural algorithms. PhD Thesis, Politecnico di Milano, 1992. 211 207. Marco Dorigo and Mauro Birattari. Swarm intelligence. Scholarpedia, 2(9):1462, 2007. 203, 223 208. Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant colony optimization. IEEE Computational Intelligence Magazine, 1(4):28\u201339, 2006. 211, 223, 224 209. Marco Dorigo and Luca Maria Gambardella. Ant colony system: a cooperative learning approach to the traveling salesman problem. IEEE Transactions on Evolutionary Computation, 1(1):53\u201366, 1997. 11, 211 210. Norman R Draper and Harry Smith. Applied Regression Analysis, volume 326. John Wiley & Sons, 1998. 14 211. Yuntao Du, Zhiwen Tan, Qian Chen, Xiaowen Zhang, Yirong Yao, and Chongjun Wang. Dual adversarial domain adaptation. arXiv preprint arXiv:2001.00153, 2020. 252 212. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u20131338, 2016. 114, 115, 116 213. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. 188, 254, 256, 265, 268, 275 214. Ishan P Durugkar, Clemens Rosenbaum, Stefan Dernbach, and Sridhar Mahadevan. Deep reinforcement learning with macro-actions. arXiv preprint arXiv:1606.04615, 2016. 241 215. Werner Duvaud and Aur\u00e8le Hainaut. MuZero general: Open reimplementation of muzero. https://github.com/werner-duvaud/muzero-general, 2019. 146, 275 216. Zach Dwiel, Madhavun Candadai, Mariano Phielipp, and Arjun K Bansal. Hierarchical policy learning is sensitive to goal space design. arXiv preprint arXiv:1905.01537, 2019. 231 217. Russell C Eberhart, Yuhui Shi, and James Kennedy. Swarm Intelligence. Elsevier, 2001. 223 218. Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018. 133 219. Tom Eccles, Edward Hughes, J\u00e1nos Kram\u00e1r, Steven Wheelwright, and Joel Z Leibo. Learning reciprocity in complex sequential social dilemmas. arXiv preprint arXiv:1903.08082, 2019. 208 220. Adrien Lucas Ecofet. An intuitive explanation of policy gradient https: //towardsdatascience.com/an-intuitive-explanation-of-policy-gradientpart-1-reinforce-aa4392cbfd3c. 116 221. Adrien Ecofet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jef Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. 84 222. Adrien Ecofet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jef Clune. First return, then explore. Nature, 590(7847):580\u2013586, 2021. 84, 238, 277, 333 223. Harrison Edwards and Amos Storkey. Towards a neural statistician. In International Conference on Learning Representations, 2017. 268 224. Agoston E Eiben and Jim E Smith. What is an evolutionary algorithm? In Introduction to Evolutionary Computing, pages 25\u201348. Springer, 2015. 223 225. Jefrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71\u201399, 1993. 176 226. Arpad E Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978. 180 227. Markus Enzenberger, Martin Muller, Broderick Arneson, and Richard Segal. Fuego\u2014an open-source framework for board games and Go engine based on Monte Carlo tree search. IEEE Transactions on Computational Intelligence and AI in Games, 2(4):259\u2013270, 2010. 179, 187 346 References 228. Tom Erez, Yuval Tassa, and Emanuel Todorov. Simulation tools for model-based robotics: Comparison of Bullet, Havok, MuJoCo, Ode and Physx. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 4397\u20134404. IEEE, 2015. 93 229. Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503\u2013556, April 2005. 87 230. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In International Conference on Machine Learning, pages 1407\u20131416. PMLR, 2018. 265 231. Eyal Even-Dar, Yishay Mansour, and Peter Bartlett. Learning rates for Q-learning. Journal of machine learning Research, 5(1), 2003. 54 232. Richard Everett and Stephen Roberts. Learning against non-stationary agents with opponent modelling and deep reinforcement learning. In 2018 AAAI Spring Symposium Series, 2018. 208 233. Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 109\u2013117. ACM, 2004. 251 234. Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa, Silvio Savarese, and Li Fei-Fei. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, pages 767\u2013782, 2018. 114 235. Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in DQN. arXiv preprint arXiv:1810.00123, 2018. 278 236. Gregory Farquhar, Tim Rockt\u00e4schel, Maximilian Igl, and SA Whiteson. TreeQN and ATreeC: Diferentiable tree planning for deep reinforcement learning. In International Conference on Learning Representations, 2018. 57, 135, 136, 138, 144 237. Li Fei-Fei, Jia Deng, and Kai Li. Imagenet: Constructing a large-scale image database. Journal of Vision, 9(8):1037\u20131037, 2009. 65, 262, 276, 323 238. Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-based value estimation for efcient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018. 132, 138, 144, 207 239. Dieqiao Feng, Carla P Gomes, and Bart Selman. Solving hard AI planning instances using curriculum-driven deep reinforcement learning. arXiv preprint arXiv:2006.02689, 2020. 57, 144, 177, 178, 188, 275 240. Santiago Fern\u00e1ndez, Alex Graves, and J\u00fcrgen Schmidhuber. An application of recurrent neural networks to discriminative keyword spotting. In International Conference on Artifcial Neural Networks, pages 220\u2013229. Springer, 2007. 313 241. Richard E Fikes, Peter E Hart, and Nils J Nilsson. Learning and executing generalized robot plans. Artifcial Intelligence, 3:251\u2013288, 1972. 231, 232 242. Richard E Fikes and Nils J Nilsson. STRIPS: A new approach to the application of theorem proving to problem solving. Artifcial Intelligence, 2(3-4):189\u2013208, 1971. 9 243. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u2013 1135. PMLR, 2017. 254, 257, 258, 259, 263, 268, 333 244. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786\u20132793. IEEE, 2017. 133, 138, 144 245. Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International Conference on Machine Learning, pages 1920\u20131930. PMLR, 2019. 268 246. Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic Model-Agnostic Meta-Learning. In Advances in Neural Information Processing Systems, pages 9516\u20139527, 2018. 268 247. Yannis Flet-Berliac. The promise of hierarchical reinforcement learning. https:// thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/, March 2019. 227, 228, 276 References 347 248. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International Conference on Machine Learning, pages 1515\u20131528. PMLR, 2018. 177, 188, 241 249. Michael J Flynn. Some computer organizations and their efectiveness. IEEE Transactions on Computers, 100(9):948\u2013960, 1972. 324 250. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 32, 2018. 207 251. Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017. 208 252. David B Fogel. An introduction to simulated evolutionary optimization. IEEE Transactions on Neural Networks, 5(1):3\u201314, 1994. 11, 202 253. David B Fogel, Timothy J Hays, Sarah L Hahn, and James Quon. Further evolution of a self-learning chess program. In Computational Intelligence in Games, 2005. 162 254. Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration. In International Conference on Learning Representations, 2018. 80, 83 255. Vincent Fran\u00e7ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An introduction to deep reinforcement learning. Foundations and Trends in Machine Learning, 11(3-4):219\u2013354, 2018. 25, 28, 59, 116 256. Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In International Conference on Learning Representations, 2018. 231, 241 257. Nicholas Frosst and Geofrey Hinton. Distilling a neural network into a soft decision tree. In Proceedings of the First International Workshop on Comprehensibility and Explanation in AI and ML, 2017. 278 258. Vittorio Gallese and Alvin Goldman. Mirror neurons and the simulation theory of mindreading. Trends in Cognitive Sciences, 2(12):493\u2013501, 1998. 193, 211, 223 259. Sam Ganzfried and Tuomas Sandholm. Game theory-based opponent modeling in large imperfect-information games. In The 10th International Conference on Autonomous Agents and Multiagent Systems, volume 2, pages 533\u2013540, 2011. 161, 222 260. Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information games. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 37\u201345, 2015. 204 261. The garage contributors. Garage: A toolkit for reproducible reinforcement learning research. https://github.com/rlworkgroup/garage, 2019. 115, 266, 332 262. Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and practice\u2014a survey. Automatica, 25(3):335\u2013348, 1989. 133, 144 263. Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International Conference on Learning Representations, 2017. 268 264. Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1704\u20131713. PMLR, 2018. 268 265. Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti, and Renato Vidoni. Path planning and trajectory planning algorithms: A general overview. In Motion and Operation Planning of Robotic Systems, pages 3\u201327. Springer, 2015. 26, 56 266. Michael Gelfond and Vladimir Lifschitz. Action languages. Electronic Transactions on Artifcial Intelligence, 2(3\u20134):193\u2013210, 1998. 232 267. Sylvain Gelly, Levente Kocsis, Marc Schoenauer, Michele Sebag, David Silver, Csaba Szepesv\u00e1ri, and Olivier Teytaud. The grand challenge of computer Go: Monte Carlo tree search and extensions. Communications of the ACM, 55(3):106\u2013113, 2012. 179 268. Sylvain Gelly and David Silver. Achieving master level play in 9 \u00d7 9 computer Go. In AAAI, volume 8, pages 1537\u20131540, 2008. 179, 187 348 References 269. Sylvain Gelly, Yizao Wang, and Olivier Teytaud. Modifcation of UCT with patterns in Monte-Carlo Go. Technical Report RR-6062, INRIA, 2006. 167, 179, 187 270. Aur\u00e9lien G\u00e9ron. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. O\u2019Reilly Media, Inc., 2019. 77, 87, 327 271. Felix A Gers, J\u00fcrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. In Ninth International Conference on Artifcial Neural Networks ICANN 99. IET, 1999. 315 272. Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning: theory and practice. Elsevier, 2004. 231 273. Mohammad Ghavamzadeh, Sridhar Mahadevan, and Rajbala Makar. Hierarchical multi-agent reinforcement learning. Autonomous Agents and Multi-Agent Systems, 13(2):197\u2013229, 2006. 238 274. Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why generalization in RL is difcult: Epistemic POMDPs and implicit partial observability. Advances in Neural Information Processing Systems, 34, 2021. 278 275. Gerd Gigerenzer and Daniel G Goldstein. Reasoning the fast and frugal way: models of bounded rationality. Psychological review, 103(4):650, 1996. 209, 223 276. Thomas Gilovich, Dale Grifn, and Daniel Kahneman. Heuristics and Biases: The Psychology of Intuitive Judgment. Cambridge university press, 2002. 209 277. Andrew Gilpin and Tuomas Sandholm. A competitive Texas Hold\u2019em poker player via automated abstraction and real-time equilibrium computation. In Proceedings of the National Conference on Artifcial Intelligence, volume 21, page 1007, 2006. 223 278. Ross Girshick, Jef Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 580\u2013587, 2014. 245 279. John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society: Series B (Methodological), 41(2):148\u2013164, 1979. 47 280. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, 2016. 11, 70, 86, 303, 304, 305, 307, 310, 312, 315, 320, 321 281. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672\u20132680, 2014. 316, 317 282. Geofrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning Proceedings 1995, pages 261\u2013268. Elsevier, 1995. 87 283. Geofrey J Gordon. Approximate solutions to Markov decision processes. Carnegie Mellon University, 1999. 66, 73 284. Tobias Graf and Marco Platzner. Adaptive playouts in Monte-Carlo tree search with policygradient reinforcement learning. In Advances in Computer Games, pages 1\u201311. Springer, 2015. 187 285. Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifths. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018. 268 286. Alex Graves, Santiago Fern\u00e1ndez, and J\u00fcrgen Schmidhuber. Bidirectional LSTM networks for improved phoneme classifcation and recognition. In International Conference on Artifcial Neural Networks, pages 799\u2013804. Springer, 2005. 315 287. Alex Graves, Abdel-rahman Mohamed, and Geofrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645\u20136649. IEEE, 2013. 307 288. Klaus Gref, Rupesh K Srivastava, Jan Koutn\u00edk, Bas R Steunebrink, and J\u00fcrgen Schmidhuber. LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10):2222\u20132232, 2017. 315 289. Jean-Bastien Grill, Florent Altch\u00e9, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, and R\u00e9mi Munos. Monte-carlo tree search as regularized policy optimization. In International Conference on Machine Learning, pages 3769\u20133778. PMLR, 2020. 136, 141, 170 References 349 290. Christopher Grimm, Andr\u00e9 Barreto, Satinder Singh, and David Silver. The value equivalence principle for model-based reinforcement learning. In Advances in Neural Information Processing Systems, 2020. 130, 140, 141, 142 291. Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. There is no turning back: A self-supervised approach for reversibility-aware reinforcement learning. arXiv preprint arXiv:2106.04480, 2021. 242 292. Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artifcial Intelligence Review, pages 1\u201349, 2021. 198, 222 293. Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actorcritic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291\u20131307, 2012. 83, 99 294. Peter D Gr\u00fcnwald. The minimum description length principle. MIT press, 2007. 15, 318 295. Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. The reactor: A fast and sample-efcient actor-critic agent for reinforcement learning. In International Conference on Learning Representations, 2018. 85 296. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Qlearning with model-based acceleration. In International Conference on Machine Learning, pages 2829\u20132838, 2016. 132, 138, 144 297. Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored MDPs. In Advances in Neural Information Processing Systems, volume 1, pages 1523\u20131530, 2001. 207 298. Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, S\u00e9bastien Racani\u00e8re, Theophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, and Timothy P. Lillicrap. An investigation of model-free planning. In International Conference on Machine Learning, pages 2464\u20132473, 2019. 57, 138, 144 299. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio G\u00f3mez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL unplugged: Benchmarks for ofine reinforcement learning. arXiv preprint arXiv:2006.13888, 2020. 332 300. David Gunning. Explainable artifcial intelligence (XAI). Defense Advanced Research Projects Agency (DARPA), 2, 2017. 278 301. Xifeng Guo, Wei Chen, and Jianping Yin. A simple approach for unsupervised domain adaptation. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 1566\u2013 1570. IEEE, 2016. 252 302. Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. In Advances in Neural Information Processing Systems, pages 5307\u20135316, 2018. 268 303. David Ha and J\u00fcrgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pages 2450\u20132462, 2018. 131 304. David Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 130, 131, 138, 144, 278 305. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pages 1352\u20131361. PMLR, 2017. 107 306. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Of-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018. 95, 107, 140, 272 307. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actorcritic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. 107 308. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. 130, 131, 138, 140, 144 350 References 309. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pages 2555\u20132565, 2019. 130, 131, 133, 138, 140, 141, 142, 144, 333 310. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. 130, 131, 138, 140, 144, 333 311. Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control. Machine Learning, 84(1-2):137\u2013169, 2011. 108 312. Dongge Han, Wendelin Boehmer, Michael Wooldridge, and Alex Rogers. Multi-agent hierarchical reinforcement learning with dynamic termination. In Pacifc Rim International Conference on Artifcial Intelligence, pages 80\u201392. Springer, 2019. 238 313. Dongge Han, Chris Xiaoxuan Lu, Tomasz Michalak, and Michael Wooldridge. Multiagent model-based credit assignment for continuous control, 2021. 207 314. Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems, pages 2613\u20132621, 2010. 82 315. Matthew John Hausknecht. Cooperation and Communication in Multiagent Deep Reinforcement Learning. PhD thesis, University of Texas at Austin, 2016. 208 316. Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas L Dean, and Craig Boutilier. Hierarchical solution of Markov decision processes using macro-actions. In UAI \u201998: Proceedings of the Fourteenth Conference on Uncertainty in Artifcial Intelligence, University of Wisconsin Business School, Madison, Wisconsin, 1998. 230, 241 317. Conor F. Hayes, Roxana Radulescu, Eugenio Bargiacchi, Johan K\u00e4llstr\u00f6m, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Dazeley, Fredrik Heintz, Enda Howley, Athirai A. Irissappane, Patrick Mannion, Ann Now\u00e9, Gabriel de Oliveira Ramos, Marcello Restelli, Peter Vamplew, and Diederik M. Roijers. A practical guide to multi-objective reinforcement learning and planning. arXiv preprint arXiv:2103.09568, 2021. 198 318. Simon Haykin. Neural Networks: a Comprehensive Foundation. Prentice Hall, 1994. 11 319. Ryan B Hayward and Bjarne Toft. Hex: The Full Story. CRC Press, 2019. 184 320. He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9 III. Opponent modeling in deep reinforcement learning. In International Conference on Machine Learning, pages 1804\u20131813. PMLR, 2016. 161, 208 321. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016. 172, 315, 316 322. Robert A Hearn and Erik D Demaine. Games, Puzzles, and Computation. CRC Press, 2009. 27, 56 323. David Heckerman, Dan Geiger, and David M Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197\u2013243, 1995. 278 324. Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with energy-based policies. In European Workshop on Reinforcement Learning, pages 45\u201358, 2013. 74, 75, 86 325. Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, SM Eslami, Martin Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017. 90, 113, 212 326. Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages 2944\u20132952, 2015. 138, 144 327. Ernst A Heinz. New self-play results in computer chess. In International Conference on Computers and Games, pages 262\u2013276. Springer, 2000. 188 328. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artifcial Intelligence, 2018. 87, 114, 115, 116, 138 References 351 329. Mark Hendrikx, Sebastiaan Meijer, Joeri Van Der Velden, and Alexandru Iosup. Procedural content generation for games: A survey. ACM Transactions on Multimedia Computing, Communications, and Applications, 9(1):1\u201322, 2013. 59 330. John L Hennessy and David A Patterson. Computer Architecture: a Quantitative Approach. Elsevier, 2017. 324 331. Joseph Henrich, Robert Boyd, and Peter J Richerson. Five misunderstandings about cultural evolution. Human Nature, 19(2):119\u2013137, 2008. 223 332. Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183, 2017. 193, 196, 201, 222 333. Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. A survey and critique of multiagent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6):750\u2013797, 2019. 198, 222 334. Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane Weber, David Silver, and Hado van Hasselt. Muesli: Combining improvements in policy optimization. In International Conference on Machine Learning, pages 4214\u20134226, 2021. 141, 275 335. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, pages 3215\u20133222, 2018. 80, 81, 82, 87 336. Francis Heylighen. What makes a Meme Successful? Selection Criteria for Cultural Evolution. Association Internationale de Cybernetique, 1998. 223 337. Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. In International Conference on Machine Learning, pages 1480\u20131490. PMLR, 2017. 268 338. Ashley Hill, Antonin Rafn, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https: //github.com/hill-a/stable-baselines, 2018. 333 339. W Daniel Hillis. New computer architectures and their relationship to physics or why computer science is no good. International Journal of Theoretical Physics, 21(3-4):255\u2013262, 1982. 324 340. W Daniel Hillis and Lewis W Tucker. The CM-5 connection machine: A scalable supercomputer. Communications of the ACM, 36(11):30\u201341, 1993. 324 341. Geofrey Hinton, Oriol Vinyals, and Jef Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 278 342. Geofrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006. 317 343. Geofrey E Hinton and Terrence Joseph Sejnowski, editors. Unsupervised Learning: Foundations of Neural Computation. MIT press, 1999. 15 344. Geofrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 320 345. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. 313, 315 346. John Holland. Adaptation in natural and artifcial systems: an introductory analysis with application to biology. Control and Artifcial Intelligence, 1975. 47, 202 347. John H Holland. Genetic algorithms. Scientifc American, 267(1):66\u201373, 1992. 11 348. Bert H\u00f6lldobler and Edward O Wilson. The Superorganism: the Beauty, Elegance, and Strangeness of Insect Societies. WW Norton & Company, 2009. 191 349. John J Hopfeld. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8):2554\u20132558, 1982. 313 352 References 350. Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. In International Conference on Learning Representations, 2018. 221 351. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020. 247, 248, 253, 255, 267, 268 352. Ronald A Howard. Dynamic programming and Markov processes. New York: John Wiley, 1964. 28 353. Chloe Ching-Yun Hsu, Celestine Mendler-D\u00fcnner, and Moritz Hardt. Revisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020. 106 354. Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated the world chess champion. Princeton University Press, 2004. 64 355. Feng-Hsiung Hsu, Thomas Anantharaman, Murray Campbell, and Andreas Nowatzyk. A grandmaster chess machine. Scientifc American, 263(4):44\u201351, 1990. 69 356. R Lily Hu, Caiming Xiong, and Richard Socher. Zero-shot image classifcation guided by natural language descriptions of classes: A meta-learning approach. In Advances in Neural Information Processing Systems, 2018. 268 357. David H Hubel and Torsten N Wiesel. Shape and arrangement of columns in cat\u2019s striate cortex. The Journal of Physiology, 165(3):559\u2013568, 1963. 310 358. David H Hubel and Torsten N Wiesel. Receptive felds and functional architecture of monkey striate cortex. The Journal of Physiology, 195(1):215\u2013243, 1968. 310 359. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, pages 4476\u20134486, 2021. 140, 275 360. Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes ImageNet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016. 245, 268 361. Jonathan Hui. RL\u2014DQN Deep Q-network https://medium.com/@jonathan_hui/rl-dqndeep-q-network-e207751f7ae4. Medium post. 84 362. Jonathan Hui. Model-based reinforcement learning https://medium.com/@jonathan_hui/ rl-model-based-reinforcement-learning-3c2b6f0aa323. Medium post, 2018. 129 363. Mike Huisman, Jan van Rijn, and Aske Plaat. Metalearning for deep neural networks. In Pavel Brazdil et al., editors, Metalearning: Applications to data mining. Springer, 2022. 247, 258, 259, 267, 268 364. Mike Huisman, Jan N. van Rijn, and Aske Plaat. A survey of deep meta-learning. Artifcial Intelligence Review, 2021. 248, 253, 254, 255, 256, 257, 267, 268, 379 365. Matthew Hutson. Artifcial Intelligence faces reproducibility crisis. Science, 359:725\u2013726, 2018. 87 366. Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm confguration. In International Conference on Learning and Intelligent Optimization, pages 507\u2013523. Springer, 2011. 259 367. Frank Hutter, Holger H Hoos, Kevin Leyton-Brown, and Thomas St\u00fctzle. ParamILS: an automatic algorithm confguration framework. Journal of Artifcial Intelligence Research, 36:267\u2013306, 2009. 259 368. Frank Hutter, Lars Kotthof, and Joaquin Vanschoren. Automated Machine Learning: Methods, Systems, Challenges. Springer Nature, 2019. 260 369. Roman Ilin, Robert Kozma, and Paul J Werbos. Efcient learning in cellular simultaneous recurrent neural networks\u2014the case of maze navigation problem. In 2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, pages 324\u2013 329, 2007. 135, 139, 144 370. Sergey Iofe. Batch renormalization: Towards reducing minibatch dependence in batchnormalized models. In Advances in Neural Information Processing Systems, pages 1945\u20131953, 2017. 321 371. Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017. 87 References 353 372. Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and human-like gameplay with KL-regularized search. arXiv preprint arXiv:2112.07544, 2021. 170 373. Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Casta\u00f1eda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Human-level performance in 3D multiplayer games with population-based reinforcement learning. Science, 364(6443):859\u2013865, 2019. 68, 69, 214, 218, 277 374. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jef Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. 212, 213, 219, 223, 265, 277, 333 375. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLbench: the robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020. 114 376. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, pages 12498\u201312509, 2019. 129, 132, 138, 144 377. Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. arXiv preprint arXiv:2106.02039, 2021. 320 378. Andrew A Jawlik. Statistics from A to Z: Confusing concepts clarifed. John Wiley & Sons, 2016. 288 379. Michael Johanson, Nolan Bard, Marc Lanctot, Richard G Gibson, and Michael Bowling. Efcient Nash equilibrium approximation through Monte Carlo counterfactual regret minimization. In AAMAS, pages 837\u2013846, 2012. 203, 204, 223 380. Ian T Jollife and Jorge Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016. 15 381. Michael Irwin Jordan. Learning in Graphical Models, volume 89. Springer Science & Business Media, 1998. 278 382. Arthur Juliani. Simple reinforcement learning with tensorfow part 8: Asynchronous actorcritic agents (A3C) https://medium.com/emergent-future/simple-reinforcementlearning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3cc88f72a5e9f2, 2016. 103, 104 383. Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, and Danny Lange. Unity: A general platform for intelligent agents. arXiv preprint arXiv:1809.02627, 2018. 265, 332 384. Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in vision, control, and planning. arXiv preprint arXiv:1902.01378, 2019. 223 385. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589, 2021. 188, 275 386. Andreas Junghanns and Jonathan Schaefer. Sokoban: Enhancing general single-agent search methods using domain knowledge. Artifcial Intelligence, 129(1-2):219\u2013251, 2001. 27, 56 387. Niels Justesen, Philip Bontrager, Julian Togelius, and Sebastian Risi. Deep learning for video game playing. IEEE Transactions on Games, 12(1):1\u201320, 2019. 87 354 References 388. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. Illuminating generalization in deep reinforcement learning through procedural level generation. arXiv preprint arXiv:1806.10729, 2018. 177 389. Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of Artifcial Intelligence Research, 4:237\u2013285, 1996. 7, 59 390. Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook of the Fundamentals of Financial Decision Making: Part I, pages 99\u2013127. World Scientifc, 2013. 223 391. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-based reinforcement learning for Atari. arXiv:1903.00374, 2019. 130, 133, 138, 144 392. Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin L. Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. SGD on neural networks learns functions of increasing complexity. In Advances in Neural Information Processing Systems, pages 3491\u20133501, 2019. 321 393. Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep reinforcement learning. In Conference on Robot Learning, pages 195\u2013206, 2017. 132 394. Reza Kamyar and Ehsan Taheri. Aircraft optimal terrain/threat-based trajectory planning and control. Journal of Guidance, Control, and Dynamics, 37(2):466\u2013483, 2014. 133 395. Satwik Kansal and Brendan Martin. Learn data science webpage., 2018. 42, 52, 53, 54, 61, 126 396. Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of statistical mechanics: theory and experiment, 2005(11):P11011, 2005. 107 397. Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International Conference on Learning Representations, 2018. 85 398. Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational Bayes flters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016. 131 399. Andrej Karpathy. The unreasonable efectiveness of recurrent neural networks. http: //karpathy.github.io/2015/05/21/rnn-effectiveness/. Andrej Karpathy Blog, 2015. 314, 315 400. Andrej Karpathy. Deep reinforcement learning: Pong from pixels. http://karpathy. github.io/2016/05/31/rl/. Andrej Karpathy Blog, 2016. 80 401. Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015. 142 402. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. 316, 318 403. Henry J Kelley. Gradient theory of optimal fight paths. American Rocket Society Journal, 30(10):947\u2013954, 1960. 135, 139, 144 404. Stephen Kelly and Malcolm I Heywood. Multi-task learning in Atari video games with emergent tangled program graphs. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 195\u2013202. ACM, 2017. 251 405. Stephen Kelly and Malcolm I Heywood. Emergent tangled program graphs in multi-task learning. In IJCAI, pages 5294\u20135298, 2018. 251 406. James Kennedy. Swarm intelligence. In Handbook of Nature-Inspired and Innovative Computing, pages 187\u2013219. Springer, 2006. 11, 223 407. Pascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated algorithm selection: Survey and perspectives. Evolutionary Computation, 27(1):3\u201345, 2019. 259 408. Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago Miret, Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In International Conference on Machine Learning, pages 3341\u20133350. PMLR, 2019. 223, 277 References 355 409. Shauharda Khadka and Kagan Tumer. Evolutionary reinforcement learning. arXiv preprint arXiv:1805.07917, 2018. 223 410. Khimya Khetarpal, Zafarali Ahmed, Andre Cianfone, Riashat Islam, and Joelle Pineau. Reevaluate: Reproducibility in evaluating reinforcement learning algorithms. In Reproducibility in Machine Learning Workshop, ICML, 2018. 87 411. Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. 15, 131, 317 412. Diederik P Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach. Learn., 12(4):307\u2013392, 2019. 15, 131, 317 413. Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of generalisation in deep reinforcement learning. arXiv preprint arXiv:2111.09794, 2021. 279 414. Daan Klijn and AE Eiben. A coevolutionairy approach to deep multi-agent reinforcement learning. arXiv preprint arXiv:2104.05610, 2021. 221, 223 415. Craig A Knoblock. Learning abstraction hierarchies for problem solving. In AAAI, pages 923\u2013928, 1990. 231, 232 416. Donald E Knuth and Ronald W Moore. An analysis of alpha-beta pruning. Artifcial Intelligence, 6(4):293\u2013326, 1975. 151, 161, 163 417. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013. 111, 143, 144 418. Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning workshop, volume 2. Lille, 2015. 268 419. Levente Kocsis and Csaba Szepesv\u00e1ri. Bandit based Monte-Carlo planning. In European Conference on Machine Learning, pages 282\u2013293. Springer, 2006. 164, 167, 168, 187 420. Vijay R Konda and John N Tsitsiklis. Actor\u2013critic algorithms. In Advances in Neural Information Processing Systems, pages 1008\u20131014, 2000. 99 421. Vijaymohan R Konda and Vivek S Borkar. Actor\u2013Critic-type learning algorithms for Markov Decision Processes. SIAM Journal on Control and Optimization, 38(1):94\u2013123, 1999. 99 422. Richard E Korf. Depth-frst iterative-deepening: An optimal admissible tree search. Artifcial intelligence, 27(1):97\u2013109, 1985. 163 423. Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Robot motor skill coordination with em-based reinforcement learning. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3232\u20133237. IEEE, 2010. 5, 6 424. Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge \u2019naturally\u2019 in multi-agent dialog. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, pages 2962\u20132967, 2017. 208 425. Samuel Kotz, Narayanaswamy Balakrishnan, and Norman L Johnson. Continuous Multivariate Distributions, Volume 1: Models and Applications. John Wiley & Sons, 2004. 48, 106 426. Basil Kouvaritakis and Mark Cannon. Model Predictive Control. Springer, 2016. 144 427. Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82\u201394, 2016. 207 428. Mark A Kramer. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal, 37(2):233\u2013243, 1991. 317 429. Sarit Kraus, Eithan Ephrati, and Daniel Lehmann. Negotiation in a non-cooperative environment. Journal of Experimental & Theoretical Artifcial Intelligence, 3(4):255\u2013281, 1994. 151 430. Sarit Kraus and Daniel Lehmann. Diplomat, an agent in a multi agent environment: An overview. In IEEE International Performance Computing and Communications Conference, pages 434\u2013438, 1988. 208 431. Alex Krizhevsky, Ilya Sutskever, and Geofrey E Hinton. Imagenet classifcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012. 65, 246, 307, 308, 323 432. Kai A Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps. Cognition, 110(3):380\u2013394, 2009. 176 356 References 433. Steven Kuhn. Prisoner\u2019s Dilemma. The Stanford Encyclopedia of Philosophy, https:// plato.stanford.edu/entries/prisoner-dilemma/, 1997. 198 434. Jan Kuipers, Aske Plaat, Jos AM Vermaseren, and H Jaap van den Herik. Improving multivariate Horner schemes with Monte Carlo tree search. Computer Physics Communications, 184(11):2391\u20132395, 2013. 169, 170 435. Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 3675\u20133683, 2016. 229, 231, 237, 238, 241 436. Solomon Kullback and Richard A Leibler. On information and sufciency. The Annals of Mathematical Statistics, 22(1):79\u201386, 1951. 105 437. Yen-Ling Kuo, Boris Katz, and Andrei Barbu. Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas. arXiv preprint arXiv:2006.01110, 2020. 268 438. Karol Kurach, Anton Raichuk, Piotr Sta\u0144czyk, Micha\u0142 Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football: A novel reinforcement learning environment. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 34, pages 4501\u20134510, 2020. 221, 332 439. Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. In International Conference on Learning Representations, 2018. 129, 144 440. W Hi Kwon, AM Bruckstein, and T Kailath. Stabilizing state-feedback design via the moving horizon method. International Journal of Control, 37(3):631\u2013643, 1983. 133 441. Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107\u20131149, Dec 2003. 87 442. Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics, pages 1091\u20131114, 1987. 48 443. Tze Leung Lai and Herbert Robbins. Asymptotically efcient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4\u201322, 1985. 47, 48 444. John E Laird, Paul S Rosenbloom, and Allen Newell. Chunking in Soar: the anatomy of a general learning mechanism. Machine learning, 1(1):11\u201346, 1986. 230, 241 445. Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011. 253, 262, 332 446. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015. 262 447. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. The Omniglot challenge: a 3-year progress report. Current Opinion in Behavioral Sciences, 29:97\u2013104, 2019. 262 448. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402\u20136413, 2017. 133 449. Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 951\u2013958. IEEE, 2009. 261, 268 450. Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vin\u00edcius Flores Zambaldi, Satyaki Upadhyay, Julien P\u00e9rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, J\u00e1nos Kram\u00e1r, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas W. Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis. Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019. 332 451. Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling for regret minimization in extensive games. In Advances in Neural Information Processing Systems, pages 1078\u20131086, 2009. 196, 203, 204, 223 References 357 452. Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unifed game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems, pages 4190\u2013 4203, 2017. 194 453. Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2010. 74 454. Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI, volume 1, page 3, 2008. 260, 268 455. Alexandre Laterre, Yunguan Fu, Mohamed Khalil Jabri, Alain-Sam Cohen, David Kas, Karl Hajjar, Torbjorn S Dahl, Amine Kerkeni, and Karim Beguir. Ranked reward: Enabling self-play reinforcement learning for combinatorial optimization. arXiv preprint arXiv:1807.01672, 2018. 177, 178, 188, 275 456. Jean-Claude Latombe. Robot Motion Planning, volume 124. Springer Science & Business Media, 2012. 26, 56 457. Stefen L Lauritzen. Graphical Models, volume 17. Clarendon Press, 1996. 278 458. Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In International Conference on Learning Representations, 2017. 208 459. Yann LeCun, Yoshua Bengio, and Geofrey Hinton. Deep learning. Nature, 521(7553):436, 2015. 11, 12, 86, 244, 307 460. Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541\u2013551, 1989. 311 461. Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Hafner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. 308, 309, 322 462. Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609\u2013616. ACM, 2009. 307, 308 463. Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, pages 2927\u20132936. PMLR, 2018. 255 464. Joel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv preprint arXiv:1903.00742, 2019. 187, 212, 217, 275 465. Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multiagent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2017, S\u00e3o Paulo, Brazil, pages 464\u2013473, 2017. 206, 208, 209, 216 466. Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jefrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert C. Zak. The network architecture of the connection machine CM-5. In Proceedings of the fourth annual ACM Symposium on Parallel Algorithms and Architectures, pages 272\u2013285, 1992. 324 467. Matteo Leonetti, Luca Iocchi, and Peter Stone. A synthesis of automated planning and reinforcement learning for efcient, robust decision-making. Artifcial Intelligence, 241:103\u2013 130, 2016. 132 468. Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pages 1071\u20131079, 2014. 138, 144 469. Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pages 1\u20139, 2013. 128, 144 358 References 470. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 251 471. Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies with hindsight. In International Conference on Learning Representations, 2019. 231, 233, 236, 239, 241, 276 472. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems, pages 6391\u20136401, 2018. 142 473. Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. 268 474. Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit coordination graphs for multi-agent reinforcement learning. In AAMAS \u201921: 20th International Conference on Autonomous Agents and Multiagent Systems, 2021. 207 475. Siyuan Li, Rui Wang, Minxue Tang, and Chongjie Zhang. Hierarchical reinforcement learning with advantage-based auxiliary rewards. In Advances in Neural Information Processing Systems, pages 1407\u20131417, 2019. 276 476. Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017. 254, 268 477. Zhuoru Li, Akshay Narayan, and Tze-Yun Leong. An efcient approach to model-based hierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 31, 2017. 231 478. Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E Gonzalez, Michael I Jordan, and Ion Stoica. RLlib: abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pages 3059\u20133068, 2018. 332 479. Diego P\u00e9rez Li\u00e9bana, Simon M Lucas, Raluca D Gaina, Julian Togelius, Ahmed Khalifa, and Jialin Liu. General video game artifcial intelligence. Synthesis Lectures on Games and Computational Intelligence, 3(2):1\u2013191, 2019. 177 480. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016. 92, 95, 108, 109, 116, 140, 272 481. Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3-4):293\u2013321, 1992. 75, 87 482. Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. 74, 75 483. Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pages 157\u2013163. Elsevier, 1994. 28, 194, 222 484. Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A comprehensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3):385\u2013398, 2014. 223 485. Hao Liu and Pieter Abbeel. Hybrid discriminative-generative training via contrastive learning. arXiv preprint arXiv:2007.09070, 2020. 73 486. Hui Liu, Song Yu, Zhangxin Chen, Ben Hsieh, and Lei Shao. Sparse matrix-vector multiplication on NVIDIA GPU. International Journal of Numerical Analysis & Modeling, Series B, 3(2):185\u2013191, 2012. 324 487. Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel. Emergent coordination through competition. In International Conference on Learning Representations, 2019. 212, 223 488. Manuel L\u00f3pez-Ib\u00e1\u00f1ez, J\u00e9r\u00e9mie Dubois-Lacoste, Leslie P\u00e9rez C\u00e1ceres, Mauro Birattari, and Thomas St\u00fctzle. The irace package: Iterated racing for automatic algorithm confguration. Operations Research Perspectives, 3:43\u201358, 2016. 259 489. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent Actor-Critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 6379\u20136390, 2017. 207, 213, 333 References 359 490. Gabriel Loye. The attention mechanism. https://blog.floydhub.com/attentionmechanism/. 319 491. Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the efectiveness of SGD in modern over-parametrized learning. In International Conference on Machine Learning, pages 3331\u20133340, 2018. 321 492. Xiaoliang Ma, Xiaodong Li, Qingfu Zhang, Ke Tang, Zhengping Liang, Weixin Xie, and Zexuan Zhu. A survey on cooperative co-evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 23(3):421\u2013441, 2018. 210 493. Laurens van der Maaten and Geofrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579\u20132605, Nov 2008. 15 494. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artifcial Intelligence Research, 61:523\u2013562, 2018. 86 495. Hamid Reza Maei, Csaba Szepesv\u00e1ri, Shalabh Bhatnagar, and Richard S Sutton. Toward of-policy learning control with function approximation. In International Conference on Machine Learning, 2010. 74 496. Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multiagent variational exploration. In Advances in Neural Information Processing Systems, pages 7611\u20137622, 2019. 207 497. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181\u2013196, 2018. 244 498. Somdeb Majumdar, Shauharda Khadka, Santiago Miret, Stephen McAleer, and Kagan Tumer. Evolutionary reinforcement learning for sample-efcient multiagent coordination. In International Conference on Machine Learning, 2020. 223 499. Rajbala Makar, Sridhar Mahadevan, and Mohammad Ghavamzadeh. Hierarchical multi-agent reinforcement learning. In Proceedings of the Fifth International Conference on Autonomous Agents, pages 246\u2013253. ACM, 2001. 238 500. Julian N Marewski, Wolfgang Gaissmaier, and Gerd Gigerenzer. Good judgments do not require complex cognition. Cognitive Processing, 11(2):103\u2013121, 2010. 209 501. Vince Martinelli. How robots autonomously see, grasp, and pick. https://www. therobotreport.com/grasp-sight-picking-evolve-robots/, 2019. 93 502. Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. IEEE Trans. Neural Networks Learn. Syst., 31(9):3732\u20133740, 2020. 188 503. Masakazu Matsugu, Katsuhiko Mori, Yusuke Mitari, and Yuji Kaneda. Subject independent facial expression recognition with robust face detection using a convolutional neural network. Neural Networks, 16(5-6):555\u2013559, 2003. 310 504. Kiminori Matsuzaki. Empirical analysis of PUCT algorithm with evaluation functions of diferent quality. In 2018 Conference on Technologies and Applications of Artifcial Intelligence (TAAI), pages 142\u2013147. IEEE, 2018. 168, 169 505. David Q Mayne, James B Rawlings, Christopher V Rao, and Pierre OM Scokaert. Constrained model predictive control: Stability and optimality. Automatica, 36(6):789\u2013814, 2000. 144 506. James L McClelland, Bruce L McNaughton, and Randall C O\u2019Reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102(3):419, 1995. 75 507. Francisco S Melo and M Isabel Ribeiro. Convergence of Q-learning with linear function approximation. In 2007 European Control Conference (ECC), pages 2671\u20132678. IEEE, 2007. 72 508. Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas Heess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In International Conference on Learning Representations, 2019. 114 360 References 509. Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, and Bence \u00d6lveczky. Deep neuroethology of a virtual rodent. In International Conference on Learning Representations, 2020. 114 510. Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control. In International Conference on Learning Representations, 2019. 114 511. Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201, 2017. 114 512. Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Dufy, and Babak Hodjat. Evolving deep neural networks. In Artifcial Intelligence in the Age of Neural Networks and Brain Computing, pages 293\u2013312. Elsevier, 2019. 277 513. Tomas Mikolov, Kai Chen, Greg Corrado, and Jefrey Dean. Efcient estimation of word representations in vector space. In International Conference on Learning Representations, 2013. 245, 249 514. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jefrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, 2013. 245 515. Jonathan K Millen. Programming the game of Go. Byte Magazine, 1981. 153 516. S Ali Mirsoleimani, Aske Plaat, Jaap Van Den Herik, and Jos Vermaseren. Scaling Monte Carlo tree search on Intel Xeon Phi. In Parallel and Distributed Systems (ICPADS), 2015 IEEE 21st International Conference on, pages 666\u2013673. IEEE, 2015. 187 517. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018. 254, 268 518. Tom M Mitchell. The need for biases in learning generalizations. Technical Report CBM-TR117, Department of Computer Science, Rutgers University, 1980. 178 519. Tom M Mitchell. The discipline of machine learning. Technical Report CMU-ML-06-108, Carnegie Mellon University, School of Computer Science, Machine Learning, 2006. 178 520. Akshita Mittel and Purna Sowmya Munukutla. Visual transfer between Atari games using competitive reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019. 264 521. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016. 95, 103, 104, 105, 116, 272 522. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 41, 66, 72, 74, 75, 78, 79, 80, 83, 87, 150 523. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. 7, 66, 74, 75, 76, 77, 78, 79, 83, 84, 87, 179, 272 524. Thomas Moerland. Continuous Markov decision process and policy search. Lecture notes for the course reinforcement learning, Leiden University, 2021. 28, 72, 106, 108, 110, 283, 298 525. Thomas M Moerland. The Intersection of Planning and Learning. PhD thesis, Delft University of Technology, 2021. 275 526. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Efcient exploration with double uncertain value networks. arXiv preprint arXiv:1711.10789, 2017. 83 527. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. The potential of the return distribution for exploration in RL. arXiv preprint arXiv:1806.04242, 2018. 83 528. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. A framework for reinforcement learning and planning. arXiv preprint arXiv:2006.15009, 2020. 127 References 361 529. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement learning: A survey. arXiv preprint arXiv:2006.16712, 2020. 144 530. Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M Jonker. A0C: Alpha zero in continuous action space. arXiv preprint arXiv:1805.09613, 2018. 169, 187 531. Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M Jonker. Monte Carlo tree search for asymmetric trees. arXiv preprint arXiv:1805.09218, 2018. 187 532. Andrew William Moore. Efcient memory-based learning for robot control. Technical Report UCAM-CL-TR-209, University of Cambridge, UK, https://www.cl.cam.ac.uk/ techreports/UCAM-CL-TR-209.pdf, 1990. 55 533. Matej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artifcial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017. 214, 216, 223 534. Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multiagent populations. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 32, 2018. 212 535. Pol Moreno, Edward Hughes, Kevin R McKee, Bernardo Avila Pires, and Th\u00e9ophane Weber. Neural recursive belief states in multi-agent reinforcement learning. arXiv preprint arXiv:2102.02274, 2021. 208 536. David E Moriarty, Alan C Schultz, and John J Grefenstette. Evolutionary algorithms for reinforcement learning. Journal of Artifcial Intelligence Research, 11:241\u2013276, 1999. 223 537. Hossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon Whiteson. Multiobjective deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016. 223 538. Hussain Mujtaba. Introduction to autoencoders. https://www.mygreatlearning.com/ blog/autoencoder/, 2020. 318 539. Sendhil Mullainathan and Richard H Thaler. Behavioral economics. Technical report, National Bureau of Economic Research, 2000. 223 540. Martin M\u00fcller. Computer Go. Artifcial Intelligence, 134(1-2):145\u2013179, 2002. 69 541. Matthias M\u00fcller-Brockhausen, Mike Preuss, and Aske Plaat. Procedural content generation: Better benchmarks for transfer reinforcement learning. In Conference on Games, 2021. 176, 268 542. Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine Learning, pages 2554\u20132563. PMLR, 2017. 254, 268 543. Yoshio Murase, Hitoshi Matsubara, and Yuzuru Hiraga. Automatic making of Sokoban problems. In Pacifc Rim International Conference on Artifcial Intelligence, pages 592\u2013600. Springer, 1996. 27, 56 544. Derick Mwiti. Reinforcement learning applications. https://neptune.ai/blog/ reinforcement-learning-applications. 4 545. Roger B Myerson. Game Theory. Harvard university press, 2013. 222 546. Ofr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efcient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pages 3307\u2013 3317, 2018. 231, 233, 241, 276 547. Ofr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pages 2775\u20132785, 2017. 107 548. Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W Chapman. Natural language processing: an introduction. Journal of the American Medical Informatics Association, 18(5):544\u2013 551, 2011. 276 549. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fne-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559\u20137566, 2018. 133, 144, 146 550. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, 2020. 321 362 References 551. Nantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet Kohli, Philip HS Torr, and Nicolas Usunier. Value propagation networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2018. 138, 144 552. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. Journal Machine Learning Research, 2020. 178, 275 553. Sylvia Nasar. A Beautiful Mind. Simon and Schuster, 2011. 222 554. John Nash. Non-cooperative games. Annals of mathematics, pages 286\u2013295, 1951. 222 555. John F Nash. Equilibrium points in \ud835\udc5b-person games. Proceedings of the National Academy of Sciences, 36(1):48\u201349, 1950. 222 556. John F Nash Jr. The bargaining problem. Econometrica: Journal of the econometric society, pages 155\u2013162, 1950. 222 557. Yu Nasu. Efciently updatable neural-network-based evaluation functions for computer shogi. The 28th World Computer Shogi Championship Appeal Document, 2018. 162 558. Richard E Neapolitan. Learning Bayesian networks. Pearson Prentice Hall, Upper Saddle River, NJ, 2004. 278 559. Andrew Y Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. In Proceedings of the Twenty-frst International Conference on Machine Learning, page 78. ACM, 2004. 320 560. Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, volume 99, pages 278\u2013287, 1999. 52 561. Alex Nichol, Joshua Achiam, and John Schulman. On frst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. 254, 259, 268, 269 562. Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C Smith, and Jelena Kova\u010devi\u0107. Generalized value iteration networks: Life beyond lattices. In Thirty-Second AAAI Conference on Artifcial Intelligence, 2018. 134, 139 563. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Actionconditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, pages 2863\u20132871, 2015. 138, 144 564. Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118\u20136128, 2017. 130, 131, 135, 138, 140, 144, 333 565. Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pages 2661\u20132670. PMLR, 2017. 268 566. Kyoung-Su Oh and Keechul Jung. GPU implementation of neural networks. Pattern Recognition, 37(6):1311\u20131314, 2004. 324 567. Chris Olah. Understanding LSTM networks. http://colah.github.io/posts/2015-08Understanding-LSTMs/, 2015. 313 568. Frans A Oliehoek. Decentralized POMDPs. In Reinforcement Learning, pages 471\u2013503. Springer, 2012. 223 569. Frans A Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer, 2016. 151, 197, 212, 223 570. Frans A Oliehoek, Matthijs TJ Spaan, Christopher Amato, and Shimon Whiteson. Incremental clustering and expansion for faster optimal planning in Dec-POMDPs. Journal of Artifcial Intelligence Research, 46:449\u2013509, 2013. 212 571. Shayegan Omidshafei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International Conference on Machine Learning, pages 2681\u20132690. PMLR, 2017. 212 572. Joseph O\u2019Neill, Barty Pleydell-Bouverie, David Dupret, and Jozsef Csicsvari. Play it again: reactivation of waking experience and memory. Trends in Neurosciences, 33(5):220\u2013229, 2010. 75 References 363 573. Santiago Ontan\u00f3n, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. A survey of real-time strategy game AI research and competition in StarCraft. IEEE Transactions on Computational Intelligence and AI in Games, 5(4):293\u2013311, 2013. 68, 69, 219, 223 574. David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of Artifcial Intelligence Research, 11:169\u2013198, 1999. 129, 144 575. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesv\u00e1ri, Satinder Singh, Benjamin Van Roy, Richard S. Sutton, David Silver, and Hado van Hasselt. Behaviour suite for reinforcement learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2020. 332 576. Pierre-Yves Oudeyer and Frederic Kaplan. How can we defne intrinsic motivation? In the 8th International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems. Lund University Cognitive Studies, Lund: LUCS, Brighton, 2008. 241 577. Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of computational approaches. Frontiers in Neurorobotics, 1:6, 2009. 237, 241 578. Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265\u2013 286, 2007. 176, 241 579. Charles Packer, Katelyn Gao, Jernej Kos, Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, and Dawn Song. Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018. 278 580. Mark M Palatucci, Dean A Pomerleau, Geofrey E Hinton, and Tom Mitchell. Zero-shot learning with semantic output codes. In Advances in Neural Information Processing Systems 22, 2009. 260, 268 581. Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, 2010. 247, 248, 251, 268 582. Aleksandr I Panov and Aleksey Skrynnik. Automatic formation of the structure of abstract machines in hierarchical reinforcement learning with state clustering. arXiv preprint arXiv:1806.05292, 2018. 241 583. Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and Hans J Briegel. Quantum speedup for active learning agents. Physical Review X, 4(3):031002, 2014. 188 584. Philip Paquette, Yuchen Lu, Steven Bocco, Max Smith, O-G Satya, Jonathan K Kummerfeld, Joelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multiagent gameplay. In Advances in Neural Information Processing Systems, pages 4476\u20134487, 2019. 208, 223 585. German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019. 248 586. Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015. 264 587. Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, pages 7487\u20137498. PMLR, 2020. 265 588. Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In Advances in Neural Information Processing Systems, pages 1043\u20131049, 1998. 231, 232, 241 589. Gian-Carlo Pascutto. Leela zero. https://github.com/leela-zero/leela-zero, 2017. 183 590. Alexander Pashevich, Danijar Hafner, James Davidson, Rahul Sukthankar, and Cordelia Schmid. Modulated policy hierarchies. arXiv preprint arXiv:1812.00025, 2018. 231 591. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas 364 References K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In Advances in Neural Information Processing Systems, pages 8024\u20138035, 2019. 304, 321 592. Shubham Pateria, Budhitama Subagdja, Ah-hweewee Tan, and Chai Quek. Hierarchical reinforcement learning: A comprehensive survey. ACM Computing Surveys (CSUR), 54(5):1\u2013 35, 2021. 231, 241 593. Judea Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving. AddisonWesley, Reading, MA, 1984. 9, 161 594. Judea Pearl and Dana Mackenzie. The Book of Why: the New Science of Cause and Efect. Basic Books, 2018. 9 595. Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(Oct):2825\u20132830, 2011. 259, 326 596. Jefrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, 2014. 249 597. Alexandre P\u00e9r\u00e9, S\u00e9bastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised learning of goal spaces for intrinsically motivated goal exploration. In International Conference on Learning Representations, 2018. 233 598. Karl Pertsch, Oleh Rybkin, Frederik Ebert, Shenghao Zhou, Dinesh Jayaraman, Chelsea Finn, and Sergey Levine. Long-horizon visual planning with goal-conditioned hierarchical predictors. In Advances in Neural Information Processing Systems, 2020. 241 599. Aske Plaat. De vlinder en de mier / The butterfy and the ant\u2014on modeling behavior in organizations. Inaugural lecture. Tilburg University, 2010. 223 600. Aske Plaat. Learning to Play: Reinforcement Learning and Games. Springer Verlag, Heidelberg, https://learningtoplay.net, 2020. 59, 64, 161, 163, 187 601. Aske Plaat, Walter Kosters, and Mike Preuss. High-accuracy model-based reinforcement learning, a survey. arXiv preprint arXiv:2107.08241, 2021. 128, 138, 144, 379 602. Aske Plaat, Jonathan Schaefer, Wim Pijls, and Arie De Bruin. Best-frst fxed-depth minimax algorithms. Artifcial Intelligence, 87(1-2):255\u2013293, 1996. 163 603. Matthias Plappert. Keras-RL. https://github.com/keras-rl/keras-rl, 2016. 77 604. Jordan B Pollack and Alan D Blair. Why did TD-gammon work? In Advances in Neural Information Processing Systems, pages 10\u201316, 1997. 75 605. Aditya Prasad. Lessons from implementing alphazero https://medium.com/oracledevs/ lessons-from-implementing-alphazero-7e36e9054191, 2018. 173 606. Lorien Y Pratt. Discriminability-based transfer between neural networks. In Advances in Neural Information Processing Systems, pages 204\u2013211, 1993. 248, 268 607. Lutz Prechelt. Automatic early stopping using cross validation: quantifying the criteria. Neural Networks, 11(4):761\u2013767, 1998. 321 608. Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55\u201369. Springer, 1998. 321 609. Doina Precup, Richard S Sutton, and Satinder P Singh. Planning with closed-loop macro actions. In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems, pages 70\u201376, 1997. 230, 241 610. David Premack and Guy Woodruf. Does the chimpanzee have a theory of mind? Behavioral and Brain Sciences, 1(4):515\u2013526, 1978. 208 611. Hugo M Proen\u00e7a and Matthijs van Leeuwen. Interpretable multiclass classifcation by mdl-based rule lists. Information Sciences, 512:1372\u20131393, 2020. 278 612. Max Pumperla and Kevin Ferguson. Deep Learning and the Game of Go. Manning, 2019. 187 613. J Ross Quinlan. Learning efcient classifcation procedures and their application to chess end games. In Machine Learning, pages 463\u2013482. Springer, 1983. 162 References 365 614. J Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81\u2013106, 1986. 278 615. S\u00e9bastien Racani\u00e8re, Theophane Weber, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri\u00e0 Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter W. Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 5690\u20135701, 2017. 57, 138, 139 616. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 276, 277, 320 617. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://openai.com/blog/languageunsupervised/, 2018. 245, 262, 263, 320 618. Roxana R\u0103dulescu, Patrick Mannion, Diederik M Roijers, and Ann Now\u00e9. Multi-objective multi-agent decision making: a utility-based analysis and survey. Autonomous Agents and Multi-Agent Systems, 34(1):1\u201352, 2020. 198 619. Jacob Rafati and David C Noelle. Learning representations in model-free hierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 33, pages 10009\u201310010, 2019. 229, 231, 237, 241 620. Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? Towards understanding the efectiveness of maml. In International Conference on Learning Representations, 2020. 258 621. Roberta Raileanu and Tim Rockt\u00e4schel. RIDE: rewarding impact-driven exploration for procedurally-generated environments. In International Conference on Learning Representations, 2020. 177, 234 622. Rajat Raina, Andrew Y Ng, and Daphne Koller. Constructing informative priors using transfer learning. In Proceedings of the 23rd international conference on Machine learning, pages 713\u2013720, 2006. 268 623. Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems, 2019. 254, 268 624. Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efcient of-policy meta-reinforcement learning via probabilistic context variables. In International Conference on Machine Learning, pages 5331\u20135340. PMLR, 2019. 265 625. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. 276, 320 626. Jette Randlov. Learning macro-actions in reinforcement learning. In Advances in Neural Information Processing Systems, pages 1045\u20131051, 1998. 241 627. J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https:// GitHub.com/FacebookResearch/Nevergrad, 2018. 259 628. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4295\u20134304. PMLR, 2018. 207 629. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017. 254, 255, 268 630. John R. Rice. The algorithm selection problem. Advances in Computers, 15(65-118):5, 1976. 259 631. Martin Riedmiller. Neural ftted Q iteration\u2014frst experiences with a data efcient neural reinforcement learning method. In European Conference on Machine Learning, pages 317\u2013328. Springer, 2005. 74, 87 632. Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527\u2013535, 1952. 47 366 References 633. Frank R\u00f6der, Manfred Eppe, Phuong DH Nguyen, and Stefan Wermter. Curious hierarchical actor-critic reinforcement learning. In International Conference on Artifcial Neural Networks, pages 408\u2013419. Springer, 2020. 235, 236, 276 634. Diederik M Roijers, Willem R\u00f6pke, Ann Now\u00e9, and Roxana R\u0103dulescu. On following paretooptimal policies in multi-objective planning and reinforcement learning. In Multi-Objective Decision Making Workshop, 2021. 198 635. Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-objective sequential decision-making. Journal of Artifcial Intelligence Research, 48:67\u2013113, 2013. 198 636. Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In International Conference on Machine Learning, pages 2152\u20132161, 2015. 268, 269, 276 637. Willem R\u00f6pke, Roxana Radulescu, Diederik M Roijers, and Ann Ann Now\u00e9. Communication strategies in multi-objective normal-form games. In Adaptive and Learning Agents Workshop 2021, 2021. 198 638. Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artifcial Intelligence, 61(3):203\u2013230, 2011. 168, 169 639. Denis Rothman. Transformers for Natural Language Processing. Packt Publishing, 2021. 263 640. Neil Rubens, Mehdi Elahi, Masashi Sugiyama, and Dain Kaplan. Active learning in recommender systems. In Recommender Systems Handbook, pages 809\u2013846. Springer, 2015. 177 641. Jonathan Rubin and Ian Watson. Computer poker: A review. Artifcial intelligence, 175(56):958\u2013987, 2011. 223 642. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016. 85 643. Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206\u2013215, 2019. 278 644. Ben Ruijl, Jos Vermaseren, Aske Plaat, and Jaap van den Herik. Hepgame and the simplifcation of expressions. arXiv preprint arXiv:1405.6369, 2014. 188 645. Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems. Technical report, University of Cambridge, Department of Engineering Cambridge, UK, 1994. 45, 50, 59 646. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. 245, 262 647. Stuart J Russell and Peter Norvig. Artifcial intelligence: a modern approach. Pearson Education Limited, Malaysia, 2016. 14, 18, 56, 57, 59 648. Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. Found. Trends Mach. Learn., 11(1):1\u201396, 2018. 48 649. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference on Learning Representations, 2019. 268 650. Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic defnitions and new directions. Contemporary Educational Psychology, 25(1):54\u201367, 2000. 241 651. Jordi Sabater and Carles Sierra. Reputation and social network analysis in multi-agent systems. In Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 1, pages 475\u2013482, 2002. 216 652. Sumit Saha. A comprehensive guide to convolutional neural networks\u2014the ELI5 way. https://towardsdatascience.com/a-comprehensive-guide-to-convolutionalneural-networks-the-eli5-way-3bd2b1164a53. Towards Data Science, 2018. 305, 312 653. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017. 203, 212, 223, 277 654. Brian Sallans and Geofrey E Hinton. Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5:1063\u20131088, Aug 2004. 74, 75, 86 References 367 655. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201919, Montreal, 2019. 207, 219, 223, 224 656. Jason Sanders and Edward Kandrot. CUDA by example: an introduction to general-purpose GPU programming. Addison-Wesley Professional, 2010. 324 657. Tuomas Sandholm. The state of solving large incomplete-information games, and application to poker. AI Magazine, 31(4):13\u201332, 2010. 223 658. Tuomas Sandholm. Abstraction for solving large incomplete-information games. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 29, 2015. 204 659. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, pages 1842\u20131850, 2016. 268 660. Vieri Giuliano Santucci, Pierre-Yves Oudeyer, Andrew Barto, and Gianluca Baldassarre. Intrinsically motivated open-ended learning in autonomous robots. Frontiers in Neurorobotics, 13:115, 2020. 277 661. Steve Schaefer. Mathematical recreations. http://www.mathrec.org/old/2002jan/ solutions.html, 2002. 151 662. Jonathan Schaefer. One Jump Ahead: Computer Perfection at Checkers. Springer Science & Business Media, 2008. 64 663. Jonathan Schaefer, Robert Lake, Paul Lu, and Martin Bryant. Chinook, the world manmachine checkers champion. AI Magazine, 17(1):21, 1996. 57 664. Jonathan Schaefer, Aske Plaat, and Andreas Junghanns. Unifying single-agent and twoplayer search. Information Sciences, 135(3-4):151\u2013175, 2001. 188 665. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pages 1312\u20131320, 2015. 230, 241 666. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016. 80, 82 667. Tom Schaul and J\u00fcrgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010. 247, 267, 268 668. Daniel Schleich, Tobias Klamt, and Sven Behnke. Value iteration networks on multiple levels of abstraction. In Robotics: Science and Systems XV, University of Freiburg, Freiburg im Breisgau, Germany, 2019. 135 669. J\u00fcrgen Schmidhuber. Evolutionary Principles in Self-Referential Learning, or on Learning how to Learn: the Meta-Meta-. . . Hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987. 253, 268 670. J\u00fcrgen Schmidhuber. Making the world diferentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Technical report, Inst. f\u00fcr Informatik, 1990. 132 671. J\u00fcrgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In 1990 IJCNN International Joint Conference on Neural Networks, pages 253\u2013258. IEEE, 1990. 135, 139, 144 672. J\u00fcrgen Schmidhuber. Curious model-building control systems. In Proceedings International Joint Conference on Neural Networks, pages 1458\u20131463, 1991. 175 673. J\u00fcrgen Schmidhuber. Learning to generate sub-goals for action sequences. In Artifcial neural networks, pages 967\u2013972, 1991. 227 674. J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222\u2013227, 1991. 277 675. J\u00fcrgen Schmidhuber, F Gers, and Douglas Eck. Learning nonregular languages: A comparison of simple recurrent networks and LSTM. Neural Computation, 14(9):2039\u20132041, 2002. 315 676. J\u00fcrgen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple principles of metalearning. Technical report, IDSIA, 1996. 268 368 References 677. Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Kernel principal component analysis. In International Conference on Artifcial Neural Networks, pages 583\u2013588. Springer, 1997. 15 678. Nicol N Schraudolph, Peter Dayan, and Terrence J Sejnowski. Temporal diference learning of position evaluation in the game of Go. In Advances in Neural Information Processing Systems, pages 817\u2013824, 1994. 75 679. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering Atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020. 138, 140, 144, 275 680. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and ofine reinforcement learning by planning with a learned model. arXiv preprint arXiv:2104.06294, 2021. 141, 275 681. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015. 95, 104, 105, 116, 258 682. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations, 2016. 112 683. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 95, 106, 116, 272 684. Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural Networks, 16(1):5\u20139, 2003. 267, 268 685. Marco Scutari. Learning Bayesian networks with the bnlearn R package. Journal of Statistical Software, 35(i03), 2010. 278 686. Thomas D Seeley. The honey bee colony as a superorganism. American Scientist, 77(6):546\u2013 553, 1989. 191 687. Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks and symbolic AI. Nature, 555(7698):604, 2018. 188, 275 688. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, 2020. 130, 138, 140, 144 689. Oliver G Selfridge, Richard S Sutton, and Andrew G Barto. Training and tracking in robotics. In International Joint Conference on Artifcial Intelligence, pages 670\u2013672, 1985. 175 690. Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Z\u00eddek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using potentials from deep learning. Nature, 577(7792):706\u2013710, 2020. 188 691. Burr Settles. Active learning literature survey. Technical report, University of WisconsinMadison Department of Computer Sciences, 2009. 177 692. Noor Shaker, Julian Togelius, and Mark J Nelson. Procedural Content Generation in Games. Springer, 2016. 59, 176, 265 693. Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based POMDP solvers. Autonomous Agents and Multi-Agent Systems, 27(1):1\u201351, 2013. 151 694. Claude E Shannon. Programming a computer for playing chess. In Computer Chess Compendium, pages 2\u201313. Springer, 1988. 7, 152 695. Lloyd S Shapley. Stochastic games. In Proceedings of the National Academy of Sciences, volume 39, pages 1095\u20131100, 1953. 194 696. Yaron Shoham and Gal Elidan. Solving Sokoban with forward-backward reinforcement learning. In Proceedings of the International Symposium on Combinatorial Search, volume 12, pages 191\u2013193, 2021. 242 697. Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press, 2008. 193 References 369 698. Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a critical survey. Technical report, Stanford University, 2003. 222 699. Pranav Shyam, Shubham Gupta, and Ambedkar Dukkipati. Attentive recurrent comparators. In International Conference on Machine Learning, pages 3173\u20133181. PMLR, 2017. 268 700. Robin C Sickles and Valentin Zelenyuk. Measurement of productivity and efciency. Cambridge University Press, 2019. 197 701. Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond learning algorithms. In 2013 AAAI Spring Symposium Series, 2013. 246 702. David Silver. Reinforcement learning and simulation based search in the game of Go. PhD thesis, University of Alberta, 2009. 167 703. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016. 57, 150, 172, 174, 178, 179, 182, 187, 246 704. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419):1140\u20131144, 2018. 144, 178, 181, 182, 183, 187, 275 705. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning, pages 387\u2013395, 2014. 108, 132 706. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017. 75, 144, 150, 169, 171, 172, 173, 174, 178, 179, 180, 181, 182, 183, 187, 246, 275 707. David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artifcial Intelligence, page 103535, 2021. 17, 217 708. David Silver, Richard S Sutton, and Martin M\u00fcller. Reinforcement learning of local shape in the game of Go. In International Joint Conference on Artifcial Intelligence, volume 7, pages 1053\u20131058, 2007. 187 709. David Silver, Richard S Sutton, and Martin M\u00fcller. Temporal-diference search in computer Go. Machine Learning, 87(2):183\u2013219, 2012. 131 710. David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris. The predictron: End-to-end learning and planning. In Proceedings of the 34th International Conference on Machine Learning, pages 3191\u20133199, 2017. 130, 135, 136, 138, 144 711. David Sim\u00f5es, Nuno Lau, and Lu\u00eds Paulo Reis. Multi agent deep learning with cooperative communication. Journal of Artifcial Intelligence and Soft Computing Research, 10, 2020. 208 712. Satinder Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated reinforcement learning. Technical report, University of Amherst, Mass, Department of Computer Science, 2005. 237, 238, 277 713. Satinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010. 223 714. David J Slate and Lawrence R Atkin. Chess 4.5\u2014the northwestern university chess program. In Chess skill in Man and Machine, pages 82\u2013118. Springer, 1983. 163 715. Gillian Smith. An analog history of procedural content generation. In Foundations of Digital Games, 2015. 177 716. Stephen J Smith, Dana Nau, and Tom Throop. Computer bridge: A big win for AI planning. AI magazine, 19(2):93\u201393, 1998. 208 ",
    "Abstract": "",
    "Background": "Background Fig. A.4 Graphical illustration of REINFORCE estimator. Left: Example distribution \ud835\udc5d\ud835\udf03 (\ud835\udc65) and function \ud835\udc53 (\ud835\udc65). When we evaluate the expectation of Eq. A.11, we take \ud835\udc5a samples, indicated by the blue dots (in this case \ud835\udc5a = 8). The magnitude of \ud835\udc53 (\ud835\udc65) is shown with the red vertical arrows. Right: When we apply the gradient update, each sample pushes up the density at that location, but the magnitude of the push is multiplied by \ud835\udc53 (\ud835\udc65). Therefore, the higher \ud835\udc53 (\ud835\udc65), the harder we push. Since a density needs to integrate to 1, we will increase the density where we push hardest (in the example on the rightmost sample). The distribution will therefore shift to the right on this update. discrete action space, we write the expectations as summations: \ud835\udc49(\ud835\udc60) = \u2211\ufe01 \ud835\udc4e\u2208\ud835\udc34 \ud835\udf0b(\ud835\udc4e|\ud835\udc60) \ufffd \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \ud835\udc49(\ud835\udc60\u2032) \ufffd\ufffd \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \u2211\ufe01 \ud835\udc60\u2032\u2208\ud835\udc46 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \u2211\ufe01 \ud835\udc4e\u2208\ud835\udc34 \ud835\udf0b(\ud835\udc4e|\ud835\udc60)[\ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032)] \ufffd For continuous state and action spaces, the summations over policy and transition are replaced by integration: \ud835\udc49(\ud835\udc60) = \u222b \ud835\udc4e \ud835\udf0b(\ud835\udc4e|\ud835\udc60) \ufffd \u222b \ud835\udc60\u2032 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \ud835\udc49(\ud835\udc60\u2032) \ufffd d\ud835\udc60\u2032\ufffd d\ud835\udc4e The same principle applies to the Bellman equation for state-action values: \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \u222b \ud835\udc60\u2032 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \u222b \ud835\udc4e\u2032[\ud835\udf0b(\ud835\udc4e\u2032|\ud835\udc60\u2032) \u00b7 \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) \ufffd d\ud835\udc4e\u2032\ufffd d\ud835\udc60\u2032 We may also have a continuous state space (such as visual input) with a discrete action space (such as pressing buttons in a game): \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \u222b \ud835\udc60\u2032 \ud835\udc47\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) \ufffd \ud835\udc5f\ud835\udc4e(\ud835\udc60, \ud835\udc60\u2032) + \ud835\udefe \u00b7 \u2211\ufe01 \ud835\udc4e\u2032 [\ud835\udf0b(\ud835\udc4e\u2032|\ud835\udc60\u2032) \u00b7 \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) \ufffd\ufffd d\ud835\udc60\u2032 Appendix B Deep Supervised Learning This appendix provides a chapter-length overview of essentials of machine learning and deep learning. Deep reinforcement learning uses much of the machinery of deep supervised learning, and a good understanding of deep supervised learning is essential. This appendix should provide you with the basics, in case your knowledge of basic machine learning and deep learning is rusty. In doubt? Try to answer the questions on page 328. We will start with machine learning basics. We will discuss training and testing, accuracy, the confusion matrix, generalization, and overftting. Next, we will provide an overview of deep learning. We will look into neural networks, error functions, training by gradient descent, end-to-end feature learning, and the curse of dimensionality. For neural networks we will discuss convolutional networks, recurrent networks, LSTM, and measures against overftting. Finally, on the practical side, we will discuss TensorFlow, Keras, and PyTorch. We will start with methods for solving large and complex problems. B.1 Machine Learning The goal of machine learning is generalization: to create an accurate, predictive, model of the world. Such an accurate model is said to generalize well to the world.1 In machine learning we operationalize this goal with two datasets, a training set and a test set. The feld of machine learning aims to ft a function to approximate an input/output relation. This can be, for example, a regression function or a classifcation function. The most basic form of machine learning is when a dataset \ud835\udc37 of input/output pairs is given. We call this form supervised learning, since the learning process is supervised by the output values. 1 Generalization is closely related to the concepts of overftting, regularization, and smoothness, as we will see in Sect. B.1.3. 297 298 B Deep Supervised Learning Predicted class P N Actual P TP FN class N FP TN Predicted class Cat Dog Actual Cat 122 8 class Dog 3 9 Table B.1 Confusion Matrix In machine learning we often deal with large problem domains. We are interested in a function that works not just on the particular values on which it was trained, but also in the rest of the problem domain from which the data items were taken. In this section we will frst see how such a learning process for generalization works; for notation and examples we follow from [524]. Next, we will discuss the specifc problems of large domains. Finally, we will discuss the phenomenon of overftting and how it relates to the bias-variance trade-of. B.1.1 Training Set and Test Set A machine learning algorithm must learn a function \u02c6\ud835\udc53 (\ud835\udc65) \u2192 \ud835\udc66 on a training set from input values \ud835\udc65 (such as images) to approximate corresponding output values \ud835\udc66 (such as image labels). The goal of the machine learning algorithm is to learn this function \u02c6\ud835\udc53 , such that it performs well on the training data and generalizes well to the test data. Let us see how we can measure how well the machine learning methods generalize. The notions that are used to assess the quality of the approximation are as follows. Let us assume that our problem is a classifcation task. The elements that our function correctly predicts are called the true positives (TP). Elements that are correctly identifed as not belonging to a class are the true negatives (TN). Elements that are mis-identifed as belonging to a class are the false positives (FP), and elements that belong to the class but that the predictor misclassifes are the false negatives (FN). Confusion Matrix: The number of true positives divided by the total number of positives (true and false, TP/(TP+FP)) is called the precision of the classifer. The number of true positives divided by the size of the class (TP/(TP+FN)) is the recall, or the number of relevant elements that the classifer could fnd. The term accuracy is defned as the total number of true positives and negatives divided by the total number of predictions (TP+TN)/(TP+TN+FP+FN): how well correct elements are predicted [93]. These numbers are often shown in the form of a confusion matrix (Table B.1). The numbers in the cells represent the number of true positives, etc., of the experiment. B.1 Machine Learning 299 In machine learning, we are often interested in the accuracy of a method on the training set, and whether the accuracy on the test set is the same as on the training set (generalization). In regression and classifcation problems the term error value is used to indicate the diference between the true output value \ud835\udc66 and the predicted output value \u02c6\ud835\udc53 (\ud835\udc65). Measuring how well a function generalizes is typically performed using a method called \ud835\udc58-fold cross-validation. This works as follows. When a dataset of input-output examples (\ud835\udc65, \ud835\udc66) is present, this set is split into a large training set and a smaller hold-out test set, typically 80/20 or 90/10. The approximator is trained on the training set until a certain level of accuracy is achieved. For example, the approximator may be a neural network whose parameters \ud835\udf03 are iteratively adjusted by gradient descent so that the error value on the training set is reduced to a suitably low value. The approximator function is said to generalize well, when the accuracy of the approximator is about the same on the test set as it is on the training set. (Sometimes a third dataset is used, the validation set, to allow stopping before overftting occurs, see Sect. B.2.7.) Since the test set and the training set contain examples that are drawn from the same original dataset, approximators can be expected to be able to generalize well. The testing is said to be in-distribution when training and test set are from the same distribution. Out-of-distribution generalization to diferent problems (or transfer learning) is more difcult, see Sect. 9.2.1. B.1.2 Curse of Dimensionality The state space of a problem is the space of all possible diferent states (combinations of values of variables). State spaces grow exponentially with the number of dimensions (variables); high-dimensional problems have large state spaces, and modern machine learning algorithms have to be able to learn functions in such large state spaces. Example: A classic problem in AI is image classifcation: predicting what type of object is pictured in an image. Imagine that we have low-resolution greyscale images of 100\u00d7100 pixels, where each pixel takes a discrete value between 0 and 255 (a byte). Then, the input space \ud835\udc4b \u2208 {0, 1, . . . , 255}100\u00b7100, the machine learning problem has dimensionality 100 \u00b7 100 = 10000, and the state space has size 25610000. When the input space \ud835\udc4b is high dimensional, we can never store the entire state space (all possible pixels with all possible values) as a table. The efect of an exponential need for observation data as the dimensionality grows, has been called the curse of dimensionality by Richard Bellman [72]. The curse of dimensionality states that the cardinality (number of unique points) of a space scales exponentially in the dimensionality of the problem. In a formula, we have that 300 B Deep Supervised Learning |\ud835\udc4b| \u223c exp(Dim(\ud835\udc4b)). Due to the curse of dimensionality, the size of a table to represent a function increases quickly when the size of the input space increases. Example: Imagine we have a discrete input space \ud835\udc4b = {0, 1}\ud835\udc37 that maps to a real number, \ud835\udc4c = R, and we want to store this function fully as a table. We will show the required size of the table and the required memory when we use 32-bit (4 byte) foating point numbers: Dim(\ud835\udc4b) |\ud835\udc4b| Memory 1 2 8 Byte 5 25 = 32 128 Byte 10 210 = 1024 4KB 20 220 \u2248 106 4MB 50 250 \u2248 1015 4.5 million TB 100 2100 \u2248 1030 5 \u00b7 1021 TB 265 2265 \u2248 2 \u00b7 1080 The table shows how quickly exponential growth develops. At a discrete space of size 20, it appears that we are doing alright, storing 4 Megabyte of information. However, at a size of 50, we suddenly need to store 4.5 million Terabyte. We can hardly imagine the numbers that follow. At an input dimensionality of size 265, our table would have grown to size 2\u00b71080, which is close to the estimated number of atoms in the universe. Since the size of the state space grows exponentially, but the number of observations typically does not, most state spaces in large machine learning problems are sparsely populated with observations. An important challenge for machine learning algorithms is to ft good predictive models on sparse data. To reliably estimate a function, each variable needs a certain number of observations. The number of samples that are needed to maintain statistical signifcance increases exponentially as the number of dimensions grows. A large amount of training data is required to ensure that there are several samples for each combination of values;2 however, datasets rarely grow exponentially. B.1.3 Overftting and the Bias-Variance Trade-Of Basic statistics tells us that, according to the law of large numbers, the more observations we have of an experiment, the more reliable the estimate of their value is (that is, the average will be close to the expected value) [93]. This has important 2 Unless we introduce some bias into the problem, by assuming smoothness, implying that there are dependencies between variables, and that the \u201ctrue\u201d number of independent variables is smaller than the number of pixels. B.1 Machine Learning 301 Fig. B.1 Curve ftting: does the curvy red line or the straight dashed blue line best generalize the information in the data points? implications for the study of large problems where we would like to have confdence in the estimated values of our parameters. In small toy problems the number of variables of the model is also small. Singlevariable linear regression problems model the function as a straight line \ud835\udc66 = \ud835\udc4e \u00b7\ud835\udc65 +\ud835\udc4f, with only one independent variable \ud835\udc65 and two parameters \ud835\udc4e and \ud835\udc4f. Typically, when regression is performed with a small number of independent variables, then a relatively large number of observations is available per variable, giving confdence in the estimated parameter values. A rule of thumb is that there should be 5 or more training examples for each variable [769]. In machine learning dimensionality typically means the number of variables of a model. In statistics, dimensionality can be a relative concept: the ratio of the number of variables compared to the number of observations. In practice, the size of the observation dataset is limited, and then absolute and relative dimensionality do not difer much. In this book we follow the machine learning defnition of dimensions meaning variables. Modeling high-dimensional problems well typically requires a model with many parameters, the so-called high-capacity models. Let us look deeper into the consequences of working with high-dimensional problems. To see how to best ft our model, let us consider machine learning as a curve ftting problem, see Fig. B.1. In many problems, the observations are measurements of an underlying natural process. The observations therefore contain some measurement noise. The goal is (1) that the approximating curve fts the (noisy) observations as accurately as possible, but, (2) in order to generalize well, to aim to ft the signal, not the noise. How complex should the approximator be to faithfully capture the essence of a natural, noisy, process? You can think of this question as: how many parameters \ud835\udf03 the network should have, or, if the approximator is a polynomial, what the degree 302 B Deep Supervised Learning Environment Model Relative Chance of Observations Variables Dimensionality \ud835\udc5b > \ud835\udc51 low underftting, high bias \ud835\udc5b < \ud835\udc51 high overftting, high variance Table B.2 Observations, Variables, Relative Dimensionality, Overftting Fig. B.2 Bias-variance trade-of; few-parameter model: high bias; many-parameter model: high variance \ud835\udc51 of the polynomial should be? The complexity of the approximator is also called the capacity of the model, for the amount of information that it can contain (see Table B.2). When the capacity of the approximator \ud835\udc51 is lower than the number of observations \ud835\udc5b, then the model is likely to underft: the curve is too simple and cannot reach all observations well, the error is large and the accuracy is low.3 Conversely, when the number of coefcients \ud835\udc51 is as high or higher than the number of observations \ud835\udc5b, then many machine learning procedures wil be able to fnd a good ft. The error on the training set will be zero, and training accuracy reaches 100%. Will this high-capacity approximator generalize well to unseen states? Most likely it will not, since the training and test observations are from a real-world process and observations contain noise from the training set. The training noise will be modeled perfectly, and the trained function will have low accuracy on the test set and other datasets. A high-capacity model (\ud835\udc51 > \ud835\udc5b) that trains well but tests poorly is said to overft on the training data. Underftting and overftting are related to the so-called bias-variance trade-of, see Fig. B.2. High-capacity models ft \u201cpeaky\u201d high-variance curves that can ft signal and noise of the observations, and tend to overft. Low-capacity models ft \u201cstraighter\u201d low-variance curves that have higher bias and tend to underft the observations. Models with a capacity of \ud835\udc51 \u2248 \ud835\udc5b can have both good bias and variance [93]. 3 Overftting can be reduced in the loss function and training procedure, see Sect. B.2.7. B.2 Deep Learning 303 Fig. B.3 A Single Biological neuron [821] Preventing underftting and overftting is a matter of matching the capacity of our model so that \ud835\udc51 matches \ud835\udc5b. This is a delicate trade-of, since reducing the capacity also reduces expressive power of the models. To reduce overftting, we can use regularization, and many regularization methods have been devised. Regularization has the efect of dynamically adjusting capacity to the number of observations. Regularization\u2014the World is Smooth Generalization is closely related to regularization. Most functions in the real world\u2014 the functions that we wish to learn in machine learning\u2014are smooth: near similar input leads to near similar output. Few real-world functions are jagged, or locally random (when they are, they are often contrived examples). This has lead to the introduction of regularization methods, to restrict or smooth the behavior of models. Regularization methods allow us to use high capacity models, learn the complex function, and then later introduce a smoothing procedure, to reduce too much of the randomness and jaggedness. Regularization may for example restrict the weights of variables by moving the values closer to zero, but many other methods exist, as we will see. Diferent techniques for regularization have been developed for high-capacity deep neural models, and we discuss them in Sect. B.2.7. The goal is to flter out random noise, but at the same time to allow meaningful trends to be recognized, to smooth the function without restricting complex shapes [280]. Before we delve into regularization methods, let us have a look in more detail at how to implement parameterized functions, for which we will use neural networks and deep learning. B.2 Deep Learning The architecture of artifcial neural networks is inspired by the architecture of biological neural networks, such as the human brain. Neural networks consist of 304 B Deep Supervised Learning \ud835\udc651 \ud835\udc652 \ud835\udc661 \ud835\udc662 Fig. B.4 A Fully Connected Shallow Neural Network with 9 Neurons neural core cells that are connected by nerve cells [63]. Figure B.3 shows a drawing of a biological neuron, with a nucleus, an axon, and dendrites [821]. Figure B.4 shows a simple fully connected artifcial neural network, with an input layer of two neurons, an output layer of two neurons, and a single hidden layer of fve neurons. A neural network with a single hidden layer is called shallow. When the network has more hidden layers it is called deep (Fig. B.5). In this section, we provide an overview of artifcial neural networks and their training algorithms. We provide enough detail to understand the deep reinforcement learning concepts in this book. Space does provide a limit to how deep we can go. Please refer to specialized deep learning literature for more information, such as [280]. We provide a conceptual overview, which should be enough to successfully use existing high quality deep learning packages, such as TensorFlowa [1] or PyTorch [591].b a https://www.tensorflow.org b https://pytorch.org B.2.1 Weights, Neurons Neural networks consist of neurons and connections, typically organized in layers. The neurons process their input signals as a weighted combination, producing an output signal. This calculation is called the activation function, or squashing function, since it is non-linear. Popular activation functions are the rectifed linear unit (ReLU: partly linear, partly zero), the hyperbolic tangent, and the sigmoid or logistic function 1 1+\ud835\udc52\u2212\ud835\udc4e for neuron activation \ud835\udc4e. The neurons are connected B.2 Deep Learning 305 Fig. B.5 Four fully connected hidden layers [652] by weights. At each neuron \ud835\udc57 the incoming weights \ud835\udc56\ud835\udc57 are summed \ufffd and then processed by the activation function \ud835\udf0e. The output \ud835\udc5c of neuron \ud835\udc57 is therefore: \ud835\udc5c \ud835\udc57 = \ud835\udf0e( \u2211\ufe01 \ud835\udc56 \ud835\udc5c\ud835\udc56\ud835\udc64\ud835\udc56 \ud835\udc57) for weight \ud835\udc56\ud835\udc57 of predecessor neuron \ud835\udc5c\ud835\udc56. The outputs of this layer of neurons are fed to the inputs for the weights of the next layer. B.2.2 Backpropagation The neural network as a whole is a parameterized function \ud835\udc53\ud835\udf03 (\ud835\udc65) \u2192 \u02c6\ud835\udc66 that converts input to an output approximation. The behavior depends on the parameters \ud835\udf03, also known as the network weights. The parameters are adjusted such that the required input-output relation is achieved (training the network). This is done by minimizing the error (or loss) function that calculates the diference between the network output \u02c6\ud835\udc66 and the training target \ud835\udc66. The training process consists of training epochs, individual passes in which the network weights are optimized towards the target, using a method called gradient descent (since the goal is to minimize the error function). An epoch is one complete pass over the training data. Epochs are usually done in batches. Since training is an iterative optimization process, it is typical to train for multiple epochs. Listing B.1 shows simplifed pseudocode for the gradient descent training algorithm (based on [280]). When training starts, the weights of the network are initialized to 306 B Deep Supervised Learning 1 def train_sl(data , net , alpha =0.001): # train classifier 2 for epoch in range(max_epochs ): # an epoch is one pass 3 sum_sq = 0 # reset to zero for each pass 4 for (image , label) in data: 5 output = net. forward_pass (image) # predict 6 sum_sq += (output - label)**2 # compute error 7 grad = net.gradient(sum_sq) # derivative of error 8 net. backward_pass (grad , alpha) # adjust weights 9 return net Listing B.1 Network training pseudocode for supervised learning small random numbers. Each epoch consists of a forward pass (recognition, usage) and a backward pass (training, adjustment). The forward pass is just the regular recognition operation for which the network is designed. The input layer is exposed to the input (the image), which is then propagated through the network to the output layers, using the weights and activation functions. The output layer provides the answer, by having a high value at the neuron corresponding to the right label (such as Cat or Dog, or the correct number), so that an error can be calculated to be used to adjust the weights in the backward pass. The listing shows a basic version of gradient descent, that calculates the gradient over all examples in the dataset, and then updates the weights. Batch versions of gradient descent update the weights after smaller subsets, and are typically quicker. Loss Function At the output layer the propagated value \u02c6\ud835\udc66 is compared with the other part of the example pair, the label \ud835\udc66. The diference with the label is calculated, yielding the error. The error function is also known as the loss function L. Two common error functions are the mean squared error 1 \ud835\udc5b \ufffd\ud835\udc5b \ud835\udc56 (\ud835\udc66\ud835\udc56 \u2212 \u02c6\ud835\udc66\ud835\udc56)2 (for regression) and the cross-entropy error \u2212 \ufffd\ud835\udc40 \ud835\udc56 \ud835\udc66\ud835\udc56 log \u02c6\ud835\udc66\ud835\udc56 (for classifcation of \ud835\udc40 classes). The backward pass uses the diference between the forward recognition outcome and the true label to adjust the weights, so that the error becomes smaller. This method uses the gradient of the error function over the weights, and is called gradient descent. The parameters are adjusted as follows: \ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udefc\u2207\ud835\udf03\ud835\udc61 L\ud835\udc37( \ud835\udc53\ud835\udf03\ud835\udc61) where \ud835\udf03 are the network parameters, \ud835\udc61 is the optimization time step, \ud835\udefc is the learning rate, \u2207\ud835\udf03\ud835\udc61 are the current gradient of the loss function of data L\ud835\udc37, and \ud835\udc53\ud835\udf03 is the parameterized objective function. The training process can be stopped when the error has been reduced below a certain threshold for a single example, or when the loss on an entire validation set B.2 Deep Learning 307 has dropped sufciently. More elaborate stopping criteria can be used in relation to overftting (see Sect. B.2.7). Most neural nets are trained using a stochastic version of gradient descent, or SGD [723]. SGD samples a minibatch of size smaller than the total dataset, and thereby computes a noisy estimate of the true gradient. This is faster per update step, and does not afect the direction of the gradient too much. See Goodfellow et al. [280] for details. B.2.3 End-to-end Feature Learning Let us now look in more detail at how neural networks can be used to implement end-to-end feature learning. We can approximate a function through the discovery of common features in states. Let us, again, concentrate on image recognition. Traditionally, feature discovery was a manual process. Image-specialists would painstakingly pour over images to identify common features in a dataset, such as lines, squares, circles, and angles, by hand. They would write small pre-processing algorithms to recognize the features that were then used with classical machine learning methods such as decision trees, support vector machines, or principal component analysis to construct recognizers to classify an image. This hand-crafted method is a laborintensive and error prone process, and researchers have worked to fnd algorithms for the full image recognition process, end-to-end. For this to work, also the features must be learned. For example, if the function approximator consists of the sum of \ud835\udc5b features, then with hand-crafted features, only the coefcients \ud835\udc50\ud835\udc56 of the features in the function \u210e(\ud835\udc60) = \ud835\udc501 \u00d7 \ud835\udc531(\ud835\udc60) + \ud835\udc502 \u00d7 \ud835\udc532(\ud835\udc60) + \ud835\udc503 \u00d7 \ud835\udc533(\ud835\udc60) + . . . + \ud835\udc50\ud835\udc5b \u00d7 \ud835\udc53\ud835\udc5b(\ud835\udc60) are learned. In end-to-end learning, the coefcients \ud835\udc50\ud835\udc56 and the features \ud835\udc53\ud835\udc56(\ud835\udc60) are learned. Deep end-to-end learning has achieved great success in image recognition, speech recognition, and natural language processing [431, 287, 853, 193]. Endto-end learning is the learning of a classifer directly from high-dimensional, raw, un-pre-processed, pixel data, all the way to the classifcation layer, as opposed to learning pre-processed data from intermediate (lower dimensional) hand-crafted features. We will now see in more detail how neural networks can perform automated feature discovery. The hierarchy of network layers together can recognize a hierarchy of low-to-high level concepts [459, 462]. For example, in face recognition (Fig. B.6) the frst hidden layer may encode edges; the second layer then composes and encodes simple structures of edges; the third layer may encode higher-level concepts such as noses or eyes; and the fourth layer may work at the abstraction level of a face. Deep feature learning fnds what to abstract at which level on its 308 B Deep Supervised Learning Fig. B.6 Layers of features of increasing complexity [462] Function Input Output Dataset Regression continuous (number) continuous (number) Classifcation discrete (image) discrete (class label) Environment Value \ud835\udc49 state continuous (number) Action-value \ud835\udc44 state \u00d7 action continuous (number) Policy \ud835\udf0b state action(-distribution) Table B.3 Functions that are Frequently Approximated own [76], and can come up with classes of intermediate concepts, that work, but look counterintuitive upon inspection by humans. Towards the end of the 1990s the work on neural networks moved into deep learning, a term coined by Dechter in [187]. LeCun et al. [461] published an infuential paper on deep convolutional nets. The paper introduced the architecture LeNet-5, a seven-layer convolutional neural net trained to classify handwritten MNIST digits from 32 \u00d7 32 pixel images. Listing B.2 shows a modern rendering of LeNet in Keras. The code straightforwardly lists the layer defnitions. End-to-end learning is computationally quite demanding. After the turn of the century, methods, datasets, and compute power had improved to such an extent that full raw, un-pre-processed pictures could be learned, without the intermediate step of hand-crafting features. End-to-end learning proved very powerful, achieving higher accuracy in image recognition than previous methods, and even higher than human test subjects [431]. In natural language processing, deep transformer models such as BERT and GPT-2 and 3 have reached equally impressive results [193, 114]. B.2 Deep Learning 309 1 2 def lenet_model(img_shape =(28 , 28, 1), n_classes =10, l2_reg =0., 3 weights=None): 4 5 # Initialize model 6 lenet = Sequential () 7 8 # 2 sets of CRP (Convolution , RELU , Pooling) 9 lenet.add(Conv2D (20, (5, 5), padding=\"same\", 10 input_shape =img_shape , kernel_regularizer =l2( l2_reg))) 11 lenet.add(Activation (\"relu\")) 12 lenet.add( MaxPooling2D (pool_size =(2, 2), strides =(2, 2))) 13 14 lenet.add(Conv2D (50, (5, 5), padding=\"same\", 15 kernel_regularizer =l2(l2_reg))) 16 lenet.add(Activation (\"relu\")) 17 lenet.add( MaxPooling2D (pool_size =(2, 2), strides =(2, 2))) 18 19 # Fully connected layers (w/ RELU) 20 lenet.add(Flatten ()) 21 lenet.add(Dense (500 , kernel_regularizer =l2(l2_reg))) 22 lenet.add(Activation (\"relu\")) 23 24 # Softmax (for classification ) 25 lenet.add(Dense(n_classes , kernel_regularizer =l2(l2_reg)) ) 26 lenet.add(Activation (\"softmax\")) 27 28 if weights is not None: 29 lenet. load_weights (weights) 30 31 # Return the constructed network 32 return lenet Listing B.2 LeNet-5 code in Keras [461, 832] Function Approximation Let us have a look at the diferent kinds of functions that we wish to approximate in machine learning. The most basic function establishing an input/output relation is regression, which outputs a continuous number. Another important function is classifcation, which outputs a discrete number. Regression and classifcation are often learned through supervision, with a dataset of examples (observations) and labels. In reinforcement learning, three functions are typically approximated: the value function \ud835\udc49(\ud835\udc60), that relates states to their expected cumulative future rewards, the action-value function \ud835\udc44(\ud835\udc60, \ud835\udc4e) that relate actions to their values, and the policy 310 B Deep Supervised Learning function \ud835\udf0b(\ud835\udc60) that relates states to an action (or \ud835\udf0b(\ud835\udc4e|\ud835\udc60) to an action distribution).4 In reinforcement learning, the functions \ud835\udc49, \ud835\udc44, \ud835\udf0b are learned through reinforcement by the environment. Table B.3 summarizes these functions. B.2.4 Convolutional Networks The frst neural networks consisted of fully connected layers (Fig. B.5). In image recognition, the input layer of a neural network is typically connected directly to the input image. Higher resolution images therefore need a higher number of input neurons. If all layers would have more neurons, then the width of the network grows quickly. Unfortunately, growing a fully connected network (see Fig. B.4) by increasing its width (number of neurons per layer) will increase the number of parameters quadratically. The naive solution of high-resolution problem learning is to increase the capacity of the model \ud835\udc5a. However, because of the problem of overftting, as \ud835\udc5a grows, so must the number of examples, \ud835\udc5b. The solution lies in using a sparse interconnection structure instead of a fully connected network. Convolutional neural nets (CNNs) take their inspiration from biology. The visual cortex in animals and humans is not fully connected, but locally connected [357, 358, 503]. Convolutions efciently exploit prior knowledge about the structure of the data: patterns reoccur at diferent locations in the data (translation invariance), and therefore we can share parameters by moving a convolutional window over the image. A CNN consists of convolutional operators or flters. A typical convolution operator has a small receptive feld (it only connects to a limited number of neurons, say 5 \u00d7 5), whereas a fully connected neuron connects to all neurons in the layer below. Convolutional flters detect the presence of local patterns. The next layer thus acts as a feature map. A CNN layer can be seen as a set of learnable flters, invariant for local transformations [280]. Filters can be used to identify features. Features are basic elements such as edges, straight lines, round lines, curves, and colors. To work as a curve detector, for example, the flter should have a pixel structure with high values indicating a shape of a curve. By then multiplying and adding these flter values with the pixel values, we can detect whether the shape is present. The sum of the multiplications in the input image will be large if there is a shape that resembles the curve in the flter. This flter can only detect a certain shape of curve. Other flters can detect other shapes. Larger activation maps can recognize more elements in the input image. Adding more flters increases the size of the network, which efectively enlarges the activation map. The flters in the frst network layer process (\u201cconvolve\u201d) the input image and fre (have high values) when a specifc feature that it is built to 4 In Chap. 5, on model-based learning, we also approximate the transition function \ud835\udc47\ud835\udc4e (\u00b7) and the reward function \ud835\udc45\ud835\udc4e (\u00b7). B.2 Deep Learning 311 detect is in the input image. Training a convolutional net is training a flter that consists of layers of subflters. By going through the convolutional layers of the network, increasingly complex features can be represented in the activation maps. Once they are trained, they can be used for as many recognition tasks as needed. A recognition task consists of a single quick forward pass through the network. Let us spend some more time on understanding these flters. Shared Weights In CNNs the flter parameters are shared in a layer. Each layer thus defnes a flter operation. A flter is defned by few parameters but is applied to many pixels of the image; each flter is replicated across the entire visual feld. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specifc response feld. Replicating units in this way allows for features to be detected regardless of their position in the visual feld, thus constituting the property of translation invariance. This weight sharing is also important to prevent an increase in the number of weights in deep and wide nets, and to prevent overftting, as we shall see later. Real-world images consist of repetitions of many smaller elements. Due to this so-called translation invariance, the same patterns reappear throughout an image. CNNs can take advantage of this. The weights of the links are shared, resulting in a large reduction in the number of weights that have to be trained. Mathematically CNNs put constraints on what the weight values can be. This is a signifcant advantage of CNNs, since the computational requirements of training the weights of fully connected layers are prohibitive. In addition, statistical strength is gained, since the efective data per weight increases. Deep CNNs work well in image recognition tasks, for visual fltering operations in spatial dependencies, and for feature recognition (edges, shapes) [460].5 CNN Architecture Convolutions recognize features\u2014the deeper the network, the more complex the features. A typical CNN architecture consists of a number of stacked convolutional layers. In the fnal layers, fully connected layers are used to then classify the inputs. In the convolutional layers, by connecting only locally, the number of weights is dramatically reduced in comparison with a fully connected net. The ability of a single neuron to recognize diferent features, however, is less than that of a fully connected neuron. 5 Interestingly, this paper was already published in 1989. The deep learning revolution happened twenty years later, when publicly available datasets, more efcient algorithms, and more compute power in the form of GPUs were available. 312 B Deep Supervised Learning Fig. B.7 Convolutional network example architecture [652] Fig. B.8 Max and average 2 \u00d7 2 pooling [280] By stacking many such locally connected layers on top of each other we can achieve the desired nonlinear flters whose joint efect becomes increasingly global.6 The neurons become responsive to a larger region of pixel space, so that the network frst creates representations of small parts of the input, and from these representations create larger areas. The network can recognize and represent increasingly complex concepts without an explosion of weights. A typical CNN architecture consists of an architecture of multiple layers of convolution, max pooling, and ReLU layers, topped of by a fully connected layer (Fig. B.7).7 Max Pooling A further method for reducing the number of weights is weight pooling. Pooling is a kind of nonlinear downsampling (expressing the information in lower resolution 6 Nonlinearity is essential. If all neurons performed linearly, then there would be no need for layers. Linear recognition functions cannot discriminate between cats and dogs. 7 Often with a softmax function. The softmax function normalizes an input vector of real numbers to a probability distribution [0, 1]; \ud835\udc5d\ud835\udf03 (\ud835\udc66|\ud835\udc65) = softmax( \ud835\udc53\ud835\udf03 (\ud835\udc65)) = \ud835\udc52 \ud835\udc53\ud835\udf03 (\ud835\udc65) \ufffd \ud835\udc58 \ud835\udc52 \ud835\udc53\ud835\udf03,\ud835\udc58 (\ud835\udc65) B.2 Deep Learning 313 Fig. B.9 RNN \ud835\udc65\ud835\udc61 is an input vector, \u210e\ud835\udc61 is the output/prediction, and \ud835\udc34 is the RNN [567] Fig. B.10 RNN unrolled in time [567] with fewer bits). Typically, a 2 \u00d7 2 block is sampled down to a scalar value (Fig. B.8). Pooling reduces the dimension of the network. The most frequently used form is max pooling. It is an important component for object detection [153] and is an integral part of most CNN architectures. Max pooling also allows small translations, such as shifting the object by a few pixels, or scaling, such as putting the object closer to the camera. B.2.5 Recurrent Networks Image recognition has had a large impact on network architectures, leading to innovations such as convolutional nets for spatial data. Speech recognition and time series analysis have also caused new architectures to be created, for sequential data. Such sequences can be modeled by recurrent neural nets (RNN) [84, 240]. Some of the better known RNNs are Hopfeld networks [349], and long short-term memory (LSTM) [345]. Figure B.9 shows a basic recurrent neural network. An RNN neuron is the same as a normal neuron, with input, output, and activation function. However, RNN neurons have an extra pair of looping input/output connections. Through this structure, the values of the parameters in an RNN can evolve. In efect, RNNs have a variable-like state. To understand how RNNs work, it helps to unroll the network, as has been done in Fig. B.10. The recurrent neuron loops have been drawn as a straight line to show 314 B Deep Supervised Learning Fig. B.11 RNN confgurations [399] the network in a deeply layered style, with connections between the layers. In reality the layers are time steps in the processing of the recurrent connections. In a sense, an RNN is a deeply layered neural net folded into a single layer of recurrent neurons. Where deep convolutional networks are successful in image classifcation, RNNs are used for tasks with a sequential nature, such as captioning challenges. In a captioning task the network is shown a picture, and then has to come up with a textual description that makes sense [816]. The main innovation of recurrent nets is that they allow us to work with sequences of vectors. Figure B.11 shows diferent combinations of sequences that we will discuss now, following an accessible and well-illustrated blog on the diferent RNN confgurations written by Karpathy [399]. There can be sequences in the input, in the output, or in both. The fgure shows diferent rectangles. Each rectangle is a vector. Arrows represent computations, such as matrix multiply. Input vectors are in red, output vectors are in blue, and green vectors hold the state. From left to right we see: 1. One to one, the standard network without RNN. This network maps a fxed-sized input to fxed-sized output, such as an image classifcation task (picture in/class out). 2. One to many adds a sequence in the output. This can be an image captioning task that takes an image and outputs a sentence of words. 3. Many to one is the opposite, with a sequence in the input. Think for example of sentiment analysis (a sentence is classifed for words with negative or positive emotional meaning). 4. Many to many has both a sequence for input and a sequence for output. This can be the case in machine translation, where a sentence in English is read and then a sentence in Fran\u00e7ais is produced. 5. Many to many is a related but diferent situation, with synchronized input and output sequences. This can be the case in video classifcation where each frame of the video should be labeled. B.2 Deep Learning 315 Fig. B.12 LSTM [399] Long Short-Term Memory Time series prediction is more complex than conventional regression or classifcation. It adds the complexity of a sequence dependence among the input variables. LSTM (long short-term memory) is a more powerful type of neuron designed to handle sequences. Figure B.12 shows the LSTM module, allowing comparison with a simple RNN. LSTMs are designed for sequential problems, such as time series, and planning. LSTMs were introduces by Hochreiter and Schmidhuber [345]. RNN training sufers from the vanishing gradient problem. For short-term sequences this problem may be controllable by the same methods as for deep CNN [280]. For long-term remembering LSTM are better suited. LSTMs are frequently used to solve diverse problems [675, 286, 288, 271], and we encounter them at many places throughout this book. B.2.6 More Network Architectures Deep learning is a highly active feld of research, in which many advanced network architectures have been developed. We will describe some of the better known architectures. Residual Networks An important innovation on CNNs is the residual network architecture, or ResNets. This idea was introduced in the 2015 ImageNet challenge, which He et al. [321] won with a very low error rate of 3.57%. This error rate is actually lower than what most humans achieve: 5\u201310%. ResNet has no fewer than 152 layers. Residual nets introduce skip links. Skip links are connections skipping one or more layers, allowing the training to go directly to other layers, reducing the efective depth of the network (Fig. B.13). Skip links create a mixture of a shallow and a deep network, preventing the accuracy degradation and vanishing gradients of deep networks [280]. 316 B Deep Supervised Learning Fig. B.13 Residual Net with Skip Links [321] Generative Adversarial Networks Normally neural networks are used in forward mode, to discriminate input images into classes, going from high dimensional to low dimensional. Networks can also be run backwards, to generate an image that goes with a certain class, going from low dimensional to high dimensional.8 Going from small to large implies many possibilities for the image to be instantiated. Extra input is needed to fll in the degrees of freedom. Running the recognizers backwards, in generative mode, has created an active research area called deep generative modeling. An important type of generative model that has made quite an impact is the Generative adversarial network, or GAN [281]. Deep networks are susceptible to adversarial attacks. A well-known problem of the image recognition process is that it is brittle. It was found that if an image is slightly perturbed, and imperceptibly to the human eye, deep networks can easily be fooled to characterize an image as the wrong category [747]. This brittleness is known as the one-pixel problem: changing a single unimportant pixel in an image could cause classifers to switch from classifying an image from cat to dog [730, 747]. GANs are used to generate input images that are slightly diferent from the original input. GANs generate adversarial examples whose purpose it is to fool the discriminator (recognizer). The frst network, the generator, generates an image. The second network, the discriminator, tries to recognize the image. The goal for the generator is to mislead the discriminator, in order to improve the robustness of the discriminator. In this way, GANs can be used to make image recognition more robust. The one-pixel problem has spawned an active area of research to understand this problem, and to make deep networks more robust. Another use of generative networks is to generate artifcial photo-realistic images known as deep fake images [857] and deep dreaming [402], see Fig. B.14 and 8 Just like the decoding phase of autoencoders. B.2 Deep Learning 317 Fig. B.14 Deep fakes [857] Fig. B.15.9 GANs have signifcantly increased our theoretical understanding of deep learning. Autoencoders Autoencoders and variational autoencoders (VAE) are used in deep learning for dimensionality reduction, in unsupervised learning [428, 411, 412]. An autoencoder network has a butterfy-like architecture, with the same number of neurons in the input and the output layers, but a decreasing layer-size as we go to the center (Fig. B.16). The input (contracting) side is said to perform a discriminative action, such as image classifcation, and the other (expanding) side is generative [281]. When an image is fed to both the input and the output of the autoencoder, results in the center layers are exposed to a compression/decompression process, resulting in the same image, only smoothed. The discriminative/generative process performed by autoencoders reduces the dimensionality, and is generally thought of as going to the \u201cessence\u201d of a problem, de-noising it [342]. 9 Deep Dream Generator at https://deepdreamgenerator.com 318 B Deep Supervised Learning Fig. B.15 Deep dream [402] Fig. B.16 Autoencoder, fnding the \u201cessence\u201d of the data in the middle [538] Autoencoding illustrates a deep relation between supervised and unsupervised learning. The architecture of an autoencoders consists of an encoder and a decoder. The encoder is a regular discriminative network, as is common in supervised learning. The decoder is a generative network, creating high dimensional output from low dimensional input. Together, the encoder/decoder butterfy perform dimensionality reduction, or compression, a form of unsupervised learning[294]. Autoencoders and generative networks are an active area of research. B.2 Deep Learning 319 Fig. B.17 Attention Architecture [47, 490] Fig. B.18 Example for Attention Mechanism [490] Attention Mechanism Another important architecture is the attention mechanism. Sequence to sequence learning occurs in many applications of machine learning, for example, in machine translation. The dimensionality of the output of basic RNNs is the same as the input. The attention architecture allows a fexible mapping between input and output dimensionality [739]. It does so by augmenting the RNNs with an extra network that focuses attention over the sequence of encoder RNN states [47], see Fig. B.17. The extra network focuses perception and memory access. It has been shown to achieve state of the art results in machine translation and natural language tasks [47], see Fig. B.18. The attention mechanism is especially useful for time series forecasting and translation. 320 B Deep Supervised Learning Transformers Finally, we discuss the transformer architecture. The transformer architecture is introduced by Vaswani et al. [805]. Transformers are based on the concept of selfattention: they use attention encoder-decoder models, but weigh the infuence of diferent parts of the data. They are central to the highly successful BERT [193] and GPT-3 [617, 114] natural language models, but have also been successful in imaginative text-to-image creation. Transformer-based foundation models play an important role in multi-modal (text/image) learning [625, 616], such as the one that we saw in Fig. 10.1. Treating deep reinforcement learning as a sequence problem, transformers are also being applied to reinforcement learning, with some early success [140, 377]. Transformers are an active area of research. For a detailed explanation of how they work, see, for example [12, 94, 805]. B.2.7 Overftting Now that we have discussed network architectures, it is time to discuss an important problem of deep neural networks: overftting, and how we can reduce it. Overftting in neural networks can be reduced in a number of ways. Some of the methods are aimed at restoring the balance between the number of network parameters and the number of training examples, others on data augmentation and capacity reduction. Another approach is to look at the training process. Let us list the most popular approaches [280]. \u2022 Data Augmentation Overftting occurs when there are more parameters in the network than examples to train on. The training dataset is increased through manipulations such as rotations, refections, noise, rescaling, etc. A disadvantage of this method is that the computational cost of training increases. \u2022 Capacity Reduction Another solution to overftting lies in the realization that overftting is a result of the network having too large a capacity; the network has too many parameters. A cheap way of preventing this situation is to reduce the capacity of the network, by reducing the width and depth of the network. \u2022 Dropout A popular method to reduce overftting is to introduce dropout layers into the networks. Dropout reduces the efective capacity of the network by stochastically dropping a certain percentage of neurons from the backpropagation process [344, 725]. Dropout is an efective and computationally efcient method to reduce overftting [280]. \u2022 L1 and L2 Regularization Regularization involves adding an extra term to the loss function that forces the network to not be too complex. The term penalizes the model for using too high weight values. This limits fexibility, but also encourages building solutions based on multiple features. Two popular versions of this method are L1 and L2 regularization [559, 280]. B.3 Datasets and Software 321 \u2022 Early Stopping Early stopping is based on the observation that overftting can be regarded as a consequence of so-called overtraining (training that progresses beyond the signal, into the noise). By terminating the training process earlier, for example by using a higher stopping threshold for the error function, we can prevent overftting from occurring [130, 607, 608]. A convenient and popular way is to add a third set to the training set/test set duo which then becomes a training set, a test set, and a holdout validation set. After each training epoch, the network is evaluated against the holdout validation set, to see if under- or overftting occurs, and if we should stop training. In this way, overftting can be prevented dynamically during training [608, 93, 280]. \u2022 Batch Normalization Another method is batch normalization. Batch normalization periodically normalizes the input to the layers [370]. This has many benefts, including a reduction of overftting. Overftting and regularization are an important topic of research. In fact, a basic question is why large neural networks perform so well at all. The network capacity is in the millions to billions of parameters, much larger than the number of observations, yet networks perform well. There appears to be a regime, beyond where performance sufers due to overftting, where performance increases as we continue to increase the capacity of our networks [68, 66]. Belkin et al. have performed studies on interpolation in SGD. Their studies suggests an implicit regularization regime of many parameters beyond overftting, where SGD generalizes well to test data, explaining in part the good results in practice of deep learning. Nakkiran et al. report similar experimental results, termed the double descent phenomenon [550, 392]. Research into the nature of overftting is active [68, 491, 66, 67, 867]. B.3 Datasets and Software We have discussed in depth background concepts in machine learning, and important aspects of the theory of neural networks. It is time to turn our attention to practical matters. Here we encounter a rich feld of data, environments, software and blogs on how to use deep learning in practice. The feld of deep reinforcement learning is an open feld. Researchers release their algorithms and code allowing replication of results. Datasets and trained networks are shared. The barrier to entry is low: high-quality software is available on GitHub for you to download and start doing research in the feld. We point to code bases and open software suites at GitHub throughout the book, Appendix C has pointers to software environments and open-source code frameworks. The most popular deep learning packages are PyTorch [591] and TensorFlow [1], and its top-level language Keras [147]. Some machine learning and mathematical packages also ofer deep learning tools, such as scikit-learn, MATLAB and R. In this section we will start with some easy classifcation and behavior examples, using the Keras library. We will use Python as our programming language. Python has become the language of choice for machine learning packages; not just for 322 B Deep Supervised Learning Fig. B.19 Some MNIST images [461] PyTorch and TensorFlow, but also for numpy, scipy, scikit-learn, matplotlib, and many other mature and high-quality machine learning libraries. If Python is not present on your computer, or if you want to download a newer version, please go to https://www.python.org to install it. Note that due to some unfortunate version issues of the Stable Baselines and TensorFlow, you may have to install diferent versions of Python to get these examples to work, and to use virtual environments to manage your software versions. Since deep learning is very computationally intensive, these packages typically support GPU parallelism, which can speedup your training tenfold or more, if you have the right GPU card in your system. Cloud providers of computing, such as AWS, Google, Azure, and others, also typically provide modern GPU hardware to support machine learning, often with student discounts. B.3.1 MNIST and ImageNet One of the most important elements for the success in image recognition was the availability of good datasets. In the early days of deep learning, the feld benefted greatly from eforts in handwriting recognition. This application was of great value to the postal service, where accurate recognition of handwritten zip codes or postal codes allowed great improvements to efcient sorting and delivery of the mail. A standard test set for handwriting recognition was MNIST (for Modifed National Institute of Standards and Technology) [461]. Standard MNIST images are low-resolution 32 \u00d7 32 pixel images of single handwritten digits (Fig. B.19). Of course, researchers wanted to process more complex scenes than single digits, and higher-resolution images. To achieve higher accuracy, and to process more complex scenes, networks (and datasets) needed to grow in size and complexity. B.3 Datasets and Software 323 ImageNet A major dataset in deep learning is ImageNet [237, 191]. It is a collection of more than 14 million URLs of images that have been hand annotated with the objects that are in the picture. It contains more than 20,000 categories. A typical category contains several hundred training images. The importance of ImageNet for the progress in deep learning is large. The availability of an accepted standard set of labeled images allowed learning algorithms to be tested and improved, and new algorithms to be created. ImageNet was conceived by Fei-Fei Li et al. in 2006, and in later years she developed it further with her group. Since 2010 an annual software contest has been organized, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [191]. Since 2012 ILSVRC has been won by deep networks, starting the deep learning boom. The network architecture that won this challenge in that year has become known as AlexNet, after one of its authors [431]. The 2012 ImageNet database as used by AlexNet has 14 million labeled images. The network featured a highly optimized 2D two-GPU implementation of 5 convolutional layers and 3 fully connected layers. The flters in the convolutional layers are 11\u00d711 in size. The neurons use a ReLU activation function. In AlexNet images were scaled to 256 \u00d7 256 RGB pixels. The size of the network was large, with 60 million parameters. This causes considerable overftting. AlexNet used data augmentation and dropouts to reduce the impact of overftting. Krizhevsky et al. won the 2012 ImageNet competition with an error rate of 15%, signifcantly better than the number two, who achieved 26%. Although there were earlier reports of CNNs that were successful in applications such as bioinformatics and Chinese handwriting recognition, it was this win of the 2012 ImageNet competition for which AlexNet has become well known. B.3.2 GPU Implementations The deep learning breakthrough around 2012 was caused by the co-occurrence of three major developments: (1) algorithmic advances that solved key problems in deep learning, (2) the availability of large datasets of labeled training data, and (3) the availability of computational power in the form of graphical processing units, GPUs. The most expensive operations in image processing and neural network training are operations on matrices. Matrix operations are some of the most well-studied problems in computer science. Their algorithmic structure is well understood, and for basic linear algebra operations high-performance parallel implementations for CPU exist, such as the BLAS [202, 145]. GPUs were originally designed for smooth graphics performance in video games. Graphical processing requires fast linear algebra computations such as matrix multiply. These are precisely the kind of operations that are at the core of deep 324 B Deep Supervised Learning Fig. B.20 SIMD: Connection Machine 1 and GPU learning training algorithms. Modern GPUs consist of thousands of small arithmetic units that are capable of performing linear algebra matrix operations very fast in parallel. This kind of data parallel processing is based on SIMD computing, for singleinstruction-multiple-data [249, 330]. SIMD data parallism goes back to designs from 1960s and 1970s vector supercomputers such as the Connection Machine series from Thinking Machines [339, 340, 466]. Figure B.20 shows a picture of the historic CM-1, and of a modern GPU. Modern GPUs consist of thousands of processing units optimized to process linear algebra matrix operations in parallel [656, 486, 721], ofering matrix performance that is orders of magnitude faster than CPUs [566, 152, 746]. B.3.3 Hands On: Classifcation Example It is high time to try out some of the material in practice. Let us see if we can do some image recognition ourselves. Installing TensorFlow and Keras We will frst install TensorFlow. It is possible to run TensorFlow in the cloud, in Colab, or in a Docker container. Links to ready to run Colab environments are on the TensorFlow website. We will, however, asssume a traditional local installation on your own computer. All major operating systems are supported: Linux/Ubuntu, macOS, Windows. The programming model of TensorFlow is complex, and not very user friendly. Fortunately, an easy to use language has been built on top of TensorFlow: Keras. B.3 Datasets and Software 325 The Keras language is easy to use, well-documented, and many examples exist to get you started. When you install TensorFlow, Keras is installed automatically as well. To install TensorFlow and Keras go to the TensorFlow page on https://www. tensorflow.org. It is recommended to make a virtual environment to isolate the package installation from the rest of your system. This is achieved by typing python3 -m venv --system-site-packages ./venv (or equivalent) to create the virtual environment. Using the virtual environment requires activation:10 source ./venv/bin/activate You will most likely run into version issues when installing packages. Note that for deep reinforcement learning we will be making extensive use of reinforcement learning agent algorithms from the so-called Stable Baselines.11 The stable baselines work with version 1 of TensorFlow and with PyTorch, but, as of this writing, not with version 2 of TensorFlow. TensorFlow version 1.14 has been tested to work.12 Installing is easy with Python\u2019s pip package manager: just type pip install tensorflow==1.14 or, for GPU support, pip install tensorflow-gpu==1.14 This should now download and install TensorFlow and Keras. We will check if everything is working by executing the MNIST training example with the default training dataset. 10 More installation guidance can be found on the TensorFlow page at https://www.tensorflow. org/install/pip. 11 https://github.com/hill-a/stable-baselines 12 You may be surprised that so many version numbers were mentioned. Unfortunately not all versions are compatible; some care is necessary to get the software to work: Python 3.7.9, TensorFlow 1.14.0, Stable Baselines 2, pyglet 1.5.11 worked at the time of writing. This slightly embarrassing situation is because the feld of deep reinforcement learning is driven by a community of researchers, who collaborate to make code bases work. When new insights trigger a rewrite that loses backward compatibility, as happened with TensorFlow 2.0, frantic rewriting of dependent software occurs. The feld is still a new feld, and some instability of software packages will remain with us for the foreseeable future. 326 B Deep Supervised Learning 1 from tensorflow.keras.models import Sequential 2 from tensorflow.keras.layers import Dense 3 4 model = Sequential () 5 6 model.add(Dense(units =64, activation =\u2019relu \u2019)) 7 model.add(Dense(units =10, activation =\u2019softmax \u2019)) 8 9 model.compile(loss=\u2019categorical_crossentropy \u2019, 10 optimizer=\u2019sgd \u2019, 11 metrics =[\u2019accuracy \u2019]) 12 13 # x_train and y_train are Numpy arrays --just like in the Scikit Learn API. 14 model.fit(x_train , y_train , epochs =5, batch_size =32) 15 16 # evaluation on test data is simple 17 loss_and_metrics = model.evaluate(x_test , y_test , batch_size =128) 18 19 # as is predicting output 20 classes = model.predict(x_test , batch_size =128) Listing B.3 Sequential Model in Keras Keras MNIST Example Keras is built on top of TensorFlow, and is installed with TensorFlow. Basic Keras mirrors the familiar Scikit-learn interface [595]. Each Keras program specifes a model that is to be learned. The model is the neural network, that consists of weights and layers (of neurons). You will specify the architecure of the model in Keras, and then ft the model on the training data. When the model is trained, you can evaluate the loss function on test data, or perform predictions of the outcome based on some test input example. Keras has two main programming paradigms: sequential and functional. Listing B.3 shows the most basic Sequential Keras model, from the Keras documentation, with a two-layer model, a ReLU layer, and a softmax layer, using simple SGD for backpropagation. The Sequential model in Keras has an object-oriented syntax. The Keras documentation is at https://keras.io/getting_started/intro_ to_keras_for_researchers/. It is quite accessible, and you are encouraged to learn Keras by working through the online tutorials. A slightly more useful example is ftting a model on MNIST, see Listing B.4. This example uses a more fexible Keras syntax, the functional API, in which transformations are chained on top of the previous layers. This example of Keras code loads MNIST images in a training set and a test set, and creates a model of dense ReLU layers using the Functional API. It then creates a Keras model of these layers, and prints a summary of the model. Then the model is trained from numpy data, with B.3 Datasets and Software 327 1 # Get the data as Numpy arrays 2 (x_train , y_train), (x_test , y_test) = keras.datasets.mnist. load_data () 3 4 # Build a simple model 5 inputs = keras.Input(shape =(28 , 28)) 6 x = layers. experimental . preprocessing .Rescaling (1.0 / 255)(inputs ) 7 x = layers.Flatten ()(x) 8 x = layers.Dense (128 , activation=\"relu\")(x) 9 x = layers.Dense (128 , activation=\"relu\")(x) 10 outputs = layers.Dense (10, activation =\"softmax\")(x) 11 model = keras.Model(inputs , outputs) 12 model.summary () 13 14 # Compile the model 15 model.compile(optimizer=\"adam\", loss=\" sparse_categorical_crossentropy \") 16 17 # Train the model for 1 epoch from Numpy data 18 batch_size = 64 19 print(\"Fit\u2423on\u2423NumPy\u2423data\") 20 history = model.fit(x_train , y_train , batch_size =batch_size , epochs =1) 21 22 # Train the model for 1 epoch using a dataset 23 dataset = tf.data.Dataset. from_tensor_slices (( x_train , y_train)). batch(batch_size) 24 print(\"Fit\u2423on\u2423Dataset\") 25 history = model.fit(dataset , epochs =1) Listing B.4 Functional MNIST Model in Keras the ft function, for a single epoch, and also from a dataset, and the loss history is printed. The code of the example shows how close Keras is to the way in which we think and reason about neural networks. The examples only show the briefest of glimpses to what is possible in Keras. Keras has options for performance monitoring, for checkpointing of long training runs, and for interfacing with TensorBoard, to visualize the training process. TensorBoard is an indispensible tool, allowing you to debug your intuition of what should be going on in your network, and what is going on. Deep reinforcement learning is still very much a feld with more degrees of freedom in experimentation than established practices, and being able to plot the progress of training processes is essential for a better understanding of how the model behaves. The more you explore Keras, the better you will be able to progress in deep reinforcement learning [270]. 328 B Deep Supervised Learning Exercises Below are some questions to check your understanding of deep learning. Each question is a closed question where a simple, single sentence answer is expected. Questions 1. Datasets are often split into two sub-datasets in machine learning. Which are those two, and why are they split? 2. What is generalization? 3. Sometimes a third sub-dataset is used. What is it called and what is it used for? 4. If we consider observations and model parameters, when is a problem highdimensional and when is it low-dimensional? 5. How do we measure the capacity of a machine learning model? 6. What is a danger with low capacity models? 7. What is a danger with high capacity models? 8. What is the efect of overftting on generalization? 9. What is the diference between supervised learning and reinforcement learning? 10. What is the diference between a shallow network and a deep network? 11. Supervised learning has a model and a dataset, reinforcement learning has which two central concepts? 12. What phases does a learning epoch have? What happens in each phase? 13. Name three factors that were essential for the deep learning breakthrough, and why. 14. What is end-to-end learning? Do you know an alternative? What are the advantages of each? 15. What is underftting, and what causes it? What is overftting, and what causes it? How can you see if you have overftting? 16. Name three ways to prevent overftting. 17. Which three types of layers does a neural network have? 18. How many hidden layers does a shallow neural network have? 19. Describe how adjusting weights works in a neural network. Hint: think of a examples, labels, forward phase, a backward phase, error functions, gradients. 20. What is the diference between a fully connected network and a convolutional neural network? 21. What is max pooling? 22. Why are shared weights advantageous? 23. What is feature learning? 24. What is representation learning? 25. What is deep learning? 26. What is an advantage of convolutional neural networks of fully connected neural networks? 27. Name two well-known image recognition data sets. B.3 Datasets and Software 329 Exercises Let us now start with some exercises. If you have not done so already, install PyTorch13 or TensorFlow and Keras (see Sect. 2.2.4.1 or go to the TensorFlow page).14 Be sure to check the right versions of Python, TensorFlow, and the Stable Baselines to make sure they work well together. The exercises below are meant to be done in Keras. 1. Generalization Install Keras. Go to the Keras MNIST example. Perform a classifcation task. Note how many epochs the training takes, and in testing, how well it generalizes. Perform the classifcation on a smaller training set, how does learning rate change, how does generalization change. Vary other elements: try a diferent optimizer than adam, try a diferent learning rate, try a diferent (deeper) architecture, try wider hidden layers. Does it learn faster? Does it generalize better? 2. Overftting Use Keras again, but this time on ImageNet. Now try diferent overftting solutions. Does the training speed change? Does generalization change? Now try the hold-out validation set. Do training and generalization change? 3. Confdence How many runs did you do in the previous exercises, just a single run to see how long training took and how well generalization worked? Try to run it again. Do you get the same results? How large is the diference? Can you change the random seeds of Keras or TensorFlow? Can you calculate the confdence interval, how much does the confdence improve when you do 10 randomized runs? How about 100 runs? Make graphs with error bars. 4. GPU It might be that you have access to a GPU machine that is capable of running PyTorch or TensorFlow in parallel to speed up the training. Install the GPU version and check that it recognizes the GPU and is indeed using it. 5. Parallelism It might be that you have access to a multicore CPU machine. When you are running multiple runs in order to improve confedence, then an easy way to speed up your experiment is to spawn multiple jobs at the shell, assigning the output to diferent log fles, and write a script to combine results and draw graphs. Write the scripts necessary to achieve this, test them, and do a large-confdence experiment. 13 https://pytorch.org 14 https://www.tensorflow.org Appendix C Deep Reinforcement Learning Suites Deep reinforcement learning is a highly active feld of research. One reason for the progress is the availability of high quality algorithms and code: high quality environments, algorithms and deep learning suites are all being made available by researchers along with their research papers. This appendix provides pointers to these codes. 331 332 C Deep Reinforcement Learning Suites C.1 Environments Progress has benefted greatly from the availability of high quality environments, on which the algorithms can be tested. We provide pointers to some of the environments (Table C.1). Environment Type URL Ref Gym ALE, MuJoCo https://gym.openai.com [108] ALE Atari https://github.com/mgbellemare/ArcadeLearning-Environment [71] MuJoCo Simulated Robot http://www.mujoco.org [780] DeepMind Lab 3D navigation https://github.com/deepmind/lab [64] Control Suite Physics tasks https://github.com/deepmind/dm_control [755] Behavior Suite Core RL https://github.com/deepmind/bsuite [575] StarCraft Python interface https://github.com/deepmind/pysc2 [815] Omniglot Images https://github.com/brendenlake/omniglot [445] Mini ImageNet Images https://github.com/yaoyao-liu/mini-imagenettools [814] ProcGen Procedurally Gen. https://openai.com/blog/procgen-benchmark/ [158] OpenSpiel Board Games https://github.com/deepmind/open_spiel [450] RLlib Scalable RL https://github.com/ray-project/ray [478] Meta Dataset dataset of datas. https://github.com/google-research/metadataset [785] Meta World meta RL https://meta-world.github.io [864] Alchemy meta RL https://github.com/deepmind/dm_alchemy [825] Garage reproducible RL https://github.com/rlworkgroup/garage [261] Football multi agent https://github.com/google-research/football [438] Emergence Hide and Seek https://github.com/openai/multi-agentemergence-environments [49] Unplugged Ofine RL https://github.com/deepmind/deepmindresearch/tree/master/rl_unplugged [299] Unity 3D https://github.com/Unity-Technologies/mlagents [383] PolyGames board games https://github.com/teytaud/Polygames [133] Dopamine RL framework https://github.com/google/dopamine [131] Table C.1 Reinforcement Learning Environments C.2 Agent Algorithms 333 C.2 Agent Algorithms For value-based and policy-based methods most mainstream algorithms have been collected and are freely available. For two-agent, multi-agent, hierarchical, and meta learning, the agent algorithms are also on GitHub, but not always in the same place as the basic algorithms. Table C.2 provides pointers. Repo URL Algorithms Ref Spinning Up https://spinningup.openai.com Tutorial on DDPG, PPO, etc OpenAI Baselines https://github.com/openai/ baselines DQN, PPO, etc [194] Stable Basel. https://stable-baselines. readthedocs.io/en/master/ Refactored baselines [338] PlaNet https://planetrl.github.io Latent Model [309] Dreamer https://github.com/danijar/ dreamerv2 Latent Model [310] VPN https://github.com/junhyukoh/ value-prediction-network Value Prediction Network [564] MuZero https://github.com/kaesve/muzero Reimplementation of MuZero [186] MCTS pip install mcts MCTS [117] AlphaZ. Gen https://github.com/suragnair/ alpha-zero-general AZ in Python [768] ELF https://github.com/ facebookresearch/ELF Framework for Game research [776] PolyGames https://github.com/teytaud/ Polygames Zero Learning [133] CFR https://github.com/bakanaouji/ cpp-cfr CFR [881] DeepCFR https://github.com/ EricSteinberger/Deep-CFR Deep CFR [726] MADDPG https://github.com/openai/maddpg Multi-agent DDPG [489] PBT https://github.com/voiler/ PopulationBasedTraining Population Based Training [374] Go-Explore https://github.com/uberresearch/go-explore Go Explore [222] MAML https://github.com/cbfinn/maml MAML [243] Table C.2 Agent Algorithms 334 C Deep Reinforcement Learning Suites C.3 Deep Learning Suites The two most well-known deep learning suites are TensorFlow and PyTorch. BaseTensorFlow has a complicated programming model. Keras has been developed as an easy to use layer on top of TensorFlow. When you use TensorFlow, start with Keras. Or use PyTorch. TensorFlow and Keras are at https://www.tensorflow.org. PyTorch is at https://pytorch.org. References 1. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jefrey Dean, Matthieu Devin, Sanjay Ghemawat, Geofrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorfow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265\u2013283, 2016. 304, 321 2. Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608\u20131639, 2010. 6 3. Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of reinforcement learning to aerobatic helicopter fight. In Advances in Neural Information Processing Systems, pages 1\u20138, 2007. 5, 6 4. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. 170 5. Abhishek. Multi-arm bandits: a potential alternative to a/b tests https://medium.com/ brillio-data-science/multi-arm-bandits-a-potential-alternative-to-a-btests-a647d9bf2a7e, 2019. 48 6. Bruce Abramson. Expected-outcome: A general model of static evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(2):182\u2013193, 1990. 163 7. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. arXiv preprint arXiv:2108.13264, 2021. 115 8. Pulkit Agrawal, Ross Girshick, and Jitendra Malik. Analyzing the performance of multilayer neural networks for object recognition. In European Conference on Computer Vision, pages 329\u2013344. Springer, 2014. 245 9. Sanjeevan Ahilan and Peter Dayan. Feudal multi-agent hierarchies for cooperative reinforcement learning. arXiv preprint arXiv:1901.08492, 2019. 233 10. Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-based classifcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 819\u2013826, 2013. 261, 268, 269, 276 11. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2623\u20132631, 2019. 259 12. Jay Alammer. The illustrated transformer. https://jalammar.github.io/illustratedtransformer/. 320 335 336 References 13. Stefano Albrecht and Peter Stone. Multiagent learning: foundations and recent trends. In Tutorial at IJCAI-17 conference, 2017. 222 14. Stefano Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artifcial Intelligence, 258:66\u201395, 2018. 208, 222 15. Ethem Alpaydin. Introduction to Machine Learning. MIT press, 2009. 40, 45 16. Safa Alver. The option-critic architecture. https://alversafa.github.io/blog/2018/ 11/28/optncrtc.html, 2018. 229 17. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016. 268 18. Ankesh Anand, Jacob Walker, Yazhe Li, Eszter V\u00e9rtes, Julian Schrittwieser, Sherjil Ozair, Th\u00e9ophane Weber, and Jessica B Hamrick. Procedural generalization by planning with self-supervised world models. arXiv preprint arXiv:2111.01587, 2021. 136, 141 19. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048\u20135058, 2017. 231, 233, 241 20. Anonymous. Go AI strength vs. time. Reddit post, 2017. 154 21. Thomas Anthony, Tom Eccles, Andrea Tacchetti, J\u00e1nos Kram\u00e1r, Ian M. Gemp, Thomas C. Hudson, Nicolas Porcel, Marc Lanctot, Julien P\u00e9rolat, Richard Everett, Satinder Singh, Thore Graepel, and Yoram Bachrach. Learning to play no-press diplomacy with best response policy iteration. In Advances in Neural Information Processing Systems, 2020. 208, 223 22. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Advances in Neural Information Processing Systems, pages 5360\u20135370, 2017. 144, 179 23. Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. arXiv preprint arXiv:1810.09502, 2018. 268 24. Grigoris Antoniou and Frank Van Harmelen. A Semantic Web Primer. MIT press Cambridge, MA, 2008. 9 25. Oleg Arenz. Monte Carlo Chess. Master\u2019s thesis, Universit\u00e4t Darmstadt, 2012. 188 26. Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems, pages 41\u201348, 2007. 251 27. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26\u201338, 2017. 25, 28, 59, 86 28. John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A Bayesian sampling approach to exploration in reinforcement learning. In Proceedings of the TwentyFifth Conference on Uncertainty in Artifcial Intelligence, pages 19\u201326. AUAI Press, 2009. 278 29. Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in reinforcement learning. arXiv preprint arXiv:1908.06976, 2019. 237, 238, 241, 277 30. Peter Auer. Using confdence bounds for exploitation-exploration trade-ofs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002. 47, 168 31. Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235\u2013256, 2002. 168, 170 32. Peter Auer and Ronald Ortner. UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem. Periodica Mathematica Hungarica, 61(1-2):55\u201365, 2010. 168 33. Robert Axelrod. An evolutionary approach to norms. The American Political Science Review, pages 1095\u20131111, 1986. 223 34. Robert Axelrod. The complexity of cooperation: Agent-based models of competition and collaboration, volume 3. Princeton university press, 1997. 209, 216, 223 35. Robert Axelrod. The dissemination of culture: A model with local convergence and global polarization. Journal of Confict Resolution, 41(2):203\u2013226, 1997. 223 36. Robert Axelrod and Douglas Dion. The further evolution of cooperation. Science, 242(4884):1385\u20131390, 1988. 200 37. Robert Axelrod and William D Hamilton. The evolution of cooperation. Science, 211(4489):1390\u20131396, 1981. 200, 223 References 337 38. Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary C Lipton, and Animashree Anandkumar. Surprising negative results for generative adversarial tree search. arXiv preprint arXiv:1806.05780, 2018. 144 39. Mohammad Babaeizadeh, Mohammad Taghi Safar, Danijar Hafner, Harini Kannan, Chelsea Finn, Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design trade-ofs in visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603, 2020. 141 40. Thomas B\u00e4ck. Evolutionary Algorithms in Theory and Practice: Evolutionary Strategies, Evolutionary Programming, Genetic Algorithms. Oxford University Press, 1996. 209, 210, 223 41. Thomas B\u00e4ck, David B Fogel, and Zbigniew Michalewicz. Handbook of evolutionary computation. Release, 97(1):B1, 1997. 223 42. Thomas B\u00e4ck, Frank Hofmeister, and Hans-Paul Schwefel. A survey of evolution strategies. In Proceedings of the fourth International Conference on Genetic Algorithms, 1991. 202, 223 43. Thomas B\u00e4ck and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter optimization. Evolutionary Computation, 1(1):1\u201323, 1993. 11, 223 44. Christer Backstrom and Peter Jonsson. Planning with abstraction hierarchies can be exponentially less efcient. In Proceedings of the 14th International Joint Conference on Artifcial Intelligence, volume 2, pages 1599\u20131604, 1995. 228, 232 45. Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 31, 2017. 231, 233, 234, 241 46. Adri\u00e0 Puigdom\u00e8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari human benchmark. arXiv preprint arXiv:2003.13350, 2020. 85 47. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 319 48. Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pages 30\u201337. Elsevier, 1995. 66, 73 49. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019. 214, 216, 217, 332 50. Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017. 212 51. Chitta Baral. Knowledge Representation, Reasoning and Declarative Problem Solving. Cambridge university press, 2003. 232 52. Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc G. Bellemare, and Michael Bowling. The Hanabi challenge: A new frontier for AI research. Artifcial Intelligence, 280:103216, 2020. 221 53. Nolan Bard, John Hawkin, Jonathan Rubin, and Martin Zinkevich. The annual computer poker competition. AI Magazine, 34(2):112, 2013. 214, 223 54. Simon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a \u201ctheory of mind\u201d? Cognition, 21(1):37\u201346, 1985. 193, 211 55. Andrew Barron, Jorma Rissanen, and Bin Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743\u20132760, 1998. 15 56. Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(1-2):41\u201377, 2003. 232, 241 57. Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, (5):834\u2013846, 1983. 56, 99, 116 58. OpenAI Baselines. DQN https://openai.com/blog/openai-baselines-dqn/, 2017. 80, 84 59. Jonathan Baxter. A model of inductive bias learning. Journal of Artifcial Intelligence Research, 12:149\u2013198, 2000. 247 60. Jonathan Baxter, Andrew Tridgell, and Lex Weaver. Knightcap: a chess program that learns by combining TD (\ud835\udf06) with game-tree search. arXiv preprint cs/9901002, 1999. 188 338 References 61. Jonathan Baxter, Andrew Tridgell, and Lex Weaver. Learning to play chess using temporal diferences. Machine Learning, 40(3):243\u2013263, 2000. 162, 188 62. Don Beal and Martin C. Smith. Temporal diference learning for heuristic search and game playing. Information Sciences, 122(1):3\u201321, 2000. 188 63. Mark F Bear, Barry W Connors, and Michael A Paradiso. Neuroscience, volume 2. Lippincott Williams & Wilkins, 2007. 304 64. Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fcttler, Andrew Lefrancq, Simon Green, V\u00edctor Vald\u00e9s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gafney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. 332 65. Laurens Beljaards. Ai agents for the abstract strategy game tak. Master\u2019s thesis, Leiden University, 2017. 163 66. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-of. arXiv preprint arXiv:1812.11118, 2018. 321 67. Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv preprint arXiv:1903.07571, 2019. 321 68. Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overftting or perfect ftting? Risk bounds for classifcation and regression rules that interpolate. In Advances in Neural Information Processing Systems, pages 2300\u20132311, 2018. 321 69. Marc Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively factored environments. In International Conference on Machine Learning, pages 1211\u20131219, 2013. 278 70. Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449\u2013458, 2017. 80, 83 71. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artifcial Intelligence Research, 47:253\u2013279, 2013. 7, 41, 65, 67, 85, 86, 237, 262, 332 72. Richard Bellman. Dynamic Programming. Courier Corporation, 1957, 2013. 39, 45, 59, 295, 299 73. Richard Bellman. On the application of dynamic programing to the determination of optimal play in chess and checkers. Proceedings of the National Academy of Sciences, 53(2):244\u2013247, 1965. 59 74. Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in Neural Information Processing Systems, 19:137, 2007. 252 75. Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Technical report, Montreal, 1990. 268 76. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u2013 1828, 2013. 308 77. Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 41\u201348, 2009. 176, 188 78. Gerardo Beni. Swarm intelligence. Complex Social and Behavioral Systems: Game Theory and Agent-Based Models, pages 791\u2013818, 2020. 223 79. Gerardo Beni and Jing Wang. Swarm intelligence in cellular robotic systems. In Robots and Biological Systems: Towards a New Bionics?, pages 703\u2013712. Springer, 1993. 211 80. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond\u00e9 de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. 69 References 339 81. Tim Berners-Lee, James Hendler, and Ora Lassila. The semantic web. Scientifc American, 284(5):28\u201337, 2001. 9 82. Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of markov decision processes. Mathematics of Operations Research, 27(4):819\u2013840, 2002. 197, 206 83. Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with diferentiable closed-form solvers. In International Conference on Learning Representations, 2018. 268 84. R Bertolami, H Bunke, S Fernandez, A Graves, M Liwicki, and J Schmidhuber. A novel connectionist system for improved unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5), 2009. 313 85. Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena scientifc Belmont, MA, 1995. 10 86. Dimitri P Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. MIT Press Cambridge, 1996. 7, 59 87. Shrisha Bharadwaj. Embarrsingly simple zero shot learning. https://github.com/ chichilicious/embarrsingly-simple-zero-shot-learning, 2018. 268, 269 88. Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba Szepesv\u00e1ri. Convergent temporal-diference learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems, pages 1204\u20131212, 2009. 74, 86 89. Darse Billings, Aaron Davidson, Jonathan Schaefer, and Duane Szafron. The challenge of poker. Artifcial Intelligence, 134(1-2):201\u2013240, 2002. 214, 223 90. Darse Billings, Aaron Davidson, Terence Schauenberg, Neil Burch, Michael Bowling, Robert Holte, Jonathan Schaefer, and Duane Szafron. Game-tree search with adaptation in stochastic imperfect-information games. In International Conference on Computers and Games, pages 21\u201334. Springer, 2004. 223 91. Darse Billings, Denis Papp, Jonathan Schaefer, and Duane Szafron. Opponent modeling in poker. AAAI/IAAI, 493:499, 1998. 161 92. Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. O\u2019Reilly Media, Inc., 2009. 276 93. Christopher M Bishop. Pattern Recognition and Machine Learning. Information science and statistics. Springer Verlag, Heidelberg, 2006. 7, 14, 59, 105, 128, 129, 259, 298, 300, 302, 321 94. Peter Bloem. Transformers http://peterbloem.nl/blog/transformers. 320 95. Christian Blum and Daniel Merkle. Swarm Intelligence: Introduction and Applications. Springer Science & Business Media, 2008. 203, 211 96. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geof Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, and Rohith Kuditipudi. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 244, 263 97. Eric Bonabeau, Marco Dorigo, and Guy Theraulaz. Swarm Intelligence: From Natural to Artifcial Systems. Oxford University Press, 1999. 11, 203 98. Borealis. Few shot learning tutorial https://www.borealisai.com/en/blog/tutorial2-few-shot-learning-and-meta-learning-i/. 254 99. Zdravko I Botev, Dirk P Kroese, Reuven Y Rubinstein, and Pierre L\u2019Ecuyer. The cross-entropy method for optimization. In Handbook of Statistics, volume 31, pages 35\u201359. Elsevier, 2013. 133 340 References 100. Matthew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and Demis Hassabis. Reinforcement learning, fast and slow. Trends in Cognitive Sciences, 23(5):408\u2013422, 2019. 267 101. Matthew M Botvinick, Yael Niv, and Andew G Barto. Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective. Cognition, 113(3):262\u2013280, 2009. 241 102. Bruno Bouzy and Bernard Helmstetter. Monte Carlo Go developments. In Advances in Computer Games, pages 159\u2013174. Springer, 2004. 163, 167 103. Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up Limit Hold\u2019em poker is solved. Science, 347(6218):145\u2013149, 2015. 196, 214, 223 104. Michael H. Bowling, Nicholas Abou Risk, Nolan Bard, Darse Billings, Neil Burch, Joshua Davidson, John Alexander Hawkin, Robert Holte, Michael Johanson, Morgan Kan, Bryce Paradis, Jonathan Schaefer, David Schnizlein, Duane Szafron, Kevin Waugh, and Martin Zinkevich. A demonstration of the polaris poker system. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems, volume 2, pages 1391\u20131392, 2009. 223 105. Robert Boyd and Peter J Richerson. Culture and the Evolutionary Process. University of Chicago press, 1988. 209, 216, 223 106. Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning: Applications to data mining. Springer Science & Business Media, 2008. 247, 259, 267, 268 107. Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. 278 108. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. 41, 85, 86, 94, 116, 332 109. Rodney A Brooks. Intelligence without representation. Artifcial Intelligence, 47(1-3):139\u2013159, 1991. 11 110. Noam Brown, Sam Ganzfried, and Tuomas Sandholm. Hierarchical abstraction, distributed equilibrium computation, and post-processing, with application to a champion No-Limit Texas Hold\u2019em agent. In AAAI Workshop: Computer Poker and Imperfect Information, 2015. 204 111. Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization. In International Conference on Machine Learning, pages 793\u2013802. PMLR, 2019. 206, 215, 223 112. Noam Brown and Tuomas Sandholm. Superhuman AI for Heads-up No-limit poker: Libratus beats top professionals. Science, 359(6374):418\u2013424, 2018. 204, 215, 223 113. Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):885\u2013890, 2019. 214, 215, 223 114. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jefrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. 246, 308, 320 115. Cameron Browne. Hex Strategy. AK Peters/CRC Press, 2000. 184 116. Cameron Browne, Dennis JNJ Soemers, and Eric Piette. Strategic features for general games. In KEG@ AAAI, pages 70\u201375, 2019. 278 117. Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of Monte Carlo Tree Search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1\u201343, 2012. 164, 165, 166, 167, 168, 169, 170, 187, 333 118. Bernd Br\u00fcgmann. Monte Carlo Go. Technical report, Syracuse University, 1993. 163, 167 References 341 119. Bruno Buchberger, George E Collins, R\u00fcdiger Loos, and Rudolph Albrecht. Computer algebra symbolic and algebraic computation. ACM SIGSAM Bulletin, 16(4):5\u20135, 1982. 9 120. Cristian Bucilu\u02c7a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 535\u2013541, 2006. 278 121. Lars Buesing, Theophane Weber, S\u00e9bastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, and Daan Wierstra. Learning and querying fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018. 131, 139 122. Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156\u2013172, 2008. 222 123. Zhiyuan Cai, Huanhui Cao, Wenjie Lu, Lin Zhang, and Hao Xiong. Safe multi-agent reinforcement learning through decentralized multiple control barrier functions. arXiv preprint arXiv:2103.12553, 2021. 207 124. Murray Campbell, A Joseph Hoane Jr, and Feng-Hsiung Hsu. Deep Blue. Artifcial Intelligence, 134(1-2):57\u201383, 2002. 57, 64 125. Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with AMIGo: Adversarially motivated intrinsic goals. In International Conference on Learning Representations, 2020. 176, 231, 234 126. Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark. Emergent communication through negotiation. In International Conference on Learning Representations, 2018. 208 127. Thomas Carr, Maria Chli, and George Vogiatzis. Domain adaptation for reinforcement learning on the atari. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201919, Montreal, pages 1859\u20131861, 2018. 252 128. Edward Cartwright. Behavioral Economics. Routledge, 2018. 223 129. Rich Caruana. Multitask learning. Machine Learning, 28(1):41\u201375, 1997. 247, 248, 250, 251 130. Rich Caruana, Steve Lawrence, and C Lee Giles. Overftting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in Neural Information Processing Systems, pages 402\u2013408, 2001. 321 131. Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018. 332 132. Tristan Cazenave. Residual networks for computer Go. IEEE Transactions on Games, 10(1):107\u2013 110, 2018. 172 133. Tristan Cazenave, Yen-Chi Chen, Guan-Wei Chen, Shi-Yu Chen, Xian-Dong Chiu, Julien Dehos, Maria Elsa, Qucheng Gong, Hengyuan Hu, Vasil Khalidov, Cheng-Ling Li, Hsin-I Lin, Yu-Jin Lin, Xavier Martinet, Vegard Mella, J\u00e9r\u00e9my Rapin, Baptiste Rozi\u00e8re, Gabriel Synnaeve, Fabien Teytaud, Olivier Teytaud, Shi-Cheng Ye, Yi-Jun Ye, Shi-Jim Yen, and Sergey Zagoruyko. Polygames: Improved zero learning. arXiv preprint arXiv:2001.09832, 2020. 183, 184, 187, 332, 333 134. Tristan Cazenave and Bernard Helmstetter. Combining tactical search and Monte-Carlo in the game of Go. In Proceedings of the 2005 IEEE Symposium on Computational Intelligence and Games (CIG05), Essex University, volume 5, pages 171\u2013175, 2005. 170 135. Hyeong Soo Chang, Michael C Fu, Jiaqiao Hu, and Steven I Marcus. An adaptive sampling algorithm for solving Markov decision processes. Operations Research, 53(1):126\u2013139, 2005. 170 136. Yang Chao. Share and play new sokoban levels. http://Sokoban.org, 2013. 26 137. Guillaume Chaslot. Monte-Carlo tree search. PhD thesis, Maastricht University, 2010. 167 138. Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-Carlo tree search: A new framework for game AI. In AIIDE, 2008. 170, 179 139. Kumar Chellapilla and David B Fogel. Evolving neural networks to play checkers without relying on expert knowledge. IEEE Transactions on Neural Networks, 10(6):1382\u20131391, 1999. 75 342 References 140. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv preprint arXiv:2106.01345, 2021. 320 141. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classifcation. In International Conference on Learning Representations, 2019. 253, 264, 268 142. Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. 278 143. Maxime Chevalier-Boisvert, Lucas Willems, and Sumans Pal. Minimalistic gridworld environment for OpenAI Gym https://github.com/maximecb/gym-minigrid, 2018. 177, 234 144. Silvia Chiappa, S\u00e9bastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In International Conference on Learning Representations, 2017. 139 145. Jaeyoung Choi, Jack J Dongarra, and David W Walker. PB-BLAS: a set of parallel block basic linear algebra subprograms. Concurrency: Practice and Experience, 8(7):517\u2013535, 1996. 323 146. Fran\u00e7ois Chollet. Keras. https://keras.io, 2015. 87 147. Fran\u00e7ois Chollet. Deep learning with Python. Manning Publications Co., 2017. 321 148. Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. Back to basics: Benchmarking canonical evolution strategies for playing Atari. In Proceedings of the Twenty-Seventh International Joint Conference on Artifcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, pages 1419\u20131426, 2018. 223 149. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754\u20134765, 2018. 129, 133, 138, 144 150. Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In International Conference on Learning Representations, 2016. 276 151. Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, and Lorenzo Rosasco. Convex learning of multiple tasks and their structure. In International Conference on Machine Learning, pages 1548\u20131557. PMLR, 2015. 247 152. Dan Cire\u015fan, Ueli Meier, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber. Deep, big, simple neural nets for handwritten digit recognition. Neural Computation, 22(12):3207\u20133220, 2010. 324 153. Dan Cire\u015fan, Ueli Meier, and J\u00fcrgen Schmidhuber. Multi-column deep neural networks for image classifcation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, US, pages 3642\u20133649, 2012. 313 154. Christopher Clark and Amos Storkey. Teaching deep convolutional neural networks to play Go. arxiv preprint. arXiv preprint arXiv:1412.3409, 1, 2014. 75, 187 155. Christopher Clark and Amos Storkey. Training deep convolutional neural networks to play Go. In International Conference on Machine Learning, pages 1766\u20131774, 2015. 179, 187 156. Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. In 2nd Annual Conference on Robot Learning, CoRL 2018, Z\u00fcrich, Switzerland, pages 617\u2013629, 2018. 129, 138, 144 157. William F Clocksin and Christopher S Mellish. Programming in Prolog: Using the ISO standard. Springer Science & Business Media, 1981. 9 158. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International Conference on Machine Learning, pages 2048\u20132056. PMLR, 2020. 177, 279, 332 159. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In International Conference on Machine Learning, pages 1282\u20131289, 2018. 278 160. Helder Coelho and Luis Moniz Pereira. Automated reasoning in geometry theorem proving with prolog. Journal of Automated Reasoning, 2(4):329\u2013390, 1986. 278 References 343 161. C\u00e9dric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer. Curious: intrinsically motivated modular multi-goal reinforcement learning. In International Conference on Machine Learning, pages 1331\u20131340. PMLR, 2019. 277 162. C\u00e9dric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Intrinsically motivated goal-conditioned reinforcement learning: a short survey. arXiv preprint arXiv:2012.09830, 2020. 277 163. Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O Stanley, and Jef Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Advances in Neural Information Processing Systems, pages 5032\u20135043, 2018. 223 164. R\u00e9mi Coulom. Efcient selectivity and backup operators in Monte-Carlo Tree Search. In International Conference on Computers and Games, pages 72\u201383. Springer, 2006. 164, 170, 187 165. R\u00e9mi Coulom. Monte-Carlo tree search in Crazy Stone. In Proceedings Game Programming Workshop, Tokyo, Japan, pages 74\u201375, 2007. 167, 170 166. R\u00e9mi Coulom. The Monte-Carlo revolution in Go. In The Japanese-French Frontiers of Science Symposium (JFFoS 2008), Roscof, France, 2009. 170, 179 167. Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016\u20132019. 92, 93, 116 168. Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. In Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition, pages 1\u201335. Springer, 2017. 252, 268 169. Joseph Culberson. Sokoban is PSPACE-complete. Technical report, University of Alberta, 1997. 27, 56 170. Joseph C Culberson and Jonathan Schaefer. Pattern databases. Computational Intelligence, 14(3):318\u2013334, 1998. 167 171. Ken Currie and Austin Tate. O-plan: the open planning architecture. Artifcial Intelligence, 52(1):49\u201386, 1991. 231 172. Wojciech Marian Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omidshafei, David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. In Advances in Neural Information Processing Systems, 2020. 150 173. Kamil Czarnog\u00f3rski. Monte Carlo Tree Search beginners guide https://int8.io/montecarlo-tree-search-beginners-guide/, 2018. 166, 167 174. Will Dabney, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis Hassabis, R\u00e9mi Munos, and Matthew Botvinick. A distributional code for value in dopamine-based reinforcement learning. Nature, pages 1\u20135, 2020. 83 175. Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R McKee, Joel Z Leibo, Kate Larson, and Thore Graepel. Open problems in cooperative AI. arXiv preprint arXiv:2012.08630, 2020. 206 176. Zhongxiang Dai, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet, and Teck-Hua Ho. R2-B2: recursive reasoning-based Bayesian optimization for no-regret learning in games. In International Conference on Machine Learning, pages 2291\u20132301. PMLR, 2020. 208 177. Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2):337\u2013357, 2016. 241 178. Shubhomoy Das, Weng-Keen Wong, Thomas Dietterich, Alan Fern, and Andrew Emmott. Incorporating expert feedback into active anomaly discovery. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pages 853\u2013858. IEEE, 2016. 177 179. Hal Daum\u00e9 III. Frustratingly easy domain adaptation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, 2007. 252 180. Morton D Davis. Game Theory: a Nontechnical Introduction. Courier Corporation, 2012. 222 181. Richard Dawkins and Nicola Davis. The Selfsh Gene. Macat Library, 2017. 209 182. Peter Dayan and Geofrey E Hinton. Feudal reinforcement learning. In Advances in Neural Information Processing Systems, pages 271\u2013278, 1993. 231, 232 344 References 183. Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of Operations Research, 134(1):19\u201367, 2005. 133 184. Luis M De Campos, Juan M Fernandez-Luna, Jos\u00e9 A G\u00e1mez, and Jos\u00e9 M Puerta. Ant colony optimization for learning Bayesian networks. International Journal of Approximate Reasoning, 31(3):291\u2013311, 2002. 278 185. Dave De Jonge, Tim Baarslag, Reyhan Aydo\u011fan, Catholijn Jonker, Katsuhide Fujita, and Takayuki Ito. The challenge of negotiation in the game of diplomacy. In International Conference on Agreement Technologies, pages 100\u2013114. Springer, 2018. 151, 208 186. Joery A. de Vries, Ken S. Voskuil, Thomas M. Moerland, and Aske Plaat. Visualizing MuZero models. arXiv preprint arXiv:2102.12924, 2021. 141, 142, 146, 275, 333 187. Rina Dechter. Learning while searching in constraint-satisfaction problems. AAAI, 1986. 308 188. Marc Deisenroth and Carl E Rasmussen. PILCO: a model-based and data-efcient approach to policy search. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 465\u2013472, 2011. 128, 138, 144 189. Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen. Gaussian processes for data-efcient learning in robotics and control. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):408\u2013423, 2013. 128, 144 190. Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. In Foundations and Trends in Robotics 2, pages 1\u2013142. Now publishers, 2013. 143, 144 191. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. Ieee, 2009. 65, 276, 323 192. Mohit Deshpande. Deep RL policy methods https://mohitd.github.io/2019/01/20/ deep-rl-policy-methods.html. 116 193. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, 2018. 245, 246, 262, 263, 276, 307, 308, 320 194. Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI baselines. https://github.com/openai/baselines, 2017. 333 195. Thomas G Dietterich. The MAXQ method for hierarchical reinforcement learning. In International Conference on Machine Learning, volume 98, pages 118\u2013126, 1998. 241 196. Thomas G Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artifcial Intelligence Research, 13:227\u2013303, 2000. 27, 56, 231, 232, 241 197. Chuong B Do and Andrew Y Ng. Transfer learning for text classifcation. Advances in Neural Information Processing Systems, 18:299\u2013306, 2005. 268 198. Thang Doan, Joao Monteiro, Isabela Albuquerque, Bogdan Mazoure, Audrey Durand, Joelle Pineau, and R Devon Hjelm. On-line adaptative curriculum learning for GANs. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 33, pages 3470\u20133477, 2019. 178, 188 199. Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint, and Sebastian Trimpe. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018. 131 200. Jef Donahue, Yangqing Jia, Oriol Vinyals, Judy Hofman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, pages 647\u2013655. PMLR, 2014. 245, 268 201. Hao Dong, Zihan Ding, and Shanghang Zhang. Deep Reinforcement Learning. Springer, 2020. 86 202. Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J Hanson. An extended set of FORTRAN basic linear algebra subprograms. ACM Transactions on Mathematical Software, 14(1):1\u201317, 1988. 323 References 345 203. Christian Donninger. Null move and deep search. ICGA Journal, 16(3):137\u2013143, 1993. 163 204. Dorit Dor and Uri Zwick. Sokoban and other motion planning problems. Computational Geometry, 13(4):215\u2013228, 1999. 27, 56 205. Derek Doran, Sarah Schulz, and Tarek R Besold. What does explainable AI really mean? A new conceptualization of perspectives. arXiv preprint arXiv:1710.00794, 2017. 278 206. Marco Dorigo. Optimization, learning and natural algorithms. PhD Thesis, Politecnico di Milano, 1992. 211 207. Marco Dorigo and Mauro Birattari. Swarm intelligence. Scholarpedia, 2(9):1462, 2007. 203, 223 208. Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant colony optimization. IEEE Computational Intelligence Magazine, 1(4):28\u201339, 2006. 211, 223, 224 209. Marco Dorigo and Luca Maria Gambardella. Ant colony system: a cooperative learning approach to the traveling salesman problem. IEEE Transactions on Evolutionary Computation, 1(1):53\u201366, 1997. 11, 211 210. Norman R Draper and Harry Smith. Applied Regression Analysis, volume 326. John Wiley & Sons, 1998. 14 211. Yuntao Du, Zhiwen Tan, Qian Chen, Xiaowen Zhang, Yirong Yao, and Chongjun Wang. Dual adversarial domain adaptation. arXiv preprint arXiv:2001.00153, 2020. 252 212. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u20131338, 2016. 114, 115, 116 213. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. 188, 254, 256, 265, 268, 275 214. Ishan P Durugkar, Clemens Rosenbaum, Stefan Dernbach, and Sridhar Mahadevan. Deep reinforcement learning with macro-actions. arXiv preprint arXiv:1606.04615, 2016. 241 215. Werner Duvaud and Aur\u00e8le Hainaut. MuZero general: Open reimplementation of muzero. https://github.com/werner-duvaud/muzero-general, 2019. 146, 275 216. Zach Dwiel, Madhavun Candadai, Mariano Phielipp, and Arjun K Bansal. Hierarchical policy learning is sensitive to goal space design. arXiv preprint arXiv:1905.01537, 2019. 231 217. Russell C Eberhart, Yuhui Shi, and James Kennedy. Swarm Intelligence. Elsevier, 2001. 223 218. Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018. 133 219. Tom Eccles, Edward Hughes, J\u00e1nos Kram\u00e1r, Steven Wheelwright, and Joel Z Leibo. Learning reciprocity in complex sequential social dilemmas. arXiv preprint arXiv:1903.08082, 2019. 208 220. Adrien Lucas Ecofet. An intuitive explanation of policy gradient https: //towardsdatascience.com/an-intuitive-explanation-of-policy-gradientpart-1-reinforce-aa4392cbfd3c. 116 221. Adrien Ecofet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jef Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. 84 222. Adrien Ecofet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jef Clune. First return, then explore. Nature, 590(7847):580\u2013586, 2021. 84, 238, 277, 333 223. Harrison Edwards and Amos Storkey. Towards a neural statistician. In International Conference on Learning Representations, 2017. 268 224. Agoston E Eiben and Jim E Smith. What is an evolutionary algorithm? In Introduction to Evolutionary Computing, pages 25\u201348. Springer, 2015. 223 225. Jefrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71\u201399, 1993. 176 226. Arpad E Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978. 180 227. Markus Enzenberger, Martin Muller, Broderick Arneson, and Richard Segal. Fuego\u2014an open-source framework for board games and Go engine based on Monte Carlo tree search. IEEE Transactions on Computational Intelligence and AI in Games, 2(4):259\u2013270, 2010. 179, 187 346 References 228. Tom Erez, Yuval Tassa, and Emanuel Todorov. Simulation tools for model-based robotics: Comparison of Bullet, Havok, MuJoCo, Ode and Physx. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 4397\u20134404. IEEE, 2015. 93 229. Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503\u2013556, April 2005. 87 230. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In International Conference on Machine Learning, pages 1407\u20131416. PMLR, 2018. 265 231. Eyal Even-Dar, Yishay Mansour, and Peter Bartlett. Learning rates for Q-learning. Journal of machine learning Research, 5(1), 2003. 54 232. Richard Everett and Stephen Roberts. Learning against non-stationary agents with opponent modelling and deep reinforcement learning. In 2018 AAAI Spring Symposium Series, 2018. 208 233. Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 109\u2013117. ACM, 2004. 251 234. Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa, Silvio Savarese, and Li Fei-Fei. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, pages 767\u2013782, 2018. 114 235. Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in DQN. arXiv preprint arXiv:1810.00123, 2018. 278 236. Gregory Farquhar, Tim Rockt\u00e4schel, Maximilian Igl, and SA Whiteson. TreeQN and ATreeC: Diferentiable tree planning for deep reinforcement learning. In International Conference on Learning Representations, 2018. 57, 135, 136, 138, 144 237. Li Fei-Fei, Jia Deng, and Kai Li. Imagenet: Constructing a large-scale image database. Journal of Vision, 9(8):1037\u20131037, 2009. 65, 262, 276, 323 238. Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-based value estimation for efcient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018. 132, 138, 144, 207 239. Dieqiao Feng, Carla P Gomes, and Bart Selman. Solving hard AI planning instances using curriculum-driven deep reinforcement learning. arXiv preprint arXiv:2006.02689, 2020. 57, 144, 177, 178, 188, 275 240. Santiago Fern\u00e1ndez, Alex Graves, and J\u00fcrgen Schmidhuber. An application of recurrent neural networks to discriminative keyword spotting. In International Conference on Artifcial Neural Networks, pages 220\u2013229. Springer, 2007. 313 241. Richard E Fikes, Peter E Hart, and Nils J Nilsson. Learning and executing generalized robot plans. Artifcial Intelligence, 3:251\u2013288, 1972. 231, 232 242. Richard E Fikes and Nils J Nilsson. STRIPS: A new approach to the application of theorem proving to problem solving. Artifcial Intelligence, 2(3-4):189\u2013208, 1971. 9 243. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u2013 1135. PMLR, 2017. 254, 257, 258, 259, 263, 268, 333 244. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786\u20132793. IEEE, 2017. 133, 138, 144 245. Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International Conference on Machine Learning, pages 1920\u20131930. PMLR, 2019. 268 246. Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic Model-Agnostic Meta-Learning. In Advances in Neural Information Processing Systems, pages 9516\u20139527, 2018. 268 247. Yannis Flet-Berliac. The promise of hierarchical reinforcement learning. https:// thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/, March 2019. 227, 228, 276 References 347 248. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International Conference on Machine Learning, pages 1515\u20131528. PMLR, 2018. 177, 188, 241 249. Michael J Flynn. Some computer organizations and their efectiveness. IEEE Transactions on Computers, 100(9):948\u2013960, 1972. 324 250. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 32, 2018. 207 251. Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017. 208 252. David B Fogel. An introduction to simulated evolutionary optimization. IEEE Transactions on Neural Networks, 5(1):3\u201314, 1994. 11, 202 253. David B Fogel, Timothy J Hays, Sarah L Hahn, and James Quon. Further evolution of a self-learning chess program. In Computational Intelligence in Games, 2005. 162 254. Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration. In International Conference on Learning Representations, 2018. 80, 83 255. Vincent Fran\u00e7ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An introduction to deep reinforcement learning. Foundations and Trends in Machine Learning, 11(3-4):219\u2013354, 2018. 25, 28, 59, 116 256. Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In International Conference on Learning Representations, 2018. 231, 241 257. Nicholas Frosst and Geofrey Hinton. Distilling a neural network into a soft decision tree. In Proceedings of the First International Workshop on Comprehensibility and Explanation in AI and ML, 2017. 278 258. Vittorio Gallese and Alvin Goldman. Mirror neurons and the simulation theory of mindreading. Trends in Cognitive Sciences, 2(12):493\u2013501, 1998. 193, 211, 223 259. Sam Ganzfried and Tuomas Sandholm. Game theory-based opponent modeling in large imperfect-information games. In The 10th International Conference on Autonomous Agents and Multiagent Systems, volume 2, pages 533\u2013540, 2011. 161, 222 260. Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information games. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 37\u201345, 2015. 204 261. The garage contributors. Garage: A toolkit for reproducible reinforcement learning research. https://github.com/rlworkgroup/garage, 2019. 115, 266, 332 262. Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and practice\u2014a survey. Automatica, 25(3):335\u2013348, 1989. 133, 144 263. Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International Conference on Learning Representations, 2017. 268 264. Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1704\u20131713. PMLR, 2018. 268 265. Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti, and Renato Vidoni. Path planning and trajectory planning algorithms: A general overview. In Motion and Operation Planning of Robotic Systems, pages 3\u201327. Springer, 2015. 26, 56 266. Michael Gelfond and Vladimir Lifschitz. Action languages. Electronic Transactions on Artifcial Intelligence, 2(3\u20134):193\u2013210, 1998. 232 267. Sylvain Gelly, Levente Kocsis, Marc Schoenauer, Michele Sebag, David Silver, Csaba Szepesv\u00e1ri, and Olivier Teytaud. The grand challenge of computer Go: Monte Carlo tree search and extensions. Communications of the ACM, 55(3):106\u2013113, 2012. 179 268. Sylvain Gelly and David Silver. Achieving master level play in 9 \u00d7 9 computer Go. In AAAI, volume 8, pages 1537\u20131540, 2008. 179, 187 348 References 269. Sylvain Gelly, Yizao Wang, and Olivier Teytaud. Modifcation of UCT with patterns in Monte-Carlo Go. Technical Report RR-6062, INRIA, 2006. 167, 179, 187 270. Aur\u00e9lien G\u00e9ron. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. O\u2019Reilly Media, Inc., 2019. 77, 87, 327 271. Felix A Gers, J\u00fcrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. In Ninth International Conference on Artifcial Neural Networks ICANN 99. IET, 1999. 315 272. Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning: theory and practice. Elsevier, 2004. 231 273. Mohammad Ghavamzadeh, Sridhar Mahadevan, and Rajbala Makar. Hierarchical multi-agent reinforcement learning. Autonomous Agents and Multi-Agent Systems, 13(2):197\u2013229, 2006. 238 274. Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why generalization in RL is difcult: Epistemic POMDPs and implicit partial observability. Advances in Neural Information Processing Systems, 34, 2021. 278 275. Gerd Gigerenzer and Daniel G Goldstein. Reasoning the fast and frugal way: models of bounded rationality. Psychological review, 103(4):650, 1996. 209, 223 276. Thomas Gilovich, Dale Grifn, and Daniel Kahneman. Heuristics and Biases: The Psychology of Intuitive Judgment. Cambridge university press, 2002. 209 277. Andrew Gilpin and Tuomas Sandholm. A competitive Texas Hold\u2019em poker player via automated abstraction and real-time equilibrium computation. In Proceedings of the National Conference on Artifcial Intelligence, volume 21, page 1007, 2006. 223 278. Ross Girshick, Jef Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 580\u2013587, 2014. 245 279. John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society: Series B (Methodological), 41(2):148\u2013164, 1979. 47 280. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, 2016. 11, 70, 86, 303, 304, 305, 307, 310, 312, 315, 320, 321 281. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672\u20132680, 2014. 316, 317 282. Geofrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning Proceedings 1995, pages 261\u2013268. Elsevier, 1995. 87 283. Geofrey J Gordon. Approximate solutions to Markov decision processes. Carnegie Mellon University, 1999. 66, 73 284. Tobias Graf and Marco Platzner. Adaptive playouts in Monte-Carlo tree search with policygradient reinforcement learning. In Advances in Computer Games, pages 1\u201311. Springer, 2015. 187 285. Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifths. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018. 268 286. Alex Graves, Santiago Fern\u00e1ndez, and J\u00fcrgen Schmidhuber. Bidirectional LSTM networks for improved phoneme classifcation and recognition. In International Conference on Artifcial Neural Networks, pages 799\u2013804. Springer, 2005. 315 287. Alex Graves, Abdel-rahman Mohamed, and Geofrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645\u20136649. IEEE, 2013. 307 288. Klaus Gref, Rupesh K Srivastava, Jan Koutn\u00edk, Bas R Steunebrink, and J\u00fcrgen Schmidhuber. LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10):2222\u20132232, 2017. 315 289. Jean-Bastien Grill, Florent Altch\u00e9, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, and R\u00e9mi Munos. Monte-carlo tree search as regularized policy optimization. In International Conference on Machine Learning, pages 3769\u20133778. PMLR, 2020. 136, 141, 170 References 349 290. Christopher Grimm, Andr\u00e9 Barreto, Satinder Singh, and David Silver. The value equivalence principle for model-based reinforcement learning. In Advances in Neural Information Processing Systems, 2020. 130, 140, 141, 142 291. Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. There is no turning back: A self-supervised approach for reversibility-aware reinforcement learning. arXiv preprint arXiv:2106.04480, 2021. 242 292. Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artifcial Intelligence Review, pages 1\u201349, 2021. 198, 222 293. Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actorcritic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291\u20131307, 2012. 83, 99 294. Peter D Gr\u00fcnwald. The minimum description length principle. MIT press, 2007. 15, 318 295. Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. The reactor: A fast and sample-efcient actor-critic agent for reinforcement learning. In International Conference on Learning Representations, 2018. 85 296. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep Qlearning with model-based acceleration. In International Conference on Machine Learning, pages 2829\u20132838, 2016. 132, 138, 144 297. Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored MDPs. In Advances in Neural Information Processing Systems, volume 1, pages 1523\u20131530, 2001. 207 298. Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, S\u00e9bastien Racani\u00e8re, Theophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, and Timothy P. Lillicrap. An investigation of model-free planning. In International Conference on Machine Learning, pages 2464\u20132473, 2019. 57, 138, 144 299. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio G\u00f3mez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL unplugged: Benchmarks for ofine reinforcement learning. arXiv preprint arXiv:2006.13888, 2020. 332 300. David Gunning. Explainable artifcial intelligence (XAI). Defense Advanced Research Projects Agency (DARPA), 2, 2017. 278 301. Xifeng Guo, Wei Chen, and Jianping Yin. A simple approach for unsupervised domain adaptation. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 1566\u2013 1570. IEEE, 2016. 252 302. Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. In Advances in Neural Information Processing Systems, pages 5307\u20135316, 2018. 268 303. David Ha and J\u00fcrgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pages 2450\u20132462, 2018. 131 304. David Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 130, 131, 138, 144, 278 305. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pages 1352\u20131361. PMLR, 2017. 107 306. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Of-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018. 95, 107, 140, 272 307. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actorcritic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. 107 308. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. 130, 131, 138, 140, 144 350 References 309. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pages 2555\u20132565, 2019. 130, 131, 133, 138, 140, 141, 142, 144, 333 310. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. 130, 131, 138, 140, 144, 333 311. Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control. Machine Learning, 84(1-2):137\u2013169, 2011. 108 312. Dongge Han, Wendelin Boehmer, Michael Wooldridge, and Alex Rogers. Multi-agent hierarchical reinforcement learning with dynamic termination. In Pacifc Rim International Conference on Artifcial Intelligence, pages 80\u201392. Springer, 2019. 238 313. Dongge Han, Chris Xiaoxuan Lu, Tomasz Michalak, and Michael Wooldridge. Multiagent model-based credit assignment for continuous control, 2021. 207 314. Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems, pages 2613\u20132621, 2010. 82 315. Matthew John Hausknecht. Cooperation and Communication in Multiagent Deep Reinforcement Learning. PhD thesis, University of Texas at Austin, 2016. 208 316. Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas L Dean, and Craig Boutilier. Hierarchical solution of Markov decision processes using macro-actions. In UAI \u201998: Proceedings of the Fourteenth Conference on Uncertainty in Artifcial Intelligence, University of Wisconsin Business School, Madison, Wisconsin, 1998. 230, 241 317. Conor F. Hayes, Roxana Radulescu, Eugenio Bargiacchi, Johan K\u00e4llstr\u00f6m, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Dazeley, Fredrik Heintz, Enda Howley, Athirai A. Irissappane, Patrick Mannion, Ann Now\u00e9, Gabriel de Oliveira Ramos, Marcello Restelli, Peter Vamplew, and Diederik M. Roijers. A practical guide to multi-objective reinforcement learning and planning. arXiv preprint arXiv:2103.09568, 2021. 198 318. Simon Haykin. Neural Networks: a Comprehensive Foundation. Prentice Hall, 1994. 11 319. Ryan B Hayward and Bjarne Toft. Hex: The Full Story. CRC Press, 2019. 184 320. He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9 III. Opponent modeling in deep reinforcement learning. In International Conference on Machine Learning, pages 1804\u20131813. PMLR, 2016. 161, 208 321. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016. 172, 315, 316 322. Robert A Hearn and Erik D Demaine. Games, Puzzles, and Computation. CRC Press, 2009. 27, 56 323. David Heckerman, Dan Geiger, and David M Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197\u2013243, 1995. 278 324. Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with energy-based policies. In European Workshop on Reinforcement Learning, pages 45\u201358, 2013. 74, 75, 86 325. Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, SM Eslami, Martin Riedmiller, and David Silver. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017. 90, 113, 212 326. Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages 2944\u20132952, 2015. 138, 144 327. Ernst A Heinz. New self-play results in computer chess. In International Conference on Computers and Games, pages 262\u2013276. Springer, 2000. 188 328. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artifcial Intelligence, 2018. 87, 114, 115, 116, 138 References 351 329. Mark Hendrikx, Sebastiaan Meijer, Joeri Van Der Velden, and Alexandru Iosup. Procedural content generation for games: A survey. ACM Transactions on Multimedia Computing, Communications, and Applications, 9(1):1\u201322, 2013. 59 330. John L Hennessy and David A Patterson. Computer Architecture: a Quantitative Approach. Elsevier, 2017. 324 331. Joseph Henrich, Robert Boyd, and Peter J Richerson. Five misunderstandings about cultural evolution. Human Nature, 19(2):119\u2013137, 2008. 223 332. Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183, 2017. 193, 196, 201, 222 333. Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. A survey and critique of multiagent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6):750\u2013797, 2019. 198, 222 334. Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane Weber, David Silver, and Hado van Hasselt. Muesli: Combining improvements in policy optimization. In International Conference on Machine Learning, pages 4214\u20134226, 2021. 141, 275 335. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, pages 3215\u20133222, 2018. 80, 81, 82, 87 336. Francis Heylighen. What makes a Meme Successful? Selection Criteria for Cultural Evolution. Association Internationale de Cybernetique, 1998. 223 337. Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. In International Conference on Machine Learning, pages 1480\u20131490. PMLR, 2017. 268 338. Ashley Hill, Antonin Rafn, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https: //github.com/hill-a/stable-baselines, 2018. 333 339. W Daniel Hillis. New computer architectures and their relationship to physics or why computer science is no good. International Journal of Theoretical Physics, 21(3-4):255\u2013262, 1982. 324 340. W Daniel Hillis and Lewis W Tucker. The CM-5 connection machine: A scalable supercomputer. Communications of the ACM, 36(11):30\u201341, 1993. 324 341. Geofrey Hinton, Oriol Vinyals, and Jef Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 278 342. Geofrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006. 317 343. Geofrey E Hinton and Terrence Joseph Sejnowski, editors. Unsupervised Learning: Foundations of Neural Computation. MIT press, 1999. 15 344. Geofrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 320 345. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. 313, 315 346. John Holland. Adaptation in natural and artifcial systems: an introductory analysis with application to biology. Control and Artifcial Intelligence, 1975. 47, 202 347. John H Holland. Genetic algorithms. Scientifc American, 267(1):66\u201373, 1992. 11 348. Bert H\u00f6lldobler and Edward O Wilson. The Superorganism: the Beauty, Elegance, and Strangeness of Insect Societies. WW Norton & Company, 2009. 191 349. John J Hopfeld. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8):2554\u20132558, 1982. 313 352 References 350. Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. In International Conference on Learning Representations, 2018. 221 351. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020. 247, 248, 253, 255, 267, 268 352. Ronald A Howard. Dynamic programming and Markov processes. New York: John Wiley, 1964. 28 353. Chloe Ching-Yun Hsu, Celestine Mendler-D\u00fcnner, and Moritz Hardt. Revisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020. 106 354. Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated the world chess champion. Princeton University Press, 2004. 64 355. Feng-Hsiung Hsu, Thomas Anantharaman, Murray Campbell, and Andreas Nowatzyk. A grandmaster chess machine. Scientifc American, 263(4):44\u201351, 1990. 69 356. R Lily Hu, Caiming Xiong, and Richard Socher. Zero-shot image classifcation guided by natural language descriptions of classes: A meta-learning approach. In Advances in Neural Information Processing Systems, 2018. 268 357. David H Hubel and Torsten N Wiesel. Shape and arrangement of columns in cat\u2019s striate cortex. The Journal of Physiology, 165(3):559\u2013568, 1963. 310 358. David H Hubel and Torsten N Wiesel. Receptive felds and functional architecture of monkey striate cortex. The Journal of Physiology, 195(1):215\u2013243, 1968. 310 359. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, pages 4476\u20134486, 2021. 140, 275 360. Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes ImageNet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016. 245, 268 361. Jonathan Hui. RL\u2014DQN Deep Q-network https://medium.com/@jonathan_hui/rl-dqndeep-q-network-e207751f7ae4. Medium post. 84 362. Jonathan Hui. Model-based reinforcement learning https://medium.com/@jonathan_hui/ rl-model-based-reinforcement-learning-3c2b6f0aa323. Medium post, 2018. 129 363. Mike Huisman, Jan van Rijn, and Aske Plaat. Metalearning for deep neural networks. In Pavel Brazdil et al., editors, Metalearning: Applications to data mining. Springer, 2022. 247, 258, 259, 267, 268 364. Mike Huisman, Jan N. van Rijn, and Aske Plaat. A survey of deep meta-learning. Artifcial Intelligence Review, 2021. 248, 253, 254, 255, 256, 257, 267, 268, 379 365. Matthew Hutson. Artifcial Intelligence faces reproducibility crisis. Science, 359:725\u2013726, 2018. 87 366. Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm confguration. In International Conference on Learning and Intelligent Optimization, pages 507\u2013523. Springer, 2011. 259 367. Frank Hutter, Holger H Hoos, Kevin Leyton-Brown, and Thomas St\u00fctzle. ParamILS: an automatic algorithm confguration framework. Journal of Artifcial Intelligence Research, 36:267\u2013306, 2009. 259 368. Frank Hutter, Lars Kotthof, and Joaquin Vanschoren. Automated Machine Learning: Methods, Systems, Challenges. Springer Nature, 2019. 260 369. Roman Ilin, Robert Kozma, and Paul J Werbos. Efcient learning in cellular simultaneous recurrent neural networks\u2014the case of maze navigation problem. In 2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, pages 324\u2013 329, 2007. 135, 139, 144 370. Sergey Iofe. Batch renormalization: Towards reducing minibatch dependence in batchnormalized models. In Advances in Neural Information Processing Systems, pages 1945\u20131953, 2017. 321 371. Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017. 87 References 353 372. Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and human-like gameplay with KL-regularized search. arXiv preprint arXiv:2112.07544, 2021. 170 373. Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Casta\u00f1eda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel. Human-level performance in 3D multiplayer games with population-based reinforcement learning. Science, 364(6443):859\u2013865, 2019. 68, 69, 214, 218, 277 374. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jef Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. 212, 213, 219, 223, 265, 277, 333 375. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLbench: the robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020. 114 376. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, pages 12498\u201312509, 2019. 129, 132, 138, 144 377. Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. arXiv preprint arXiv:2106.02039, 2021. 320 378. Andrew A Jawlik. Statistics from A to Z: Confusing concepts clarifed. John Wiley & Sons, 2016. 288 379. Michael Johanson, Nolan Bard, Marc Lanctot, Richard G Gibson, and Michael Bowling. Efcient Nash equilibrium approximation through Monte Carlo counterfactual regret minimization. In AAMAS, pages 837\u2013846, 2012. 203, 204, 223 380. Ian T Jollife and Jorge Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016. 15 381. Michael Irwin Jordan. Learning in Graphical Models, volume 89. Springer Science & Business Media, 1998. 278 382. Arthur Juliani. Simple reinforcement learning with tensorfow part 8: Asynchronous actorcritic agents (A3C) https://medium.com/emergent-future/simple-reinforcementlearning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3cc88f72a5e9f2, 2016. 103, 104 383. Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, and Danny Lange. Unity: A general platform for intelligent agents. arXiv preprint arXiv:1809.02627, 2018. 265, 332 384. Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in vision, control, and planning. arXiv preprint arXiv:1902.01378, 2019. 223 385. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589, 2021. 188, 275 386. Andreas Junghanns and Jonathan Schaefer. Sokoban: Enhancing general single-agent search methods using domain knowledge. Artifcial Intelligence, 129(1-2):219\u2013251, 2001. 27, 56 387. Niels Justesen, Philip Bontrager, Julian Togelius, and Sebastian Risi. Deep learning for video game playing. IEEE Transactions on Games, 12(1):1\u201320, 2019. 87 354 References 388. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. Illuminating generalization in deep reinforcement learning through procedural level generation. arXiv preprint arXiv:1806.10729, 2018. 177 389. Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of Artifcial Intelligence Research, 4:237\u2013285, 1996. 7, 59 390. Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook of the Fundamentals of Financial Decision Making: Part I, pages 99\u2013127. World Scientifc, 2013. 223 391. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-based reinforcement learning for Atari. arXiv:1903.00374, 2019. 130, 133, 138, 144 392. Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin L. Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. SGD on neural networks learns functions of increasing complexity. In Advances in Neural Information Processing Systems, pages 3491\u20133501, 2019. 321 393. Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep reinforcement learning. In Conference on Robot Learning, pages 195\u2013206, 2017. 132 394. Reza Kamyar and Ehsan Taheri. Aircraft optimal terrain/threat-based trajectory planning and control. Journal of Guidance, Control, and Dynamics, 37(2):466\u2013483, 2014. 133 395. Satwik Kansal and Brendan Martin. Learn data science webpage., 2018. 42, 52, 53, 54, 61, 126 396. Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of statistical mechanics: theory and experiment, 2005(11):P11011, 2005. 107 397. Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International Conference on Learning Representations, 2018. 85 398. Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational Bayes flters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016. 131 399. Andrej Karpathy. The unreasonable efectiveness of recurrent neural networks. http: //karpathy.github.io/2015/05/21/rnn-effectiveness/. Andrej Karpathy Blog, 2015. 314, 315 400. Andrej Karpathy. Deep reinforcement learning: Pong from pixels. http://karpathy. github.io/2016/05/31/rl/. Andrej Karpathy Blog, 2016. 80 401. Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015. 142 402. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. 316, 318 403. Henry J Kelley. Gradient theory of optimal fight paths. American Rocket Society Journal, 30(10):947\u2013954, 1960. 135, 139, 144 404. Stephen Kelly and Malcolm I Heywood. Multi-task learning in Atari video games with emergent tangled program graphs. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 195\u2013202. ACM, 2017. 251 405. Stephen Kelly and Malcolm I Heywood. Emergent tangled program graphs in multi-task learning. In IJCAI, pages 5294\u20135298, 2018. 251 406. James Kennedy. Swarm intelligence. In Handbook of Nature-Inspired and Innovative Computing, pages 187\u2013219. Springer, 2006. 11, 223 407. Pascal Kerschke, Holger H Hoos, Frank Neumann, and Heike Trautmann. Automated algorithm selection: Survey and perspectives. Evolutionary Computation, 27(1):3\u201345, 2019. 259 408. Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago Miret, Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In International Conference on Machine Learning, pages 3341\u20133350. PMLR, 2019. 223, 277 References 355 409. Shauharda Khadka and Kagan Tumer. Evolutionary reinforcement learning. arXiv preprint arXiv:1805.07917, 2018. 223 410. Khimya Khetarpal, Zafarali Ahmed, Andre Cianfone, Riashat Islam, and Joelle Pineau. Reevaluate: Reproducibility in evaluating reinforcement learning algorithms. In Reproducibility in Machine Learning Workshop, ICML, 2018. 87 411. Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. 15, 131, 317 412. Diederik P Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach. Learn., 12(4):307\u2013392, 2019. 15, 131, 317 413. Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of generalisation in deep reinforcement learning. arXiv preprint arXiv:2111.09794, 2021. 279 414. Daan Klijn and AE Eiben. A coevolutionairy approach to deep multi-agent reinforcement learning. arXiv preprint arXiv:2104.05610, 2021. 221, 223 415. Craig A Knoblock. Learning abstraction hierarchies for problem solving. In AAAI, pages 923\u2013928, 1990. 231, 232 416. Donald E Knuth and Ronald W Moore. An analysis of alpha-beta pruning. Artifcial Intelligence, 6(4):293\u2013326, 1975. 151, 161, 163 417. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013. 111, 143, 144 418. Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning workshop, volume 2. Lille, 2015. 268 419. Levente Kocsis and Csaba Szepesv\u00e1ri. Bandit based Monte-Carlo planning. In European Conference on Machine Learning, pages 282\u2013293. Springer, 2006. 164, 167, 168, 187 420. Vijay R Konda and John N Tsitsiklis. Actor\u2013critic algorithms. In Advances in Neural Information Processing Systems, pages 1008\u20131014, 2000. 99 421. Vijaymohan R Konda and Vivek S Borkar. Actor\u2013Critic-type learning algorithms for Markov Decision Processes. SIAM Journal on Control and Optimization, 38(1):94\u2013123, 1999. 99 422. Richard E Korf. Depth-frst iterative-deepening: An optimal admissible tree search. Artifcial intelligence, 27(1):97\u2013109, 1985. 163 423. Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Robot motor skill coordination with em-based reinforcement learning. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3232\u20133237. IEEE, 2010. 5, 6 424. Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge \u2019naturally\u2019 in multi-agent dialog. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, pages 2962\u20132967, 2017. 208 425. Samuel Kotz, Narayanaswamy Balakrishnan, and Norman L Johnson. Continuous Multivariate Distributions, Volume 1: Models and Applications. John Wiley & Sons, 2004. 48, 106 426. Basil Kouvaritakis and Mark Cannon. Model Predictive Control. Springer, 2016. 144 427. Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82\u201394, 2016. 207 428. Mark A Kramer. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal, 37(2):233\u2013243, 1991. 317 429. Sarit Kraus, Eithan Ephrati, and Daniel Lehmann. Negotiation in a non-cooperative environment. Journal of Experimental & Theoretical Artifcial Intelligence, 3(4):255\u2013281, 1994. 151 430. Sarit Kraus and Daniel Lehmann. Diplomat, an agent in a multi agent environment: An overview. In IEEE International Performance Computing and Communications Conference, pages 434\u2013438, 1988. 208 431. Alex Krizhevsky, Ilya Sutskever, and Geofrey E Hinton. Imagenet classifcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012. 65, 246, 307, 308, 323 432. Kai A Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps. Cognition, 110(3):380\u2013394, 2009. 176 356 References 433. Steven Kuhn. Prisoner\u2019s Dilemma. The Stanford Encyclopedia of Philosophy, https:// plato.stanford.edu/entries/prisoner-dilemma/, 1997. 198 434. Jan Kuipers, Aske Plaat, Jos AM Vermaseren, and H Jaap van den Herik. Improving multivariate Horner schemes with Monte Carlo tree search. Computer Physics Communications, 184(11):2391\u20132395, 2013. 169, 170 435. Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 3675\u20133683, 2016. 229, 231, 237, 238, 241 436. Solomon Kullback and Richard A Leibler. On information and sufciency. The Annals of Mathematical Statistics, 22(1):79\u201386, 1951. 105 437. Yen-Ling Kuo, Boris Katz, and Andrei Barbu. Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas. arXiv preprint arXiv:2006.01110, 2020. 268 438. Karol Kurach, Anton Raichuk, Piotr Sta\u0144czyk, Micha\u0142 Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football: A novel reinforcement learning environment. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 34, pages 4501\u20134510, 2020. 221, 332 439. Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. In International Conference on Learning Representations, 2018. 129, 144 440. W Hi Kwon, AM Bruckstein, and T Kailath. Stabilizing state-feedback design via the moving horizon method. International Journal of Control, 37(3):631\u2013643, 1983. 133 441. Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107\u20131149, Dec 2003. 87 442. Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics, pages 1091\u20131114, 1987. 48 443. Tze Leung Lai and Herbert Robbins. Asymptotically efcient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4\u201322, 1985. 47, 48 444. John E Laird, Paul S Rosenbloom, and Allen Newell. Chunking in Soar: the anatomy of a general learning mechanism. Machine learning, 1(1):11\u201346, 1986. 230, 241 445. Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011. 253, 262, 332 446. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015. 262 447. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. The Omniglot challenge: a 3-year progress report. Current Opinion in Behavioral Sciences, 29:97\u2013104, 2019. 262 448. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402\u20136413, 2017. 133 449. Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 951\u2013958. IEEE, 2009. 261, 268 450. Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vin\u00edcius Flores Zambaldi, Satyaki Upadhyay, Julien P\u00e9rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, J\u00e1nos Kram\u00e1r, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas W. Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis. Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019. 332 451. Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling for regret minimization in extensive games. In Advances in Neural Information Processing Systems, pages 1078\u20131086, 2009. 196, 203, 204, 223 References 357 452. Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unifed game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems, pages 4190\u2013 4203, 2017. 194 453. Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2010. 74 454. Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI, volume 1, page 3, 2008. 260, 268 455. Alexandre Laterre, Yunguan Fu, Mohamed Khalil Jabri, Alain-Sam Cohen, David Kas, Karl Hajjar, Torbjorn S Dahl, Amine Kerkeni, and Karim Beguir. Ranked reward: Enabling self-play reinforcement learning for combinatorial optimization. arXiv preprint arXiv:1807.01672, 2018. 177, 178, 188, 275 456. Jean-Claude Latombe. Robot Motion Planning, volume 124. Springer Science & Business Media, 2012. 26, 56 457. Stefen L Lauritzen. Graphical Models, volume 17. Clarendon Press, 1996. 278 458. Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In International Conference on Learning Representations, 2017. 208 459. Yann LeCun, Yoshua Bengio, and Geofrey Hinton. Deep learning. Nature, 521(7553):436, 2015. 11, 12, 86, 244, 307 460. Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541\u2013551, 1989. 311 461. Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Hafner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. 308, 309, 322 462. Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609\u2013616. ACM, 2009. 307, 308 463. Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, pages 2927\u20132936. PMLR, 2018. 255 464. Joel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv preprint arXiv:1903.00742, 2019. 187, 212, 217, 275 465. Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multiagent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2017, S\u00e3o Paulo, Brazil, pages 464\u2013473, 2017. 206, 208, 209, 216 466. Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jefrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert C. Zak. The network architecture of the connection machine CM-5. In Proceedings of the fourth annual ACM Symposium on Parallel Algorithms and Architectures, pages 272\u2013285, 1992. 324 467. Matteo Leonetti, Luca Iocchi, and Peter Stone. A synthesis of automated planning and reinforcement learning for efcient, robust decision-making. Artifcial Intelligence, 241:103\u2013 130, 2016. 132 468. Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pages 1071\u20131079, 2014. 138, 144 469. Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pages 1\u20139, 2013. 128, 144 358 References 470. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 251 471. Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierarchies with hindsight. In International Conference on Learning Representations, 2019. 231, 233, 236, 239, 241, 276 472. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems, pages 6391\u20136401, 2018. 142 473. Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. 268 474. Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit coordination graphs for multi-agent reinforcement learning. In AAMAS \u201921: 20th International Conference on Autonomous Agents and Multiagent Systems, 2021. 207 475. Siyuan Li, Rui Wang, Minxue Tang, and Chongjie Zhang. Hierarchical reinforcement learning with advantage-based auxiliary rewards. In Advances in Neural Information Processing Systems, pages 1407\u20131417, 2019. 276 476. Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017. 254, 268 477. Zhuoru Li, Akshay Narayan, and Tze-Yun Leong. An efcient approach to model-based hierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 31, 2017. 231 478. Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E Gonzalez, Michael I Jordan, and Ion Stoica. RLlib: abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pages 3059\u20133068, 2018. 332 479. Diego P\u00e9rez Li\u00e9bana, Simon M Lucas, Raluca D Gaina, Julian Togelius, Ahmed Khalifa, and Jialin Liu. General video game artifcial intelligence. Synthesis Lectures on Games and Computational Intelligence, 3(2):1\u2013191, 2019. 177 480. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016. 92, 95, 108, 109, 116, 140, 272 481. Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3-4):293\u2013321, 1992. 75, 87 482. Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. 74, 75 483. Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pages 157\u2013163. Elsevier, 1994. 28, 194, 222 484. Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A comprehensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3):385\u2013398, 2014. 223 485. Hao Liu and Pieter Abbeel. Hybrid discriminative-generative training via contrastive learning. arXiv preprint arXiv:2007.09070, 2020. 73 486. Hui Liu, Song Yu, Zhangxin Chen, Ben Hsieh, and Lei Shao. Sparse matrix-vector multiplication on NVIDIA GPU. International Journal of Numerical Analysis & Modeling, Series B, 3(2):185\u2013191, 2012. 324 487. Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel. Emergent coordination through competition. In International Conference on Learning Representations, 2019. 212, 223 488. Manuel L\u00f3pez-Ib\u00e1\u00f1ez, J\u00e9r\u00e9mie Dubois-Lacoste, Leslie P\u00e9rez C\u00e1ceres, Mauro Birattari, and Thomas St\u00fctzle. The irace package: Iterated racing for automatic algorithm confguration. Operations Research Perspectives, 3:43\u201358, 2016. 259 489. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent Actor-Critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 6379\u20136390, 2017. 207, 213, 333 References 359 490. Gabriel Loye. The attention mechanism. https://blog.floydhub.com/attentionmechanism/. 319 491. Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the efectiveness of SGD in modern over-parametrized learning. In International Conference on Machine Learning, pages 3331\u20133340, 2018. 321 492. Xiaoliang Ma, Xiaodong Li, Qingfu Zhang, Ke Tang, Zhengping Liang, Weixin Xie, and Zexuan Zhu. A survey on cooperative co-evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 23(3):421\u2013441, 2018. 210 493. Laurens van der Maaten and Geofrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579\u20132605, Nov 2008. 15 494. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artifcial Intelligence Research, 61:523\u2013562, 2018. 86 495. Hamid Reza Maei, Csaba Szepesv\u00e1ri, Shalabh Bhatnagar, and Richard S Sutton. Toward of-policy learning control with function approximation. In International Conference on Machine Learning, 2010. 74 496. Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multiagent variational exploration. In Advances in Neural Information Processing Systems, pages 7611\u20137622, 2019. 207 497. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181\u2013196, 2018. 244 498. Somdeb Majumdar, Shauharda Khadka, Santiago Miret, Stephen McAleer, and Kagan Tumer. Evolutionary reinforcement learning for sample-efcient multiagent coordination. In International Conference on Machine Learning, 2020. 223 499. Rajbala Makar, Sridhar Mahadevan, and Mohammad Ghavamzadeh. Hierarchical multi-agent reinforcement learning. In Proceedings of the Fifth International Conference on Autonomous Agents, pages 246\u2013253. ACM, 2001. 238 500. Julian N Marewski, Wolfgang Gaissmaier, and Gerd Gigerenzer. Good judgments do not require complex cognition. Cognitive Processing, 11(2):103\u2013121, 2010. 209 501. Vince Martinelli. How robots autonomously see, grasp, and pick. https://www. therobotreport.com/grasp-sight-picking-evolve-robots/, 2019. 93 502. Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. IEEE Trans. Neural Networks Learn. Syst., 31(9):3732\u20133740, 2020. 188 503. Masakazu Matsugu, Katsuhiko Mori, Yusuke Mitari, and Yuji Kaneda. Subject independent facial expression recognition with robust face detection using a convolutional neural network. Neural Networks, 16(5-6):555\u2013559, 2003. 310 504. Kiminori Matsuzaki. Empirical analysis of PUCT algorithm with evaluation functions of diferent quality. In 2018 Conference on Technologies and Applications of Artifcial Intelligence (TAAI), pages 142\u2013147. IEEE, 2018. 168, 169 505. David Q Mayne, James B Rawlings, Christopher V Rao, and Pierre OM Scokaert. Constrained model predictive control: Stability and optimality. Automatica, 36(6):789\u2013814, 2000. 144 506. James L McClelland, Bruce L McNaughton, and Randall C O\u2019Reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102(3):419, 1995. 75 507. Francisco S Melo and M Isabel Ribeiro. Convergence of Q-learning with linear function approximation. In 2007 European Control Conference (ECC), pages 2671\u20132678. IEEE, 2007. 72 508. Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas Heess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In International Conference on Learning Representations, 2019. 114 360 References 509. Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, and Bence \u00d6lveczky. Deep neuroethology of a virtual rodent. In International Conference on Learning Representations, 2020. 114 510. Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control. In International Conference on Learning Representations, 2019. 114 511. Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201, 2017. 114 512. Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Dufy, and Babak Hodjat. Evolving deep neural networks. In Artifcial Intelligence in the Age of Neural Networks and Brain Computing, pages 293\u2013312. Elsevier, 2019. 277 513. Tomas Mikolov, Kai Chen, Greg Corrado, and Jefrey Dean. Efcient estimation of word representations in vector space. In International Conference on Learning Representations, 2013. 245, 249 514. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jefrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, 2013. 245 515. Jonathan K Millen. Programming the game of Go. Byte Magazine, 1981. 153 516. S Ali Mirsoleimani, Aske Plaat, Jaap Van Den Herik, and Jos Vermaseren. Scaling Monte Carlo tree search on Intel Xeon Phi. In Parallel and Distributed Systems (ICPADS), 2015 IEEE 21st International Conference on, pages 666\u2013673. IEEE, 2015. 187 517. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018. 254, 268 518. Tom M Mitchell. The need for biases in learning generalizations. Technical Report CBM-TR117, Department of Computer Science, Rutgers University, 1980. 178 519. Tom M Mitchell. The discipline of machine learning. Technical Report CMU-ML-06-108, Carnegie Mellon University, School of Computer Science, Machine Learning, 2006. 178 520. Akshita Mittel and Purna Sowmya Munukutla. Visual transfer between Atari games using competitive reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019. 264 521. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016. 95, 103, 104, 105, 116, 272 522. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 41, 66, 72, 74, 75, 78, 79, 80, 83, 87, 150 523. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. 7, 66, 74, 75, 76, 77, 78, 79, 83, 84, 87, 179, 272 524. Thomas Moerland. Continuous Markov decision process and policy search. Lecture notes for the course reinforcement learning, Leiden University, 2021. 28, 72, 106, 108, 110, 283, 298 525. Thomas M Moerland. The Intersection of Planning and Learning. PhD thesis, Delft University of Technology, 2021. 275 526. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Efcient exploration with double uncertain value networks. arXiv preprint arXiv:1711.10789, 2017. 83 527. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. The potential of the return distribution for exploration in RL. arXiv preprint arXiv:1806.04242, 2018. 83 528. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. A framework for reinforcement learning and planning. arXiv preprint arXiv:2006.15009, 2020. 127 References 361 529. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement learning: A survey. arXiv preprint arXiv:2006.16712, 2020. 144 530. Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M Jonker. A0C: Alpha zero in continuous action space. arXiv preprint arXiv:1805.09613, 2018. 169, 187 531. Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M Jonker. Monte Carlo tree search for asymmetric trees. arXiv preprint arXiv:1805.09218, 2018. 187 532. Andrew William Moore. Efcient memory-based learning for robot control. Technical Report UCAM-CL-TR-209, University of Cambridge, UK, https://www.cl.cam.ac.uk/ techreports/UCAM-CL-TR-209.pdf, 1990. 55 533. Matej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artifcial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017. 214, 216, 223 534. Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multiagent populations. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 32, 2018. 212 535. Pol Moreno, Edward Hughes, Kevin R McKee, Bernardo Avila Pires, and Th\u00e9ophane Weber. Neural recursive belief states in multi-agent reinforcement learning. arXiv preprint arXiv:2102.02274, 2021. 208 536. David E Moriarty, Alan C Schultz, and John J Grefenstette. Evolutionary algorithms for reinforcement learning. Journal of Artifcial Intelligence Research, 11:241\u2013276, 1999. 223 537. Hossam Mossalam, Yannis M Assael, Diederik M Roijers, and Shimon Whiteson. Multiobjective deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016. 223 538. Hussain Mujtaba. Introduction to autoencoders. https://www.mygreatlearning.com/ blog/autoencoder/, 2020. 318 539. Sendhil Mullainathan and Richard H Thaler. Behavioral economics. Technical report, National Bureau of Economic Research, 2000. 223 540. Martin M\u00fcller. Computer Go. Artifcial Intelligence, 134(1-2):145\u2013179, 2002. 69 541. Matthias M\u00fcller-Brockhausen, Mike Preuss, and Aske Plaat. Procedural content generation: Better benchmarks for transfer reinforcement learning. In Conference on Games, 2021. 176, 268 542. Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine Learning, pages 2554\u20132563. PMLR, 2017. 254, 268 543. Yoshio Murase, Hitoshi Matsubara, and Yuzuru Hiraga. Automatic making of Sokoban problems. In Pacifc Rim International Conference on Artifcial Intelligence, pages 592\u2013600. Springer, 1996. 27, 56 544. Derick Mwiti. Reinforcement learning applications. https://neptune.ai/blog/ reinforcement-learning-applications. 4 545. Roger B Myerson. Game Theory. Harvard university press, 2013. 222 546. Ofr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efcient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pages 3307\u2013 3317, 2018. 231, 233, 241, 276 547. Ofr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pages 2775\u20132785, 2017. 107 548. Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W Chapman. Natural language processing: an introduction. Journal of the American Medical Informatics Association, 18(5):544\u2013 551, 2011. 276 549. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fne-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559\u20137566, 2018. 133, 144, 146 550. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, 2020. 321 362 References 551. Nantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet Kohli, Philip HS Torr, and Nicolas Usunier. Value propagation networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2018. 138, 144 552. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. Journal Machine Learning Research, 2020. 178, 275 553. Sylvia Nasar. A Beautiful Mind. Simon and Schuster, 2011. 222 554. John Nash. Non-cooperative games. Annals of mathematics, pages 286\u2013295, 1951. 222 555. John F Nash. Equilibrium points in \ud835\udc5b-person games. Proceedings of the National Academy of Sciences, 36(1):48\u201349, 1950. 222 556. John F Nash Jr. The bargaining problem. Econometrica: Journal of the econometric society, pages 155\u2013162, 1950. 222 557. Yu Nasu. Efciently updatable neural-network-based evaluation functions for computer shogi. The 28th World Computer Shogi Championship Appeal Document, 2018. 162 558. Richard E Neapolitan. Learning Bayesian networks. Pearson Prentice Hall, Upper Saddle River, NJ, 2004. 278 559. Andrew Y Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. In Proceedings of the Twenty-frst International Conference on Machine Learning, page 78. ACM, 2004. 320 560. Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, volume 99, pages 278\u2013287, 1999. 52 561. Alex Nichol, Joshua Achiam, and John Schulman. On frst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. 254, 259, 268, 269 562. Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C Smith, and Jelena Kova\u010devi\u0107. Generalized value iteration networks: Life beyond lattices. In Thirty-Second AAAI Conference on Artifcial Intelligence, 2018. 134, 139 563. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Actionconditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, pages 2863\u20132871, 2015. 138, 144 564. Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118\u20136128, 2017. 130, 131, 135, 138, 140, 144, 333 565. Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pages 2661\u20132670. PMLR, 2017. 268 566. Kyoung-Su Oh and Keechul Jung. GPU implementation of neural networks. Pattern Recognition, 37(6):1311\u20131314, 2004. 324 567. Chris Olah. Understanding LSTM networks. http://colah.github.io/posts/2015-08Understanding-LSTMs/, 2015. 313 568. Frans A Oliehoek. Decentralized POMDPs. In Reinforcement Learning, pages 471\u2013503. Springer, 2012. 223 569. Frans A Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer, 2016. 151, 197, 212, 223 570. Frans A Oliehoek, Matthijs TJ Spaan, Christopher Amato, and Shimon Whiteson. Incremental clustering and expansion for faster optimal planning in Dec-POMDPs. Journal of Artifcial Intelligence Research, 46:449\u2013509, 2013. 212 571. Shayegan Omidshafei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International Conference on Machine Learning, pages 2681\u20132690. PMLR, 2017. 212 572. Joseph O\u2019Neill, Barty Pleydell-Bouverie, David Dupret, and Jozsef Csicsvari. Play it again: reactivation of waking experience and memory. Trends in Neurosciences, 33(5):220\u2013229, 2010. 75 References 363 573. Santiago Ontan\u00f3n, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. A survey of real-time strategy game AI research and competition in StarCraft. IEEE Transactions on Computational Intelligence and AI in Games, 5(4):293\u2013311, 2013. 68, 69, 219, 223 574. David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of Artifcial Intelligence Research, 11:169\u2013198, 1999. 129, 144 575. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesv\u00e1ri, Satinder Singh, Benjamin Van Roy, Richard S. Sutton, David Silver, and Hado van Hasselt. Behaviour suite for reinforcement learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2020. 332 576. Pierre-Yves Oudeyer and Frederic Kaplan. How can we defne intrinsic motivation? In the 8th International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems. Lund University Cognitive Studies, Lund: LUCS, Brighton, 2008. 241 577. Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of computational approaches. Frontiers in Neurorobotics, 1:6, 2009. 237, 241 578. Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265\u2013 286, 2007. 176, 241 579. Charles Packer, Katelyn Gao, Jernej Kos, Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, and Dawn Song. Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018. 278 580. Mark M Palatucci, Dean A Pomerleau, Geofrey E Hinton, and Tom Mitchell. Zero-shot learning with semantic output codes. In Advances in Neural Information Processing Systems 22, 2009. 260, 268 581. Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, 2010. 247, 248, 251, 268 582. Aleksandr I Panov and Aleksey Skrynnik. Automatic formation of the structure of abstract machines in hierarchical reinforcement learning with state clustering. arXiv preprint arXiv:1806.05292, 2018. 241 583. Giuseppe Davide Paparo, Vedran Dunjko, Adi Makmal, Miguel Angel Martin-Delgado, and Hans J Briegel. Quantum speedup for active learning agents. Physical Review X, 4(3):031002, 2014. 188 584. Philip Paquette, Yuchen Lu, Steven Bocco, Max Smith, O-G Satya, Jonathan K Kummerfeld, Joelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multiagent gameplay. In Advances in Neural Information Processing Systems, pages 4476\u20134487, 2019. 208, 223 585. German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019. 248 586. Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015. 264 587. Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, pages 7487\u20137498. PMLR, 2020. 265 588. Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In Advances in Neural Information Processing Systems, pages 1043\u20131049, 1998. 231, 232, 241 589. Gian-Carlo Pascutto. Leela zero. https://github.com/leela-zero/leela-zero, 2017. 183 590. Alexander Pashevich, Danijar Hafner, James Davidson, Rahul Sukthankar, and Cordelia Schmid. Modulated policy hierarchies. arXiv preprint arXiv:1812.00025, 2018. 231 591. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas 364 References K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In Advances in Neural Information Processing Systems, pages 8024\u20138035, 2019. 304, 321 592. Shubham Pateria, Budhitama Subagdja, Ah-hweewee Tan, and Chai Quek. Hierarchical reinforcement learning: A comprehensive survey. ACM Computing Surveys (CSUR), 54(5):1\u2013 35, 2021. 231, 241 593. Judea Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving. AddisonWesley, Reading, MA, 1984. 9, 161 594. Judea Pearl and Dana Mackenzie. The Book of Why: the New Science of Cause and Efect. Basic Books, 2018. 9 595. Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(Oct):2825\u20132830, 2011. 259, 326 596. Jefrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, 2014. 249 597. Alexandre P\u00e9r\u00e9, S\u00e9bastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised learning of goal spaces for intrinsically motivated goal exploration. In International Conference on Learning Representations, 2018. 233 598. Karl Pertsch, Oleh Rybkin, Frederik Ebert, Shenghao Zhou, Dinesh Jayaraman, Chelsea Finn, and Sergey Levine. Long-horizon visual planning with goal-conditioned hierarchical predictors. In Advances in Neural Information Processing Systems, 2020. 241 599. Aske Plaat. De vlinder en de mier / The butterfy and the ant\u2014on modeling behavior in organizations. Inaugural lecture. Tilburg University, 2010. 223 600. Aske Plaat. Learning to Play: Reinforcement Learning and Games. Springer Verlag, Heidelberg, https://learningtoplay.net, 2020. 59, 64, 161, 163, 187 601. Aske Plaat, Walter Kosters, and Mike Preuss. High-accuracy model-based reinforcement learning, a survey. arXiv preprint arXiv:2107.08241, 2021. 128, 138, 144, 379 602. Aske Plaat, Jonathan Schaefer, Wim Pijls, and Arie De Bruin. Best-frst fxed-depth minimax algorithms. Artifcial Intelligence, 87(1-2):255\u2013293, 1996. 163 603. Matthias Plappert. Keras-RL. https://github.com/keras-rl/keras-rl, 2016. 77 604. Jordan B Pollack and Alan D Blair. Why did TD-gammon work? In Advances in Neural Information Processing Systems, pages 10\u201316, 1997. 75 605. Aditya Prasad. Lessons from implementing alphazero https://medium.com/oracledevs/ lessons-from-implementing-alphazero-7e36e9054191, 2018. 173 606. Lorien Y Pratt. Discriminability-based transfer between neural networks. In Advances in Neural Information Processing Systems, pages 204\u2013211, 1993. 248, 268 607. Lutz Prechelt. Automatic early stopping using cross validation: quantifying the criteria. Neural Networks, 11(4):761\u2013767, 1998. 321 608. Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55\u201369. Springer, 1998. 321 609. Doina Precup, Richard S Sutton, and Satinder P Singh. Planning with closed-loop macro actions. In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems, pages 70\u201376, 1997. 230, 241 610. David Premack and Guy Woodruf. Does the chimpanzee have a theory of mind? Behavioral and Brain Sciences, 1(4):515\u2013526, 1978. 208 611. Hugo M Proen\u00e7a and Matthijs van Leeuwen. Interpretable multiclass classifcation by mdl-based rule lists. Information Sciences, 512:1372\u20131393, 2020. 278 612. Max Pumperla and Kevin Ferguson. Deep Learning and the Game of Go. Manning, 2019. 187 613. J Ross Quinlan. Learning efcient classifcation procedures and their application to chess end games. In Machine Learning, pages 463\u2013482. Springer, 1983. 162 References 365 614. J Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81\u2013106, 1986. 278 615. S\u00e9bastien Racani\u00e8re, Theophane Weber, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri\u00e0 Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter W. Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 5690\u20135701, 2017. 57, 138, 139 616. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 276, 277, 320 617. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://openai.com/blog/languageunsupervised/, 2018. 245, 262, 263, 320 618. Roxana R\u0103dulescu, Patrick Mannion, Diederik M Roijers, and Ann Now\u00e9. Multi-objective multi-agent decision making: a utility-based analysis and survey. Autonomous Agents and Multi-Agent Systems, 34(1):1\u201352, 2020. 198 619. Jacob Rafati and David C Noelle. Learning representations in model-free hierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 33, pages 10009\u201310010, 2019. 229, 231, 237, 241 620. Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? Towards understanding the efectiveness of maml. In International Conference on Learning Representations, 2020. 258 621. Roberta Raileanu and Tim Rockt\u00e4schel. RIDE: rewarding impact-driven exploration for procedurally-generated environments. In International Conference on Learning Representations, 2020. 177, 234 622. Rajat Raina, Andrew Y Ng, and Daphne Koller. Constructing informative priors using transfer learning. In Proceedings of the 23rd international conference on Machine learning, pages 713\u2013720, 2006. 268 623. Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems, 2019. 254, 268 624. Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efcient of-policy meta-reinforcement learning via probabilistic context variables. In International Conference on Machine Learning, pages 5331\u20135340. PMLR, 2019. 265 625. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. 276, 320 626. Jette Randlov. Learning macro-actions in reinforcement learning. In Advances in Neural Information Processing Systems, pages 1045\u20131051, 1998. 241 627. J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https:// GitHub.com/FacebookResearch/Nevergrad, 2018. 259 628. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4295\u20134304. PMLR, 2018. 207 629. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017. 254, 255, 268 630. John R. Rice. The algorithm selection problem. Advances in Computers, 15(65-118):5, 1976. 259 631. Martin Riedmiller. Neural ftted Q iteration\u2014frst experiences with a data efcient neural reinforcement learning method. In European Conference on Machine Learning, pages 317\u2013328. Springer, 2005. 74, 87 632. Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527\u2013535, 1952. 47 366 References 633. Frank R\u00f6der, Manfred Eppe, Phuong DH Nguyen, and Stefan Wermter. Curious hierarchical actor-critic reinforcement learning. In International Conference on Artifcial Neural Networks, pages 408\u2013419. Springer, 2020. 235, 236, 276 634. Diederik M Roijers, Willem R\u00f6pke, Ann Now\u00e9, and Roxana R\u0103dulescu. On following paretooptimal policies in multi-objective planning and reinforcement learning. In Multi-Objective Decision Making Workshop, 2021. 198 635. Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-objective sequential decision-making. Journal of Artifcial Intelligence Research, 48:67\u2013113, 2013. 198 636. Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In International Conference on Machine Learning, pages 2152\u20132161, 2015. 268, 269, 276 637. Willem R\u00f6pke, Roxana Radulescu, Diederik M Roijers, and Ann Ann Now\u00e9. Communication strategies in multi-objective normal-form games. In Adaptive and Learning Agents Workshop 2021, 2021. 198 638. Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artifcial Intelligence, 61(3):203\u2013230, 2011. 168, 169 639. Denis Rothman. Transformers for Natural Language Processing. Packt Publishing, 2021. 263 640. Neil Rubens, Mehdi Elahi, Masashi Sugiyama, and Dain Kaplan. Active learning in recommender systems. In Recommender Systems Handbook, pages 809\u2013846. Springer, 2015. 177 641. Jonathan Rubin and Ian Watson. Computer poker: A review. Artifcial intelligence, 175(56):958\u2013987, 2011. 223 642. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016. 85 643. Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206\u2013215, 2019. 278 644. Ben Ruijl, Jos Vermaseren, Aske Plaat, and Jaap van den Herik. Hepgame and the simplifcation of expressions. arXiv preprint arXiv:1405.6369, 2014. 188 645. Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems. Technical report, University of Cambridge, Department of Engineering Cambridge, UK, 1994. 45, 50, 59 646. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. 245, 262 647. Stuart J Russell and Peter Norvig. Artifcial intelligence: a modern approach. Pearson Education Limited, Malaysia, 2016. 14, 18, 56, 57, 59 648. Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. Found. Trends Mach. Learn., 11(1):1\u201396, 2018. 48 649. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference on Learning Representations, 2019. 268 650. Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic defnitions and new directions. Contemporary Educational Psychology, 25(1):54\u201367, 2000. 241 651. Jordi Sabater and Carles Sierra. Reputation and social network analysis in multi-agent systems. In Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems: Part 1, pages 475\u2013482, 2002. 216 652. Sumit Saha. A comprehensive guide to convolutional neural networks\u2014the ELI5 way. https://towardsdatascience.com/a-comprehensive-guide-to-convolutionalneural-networks-the-eli5-way-3bd2b1164a53. Towards Data Science, 2018. 305, 312 653. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017. 203, 212, 223, 277 654. Brian Sallans and Geofrey E Hinton. Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5:1063\u20131088, Aug 2004. 74, 75, 86 References 367 655. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS \u201919, Montreal, 2019. 207, 219, 223, 224 656. Jason Sanders and Edward Kandrot. CUDA by example: an introduction to general-purpose GPU programming. Addison-Wesley Professional, 2010. 324 657. Tuomas Sandholm. The state of solving large incomplete-information games, and application to poker. AI Magazine, 31(4):13\u201332, 2010. 223 658. Tuomas Sandholm. Abstraction for solving large incomplete-information games. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 29, 2015. 204 659. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, pages 1842\u20131850, 2016. 268 660. Vieri Giuliano Santucci, Pierre-Yves Oudeyer, Andrew Barto, and Gianluca Baldassarre. Intrinsically motivated open-ended learning in autonomous robots. Frontiers in Neurorobotics, 13:115, 2020. 277 661. Steve Schaefer. Mathematical recreations. http://www.mathrec.org/old/2002jan/ solutions.html, 2002. 151 662. Jonathan Schaefer. One Jump Ahead: Computer Perfection at Checkers. Springer Science & Business Media, 2008. 64 663. Jonathan Schaefer, Robert Lake, Paul Lu, and Martin Bryant. Chinook, the world manmachine checkers champion. AI Magazine, 17(1):21, 1996. 57 664. Jonathan Schaefer, Aske Plaat, and Andreas Junghanns. Unifying single-agent and twoplayer search. Information Sciences, 135(3-4):151\u2013175, 2001. 188 665. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pages 1312\u20131320, 2015. 230, 241 666. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016. 80, 82 667. Tom Schaul and J\u00fcrgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010. 247, 267, 268 668. Daniel Schleich, Tobias Klamt, and Sven Behnke. Value iteration networks on multiple levels of abstraction. In Robotics: Science and Systems XV, University of Freiburg, Freiburg im Breisgau, Germany, 2019. 135 669. J\u00fcrgen Schmidhuber. Evolutionary Principles in Self-Referential Learning, or on Learning how to Learn: the Meta-Meta-. . . Hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987. 253, 268 670. J\u00fcrgen Schmidhuber. Making the world diferentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Technical report, Inst. f\u00fcr Informatik, 1990. 132 671. J\u00fcrgen Schmidhuber. An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In 1990 IJCNN International Joint Conference on Neural Networks, pages 253\u2013258. IEEE, 1990. 135, 139, 144 672. J\u00fcrgen Schmidhuber. Curious model-building control systems. In Proceedings International Joint Conference on Neural Networks, pages 1458\u20131463, 1991. 175 673. J\u00fcrgen Schmidhuber. Learning to generate sub-goals for action sequences. In Artifcial neural networks, pages 967\u2013972, 1991. 227 674. J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222\u2013227, 1991. 277 675. J\u00fcrgen Schmidhuber, F Gers, and Douglas Eck. Learning nonregular languages: A comparison of simple recurrent networks and LSTM. Neural Computation, 14(9):2039\u20132041, 2002. 315 676. J\u00fcrgen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple principles of metalearning. Technical report, IDSIA, 1996. 268 368 References 677. Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Kernel principal component analysis. In International Conference on Artifcial Neural Networks, pages 583\u2013588. Springer, 1997. 15 678. Nicol N Schraudolph, Peter Dayan, and Terrence J Sejnowski. Temporal diference learning of position evaluation in the game of Go. In Advances in Neural Information Processing Systems, pages 817\u2013824, 1994. 75 679. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering Atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020. 138, 140, 144, 275 680. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and ofine reinforcement learning by planning with a learned model. arXiv preprint arXiv:2104.06294, 2021. 141, 275 681. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015. 95, 104, 105, 116, 258 682. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations, 2016. 112 683. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 95, 106, 116, 272 684. Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural Networks, 16(1):5\u20139, 2003. 267, 268 685. Marco Scutari. Learning Bayesian networks with the bnlearn R package. Journal of Statistical Software, 35(i03), 2010. 278 686. Thomas D Seeley. The honey bee colony as a superorganism. American Scientist, 77(6):546\u2013 553, 1989. 191 687. Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks and symbolic AI. Nature, 555(7698):604, 2018. 188, 275 688. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, 2020. 130, 138, 140, 144 689. Oliver G Selfridge, Richard S Sutton, and Andrew G Barto. Training and tracking in robotics. In International Joint Conference on Artifcial Intelligence, pages 670\u2013672, 1985. 175 690. Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Z\u00eddek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using potentials from deep learning. Nature, 577(7792):706\u2013710, 2020. 188 691. Burr Settles. Active learning literature survey. Technical report, University of WisconsinMadison Department of Computer Sciences, 2009. 177 692. Noor Shaker, Julian Togelius, and Mark J Nelson. Procedural Content Generation in Games. Springer, 2016. 59, 176, 265 693. Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based POMDP solvers. Autonomous Agents and Multi-Agent Systems, 27(1):1\u201351, 2013. 151 694. Claude E Shannon. Programming a computer for playing chess. In Computer Chess Compendium, pages 2\u201313. Springer, 1988. 7, 152 695. Lloyd S Shapley. Stochastic games. In Proceedings of the National Academy of Sciences, volume 39, pages 1095\u20131100, 1953. 194 696. Yaron Shoham and Gal Elidan. Solving Sokoban with forward-backward reinforcement learning. In Proceedings of the International Symposium on Combinatorial Search, volume 12, pages 191\u2013193, 2021. 242 697. Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press, 2008. 193 References 369 698. Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a critical survey. Technical report, Stanford University, 2003. 222 699. Pranav Shyam, Shubham Gupta, and Ambedkar Dukkipati. Attentive recurrent comparators. In International Conference on Machine Learning, pages 3173\u20133181. PMLR, 2017. 268 700. Robin C Sickles and Valentin Zelenyuk. Measurement of productivity and efciency. Cambridge University Press, 2019. 197 701. Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond learning algorithms. In 2013 AAAI Spring Symposium Series, 2013. 246 702. David Silver. Reinforcement learning and simulation based search in the game of Go. PhD thesis, University of Alberta, 2009. 167 703. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016. 57, 150, 172, 174, 178, 179, 182, 187, 246 704. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419):1140\u20131144, 2018. 144, 178, 181, 182, 183, 187, 275 705. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning, pages 387\u2013395, 2014. 108, 132 706. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017. 75, 144, 150, 169, 171, 172, 173, 174, 178, 179, 180, 181, 182, 183, 187, 246, 275 707. David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artifcial Intelligence, page 103535, 2021. 17, 217 708. David Silver, Richard S Sutton, and Martin M\u00fcller. Reinforcement learning of local shape in the game of Go. In International Joint Conference on Artifcial Intelligence, volume 7, pages 1053\u20131058, 2007. 187 709. David Silver, Richard S Sutton, and Martin M\u00fcller. Temporal-diference search in computer Go. Machine Learning, 87(2):183\u2013219, 2012. 131 710. David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris. The predictron: End-to-end learning and planning. In Proceedings of the 34th International Conference on Machine Learning, pages 3191\u20133199, 2017. 130, 135, 136, 138, 144 711. David Sim\u00f5es, Nuno Lau, and Lu\u00eds Paulo Reis. Multi agent deep learning with cooperative communication. Journal of Artifcial Intelligence and Soft Computing Research, 10, 2020. 208 712. Satinder Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated reinforcement learning. Technical report, University of Amherst, Mass, Department of Computer Science, 2005. 237, 238, 277 713. Satinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010. 223 714. David J Slate and Lawrence R Atkin. Chess 4.5\u2014the northwestern university chess program. In Chess skill in Man and Machine, pages 82\u2013118. Springer, 1983. 163 715. Gillian Smith. An analog history of procedural content generation. In Foundations of Digital Games, 2015. 177 716. Stephen J Smith, Dana Nau, and Tom Throop. Computer bridge: A big win for AI planning. AI magazine, 19(2):93\u201393, 1998. 208 370 References 717. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pages 4077\u20134087, 2017. 263, 268 718. Doron Sobol, Lior Wolf, and Yaniv Taigman. Visual analogies between Atari games for studying transfer learning in RL. arXiv preprint arXiv:1807.11074, 2018. 264 719. Sungryull Sohn, Junhyuk Oh, and Honglak Lee. Hierarchical reinforcement learning for zero-shot generalization with subtask dependencies. In Advances in Neural Information Processing Systems, pages 7156\u20137166, 2018. 268, 276 720. Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pages 5887\u20135896. PMLR, 2019. 207 721. Fengguang Song and Jack Dongarra. Scaling up matrix computations on shared-memory manycore systems with 1000 CPU cores. In Proceedings of the 28th ACM International Conference on Supercomputing, pages 333\u2013342. ACM, 2014. 324 722. H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov, Martin A. Riedmiller, and Matthew M. Botvinick. V-MPO: on-policy maximum a posteriori policy optimization for discrete and continuous control. In International Conference on Learning Representations, 2019. 265 723. Mei Song, A Montanari, and P Nguyen. A mean feld view of the landscape of two-layers neural networks. In Proceedings of the National Academy of Sciences, volume 115, pages E7665\u2013E7671, 2018. 307 724. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. In International Conference on Machine Learning, pages 4739\u20134748, 2018. 135 725. Nitish Srivastava, Geofrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overftting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014. 320 726. Eric Steinberger. Single deep counterfactual regret minimization. arXiv preprint arXiv:1901.07621, 2019. 333 727. Martin Stolle and Doina Precup. Learning options in reinforcement learning. In International Symposium on Abstraction, Reformulation, and Approximation, pages 212\u2013223. Springer, 2002. 230, 231, 232 728. Lise Stork, Andreas Weber, Jaap van den Herik, Aske Plaat, Fons Verbeek, and Katherine Wolstencroft. Large-scale zero-shot learning in the wild: Classifying zoological illustrations. Ecological Informatics, 62:101222, 2021. 268 729. Darin Straus. Alphazero implementation and tutorial. https://towardsdatascience.com/ alphazero-implementation-and-tutorial-f4324d65fdfc, 2018. 173 730. Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks. IEEE Transactions on Evolutionary Computation, 2019. 316 731. Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jef Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017. 223 732. Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam, and Rob Fergus. Learning goal embeddings via self-play for hierarchical reinforcement learning. arXiv preprint arXiv:1811.09083, 2018. 231, 233 733. Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 30, 2016. 252 734. Wenyu Sun and Ya-Xiang Yuan. Optimization Theory and Methods: Nonlinear Programming, volume 1. Springer Science & Business Media, 2006. 104 735. Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition networks for cooperative multi-agent learning. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, 2017. 207, 241 References 371 736. Peter Sunehag, Guy Lever, Siqi Liu, Josh Merel, Nicolas Heess, Joel Z Leibo, Edward Hughes, Tom Eccles, and Thore Graepel. Reinforcement learning agents acquire focking and symbiotic behaviour in simulated ecosystems. In Artifcial Life Conference Proceedings, pages 103\u2013110. MIT Press, 2019. 211 737. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1199\u20131208, 2018. 263, 268 738. Ilya Sutskever and Vinod Nair. Mimicking Go experts with convolutional neural networks. In International Conf. on Artifcial Neural Networks, pages 101\u2013110. Springer, 2008. 75 739. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014. 319 740. Richard S Sutton. Learning to predict by the methods of temporal diferences. Machine Learning, 3(1):9\u201344, 1988. 45, 59 741. Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning Proceedings 1990, pages 216\u2013224. Elsevier, 1990. 121, 123, 124, 125, 144 742. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991. 144 743. Richard S Sutton and Andrew G Barto. Reinforcement learning, An Introduction, Second Edition. MIT Press, 2018. 7, 27, 31, 51, 59, 66, 70, 73, 99, 122, 124, 143, 168 744. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, pages 1057\u20131063, 2000. 116 745. Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning. Artifcial Intelligence, 112(12):181\u2013211, 1999. 229, 230, 235, 240, 241, 276 746. Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efcient processing of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295\u20132329, 2017. 324 747. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2013. 316 748. Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances in Neural Information Processing Systems, pages 2154\u20132162, 2016. 134, 135, 138, 139, 144 749. Oskari Tammelin. Solving large imperfect information games using CFR+. arXiv preprint arXiv:1407.5042, 2014. 203, 204 750. Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4):e0172395, 2017. 208, 222 751. Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International Conference on Machine Learning, pages 330\u2013337, 1993. 206 752. Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to Data Mining. Pearson Education India, 2016. 248 753. Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang Zhang, Hangtian Jia, Chunxu Ren, Yan Zheng, Zhaopeng Meng, Changjie Fan, and Li Wang. Hierarchical deep multiagent reinforcement learning with temporal abstraction. arXiv preprint arXiv:1809.09332, 2018. 238 754. Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi, and Aditya Nori. Adaptive neural trees. In International Conference on Machine Learning, pages 6166\u20136175, 2019. 278 755. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. 114, 116, 262, 332 372 References 756. Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4906\u20134913, 2012. 138, 144 757. Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm_control: Software and tasks for continuous control. arXiv preprint arXiv:2006.12983, 2020. 114 758. Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633\u20131685, 2009. 268 759. Shoshannah Tekofsky, Pieter Spronck, Martijn Goudbeek, Aske Plaat, and Jaap van den Herik. Past our prime: A study of age and play style development in Battlefeld 3. IEEE Transactions on Computational Intelligence and AI in Games, 7(3):292\u2013303, 2015. 223 760. Justin K Terry and Benjamin Black. Multiplayer support for the arcade learning environment. arXiv preprint arXiv:2009.09341, 2020. 221 761. Justin K Terry, Benjamin Black, Ananth Hari, Luis Santos, Clemens Diefendahl, Niall L Williams, Yashas Lokesh, Caroline Horsch, and Praveen Ravi. Pettingzoo: Gym for multiagent reinforcement learning. arXiv preprint arXiv:2009.14471, 2020. 221, 242 762. Gerald Tesauro. Neurogammon wins Computer Olympiad. Neural Computation, 1(3):321\u2013323, 1989. 74, 149 763. Gerald Tesauro. TD-gammon: A self-teaching backgammon program. In Applications of Neural Networks, pages 267\u2013285. Springer, 1995. 46, 74, 75, 178, 275 764. Gerald Tesauro. Temporal diference learning and TD-Gammon. Communications of the ACM, 38(3):58\u201368, 1995. 74, 150 765. Gerald Tesauro. Programming backgammon using self-teaching neural nets. Artifcial Intelligence, 134(1-2):181\u2013199, 2002. 149 766. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, and Shie Mannor. A deep hierarchical approach to lifelong learning in minecraft. In Proceedings of the AAAI Conference on Artifcial Intelligence, volume 31, 2017. 231, 233 767. Marc Teyssier and Daphne Koller. Ordering-based search: A simple and efective algorithm for learning Bayesian networks. arXiv preprint arXiv:1207.1429, 2012. 278 768. Shantanu Thakoor, Surag Nair, and Megha Jhunjhunwala. Learning to play othello without human knowledge. Stanford University CS238 Final Project Report, 2017. 173, 183, 333 769. Sergios Theodoridis and Konstantinos Koutroumbas. Pattern recognition. Academic Press, 1999. 301 770. William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933. 48 771. Sebastian Thrun. Learning to play the game of chess. In Advances in Neural Information Processing Systems, pages 1069\u20131076, 1995. 162 772. Sebastian Thrun. Is learning the \ud835\udc5b-th thing any easier than learning the frst? In Advances in Neural Information Processing Systems, pages 640\u2013646. Morgan Kaufman, 1996. 250, 253 773. Sebastian Thrun. Explanation-based neural network learning: A lifelong learning approach, volume 357. Springer, 2012. 248 774. Sebastian Thrun and Lorien Pratt. Learning to Learn. Springer, 2012. 248, 253, 260, 268 775. Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot image classifcation: a good embedding is all you need? In European Conference on Computer Vision, 2020. 264, 268 776. Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, and C Lawrence Zitnick. ELF: An extensive, lightweight and fexible research platform for real-time strategy games. In Advances in Neural Information Processing Systems, pages 2659\u20132669, 2017. 173, 183, 333 777. Yuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, and C. Lawrence Zitnick. ELF OpenGo. https://github.com/pytorch/ELF, 2018. 184 778. Yuandong Tian and Yan Zhu. Better computer Go player with neural network and long-term prediction. In International Conference on Learning Representations, 2016. 184 779. Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in Neural Information Processing Systems, pages 1369\u20131376, 2007. 107 References 373 780. Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5026\u20135033, 2012. 41, 92, 93, 116, 262, 332 781. Julian Togelius, Alex J Champandard, Pier Luca Lanzi, Michael Mateas, Ana Paiva, Mike Preuss, and Kenneth O Stanley. Procedural content generation: Goals, challenges and actionable steps. In Artifcial and Computational Intelligence in Games. Schloss DagstuhlLeibniz-Zentrum f\u00fcr Informatik, 2013. 59, 177, 223 782. Tatiana Tommasi, Martina Lanzi, Paolo Russo, and Barbara Caputo. Learning the roots of visual domain shift. In European Conference on Computer Vision, pages 475\u2013482. Springer, 2016. 247, 251, 252 783. Armon Toubman, Jan Joris Roessingh, Pieter Spronck, Aske Plaat, and Jaap Van Den Herik. Dynamic scripting with team coordination in air combat simulation. In International Conference on Industrial, Engineering and other Applications of Applied Intelligent Systems, pages 440\u2013449. Springer, 2014. 223 784. Thomas Trenner. Beating kuhn poker with CFR using python. https://ai.plainenglish. io/building-a-poker-ai-part-6-beating-kuhn-poker-with-cfr-using-python1b4172a6ab2d. 204, 205 785. Eleni Triantafllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020. 262, 263, 268, 332 786. John Tromp. Number of legal Go states. http://tromp.github.io/go/legal.html, 2016. 69 787. John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-difference learning with function approximation. In Advances in Neural Information Processing Systems, pages 1075\u2013 1081, 1997. 66, 73, 86 788. Alan M Turing. Digital Computers Applied to Games. Pitman & Sons, 1953. 7, 152, 161, 163 789. Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Z Leibo, and Thore Graepel. A generalised method for empirical game theoretic analysis. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, 2018. 194 790. Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics, challenges, and prospects. AI Magazine, 33(3):41\u201341, 2012. 222 791. Paul Tylkin, Goran Radanovic, and David C Parkes. Learning robust helpful behaviors in two-player cooperative atari environments. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pages 1686\u20131688, 2021. 221 792. Eric Tzeng, Judy Hofman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7167\u20137176, 2017. 252 793. Wiebe Van der Hoek and Michael Wooldridge. Multi-agent systems. Foundations of Artifcial Intelligence, 3:887\u2013928, 2008. 223 794. Michiel Van Der Ree and Marco Wiering. Reinforcement learning in the game of Othello: learning against a fxed opponent and learning from self-play. In IEEE Adaptive Dynamic Programming and Reinforcement Learning, pages 108\u2013115. IEEE, 2013. 188 795. Max J van Duijn. The Lazy Mindreader: a Humanities Perspective on Mindreading and MultipleOrder Intentionality. PhD thesis, Leiden University, 2016. 208 796. Max J Van Duijn, Ineke Sluiter, and Arie Verhagen. When narrative takes over: The representation of embedded mindstates in Shakespeare\u2019s Othello. Language and Literature, 24(2):148\u2013166, 2015. 208 797. Max J Van Duijn and Arie Verhagen. Recursive embedding of viewpoints, irregularity, and the role for a fexible framework. Pragmatics, 29(2):198\u2013225, 2019. 208 798. Frank Van Harmelen, Vladimir Lifschitz, and Bruce Porter. Handbook of Knowledge Representation. Elsevier, 2008. 232 799. Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. arXiv:1812.02648, 2018. 75 374 References 800. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with Double Q-Learning. In AAAI, volume 2, page 5. Phoenix, AZ, 2016. 80, 81, 82 801. Matthijs Van Leeuwen and Arno Knobbe. Diverse subgroup set discovery. Data Mining and Knowledge Discovery, 25(2):208\u2013242, 2012. 15 802. Kristof Van Mofaert and Ann Now\u00e9. Multi-objective reinforcement learning using sets of pareto dominating policies. Journal of Machine Learning Research, 15(1):3483\u20133512, 2014. 198, 223 803. Gerard JP Van Westen, J\u00f6rg K Wegner, Peggy Geluykens, Leen Kwanten, Inge Vereycken, Anik Peeters, Adriaan P IJzerman, Herman WT van Vlijmen, and Andreas Bender. Which compound to select in lead optimization? Prospectively validated proteochemometric models guide preclinical development. PloS One, 6(11):e27518, 2011. 188 804. Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018. 267, 268 805. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017. 320 806. Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado van Hasselt, David Silver, and Satinder Singh. Discovery of options via meta-learned subgoals. arXiv preprint arXiv:2102.06741, 2021. 241 807. Alfredo Vellido, Jos\u00e9 David Mart\u00edn-Guerrero, and Paulo JG Lisboa. Making machine learning models interpretable. In ESANN, volume 12, pages 163\u2013172, 2012. 278 808. Jos AM Vermaseren. New features of form. arXiv preprint math-ph/0010025, 2000. 169 809. Jean-Philippe Vert, Koji Tsuda, and Bernhard Sch\u00f6lkopf. A primer on kernel methods. In Kernel Methods in Computational Biology, volume 47, pages 35\u201370. MIT press Cambridge, MA, 2004. 248 810. Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, and Koray Kavukcuoglu. Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing Systems, pages 3486\u20133494, 2016. 231, 233 811. Alexander Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In Intl Conf on Machine Learning, pages 3540\u20133549. PMLR, 2017. 231, 232, 241 812. Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artifcial Intelligence Review, 18(2):77\u201395, 2002. 259, 268 813. Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, \u00c7aglar G\u00fcl\u00e7ehre, Ziyu Wang, Tobias Pfaf, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019. 7, 69, 214, 219, 220, 223, 238, 277 814. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630\u20133638, 2016. 262, 263, 268, 332 815. Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John P. Agapiou, Julian Schrittwieser, John Quan, Stephen Gafney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. arXiv:1708.04782, 2017. 223, 332 816. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164, 2015. 314 817. Vanessa Volz, Jacob Schrum, Jialin Liu, Simon M Lucas, Adam Smith, and Sebastian Risi. Evolving mario levels in the latent space of a deep convolutional generative adversarial References 375 network. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 221\u2013228, 2018. 277 818. John Von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, 1944. 194, 222 819. John Von Neumann, Oskar Morgenstern, and Harold William Kuhn. Theory of Games and Economic Behavior (commemorative edition). Princeton University Press, 2007. 194 820. Jilles Vreeken, Matthijs Van Leeuwen, and Arno Siebes. Krimp: mining itemsets that compress. Data Mining and Knowledge Discovery, 23(1):169\u2013214, 2011. 15 821. Loc Vu-Quoc. Neuron and myelinated axon. https://commons.wikimedia.org/w/index. php?curid=72816083, 2018. 303, 304 822. Douglas Walker and Graham Walker. The Ofcial Rock Paper Scissors Strategy Guide. Simon and Schuster, 2004. 223 823. Hui Wang, Michael Emmerich, Mike Preuss, and Aske Plaat. Alternative loss functions in AlphaZero-like self-play. In 2019 IEEE Symposium Series on Computational Intelligence (SSCI), pages 155\u2013162, 2019. 172, 188 824. Hui Wang, Mike Preuss, Michael Emmerich, and Aske Plaat. Tackling Morpion Solitaire with AlphaZero-like Ranked Reward reinforcement learning. In 22nd International Symposium on Symbolic and Numeric Algorithms for Scientifc Computing, SYNASC 2020, Timisoara, Romania, 2020. 177 825. Jane X. Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Peter Choy, Mary Cassin, Malcolm Reynolds, H. Francis Song, Gavin Buttimore, David P. Reichert, Neil C. Rabinowitz, Loic Matthey, Demis Hassabis, Alexander Lerchner, and Matthew Botvinick. Alchemy: A structured task distribution for meta-reinforcement learning. arXiv:2102.02926, 2021. 262, 265, 268, 332 826. Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R\u00e9mi Munos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. 254, 256, 268 827. Panqu Wang and Garrison W Cottrell. Basic level categorization facilitates visual object recognition. arXiv preprint arXiv:1511.04103, 2015. 178 828. Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. arXiv:1907.02057, 2019. 123, 139, 144 829. Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys, 53(3):1\u201334, 2020. 253 830. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995\u20132003, 2016. 80, 82, 85 831. Christopher JCH Watkins. Learning from Delayed Rewards. PhD thesis, King\u2019s College, Cambridge, 1989. 45, 50, 51, 59, 70, 71 832. Eddie Weill. LeNet in Keras on Github. https://github.com/eweill/keras-deepcv/ tree/master/models/classification. 309 833. Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning: Theory and experiments with deep networks. In International Conference on Machine Learning, pages 5235\u20135243, 2018. 176 834. Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big data, 3(1):1\u201340, 2016. 252, 268 835. Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In International Conference on Learning Representations, 2019. 208 836. Lilian Weng. Meta-learning: Learning to learn fast. Lil\u2019Log https://lilianweng.github. io/lil-log/2018/11/30/meta-learning.html, November 2018. 267, 268 837. Lilian Weng. Curriculum for reinforcement learning https://lilianweng.github.io/ lil-log/2020/01/29/curriculum-for-reinforcement-learning.html. Lil\u2019Log, January 2020. 178, 188 376 References 838. Shimon Whiteson. Evolutionary computation for reinforcement learning. In Marco A. Wiering and Martijn van Otterlo, editors, Reinforcement Learning, volume 12 of Adaptation, Learning, and Optimization, pages 325\u2013355. Springer, 2012. 223 839. Shimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against evaluation overftting in empirical reinforcement learning. In 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 120\u2013127. IEEE, 2011. 278 840. Marco A Wiering. Self-play and using an expert to learn to play backgammon with temporal diference learning. JILSA, 2(2):57\u201368, 2010. 188 841. Marco A Wiering, Maikel Withagen, and M\u0103d\u0103lina M Drugan. Model-based multi-objective reinforcement learning. In 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 1\u20136. IEEE, 2014. 223 842. Daan Wierstra, Tom Schaul, Jan Peters, and J\u00fcrgen Schmidhuber. Natural evolution strategies. In IEEE Congress on Evolutionary Computation, pages 3381\u20133387, 2008. 223 843. Nick Wilkinson and Matthias Klaes. An Introduction to Behavioral Economics. Macmillan International Higher Education, 2017. 223 844. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229\u2013256, 1992. 95, 97, 98, 116 845. Ian H Witten. The apparent confict between estimation and control\u2014a survey of the two-armed bandit problem. Journal of the Franklin Institute, 301(1-2):161\u2013189, 1976. 47 846. Annie Wong, Thomas B\u00e4ck, Anna V. Kononova, and Aske Plaat. Deep multiagent reinforcement learning: Challenges and directions. Artifcial Intelligence Review, 2022. 46, 206, 208, 222 847. Michael Wooldridge. An Introduction to Multiagent Systems. Wiley, 2009. 193, 223 848. Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, and Thomas W Malone. Evidence for a collective intelligence factor in the performance of human groups. science, 330(6004):686\u2013688, 2010. 191, 211, 216, 223 849. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trustregion method for deep reinforcement learning using kronecker-factored approximation. In Advances in Neural Information Processing Systems, pages 5279\u20135288, 2017. 103 850. Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Addressing appearance change in outdoor robotics with adversarial domain adaptation. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1551\u20131558. IEEE, 2017. 252 851. Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9):2251\u20132265, 2018. 268, 276 852. Yuchen Xiao, Joshua Hofman, and Christopher Amato. Macro-action-based deep multi-agent reinforcement learning. In Conference on Robot Learning, pages 1146\u20131161. PMLR, 2020. 241 853. Wayne Xiong, Lingfeng Wu, Fil Alleva, Jasha Droppo, Xuedong Huang, and Andreas Stolcke. The microsoft 2017 conversational speech recognition system 2017 conversational speech recognition system. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5934\u20135938. IEEE, 2018. 307 854. Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Satzilla: portfolio-based algorithm selection for sat. Journal of Artifcial Intelligence Research, 32:565\u2013606, 2008. 259 855. Sijia Xu, Hongyu Kuang, Zhuang Zhi, Renjie Hu, Yang Liu, and Huyang Sun. Macro action selection with deep reinforcement learning in starcraft. In AAAI Artifcial Intelligence and Interactive Digital Entertainment, volume 15, pages 94\u201399, 2019. 241 856. Wen Xu, Jing He, and Yanfeng Shu. Transfer learning and deep domain adaptation. In Advances in Deep Learning. IntechOpen, 2020. 252 857. Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8261\u20138265. IEEE, 2019. 316, 317 858. Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020. 222 References 377 859. Zhao Yang, Mike Preuss, and Aske Plaat. Transfer learning and curriculum learning in sokoban. arXiv preprint arXiv:2105.11702, 2021. 59, 268 860. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering Atari games with limited data. arXiv preprint arXiv:2111.00210, 2021. 141 861. Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 7343\u20137353, 2018. 254, 268 862. Jason Yosinski, Jef Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Neural Information Processing Systems, pages 3320\u20133328, 2014. 249 863. Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising efectiveness of PPO in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021. 207 864. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020. 114, 262, 264, 265, 268, 269, 332 865. Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision, pages 818\u2013833. Springer, 2014. 245 866. Qinsong Zeng, Jianchang Zhang, Zhanpeng Zeng, Yongsheng Li, Ming Chen, and Sifan Liu. PhoenixGo. https://github.com/Tencent/PhoenixGo, 2018. 173, 183, 184 867. Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overftting and generalization in continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018. 278, 321 868. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021. 278 869. Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overftting in deep reinforcement learning. arXiv preprint arXiv:1804.06893, 2018. 278 870. Kaiqing Zhang, Zhuoran Yang, and Tamer Ba\u015far. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019. 194, 195 871. Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning, pages 5872\u20135881. PMLR, 2018. 212 872. Lei Zhang. Transfer adaptation learning: A decade survey. arXiv:1903.04687, 2019. 252, 268 873. Lunjun Zhang, Ge Yang, and Bradly C Stadie. World model as a graph: Learning latent landmarks for planning. In International Conference on Machine Learning, pages 12611\u201312620. PMLR, 2021. 132, 138, 233 874. Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J Johnson, and Sergey Levine. Solar: Deep structured representations for model-based reinforcement learning. In International Conference on Machine Learning, pages 7444\u20137453, 2019. 144 875. Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint arXiv:1712.01275, 2017. 75, 76, 86 876. Wenshuai Zhao, Jorge Pe\u00f1a Queralta, and Tomi Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pages 737\u2013744. IEEE, 2020. 279 877. Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A deep bayesian policy reuse approach against non-stationary agents. In 32nd Neural Information Processing Systems, pages 962\u2013972, 2018. 208 878. Neng-Fa Zhou and Agostino Dovier. A tabled Prolog program for solving Sokoban. Fundamenta Informaticae, 124(4):561\u2013575, 2013. 27, 56 879. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2020. 248, 268 880. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008. 107 ",
    "References": "References 881. Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In Advances in Neural Information Processing Systems, pages 1729\u20131736, 2008. 203, 223, 333 882. Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, pages 7693\u20137702. PMLR, 2019. 267 List of Tables 1.1 The Constituents of Deep Reinforcement Learning . . . . . . . . . . . . . . . 2 1.2 (Input/output)-Pairs for a Supervised Classifcation Problem. . . . . . . 15 1.3 Supervised vs. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.1 Tabular Value-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.1 Deep Value-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.1 Policy-based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 5.1 Diference between Planning and Learning . . . . . . . . . . . . . . . . . . . . . . 127 5.2 Model-Based Reinforcement Learning Approaches [601] . . . . . . . . . . 138 6.1 Characteristics of games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 6.2 Self-Play Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 6.3 Self-learning environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 7.1 Prisoner\u2019s Dilemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 7.2 Multi-Agent Game Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214 8.1 Hierarchical Reinforcement Learning Approaches . . . . . . . . . . . . . . . . 231 9.1 Diferent kinds of supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 9.2 Meta Reinforcement Learning Approaches [364] . . . . . . . . . . . . . . . . . 254 9.3 Meta-Learning Datasets and Environments . . . . . . . . . . . . . . . . . . . . . . 262 A.1 Comparison of discrete and continuous probability distributions . . . 289 B.1 Confusion Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 B.2 Observations, Variables, Relative Dimensionality, Overftting . . . . . . 302 B.3 Functions that are Frequently Approximated . . . . . . . . . . . . . . . . . . . . . 308 C.1 Reinforcement Learning Environments . . . . . . . . . . . . . . . . . . . . . . . . . . 332 379 380 List of Tables C.2 Agent Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 Glossary \ud835\udc34 action in a state. 30 \ud835\udc34 advantage function in actor critic. 101 \ud835\udc36\ud835\udc5d exploration/exploitation constant in MCTS; high is more exploration. 169 \ud835\udc37 dataset. 297 \ud835\udc3c\ud835\udf14 initiation set for option \ud835\udf14. 229 \ud835\udc44 state-action value. 37 \ud835\udc45 reward. 33 \ud835\udc46 state. 29 \ud835\udc47 transition. 31 \ud835\udc49 value. 36 \u03a9 set of options \ud835\udf14 (hierarchical reinforcement learning). 229 \ud835\udefc learning rate. 46 \ud835\udefd\ud835\udf14 (\ud835\udc60) termination condition for option \ud835\udf14 at state \ud835\udc60. 229 \ud835\udf16 -greedy exploration/exploitation rule that selects an \ud835\udf16 fraction of random exploration actions. 48 \ud835\udefe discount rate, to reduce the importance of future rewards. 33 L loss function. 306 T\ud835\udc56 base-learning task T\ud835\udc56 = (\ud835\udc37\ud835\udc56,\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b, L\ud835\udc56), part of a meta-learning task. 254 \ud835\udf14 option (hierarchical reinforcement learning). 229 \ud835\udf14 hyperparameters (meta-learning). 254 381 382 Glossary \ud835\udf19 parameters for the value network in actor critic (as opposed to \ud835\udf03, the policy parameters). 100 \ud835\udf0b policy. 33 \ud835\udf0f trajectory, trace, episode, sequence. 35 \ud835\udf03 parameters, weights in the neural network. 69, 100, 299 A2C advantage actor critic. 103 A3C asynchronous advantage actor critic. 103 accuracy the total number of true positives and negatives divided by the total number of predictions. 299 ACO ant colony optimization. 211 ALE atari learning environment. 67 BERT bidirectional encoder representations from transformers. 263 bootstrapping old estimates of a value are refned with new updates. 45 CFR counterfactual regret minimization. 202 CPU central processing unit. 323 D4PG distributional distributed deep deterministic policy gradient. 114 DDDQN dueling double deep Q-network. 82 DDPG deep deterministic policy gradient. 108 DDQN double deep Q-network. 81 deep learning training a deep neural network to approximate a function, used for high-dimensional problems. 2 deep reinforcement learning approximating value, policy, and transition functions with a deep neural network. 3 deep supervised learning approximating a function with a deep neural network; often for regression of image classifcation. 297 DQN deep Q-network. 75 entropy measure of the amount of uncertainty in a distribution. 292 exploitation selecting actions as suggested by the current best policy \ud835\udf0b(\ud835\udc60). 48 exploration selecting other actions than those that the policy \ud835\udf0b(\ud835\udc60) suggests. 47 Glossary 383 few-shot learning task with which meta learning is often evaluated, to see how well the meta-learner can learn with only a few training examples. 253 fnetuning training the pre-trained network on the new dataset. 247 function approximation approximation of a mathematical function, a main goal of machine learning, often performed by deep learning. 12 GAN generative adversarial network. 316 GPT-3 generative pretrained transformer 3. 263 GPU graphical processing unit. 322 hyperparameters determine the behavior of a learning algorithm; Base-learning learns parameters \ud835\udf03, metalearning learns hyperparameters \ud835\udf14. 267 LSTM long short-term memory. 313 machine learning learning a function or model from data. 12 MADDPG multi agent DDPG. 207 MAML model-agnostic meta-learning. 257 Markov decision process stochastic decision process that has the Markov (no-memory) property: the next state depends only on the current state and the action. 28 meta-learning learning to learn hyperparameters; Use a sequence of related tasks to learn a new task quicker. 247 MuJoCo multi Joint dynamics with Contact. 92 optimization fnd an optimal element in a space; used in many aspects in machine learning. 9 overftting high-capacity models can overtrain, where they model the signal and the noise, instead of just the signal. 302 parameters the parameters \ud835\udf03 (weights of a neural network) connect the neurons, together they determine the functional relation between input and output. 299 PBT population based training. 213 PETS probabilisitic ensemble with trajectory sampling. 129 PEX prioritized experience replay. 82 PILCO probabilistic inference for learning control. 128 384 Glossary PPO proximal policy optimization. 106 pretraining parameter transfer of the old task to the new task. 247 REINFORCE REward Increment = Non-negative Factor \u00d7 Ofset Reinforcement \u00d7 Characteristic Eligibility. 97 reinforcement learning agent learns a policy for a sequential decision problem from environment feedback on its actions. 3 SAC soft actor critic. 107 SARSA state action reward state action. 50 sequential decision problem problem consisting of a sequence of decisions. 4 supervised learning training a predictive model on a labeled dataset. 3 TD temporal diference. 45 TPU tensor processing unit. 180 transfer learning using part of a network (pretraining) to speedup learning (fnetuning) on a new dataset. 247 TRPO trust region policy optimization. 104 unsupervised learning clustering elements in an unlabeled dataset based on an inherent metric. 15 VI value iteration. 40 VIN value iteration network. 134 VPN value prediction network. 130 XAI explainable artifcial intelligence. 278 zero-shot learning an example has to be recognized as belonging to a class without ever having been trained on an example of this class. 260 ZSL zero-shot learning. 260 Index \ud835\udc58-fold cross validation, 299 A0G, 183 A3C, 103 accuracy, 298 action, 33 active learning, 3 active learning, 177 actor critic, 99 adaptive neural trees, 278 advantage function, 82, 99 adversarial attack, 316 agent, 16, 27 Agent57, 85 alchemy benchmark, 265 ALE, 67 AlexNet, 308, 323 alpha-beta, 162 AlphaStar, 219 AlphaZero General, 183 AMIGo, 233 Ant colony optimization, 211 Ape-X DQN, 221 Arcade Learning Environment, 67 Atari, 67 Atari 2600, 67 attention networks, 319 autoencoder, 317 AutoML, 260 backgammon, 46, 148 backpropagation, 304 bandit theory, 47 baseline function, 99 batch updates, 98 behavior policy, 47 Bellman equation, 39 Bellman, Richard, 39 Bengio, Yoshua, 12 BERT, 263, 308 bias-variance trade-of, 300 bootstrapping, 45, 49 brittleness, 106, 107, 138, 316 captioning challenge, 313 Capture the Flag, 218 cartpole, 55 CFR, 203 chess, 7 CNN, 308 collective intelligence, 211 competitive, 195 competitive behavior, 203 confdence, 47 continuous action space, 30 continuous task, 33 convergence, 73, 80 convolutional network, 308 convolutional networks, 310 cooperation, 206 cooperative, 195 correlation, 73 counterfactual regret minimization, 203 coverage, 72 coverage, correlation, convergence, 72 cross validation, 299 cross-entropy, 293 curriculum learning, 174, 175 curse of dimensionality, 299 dan (Go rank), 153 data augmentation, 320 DDPG, 108 DDQN, 81 385 386 Index deadly triad, 73 decentralized computing, 211 decorrelation, 75 deep deterministic policy gradient, 108 deep dreaming, 317 deep fake, 316, 317 DeepMind control suite, 114 DeepStack, 215 deterministic environment, 29 developmental robotics, 174 discrete action space, 30 distributional DQN, 83 divide and conquer, 40 domain adaptation, 251 downward selection, 32 dropouts, 320 dueling DDQN, 82 dynamic programming, 39, 40 dynamics model, 39 early stopping, 320 ELF, 184 Elmo, 182 Elo, Arpad, 180 end-to-end learning, 307 entropy, 106 environment, 16, 27 episode, 34 episodic task, 33 epoch, 304 error value, 298 evaluation function, 160 evolutionary algorithms, 209 evolutionary strategies, 209 experience replay, 75 explainable AI, 278 exploitation, 47 exploration, 47 feature discovery, 307 few-shot learning, 253, 260 foundation model, 245 foundation models, 244 function approximation, 298 game theory, 194 GAN, 177, 316 Gaussian processes, 128 general-sum, 195 generalization, 278, 298, 299 generative adversarial networks, 316 Go, 7 goal-conditioned, 236 Google Football, 224 Google Research Football, 221 GPT-3, 263, 308 GPU, 323 gradient descent, 305 graph view of state space, 31 Gym, 41 heuristic, 160 Hex, 184 Hide and Seek, 216 hierarchical actor critic, 238 hierarchical methods, 275 hierarchical reinforcement learning, 225 Hinton, Geofrey, 12 holdout validation set, 320 hyperparameter optimization, 259 hypothesis testing, 3 ImageNet, 64, 323 imagenet, 323 IMPALA, 221 in-distribution learning, 252 inductive bias, 259 interpretable models, 278 intrinsic motivation, 236 intrinsic motivation, 174, 277 irace, 259 irreversible actions, 30, 127 iterated prisoner\u2019s dilemma, 200 Keras, 308, 322, 324 KL divergence, 293 Knightcap, 188 knowledge representation, 278 Kocsis, Levente, 168 kyu (Go rank), 153 latent models, 275 league learning, 213 learning, 122, 126, 127 learning rate tuning, 54 LeCun, Yann, 12 Leela, 184 LeNet-5, 308 Libratus, 214 lifelong learning, 246 locomotion, 112 LSTM, 313, 315 machine learning, 298 MADDPG, 206, 207 make-move, 189 MAML, 257 MAPPO, 207 Index 387 Markov decision process, 28 max pooling, 312 MCTS, 164 backup, 167 expand, 167 playout, 167 select, 166 meta dataset benchmark, 263 meta learning, 252, 276 meta world benchmark, 264, 266 minimax, 160 mixed, 195 MNIST, 308, 322 model compression, 278 model-predictive control, 133 Monte Carlo Tree Search, 164 Montezuma\u2019s revenge, 84, 236 mountain car, 55 MuJoCo, 92 multi-agent DDPG, 206 multi-armed bandit, 47, 168 multi-task learning, 248, 250 N-way-k-shot learning, 253 Nash equilibrium, 196 neural ftted Q learning, 74 neural networks, 303 Neurogammon, 74 nevergrad, 259 NNUE, 162 noisy DQN, 83 non-stationarity, 201 objective, 38 of-policy learning, 49 Omniglot, 262 on-policy learning, 49 one-pixel problem, 316 online updates, 98 OpenAI Gym, 41 OpenGo, 184 opponent modeling, 208 optuna, 259 out-of-distribution training, 72 overftting, 301, 320 P-UCT, 168, 169 ParamILS, 259 Pareto optimum, 197 pattern database, 167 Pavlov, 8 PETS, 129 Petting Zoo, 221 PhoenixGo, 184 planning, 40, 126, 127 planning and learning, 126 Pluribus, 215 poker, 214 policy, 33 policy function, 33 Polygames, 184 POMDP, 151 population based self-play, 276 population-based strategies, 209 PPO, 106 precision, 298 prioritized DDQN, 82 prisoner\u2019s dilemma, 198 procedural content generation, 176 proximal policy optimization, 106 PyBullet, 92 PyTorch, 322 Q-learning, 50 query set, 253 R2D2, 85 rainbow algorithms, 80 random number seed, 106 recall, 298 recurrent neural network, 313 recursion, 40 regularization, 320 reinforcement learning, 16, 27 replay bufer, 75 residual network, 172, 315 ResNet, 315 reversible actions, 126, 127 reward, 51 reward is enough, 217 reward shaping, 51 RNN, 313 robustness, 138 SAC, 107 sample efciency, 123 SARSA, 50 SATzilla, 259 scikit-learn, 259 scikit-optimize, 259 search-eval, 160 self-learning, 159 self-play, 159, 275 sequence, 34 sequential decision problem, 4 SGD, 304, 307 shared weights, 311 Shogi, 182 388 Index Skinner, 8 SMAC, 259 smoothness assumption, 303 soft actor critic, 107 soft decision trees, 278 softmax, 312 sparse reward, 51 stable baselines, 77 StarCraft, 7, 219 state, 29 state space, 299 stochastic environment, 29 stochastic gradient descent, 307 Stockfsh, 182 supervised learning, 14 support set, 253 swarm computing, 211 Szepesv\u00e1ri, Csaba, 168 tabula rasa, 178 tabular method, 44 TD, 45 TD-Gammon, 74, 148 temporal abstraction, 227 temporal diference, 45 TensorFlow, 322 Tesauro, Gerald, 74, 149 test set, 298 text to image, 313 theory of mind, 208, 211 Tit for Tat, 200 trace, 34 training set, 298 trajectory, 34 transfer learning, 248 transformers, 263, 320 transposition table, 162 trial and error, 3, 32 TRPO, 104 trust region policy optimization, 104 UCT, 168 underftting, 301 undo-move, 189 universal value function, 230 unsupervised learning, 15, 317 upward learning, 32 VAE, 317 value function, 33 value iteration, 40 value prediction network, 130 variance, 47 variational autoencoder, 317 weight sharing, 311 XAI, 278 zero-shot learning, 260 zero-sum game, 152 ",
    "title": "Deep Reinforcement Learning",
    "paper_info": "Aske Plaat\nDeep Reinforcement Learning\nJune 22, 2022\nSpringer Nature\narXiv:2201.02135v4  [cs.AI]  19 Jun 2022\n",
    "GPTsummary": "                    - (1): The research background of this article is the recent trend in deep reinforcement learning, which has achieved impressive results in various fields such as autonomous driving, game playing, molecular recombination, and robotics.\n \n                    - (2): The past methods in deep reinforcement learning are reviewed, including model-free and model-based methods. The approach is well motivated as it mimics how children learn through playful exploration and feedback. The problems with the past methods include difficulties in dealing with large state and action spaces, and the need for extensive training. \n\n                    - (3): The research methodology proposed in this paper is to provide a comprehensive overview of the field of deep reinforcement learning, covering both the established and advanced methods such as multi-agent, hierarchical, and meta learning. \n\n                    - (4): The methods described in the paper are not evaluated on a specific task, but rather provide a broader understanding of the field. The performance of the methods is evaluated based on their ability to solve difficult problems and achieve impressive results in various applications. These results support the potential of deep reinforcement learning for artificial intelligence.\n7. Methods: \n\n- (1): The article provides a comprehensive review of deep reinforcement learning, covering both established and advanced methods such as model-free and model-based techniques. \n\n- (2): The paper discusses the limitations of past methods, including difficulties in dealing with large state and action spaces, and the need for extensive training. \n\n- (3): The proposed methodology is to provide a broader understanding of the field, including advanced topics such as multi-agent, hierarchical, and meta learning. \n\n- (4): The article does not evaluate the methods described on a specific task but rather provides evidence of their potential through their ability to solve difficult problems and achieve impressive results in various applications.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to provide a comprehensive overview of the field of deep reinforcement learning and to highlight its potential for various applications in artificial intelligence. The paper covers both established and advanced methods, including multi-agent, hierarchical, and meta learning, providing a broader understanding of the field. \n\n- (2): Innovation point: The paper highlights the potential of deep reinforcement learning for artificial intelligence, providing a comprehensive overview of both established and advanced methods. Performance: Although the methods described are not evaluated on a specific task, the paper provides evidence of their potential through their ability to solve difficult problems and achieve impressive results in various applications. Workload: The paper provides a detailed literature review and covers advanced topics, which may require a substantial effort to understand for readers unfamiliar with the field.\n\n\n",
    "GPTmethods": "- (1): The article provides a comprehensive review of deep reinforcement learning, covering both established and advanced methods such as model-free and model-based techniques. \n\n- (2): The paper discusses the limitations of past methods, including difficulties in dealing with large state and action spaces, and the need for extensive training. \n\n- (3): The proposed methodology is to provide a broader understanding of the field, including advanced topics such as multi-agent, hierarchical, and meta learning. \n\n- (4): The article does not evaluate the methods described on a specific task but rather provides evidence of their potential through their ability to solve difficult problems and achieve impressive results in various applications.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to provide a comprehensive overview of the field of deep reinforcement learning and to highlight its potential for various applications in artificial intelligence. The paper covers both established and advanced methods, including multi-agent, hierarchical, and meta learning, providing a broader understanding of the field. \n\n- (2): Innovation point: The paper highlights the potential of deep reinforcement learning for artificial intelligence, providing a comprehensive overview of both established and advanced methods. Performance: Although the methods described are not evaluated on a specific task, the paper provides evidence of their potential through their ability to solve difficult problems and achieve impressive results in various applications. Workload: The paper provides a detailed literature review and covers advanced topics, which may require a substantial effort to understand for readers unfamiliar with the field.\n\n\n",
    "GPTconclusion": "- (1): The significance of this piece of work is to provide a comprehensive overview of the field of deep reinforcement learning and to highlight its potential for various applications in artificial intelligence. The paper covers both established and advanced methods, including multi-agent, hierarchical, and meta learning, providing a broader understanding of the field. \n\n- (2): Innovation point: The paper highlights the potential of deep reinforcement learning for artificial intelligence, providing a comprehensive overview of both established and advanced methods. Performance: Although the methods described are not evaluated on a specific task, the paper provides evidence of their potential through their ability to solve difficult problems and achieve impressive results in various applications. Workload: The paper provides a detailed literature review and covers advanced topics, which may require a substantial effort to understand for readers unfamiliar with the field.\n\n\n"
}