{
    "Abstract": "Abstract\u2014This paper addresses two important estimation problems for linear systems, namely system identi\ufb01cation and model-free state estimation. Our focus is on ARMAX models with unknown parameters. We \ufb01rst provide a reinforcement learning algorithm for system identi\ufb01cation with guaranteed consistency. This algorithm is then used to provide a novel solution to modelfree state estimation. These results are then applied to solving the model-free LQG control problem in the reinforcement learning setting. Index Terms\u2014Reinforcement learning, system identi\ufb01cation, model-free state estimation, model-free control design. I. ",
    "Introduction": "INTRODUCTION It is well known that system identi\ufb01cation and state estimation are closely related learning problems for dynamic systems, with a rich history of research and rich set of methodologies; see, e.g., classical monographs [1], [2] for the former and [3], [4] for the latter. The task of system identi\ufb01cation is to estimate the system parameters, whereas that of state estimation is to provide an estimate of the state for a given system model. The main motivation for this paper is to understand how to do state estimation without a system model. A simple approach is, of course, to estimate the system parameters \ufb01rst and then use them to estimate the state. But this approach is not suitable for on-line model-free state estimation where the estimates need to be updated recursively (or iteratively) along with the output measurement samples. That is, an online estimation algorithm is preferred. The second motivation for this paper is to know whether these estimation problems can be studied in the framework of reinforcement learning [5], [6]. The system under study is the classical Auto-Regressive Moving-Average eXogenous (ARMAX) model with known orders but unknown parameters. We \ufb01rst consider the online system identi\ufb01cation problem formulated in the reinforcement learning framework, and the objective is to provide a recursive (or iterative) estimate of the system parameters along with the update of the output measurement. By blending the tools of instrumental variables and bootstrapping, we provide a new recursive learning algorithm that globally optimises a cost function in the reinforcement learning setting and provides a convergent and consistent parameter estimate in the system identi\ufb01cation setting at the same time. We then extend this algorithm to solve the model-free state estimation problem under a similar reinforcement learning setting and give an asymptotically optimal state estimate in the Kalman \ufb01ltering sense. The reinforcement learning algorithms for system identi\ufb01cation and state estimation will then be used to solve the 1School of Electrical Engineering and Computing, The University of Newcastle, University Drive, Callaghan, 2308, NSW, Australia. E-mail: minyue.fu@newcastle.edu.au. classical linear quadratic Gaussian (LQG) control problem for an ARMAX model with unknown parameters. The solution is a reinforcement learning algorithm for model-free LQG control. The contributions of the paper are summarised below: \u2022 Reformulation and reinterpretation of the classical system identi\ufb01cation tools (least-squares, instrumental variables, bootstrapping...) in the framework of reinforcement learning; \u2022 New recursive parameter estimation algorithm for system identi\ufb01cation with consistency; \u2022 Reinforcement learning algorithm for model-free state estimation; \u2022 Application to model-free linear quadratic Gaussian (LQG) control. II. PROBLEM STATEMENTS A. System Model In this paper, we consider a system with the following stationary ARMAX model [1]: yk + a1yk\u22121 + . . . + anykn = b1uk\u22121 + . . . bmuk\u2212m + wk + c1wk\u22121 + . . . cpwk\u2212p, (1) where uk is the exogenous input, yk is the measured output, wk is the process noise, n, m, p are the parameter dimensions (orders) which are assumed to be known, ai, bi, ci are system parameters which are constant but unknown. The process noise is assumed to be Gaussian white noise with zero mean and variance \u03c32 which is also unknown. The exogenous input is known and assumed to be stationary and independent of the process noise. The system parameter vector will be denoted by \u03b8\u22c6 = [a1 . . . an b1 . . . bm c1 . . . cp]T . The time index k is allowed to range from \u2212\u221e to +\u221e. Denoting the delay operator by z\u22121, the system model (1) can be rewritten as a(z)yk = b(z)uk + c(z)wk, (2) where a(z) = 1 + a1z\u22121 + . . . + anz\u2212n, b(z) = b1z\u22121 + . . . bmz\u2212m and c(z) = 1 + c1z\u22121 + . . . cpz\u2212p. It is further assumed that c(z) is stable (i.e., with all their zeros strictly inside the unit circle) and that a(z), b(z) and c(z) do not have a common factor. 2 Lemma 1: Under the assumption that n \u2265 m and n \u2265 p, the observable-canonical state-space realisation of (2) is given by xk+1 = Axk + B1uk + B2wk = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 0 . . . 0 \u2212an 1 ... ... \u2212an\u22121 ... 0 ... 0 . . . 1 \u2212a1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb xk + \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 0 bm ... b1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fbuk + \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 \u02dccn \u02dccn\u22121 ... \u02dcc1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fbwk yk = Cxk + wk = [0 . . . 0 1]xk + wk, (3) where xk is the state of the system, and \u02dcci = ci \u2212 ai, i = 1, 2, . . . , n with the extended cn = . . . = cp+1 = 0. See Appendix A for proof. B. Reinforcement Learning Reinforcement learning (RL) is an iconic tool in machine learning with huge success in applications [5] and has deep connections with the control theory [6]. Consider a system xk+1 = f(xk, uk, wk) zk = g(xk, uk, wk), (4) where xk is the state, uk is the control input, wk is the process noise, zk is the output known as the cost, f(\u00b7) and g(\u00b7) are unknown functions. Under the assumption that both the state and output are measurable, the aim of RL is to design an optimal control law uk = \u03c0(xk) such that the following total cost Jk is minimised: Jk = E[ \u221e \ufffd t=0 \u03b3tzk+t], (5) where 0 < \u03b3 < 1 is a forgetting factor. In the standard RL terminology, control is called action, control law is called policy, forgetting factor is called the discount factor, \u2212zk is called the reward, \u2212Jk is called the value function, and (5) is equivalent to maximise the value function. To get around of the dif\ufb01culty with unknown f(\u00b7) and g(\u00b7) and unknown structure of feasible policy \u03c0(\u00b7), total cost and policy are parameterised as J\u03b8 k and \u03c0\u03b8(\u00b7) with some (high-dimensional) parameter vector \u03b8. These functions are then approximated using neural networks and an iterative algorithm is applied to tune \u03b8, based on the available xk and zk sequences (from simulations and/or experiments) such that J\u03b8 k is minimised. It is worth noting that apart from some simple cases, RL represents a learning paradigm rather than a guarantee for optimal policies. Important cases where the optimal policy is guaranteed include 1) Markov decision process (MDP) with \ufb01nite numbers of states and actions [5]; 2) linear quadratic regulation (LQR) for state feedback control of linear systems [7]. Two types of iterative algorithms are most commonly used in RL: policy iteration (PI) and value iteration (VI). PI aims to improve the policy after each iteration whereas VI focuses on improving the value function. The key difference between PI and VI is the following: In PI, a single policy (known as on-policy) is used in every time step, whereas in VI, different policies (known as off-policy) can be used in different time steps. This is illustrated in Fig. 1 below. This seemingly subtle difference has a profound in\ufb02uence on the ef\ufb01ciency and effectiveness of the algorithm. Namely, in PI, a complete evaluation of a new policy needs to be performed before the next iteration, whereas in VI, past value functions evaluated based on old policies can be used in evaluating the new policy, making VI a much more popular choice in RL. Policy Iteration: Same policy \u03c0(i) is used throughout Value Iteration: Current and past policies are mixed . . . \u2732 k \u03c0(i) \u2732 k + 1 \u03c0(i) \u2732 k + 2 \u03c0(i) \u2732 k + 3 \u03c0(i) . . . . . . \u2732 k \u03c0(i) \u2732 k + 1 \u03c0(i\u22121) \u2732 k + 2 \u03c0(i\u22122) \u2732 k + 3 \u03c0(i\u22123) . . . Fig. 1. Illustration of Policy Iteration and Value Iteration C. RL Formulation of System Identi\ufb01cation We now formulate the system identi\ufb01cation problem as a reinforcement learning problem. Let \u02c6yk = \u03c0(y<k, u<k), (6) be a (one-step-ahead) predictor of yk, where y<k = [yk\u22121, yk\u22122, . . .] and u<k is similarly de\ufb01ned. The prediction error is given by ek = yk \u2212 \u02c6yk. (7) The total cost is de\ufb01ned to be Jk = E[ \u221e \ufffd t=0 \u03b3te2 k\u2212t] (8) for some discount factor 0 < \u03b3 < 1. Notice that this sequence goes backwards in time, and that the initial state is not present because the sequence of yk starts from k = \u2212\u221e. The RL problem is to \ufb01nd the optimal policy (i.e., predictor) \u03c0 such that Jk is minimised. We will show later that this formulation coincides with the classical system identi\ufb01cation problem. D. RL Formulation of Optimal State Estimation State estimation without a system model has a unique dif\ufb01culty due to in\ufb01nite choices of state coordinates. Therefore, the state estimation problem not only needs to provide an optimal state estimate, but also to specify the system structure. Mathematically, we need to determine the following model: \u03c0 : \u02c6xk+1 = \u02c6f(\u02c6xk, yk, uk) \u02c6yk = \u02c6g(\u02c6xk) (9) where \u02c6xk represents the estimated state, \u02c6f(\u00b7) and \u02c6g are the unknown functions (i.e., structure and parameters). Collectively, \u02c6f(\u00b7) and \u02c6g(\u00b7) constitute the policy to be optimised. A \u201csimple\u201d choice for the estimated state is \u02c6xk = col[y<k, u<k], but this is not desirable because its dimension is in\ufb01nite. It is natural that we want \u02c6xk to have a \ufb01xed \ufb01nite dimension. 3 The RL formulation for model-free state estimation is to \ufb01nd the optimal \u03c0 in (9) such that the total cost Jk in (8) is minimised. Again, we will show later that this formulation is consistent with the classical Kalman \ufb01ltering problem. III. SYSTEM IDENTIFICATION This section solves the RL problem for system identi\ufb01cation. We \ufb01rst make a simple observation that the discount factor does not play any role and that the problem formulation (8) can be simpli\ufb01ed. Lemma 2: For any given (stationary) policy \u03c0 in (6), the total cost Jk in (8) can be simpli\ufb01ed to Jk = 1 1 \u2212 \u03b3 E[e2 k]. (10) Proof: The result follows from the stationarity of the system model (1) and that of the policy. That is, E[e2 k] is independent of k. Hence, Jk = E[e2 k] \ufffd\u221e t=0 \u03b3t, giving (10). The result above indicates that we effectively minimise the squared prediction error, for which the following holds. Lemma 3: Suppose, for any k, wk is independent of uk\u2212i for any i = 1, . . . , m. Then, the optimal policy \u03c0\u22c6 of (6) that minimises E[Jk] is given by \u02c6yk = \u2212 a1yk\u22121 \u2212 . . . \u2212 anyk\u2212n + b1uk\u22121 + . . . + bmuk\u2212m + c1ek\u22121 + . . . + cpek\u2212p (11) with ek\u2212i = yk\u2212i \u2212 \u02c6yk\u2212i de\ufb01ned recursively. The corresponding minimum is given by min \u03c0 E[Jk] = 1 1 \u2212 \u03b3 \u03c32. (12) Proof: Firstly, it is obvious from (1) and (6)-(7) that ek can be rewritten as ek = wk + \u02dcek where \u02dcek a function of u<k, y<k and w<k, hence independent of wk. It is clear that E[e2 k] \u2265 E[w2 k] = \u03c32. By taking \u02c6yk as in (11), we get ek + c1ek\u22121 + . . . cpek\u2212p = wk + c1wk\u22121 + . . . cpwk\u2212p, i.e., ek and wk have the same power spectrum, hence E[e2 k] = E[w2 k] = \u03c32, con\ufb01rming the optimality of (11). Finally, (12) is obtained by using Lemma 2. (We note that the result for minimum E[e2 k] is consistent with [1].) With Lemma 3, we can take the policy structure to be \u02c6yk(\u03b8) = \u2212 \u02c6a1yk\u22121 \u2212 . . . \u2212 \u02c6anyk\u2212n+ \u02c6b1uk\u22121+ . . . + \u02c6bmuk\u2212m + \u02c6c1ek\u22121(\u03b8) + . . . + \u02c6cpek\u2212p(\u03b8) (13) ek(\u03b8) = yk \u2212 \u02c6yk(\u03b8), (14) with \u03b8 = [\u02c6a1 . . . \u02c6an \u02c6b1 . . . \u02c6bm \u02c6c1 . . . \u02c6cp]T . The most popular method for ARMAX estimation is the so-called pseudo-linear regression (PLR) method [1], [2]. De\ufb01ning the pseudo-linear regressor as \u03d5k(\u03b8) =[\u2212yk\u22121 . . . \u2212 yk\u2212n uk\u22121 . . . uk\u2212m ek\u22121(\u03b8) . . . ek\u2212p(\u03b8)]T , (15) then \u02c6yk(\u03b8) = \u03d5T k (\u03b8)\u03b8; ek(\u03b8) = yk \u2212 \u03d5T k (\u03b8)\u03b8. (16) The PRL estimate of \u03b8 is computed by solving E[\u03d5k(\u03b8)(yk \u2212 \u03d5T k (\u03b8)\u03b8)] = 0. (17) This is typically done recursively (known as bootstrapping method in the system identi\ufb01cation literature): Starting from some initial estimate \u03b8(0), then for each i = 1, 2, . . ., solve \u03b8(i) using E[\u03d5k(\u03b8(i\u22121))(yk \u2212 \u03d5T k (\u03b8(i\u22121))\u03b8(i))] = 0, (18) which is a repeated least-squares problem. The PRL method is also often combined with the instrumental variable method, where the \ufb01rst term \u03d5k(\u03b8) in (17) is replaced with an instrumental variable (vector) \u03b6k(\u03b8) which is designed to be uncorrelated with ek(\u03b8). The convergence properties of the bootstrapping and the instrumental variable method depend on many factors; see [2], [1] for detailed analysis. We emphasise two key observations: 1) Global convergence to the optimal solution is not always guaranteed; 2) The bootstrapping method above is a form of policy iteration in the viewpoint of RL. We will see below that by using a value iteration method in combination with the instrumental variable method, a globally convergent algorithm can be derived for ARMAX estimation. We will \ufb01rst study off-line identi\ufb01cation before giving an online algorithm. A. Off-line Identi\ufb01cation of MA Models We \ufb01rst consider the case of MA models, as this is the stumbling block in system identi\ufb01cation, causing the regressor (15) to depend on \u03b8. The MA model is given by yk = wk + c1wk\u22121 + . . . cpwk\u2212p (19) with the assumption that c(z) = 1 + c1z\u22121 + . . . + cpz\u2212p is strictly stable. Also, \u03b8 = [\u02c6c1 . . . \u02c6cp]T in this case. Identi\ufb01cation of MA models can be traced back at least to [8], [9]. But earlier methods all require solving dif\ufb01cult nonlinear equations. In [1] (p. 337), a two-step, non-iterative method is provided: Step 1 estimates a high-order AR model to approximate the MA model; Step 2 uses the prediction errors (known as innovations) from the AR model as an estimate of the past process noise and estimate the MA parameters using the least-squares method. This method requires heavy computation for the \ufb01rst step due to the use of a high-order AR model and gives only an approximate solution. Alternatively, the PRL method can be used to reduce complexity, but there is no theoretical guarantee for an optimal solution. Here we introduce a new algorithm based on an VI method in RL. That is, we generalise the bootstrapping method (18) by allowing the PLR to depend on multiple past estimates of \u03b8, as illustrated in Fig. 1. We \ufb01rst consider an off-line iterative algorithm before extending it to on-line learning. 4 We start estimating \u03b8 from k = 0. Denote by \u03b8(k), k \u2265 0 the k-th estimate. We revise (13)-(14) to the following: \u02c6yk(\u03b8) =\u02c6c1ek\u22121(\u03b8(k\u22121)) + . . . + \u02c6cpek\u2212p(\u03b8(k\u2212p)) (20) ek(\u03b8) = yk \u2212 \u02c6yk(\u03b8). (21) That is, ek\u2212i(\u03b8) is replaced with ek\u2212i(\u03b8(k\u2212i)). With some abuse of notation, the latter will denoted by ek if not confusing. Suppose yk is measured for all k < 0. Due to the stationarity of (19), we can compute all the autocorrelations ry(i) = E[ykyk\u2212i] using the available measurements prior to k = 0. Our off-line iterative algorithm assumes that ry(i), i = 0, 1, . . . , p, are available and produces a sequence of \u03b8(k), k \u2265 0, such that \u03b8(k) \u2192 \u03b8\u22c6 as k \u2192 \u221e. Initialise e\u22121 = . . . = e\u2212p = 0 and \u03b8(\u22121) = . . . = \u03b8(\u2212p) = 0. For k = 0, 1, . . ., solve \u03b8(k) from min \u03b8 E[e2 k] = E[(yk \u2212 \u02c6c1ek\u22121 \u2212 . . . \u2212 \u02c6cpek\u2212p)2] (22) and construct the resulting ek using \u03b8(k). Differentiating the above results in the orthogonality condition: E[ekek\u2212i] = 0, i = 1, . . . , p. (23) As we will show later that the orthogonality condition holds recursively, i.e., E[ek\u2212jek\u2212j\u2212i] = 0 for all j > 0 and i > 0 as well. This implies that (23) can be simpli\ufb01ed to E[(yk \u2212 \u02c6ciek\u2212i)ek\u2212i] = 0, giving the simple solution for \u02c6ci as c(k) i = \ufffd \u03c1k(i)/E[e2 k\u2212i] if E[e2 k\u2212i] > 0 0 otherwise , (24) where \u03c1k(i) = E[ykek\u2212i]. The resulting E[e2 k] is given by E[e2 k] = ry(0) \u2212 (c(k) 1 )2E[e2 k\u22121] \u2212 . . . (c(k) p )2E[e2 k\u2212p]. (25) Note that, by construction, ek explicitly depends on \u03b8(k) but implicitly depends on \u03b8(k\u22121), \u03b8(k\u22122), . . . because of ek\u22121, . . ., hence the method above is an VI method in the RL framework. The computation of (24) involves \u03c1k(i), which can also be easily updated. Indeed, for i = 1, . . . , p, \u03c1k(i) =E[ykek\u2212i] =E[yk(yk\u2212i \u2212 c(k\u2212i) 1 ek\u2212i\u22121 \u2212 . . . \u2212 c(k\u2212i) p ek\u2212i\u2212p)] =ry(i) \u2212 c(k\u2212i) 1 \u03c1k(i + 1) \u2212 . . . \u2212 c(k\u2212i) p \u03c1k(i + p). From (19), \u03c1k(i) = E[ykek\u2212j] = 0 for j > p. Therefore, \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 1 c(k\u22121) 1 . . . c(k\u22121) p\u22121 0 1 c(k\u22122) 1 ... ... ... 1 c(k\u2212p+1) 1 0 . . . 0 1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 \u03c1k(1) \u03c1k(2) ... \u03c1k(p) \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fb= \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 ry(1) ry(2) ... ry(p) \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fb (26) which can be easily computed due to the triangular structure. We have the following result for convergence. Theorem 1: The VI method above has two properties: \u2022 (Orthogonality:) E[ekek\u2212i] = 0 for all k \u2265 i and i > 0; \u2022 (Convergence and Consistency:) \u03b8(k) \u2192 \u03b8\u22c6 as k \u2192 \u221e. Proof: Take any k \u2265 0 and i > 0. The orthogonality condition for i \u2264 p was given in (23). Now consider i = p+1, E[ekek\u2212i] = E[(yk \u2212 c(k) 1 ek\u22121 \u2212 . . . \u2212 c(k) p ek\u2212p)ek\u2212i] The \ufb01rst term E[ykek\u2212i] = 0 due to i > p. Thus, E[ekek\u2212i] = 0 because E[ek\u2212jek\u2212i] = 0 for all j = 1, . . . , p due to i = p + 1. This process can be repeated for i = p + 2, p + 3, . . .. Hence, the orthogonality condition holds for all i > 0. To show convergence and consistency, we note that the sequences {e0, e1, . . . ek\u22121} and {y0, y1, . . . , yk\u22121} form a linear invertible mapping. Let \u03c0\u22c6 k be the optimal function in (6), linear or nonlinear, such that E[(yk \u2212 \u02c6yk)2] is minimised. Due to the assumption that wk is a Gaussian white noise, it is well-known [3] that the optimal \u03c0\u22c6 k is a linear mapping. Due to the invertibility above, the optimal \u02c6yk can be represented as the following linear mapping: \u02c6yk = \u03b41ek\u22121 + . . . \u03b4pek\u2212p + \u03b4p+1ek\u2212p\u22121 + . . . + \u03b4ke0 and the optimal \u03b4i can be solved by minimising E[(yk \u2212 \u02c6yk)2]. Due to the orthogonality peroperty of ek and the fact that yk is orthogonal to ek\u2212i for i > p, it is easy to see that \u03b4i = 0 for any i > p and, for any i = 1, 2, . . ., p, \u03b4i are the same as \u02c6ci in (24). That is, the optimal \u02c6yk is given by \u02c6yk = \u02c6c1ek\u22121 + . . . + \u02c6cpek\u2212p (27) with [\u02c6c1 . . . \u02c6cp] = \u03b8(k). On the other hand, it is also well known [3] that, as k \u2192 \u221e, the stability of c(z) and stationarity of (22) implies that the optimal \u02c6yk is such that ek = yk \u2212 \u02c6yk \u2192 c\u22121(z)yk = wk That is, c(z)ek \u2192 yk, i.e., ek \u2192 \u2212c1ek\u22121 \u2212 . . . \u2212 cpek\u2212p + yk \u02c6yk \u2192 c1ek\u22121 + . . . cpek\u2212p Comparing this to (27), we see that \u03b8(k) \u2192 \u03b8\u22c6 as k \u2192 \u221e. B. Off-line Identi\ufb01cation of ARMAX Model Now let us return to the ARMAX model (1). Using the policy structure (13)-(17), the task is to solve \u03b8 to minimise E[e2 k(\u03b8)] = E[(yk \u2212 \u03d5k(\u03b8)T \u03b8)2]. (28) However, the coupling between the ARX part and MA part of the model makes it dif\ufb01cult to minimise (28) directly. To get around this dif\ufb01culty, we can \ufb01rst use a classical instrumental variable method in system identi\ufb01cation to estimate the ARX part and then the proposed VI method to estimate the MA model [1], [2]. De\ufb01ne the instrumental variable (vector) as \u03b6k = F(z)[\u2212yk\u2212p\u22121 . . . \u2212 yk\u2212p\u2212n uk\u22121 . . . uk\u2212m]T , (29) 5 where F(z) is a causal linear \ufb01lter with both F(z) and F \u22121(z) being stable. In particular, we can take F(z) = 1. Using (1) and properties of uk and wk, we get E[\u03b6k(yk + a1yk\u22121 + . . . + anyk\u2212n \u2212 b1uk\u22121 \u2212 . . . \u2212 bmuk\u2212m)] = 0. (30) This gives (n + m) linear equations: R\u02dc\u03b8 = r (31) with \u02dc\u03b8 = col{a, b}, R = E[\u03b6k \u02dc\u03d5T k ], r = E[\u03b6kyk] and \u02dc\u03d5k = [\u2212yk\u22121 . . . \u2212 yk\u2212n uk\u22121 . . . uk\u2212m]T . This allows us to solve a and b under the mild persistent excitation condition of nonsingular R [1], [2]. For the case of ARMA models (with b = 0), if F(z) = I, then \u03b6k = [\u2212yk\u2212p\u22121 . . . \u2212 yk\u2212p\u2212n]T and (31) reduces to Rya = ry (32) with Ry = E[\u03b6k\u03b6T k+p] and ry = E[\u03b6kyk], and we have the following result. Proposition 1: For the case of an ARMA model, Ry is nonsingular if an \u0338= 0 and c(z)/a(z) is a minimal realisation (i.e., a(z) is not degenerate in its order and there is no zeropole cancellation between c(z) and a(z)). Proof: See Appendix B. After the ARX part of the model is identi\ufb01ed, we de\ufb01ne \u02dcyk = yk \u2212 \u02dc\u03d5T k \u02dc\u03b8. (33) Then the new MA model \u02dcyk = wk + c1wk\u22121 + . . . cpwk\u2212p (34) can be identi\ufb01ed by the value iteration method for MA models. C. On-line Identi\ufb01cation of ARMAX Models In order to obtain an on-line identi\ufb01cation method for ARMAX models, we need to convert the instrumental variable method for the ARX part into a recursive algorithm and combine it with a recursive algorithm of the value iteration method for the MA model. We do the conversion for the ARX part \ufb01rst. At each time instant k = 1, 2, . . ., we replace (31) with R(k)\u02dc\u03b8(k) = r(k), (35) by approximating expectations with empirical averages, i.e., R(k) = 1 k + 1 k \ufffd t=0 \u03b6t \u02dc\u03d5T t = k k + 1R(k\u22121) + 1 k + 1\u03b6k \u02dc\u03d5T k , (36) r(k) = 1 k + 1 k \ufffd t=0 \u03b6tyt = k k + 1r(k\u22121) + 1 k + 1\u03b6kyk. (37) Denoting P (k) = (R(k))\u22121, it is standard [1] to obtain the recursive solution to (35) as below. Proposition 2: The solution to (35) has the following recursion for k \u2265 1: \u02dc\u03b8(k) = \u02dc\u03b8(k\u22121) + 1 k P (k\u22121)\u03b6k\u03b3\u22121 k (yk \u2212 \u02dc\u03d5T k \u02dc\u03b8(k\u22121)), (38) P (k) = \ufffd I \u2212 1 kP (k\u22121)\u03b6k\u03b3\u22121 k \u02dc\u03d5T k \ufffd k + 1 k P (k\u22121) (39) where \u03b3k = 1 + 1 k \u02dc\u03d5T k P (k\u22121)\u03b6k (40) Moreover, \u02dc\u03b8(k) \u2192 \u02dc\u03b8\u22c6 (the true value of col{a, b}) and R(k) \u2192 R as k \u2192 \u221e with probability 1, provided that R is nonsingular. Proof: The recursion (39) is obtained by applying the well-known matrix inversion lemma to (36). Then, (38) is obtained by applying (37) and (39) to solving (35). Also, R(k) \u2192 R and r(k) \u2192 r (with probability 1) owing to the stationarity of the system, and \u02dc\u03b8(k) \u2192 \u02dc\u03b8\u22c6 (with probability 1) because R is nonsingular. Next, we convert the MA part. First, we revise \u02dcyk to \u02dcyk = yk \u2212 \u02dc\u03d5T k \u02dc\u03b8(k), (41) which converges to (33) as k \u2192 \u221e. Secondly, we replace ry(i), i = 0, 1, . . . , p, with empirical averages, i.e., r(k) y (i) = 1 k + 1 k \ufffd t=0 \u02dcyt\u02dcyt\u2212i = k k + 1r(k\u22121) y (i) + 1 k + 1 \u02dcyk\u02dcyk\u2212i. (42) Again, r(k) y (i) \u2192 ry(i) as k \u2192 \u221e. Thirdly, using r(k) y (i) above, we modify (26) to \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 1 c(k\u22121) 1 . . . c(k\u22121) p\u22121 0 1 c(k\u22122) 1 ... ... ... 1 c(k\u2212p+1) 1 0 . . . 0 1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 \u03c1k(1) \u03c1k(2) ... \u03c1k(p)] \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fb= \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 r(k) y (1) r(k) y (2) ... r(k) y (p) \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (43) Finally, we replace E[e2 k] with \u01eb2 k and modify (24)-(25) as c(k) i = \ufffd \u03c1k(i)/\u01eb2 k\u2212i if \u01eb2 k\u2212i > 0 0 otherwise , i = 1, 2, . . . , p, (44) \u01eb2 k = r(k) y (0) \u2212 (c(k) 1 )2\u01eb2 k\u22121 \u2212 . . . (c(k) p )2\u01eb2 k\u2212p; (45) The resulting on-line algorithm is summarised below. We have the following result. Theorem 2: Under the persistent excitation condition R > 0, Algorithm 1 has the following properties as k \u2192 \u221e: \u2022 \u01eb2 k \u2192 E[e2 k] with probability 1; \u2022 \u03b8(k) \u2192 \u03b8\u22c6 with probability 1. Proof: The proof follows directly from Theorem 1, Proposition 2, and r(k) y (i) \u2192 ry(i) with probability 1 for all i. 6 Algorithm 1 (On-line Identi\ufb01cation for ARMAX Models) \u2022 Initialisation: \u2013 Set \u01eb2 \u2212i = 0, i = 1, 2, . . . p \u2212 1 and \u01eb2 0 = y2 0; \u2013 Set r(0) y (i) = 0, i = 1, 2, . . . , p and r(0) y (0) = y2 0; \u2013 Set col{\u02dc\u03b8(0), c(0) 1 , . . . , c(0) p } = 0; \u2013 Set P (0) = p0I for any (large) p0 > 0. \u2022 Main loop: At iteration k = 1, 2, \u00b7 \u00b7 \u00b7 , \u2013 Compute \u02dc\u03b8(k) and P (k) using (38)-(39); \u2013 Compute r(k) y (i), i = 0, 1, . . ., p, using (42); \u2013 Compute \u03c1k(i), i = 1, 2, . . . , p, using (43); \u2013 Compute c(k) i , i = 1, 2, . . ., p, using (44); \u2013 Compute \u01eb2 k using (45); IV. MODEL-BASED STATE ESTIMATION A. Optimal State Estimation for a Known Model Consider the following state-space model: xk+1 = Axk + B1uk + B2wk yk = Cxk + vk, (46) where xk is the state, uk is the known input, wk is the process noise, vk is the measurement noise, {(wk, vk)} is zero-mean Gaussian noise with E \ufffd\ufffd wk vk \ufffd [wT l vT l ] \ufffd = \ufffd Q S ST R \ufffd \u03b4kl, k, l \u2208 R. (47) When the system model is known, the steady-state Kalman \ufb01lter of (46) is given by [3] \u02c6xk+1 = A\u02c6xk + B1uk + L(yk \u2212 C\u02c6xk) (48) with the optimal observer gain L given by [3] (Section 5.4) L = (A\u03a3CT + B2S)(C\u03a3CT + R)\u22121 (49) \u03a3 = A\u03a3AT \u2212 (A\u03a3CT + B2S)(C\u03a3CT + R)\u22121 \u00b7 (A\u03a3CT + B2S)T + B2QBT 2 . (50) In the above, \u03a3 = E[(xk \u2212 \u02c6xk)(xk \u2212 \u02c6xk)T ] is the steadystate state estimation error covariance, and (49) is an algebraic Riccati equation (ARE). B. Pitfall for Model-Free State Estimation Example 1: Consider the scalar sequence {yk}: yk = wk\u22121 + wk + \u00b5k = (1 + z\u22121)wk + \u00b5k (51) which has the following state-space realisation: xk+1 = wk yk = xk + wk + \u00b5k (52) where wk and \u00b5k are independent zero-mean Gaussian white noises with variance equal to 1. Comparing with (46)-(47), we verify that vk = wk + \u00b5k, A = 1, B1 = 0, B2 = 1, C = 1, Q = 1, R = 2, S = 1. The state estimator (48) becomes \u02c6xk+1 = L(yk \u2212 \u02c6xk). (53) Solving (50) gives \u03a3 = \u03a3 \u2212 (\u03a3 + 1)2(\u03a3 + 2)\u22121 + 1 resulting in \u03a3 = ( \u221a 5 \u2212 1)/2 \u2248 0.618 and L \u2248 0.618. On the other hand, the spectrum of yk in (51) is Sy = (1 + z\u22121)(1 + z) + 1 = \u03b1(1 + \u03b1\u22121z\u22121)(1 + \u03b1\u22121z) with \u03b1 = 0.5(3+ \u221a 5). Now consider an alternative state-space realisation: xk+1 = wk yk = \u03b1\u22121xk + wk (54) with wk being a zero-mean Gaussian white noise with variance of \u03b1. For (54), the counterpart of (Q, R, S) is given by Q = R = S = \u03b1. The optimal state estimator is given by \u02c6xk+1 = L(yk \u2212 \u02c6xk). (55) Solving (50) for this estimator gives L = 1, and the corresponding steady-state state estimation error covariance \u03a3 = 0. That is, in steady state, xk can be perfectly predicted by y<k! Since the noises wk and \u00b5k are not directly measurable, the state-space representation (46) is indistinguishable from (54). We see from this example that different state-space realisations can result in vastly different state estimation results, which is a unique feature for the state estimation problem when the state-space model is not speci\ufb01ed! C. State-Space Realisation for ARMAX Models Motivated by Example 1, we see that different state-space realisations may result in vastly different state estimation errors. Here, we present a state-space realisation that has zero optimal state estimation error in steady state. Our chosen state-space realisation for (1) is (3), for which we have the following result. Theorem 3: Suppose the system model (2) is such that 1) n \u2265 m and n \u2265 p; and 2) c(z) is stable. Then, the optimal state estimator (48) for the state-space realisation (3) has the observer gain L = B2 = [\u02dccn . . . \u02dcc1]T (56) and its associated state estimation error is zero with probability 1 in steady state, i.e., \u03a3 = 0. Proof: Comparing the realisation (3) with (46)-(47), it is clear that Q = S = R = E[wkwT k ] = \u03c32 for (3). Using the state estimator in (48) and de\ufb01ning the estimation error \u03b5k = xk \u2212 \u02c6xk and its covariance \u03a3k = E[\u03b5k\u03b5T k ], we have \u03b5k+1 = (A \u2212 LC)\u03b5k + (B2 \u2212 L)wk \u03a3k+1 = (A \u2212 LC)\u03a3k(A \u2212 LC)T + \u03c32(B2 \u2212 L)(B2 \u2212 L)T . It is clear that if L = B2 then, \u03a3k+1 = (A \u2212 B2C)\u03a3k(A \u2212 B2C)T . (57) 7 From Lemma 1, we have A \u2212 B2C = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 0 . . . 0 \u2212cn 1 ... ... \u2212cn\u22121 ... 0 ... 0 . . . 1 \u2212c1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb It is easy to verify that det(I \u2212 (A \u2212 B2C)z\u22121) = c(z), (58) which is assumed to be stable. Hence, A \u2212 B2C is stable. It follows from (57) that \u03a3k \u2192 0 as k \u2192 \u221e. That is, the steadystate estimation error covariance \u03a3 = 0, which is obviously optimal. Hence, L = B2 is the optimal observer gain. D. Model-free State Estimation The result in Theorem 3 shows that with an appropriate choice of the state-space realisation, perfect state estimation can be achieved asymptotically. However, this result requires known parameters for the system. We now show how to achieve something similar without a known model. Using Algorithm 1, we can build the one-step-ahead prediction \u02c6yk of yk and the prediction error ek as \u02c6yk = \u2212 a(k) 1 yk\u22121 \u2212 . . . \u2212 a(k) n yk\u2212n + b(k) 1 uk\u22121 + . . . + b(k) m uk\u2212m + c(k) 1 ek\u22121 + . . . + c(k) p ek\u2212p ek =yk \u2212 \u02c6yk (59) for k \u2265 0, with yk = 0, uk = 0, ek = 0 for all k < 0. Following Lemma 1, its state-space realisation is given by \u02c6xk+1 = A(k)\u02c6xk + B(k) 1 uk + B(k) 2 ek = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 0 . . . 0 \u2212a(k) n 1 ... ... \u2212a(k) n\u22121 ... 0 ... 0 . . . 1 \u2212a(k) 1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \u02c6xk + \uf8ee \uf8ef\uf8ef\uf8ef\uf8f0 0 b(k) m ... b(k) 1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fbuk + \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 \u02dcc(k) n \u02dcc(k) n\u22121 ... \u02dcc(k) 1 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb ek yk = C\u02c6xk + ek = [0 . . . 0 1]\u02c6xk + ek. (60) Theorem 4: Under the same conditions as in Theorem 3, the state-space realisation (60) approaches the optimal state estimator of (3) asymptotically. That is, de\ufb01ning the state estimation error \u03b5k = xk \u2212 \u02c6xk between the states of (3) and (60), then the estimation error covariance \u03a3k = E[\u03b5k\u03b5T k ] \u2192 0 as k \u2192 \u221e. (61) Proof: From (3) and (60), the estimation error dynamics is given by \u03b5k+1 = Axk \u2212 A(k)\u02c6xk + (B1 \u2212 B(k) 1 )uk + B2wk \u2212 B(k) 2 ek. As k \u2192 \u221e, the above approaches \u03b5k+1 \u2192 A\u03b5k + B2(wk \u2212 ek) = A\u03b5k + B2(wk \u2212 yk + C\u02c6xk) = A\u03b5k \u2212 B2C\u03b5k = (A \u2212 B2C)\u03b5k. From (58), the above is stable, hence \u03a3k \u2192 0 as k \u2192 \u221e. V. MODEL-FREE LQG CONTROL FOR ARMAX SYSTEMS In this section, we apply the model-free state estimation results in the previous section to LQG control. We \ufb01rst give a result for model-based LQG control, then derive an algorithm for model-free LQG control. A. Model-based LQG Control Consider the system model (3) and the value function V\u03c0(xk) = E[ \ufffd t=k \u03b3t\u2212k(xT t Qxt + uT t Rut)] (62) with discount factor 0 < \u03b3 < 1, Q \u2265 0 and R > 0. The objective is to design a stationary control policy ut = \u03c0(y<t) to minimise V\u03c0(xk). This is a generalisation of the deterministic LQR problem studied in [7] where the noise wk void and the state xk is available. The discount factor is necessary to ensure the boundedness of the value function. We have the following result. Proposition 3: Consider the system (3) and the value function (62). The optimal control policy is given by uk = u\u22c6 k = K\u02c6xk, (63) where \u02c6xk is the optimal estimate of xk based on y<k and K is given by K = (B1PBT 1 + \u03b3\u22121R)\u22121BT 1 PA (64) with P being the solution to the discrete-time algebraic Riccati equation (DARE): P = Q + \u03b3{AT PA \u2212 AT PB1(BT 1 PB1 + \u03b3\u22121R)\u22121BT 1 PA}, (65) and the optimal value function in steady state is given by V\u22c6(xk) = xT k Pxk + \u03b3\u03c32 1 \u2212 \u03b3 BT 2 PB2. (66) (See Appendix for proof.) B. Model-free LQG Control We now solve the model-free LQG control problem. For any control policy \u03c0, de\ufb01ne the Q-function [7] as follows: Q\u03c0(xk, uk) = xT k Qxk + uT k Ruk + \u03b3E[V\u03c0(xk+1)] (67) For the optimal policy \u03c0\u22c6, using the optimal value function in steady state (66), we get Q\u22c6(xk, uk) =xT k Qxk + uT k Ruk + \u03b3E[V\u22c6(xk+1)] =xT k Qxk + uT k Ruk + \u03b32\u03c32 1 \u2212 \u03b3 BT 2 PB2 + \u03b3E[(Axk + B1uk + B2wk)T P(Axk + B1uk + B2wk)] =xT k Qxk + uT k Ruk + \u03b3(Axk + B1uk)T P(Axk + B1uk) + \u03b32\u03c32 1 \u2212 \u03b3 BT 2 PB2 + \u03b3\u03c32BT 2 PB2 =[xT k uT k ] \ufffd H11 H12 HT 12 H22 \ufffd \ufffd xk uk \ufffd + \u03b3\u03c32 1 \u2212 \u03b3 BT 2 PB2. ",
    "Conclusion": "CONCLUSION Model-free state estimation is a challenging problem due to the fact that both the system model is unknown and the measurement contains partial state and noises. By reformulating the classical system identi\ufb01cation as a reinforcement learning problem and incorporating the classical tools of instrumental variables and bootstrapping, we have provided a value-iteration based reinforcement learning algorithm for system identi\ufb01cation of an ARMAX system with guaranteed consistency. This algorithm is then used in solving the modelfree state estimation problem for an ARMAX system, and a reinforcement learning solution has been obtained. These results have also been applied to solving the model-free LQG problem for an ARMAX system. The key to our model-free state estimation solution is to use the observable-canonical realisation, which leads to the optimal state estimation by the driving the state estimation error covariance to zero. How to generalise this observation to more general systems, linear or nonlinear, will be crucial to more general solutions to model-free state estimation. This will alleviate a stumbling block to reinforcement learning applications where only measurement of partial state with noise is available. APPENDIX A: PROOF OF LEMMA 1 Proof: Extend bn = . . . = bm+1 = 0. From (3), we get zxk,1 + anxk,n = bnuk + \u02dccnwk \u2212xk,1 + zxk,2 + an\u22121xk,n = bn\u22121uk + \u02dccn\u22121wk \u2212xk,2 + zxk,3 + an\u22122xk,n = bn\u22122uk + \u02dccn\u22122wk . . . . . . \u2212xk,n\u22121 + (z + a1)xk,n = b1uk + \u02dcc1wk (74) Multiplying the \ufb01rst row above by z\u22121 and adding it the second row, we get zxk,2 + (an\u22121 + anz\u22121)xk,n =(bn\u22121 + bnz\u22121)uk + (\u02dccn\u22121 + \u02dccnz\u22121)wk Again, multiplying this row by z\u22121 and adding it to the third row in (74), we get zxk,3 + (an\u22122 + an\u22121z\u22121 + anz\u22122)xk,n =(bn\u22122 + bn\u22121z\u22121 + bnz\u22122)uk + (\u02dccn\u22122 + \u02dccn\u22121z\u22121 + \u02dccnz\u22122)wk Repeating this until the \ufb01nal row of (74), we get (z + a1 + a2z\u22121 + . . . anzn\u22121)xk,n =(b1 + b2z\u22121 + . . . bnzn\u22121)uk + (\u02dcc1 + \u02dcc2z\u22121 + . . . + \u02dccnzn\u22121)wk Multiplying the above by z\u22121 again, we get a(z)xk,n = b(z)uk + \u02dcc(z)wk where \u02dcc(z) = \u02dcc1z\u22121 + . . . + \u02dccnz\u2212n. It follows from (3) that a(z)yk = a(z)xk,n + a(z)wk = b(z)uk + (a(z) + \u02dcc(z))wk = b(z)uk + c(z)wk. Hence (3) is a state-space realisation of (2). 9 APPENDIX B: PROOF OF PROPOSITION 1 Proof: We prove by contradiction. Suppose Ry is rank de\ufb01cient. Then, there exists some vector v = [v1 . . . vn]T \u0338= 0 such that Ryv = 0. We \ufb01rst consider the case the \ufb01rst element of v, v1 \u0338= 0 and take v1 = 1 without loss of generality. It is easy to verify that the (i + 1)-th row of Ry, i = 0, 1, . . . , n \u2212 1 is given by Ry,i = [ry(p + i) . . . ry(p + i \u2212 n + 1)]. Then, Ry,iv = 0 for all i = 0, 1, . . . , n \u2212 1. For any i \u2265 1, it holds that ry(p + i) = E[yk\u2212p\u2212iyk] = E[yk\u2212p\u2212i(\u2212a1yk\u22121 \u2212 . . . \u2212 anyk\u2212n + wk + c1wk\u22121 + . . . cpwk\u2212p)] = E[yk\u2212p\u2212i(\u2212a1yk\u22121 \u2212 . . . \u2212 anyk\u2212n)] = \u2212[an . . . a1][ry(p + i \u2212 n) . . . ry(p + i \u2212 1)]T . It follows that [ry(p + n) . . . ry(p + 1)] = \u2212[an . . . a1]Ry, giving Ry,n = [ry(p+n) . . . ry(p+1)]v = 0. That is, we have extended Ry by one row at the bottom and still maintains its rank de\ufb01ciency. The above process can be repeated inde\ufb01nitely to give the result that Ry,i = [ry(p + i) . . . ry(p + i \u2212 n + 1)]v = 0, \u2200i \u2265 0. (75) Denoting V (z) = v1 + v2z\u22121 + . . . vnz\u2212(n\u22121) and the onesided Z-transform of ry(k) as \u02c6ry(z) = ry(0) + ry(1)z\u22121 + ry(2)z\u22122 + . . . Then, using (75) and Z-transform properties, we get V (z)\u02c6ry(z) = D(z) = d0 + d1z\u22121 + dp\u22121z\u2212p where d0, . . . dp\u22121 depend on ry(0), . . . , ry(p \u2212 1). It follows that the spectrum of yk is given by Sy(z) = \u221e \ufffd k=\u2212\u221e ry(|k|)z\u2212k = \u02c6ry(z) + \u02c6ry(z\u22121) \u2212 ry(0) = D(z) V (z) + D(z\u22121) V (z\u22121) \u2212 ry(0). But from (1) (without uk), the spectrum should be given by Sy(z) = c(z)c(z\u22121) a(z)a(z\u22121)\u03c32 These two expressions have a clear mismatch of the order in the denominator because a(z) is n-th order and V (z) is (n \u2212 1)-th order). This contradiction implies that Ry can not be rank de\ufb01cient. A similar proof works if v1 = . . . vj\u22121 = 0 for j > 1 but vj \u0338= 0. But the details are omitted. APPENDIX C: PROOF OF PROPOSITION 3 Lemma 4: Consider the system xk+1 = Axk + Bwk (76) with stable A and wk \u223c N(0, \u03c32), and the value function V (xk) = E[ \u221e \ufffd t=k \u03b3t\u2212kxT t Qxt] (77) with Q \u2265 0. Then, V (xk) = xT k Pxk + \u03b3\u03c32 1 \u2212 \u03b3 BT PB (78) with P = \u221e \ufffd k=0 \u03b3k(Ak)T QAk = Q + \u03b3AT PA. (79) Proof: It is straightforward to verify that V (xk) = xT k Qxk + E[ \u221e \ufffd t=k+1 \u03b3t\u2212kxT t Qxt] = xT k Qxk + \u03b3E[ \u221e \ufffd t=k \u03b3t\u2212kxT t+1Qxt+1] = xT k Qxk + \u03b3E[ \u221e \ufffd t=k \u03b3t\u2212k(Axt + Bwt)T Q(Axt + Bwt)] = xT k Qxk + \u03b3 \u221e \ufffd t=k \u03b3t\u2212kBT QB\u03c32 + \u03b3E[ \u221e \ufffd t=k \u03b3t\u2212kxT t (AT QA)xt] = xT k Qxk + \u03b3\u03c32 1 \u2212 \u03b3 BT QB + \u03b3 \u02dcV (xk) where \u02dcV (xk) = E[ \u221e \ufffd t=k \u03b3t\u2212kxT t \u02dcQxt] with \u02dcQ = AT QA. Do the above repeatedly, we get V (xk) =xT k Qxk + \u03b3xT k AT QAxk + \u03b32xT k (A2)T QA2xk + . . . + \u03b3\u03c32 1 \u2212 \u03b3 (BT QB + \u03b3BT AT QAB + . . .) = xT Pxk + \u03b3\u03c32 1 \u2212 \u03b3 BT PB with P given by the \ufb01rst part of (79). The second part of (79) is then easily veri\ufb01ed and the convergence of the sum in (79) is guaranteed by the stability of A. Now we are ready to prove Proposition 3. Proof: We \ufb01rst consider the state feedback case where xk is available. This is an in\ufb01nite-horizon linear quadratic control problem with Gaussian noise. It is well known [10] that the optimal control policy \u03c0\u22c6 is given by uk = Kxk for some ",
    "References": "REFERENCES [1] L. Ljung. System Identi\ufb01cation: Theory for the User, Prentice Hall, 2nd Edition,1999. [2] T. S\u00a8oderstr\u00a8om and P. G. Stoica, Instrumental Variable Methods for System Identi\ufb01cation, Springer-Verlag, 1983. [3] B. D. O. Anderson and J. Moore, Optimal Filtering, Prentice Hall, 1979. [4] G. C. Goodwin and K. S. Sin, Adaptive \ufb01ltering prediction and control, Prentice Hall, 1984. [5] D. Sliver, Introduction of Reinforcement Learning with David Silver, Lecture Series, DeepMind 2015. (https://deepmind.com/learningresources/-introduction-reinforcement-learning-david-silver) [6] D. Bertsekas, Reinforcement Learning and Optimal Control, Athena Scienti\ufb01c, 2019. [7] F. Lewis and D. Vrabie, \u201cReinforcement learning and adaptive dynamic programming for feedback control,\u201d IEEE Circuits and Systems Magazine, 9(3):32-50, 2009. [8] Ef\ufb01cient estimators of parameters in moving-average models, Biometrica, 46:306-316, 1959. [9] A. M. Walker, Large-sample estimation of parameters for movingaverage models, Biometrica, 48:343-357, 1961. [10] B. D. O. Anderson and J. Moore, Optimal Control: Linear Quadratic Methods, Prentice Hall 1971. ",
    "title": "Reinforcement Learning Approach to Estimation in",
    "paper_info": "arXiv:2205.03504v1  [eess.SY]  6 May 2022\n1\nReinforcement Learning Approach to Estimation in\nLinear Systems\nMinyue Fu1\nAbstract\u2014This paper addresses two important estimation\nproblems for linear systems, namely system identi\ufb01cation and\nmodel-free state estimation. Our focus is on ARMAX models with\nunknown parameters. We \ufb01rst provide a reinforcement learning\nalgorithm for system identi\ufb01cation with guaranteed consistency.\nThis algorithm is then used to provide a novel solution to model-\nfree state estimation. These results are then applied to solving the\nmodel-free LQG control problem in the reinforcement learning\nsetting.\nIndex Terms\u2014Reinforcement learning, system identi\ufb01cation,\nmodel-free state estimation, model-free control design.\nI. INTRODUCTION\nIt is well known that system identi\ufb01cation and state estima-\ntion are closely related learning problems for dynamic systems,\nwith a rich history of research and rich set of methodologies;\nsee, e.g., classical monographs [1], [2] for the former and [3],\n[4] for the latter. The task of system identi\ufb01cation is to estimate\nthe system parameters, whereas that of state estimation is to\nprovide an estimate of the state for a given system model.\nThe main motivation for this paper is to understand how to\ndo state estimation without a system model. A simple approach\nis, of course, to estimate the system parameters \ufb01rst and then\nuse them to estimate the state. But this approach is not suitable\nfor on-line model-free state estimation where the estimates\nneed to be updated recursively (or iteratively) along with the\noutput measurement samples. That is, an online estimation\nalgorithm is preferred. The second motivation for this paper\nis to know whether these estimation problems can be studied\nin the framework of reinforcement learning [5], [6].\nThe system under study is the classical Auto-Regressive\nMoving-Average eXogenous (ARMAX) model with known\norders but unknown parameters. We \ufb01rst consider the online\nsystem identi\ufb01cation problem formulated in the reinforcement\nlearning framework, and the objective is to provide a recursive\n(or iterative) estimate of the system parameters along with the\nupdate of the output measurement. By blending the tools of\ninstrumental variables and bootstrapping, we provide a new\nrecursive learning algorithm that globally optimises a cost\nfunction in the reinforcement learning setting and provides\na convergent and consistent parameter estimate in the system\nidenti\ufb01cation setting at the same time. We then extend this\nalgorithm to solve the model-free state estimation problem\nunder a similar reinforcement learning setting and give an\nasymptotically optimal state estimate in the Kalman \ufb01ltering\nsense. The reinforcement learning algorithms for system iden-\nti\ufb01cation and state estimation will then be used to solve the\n1School of Electrical Engineering and Computing, The University of\nNewcastle, University Drive, Callaghan, 2308, NSW, Australia.\nE-mail: minyue.fu@newcastle.edu.au.\nclassical linear quadratic Gaussian (LQG) control problem for\nan ARMAX model with unknown parameters. The solution\nis a reinforcement learning algorithm for model-free LQG\ncontrol.\nThe contributions of the paper are summarised below:\n\u2022 Reformulation and reinterpretation of the classical system\nidenti\ufb01cation tools (least-squares, instrumental variables,\nbootstrapping...) in the framework of reinforcement learn-\ning;\n\u2022 New recursive parameter estimation algorithm for system\nidenti\ufb01cation with consistency;\n\u2022 Reinforcement learning algorithm for model-free state\nestimation;\n\u2022 Application to model-free linear quadratic Gaussian\n(LQG) control.\nII. PROBLEM STATEMENTS\nA. System Model\nIn this paper, we consider a system with the following\nstationary ARMAX model [1]:\nyk + a1yk\u22121 + . . . + anykn\n= b1uk\u22121 + . . . bmuk\u2212m + wk + c1wk\u22121 + . . . cpwk\u2212p, (1)\nwhere uk is the exogenous input, yk is the measured output,\nwk is the process noise, n, m, p are the parameter dimensions\n(orders) which are assumed to be known, ai, bi, ci are system\nparameters which are constant but unknown. The process noise\nis assumed to be Gaussian white noise with zero mean and\nvariance \u03c32 which is also unknown. The exogenous input is\nknown and assumed to be stationary and independent of the\nprocess noise. The system parameter vector will be denoted\nby \u03b8\u22c6 = [a1 . . . an b1 . . . bm c1 . . . cp]T . The time index k is\nallowed to range from \u2212\u221e to +\u221e.\nDenoting the delay operator by z\u22121, the system model (1)\ncan be rewritten as\na(z)yk = b(z)uk + c(z)wk,\n(2)\nwhere a(z) = 1 + a1z\u22121 + . . . + anz\u2212n, b(z) = b1z\u22121 +\n. . . bmz\u2212m and c(z) = 1 + c1z\u22121 + . . . cpz\u2212p. It is further\nassumed that c(z) is stable (i.e., with all their zeros strictly\ninside the unit circle) and that a(z), b(z) and c(z) do not have\na common factor.\n",
    "GPTsummary": "\n                    - (1): The paper aims at studying the problems of state estimation and system identification for linear systems. The focus is on Auto-Regressive Moving-Average eXogenous (ARMAX) models with unknown parameters.\n\n\n                    - (2): Past methods for state estimation require a system model, which is not ideal for online model-free state estimation. The paper motivates the need for studying these problems in the framework of reinforcement learning. Least-squares, instrumental variables, and bootstrapping are the classical system identification tools that the paper reformulates and interprets in the reinforcement learning framework.\n\n\n                    - (3): The paper proposes a reinforcement learning algorithm for system identification with guaranteed consistency, using the tools of instrumental variables and bootstrapping. This algorithm is then extended to solve the model-free state estimation problem under a similar reinforcement learning setting and provides an asymptotically optimal state estimate in the Kalman filtering sense. The algorithm for system identification is globally optimal and provides a consistent parameter estimate in the system identification setting. The paper then uses these algorithms to solve the model-free LQG control problem for an ARMAX model with unknown parameters.\n\n\n                    - (4): The proposed algorithms achieve consistency and provide an asymptotically optimal state estimate. They are applied to solving the model-free LQG control problem in the reinforcement learning setting. The performance supports the goals of the paper.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in the development of a reinforcement learning algorithm for state estimation and system identification of linear systems, which does not require a priori knowledge of the system model. The proposed algorithms achieve consistency and provide an asymptotically optimal state estimate.\n\n- (2): Innovation point: The paper reformulates the classical system identification tools of least-squares, instrumental variables, and bootstrapping in the framework of reinforcement learning, and develops a novel algorithm for model-free state estimation. Performance: The proposed algorithms achieve consistency and provide an asymptotically optimal state estimate in the Kalman filtering sense. Workload: The computational complexity of the algorithms is not explicitly discussed in the paper, with only qualitative remarks provided.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in the development of a reinforcement learning algorithm for state estimation and system identification of linear systems, which does not require a priori knowledge of the system model. The proposed algorithms achieve consistency and provide an asymptotically optimal state estimate.\n\n- (2): Innovation point: The paper reformulates the classical system identification tools of least-squares, instrumental variables, and bootstrapping in the framework of reinforcement learning, and develops a novel algorithm for model-free state estimation. Performance: The proposed algorithms achieve consistency and provide an asymptotically optimal state estimate in the Kalman filtering sense. Workload: The computational complexity of the algorithms is not explicitly discussed in the paper, with only qualitative remarks provided.\n\n\n"
}