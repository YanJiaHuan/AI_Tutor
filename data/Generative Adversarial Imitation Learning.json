{
    "Abstract": "Abstract Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert\u2019s cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments. 1 ",
    "Introduction": "Introduction We are interested in a speci\ufb01c setting of imitation learning\u2014the problem of learning to perform a task from expert demonstrations\u2014in which the learner is given only samples of trajectories from the expert, is not allowed to query the expert for more data while training, and is not provided reinforcement signal of any kind. There are two main approaches suitable for this setting: behavioral cloning [20], which learns a policy as a supervised learning problem over state-action pairs from expert trajectories; and inverse reinforcement learning [25, 18], which \ufb01nds a cost function under which the expert is uniquely optimal. Behavioral cloning, while appealingly simple, only tends to succeed with large amounts of data, due to compounding error caused by covariate shift [23, 24]. Inverse reinforcement learning (IRL), on the other hand, learns a cost function that prioritizes entire trajectories over others, so compounding error, a problem for methods that \ufb01t single-timestep decisions, is not an issue. Accordingly, IRL has succeeded in a wide range of problems, from predicting behaviors of taxi drivers [31] to planning footsteps for quadruped robots [22]. Unfortunately, many IRL algorithms are extremely expensive to run, requiring reinforcement learning in an inner loop. Scaling IRL methods to large environments has thus been the focus of much recent work [7, 14]. Fundamentally, however, IRL learns a cost function, which explains expert behavior but does not directly tell the learner how to act. Given that learner\u2019s true goal often is to take actions imitating the expert\u2014indeed, many IRL algorithms are evaluated on the quality of the optimal actions of the costs they learn\u2014why, then, must we learn a cost function, if doing so possibly incurs signi\ufb01cant computational expense yet fails to directly yield actions? We desire an algorithm that tells us explicitly how to act by directly learning a policy. To develop such an algorithm, we begin in Section 3, where we characterize the policy given by running reinforcement learning on a cost function learned by maximum causal entropy IRL [31, 32]. Our characterization introduces a framework for directly learning policies from data, bypassing any intermediate IRL step. Then, we instantiate our framework in Sections 4 and 5 with a new model-free imitation learning algorithm. We show that our resulting algorithm is intimately connected to generative adversarial arXiv:1606.03476v1  [cs.LG]  10 Jun 2016 ",
    "Background": "Background Preliminaries R will denote the extended real numbers R \u222a {\u221e}. Section 3 will work with \ufb01nite state and action spaces S and A to avoid technical machinery out of the scope of this paper (concerning compactness of certain sets of functions), but our algorithms and experiments later in the paper will run in high-dimensional continuous environments. \u03a0 is the set of all stationary stochastic policies that take actions in A given states in S; successor states are drawn from the dynamics model P(s\u2032|s, a). We work in the \u03b3-discounted in\ufb01nite horizon setting, and we will use an expectation with respect a policy \u03c0 \u2208 \u03a0 to denote an expectation with respect to the trajectory it generates: E\u03c0[c(s, a)] \u225c E [\ufffd\u221e t=0 \u03b3tc(st, at)], where s0 \u223c p0, at \u223c \u03c0(\u00b7|st), and st+1 \u223c P(\u00b7|st, at) for t \u2265 0. We will use \u02c6E\u03c4 to denote empirical expectation with respect to trajectory samples \u03c4, and we will always refer to the expert policy as \u03c0E. Inverse reinforcement learning Suppose we are given an expert policy \u03c0E that we wish to rationalize with IRL. For the remainder of this paper, we will adopt maximum causal entropy IRL [31, 32], which \ufb01ts a cost function from a family of functions C with the optimization problem maximize c\u2208C \ufffd min \u03c0\u2208\u03a0 \u2212H(\u03c0) + E\u03c0[c(s, a)] \ufffd \u2212 E\u03c0E[c(s, a)] (1) where H(\u03c0) \u225c E\u03c0[\u2212 log \u03c0(a|s)] is the \u03b3-discounted causal entropy [3] of the policy \u03c0. In practice, \u03c0E will only be provided as a set of trajectories sampled by executing \u03c0E in the environment, so the expected cost of \u03c0E in Eq. (1) is estimated using these samples. Maximum causal entropy IRL looks for a cost function c \u2208 C that assigns low cost to the expert policy and high cost to other policies, thereby allowing the expert policy to be found via a certain reinforcement learning procedure: RL(c) = arg min \u03c0\u2208\u03a0 \u2212H(\u03c0) + E\u03c0[c(s, a)] (2) which maps a cost function to high-entropy policies that minimize the expected cumulative cost. 3 Characterizing the induced optimal policy To begin our search for an imitation learning algorithm that both bypasses an intermediate IRL step and is suitable for large environments, we will study policies found by reinforcement learning on costs learned by IRL on the largest possible set of cost functions C in Eq. (1): all functions RS\u00d7A = {c : S \u00d7 A \u2192 R}. Using expressive cost function classes, like Gaussian processes [15] and neural networks [7], is crucial to properly explain complex expert behavior without meticulously hand-crafted features. Here, we investigate the best IRL can do with respect to expressiveness, by examining its capabilities with C = RS\u00d7A. Of course, with such a large C, IRL can easily over\ufb01t when provided a \ufb01nite dataset. Therefore, we will incorporate a (closed, proper) convex cost function regularizer \u03c8 : RS\u00d7A \u2192 R into our study. Note that convexity is a not particularly restrictive requirement: \u03c8 must be convex as a function de\ufb01ned on all of RS\u00d7A, not as a function de\ufb01ned on a small parameter space; indeed, the cost regularizers of Finn et al. [7], effective for a range of robotic manipulation tasks, satisfy this requirement. Interestingly, will in fact \ufb01nd that \u03c8 plays a central role in our discussion, not a nuisance in our analysis. Now, let us de\ufb01ne an IRL primitive procedure, which \ufb01nds a cost function such that the expert performs better than all other policies, with the cost regularized by \u03c8: IRL\u03c8(\u03c0E) = arg max c\u2208RS\u00d7A \u2212\u03c8(c) + \ufffd min \u03c0\u2208\u03a0 \u2212H(\u03c0) + E\u03c0[c(s, a)] \ufffd \u2212 E\u03c0E[c(s, a)] (3) 2 Now let \u02dcc \u2208 IRL\u03c8(\u03c0E). We are interested in a policy given by RL(\u02dcc)\u2014this is the policy given by running reinforcement learning on the output of IRL. To characterize RL(\u02dcc), it will be useful to transform optimization problems over policies into convex problems. For a policy \u03c0 \u2208 \u03a0, de\ufb01ne its occupancy measure \u03c1\u03c0 : S \u00d7 A \u2192 R as \u03c1\u03c0(s, a) = \u03c0(a|s) \ufffd\u221e t=0 \u03b3tP(st = s|\u03c0). The occupancy measure can be interpreted as the distribution of state-action pairs that an agent encounters when navigating the environment with policy \u03c0, and it allows us to write E\u03c0[c(s, a)] = \ufffd s,a \u03c1\u03c0(s, a)c(s, a) for any cost function c. A basic result [21] is that the set of valid occupancy measures D \u225c {\u03c1\u03c0 : \u03c0 \u2208 \u03a0} can be written as a feasible set of af\ufb01ne constraints: if p0(s) is the distribution of starting states and P(s\u2032|s, a) is the dynamics model, then D = \ufffd \u03c1 : \u03c1 \u2265 0 and \ufffd a \u03c1(s, a) = p0(s) + \u03b3 \ufffd s\u2032,a P(s|s\u2032, a)\u03c1(s\u2032, a) \u2200 s \u2208 S \ufffd . Furthermore, there is a one-to-one correspondence between \u03a0 and D: Proposition 3.1 (Theorem 2 of Syed et al. [29]). If \u03c1 \u2208 D, then \u03c1 is the occupancy measure for \u03c0\u03c1(a|s) \u225c \u03c1(s, a)/ \ufffd a\u2032 \u03c1(s, a\u2032), and \u03c0\u03c1 is the only policy whose occupancy measure is \u03c1. We are therefore justi\ufb01ed in writing \u03c0\u03c1 to denote the unique policy for an occupancy measure \u03c1. We will need one more tool: for a function f : RS\u00d7A \u2192 R, its convex conjugate f \u2217 : RS\u00d7A \u2192 R is given by f \u2217(x) = supy\u2208RS\u00d7A xT y \u2212 f(y). Now, we are ready to characterize RL(\u02dcc), the policy learned by RL on the cost recovered by IRL: Proposition 3.2. RL \u25e6 IRL\u03c8(\u03c0E) = arg min\u03c0\u2208\u03a0 \u2212H(\u03c0) + \u03c8\u2217(\u03c1\u03c0 \u2212 \u03c1\u03c0E) (4) The proof of Proposition 3.2 is in Appendix A.1. The proof relies on the observation that the optimal cost function and policy form a saddle point of a certain function. IRL \ufb01nds one coordinate of this saddle point, and running reinforcement learning on the output of IRL reveals the other coordinate. Proposition 3.2 tells us that \u03c8-regularized inverse reinforcement learning, implicitly, seeks a policy whose occupancy measure is close to the expert\u2019s, as measured by the convex function \u03c8\u2217. Enticingly, this suggests that various settings of \u03c8 lead to various imitation learning algorithms that directly solve the optimization problem given by Proposition 3.2. We explore such algorithms in Sections 4 and 5, where we show that certain settings of \u03c8 lead to both existing algorithms and a novel one. The special case when \u03c8 is a constant function is particularly illuminating, so we state and show it directly using concepts from convex optimization. Corollary 3.2.1. If \u03c8 is a constant function, \u02dcc \u2208 IRL\u03c8(\u03c0E), and \u02dc\u03c0 \u2208 RL(\u02dcc), then \u03c1\u02dc\u03c0 = \u03c1\u03c0E. In other words, if there were no cost regularization at all, then the recovered policy will exactly match the expert\u2019s occupancy measure. To show this, we will need a lemma that lets us speak about causal entropies of occupancy measures: Lemma 3.1. Let \u00afH(\u03c1) = \u2212 \ufffd s,a \u03c1(s, a) log(\u03c1(s, a)/ \ufffd a\u2032 \u03c1(s, a\u2032)). Then, \u00afH is strictly concave, and for all \u03c0 \u2208 \u03a0 and \u03c1 \u2208 D, we have H(\u03c0) = \u00afH(\u03c1\u03c0) and \u00afH(\u03c1) = H(\u03c0\u03c1). The proof of this lemma is in Appendix A.1. Proposition 3.1 and Lemma 3.1 together allow us to freely switch between policies and occupancy measures when considering functions involving causal entropy and expected costs, as in the following lemma: Lemma 3.2. If L(\u03c0, c) = \u2212H(\u03c0) + E\u03c0[c(s, a)] and \u00afL(\u03c1, c) = \u2212 \u00afH(\u03c1) + \ufffd s,a \u03c1(s, a)c(s, a), then, for all cost functions c, L(\u03c0, c) = \u00afL(\u03c1\u03c0, c) for all policies \u03c0 \u2208 \u03a0, and \u00afL(\u03c1, c) = L(\u03c0\u03c1, c) for all occupancy measures \u03c1 \u2208 D. Now, we are ready to give a direct proof of Corollary 3.2.1. Proof of Corollary 3.2.1. De\ufb01ne \u00afL(\u03c1, c) = \u2212 \u00afH(\u03c1) + \ufffd s,a c(s, a)(\u03c1(s, a) \u2212 \u03c1E(s, a)). Given that \u03c8 is a constant function, we have the following, due to Lemma 3.2: \u02dcc \u2208 IRL\u03c8(\u03c0E) = arg max c\u2208RS\u00d7A min \u03c0\u2208\u03a0 \u2212H(\u03c0) + E\u03c0[c(s, a)] \u2212 E\u03c0E[c(s, a)] + const. (5) = arg max c\u2208RS\u00d7A min \u03c1\u2208D \u2212 \u00afH(\u03c1) + \ufffd s,a \u03c1(s, a)c(s, a) \u2212 \ufffd s,a \u03c1E(s, a)c(s, a) = arg max c\u2208RS\u00d7A min \u03c1\u2208D \u00afL(\u03c1, c). (6) 3 This is the dual of the optimization problem minimize \u03c1\u2208D \u2212 \u00afH(\u03c1) subject to \u03c1(s, a) = \u03c1E(s, a) \u2200 s \u2208 S, a \u2208 A (7) with Lagrangian \u00afL, for which the costs c(s, a) serve as dual variables for equality constraints. Thus, \u02dcc is a dual optimum for (7). Because D is a convex set and \u2212 \u00afH is convex, strong duality holds; moreover, Lemma 3.1 guarantees that \u2212 \u00afH is in fact strictly convex, so the primal optimum can be uniquely recovered from the dual optimum [4, Section 5.5.5] via \u02dc\u03c1 = arg min\u03c1\u2208D \u00afL(\u03c1, \u02dcc) = arg min\u03c1\u2208D \u2212 \u00afH(\u03c1) + \ufffd s,a \u02dcc(s, a)\u03c1(s, a) = \u03c1E, where the \ufb01rst equality indicates that \u02dc\u03c1 is the unique minimizer of \u00afL(\u00b7, \u02dcc), and the third follows from the constraints in the primal problem (7). But if \u02dc\u03c0 \u2208 RL(\u02dcc), then, by Lemma 3.2, its occupancy measure satis\ufb01es \u03c1\u02dc\u03c0 = \u02dc\u03c1 = \u03c1E. From this argument, we can deduce the following: IRL is a dual of an occupancy measure matching problem, and the recovered cost function is the dual optimum. Classic IRL algorithms that solve reinforcement learning repeatedly in an inner loop, such as the algorithm of Ziebart et al. [31] that runs a variant of value iteration in an inner loop, can be interpreted as a form of dual ascent, in which one repeatedly solves the primal problem (reinforcement learning) with \ufb01xed dual values (costs). Dual ascent is effective if solving the unconstrained primal is ef\ufb01cient, but in the case of IRL, it amounts to reinforcement learning! The induced optimal policy is the primal optimum. The induced optimal policy is obtained by running RL after IRL, which is exactly the act of recovering the primal optimum from the dual optimum; that is, optimizing the Lagrangian with the dual variables \ufb01xed at the dual optimum values. Strong duality implies that this induced optimal policy is indeed the primal optimum, and therefore matches occupancy measures with the expert. IRL is traditionally de\ufb01ned as the act of \ufb01nding a cost function such that the expert policy is uniquely optimal, but now, we can alternatively view IRL as a procedure that tries to induce a policy that matches the expert\u2019s occupancy measure. 4 Practical occupancy measure matching We saw in Corollary 3.2.1 that if \u03c8 is constant, the resulting primal problem (7) simply matches occupancy measures with expert at all states and actions. Such an algorithm, however, is not practically useful. In reality, the expert trajectory distribution will be provided only as a \ufb01nite set of samples, so in large environments, most of the expert\u2019s occupancy measure values will be exactly zero, and exact occupancy measure matching will force the learned policy to never visit these unseen state-action pairs simply due to lack of data. Furthermore, with large environments, we would like to use function approximation to learn a parameterized policy \u03c0\u03b8. The resulting optimization problem of \ufb01nding the appropriate \u03b8 would have as many constraints as points in S \u00d7 A, leading to an intractably large problem and defeating the very purpose of function approximation. Keeping in mind that we wish to eventually develop an imitation learning algorithm suitable for large environments, we would like to relax Eq. (7) into the following form, motivated by Proposition 3.2: minimize \u03c0 d\u03c8(\u03c1\u03c0, \u03c1E) \u2212 H(\u03c0) (8) by modifying the IRL regularizer \u03c8 so that d\u03c8(\u03c1\u03c0, \u03c1E) \u225c \u03c8\u2217(\u03c1\u03c0 \u2212\u03c1E) smoothly penalizes violations in difference between the occupancy measures. Entropy-regularized apprenticeship learning It turns out that with certain settings of \u03c8, Eq. (8) takes on the form of regularized variants of existing apprenticeship learning algorithms, which indeed do scale to large environments with parameterized policies [11]. For a class of cost functions C \u2282 RS\u00d7A, an apprenticeship learning algorithm \ufb01nds a policy that performs better than the expert across C, by optimizing the objective minimize \u03c0 max c\u2208C E\u03c0[c(s, a)] \u2212 E\u03c0E[c(s, a)] (9) Classic apprenticeship learning algorithms restrict C to convex sets given by linear combinations of basis functions f1, . . . , fd, which give rise a feature vector f(s, a) = [f1(s, a), . . . , fd(s, a)] for each state-action pair. Abbeel and Ng [1] and Syed et al. [29] use, respectively, Clinear = {\ufffd iwifi : \u2225w\u22252 \u2264 1} and Cconvex = {\ufffd iwifi : \ufffd iwi = 1, wi \u2265 0 \u2200i} . (10) 4 Clinear leads to feature expectation matching [1], which minimizes \u21132 distance between expected feature vectors: maxc\u2208Clinear E\u03c0[c(s, a)]\u2212E\u03c0E[c(s, a)] = \u2225E\u03c0[f(s, a)]\u2212E\u03c0E[f(s, a)]\u22252. Meanwhile, Cconvex leads to MWAL [28] and LPAL [29], which minimize worst-case excess cost among the individual basis functions, as maxc\u2208Cconvex E\u03c0[c(s, a)] \u2212 E\u03c0E[c(s, a)] = maxi\u2208{1,...,d} E\u03c0[fi(s, a)] \u2212 E\u03c0E[fi(s, a)]. We now show how Eq. (9) is a special case of Eq. (8) with a certain setting of \u03c8. With the indicator function \u03b4C : RS\u00d7A \u2192 R, de\ufb01ned by \u03b4C(c) = 0 if c \u2208 C and +\u221e otherwise, we can write the apprenticeship learning objective (9) as max c\u2208C E\u03c0[c(s, a)]\u2212E\u03c0E[c(s, a)] = max c\u2208RS\u00d7A\u2212\u03b4C(c) + \ufffd s,a (\u03c1\u03c0(s, a)\u2212\u03c1\u03c0E(s, a))c(s, a) = \u03b4\u2217 C(\u03c1\u03c0\u2212\u03c1\u03c0E) Therefore, we see that entropy-regularized apprenticeship learning minimize \u03c0 \u2212H(\u03c0) + max c\u2208C E\u03c0[c(s, a)] \u2212 E\u03c0E[c(s, a)] (11) is equivalent to performing RL following IRL with cost regularizer \u03c8 = \u03b4C, which forces the implicit IRL procedure to recover a cost function lying in C. Note that we can scale the policy\u2019s entropy regularization strength in Eq. (11) by scaling C by a constant \u03b1 as {\u03b1c : c \u2208 C}, recovering the original apprenticeship objective (9) by taking \u03b1 \u2192 \u221e. Cons of apprenticeship learning It is known that apprenticeship learning algorithms generally do not recover expert-like policies if C is too restrictive [29, Section 1]\u2014which is often the case for the linear subspaces used by feature expectation matching, MWAL, and LPAL, unless the basis functions f1, . . . , fd are very carefully designed. Intuitively, unless the true expert cost function (assuming it exists) lies in C, there is no guarantee that if \u03c0 performs better than \u03c0E on all of C, then \u03c0 equals \u03c0E. With the aforementioned insight based on Proposition 3.2 that apprenticeship learning is equivalent to RL following IRL, we can understand exactly why apprenticeship learning may fail to imitate: it forces \u03c0E to be encoded as an element of C. If C does not include a cost function that explains expert behavior well, then attempting to recover a policy from such an encoding will not succeed. Pros of apprenticeship learning While restrictive cost classes C may not lead to exact imitation, apprenticeship learning with such C can scale to large state and action spaces with policy function approximation. Ho et al. [11] rely on the following policy gradient formula for the apprenticeship objective (9) for a parameterized policy \u03c0\u03b8: \u2207\u03b8 max c\u2208C E\u03c0\u03b8[c(s, a)] \u2212 E\u03c0E[c(s, a)] = \u2207\u03b8E\u03c0\u03b8[c\u2217(s, a)] = E\u03c0\u03b8 [\u2207\u03b8 log \u03c0\u03b8(a|s)Qc\u2217(s, a)] where c\u2217 = arg max c\u2208C E\u03c0\u03b8[c(s, a)] \u2212 E\u03c0E[c(s, a)], Qc\u2217(\u00afs, \u00afa) = E\u03c0\u03b8[c\u2217(\u00afs, \u00afa) | s0 = \u00afs, a0 = \u00afa] (12) Observing that Eq. (12) is the policy gradient for a reinforcement learning objective with cost c\u2217, Ho et al. propose an algorithm that alternates between two steps: 1. Sample trajectories of the current policy \u03c0\u03b8i by simulating in the environment, and \ufb01t a cost function c\u2217 i , as de\ufb01ned in Eq. (12). For the cost classes Clinear and Cconvex (10), this cost \ufb01tting amounts to evaluating simple analytical expressions [11]. 2. Form a gradient estimate with Eq. (12) with c\u2217 i and the sampled trajectories, and take a trust region policy optimization (TRPO) [26] step to produce \u03c0\u03b8i+1. This algorithm relies crucially on the TRPO policy step, which is a natural gradient step constrained to ensure that \u03c0\u03b8i+1 does not stray too far \u03c0\u03b8i, as measured by KL divergence between the two policies averaged over the states in the sampled trajectories. This carefully constructed step scheme ensures that divergence does not occur due to high noise in estimating the gradient (12). We refer the reader to Schulman et al. [26] for more details on TRPO. With the TRPO step scheme, Ho et al. were able train large neural network policies for apprenticeship learning with linear cost function classes (10) in environments with hundreds of observation dimensions. Their use of these linear cost function classes, however, limits their approach to settings in which expert behavior is well-described by such classes. We will draw upon their algorithm to develop an imitation learning method that both scales to large environments and imitates arbitrarily complex expert behavior. To do so, we \ufb01rst turn to proposing a new regularizer \u03c8 that wields more expressive power than the regularizers corresponding to Clinear and Cconvex (10). 5 5 Generative adversarial imitation learning As discussed in Section 4, the constant regularizer leads to an imitation learning algorithm that exactly matches occupancy measures, but is intractable in large environments. The indicator regularizers for the linear cost function classes (10), on the other hand, lead to algorithms incapable of exactly matching occupancy measures without careful tuning, but are tractable in large environments. We propose the following new cost regularizer that combines the best of both worlds, as we will show in the coming sections: \u03c8GA(c) \u225c \ufffdE\u03c0E[g(c(s, a))] if c < 0 +\u221e otherwise where g(x) = \ufffd\u2212x \u2212 log(1 \u2212 ex) if x < 0 +\u221e otherwise (13) This regularizer places low penalty on cost functions c that assign an amount of negative cost to expert state-action pairs; if c, however, assigns large costs (close to zero, which is the upper bound for costs feasible for \u03c8GA) to the expert, then \u03c8GA will heavily penalize c. An interesting property of \u03c8GA is that it is an average over expert data, and therefore can adjust to arbitrary expert datasets. The indicator regularizers \u03b4C, used by the linear apprenticeship learning algorithms described in Section 4, are always \ufb01xed, and cannot adapt to data as \u03c8GA can. Perhaps the most important difference between \u03c8GA and \u03b4C, however, is that \u03b4C forces costs to lie in a small subspace spanned by \ufb01nitely many basis functions, whereas \u03c8GA allows for any cost function, as long as it is negative everywhere. Our choice of \u03c8GA is motivated by the following fact, shown in the appendix (Corollary A.1.1): \u03c8\u2217 GA(\u03c1\u03c0 \u2212 \u03c1\u03c0E) = max D\u2208(0,1)S\u00d7A E\u03c0[log(D(s, a))] + E\u03c0E[log(1 \u2212 D(s, a))] (14) where the maximum ranges over discriminative classi\ufb01ers D : S \u00d7 A \u2192 (0, 1). Equation (14) is the optimal negative log loss of the binary classi\ufb01cation problem of distinguishing between state-action pairs of \u03c0 and \u03c0E. It turns out that this optimal loss is (up to a constant shift) the Jensen-Shannon divergence DJS(\u03c1\u03c0, \u03c1\u03c0E) \u225c DKL (\u03c1\u03c0\u2225(\u03c1\u03c0 + \u03c1E)/2) + DKL (\u03c1E\u2225(\u03c1\u03c0 + \u03c1E)/2), which is a squared metric between distributions [9, 19]. Treating the causal entropy H as a policy regularizer, controlled by \u03bb \u2265 0, we obtain a new imitation learning algorithm: minimize \u03c0 \u03c8\u2217 GA(\u03c1\u03c0 \u2212 \u03c1\u03c0E) \u2212 \u03bbH(\u03c0) = DJS(\u03c1\u03c0, \u03c1\u03c0E) \u2212 \u03bbH(\u03c0), (15) which \ufb01nds a policy whose occupancy measure minimizes Jensen-Shannon divergence to the expert\u2019s. Equation (15) minimizes a true metric between occupancy measures, so, unlike linear apprenticeship learning algorithms, it can imitate expert policies exactly. Algorithm Equation (15) draws a connection between imitation learning and generative adversarial networks [9], which train a generative model G by having it confuse a discriminative classi\ufb01er D. The job of D is to distinguish between the distribution of data generated by G and the true data distribution. When D cannot distinguish data generated by G from the true data, then G has successfully matched the true data. In our setting, the learner\u2019s occupancy measure \u03c1\u03c0 is analogous to the data distribution generated by G, and the expert\u2019s occupancy measure \u03c1\u03c0E is analogous to the true data distribution. Now, we present a practical algorithm, which we call generative adversarial imitation learning (Algorithm 1), for solving Eq. (15) for model-free imitation in large environments. Explicitly, we wish to \ufb01nd a saddle point (\u03c0, D) of the expression E\u03c0[log(D(s, a))] + E\u03c0E[log(1 \u2212 D(s, a))] \u2212 \u03bbH(\u03c0) (16) To do so, we \ufb01rst introduce function approximation for \u03c0 and D: we will \ufb01t a parameterized policy \u03c0\u03b8, with weights \u03b8, and a discriminator network Dw : S \u00d7 A \u2192 (0, 1), with weights w. Then, we alternate between an Adam [12] gradient step on w to increase Eq. (16) with respect to D, and a TRPO step on \u03b8 to decrease Eq. (16) with respect to \u03c0. The TRPO step serves the same purpose as it does with the apprenticeship learning algorithm of Ho et al. [11]: it prevents the policy from changing too much due to noise in the policy gradient. The discriminator network can be interpreted as a local cost function providing learning signal to the policy\u2014speci\ufb01cally, taking a policy step that decreases expected cost with respect to the cost function c(s, a) = log D(s, a) will move toward expert-like regions of state-action space, as classi\ufb01ed by the discriminator. (We derive an estimator for the causal entropy gradient \u2207\u03b8H(\u03c0\u03b8) in Appendix A.2.) 6 ",
    "Experiments": "Experiments We evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from lowdimensional control tasks from the classic RL literature\u2014the cartpole [2], acrobot [8], and mountain car [17]\u2014to dif\ufb01cult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently by model-free reinforcement learning [27, 26]. All environments, other than the classic control tasks, were simulated with MuJoCo [30]. See Appendix B for a complete description of all the tasks. Each task comes with a true cost function, de\ufb01ned in the OpenAI Gym [5]. We \ufb01rst generated expert behavior for these tasks by running TRPO [26] on these true cost functions to create expert policies. Then, to evaluate imitation performance with respect to sample complexity of expert data, we sampled datasets of varying trajectory counts from the expert policies. The trajectories constituting each dataset each consisted of about 50 state-action pairs. We tested Algorithm 1 against three baselines: 1. Behavioral cloning: a given dataset of state-action pairs is split into 70% training data and 30% validation data. The policy is trained with supervised learning, using Adam [12] with minibatches of 128 examples, until validation error stops decreasing. 2. Feature expectation matching (FEM): the algorithm of Ho et al. [11] using the cost function class Clinear (10) of Abbeel and Ng [1] 3. Game-theoretic apprenticeship learning (GTAL): the algorithm of Ho et al. [11] using the cost function class Cconvex (10) of Syed and Schapire [28] We used all algorithms to train policies of the same neural network architecture for all tasks: two hidden layers of 100 units each, with tanh nonlinearities in between. The discriminator networks for Algorithm 1 also used the same architecture. All networks were always initialized randomly at the start of each trial. For each task, we gave FEM, GTAL, and Algorithm 1 exactly the same amount of environment interaction for training. Figure 1 depicts the results, and the tables in Appendix B provide exact performance numbers. We found that on the classic control tasks (cartpole, acrobot, and mountain car), behavioral cloning suffered in expert data ef\ufb01ciency compared to FEM and GTAL, which for the most part were able produce policies with near-expert performance with a wide range of dataset sizes. On these tasks, our generative adversarial algorithm always produced policies performing better than behavioral cloning, FEM, and GTAL. However, behavioral cloning performed excellently on the Reacher task, on which it was more sample ef\ufb01cient than our algorithm. We were able to slightly improve our algorithm\u2019s performance on Reacher using causal entropy regularization\u2014in the 4-trajectory setting, the improvement from \u03bb = 0 to \u03bb = 10\u22123 was statistically signi\ufb01cant over training reruns, according to a one-sided Wilcoxon rank-sum test with p = .05. We used no causal entropy regularization for all other tasks. On the other MuJoCo environments, we saw a large performance boost for our algorithm over the baselines. Our algorithm almost always achieved at least 70% of expert performance for all dataset 7 ",
    "References": "References [1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning, 2004. [2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve dif\ufb01cult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, (5):834\u2013846, 1983. 8 [3] M. Bloem and N. Bambos. In\ufb01nite time horizon maximum causal entropy inverse reinforcement learning. In Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on, pages 4911\u20134916. IEEE, 2014. [4] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004. [5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. [6] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012. [7] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In Proceedings of the 33rd International Conference on Machine Learning, 2016. [8] A. Geramifard, C. Dann, R. H. Klein, W. Dabney, and J. P. How. Rlpy: A value-function-based reinforcement learning framework for education and research. JMLR, 2015. [9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages 2672\u20132680, 2014. [10] J.-B. Hiriart-Urruty and C. Lemar\u00e9chal. Convex Analysis and Minimization Algorithms, volume 305. Springer, 1996. [11] J. Ho, J. K. Gupta, and S. Ermon. Model-free imitation learning with policy optimization. In Proceedings of the 33rd International Conference on Machine Learning, 2016. [12] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [13] S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pages 1071\u20131079, 2014. [14] S. Levine and V. Koltun. Continuous inverse optimal control with locally optimal examples. In Proceedings of the 29th International Conference on Machine Learning, pages 41\u201348, 2012. [15] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement learning with gaussian processes. In Advances in Neural Information Processing Systems, pages 19\u201327, 2011. [16] P. W. Millar. The minimax principle in asymptotic statistical theory. In Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de Saint-Flour XI\u20141981, pages 75\u2013265. Springer, 1983. [17] A. W. Moore and T. Hall. Ef\ufb01cient memory-based learning for robot control. 1990. [18] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In ICML, 2000. [19] X. Nguyen, M. J. Wainwright, and M. I. Jordan. On surrogate loss functions and f-divergences. The Annals of Statistics, pages 876\u2013904, 2009. [20] D. A. Pomerleau. Ef\ufb01cient training of arti\ufb01cial neural networks for autonomous navigation. Neural Computation, 3(1):88\u201397, 1991. [21] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. [22] N. D. Ratliff, D. Silver, and J. A. Bagnell. Learning to search: Functional gradient techniques for imitation learning. Autonomous Robots, 27(1):25\u201353, 2009. [23] S. Ross and D. Bagnell. Ef\ufb01cient reductions for imitation learning. In AISTATS, pages 661\u2013668, 2010. [24] S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, pages 627\u2013635, 2011. [25] S. Russell. Learning agents for uncertain environments. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 101\u2013103. ACM, 1998. [26] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of The 32nd International Conference on Machine Learning, pages 1889\u20131897, 2015. [27] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [28] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems, pages 1449\u20131456, 2007. [29] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th International Conference on Machine Learning, pages 1032\u20131039, 2008. [30] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012. [31] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI, AAAI\u201908, 2008. [32] B. D. Ziebart, J. A. Bagnell, and A. K. Dey. Modeling interaction via the principle of maximum causal entropy. In ICML, pages 1255\u20131262, 2010. 9 A Proofs A.1 Proofs for Section 3 Proof of Lemma 3.1. First, we show strict concavity of \u00afH. Let \u03c1 and \u03c1\u2032 be occupancy measures, and suppose \u03bb \u2208 [0, 1]. For all s and a, the log-sum inequality [6] implies: \u2212(\u03bb\u03c1(s, a) + (1 \u2212 \u03bb)\u03c1\u2032(s, a)) log \u03bb\u03c1(s, a) + (1 \u2212 \u03bb)\u03c1\u2032(s, a) \ufffd a\u2032(\u03bb\u03c1(s, a\u2032) + (1 \u2212 \u03bb)\u03c1\u2032(s, a\u2032)) (19) = \u2212(\u03bb\u03c1(s, a) + (1 \u2212 \u03bb)\u03c1\u2032(s, a)) log \u03bb\u03c1(s, a) + (1 \u2212 \u03bb)\u03c1\u2032(s, a) \u03bb \ufffd a\u2032 \u03c1(s, a\u2032) + (1 \u2212 \u03bb) \ufffd a\u2032 \u03c1\u2032(s, a\u2032) (20) \u2265 \u2212\u03bb\u03c1(s, a) log \u03bb\u03c1(s, a) \u03bb \ufffd a\u2032 \u03c1(s, a\u2032) \u2212 (1 \u2212 \u03bb)\u03c1\u2032(s, a) log (1 \u2212 \u03bb)\u03c1\u2032(s, a) (1 \u2212 \u03bb) \ufffd a\u2032 \u03c1\u2032(s, a\u2032) (21) = \u03bb \ufffd \u2212\u03c1(s, a) log \u03c1(s, a) \ufffd a\u2032 \u03c1(s, a\u2032) \ufffd + (1 \u2212 \u03bb) \ufffd \u2212\u03c1\u2032(s, a) log \u03c1\u2032(s, a) \ufffd a\u2032 \u03c1\u2032(s, a\u2032) \ufffd , (22) with equality if and only if \u03c0\u03c1 \u225c \u03c1(s, a)/ \ufffd a\u2032 \u03c1(s, a\u2032) = \u03c1\u2032(s, a)/ \ufffd a\u2032 \u03c1\u2032(s, a\u2032) \u225c \u03c0\u03c1\u2032. Summing both sides over all s and a shows that \u00afH(\u03bb\u03c1 + (1 \u2212 \u03bb)\u03c1\u2032) \u2265 \u03bb \u00afH(\u03c1) + (1 \u2212 \u03bb) \u00afH(\u03c1\u2032) with equality if and only if \u03c0\u03c1 = \u03c0\u03c1\u2032. Applying Proposition 3.1 shows that equality in fact holds if and only if \u03c1 = \u03c1\u2032, so \u00afH is strictly concave. Now, we turn to verifying the last two statements, which also follow from Proposition 3.1 and the de\ufb01nition of occupancy measures. First, H(\u03c0) = E\u03c0[\u2212 log \u03c0(a|s)] (23) = \u2212 \ufffd s,a \u03c1\u03c0(s, a) log \u03c0(a|s) (24) = \u2212 \ufffd s,a \u03c1\u03c0(s, a) log \u03c1\u03c0(s, a) \ufffd a\u2032 \u03c1\u03c0(s, a\u2032) (25) = \u00afH(\u03c1\u03c0), (26) and second, \u00afH(\u03c1) = \u2212 \ufffd s,a \u03c1(s, a) log \u03c1(s, a) \ufffd a\u2032 \u03c1(s, a\u2032) (27) = \u2212 \ufffd s,a \u03c1\u03c0\u03c1(s, a) log \u03c0\u03c1(a|s) (28) = E\u03c0\u03c1[\u2212 log \u03c0\u03c1(a|s)] (29) = H(\u03c0\u03c1). (30) Proof of Proposition 3.2. This proof relies on properties of saddle points. For a reference, we refer the reader to Hiriart-Urruty and Lemar\u00e9chal [10, section VII.4]. Let \u02dcc \u2208 IRL\u03c8(\u03c0E), \u02dc\u03c0 \u2208 RL(\u02dcc) = RL \u25e6 IRL\u03c8(\u03c0E), and \u03c0A \u2208 arg min \u03c0 \u2212H(\u03c0) + \u03c8\u2217(\u03c1\u03c0 \u2212 \u03c1\u03c0E) (31) = arg min \u03c0 max c \u2212H(\u03c0) \u2212 \u03c8(c) + \ufffd s,a (\u03c1\u03c0(s, a) \u2212 \u03c1\u03c0E(s, a))c(s, a) (32) We wish to show that \u03c0A = \u02dc\u03c0. To do this, let \u03c1A be the occupancy measure of \u03c0A, let \u02dc\u03c1 be the occupancy measure of \u02dc\u03c0, and de\ufb01ne \u00afL : D \u00d7 RS\u00d7A \u2192 R by \u00afL(\u03c1, c) = \u2212 \u00afH(\u03c1) \u2212 \u03c8(c) + \ufffd s,a \u03c1(s, a)c(s, a) \u2212 \ufffd s,a \u03c1\u03c0E(s, a)c(s, a). (33) 10 The following relationships then hold, due to Proposition 3.1: \u03c1A \u2208 arg min \u03c1\u2208D max c \u00afL(\u03c1, c), (34) \u02dcc \u2208 arg max c min \u03c1\u2208D \u00afL(\u03c1, c), (35) \u02dc\u03c1 \u2208 arg min \u03c1\u2208D \u00afL(\u03c1, \u02dcc). (36) Now D is compact and convex and RS\u00d7A is convex; furthermore, due to convexity of \u2212 \u00afH and \u03c8, we also have that \u00afL(\u00b7, c) is convex for all c, and that \u00afL(\u03c1, \u00b7) is concave for all \u03c1. Therefore, we can use minimax duality [16]: min \u03c1\u2208D max c\u2208C \u00afL(\u03c1, c) = max c\u2208C min \u03c1\u2208D \u00afL(\u03c1, c) (37) Hence, from Eqs. (34) and (35), (\u03c1A, \u02dcc) is a saddle point of \u00afL, which implies that \u03c1A \u2208 arg min \u03c1\u2208D \u00afL(\u03c1, \u02dcc). (38) Because \u00afL(\u00b7, c) is strictly convex for all c (Lemma 3.1), Eqs. (36) and (38) imply \u03c1A = \u02dc\u03c1. Since policies corresponding to occupancy measures are unique (Proposition 3.1), we get \u03c0A = \u02dc\u03c0. A.2 Proofs for Section 5 In Eq. (13) of Section 5, we described a cost regularizer \u03c8GA, which leads to an imitation learning algorithm (15) that minimizes Jensen-Shannon divergence between occupancy measures. To justify our choice of \u03c8GA, we show how to convert certain surrogate loss functions \u03c6, for binary classi\ufb01cation of state-action pairs drawn from the occupancy measures \u03c1\u03c0 and \u03c1\u03c0E, into cost function regularizers \u03c8, for which \u03c8\u2217(\u03c1\u03c0 \u2212 \u03c1\u03c0E) is the minimum expected risk R\u03c6(\u03c1\u03c0, \u03c1\u03c0E) for \u03c6: R\u03c6(\u03c0, \u03c0E) = \ufffd s,a min \u03b3\u2208R \u03c1\u03c0(s, a)\u03c6(\u03b3) + \u03c1\u03c0E(s, a)\u03c6(\u2212\u03b3) (39) Speci\ufb01cally, we will restrict ourselves to strictly decreasing convex loss functions. Nguyen et al. [19] show a correspondence between minimum expected risks R\u03c6 and f-divergences, of which Jensen-Shannon divergence is a special case. Our following construction, therefore, can generate any imitation learning algorithm that minimizes an f-divergence between occupancy measures, as long as that f-divergence is induced by a strictly decreasing convex surrogate \u03c6. Proposition A.1. Suppose \u03c6 : R \u2192 R is a strictly decreasing convex function. Let T be the range of \u2212\u03c6, and de\ufb01ne g\u03c6 : R \u2192 R and \u03c8\u03c6 : RS\u00d7A \u2192 R by: g\u03c6(x) = \ufffd\u2212x + \u03c6(\u2212\u03c6\u22121(\u2212x)) if x \u2208 T +\u221e otherwise \u03c8\u03c6(c) = \uf8f1 \uf8f2 \uf8f3 \ufffd s,a \u03c1\u03c0E(s, a)g\u03c6(c(s, a)) if c(s, a) \u2208 T for all s, a +\u221e otherwise (40) Then, \u03c8\u03c6 is closed, proper, and convex, and RL \u25e6 IRL\u03c8\u03c6(\u03c0E) = arg min\u03c0 \u2212H(\u03c0) \u2212 R\u03c6(\u03c1\u03c0, \u03c1\u03c0E). Proof. To verify the \ufb01rst claim, it suf\ufb01ces to check that g\u03c6(x) = \u2212x + \u03c6(\u2212\u03c6\u22121(\u2212x)) is closed, proper, and convex. Convexity follows from the fact that x \ufffd\u2192 \u03c6(\u2212\u03c6\u22121(\u2212x)) is convex, because it is a concave function followed by a nonincreasing convex function. Furthermore, because T is nonempty, g\u03c6 is proper. To show that g\u03c6 is closed, note that because \u03c6 is strictly decreasing and convex, the range of \u03c6 is either all of R or an open interval (b, \u221e) for some b \u2208 R. If the range of \u03c6 is R, then g\u03c6 is \ufb01nite everywhere and is therefore closed. On the other hand, if the range of \u03c6 is (b, \u221e), then \u03c6(x) \u2192 b as x \u2192 \u221e, and \u03c6(x) \u2192 \u221e as x \u2192 \u2212\u221e. Thus, as x \u2192 b, \u03c6\u22121(\u2212x) \u2192 \u221e, so \u03c6(\u2212\u03c6\u22121(\u2212x)) \u2192 \u221e too, implying that g\u03c6(x) \u2192 \u221e as x \u2192 b, which means g\u03c6 is closed. 11 Now, we verify the second claim. By Proposition 3.2, all we need to check is that \u2212R\u03c6(\u03c1\u03c0, \u03c1\u03c0E) = \u03c8\u2217 \u03c6(\u03c1\u03c0 \u2212 \u03c1\u03c0E): \u03c8\u2217 \u03c6(\u03c1\u03c0 \u2212 \u03c1\u03c0E) = max c\u2208C \ufffd s,a (\u03c1\u03c0(s, a) \u2212 \u03c1\u03c0E(s, a))c(s, a) \u2212 \ufffd s,a \u03c1\u03c0E(s, a)g\u03c6(c(s, a)) (41) = \ufffd s,a max c\u2208T (\u03c1\u03c0(s, a) \u2212 \u03c1\u03c0E(s, a))c \u2212 \u03c1\u03c0E(s, a)[\u2212c + \u03c6(\u2212\u03c6\u22121(\u2212c))] (42) = \ufffd s,a max c\u2208T \u03c1\u03c0(s, a)c \u2212 \u03c1\u03c0E(s, a)\u03c6(\u2212\u03c6\u22121(\u2212c)) (43) = \ufffd s,a max \u03b3\u2208R \u03c1\u03c0(s, a)(\u2212\u03c6(\u03b3)) \u2212 \u03c1\u03c0E(s, a)\u03c6(\u2212\u03c6\u22121(\u03c6(\u03b3))) (44) = \ufffd s,a max \u03b3\u2208R \u03c1\u03c0(s, a)(\u2212\u03c6(\u03b3)) \u2212 \u03c1\u03c0E(s, a)\u03c6(\u2212\u03b3) (45) = \u2212R\u03c6(\u03c1\u03c0, \u03c1\u03c0E) (46) where we made the change of variables c \u2192 \u2212\u03c6(\u03b3), justi\ufb01ed because T is the range of \u2212\u03c6. Having showed how to construct a cost function regularizer \u03c8\u03c6 from \u03c6, we obtain, as a corollary, a cost function regularizer for the logistic loss, whose optimal expected risk is, up to a constant, the Jensen-Shannon divergence. Corollary A.1.1. The cost regularizer (13) \u03c8GA(c) \u225c \ufffdE\u03c0E[g(c(s, a))] if c < 0 +\u221e otherwise where g(x) = \ufffd\u2212x \u2212 log(1 \u2212 ex) if x < 0 +\u221e otherwise satis\ufb01es \u03c8\u2217 GA(\u03c1\u03c0 \u2212 \u03c1\u03c0E) = max D\u2208(0,1)S\u00d7A E\u03c0[log(D(s, a))] + E\u03c0E[log(1 \u2212 D(s, a))]. (47) Proof. Using the logistic loss \u03c6(x) = log(1 + e\u2212x), we see that Eq. (40) reduces to the claimed \u03c8GA. Applying Proposition A.1, we get \u03c8\u2217 GA(\u03c1\u03c0 \u2212 \u03c1\u03c0E) = \u2212R\u03c6(\u03c1\u03c0, \u03c1\u03c0E) (48) = \ufffd s,a max \u03b3\u2208R \u03c1\u03c0(s, a) log \ufffd 1 1 + e\u2212\u03b3 \ufffd + \u03c1\u03c0E(s, a) log \ufffd 1 1 + e\u03b3 \ufffd (49) = \ufffd s,a max \u03b3\u2208R \u03c1\u03c0(s, a) log \ufffd 1 1 + e\u2212\u03b3 \ufffd + \u03c1\u03c0E(s, a) log \ufffd 1 \u2212 1 1 + e\u2212\u03b3 \ufffd (50) = \ufffd s,a max \u03b3\u2208R \u03c1\u03c0(s, a) log(\u03c3(\u03b3)) + \u03c1\u03c0E(s, a) log(1 \u2212 \u03c3(\u03b3)), (51) where \u03c3(x) = 1/(1 + e\u2212x) is the sigmoid function. Because the range of \u03c3 is (0, 1), we can write \u03c8\u2217 GA(\u03c1\u03c0 \u2212 \u03c1\u03c0E) = \ufffd s,a max d\u2208(0,1) \u03c1\u03c0(s, a) log d + \u03c1\u03c0E(s, a) log(1 \u2212 d) (52) = max D\u2208(0,1)S\u00d7A \ufffd s,a \u03c1\u03c0(s, a) log(D(s, a)) + \u03c1\u03c0E(s, a) log(1 \u2212 D(s, a)), (53) which is the desired expression. We conclude with a policy gradient formula for causal entropy. Lemma A.1. The causal entropy gradient is given by \u2207\u03b8E\u03c0\u03b8[\u2212 log \u03c0\u03b8(a|s)] = E\u03c0\u03b8 [\u2207\u03b8 log \u03c0\u03b8(a|s)Qlog(s, a)] , where Qlog(\u00afs, \u00afa) = E\u03c0\u03b8[\u2212 log \u03c0\u03b8(a|s) | s0 = \u00afs, a0 = \u00afa]. (54) 12 Proof. For an occupancy measure \u03c1(s, a), de\ufb01ne \u03c1(s) = \ufffd a \u03c1(s, a). Next, \u2207\u03b8E\u03c0\u03b8[\u2212 log \u03c0\u03b8(a|s)] = \u2212\u2207\u03b8 \ufffd s,a \u03c1\u03c0\u03b8(s, a) log \u03c0\u03b8(a|s) = \u2212 \ufffd s,a (\u2207\u03b8\u03c1\u03c0\u03b8(s, a)) log \u03c0\u03b8(a|s) \u2212 \ufffd s \u03c1\u03c0\u03b8(s) \ufffd a \u03c0\u03b8(a|s)\u2207\u03b8 log \u03c0\u03b8(a|s) = \u2212 \ufffd s,a (\u2207\u03b8\u03c1\u03c0\u03b8(s, a)) log \u03c0\u03b8(a|s) \u2212 \ufffd s \u03c1\u03c0\u03b8(s) \ufffd a \u2207\u03b8\u03c0\u03b8(a|s) The second term vanishes, because \ufffd a \u2207\u03b8\u03c0\u03b8(a|s) = \u2207\u03b8 \ufffd a \u03c0\u03b8(a|s) = \u2207\u03b81 = 0. We are left with \u2207\u03b8E\u03c0\u03b8[\u2212 log \u03c0\u03b8(a|s)] = \ufffd s,a (\u2207\u03b8\u03c1\u03c0\u03b8(s, a))(\u2212 log \u03c0\u03b8(a|s)), which is the policy gradient for RL with the \ufb01xed cost function clog(s, a) \u225c \u2212 log \u03c0\u03b8(a|s). The resulting formula is given by the standard policy gradient formula for clog. B Environments and detailed results The environments we used for our experiments are from the OpenAI Gym [5]. The names and version numbers of these environments are listed in Table 1, which also lists dimension or cardinality of their observation and action spaces (numbers marked \u201ccontinuous\u201d indicate dimension for a continuous space, and numbers marked \u201cdiscrete\u201d indicate cardinality for a \ufb01nite space). Table 1: Environments Task Observation space Action space Random policy performance Expert performance Cartpole-v0 4 (continuous) 2 (discrete) 18.64 \u00b1 7.45 200.00 \u00b1 0.00 Acrobot-v0 4 (continuous) 3 (discrete) \u2212200.00 \u00b1 0.00 \u221275.25 \u00b1 10.94 Mountain Car-v0 2 (continuous) 3 (discrete) \u2212200.00 \u00b1 0.00 \u221298.75 \u00b1 8.71 Reacher-v1 11 (continuous) 2 (continuous) \u221243.21 \u00b1 4.32 \u22124.09 \u00b1 1.70 HalfCheetah-v1 17 (continuous) 6 (continuous) \u2212282.43 \u00b1 79.53 4463.46 \u00b1 105.83 Hopper-v1 11 (continuous) 3 (continuous) 14.47 \u00b1 7.96 3571.38 \u00b1 184.20 Walker-v1 17 (continuous) 6 (continuous) 0.57 \u00b1 4.59 6717.08 \u00b1 845.62 Ant-v1 111 (continuous) 8 (continuous) \u221269.68 \u00b1 111.10 4228.37 \u00b1 424.16 Humanoid-v1 376 (continuous) 17 (continuous) 122.87 \u00b1 35.11 9575.40 \u00b1 1750.80 The amount of environment interaction used for FEM, GTAL, and our algorithm is shown in Table 2. To reduce gradient variance for these three algorithms, we also \ufb01t value functions, with the same neural network architecture as the policies, and employed generalized advantage estimation [27] (with \u03b3 = .995 and \u03bb = .97). The exact experimental results are listed in Table 3. Means and standard deviations are computed over 50 trajectories. For the cartpole, mountain car, acrobot, and reacher, these statistics are further computed over 7 policies learned from random initializations. Table 2: Parameters for FEM, GTAL, and Algorithm 1 Task Training iterations State-action pairs per iteration Cartpole 300 5000 Mountain Car 300 5000 Acrobot 300 5000 Reacher 200 50000 HalfCheetah 500 50000 Hopper 500 50000 Walker 500 50000 Ant 500 50000 Humanoid 1500 50000 13 Table 3: Learned policy performance Task Dataset size Behavioral cloning FEM GTAL Ours Cartpole 1 72.02 \u00b1 35.82 200.00 \u00b1 0.00 200.00 \u00b1 0.00 200.00 \u00b1 0.00 4 169.18 \u00b1 59.81 200.00 \u00b1 0.00 200.00 \u00b1 0.00 200.00 \u00b1 0.00 7 188.60 \u00b1 29.61 200.00 \u00b1 0.00 199.94 \u00b1 1.14 200.00 \u00b1 0.00 10 177.19 \u00b1 52.83 199.75 \u00b1 3.50 200.00 \u00b1 0.00 200.00 \u00b1 0.00 Acrobot 1 \u2212130.60 \u00b1 55.08 \u2212133.14 \u00b1 60.80 \u221281.35 \u00b1 22.40 \u221277.26 \u00b1 18.03 4 \u221293.20 \u00b1 32.58 \u221294.21 \u00b1 47.20 \u221294.80 \u00b1 46.08 \u221283.12 \u00b1 23.31 7 \u221296.92 \u00b1 34.51 \u221295.08 \u00b1 46.67 \u221295.75 \u00b1 46.57 \u221282.56 \u00b1 20.95 10 \u221295.09 \u00b1 33.33 \u221277.22 \u00b1 18.51 \u221294.32 \u00b1 46.51 \u221278.91 \u00b1 15.76 Mountain Car 1 \u2212136.76 \u00b1 34.44 \u2212100.97 \u00b1 12.54 \u2212115.48 \u00b1 36.35 \u2212101.55 \u00b1 10.32 4 \u2212133.25 \u00b1 29.97 \u221299.29 \u00b1 8.33 \u2212143.58 \u00b1 50.08 \u2212101.35 \u00b1 10.63 7 \u2212127.34 \u00b1 29.15 \u2212100.65 \u00b1 9.36 \u2212128.96 \u00b1 46.13 \u221299.90 \u00b1 7.97 10 \u2212123.14 \u00b1 28.26 \u2212100.48 \u00b1 8.14 \u2212120.05 \u00b1 36.66 \u2212100.83 \u00b1 11.40 HalfCheetah 4 \u2212493.62 \u00b1 246.58 734.01 \u00b1 84.59 1008.14 \u00b1 280.42 4515.70 \u00b1 549.49 11 637.57 \u00b1 1708.10 \u2212375.22 \u00b1 291.13 226.06 \u00b1 307.87 4280.65 \u00b1 1119.93 18 2705.01 \u00b1 2273.00 343.58 \u00b1 159.66 1084.26 \u00b1 317.02 4749.43 \u00b1 149.04 25 3718.58 \u00b1 1856.22 502.29 \u00b1 375.78 869.55 \u00b1 447.90 4840.07 \u00b1 95.36 Hopper 4 50.57 \u00b1 0.95 3571.98 \u00b1 6.35 3065.21 \u00b1 147.79 3614.22 \u00b1 7.17 11 1025.84 \u00b1 266.86 3572.30 \u00b1 12.03 3502.71 \u00b1 14.54 3615.00 \u00b1 4.32 18 1949.09 \u00b1 500.61 3230.68 \u00b1 4.58 3201.05 \u00b1 6.74 3600.70 \u00b1 4.24 25 3383.96 \u00b1 657.61 3331.05 \u00b1 3.55 3458.82 \u00b1 5.40 3560.85 \u00b1 3.09 Walker 4 32.18 \u00b1 1.25 3648.17 \u00b1 327.41 4945.90 \u00b1 65.97 4877.98 \u00b1 2848.37 11 5946.81 \u00b1 1733.73 4723.44 \u00b1 117.18 6139.29 \u00b1 91.48 6850.27 \u00b1 39.19 18 1263.82 \u00b1 1347.74 4184.34 \u00b1 485.54 5288.68 \u00b1 37.29 6964.68 \u00b1 46.30 25 1599.36 \u00b1 1456.59 4368.15 \u00b1 267.17 4687.80 \u00b1 186.22 6832.01 \u00b1 254.64 Ant 4 1611.75 \u00b1 359.54 \u22122052.51 \u00b1 49.41 \u22125743.81 \u00b1 723.48 3186.80 \u00b1 903.57 11 3065.59 \u00b1 635.19 \u22124462.70 \u00b1 53.84 \u22126252.19 \u00b1 409.42 3306.67 \u00b1 988.39 18 2597.22 \u00b1 1366.57 \u22125148.62 \u00b1 37.80 \u22123067.07 \u00b1 177.20 3033.87 \u00b1 1460.96 25 3235.73 \u00b1 1186.38 \u22125122.12 \u00b1 703.19 \u22123271.37 \u00b1 226.66 4132.90 \u00b1 878.67 Humanoid 80 1397.06 \u00b1 1057.84 5093.12 \u00b1 583.11 5096.43 \u00b1 24.96 10200.73 \u00b1 1324.47 160 3655.14 \u00b1 3714.28 5120.52 \u00b1 17.07 5412.47 \u00b1 19.53 10119.80 \u00b1 1254.73 240 5660.53 \u00b1 3600.70 5192.34 \u00b1 24.59 5145.94 \u00b1 21.13 10361.94 \u00b1 61.28 Task Dataset size Behavioral cloning Ours (\u03bb = 0) Ours (\u03bb = 10\u22123) Ours (\u03bb = 10\u22122) Reacher 4 \u221210.97 \u00b1 7.07 \u221267.23 \u00b1 88.99 \u221232.37 \u00b1 39.81 \u221246.72 \u00b1 82.88 11 \u22126.23 \u00b1 3.29 \u22126.06 \u00b1 5.36 \u22126.61 \u00b1 5.11 \u22129.26 \u00b1 21.88 18 \u22124.76 \u00b1 2.31 \u22128.25 \u00b1 21.99 \u22125.66 \u00b1 3.15 \u22125.04 \u00b1 2.22 14 ",
    "title": "Generative Adversarial Imitation Learning",
    "paper_info": "Generative Adversarial Imitation Learning\nJonathan Ho\nStanford University\nhoj@cs.stanford.edu\nStefano Ermon\nStanford University\nermon@cs.stanford.edu\nAbstract\nConsider learning a policy from example expert behavior, without interaction\nwith the expert or access to reinforcement signal. One approach is to recover the\nexpert\u2019s cost function with inverse reinforcement learning, then extract a policy\nfrom that cost function with reinforcement learning. This approach is indirect\nand can be slow. We propose a new general framework for directly extracting a\npolicy from data, as if it were obtained by reinforcement learning following inverse\nreinforcement learning. We show that a certain instantiation of our framework\ndraws an analogy between imitation learning and generative adversarial networks,\nfrom which we derive a model-free imitation learning algorithm that obtains signif-\nicant performance gains over existing model-free methods in imitating complex\nbehaviors in large, high-dimensional environments.\n1\nIntroduction\nWe are interested in a speci\ufb01c setting of imitation learning\u2014the problem of learning to perform a\ntask from expert demonstrations\u2014in which the learner is given only samples of trajectories from\nthe expert, is not allowed to query the expert for more data while training, and is not provided\nreinforcement signal of any kind. There are two main approaches suitable for this setting: behavioral\ncloning [20], which learns a policy as a supervised learning problem over state-action pairs from\nexpert trajectories; and inverse reinforcement learning [25, 18], which \ufb01nds a cost function under\nwhich the expert is uniquely optimal.\nBehavioral cloning, while appealingly simple, only tends to succeed with large amounts of data, due\nto compounding error caused by covariate shift [23, 24]. Inverse reinforcement learning (IRL), on\nthe other hand, learns a cost function that prioritizes entire trajectories over others, so compounding\nerror, a problem for methods that \ufb01t single-timestep decisions, is not an issue. Accordingly, IRL has\nsucceeded in a wide range of problems, from predicting behaviors of taxi drivers [31] to planning\nfootsteps for quadruped robots [22].\nUnfortunately, many IRL algorithms are extremely expensive to run, requiring reinforcement learning\nin an inner loop. Scaling IRL methods to large environments has thus been the focus of much\nrecent work [7, 14]. Fundamentally, however, IRL learns a cost function, which explains expert\nbehavior but does not directly tell the learner how to act. Given that learner\u2019s true goal often is to\ntake actions imitating the expert\u2014indeed, many IRL algorithms are evaluated on the quality of the\noptimal actions of the costs they learn\u2014why, then, must we learn a cost function, if doing so possibly\nincurs signi\ufb01cant computational expense yet fails to directly yield actions?\nWe desire an algorithm that tells us explicitly how to act by directly learning a policy. To develop such\nan algorithm, we begin in Section 3, where we characterize the policy given by running reinforcement\nlearning on a cost function learned by maximum causal entropy IRL [31, 32]. Our characterization\nintroduces a framework for directly learning policies from data, bypassing any intermediate IRL step.\nThen, we instantiate our framework in Sections 4 and 5 with a new model-free imitation learning\nalgorithm. We show that our resulting algorithm is intimately connected to generative adversarial\narXiv:1606.03476v1  [cs.LG]  10 Jun 2016\n",
    "GPTsummary": "\n\n\n\n\n8. Conclusion: \n\n- (1): The work presented in this article is significant because it proposes a new framework for learning policies directly from expert behavior without access to rewards or interactions with the expert. This can greatly simplify the learning process and has potential applications in various fields, such as robotics and gaming.\n\n- (2): In terms of innovation, this article proposes a novel framework that bridges the gap between inverse reinforcement learning and imitation learning, and directly extracts a policy from data. The performance of the proposed algorithm is evaluated on several domains, and it achieves significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments. However, the workload of this algorithm may be high due to the complexity of the domains, and further research could focus on improving its scalability.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The work presented in this article is significant because it proposes a new framework for learning policies directly from expert behavior without access to rewards or interactions with the expert. This can greatly simplify the learning process and has potential applications in various fields, such as robotics and gaming.\n\n- (2): In terms of innovation, this article proposes a novel framework that bridges the gap between inverse reinforcement learning and imitation learning, and directly extracts a policy from data. The performance of the proposed algorithm is evaluated on several domains, and it achieves significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments. However, the workload of this algorithm may be high due to the complexity of the domains, and further research could focus on improving its scalability.\n\n\n"
}