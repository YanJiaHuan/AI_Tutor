{
    "Abstract": "Abstract A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system. 1. ",
    "Introduction": "Introduction An agent is an abstraction of a decision-maker. At each time instance t, it receives an observation ot \u2208 O, and outputs an action at \u2208 A to be carried out in the environment it lives in. Here, O is the (\ufb01nite) set of possible observations the agent can receive, and A is the (\ufb01nite) set of actions the agent can choose from. An agent\u2019s observation ot depends on the current environment state st \u2208 S through an agent observation function S \u2192 O, where S is the set of possible environment states. The observation history ho t = (o1, o2..., ot) is the sequence of observations the agent has received till time t. Let Ho t be the set of possible observation histories of length t, the policy \u03c0t : Ho t \u2192 A at time t is de\ufb01ned as the mapping from an observation history of length t to the action the agent will take. An agent\u2019s behavior can thus be fully speci\ufb01ed by its policy across all timesteps \u03c0 = (\u03c01, \u03c02, ..., \u03c0t, ...). Throughout the paper, it is assumed that an agent has a \ufb01nite lifespan T. 1.1. Scalar Reward Reinforcement Learning System We are interested in agents that can achieve some goal. In reinforcement learning, a goal is expressed by a scalar signal rt \u2208 R called the reward. The reward is dependent on the agent\u2019s observation history, and is assumed to be available to the agent at each timestep in addition to the observation 1Department of Electrical and Computer Engineering, University of Waterloo, Canada. Correspondence to: Changjian Li <changjian.li@uwaterloo.ca>. Preliminary work. Under Review. ot. Our aim is to \ufb01nd policies that maximize the expected cumulative reward an agent receives over its lifetime: max \u03c0 E[ T \ufffd t=1 rt(ho t)] (1) Using the maximization of expected cumulative scalar reward to formulate the general notion of goal is a design choice in reinforcement learning, based on what is commonly known as the reward hypothesis (Sutton & Barto, 2018), In Sutton\u2019s own words: That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). This design choice, however, is somewhat arbitrary. Among other things, the reward needs not be a scalar (e.g. multiobjective reinforcement learning (White, 1982)), nor does it have to be a quantity whose cumulative sum is to be maximized (which we will come to shortly). Leaving aside the question of whether or not all goals can be formulated by Eq. 1, I intend to show in this paper that the problem of lifelong reinforcement learning probably should not be formulated as such. Note that in Eq. 1, I de\ufb01ned the reward in terms of the observation history, instead of the history of environment states as in most reinforcement learning literature. This re\ufb02ects the view that reward signals are internal to the agent, as pointed out by Singh et al. (2004) in their work on intrinsic motivation. Since the observations are all that the agent has access to from the external environment, the intrinsic reward should depend on the environment state only through the agent\u2019s observation history. Although the above reinforcement learning formulation recognizes the reward as a signal intrinsic to an agent, it focuses on learning across different generations 1 of agents, as opposed to learning within an agent\u2019s lifespan. From an agent\u2019s point of view, the cumulative reward is known only when it reaches its end of life, by which time no learning can 1Usage of the word \u2018generation\u2019 here is only to emphasize that learning cannot be achieved within an agent\u2019s lifespan, and does not imply that evolution algorithms need to be used. arXiv:2001.09608v1  [cs.LG]  27 Jan 2020 Some Insights into Lifelong Reinforcement Learning Systems Figure 1: Architecture of a traditional reinforcement learning system. At the beginning of an agent\u2019s life, it receives a policy \u03c0i = (\u03c0i 1, \u03c0i 2, ...\u03c0i T ) from the learning algorithm that carries out a mix of exploitation and exploration, where the superscript i indicates that the agent belongs to the ith generation. The agent receives an observation ot at each timestep t, and act according to \u03c0i t. At the end of the agent\u2019s life, the learning algorithm gathers the observation history ho T and the cumulative reward \ufffdT t=1 r(ho t) from the agent, and outputs the the next policy \u03c0i+1 to be executed. The learning algorithm does not need to optimize the performance of any particular \u03c0i, as long as it is guaranteed to be able to eventually \ufb01nd the policies that maximize the expected cumulative reward. be done by the \u2018dying\u2019 agent itself. The individual reward received at each timestep does not really matter, since the optimization objective is the cumulative sum (of reward). The information gathered by the agent, however, can be used to improve the policy of the next generation. In other words, with the conventional reinforcement learning formulation, learning can only happen at a level higher than the lives of individual agents (Figure 1), with the goal that an optimal agent can eventually be found \u2014 the lifetime behavior of a particular agent is not of concern. 1.2. Towards Lifelong Reinforcement Learning In lifelong reinforcement learning, on the other hand, the focus is the agent\u2019s ability to learn and adapt to the environment throughout its lifetime. Intuitively, this implies that learning component of the learning system should reside within the agent. To shed some lights on lifelong reinforcement learning, consider the Q-learning (Watkins & Dayan, 1992) algorithm for the standard reinforcement learning problem formulated by Eq. 1. For the purpose of this example only, it is further assumed that: \u2022 The reward depends only on the current observation. I.e., r(ho t) = r(ot) \u2022 Observations are Markov with respect to past observations and actions. I.e., P(ot|ot\u22121, at\u22121, ..., o1, a1) = P(ot|ot\u22121, at\u22121) These assumptions are only made so that Q-learning will \ufb01nd the solution to Eq. 1, and are not essential for the general discussion. The (non-lifelong) learning system works as follows: 1. The agent receives its initial Q estimate from the past generation. 2. At each timestep t, the agent takes an \u03f5-greedy action based on the current Q estimate, then does a Bellman update on the Q estimate: Q(ot, at) := Q(ot, at) + \u03b1(r(ot) + max a Q(ot+1, a) \u2212 Q(ot, at)) (2) 3. When the agent dies, pass the updated Q estimate to the next generation. At \ufb01rst sight, the fact that the Q estimate is updated every timestep seems to contradict my argument that learning only happens across generations. However, for Eq. 2 to be a valid update, the timestep t needs to be part of the observation \u2014 the observation ot here is in fact the raw observation o\u2212 t augmented by time t, i.e., ot = (o\u2212 t , t). Since the timestep is part of the observation, no same observation will be experienced more than once throughout the agent\u2019s lifetime, and it makes no difference to the agent whether the Q estimate is updated every timestep, or after its life ends 2. It\u2019s clear that for an agent to exhibit any sensible behavior, the initial Q estimate it inherits from the past generation is vital. If the agent receives a random initial Q estimate, then it\u2019s lifelong behavior is bound to be random and meaningless. On the other side of the spectrum, if the agent receives the true Q function, then it will behave optimally. This suggests that if we care about the lifetime behaviour (which includes lifelong learning behavior) of a Q-learning agent, then Q(ot, \u00b7) is a fundamental signal the agent needs to receive in addition to the scalar reward. In a sense, if the signal represented by the scalar reward is a speci\ufb01cation of what the goal is, then the signal represented by the Q estimate is the knowledge past generations have collected about what the goal means for this type of agent. As an analogy, the pain associated with falling to the ground could be the former signal, while the innate fear of height could be the latter. 2The statement does not strictly hold true if function approximation is used. An update to Q\u03b8(ot, a) can potentially affect the Q estimate of all other observations. However, this is more a side effect than a desired property. Some Insights into Lifelong Reinforcement Learning Systems From a computational perspective, the separation of these two signals may not be necessary. Both signals can be considered as \u2018annotations\u2019 for the observation history that the agent receives along with its observation, and can be incorporated into the concept of reward. The reward signals are no longer restricted scalars, nor are they necessarily quantities whose cumulative sum is to be maximized \u2014 they are just messages in some reward language that \u2018encode\u2019 the knowledge pertaining to an agent\u2019s observation history \u2014 knowledge that enables the agent to learn continuously throughout its life. Such knowledge may include the goals of the agent, the subgoals that constitute these goals, the heuristics for achieving them, and so on. The reward is then \u2018decoded\u2019 by the learning algorithm, which de\ufb01nes how the agent responds to the reward given the observation history. The learning system should be designed such that by responding to the reward in its intended way, the agent will learn to achieve the goals implied by the reward before its end of life (Figure 2). To be precise, the reward r(ho t) \u2208 \u03a3 now belongs to some reward space \u03a3. The learning algorithm is a mapping from reward histories to policies. Denoting the set of possible reward history of length t as Hr t , and the set of all possible policies at time t as \u03a0t, the learning algorithm m can be represented by m = (m1, m2, ..., mt, ..., mT ), where mt : Hr t \u2192 \u03a0t. The formulation is general, and a learning system formulated as such is not automatically a lifelong learning system. In fact, it subsumes traditional reinforcement learning: the reward space is set to the real numbers (\u03a3 = R), and the learning algorithm can be set to any algorithm that converges to a policy that maximizes the expected cumulative reward. Unfortunately, the reward in traditional reinforcement learning does not contain enough information for an agent to learn within its lifetime. Viewing the reward as a general language, and the learning algorithm as the response to the reward opens up the possibilities for principled ways to embed learning bias such as guidance and intrinsic motivation into the learning system, instead of relying solely on manipulating the scalar reward on an ad-hoc basis. In the rest of the paper, my focus remains on lifelong reinforcement learning, more speci\ufb01cally, what lifelong reinforcement learning requires of the reward language and the corresponding learning algorithm. 1.3. Reward as Formal Language Although the term \u2018language\u2019 used above can be understood in its colloquial sense, it can also be understood as the formal term in automata theory. To see this, consider the following deterministic \ufb01nite automaton \u27e8\u03a3, Q, \u03b4, q0, F\u27e9, where: \u2022 \u03a3 is the alphabet of the automaton, and is set to the reward space of the learning system. In other words, Figure 2: Architecture of lifelong reinforcement learning system. In contrast to traditional reinforcement learning (Figure 1), the learning algorithm resides inside the agent. The internal environment of the agent can be thought of as a built-in mechanism for the agent-designer to communicate with the agent (through the reward). At each timestep, the learning algorithm receives some message (encoded in the form of reward r(ho t)) from the agent\u2019s internal environment, and outputs a policy \u03c0t as a response. the alphabet of this automaton consists of all possible reward the agent can receive at any single timestep. A string is a sequence of symbols chosen from some alphabet. For this particular automaton, a string is in fact a sequence of reward, so the notation for reward history hr t is also used to denote a string of length t. The set of all strings of length k over \u03a3 is denoted as \u03a3k, and the set of all strings (of any length) is denoted as \u03a3\u2217. \u2022 Q is the set of states of the automaton. Each state of this automaton is a possible pair of reward history and policies till some timestep t. For example, members of Q include: \u27e8hr t=1, (\u03c01)\u27e9 \u27e8hr t=2, (\u03c01, \u03c02)\u27e9 ... \u27e8hr t=T , (\u03c01, \u03c02, ..., \u03c0T )\u27e9 for any \u03c01 \u2208 \u03a01, \u03c02 \u2208 \u03a02, ..., \u03c0T \u2208 \u03a0T , and hr t=1 \u2208 \u03a31, hr t=2 \u2208 \u03a32, ..., hr t=T \u2208 \u03a3T . In addition, Q has a special \u2018empty\u2019 member q0, which corresponds to the initial state before any reward is received. \u2022 \u03b4 : (Q \u00d7 \u03a3) \u2192 Q is the transition function. The transition function corresponds to the learning algorithm of the learning system, so we have \u03b4(\u27e8hr t, (\u03c01, ..., \u03c0t)\u27e9, rt+1) = \u27e8hr t+1, (\u03c01, ..., \u03c0t, mt+1(hr t+1))\u27e9, where hr t+1 = (hr t, rt+1). Some Insights into Lifelong Reinforcement Learning Systems \u2022 q0 is the initial state of the automaton as explained above. \u2022 F \u2282 Q is the set of accepting states, which are the desired states of the automaton. It\u2019s not hard to see that this automaton is a model of the learning system described in Section 1.2, with its desired property speci\ufb01ed by the accepting states F. In this paper, the desired property is that the system be a lifelong learning system, so the accepting states F are the set of \u27e8hr T , (\u03c01, \u03c02, ..., \u03c0T )\u27e9 pairs that correspond to a lifelong learner 3. To specify learning objectives, each possible reward r \u2208 \u03a3 is assigned some semantics. These semantics implicitly de\ufb01ne the set of valid reward sequences L \u2282 \u03a3\u2217. Since L is a subset of \u03a3\u2217, it is a language over \u03a3. We want to make sure that \u2014 for all reward sequences in L, lifelong learning can be achieved by the learning system abstracted by this automaton, or equivalently, all reward sequences in L lead to accepting states F. 2. A Prototype Lifelong Reinforcement Learning System Designing a lifelong reinforcement learning system involves designing the reward language and the learning algorithm holistically. Intuitively, the reward needs to contain enough information to control the relevant aspects of the learning algorithm, and the learning algorithm in turn needs to \u2018interpret\u2019 the reward signal in its intended way. In this section, I aim to provide some insights into the design process with a prototype lifelong reinforcement learning system. 2.1. Reward Language The main reason lifelong learning is impossible in conventional reinforcement learning is that the learning objective in conventional reinforcement learning is global, in the sense that the goal of the agent is de\ufb01ned in terms of the observation history of its entire life. For a lifelong reinforcement learning agent, the learning objectives should instead be local, meaning that the goals should be de\ufb01ned only for some smaller tasks that the agent can encounter multiple times during its lifetime. Once a local goal expires, whether it is because the goal has been achieved or because the agent has failed to achieve it within a certain time limit, a new local goal (can potentially be another instantiation of the same goal) ensues. This way, the agent has the opportunity to gather knowledge for each of the goals, and improve upon 3Recall that an agent\u2019s behavior is fully decided by its policy \u03c0 = (\u03c01, \u03c02, ..., \u03c0T ). Therefore given a reward history hr T , the policy is suf\ufb01cient for us to tell whether the agent is a successful lifelong learner. them, all within one life. Local goals like this are ubiquitous for humans. For example, when a person is hungry, his main concern is probably not the global goal of being happy for the rest of his life \u2014 his goal is to have food. After the person is full, he might feel like taking a nap, which is another local goal. In fact, the local goals and the transition of them seems to embody what we mean by intrinsic motivation. To be able to specify a series of local goals, the reward in this prototype learning system has two parts: the reward state rs t \u2208 G, and the reward value rv t \u2208 R, where G is the set of local goals the agent may have. This form of reward is inspired by the reward machine (Icarte et al., 2018), a Mealy machine for specifying history-dependent reward, but the semantics we assign to the reward will be different. Also note that this Mealy machine bears no relation to the automaton we discussed in Section 1.3 \u2014 the reward machine models the reward, while the automaton in Section 1.3 models the learning system, and takes the reward as input. Each reward state rs corresponds to a local goal. When a local goal (or equivalently, a reward state) expires, the agent receives a numerical reward value rv. For all other timesteps (other than the expiration of local goals), the reward value can be considered to take a special NULL value, meaning that no reward value is received. The reward value is an evaluation of the agent\u2019s performance in an episode of a reward state, where an episode of a reward state is de\ufb01ned as the time period between the expiration of the previous reward state (exclusive) and the expiration of the reward state itself (inclusive). The reward state can potentially depend on the entire observation history, while the reward value can only depend on the observation history of the episode it is assessing. Overall, the reward is speci\ufb01ed by (rs t , rv t ) = r(ho t). The local goals described here are technically similar to subgoals in hierarchical reinforcement learning (Dietterich, 2000; Sutton et al., 1999; Parr & Russell, 1997). However, the term \u2018subgoal\u2019 suggests that there is some higher-level goal that the agent needs to achieve, and that the higher-level goal is the true objective the agent needs to optimize. That is not the case here \u2014 although it is totally possible that the local goals are designed in such a way that some global goal can be achieved, the agent only needs to optimize the local goals. The reward language in this prototype system makes two assumptions on the learning algorithm. As long as the two assumptions are met, the learning algorithm is considered to \u2018interpret\u2019 the reward correctly. The \ufb01rst assumption is that the learning algorithm only generates policies that are episode-wise stationary, meaning that \u03c0t1 = \u03c0t2 for any timesteps t1 and t2 in the same episode of a reward state, and that \u03c0t1 : O \u2192 A. This assumption is not particularly restrictive, because in cases where a local goal requires a more complex policy, we can always split the goal into mulSome Insights into Lifelong Reinforcement Learning Systems tiple goals (by modifying the reward function) for which the policies are episode-wise stationary. With this assumption, we can use a single policy \u03c0rs : O \u2192 A to represent the policies at all timesteps within an episode of reward state rs. The second assumption is that the learning algorithm keeps a pool of \u2018elite\u2019 policies for each reward state: a policy that led to high reward value in some episode has the opportunity to enter the pool, and a policy that consistently leads to higher reward value eventually dominates the policy pool. The exact criterion for selection into the pool (e.g., to use the expected reward value as the criterion, or to use the probability of the reward value being higher than a certain threshold, etc.) is not enforced, and is left up to the learning algorithm. 2.2. Learning Algorithm The learning algorithm in this prototype lifelong learning system is an evolutionary algorithm, adjusted to meet the assumptions made by the reward. The algorithm maintains a policy pool Drs of maximum size d for each reward state rs \u2208 G. Each item in the pool is a two tuple \u27e8\u03c0, rv \u03c0\u27e9 where \u03c0 is a policy and rv \u03c0 is the reward value of the last episode in which \u03c0 was executed. Conceptually, the algorithm consists of three steps: policy generation, policy execution, and (policy) pool update, which are described below. POLICY GENERATION When an episode of reward state rs starts, a policy \u03c0rs is generated from one of the following methods with probability p1, p2, p3, respectively: 1. Randomly sample a policy from the policy pool Drs, and mutate the policy. 2. Randomly sample a policy from Drs and keep it as is. Remove the sampled policy from Drs. This is to re-evaluate a policy in the pool. Since the transition of observations might be stochastic, the same policy does not necessarily always result in the same reward value. 3. Randomly generate a new policy \u03c0rs : O \u2192 A from scratch. This is to keep the diversity of the policy pool. p1, p2 and p3 should sum up to 1, and are hyper-parameters of the algorithm. POLICY EXECUTION Execute the generated policy \u03c0rs until a numerical reward value rv is received. POOL UPDATE If the policy pool is not full, insert \u27e8\u03c0rs, rv\u27e9 into the pool. Otherwise compare rv with the minimum reward value in Figure 3: A simplistic abstraction of guidance in reinforcement learning. the pool. If rv is greater than or equal to the minimum reward value, replace the policy and reward value pair (that has the minimum reward value) with \u27e8\u03c0rs, rv\u27e9. 2.3. Embedding Learning Bias Learning bias in reinforcement learning systems refers to the explicit or implicit assumptions made by the learning algorithm about the policy. Our assumption that the policy is episode-wise stationary is an example of learning bias. Arguably, a good learning bias is as important as a good learning algorithm, therefore it is important that mechanisms are provided to embed learning bias into the learning system. A straight-forward way to embed learning bias into the above lifelong learning system is through the policy generation process. This includes how existing policies are mutated, and what distribution new policies are sampled from. The learning bias provided this way does not depend on the agent\u2019s observation and reward history, and is sometimes implicit (e.g., the learning bias introduced by using a neural network of particular architecture). Another type of learning bias common in reinforcement learning is guidance, the essence of which can be illustrated by Figure 3. Suppose in some reward state, the agent starts from observation o and the goal is to reach 4 observation o\u2032. Prior knowledge indicates that to reach o\u2032, visiting o\u2032\u2032 is a good heuristic, but reaching o\u2032\u2032 itself has little or no merit. In other words, we would like to encourage the agent to visit and explore around o\u2032\u2032 more frequently (than other parts of the observation space) until a reliable policy to reach o\u2032 is found. To provide guidance to the agent in the prototype lifelong learning system, we can utilize the property of the learning algorithm that policies leading to high reward values will enter the policy pool. Once a policy enters the pool, it has the opportunity to be sampled (possibly with mutation) and executed. Therefore, we just need to assign a higher reward value for reaching o\u2032\u2032 (before the expiration of the reward state) than reaching neither o\u2032 nor o\u2032\u2032. Also important is the ability to control the extent to which region around o\u2032\u2032 is explored. To achieve this, recall that the learning algorithm occasionally re-evaluates policies in the policy pool. If we assign a lower reward value for reaching o\u2032\u2032 with some probability, we can prevent the 4For sake of terminological convenience, we pretend that the observations here are environment states. ",
    "Experiment": "Experiment Now we evaluate the behaviour of the prototype lifelong reinforcement learning system. The source code of the experiments can be found at https://gitlab.com/ lifelong-rl/lifelongRL_gridworld 3.1. Environment Consider a gridworld agent whose life revolves around getting food and taking the food back home for consumption. The agent lives in a 11 by 11 gridworld shown in Figure 4. The shaded areas are barriers that the agent cannot go through. Some potential positions of interest are marked with letters: F is the food source and is assumed to have in\ufb01nite supply of food; H is the agent\u2019s home. To get to the food source from home, and to carry the food home, the agent must pass through one of the two tunnels \u2014 the tunnel on the left is marked with L and the tunnel on the right is marked with R. At each timestep, the agent observes its position in the gridworld as well as a signal indicating whether it is in one of the four positions of interest (if yes, which), and chooses from one of the four actions: UP, RIGHT, DOWN and LEFT. Each action deterministically takes the agent to the adjacent grid in the corresponding direction, unless the destination is a barrier, in which case the agent remains in its original position. The agent starts from home at the beginning of its life, and needs to go to the food source to get food. Once it reaches the food source, it needs to carry the food back home. This process repeats until the agent dies. The lifespan of the agent is assumed to be 100 million timesteps. The agent is supposed to learn to reliably achieve these two local goals within its lifetime. 5Note that the word \u2018probability\u2019 here should be interpreted as the \u2018long-run proportion\u2019, and therefore the reward value needs not be truly stochastic. E.g., we can imagine that the reward has a third component which is the state of a pseudo-random generator. Figure 4: Gridworld environment. 3.2. Learning System Setup The reward state in this experiment is represented by the conjunction of Boolean variables. For example, if three Boolean variables A, B and C are de\ufb01ned, then the reward state would be in the form of rs = A \u2227 B \u2227 C or rs = A \u2227 \u00acB \u2227 C, etc. At the bare minimum, one Boolean variable GET FOOD needs to be de\ufb01ned for this agent, where GET FOOD being true corresponds to the local goal of going to the food source, and \u00acGET FOOD corresponds to the local goal of carrying the food home. The agent receives a reward value of +1 if GET FOOD is true and the agent reaches F, in which case the Boolean variable GET FOOD transitions to false. Similarly, the agent receives a reward value of +1 if \u00acGET FOOD is true and the agent reaches H, in which case GET FOOD transitions to true. On top of GET FOOD, we de\ufb01ne another Boolean variable TIMED OUT, which indicates whether the agent has exceeded a certain time limit for trying to get to the food source, or for trying to carry the food home. If the reward state is \u00acTIMED OUT \u2227 GET FOOD, and the agent fails to reach F within the time limit, itreceives a reward value of \u22121, and the reward state transition to TIMED OUT \u2227 GET FOOD. From TIMED OUT \u2227 GET FOOD, if the agent still fails to get to F within the time limit, it receives a reward value of 0. The agent will remain in TIMED OUT \u2227 GET FOOD, until it reaches F, when the reward state transitions to \u00acTIMED OUT \u2227 \u00acGET FOOD (and receive a +1 reward value as already mentioned). For the case when GET FOOD is false, the reward transition is de\ufb01ned similarly. Throughout the experiments, the time limit is set to 24, which is enough for the agent to accomplish any of the local goals. We refer to this reward design as the base case. Unfortunately, even for a toy problem like this, learning can be dif\ufb01cult if no proper learning bias is provided. Since there are 4 actions and 74 possible positions, the number of possible episode-wise stationary policies is 474 for each reward state. Among those policies, very few can achieve the local goals. If the policy generation and mutation is purely random, it will take a long time for the agent to \ufb01nd ",
    "Results": "Results (a) unbiased policy, progressed-based guidance (b) biased policy, progressed-based guidance Figure 5: Learning curve for unbiased/biased policy with progress-based guidance, averaged over 20 runs. Figure 5 shows the learning curves for reward state GET FOOD \u2227 \u00acTIMED OUT with progress-based guidance. The x-axis is the timesteps (in million), and the y-axis is the percentage of times the agent transitions into a particular next reward state starting from GET FOOD\u2227\u00acTIMED OUT. A next reward state of \u00acGET FOOD \u2227 \u00acTIMED OUT means that the agent successfully reached F within the time limit, and a next reward state of GET FOOD \u2227 TIMED OUT means that the agent failed to do so. As we can see, with unbiased policy, it took the agent around 25 million timesteps to achieve 100% success rate; while with biased policy, this only took around 8 million timesteps. Figure 6 shows the learning curves for reward state GET FOOD \u2227 \u00acTIMED OUT \u2227 \u00acVISITED LEFT with the suboptimal guidance described in Section 3.2. Similar to Figure 5, the x-axis is the timesteps (in million), and the yaxis is the percentage of times the agent transitioned into a particular next reward state starting from GET FOOD \u2227 ",
    "References": "References Abel, D., Arumugam, D., Lehnert, L., and Littman, M. L. State abstractions for lifelong reinforcement learning. In Proceedings of the 35th International Conference on MaSome Insights into Lifelong Reinforcement Learning Systems chine Learning, ICML 2018, Stockholmsm\u00a8assan, Stockholm, Sweden, July 10-15, 2018, pp. 10\u201319, 2018. Dietterich, T. G. Hierarchical reinforcement learning with the MAXQ value function decomposition. J. Artif. Intell. Res., 13:227\u2013303, 2000. Icarte, R. T., Klassen, T. Q., Valenzano, R. A., and McIlraith, S. A. Using reward machines for high-level task speci\ufb01cation and decomposition in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00a8assan, Stockholm, Sweden, July 10-15, 2018, pp. 2112\u20132121, 2018. Parr, R. and Russell, S. J. Reinforcement learning with hierarchies of machines. In Advances in Neural Information Processing Systems 10, [NIPS Conference, Denver, Colorado, USA, 1997], pp. 1043\u20131049, 1997. Ring, M. B. Child: A \ufb01rst step towards continual learning. In Learning to Learn, pp. 261\u2013292. 1998. doi: 10.1007/ 978-1-4615-5529-2\\ 11. URL https://doi.org/ 10.1007/978-1-4615-5529-2_11. Singh, S. P., Barto, A. G., and Chentanez, N. Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pp. 1281\u20131288, 2004. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Sutton, R. S., Precup, D., and Singh, S. P. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181\u2013211, 1999. Taylor, M. E. and Stone, P. Transfer learning for reinforcement learning domains: A survey. J. Mach. Learn. Res., 10:1633\u20131685, 2009. Watkins, C. J. C. H. and Dayan, P. Technical note q-learning. Machine Learning, 8:279\u2013292, 1992. White, D. Multi-objective in\ufb01nite-horizon discounted markov decision processes. Journal of Mathematical Analysis and Applications, 89(2):639 \u2013 647, 1982. ISSN 0022-247X. ",
    "title": "Some Insights into Lifelong Reinforcement Learning Systems",
    "paper_info": "Some Insights into Lifelong Reinforcement Learning Systems\nChangjian Li 1\nAbstract\nA lifelong reinforcement learning system is a\nlearning system that has the ability to learn\nthrough trail-and-error interaction with the en-\nvironment over its lifetime. In this paper, I give\nsome arguments to show that the traditional rein-\nforcement learning paradigm fails to model this\ntype of learning system. Some insights into life-\nlong reinforcement learning are provided, along\nwith a simplistic prototype lifelong reinforcement\nlearning system.\n1. Introduction\nAn agent is an abstraction of a decision-maker. At each time\ninstance t, it receives an observation ot \u2208 O, and outputs an\naction at \u2208 A to be carried out in the environment it lives in.\nHere, O is the (\ufb01nite) set of possible observations the agent\ncan receive, and A is the (\ufb01nite) set of actions the agent can\nchoose from. An agent\u2019s observation ot depends on the cur-\nrent environment state st \u2208 S through an agent observation\nfunction S \u2192 O, where S is the set of possible environment\nstates. The observation history ho\nt = (o1, o2..., ot) is the\nsequence of observations the agent has received till time t.\nLet Ho\nt be the set of possible observation histories of length\nt, the policy \u03c0t : Ho\nt \u2192 A at time t is de\ufb01ned as the map-\nping from an observation history of length t to the action the\nagent will take. An agent\u2019s behavior can thus be fully speci-\n\ufb01ed by its policy across all timesteps \u03c0 = (\u03c01, \u03c02, ..., \u03c0t, ...).\nThroughout the paper, it is assumed that an agent has a \ufb01nite\nlifespan T.\n1.1. Scalar Reward Reinforcement Learning System\nWe are interested in agents that can achieve some goal. In\nreinforcement learning, a goal is expressed by a scalar signal\nrt \u2208 R called the reward. The reward is dependent on the\nagent\u2019s observation history, and is assumed to be available\nto the agent at each timestep in addition to the observation\n1Department of Electrical and Computer Engineering, Uni-\nversity of Waterloo, Canada. Correspondence to: Changjian Li\n<changjian.li@uwaterloo.ca>.\nPreliminary work. Under Review.\not. Our aim is to \ufb01nd policies that maximize the expected\ncumulative reward an agent receives over its lifetime:\nmax\n\u03c0\nE[\nT\n\ufffd\nt=1\nrt(ho\nt)]\n(1)\nUsing the maximization of expected cumulative scalar re-\nward to formulate the general notion of goal is a design\nchoice in reinforcement learning, based on what is com-\nmonly known as the reward hypothesis (Sutton & Barto,\n2018), In Sutton\u2019s own words:\nThat all of what we mean by goals and purposes\ncan be well thought of as the maximization of\nthe expected value of the cumulative sum of a\nreceived scalar signal (called reward).\nThis design choice, however, is somewhat arbitrary. Among\nother things, the reward needs not be a scalar (e.g. multi-\nobjective reinforcement learning (White, 1982)), nor does\nit have to be a quantity whose cumulative sum is to be\nmaximized (which we will come to shortly). Leaving aside\nthe question of whether or not all goals can be formulated\nby Eq. 1, I intend to show in this paper that the problem\nof lifelong reinforcement learning probably should not be\nformulated as such.\nNote that in Eq. 1, I de\ufb01ned the reward in terms of the obser-\nvation history, instead of the history of environment states\nas in most reinforcement learning literature. This re\ufb02ects\nthe view that reward signals are internal to the agent, as\npointed out by Singh et al. (2004) in their work on intrinsic\nmotivation. Since the observations are all that the agent has\naccess to from the external environment, the intrinsic reward\nshould depend on the environment state only through the\nagent\u2019s observation history.\nAlthough the above reinforcement learning formulation rec-\nognizes the reward as a signal intrinsic to an agent, it fo-\ncuses on learning across different generations 1 of agents,\nas opposed to learning within an agent\u2019s lifespan. From an\nagent\u2019s point of view, the cumulative reward is known only\nwhen it reaches its end of life, by which time no learning can\n1Usage of the word \u2018generation\u2019 here is only to emphasize that\nlearning cannot be achieved within an agent\u2019s lifespan, and does\nnot imply that evolution algorithms need to be used.\narXiv:2001.09608v1  [cs.LG]  27 Jan 2020\n",
    "GPTsummary": "- (1): The article discusses the problem of traditional reinforcement learning in modeling a lifelong reinforcement learning system, and provides insights into lifelong reinforcement learning.\n\n- (2): The traditional reinforcement learning paradigm fails to model the learning system in which an agent learns through trial-and-error interaction with the environment over its lifetime. The article argues that the problem of lifelong reinforcement learning probably should not be formulated based on the maximization of expected cumulative scalar reward. The article introduces the methodology of lifelong reinforcement learning and proposes ways to create prototype lifelong reinforcement learning system.\n\n- (3): The article proposes a Q-learning algorithm to model lifelong reinforcement learning. The Q estimate is a fundamental signal for lifelong learning behavior of an agent, which, in addition to the scalar reward signal, represents the knowledge of what the goal means for the current type of agent.\n\n- (4): The article does not present any specific task, but provides insights into lifelong reinforcement learning and proposes a Q-learning algorithm for such a task. The performance of the algorithm is not evaluated in this paper, and it is just a proposal for modeling lifelong reinforcement learning.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This article provides valuable insights into the problem of lifelong reinforcement learning and proposes a Q-learning algorithm for modeling such a system. Its significance lies in contributing to the development of machine learning systems that learn through trial-and-error interaction with the environment over their lifespan.\n\n- (2): Innovation Point: The article proposes a novel approach to modeling lifelong reinforcement learning systems by incorporating Q-learning. Performance: While the Q-learning algorithm is promising, the article does not provide any evaluation of its performance on specific tasks. Workload: The article is well-written and organized, but it requires a certain level of knowledge in reinforcement learning to fully understand the content.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This article provides valuable insights into the problem of lifelong reinforcement learning and proposes a Q-learning algorithm for modeling such a system. Its significance lies in contributing to the development of machine learning systems that learn through trial-and-error interaction with the environment over their lifespan.\n\n- (2): Innovation Point: The article proposes a novel approach to modeling lifelong reinforcement learning systems by incorporating Q-learning. Performance: While the Q-learning algorithm is promising, the article does not provide any evaluation of its performance on specific tasks. Workload: The article is well-written and organized, but it requires a certain level of knowledge in reinforcement learning to fully understand the content.\n\n\n"
}