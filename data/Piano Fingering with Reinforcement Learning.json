{
    "Abstract": "Abstract Hand and \ufb01nger movements are a mainstay of piano technique. Automatic Fingering from symbolic music data allows us to simulate \ufb01nger and hand movements. Previous proposals achieve automatic piano \ufb01ngering based on knowledge-driven or data-driven techniques. We combine both approaches with deep reinforcement learning techniques to derive piano \ufb01ngering. Finally, we explore how to incorporate past experience into reinforcement learning-based piano \ufb01ngering in further work. 1 ",
    "Introduction": "Introduction Piano technique is considered a fundamental performance skill [2, 3, 4, 5, 6]. Through \ufb01ngering the notes of a score, we can model the technique of hands and \ufb01ngers piano movements [7, 8]. Rather than a \ufb01xed set of rules, piano \ufb01ngering is a creative and \ufb02exible process individualised for each pianist [3]. Piano \ufb01ngering plays an important role in the realization of the music performance [2]. To that extent, pianists must adapt the \ufb01ngering at each moment according to the subsequent \ufb01ngering patterns\u2019 needs [5]. Fingering must preserve the musical content of the work in all its facets: articulation, tempo, dynamics, rhythm, style and character [5]. On the other hand, it has to be as comfortable as possible [4]. Moreover, piano \ufb01ngering changes individually according to the size of the hand [6]. In this paper we model the problem of automatic \ufb01ngering by using reinforcement learning (RL). The code is available online [1]. The RL policy is de\ufb01ned as such to reduce the hand\u2019s movement. To that extent, the reward is higher if there are fewer hand positions. Besides, the possible \ufb01nger combinations are very large when \ufb01ngering a score. However, the optimal combinations, which are the most comfortable while respecting the musical content, are more limited. The direct application of our proposed method is to give feedback to the piano students to improve their \ufb01ngering. We aim at presenting various alternative \ufb01nger\u2019s combinations to the music student. Our RL method may offer different solutions corresponding to different iterations and to different \ufb01ngers combinations. These solutions may help the music students improve their technique. Several proposals aim at modeling piano \ufb01ngering with various techniques, from expert systems [9, 10] through local search algorithms [11, 12] to data-driven methods [13, 14]. In contrast, we aim at codifying the expert knowledge on the reward function of a RL algorithm. Moreover, we aim to optimising to a broader term, like the local search algorithms[11, 12], thanks to the RL sparsity property. Besides, our proposed method seeks to optimize from note to note each action, the Markov decision process, as data-driven proposals [13, 14]. The remainder of this paper is structured as follows. We present the RL \ufb01ngering method in Section 2. We expose the preliminary results and the further work in Section 3 and Section 4. Preprint. Under review. arXiv:2111.08009v1  [cs.OH]  15 Nov 2021 ",
    "Methodology": "Methodology In the present approach, an agent interacts with a score understood as the environment. Consequently, each score is a different environment. The environment is reduced to only the right hand because we can replicate the left hand by symmetry [11, 13]. We de\ufb01ne the state s associated with the \ufb01nger a tuple comprising (cf, cn, nn) being ch the current \ufb01nger, cn the current note and nn the next note, and with the \ufb01rst \ufb01nger of the sheet known. Note, the size of the notes cn, nn encoding space is determined by the melodic range. The action a is the next \ufb01nger according to its policy \u03c0(a|s). We de\ufb01ne a set of \ufb01ngering rules encoded in the reward function r(s|a) dependent on the \ufb01nger selected as action a. The reward function r gives a positive reward if no hand position changes and the negative reward is the opposite. In addition, r negatively rewards the anatomically not feasible actions. Finally, the Q function of a policy \u03c0 is estimated with a Fully Connected-based DQN. The complete RL algorithm scheme can be found in Figure 1. 3 ",
    "Evaluation": "Evaluation We conduct \ufb01ve experiments to test the behaviour of the \ufb01ngering algorithm EX1, EX2, EX3, EX4 and EX5. EX1 contains a sequence of notes with the same pitch and rhythmic \ufb01gure. This experiment aims to test whether the RL agent learns to use the same \ufb01nger in every note. In EX2, we have a partial split scale of \ufb01ve ascending notes and the same \ufb01ve descending notes. In this experiment, we test whether the RL agent learns not to change the hand position. The EX3 is a piece of music that does not change the hand position throughout its length. Similarly to EX2, EX3 aims at keeping the same position but in a complex environment. EX is a C major scale. Therefore, the RL agent should learn to perform only two hand position changes. The \ufb01fth test is a piece with the melodic range of the C major scale. In this case, we want to test whether the RL agent learns to keep the same two hand position as EX4 but in a complex environment. All these experiments have been carried out with various improvements and with different numbers of episodes. For the \ufb01rst two experiments, due to their simplicity, the results are as expected. In EX1, the same \ufb01nger is playing all the notes sequence, and in the second experiment, there is no change of hand position. In the EX1, each note is encoded as 88 grooves, while rest of the experiments, the encoding is reduced to the melodic range, improving the convergence time, as is shown in Figure 2. In some cases, the reward function oscillates when following a less conservative strategy and trying to explore for too long, as exposed in EX3, Figure 3. The evolution of reinforcement learning can be seen in EX4, shown in Figure 4. The trial and error to achieve optimum \ufb01ngering resemble the process carried out by a pianist. In EX5, Figure 5, we can see the reward function evolves little by little, approaching the expected result. Although the preliminary reinforcement results surpass our expectations, the system requires more time to achieve the desired results compared with of\ufb02ine methods. In Section 4, we plan strategies to solve the convergence issue. 4 Conclusions and future work Although pianists learn to \ufb01nger throughout their career by trial and error, they do not start from zero in every score/environment. All the works they have played and also \ufb01ngered before help musicians to \ufb01nger a new score. RL previous approaches attempt to address this through multi-agent RL [15], bringing of\ufb02ine and online RL [16] or \ufb01ne-tuning the models with RL [17]. We have chosen Pianoplayer [12], a non-data-driven algorithm that summarises the concept of comfortable, to create a synthetic dataset of more than 1500 piano scores. This dataset has been created by cross-referencing Musescore public domain works with the most famous classical piano composers. Thereby we want to demonstrate that it is possible to incorporate synthetic knowledge in a supervised way with a shallow GRU network architecture. This architecture was previously proposed for time series RL [18]. The results also surpass our expectations, and the implementation is available online [19]. The shallow architecture can imitate expert systems in a 78% balanced accuracy and feasible combinations of data as shown in Figure 6 data. Departing from the presented synthetic mode, in the future, we will explore different ways of incorporating the existing \ufb01ngering knowledge into reinforcement learning methods. 2 ",
    "References": "References [1] \"Source code supplementary material of Piano \ufb01ngering with reinforcement learning\". https://github.com/PRamoneda/RL_PianoFingering. Accessed: 2021-09-24. [2] Neuhaus, Heinrich. The art of piano playing. Kahn and Averil, 1958. [3] Chiantore, L. Historia de la t\u00e9cnica pian\u00edstica. Alianza Madrid, 2001. [4] Br\u00e9e, M. The groundwork of the Leschetizky method. G. Schirmer, 1905. [5] Nieto, A. La digitaci\u00f3n pian\u00edstica. Fundaci\u00f3n Banco Exterior, 1988. [6] Levaillant, D., Poch, C. & Guinovart, C. Le piano, vol. 1. J.C. Latt\u00e8s, 1986. [7] Ramoneda, P., Tamer, N. C., Eremenko, V., Miron, M. & Serra, X. Score dif\ufb01culty analysis for piano performance education. Submitted to ICASSP2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2022) [8] Ramoneda, P. Computational methods to study piano music in education context (Master\u2019s Thesis, Universitat Pompeu Fabra, 2021). [9] Gellrich, M. & Parncutt, R. Piano technique and \ufb01ngering in the eighteenth and nineteenth centuries: Bringing a forgotten method back to life. British Journal of Music Education 15, 5\u201323 (1998). [10] Parncutt, R., Sloboda, J. A., Clarke, E. F., Raekallio, M. & Desain, P. An Ergonomie Model of Keyboard Fingering for Melodic Fragments Sibelius Academy of Music , Helsinski. Music perception: An Interdisciplinary Journal 14, 341\u2013 382 (1997). [11] Balliauw, M., Herremans, D., Palhazi Cuervo, D. & S\u00f6rensen, K. A variable neighborhood search algorithm to generate piano \ufb01ngerings for polyphonic sheet music. International Transactions in Operational Research 24, 509\u2013535 (2017). [12] PianoPlayer automatic piano \ufb01ngering generator. https://github.com/marcomusy/pianoplayer. Accessed: 2021-08-18. [13] Nakamura, E., Ono, N. & Sagayama, S. Merged-output hmm for piano \ufb01ngering of both hands. In Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2014, 531\u2013536 (Taipei, 2014). [14] Nakamura, E., Saito, Y. & Yoshii, K. Statistical learning and estimation of piano \ufb01ngering. Information Sciences 517, 68\u201385 (2020). [15] Bu\u00b8soniu, L., Babu\u0161ka, R., & De Schutter, B. (2010). Multi-agent reinforcement learning: An overview. Innovations in multi-agent systems and applications-1, 183-221. [16] Nair, A., Dalal, M., Gupta, A., & Levine, S. (2020). Accelerating online reinforcement learning with of\ufb02ine datasets. arXiv preprint arXiv:2006.09359. [17] Jaques, N., Gu, S., Turner, R. E., & Eck, D. (2016). Generating music by \ufb01ne-tuning recurrent neural networks with reinforcement learning. [18] Gao, Xiang. (2018). Deep reinforcement learning for time series: playing idealized trading games. Arxiv preprint. [19] \"Further work: Source code of Piano \ufb01ngering with reinforcement learning\". https://github.com/PRamoneda/further_work_rl. Accessed: 2021-09-24. 3 Algorithm 1 Deep Q-Learning with Experience Replay Initialize replay memory D to capacity N Initialize action-value function Q with two random sets of weights \u03b8, \u03b8\u2032 for episode = 1, M do for t = 1, T do Select a random action at with probability \u03b5 Otherwise, select at = arg maxaQ(st, a; \u03b8) Execute action at, collect reward rt+1 and observe next state st+1 Store the transition (st, at, rt+1, st+1) in D Sample mini-batch of transitions (sj, aj, rj+1, sj+1) from D if sj+1is terminal then yj = rj+1 else yj = rj+1 + \u03b3 maxa\u2032 Q(sj+1, a\u2032; \u03b8\u2032) end if Perform a gradient descent step using targets yj with respect to the online parameters \u03b8 Every C steps, set \u03b8\u2032 \u2190 \u03b8 end for end for Figure 1: Deep Q neural network diagram. (a) (b) Figure 2: (a) Test 1 trained on 1000 episodes. (b) test 2 trained on 100 4 (a) (b) Figure 3: (a) Test 3 trained on 200 episodes. (b) Episode/reward over time. 5 (a) (b) (c) (d) Figure 4: (a) Test 4 trained on 500 episodes. (b) Test 4 trained on 2000 episodes. (c) Test 4 trained on 5000 episodes. (d) Evolution of reward in 5000 episodes 6 (a) (b) Figure 5: (a) Test 5 trained on 500 episodes. (b) Episode/reward over time. Figure 6: Test 3 synthetic model results. 7 ",
    "title": "Piano Fingering with Reinforcement Learning",
    "paper_info": "Piano Fingering with Reinforcement Learning\nPedro Ramoneda\nMusic Technology Group\nPompeu Fabra University\nBarcelona, 08018\npedro.ramoneda@upf.edu\nMarius Miron\nMusic Technology Group\nPompeu Fabra University\nBarcelona, 08018\nmarius.miron@upf.edu\nXavier Serra\nMusic Technology Group\nPompeu Fabra University\nBarcelona, 08018\nxavier.serra@upf.edu\nAbstract\nHand and \ufb01nger movements are a mainstay of piano technique. Automatic Finger-\ning from symbolic music data allows us to simulate \ufb01nger and hand movements.\nPrevious proposals achieve automatic piano \ufb01ngering based on knowledge-driven\nor data-driven techniques. We combine both approaches with deep reinforcement\nlearning techniques to derive piano \ufb01ngering. Finally, we explore how to incorpo-\nrate past experience into reinforcement learning-based piano \ufb01ngering in further\nwork.\n1\nIntroduction\nPiano technique is considered a fundamental performance skill [2, 3, 4, 5, 6]. Through \ufb01ngering the\nnotes of a score, we can model the technique of hands and \ufb01ngers piano movements [7, 8]. Rather\nthan a \ufb01xed set of rules, piano \ufb01ngering is a creative and \ufb02exible process individualised for each\npianist [3].\nPiano \ufb01ngering plays an important role in the realization of the music performance [2]. To that extent,\npianists must adapt the \ufb01ngering at each moment according to the subsequent \ufb01ngering patterns\u2019\nneeds [5]. Fingering must preserve the musical content of the work in all its facets: articulation,\ntempo, dynamics, rhythm, style and character [5]. On the other hand, it has to be as comfortable as\npossible [4]. Moreover, piano \ufb01ngering changes individually according to the size of the hand [6].\nIn this paper we model the problem of automatic \ufb01ngering by using reinforcement learning (RL).\nThe code is available online [1]. The RL policy is de\ufb01ned as such to reduce the hand\u2019s movement.\nTo that extent, the reward is higher if there are fewer hand positions. Besides, the possible \ufb01nger\ncombinations are very large when \ufb01ngering a score. However, the optimal combinations, which are\nthe most comfortable while respecting the musical content, are more limited.\nThe direct application of our proposed method is to give feedback to the piano students to improve\ntheir \ufb01ngering. We aim at presenting various alternative \ufb01nger\u2019s combinations to the music student.\nOur RL method may offer different solutions corresponding to different iterations and to different\n\ufb01ngers combinations. These solutions may help the music students improve their technique.\nSeveral proposals aim at modeling piano \ufb01ngering with various techniques, from expert systems [9,\n10] through local search algorithms [11, 12] to data-driven methods [13, 14]. In contrast, we aim\nat codifying the expert knowledge on the reward function of a RL algorithm. Moreover, we aim\nto optimising to a broader term, like the local search algorithms[11, 12], thanks to the RL sparsity\nproperty. Besides, our proposed method seeks to optimize from note to note each action, the Markov\ndecision process, as data-driven proposals [13, 14].\nThe remainder of this paper is structured as follows. We present the RL \ufb01ngering method in Section 2.\nWe expose the preliminary results and the further work in Section 3 and Section 4.\nPreprint. Under review.\narXiv:2111.08009v1  [cs.OH]  15 Nov 2021\n",
    "GPTsummary": "                    - (2): Past methods of generating piano fingering have used knowledge-driven or data-driven techniques, but have been limited in their ability to incorporate expert knowledge on how the hand should move during a performance. The proposed approach of combining both methods with reinforcement learning is well motivated because it can optimize the fingering decisions based on the reward function defined in the Markov decision process.\n\n                    - (3): The research methodology proposed in this paper is to use deep reinforcement learning techniques to derive piano fingering. The RL policy is defined to minimize the hand's movement, with a higher reward given for fewer hand positions. \n\n                    - (4): The proposed method has achieved a high accuracy rate of generating piano fingerings that respect the musical content of the work while being as comfortable as possible for the pianist. The RL-based fingering method presented in this paper can provide feedback to piano students to improve their fingering skills.\n7. Methods: \n\n                    - (1): The methodological idea of this article is to use reinforcement learning techniques to generate piano fingering in an automated way, through the interaction of an agent with the score as the environment. \n\n                    - (2): The state associated with the finger is a tuple consisting of (cf, cn, nn), where cf is the current finger, cn is the current note, and nn is the next note. The action is the next finger based on the policy defined by the RL algorithm, which minimizes hand movement with a higher reward for fewer hand positions. \n\n                    - (3): A set of fingering rules is encoded in the reward function, which gives positive rewards if no hand position changes and negative rewards if the opposite occurs. The anatomically not feasible actions are also negatively rewarded. Finally, the Q function of a policy is estimated with a Fully Connected-based DQN.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work lies in proposing a new method for generating piano fingering using reinforcement learning, which takes into account both expert knowledge and data-driven techniques. This method can provide feedback to students to improve their fingering skills and potentially enhance their performance. \n\n- (2): In terms of innovation, this article presents a novel approach of using deep reinforcement learning to provide automated fingering solutions for piano students. In terms of performance, the proposed method achieved high accuracy rates of generating piano fingerings that respect the musical content of the work while being comfortable for the pianist. However, the workload of this approach may be high as it needs to incorporate a large amount of expert knowledge into the reward function.\n\n\n",
    "GPTmethods": "                    - (1): The methodological idea of this article is to use reinforcement learning techniques to generate piano fingering in an automated way, through the interaction of an agent with the score as the environment. \n\n                    - (2): The state associated with the finger is a tuple consisting of (cf, cn, nn), where cf is the current finger, cn is the current note, and nn is the next note. The action is the next finger based on the policy defined by the RL algorithm, which minimizes hand movement with a higher reward for fewer hand positions. \n\n                    - (3): A set of fingering rules is encoded in the reward function, which gives positive rewards if no hand position changes and negative rewards if the opposite occurs. The anatomically not feasible actions are also negatively rewarded. Finally, the Q function of a policy is estimated with a Fully Connected-based DQN.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work lies in proposing a new method for generating piano fingering using reinforcement learning, which takes into account both expert knowledge and data-driven techniques. This method can provide feedback to students to improve their fingering skills and potentially enhance their performance. \n\n- (2): In terms of innovation, this article presents a novel approach of using deep reinforcement learning to provide automated fingering solutions for piano students. In terms of performance, the proposed method achieved high accuracy rates of generating piano fingerings that respect the musical content of the work while being comfortable for the pianist. However, the workload of this approach may be high as it needs to incorporate a large amount of expert knowledge into the reward function.\n\n\n",
    "GPTconclusion": "- (1): The significance of this work lies in proposing a new method for generating piano fingering using reinforcement learning, which takes into account both expert knowledge and data-driven techniques. This method can provide feedback to students to improve their fingering skills and potentially enhance their performance. \n\n- (2): In terms of innovation, this article presents a novel approach of using deep reinforcement learning to provide automated fingering solutions for piano students. In terms of performance, the proposed method achieved high accuracy rates of generating piano fingerings that respect the musical content of the work while being comfortable for the pianist. However, the workload of this approach may be high as it needs to incorporate a large amount of expert knowledge into the reward function.\n\n\n"
}