{
    "Abstract": "Abstract\u2014 This paper develops a data-driven inverse reinforcement learning technique for a class of linear systems to estimate the cost function of an agent online, using input-output measurements. A simultaneous state and parameter estimator is utilized to facilitate output-feedback inverse reinforcement learning, and cost function estimation is achieved up to multiplication by a constant. I. ",
    "Introduction": "INTRODUCTION Seamless cooperation between humans and autonomous agents is a vital yet challenging aspect of modern robotic systems. Effective cooperation between humans and autonomous systems can be achieved if the autonomous systems are capable of learning to act by observing other cognitive entities. Based on the premise that a cost (or reward) function fully characterizes the intent of the demonstrator, a method to learn the cost function from observations is developed in this paper. The cost-estimation problem \ufb01rst appears in [1] in a linear-quadratic regulation (LQR) setting, and a solution is provided in [2] via linear matrix inequalities. For nonlinear systems and cost functions, computation of closed-form solutions is generally intractable, and hence, approximate solutions are sought. In [3]\u2013[6], the cost function of a Markov decision process (MDP) is learned using inverse reinforcement learning (IRL). It is demonstrated that the IRL problem is inherently illposed in the sense that it has multiple possible solutions, including the trivial ones. To overcome the degeneracy, the cost function that differentiates the optimal behavior from the suboptimal behaviors by a margin is sought. In [7] the maximum entropy principle (cf. [8]) is utilized to solve the ill-posed IRL problem for deterministic MDPs. In [9] a causal version of the maximum entropy principle is developed and utilized to solve IRL problems in a fully stochastic setting. An IRL algorithm based on minimization of the Kullback-Leibler divergence between the empirical distribution of trajectories obtained from a baseline policy and the trajectories obtained from the cost-based policy is developed in [10]. In the past two decades, Bayesian [11], natural gradient [12], game theoretic [13], and feature construction based methods [14] have also been developed for IRL. IRL is extended to problems with locally optimal demonstrations in [15] using likelihood optimization and to problems with nonlinear cost functions in [16] using Gaussian processes (GP). Another GP-based IRL algorithm that increases the ef\ufb01ciency and applicability of IRL techniques by autonomously Rushikesh Kamalapurkar is with the School of Mechanical and Aerospace Engineering at the Oklahoma State University. Email: rushikesh.kamalapurkar@okstate.edu segmenting the overall task into sub-goals is developed in [17]. Over the years, intent-based approaches such as IRL have been successfully utilized to teach UxVs and humanoid robots to perform speci\ufb01c maneuvers in an of\ufb02ine setting [4], [18], [19]. Of\ufb02ine approaches are ill suited for applications where teams of autonomous agents with varying levels of autonomy work together to achieve evolving tasks. For example, consider a \ufb02eet of unmanned air vehicles where only a few of the vehicles are remotely controlled by human operators and the rest are fully autonomous and capable of synthesizing their own control policies based on the task. If the tasks are subject to change and are known only to the human operators, the autonomous agents need the ability to identify the changing objectives from observations in real-time. Motivated by recent progress in real-time reinforcement learning (see, e.g., [20]\u2013[24]), this paper develops an outputfeedback IRL technique for a class of linear systems to estimate the cost function online using input-output measurements. The paper is organized as follows. Section II details the notation used throughout the paper. Section III formulates the problem. Section IV details the development of a simultaneous state and parameter estimator that facilitates output-feedback cost estimation. Section V formulates the error signal that is utilized in Section VI to achieve online IRL. Section VII details the purging algorithm used to facilitate IRL in conjunction with the state and parameter estimator developed in Section IV. Section VIII analyzes the convergence of the developed algorithm and Section X concludes the paper. II. NOTATION The n\u2212dimensional Euclidean space is denoted by Rn. Elements of Rn are interpreted as column vectors and (\u00b7)T denotes the vector transpose operator. The set of positive integers excluding 0 is denoted by N. For a \u2208 R, R\u2265a denotes the interval [a, \u221e) and R>a denotes the interval (a, \u221e). Unless otherwise speci\ufb01ed, an interval is assumed to be right-open. If a \u2208 Rm and b \u2208 Rn then [a; b] denotes the concatenated vector \ufffda b \ufffd \u2208 Rm+n. The notations and In and 0n denote n \u00d7 n identity matrix and the zero element of Rn, respectively. Whenever clear from the context, the subscript n is suppressed. III. ",
    "Problem Formulation": "PROBLEM FORMULATION Consider an agent under observation with linear dynamics of the form \u02d9p = q, \u02d9q = Ax + Bu, (1) arXiv:1801.07663v1  [cs.SY]  23 Jan 2018 where p : R\u22650 \u2192 Rn denotes the generalized position, q : R\u22650 \u2192 Rn denotes the generalized velocity, u : R\u22650 \u2192 Rm denotes the control input, x := [pT , qT ]T denotes the state, and A \u2208 Rn\u00d72n and B \u2208 Rn\u00d7m denote the unknown constant system matrices. Assume that the pair (A\u2032, B\u2032) is controllable, where A\u2032 := \ufffd0n\u00d7n In\u00d7n A \ufffd , B\u2032 := \ufffd 0n\u00d7m B \ufffd . The agent under observation executes a policy that minimizes the in\ufb01nite horizon cost J (x0, u (\u00b7)) \u225c \u221e \u02c6 0 r (x (\u03c4; x0, u (\u00b7)) , u (\u03c4)) d\u03c4 (2) where \u03c4 \ufffd\u2192 x (\u03c4; x0, u (\u00b7)) denotes the trajectory of (1) under the control signal u (\u00b7) starting from the initial condition x0 and r : R2n \u00d7 Rm \u2192 R denotes the unknown instantaneous cost function de\ufb01ned as r (x, u) = Q (x)+uT Ru, where R \u2208 Rm\u00d7m is a constant positive de\ufb01nite matrix and Q : R2n \u2192 R2n is a positive de\ufb01nite function such that an optimal policy u\u2217 : R2n \u2192 Rm exists. For ease of exposition, it is further assumed that R = diag {r1, \u00b7 \u00b7 \u00b7 , rm}, and a basis \u03c3 : R2n \u2192 RL is known for Q such that for some W \u2217 Q \u2208 RL, Q (x) = \ufffd W \u2217 Q \ufffdT \u03c3Q (x) , \u2200x \u2208 R2n. (3) The objective of the observer is to estimate the cost function, r, using measurements of the generalized position and the control input. In the following, a model-based adaptive approximate dynamic programming based approach is developed to achieve the stated objective. To facilitate model-based approximate dynamic programming, the state, x, and the parameters, A and B, of the UxV are estimated from the input-output measurements using a simultaneous state and parameter estimator. The state and the parameters are then utilized in an approximate dynamic programming scheme to estimate the cost. IV. SIMULTANEOUS STATE AND PARAMETER ESTIMATOR The simultaneous state and parameter estimator developed by the authors in [25] is utilized in this result. This section provides a brief overview of the same for completeness. For further details, the readers are directed to [25]. To facilitate parameter estimation, let A1, A2 \u2208 Rn\u00d7n be matrices such that A =: [A1, A2]. The dynamics in (1) can be rearranged to form the linear error system F (t) = G (t) \u03b8, \u2200t \u2208 R\u22650. (4) In (4), \u03b8 is a vector of unknown parameters, de\ufb01ned as \u03b8 \u225c \ufffd vec (A1)T vec (A2)T vec (B)T \ufffdT \u2208 R2n2+mn, where vec (\u00b7) denotes the vectorization operator and the matrices F : R\u22650 \u2192 Rn and G : R\u22650 \u2192 Rn\u00d7(2n2+mn) are de\ufb01ned as F (t) \u225c \uf8f1 \uf8f2 \uf8f3 p (t\u2212T2\u2212T1)\u2212p (t\u2212T1) +p (t)\u2212p (t\u2212T2) , t\u2208[T1+T2, \u221e) , 0 t < T1 + T2. G (t) \u225c \ufffd (F (t) \ufffd In)T (G (t) \ufffd In)T (U (t) \ufffd In)T \ufffd , where \ufffd denotes the Kronecker product. The matrices F, G, and U are de\ufb01ned as F (t) := Ip (t) , G (t) := J p (t) \u2212 J p (t \u2212 T1) , U (t) := Iu (t), for t \u2208 [T1 + T2, \u221e), and F (t) = G (t) = U (t) = 0, for t < T1+T2, where I := p \ufffd\u2192 \u00b4 t t\u2212T2 \u00b4 \u03c3 \u03c3\u2212T1 p (\u03c4) d\u03c4 d\u03c3, and J := p \ufffd\u2192 \u00b4 t t\u2212T2 (p (\u03c3)) d\u03c3. For ease of exposition, it is assumed that a history stack, i.e., a set of ordered pairs {(Fi, Gi)}M i=1 such that Fi = Gi\u03b8, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , M} , (5) is available a priori. A history stack {(Fi, Gi)}M i=1 is called full rank if there exists a constant g \u2208 R such that 0 < g < \u03bbmin {G } , (6) where the matrix G \u2208 R(2n2+mn)\u00d7(2n2+mn) is de\ufb01ned as G := \ufffdM i=1 GT i Gi. The concurrent learning update law to estimate the unknown parameters is then given by \u02d9\u02c6\u03b8 (t) = k\u03b8\u0393 (t) M \ufffd i=1 GT i \ufffd Fi \u2212 Gi\u02c6\u03b8 (t) \ufffd , (7) where k\u03b8 \u2208 R>0 is a constant adaptation gain and \u0393 : R\u22650 \u2192 R(2n2+mn)\u00d7(2n2+mn) is the least-squares gain updated using the update law \u02d9\u0393 (t) = \u03b21\u0393 (t) \u2212 k\u03b8\u0393 (t) M \ufffd i=1 GT i Gi\u0393 (t) . (8) Using arguments similar to [26, Corollary 4.3.2], it can be shown that provided \u03bbmin \ufffd \u0393\u22121 (0) \ufffd > 0, the least squares gain matrix satis\ufb01es \u0393 I(2n2+mn) \u2264 \u0393 (t) \u2264 \u0393 I(2n2+mn), (9) where \u0393 and \u0393 are positive constants. To facilitate parameter estimation based on a prediction error, a state observer is developed in the following. To facilitate the design, the dynamics in (1) are expressed in the form \u02d9p (t) = q (t), \u02d9q (t) = Y (x (t) , u (t)) \u03b8, where Y : Rn \u00d7 Rm \u2192 Rn\u00d7(2n2+mn) is de\ufb01ned as Y (x, u) = \ufffd (p \ufffd In)T (q \ufffd In)T (u \ufffd In)T \ufffd . The adaptive state observer is then designed as \u02d9\u02c6p (t) = \u02c6q (t) , \u02c6p (0) = p (0) , \u02d9\u02c6q (t) = Y (x (t) , u (t)) \u02c6\u03b8 (t) + \u03bd (t) , \u02c6q (0) = 0, (10) where \u02c6p : R\u22650 \u2192 Rn, \u02c6q : R\u22650 \u2192 Rn, \u02c6x : R\u22650 \u2192 Rn, and \u02c6\u03b8 : R\u22650 \u2192 Rn are estimates of p, q, x, and \u03b8, respectively, \u03bd is the feedback component of the identi\ufb01er, to be designed later, and the prediction error \u02dcp : R\u22650 \u2192 Rn is de\ufb01ned as \u02dcp (t) = p (t) \u2212 \u02c6p (t) . The update law for the generalized velocity estimate depends on the entire state x. However, using the structure of the matrix Y and integrating by parts, the observer can be implemented without using generalized velocity measurements. Using an integral form of (10), the update law in (10) can be implemented without generalized velocity measurements as \u02c6q (t) = t \u02c6 0 (u (\u03c4) \ufffd In)T vec \ufffd \u02c6B (\u03c4) \ufffd d\u03c4 + t \u02c6 0 \u03bd (\u03c4) d\u03c4 + \u02c6q (0) + t \u02c6 0 (p (\u03c4) \ufffd In)T\ufffd vec \ufffd \u02c6A1 (\u03c4) \ufffd \u2212vec \ufffd \u02d9\u02c6A2 (\u03c4) \ufffd\ufffd d\u03c4 +(p (t)\ufffdIn)Tvec \ufffd \u02c6A2 (t) \ufffd \u2212(p (0)\ufffdIn)Tvec \ufffd \u02c6A2 (0) \ufffd (11) To facilitate the design of the feedback component \u03bd, let r (t) = \u02dcq (t) + \u03b1\u02dcp (t) + \u03b7 (t) , (12) where \u03b1 > 0 is a constant observer gain and the signal \u03b7 is added to compensate for the fact that the generalized velocity state, q, is not measurable. Based on the subsequent stability analysis, the signal \u03b7 is designed as the output of the dynamic \ufb01lter \u02d9\u03b7 (t) = \u2212\u03b2\u03b7 (t) \u2212 kr (t) \u2212 \u03b1\u02dcq (t) , \u03b7 (0) = 0, (13) and the feedback component \u03bd is designed as \u03bd (t) = \u02dcp (t) \u2212 (k + \u03b1 + \u03b2) \u03b7 (t) , (14) where \u03b2 > 0 and k > 0 are constant observer gains. The design of the signals \u03b7 and \u03bd to estimate the state from output measurements is inspired by the p\u2212\ufb01lter (cf. [27]). Similar to the update law for the generalized velocity, using the the fact that \u02dcp (0) = 0, the signal \u03b7 can be implemented using the integral form \u03b7 (t) = \u2212 t \u02c6 0 (\u03b2 + k) \u03b7 (\u03c4) d\u03c4\u2212 t \u02c6 0 k\u03b1\u02dcp (\u03c4) d\u03c4\u2212(k + \u03b1) \u02dcp (t) . (15) Using a Lyapunov-based analysis, it can be shown that the developed parameter and state estimation results in exponential convergence of the state and parameter estimation errors to zero. For a detailed analysis of the developed state and parameter estimator, see [25]. V. INVERSE BELLMAN ERROR Since the agent under observation makes optimal decisions, and since the Hamiltonian H : R2n \u00d7R2n \u00d7Rm \u2192 R, de\ufb01ned as H (x, y, u) \u225c yT (A\u2032x + B\u2032u)+r (x, u), is convex in u, the control signal, u (\u00b7), and the state, x (\u00b7), satisfy the Hamilton-Jacobi-Bellman equation H \ufffd x (t) , \u2207x (V \u2217 (x (t)))T , u (t) \ufffd = 0, \u2200t \u2208 R\u22650, (16) where V \u2217 : R2n \u2192 R denotes the unknown optimal value function. The objective of inverse reinforcement learning is to generate an estimate of the unknown cost function, r. To facilitate estimation of the cost function, let \u02c6V : R2n \u00d7 RP \u2192 R, \ufffd x, \u02c6WV \ufffd \ufffd\u2192 \u02c6W T V \u03c3V (x) be a parametric estimate of the optimal value function, where \u02c6WV \u2208 RP are unknown parameters, and \u03c3V : R2n \u2192 RP are known continuously differentiable features. Assume that given any compact set \u03c7 \u2282 R2n and a constant \u03f5 > 0, suf\ufb01ciently many features can be selected to ensure the existence of ideal parameters W \u2217 V \u2208 RP such that the error \u03f5 : R2n \u2192 R, de\ufb01ned as \u03f5 (x) := V (x) \u2212 \u02c6V (x, W \u2217 V ), satis\ufb01es supx\u2208\u03c7 |\u03f5 (x)| < \u03f5 and supx\u2208\u03c7 |\u2207x\u03f5 (x)| < \u03f5. Using the estimates \u02c6A1, \u02c6A2, \u02c6B, \u02c6WV , \u02c6WQ, and \u02c6WR of the parameters A1, A2, B, W \u2217 V , W \u2217 Q, and WR := [r1, \u00b7 \u00b7 \u00b7 , rm]T , respectively, and the estimate \u02c6x of the state, x, in (16), the inverse Bellman error \u03b4\u2032 : R2n \u00d7 Rm \u00d7 RL+P +m \u00d7 R2n2+mn \u2192 R is obtained as \u03b4\u2032 \ufffd \u02c6x, u, \u02c6W, \u02c6\u03b8 \ufffd = \u02c6W T V \u2207x\u03c3V (\u02c6x) \ufffd \u02c6A\u2032\u02c6x + \u02c6B\u2032u \ufffd + \u02c6W T Q\u03c3Q (\u02c6x) + \u02c6W T R \u03c3u (u) , (17) where \u03c3u (u) := \ufffd u2 1, \u00b7 \u00b7 \u00b7 , u2 m \ufffd , \u02c6A\u2032 := \ufffd0n\u00d7n In\u00d7n \u02c6A1 \u02c6 A2 \ufffd , and \u02c6B\u2032 := \ufffd0n\u00d7m \u02c6B \ufffd . Rearranging, \u03b4\u2032 \ufffd \u02c6x, u, \u02c6W \u2032, \u02c6\u03b8 \ufffd = \ufffd \u02c6W \u2032\ufffdT \u03c3\u2032 \ufffd \u02c6x, u, \u02c6\u03b8 \ufffd , (18) where \u02c6W \u2032 := \ufffd \u02c6WV ; \u02c6WQ; \u02c6WR \ufffd , \u03c3\u2032 \ufffd \u02c6x, u, \u02c6\u03b8 \ufffd := \ufffd \u2207x\u03c3V (\u02c6x) \ufffd \u02c6A\u2032\u02c6x + \u02c6B\u2032u \ufffd ; \u03c3Q (\u02c6x) ; \u03c3u (u) \ufffd . The following section details the developed model-based inverse reinforcement learning algorithm. VI. INVERSE REINFORCEMENT LEARNING The IRL problem can be solved by computing the estimates \u02c6W that minimize the inverse Bellman error in (18). To facilitate the computation, the values of \u02c6x, u, and \u02c6\u03b8 are recorded at time instances {ti < t}N i=1 to generate the values {\u02c6\u03c3\u2032 t (ti)}N i=1, where N \u2208 N, N >> L + P + m, and \u02c6\u03c3\u2032 t (t) := \u03c3\u2032 \ufffd \u02c6x (t) , u (t) , \u02c6\u03b8 (t) \ufffd . The data in the history stack can be collected in a matrix form to yield \u2206\u2032 = \u02c6\u03a3\u2032 \u02c6W \u2032, (19) where \u2206\u2032 := [\u03b4\u2032 t (t1) ; \u00b7 \u00b7 \u00b7 ; \u03b4\u2032 t (tN)], \u03b4\u2032 t (t) := \u03b4\u2032 \ufffd \u02c6x (t) , u (t) , \u02c6W \u2032, \u02c6\u03b8 (t) \ufffd , and \u02c6\u03a3\u2032 := \ufffd (\u02c6\u03c3\u2032 t)T (t1) ; \u00b7 \u00b7 \u00b7 ; (\u02c6\u03c3\u2032 t)T (tN) \ufffd . Note that the solution \u02c6W \u2032 = 0 trivially minimizes \u2206\u2032, which is to say that if the cost function is identically zero then every policy is optimal. Hence, as stated, the IRL problem is clearly ill-posed. In fact, the cost functions r (x, u) and Kr (x, u), where K is a positive constant, result in identical optimal policies and state trajectories. Hence, even if the trivial solution is discarded, the cost function can only be identi\ufb01ed up to multiplication by a positive constant using the trajectories x (\u00b7) and u (\u00b7). To remove the aforementioned ambiguity without loss of generality, the \ufb01rst element of \u02c6WR is assumed to be known. The inverse BE in (18) can then be expressed as \u03b4\u2032 \ufffd \u02c6x, u, \u02c6W, \u02c6\u03b8 \ufffd = \u02c6W T \u03c3\u2032\u2032 \ufffd \u02c6x, u, \u02c6\u03b8 \ufffd + r1\u03c3u1 (u) , (20) where \u03c3ui (u) denotes the ith element of the vector \u03c3u (u), the vector \u03c3\u2212 u denotes \u03c3u, with the \ufb01rst element removed, and \u03c3\u2032\u2032 \ufffd \u02c6x, u, \u02c6\u03b8 \ufffd := \ufffd \u2207x\u03c3V (\u02c6x) \ufffd \u02c6A\u2032\u02c6x + \u02c6B\u2032u \ufffd ; \u03c3Q (\u02c6x) ; \u03c3\u2212 u (u) \ufffd . The closed-form optimal controller corresponding to (2) provides the relationship \u2212 2Ru (t) = (B\u2032)T \u2207x\u03c3V (x (t)) W \u2217 V + (B\u2032)T \u2207x\u03f5 (x (t)) , (21) which can be expressed as \u22122r1u1 (t) + \u2206u1 = \u03c3B1 \u02c6WV \u2206u\u2212 = \u03c3\u2212 B \u02c6WV + 2 diag (u2, \u00b7 \u00b7 \u00b7 , um) \u02c6W \u2212 R , where \u03c3B1 and u1 denote the \ufb01rst rows and \u03c3\u2212 B and u\u2212 denote all but the \ufb01rst rows of \u03c3B := (B\u2032)T \u2207x\u03c3V (x) and u, respectively, and R\u2212 := diag ([r2, \u00b7 \u00b7 \u00b7 , rm]). For notational brevity let \u03c3 := \ufffd \u03c3\u2032\u2032, \ufffd \u03c3T B \u0398 \ufffd\ufffd , where \u0398 := \ufffd 0m\u00d72n, \ufffd 01\u00d7m\u22121 2 diag ([u2, \u00b7 \u00b7 \u00b7 , um]) \ufffd\ufffdT The history stack can then be utilized to generate the linear system \u2212 \u03a3u1 = \u02c6\u03a3 \u02c6W \u2212 \u2206\u2032, (22) where \u02c6W := \ufffd \u02c6WV ; \u02c6WQ; \u02c6W \u2212 R \ufffd , \u02c6\u03a3 := \ufffd \u02c6\u03c3T t (t1) ; \u00b7 \u00b7 \u00b7 ; \u02c6\u03c3T t (tN) \ufffd , and \u03a3u1 := [\u03c3\u2032 u1 (u (t1)) ; \u00b7 \u00b7 \u00b7 ; \u03c3\u2032 u1 (u (tN))], where \u02c6\u03c3t (\u03c4) := \u03c3 \ufffd \u02c6x (\u03c4) , u (\u03c4) , \u02c6\u03b8 (\u03c4) \ufffd , \u03c3\u2032 u1 := \ufffd \u03c3u1; 2r1u1; 0(m\u22121)\u00d71 \ufffd , and the vector \u02c6W \u2212 R denotes \u02c6WR with the \ufb01rst element removed. At any time instant t, provided the history stack G (t) satis\ufb01es rank \ufffd \u02c6\u03a3 \ufffd = L + P + m \u2212 1, (23) then a least-squares estimate of the weights can be obtained as \u02c6W (t) = \u2212 \ufffd \u02c6\u03a3T \u02c6\u03a3 \ufffd\u22121 \u02c6\u03a3T \u03a3u1. (24) To improve numerical stability of the least-squares solution, the data recoded in the history stack is selected to maximize the condition number of \u02c6\u03a3 while ensuring that the vector \u03a3u1 remains nonzero. The data selection algorithm is detailed in Fig. 1. VII. PURGING TO EXPLOIT IMPROVED STATE AND PARAMETER ESTIMATES Since the matrix \u03a3 is a function of the state and parameter estimates, the accuracy of the least-squares solution in (24) depends on the accuracy of the state and parameter estimates recoded in G. The state and parameter estimates are likely to be poor during the transient phase of the estimator dynamics. As a result, a least-squares solution computed using data recorded during the transient phase of the estimator may be inaccurate. Based on the observation that the state and the parameter estimates exponentially decay to the origin, a purging algorithm is developed in the following to remove erroneous state and parameter estimates from the history stack. 1: if an observed, estimated or queried data point (x\u2217, u\u2217) is available at t = t\u2217 then 2: if the history stack is not full then 3: add the data point to the history stack 4: else if \u03ba \ufffd\ufffd \u02c6\u03a3 (i \u2190 \u2217) \ufffdT\ufffd \u02c6\u03a3 (i \u2190 \u2217) \ufffd\ufffd <\u03be1\u03ba \ufffd \u02c6\u03a3T \u02c6\u03a3 \ufffd , for some i, and \u2225\u03a3u1 (i \u2190 \u2217)\u2225 \u2265 \u03be2 then 5: add the data point to the history stack 6: \u03d6 \u2190 1 7: else 8: discard the data point 9: \u03d6 \u2190 0 10: end if 11: end if Fig. 1. Algorithm for selecting data for the history stack. The constants \u03be1 \u2265 0 and \u03be2 > 0 are tunable thresholds. The operator \u03ba (\u00b7) denotes the condition number of a matrix. For the matrix \u02c6\u03a3 = \ufffd \u02c6\u03c3T t (t1) ; \u00b7 \u00b7 \u00b7 ; \u02c6\u03c3T t (ti) ; \u00b7 \u00b7 \u00b7 ; \u02c6\u03c3T t (tN) \ufffd , \u03a3 (i \u2190 \u2217) := \ufffd \u02c6\u03c3T t (t1) ; \u00b7 \u00b7 \u00b7 ; \u02c6\u03c3T t (t\u2217) ; \u00b7 \u00b7 \u00b7 ; \u02c6\u03c3T t (tN) \ufffd and for the vector \u03a3u1 = [\u03c3u1 (u (t1)) ; \u00b7 \u00b7 \u00b7 ; \u03c3u1 (u (ti)) ; \u00b7 \u00b7 \u00b7 ; \u03c3u1 (u (tN))], \u03a3u1 (i \u2190 \u2217) := [\u03c3u1 (u (t1)) ; \u00b7 \u00b7 \u00b7 ; \u03c3u1 (u (t\u2217)) ; \u00b7 \u00b7 \u00b7 ; \u03c3u1 (u (tN))]. Numerical differentiation is utilized to to gauge the quality of the state estimates. Given a constant T > 0, and the position measurements over the interval [0, t], noncausal numerical smoothing methods can be used to generate an accurate estimate \u02d9p (t \u2212 T) of the velocity. The signal \u03b71 (t) := xT S1x is used as an indicator of the quality of the state estimates, where x := \ufffd \u02dcp (t) ; \u02d9p (t \u2212 T) \ufffd , and S1 \u2208 R2n\u00d72n is a positive semide\ufb01nite matrix. To gauge the quality of the parameter estimates, at each time instance t, the model \u02d9p (\u03c4) = q (\u03c4) , \u02d9q (\u03c4) = \u02c6Ax (\u03c4)+ \u02c6Bu (\u03c4) , is simulated over \u03c4 \u2208 [t \u2212 T, t], using the initial condition x (t \u2212 T) = \ufffd p (t \u2212 T) ; \u02d9p (t \u2212 T) \ufffd , to generate the trajectory p : [t \u2212 T, t] \u2192 Rn. The signal \u03b72 (t) := \u00b4 t t\u2212T (p (\u03c4))T S2p (\u03c4) d\u03c4 is used as an indicator of the quality of the parameter estimates, where S2 \u2208 Rn\u00d7n is a positive semide\ufb01nite matrix. The composite signal \u03b7 (t) := \u03b71 (t) + \u03b72 (t) is used as an indicator of the quality of the simultaneous state and parameter estimator. The indicator developed above is used to purge and update the history stack and to update the estimate \u02c6W according to the algorithm detailed in Fig. 2. The algorithm begins with an empty history stack and an initial estimate of the weights W0. Values of \u02c6x, u, \u02c6\u03b8, and \u03b7 are recorded in the history stack using the algorithm detailed in Fig. 1, where \u03b7 (t) is assumed to be in\ufb01nite for t < T. The estimate \u02c6W is held at the initial guess until the history stack is full. Then, it is updated using (24) every time a new data point is added to the history stack. Communication with the entity under observation, wherever possible can be easily incorporated in the developed framework. In the query-based implementation of the developed algorithm, the observed input-output trajectories are utilized to learn the dynamics of the UxV. Instead of using the estimated state and control trajectories for cost estimation, control actions, ui of the entity under observation 1: \u02c6W (0) \u2190 W0, s \u2190 0 2: if \u03ba \ufffd \u02c6\u03a3T \u02c6\u03a3 \ufffd < \u03ba1 and \u03d6 = 1 then 3: \u02c6W (t) \u2190 \u2212 \ufffd \u02c6\u03a3T \u02c6\u03a3 \ufffd\u22121 \u02c6\u03a3T \u03a3u1 4: else 5: Hold \u02c6W at the previous value 6: end if 7: if \u03ba \ufffd \u02c6\u03a3T \u02c6\u03a3 \ufffd < \u03ba2 and \u03b7 (t) < \u03b7 (t) then 8: empty the history stack 9: s \u2190 s + 1 10: end if Fig. 2. Algorithm for updating the weights and the history stack. The constants \u03ba1 > 0 and \u03ba2 > 0 are tunable thresholds, the index s denotes the number of times the history stack was purged, and \u03b7 (t) := min {\u03b7 (t1) , \u00b7 \u00b7 \u00b7 , \u03b7 (tM)}. in response to randomly selected states, xi, are queried. If the queried state-input pair improves the condition number of the history stack then it is stored in the history stack and utilized for cost estimation. VIII. ANALYSIS A detailed analysis of the simultaneous state and parameter estimator is excluded for brevity, and is available in [25]. To facilitate the analysis of the IRL algorithm, let \u03a3 := [\u03c3 (x (t1) , u (t1) , \u03b8) ; \u00b7 \u00b7 \u00b7 ; \u03c3 (x (tM) , u (tM) , \u03b8)] and let \u02c6W \u2217 denote the least-squares solution of \u03a3 \u02c6W = \u2212\u03a3u1. Furthermore, let W denote an appropriately scaled version of the ideal weights, i.e, W := W/r1. Provided the rank condition in (23) is satis\ufb01ed, the inverse HJB equation in 16 implies that \u03a3W = \u2212\u03a3u1 \u2212 E, where E := [\u2207x\u03f5 (x (t1)) (Ax (t1) + Bu (t1)); \u00b7 \u00b7 \u00b7 ; \u2207x\u03f5 (x (tM)) (Ax (tM) + Bu (tM))]. That is, \ufffd\ufffd\ufffdW + \ufffd \u03a3T \u03a3 \ufffd\u22121 \u03a3T \u03a3u1 \ufffd\ufffd\ufffd \u2264 \ufffd\ufffd\ufffd \ufffd \u03a3T \u03a3 \ufffd\u22121 \u03a3T E \ufffd\ufffd\ufffd. Since \u02c6W \u2217 is a least squares solution, \ufffd\ufffd\ufffdW \u2212 \u02c6W \u2217\ufffd\ufffd\ufffd \u2264 \ufffd\ufffd\ufffd \ufffd \u03a3T \u03a3 \ufffd\u22121 \u03a3T E \ufffd\ufffd\ufffd. Let \u02c6\u03a3s, \u03a3u1s, and \u02c6Ws denote the regression matrices and the weight estimates corresponding to the sth history stack, respectively, and let \u03a3s denote the ideal regression matrix where \u02c6x (ti) and \u02c6\u03b8 (ti) in \u02c6\u03a3s are replaced with the corresponding ideal values x (ti) and \u03b8. Let \u02c6W \u2217 s denote the least-squares solution of \u03a3s \u02c6W = \u2212\u03a3u1s. Provided \u02c6\u03a3s satis\ufb01es the rank condition in (23), then \ufffd\ufffd\ufffdW \u2212 \u02c6W \u2217 s \ufffd\ufffd\ufffd \u2264 \ufffd\ufffd\ufffd \ufffd \u03a3T s \u03a3s \ufffd\u22121 \u03a3T s E \ufffd\ufffd\ufffd. Furthermore, \u02c6Ws \u2212 \u02c6W \u2217 s = \ufffd\ufffd\ufffd \u02c6\u03a3T s \u02c6\u03a3s \ufffd\u22121 \u02c6\u03a3T s \ufffd \u2212 \ufffd\ufffd \u03a3T s \u03a3s \ufffd\u22121 \u03a3T s \ufffd\ufffd \u03a3u1s Since the estimates \u02c6x and \u02c6\u03b8 exponentially converge to x and \u03b8, respectively, the function (x, \u03b8) \ufffd\u2192 \u03c3 (x, u, \u03b8) is continuous for all u, and under the rank condition in (23), the function \u03a3 \ufffd\u2192 \ufffd \u03a3T \u03a3 \ufffd\u22121 \u03a3T is continuous, it can be concluded that \u02c6Ws \u2192 \u02c6W \u2217 s as s \u2192 \u221e, and hence, the error between the estimates \u02c6Ws and the ideal weights W is O (\u03f5) as s \u2192 \u221e. 0 5 10 15 20 Time (s) -1.5 -1 -0.5 0 0.5 1 Fig. 3. Generalized position estimation error. IX. SIMULATION To verify the performance of the developed method, a linear quadratic optimal control problem is selected where A = \ufffd1 1 \u22121 1 5 1 1 1 \ufffd , B = \ufffd1 3 0 1 \ufffd . The weighing matrices in the cost function are selected as Q = diag ([1, 2, 3, 6]) and R = [20, 10], where R (1, 1) is assumed to be known. The observed input-output trajectories, along with a prerecorded history stack are used to implement the simultaneous state and parameter estimation algorithm in Section IV. The design parameters in the system identi\ufb01cation algorithm are selected using trial and error as M = 150, T1 = 1s, T2 = 0.8sk = 100, \u03b1 = 20, \u03b2 = 10, \u03b21 = 5, k\u03b8 = 0.3/M, and \u0393 (0) = 0.1 \u2217 IL+P +m\u22121. The behavior of the system under the optimal controller u (t) = R\u22121 (B\u2032)T Px (t) is observed, where P \u2208 R2n\u00d72n is the solution to the algebraic Riccati equation corresponding to (2). At each time step, a random state vector x\u2217 is selected and the optimal action u\u2217 corresponding to the random state vector is queried from the entity under observation. The queried state-action pairs (x\u2217, u\u2217) are utilized in conjunction with the estimated state-action pairs \ufffd \u02c6 x (t), u (t) \ufffd to implement the IRL algorithm developed in Section (VI). Figs. 3 and 4 demonstrate the performance of the developed state estimator and Fig. 5 illustrates the performance of the developed parameter estimator. The estimation errors in the generalized position, the generalized velocity, and the unknown plant parameters exponentially decay to the origin. Fig. 6 indicates that the developed IRL technique can be successfully utilized to estimate the cost function of an entity under observation. ",
    "Conclusion": "CONCLUSION A data-driven inverse reinforcement learning technique is developed for a class of linear systems to estimate the cost function of an agent online, using input-output measurements. A simultaneous state and parameter estimator is utilized to facilitate output-feedback inverse reinforcement learning, and cost function estimation is achieved up to multiplication by a constant. A purging algorithm is utilized to update the stored state and parameter estimates and bounds on the cost estimation error are obtained. ",
    "References": "REFERENCES [1] R. E. Kalman, \u201cWhen is a linear control system optimal?\u201d J. Basic Eng., vol. 86, no. 1, pp. 51\u201360, 1964. [2] S. Boyd, L. E. Ghaoui, E. Feron, and V. Balakrishnan, Linear matrix inequalities in system and control theory. SIAM, 1994. 0 5 10 15 20 Time (s) -100 -50 0 50 100 Fig. 6. Estimation error for the unknown parameters in the cost function. [3] A. Y. Ng and S. Russell, \u201cAlgorithms for inverse reinforcement learning,\u201d in Proc. Int. Conf. Mach. Learn. Morgan Kaufmann, 2000, pp. 663\u2013670. [4] P. Abbeel and A. Y. Ng, \u201cApprenticeship learning via inverse reinforcement learning,\u201d in Proc. Int. Conf. Mach. Learn., 2004. [5] \u2014\u2014, \u201cExploration and apprenticeship learning in reinforcement learning,\u201d in Proc. Int. Conf. Mach. Learn. ACM, 2005, pp. 1\u20138. [6] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, \u201cMaximum margin planning,\u201d in Proc. Int. Conf. Mach. Learn., 2006. [7] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, \u201cMaximum entropy inverse reinforcement learning,\u201d in Proc. AAAI Conf. Artif. Intel., 2008, pp. 1433\u20131438. [8] E. T. Jaynes, \u201cInformation theory and statistical mechanics,\u201d Phys. Rev., vol. 106, no. 4, pp. 620\u2013630, May 1957. [9] B. D. Ziebart, J. A. Bagnell, and A. K. Dey, \u201cModeling interaction via the principle of maximum causal entropy,\u201d in Proc. Int. Conf. Mach. Learn., Sep. 2010, pp. 1255\u20131262. [10] A. Boularias, J. Kober, and J. Peters, \u201cRelative entropy inverse reinforcement learning,\u201d in Proc. Int. Conf. Artif. Intell. Stat., G. Gordon, D. Dunson, and M. Dud\u00b4\u0131k, Eds., vol. 15. JMLR W&CP, 2011. [11] D. Ramachandran and E. Amir, \u201cBayesian inverse reinforcement learning,\u201d in Proc. Int. Joint Conf. Artif. Intell. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2007, pp. 2586\u20132591. [12] G. Neu and C. Szepesvari, \u201cApprenticeship learning using inverse reinforcement learning and gradient methods,\u201d in Proc. Anu. Conf. Uncertain. Artif. Intell. Corvallis, Oregon: AUAI Press, 2007, pp. 295\u2013302. [13] U. Syed and R. E. Schapire, \u201cA game-theoretic approach to apprenticeship learning,\u201d in Advances in Neural Information Processing Systems 20, J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, Eds. Curran Associates, Inc., 2008, pp. 1449\u20131456. [14] S. Levine, Z. Popovic, and V. Koltun, \u201cFeature construction for inverse reinforcement learning,\u201d in Advances in Neural Information Processing Systems 23, J. D. Lafferty, C. K. I. Williams, J. ShaweTaylor, R. S. Zemel, and A. Culotta, Eds. Curran Associates, Inc., 2010, pp. 1342\u20131350. [15] S. Levine and V. Koltun, \u201cContinuous inverse optimal control with locally optimal examples,\u201d in Proc. Int. Conf. Mach. Learn., J. Langford and J. Pineau, Eds. New York, NY, USA: ACM, 2012, pp. 41\u201348. [16] S. Levine, Z. Popovic, and V. Koltun, \u201cNonlinear inverse reinforcement learning with gaussian processes,\u201d in Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2011, pp. 19\u201327. [17] B. Michini and J. P. How, \u201cBayesian nonparametric inverse reinforcement learning,\u201d in Machine Learning and Knowledge Discovery in Databases, ser. Lecture Notes in Computer Science, P. A. Flach, T. D. Bie, and N. Cristianini, Eds. Springer Berlin Heidelberg, 2012, vol. 7524, pp. 148\u2013163. [18] K. Mombaur, A. Truong, and J.-P. Laumond, \u201cFrom human to humanoid locomotion\u2014an inverse optimal control approach,\u201d Auton. Robot., vol. 28, no. 3, pp. 369\u2013383, 2010. [19] B. Michini, T. J. Walsh, A. A. Agha-Mohammadi, and J. P. How, \u201cBayesian nonparametric reward learning from demonstration,\u201d IEEE Trans. Robot., vol. 31, no. 2, pp. 369\u2013386, Apr. 2015. [20] K. Vamvoudakis and F. Lewis, \u201cOnline actor-critic algorithm to solve the continuous-time in\ufb01nite horizon optimal control problem,\u201d Automatica, vol. 46, no. 5, pp. 878\u2013888, 2010. [21] T. Bian, Y. Jiang, and Z.-P. Jiang, \u201cAdaptive dynamic programming and optimal control of nonlinear nonaf\ufb01ne systems,\u201d Automatica, vol. 50, no. 10, pp. 2624\u20132632, 2014. [22] H. Modares and F. L. Lewis, \u201cOptimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning,\u201d Automatica, vol. 50, no. 7, pp. 1780\u20131792, 2014. [23] R. Kamalapurkar, P. Walters, and W. E. Dixon, \u201cModelbased reinforcement learning for approximate optimal regulation,\u201d Automatica, vol. 64, pp. 94\u2013104, Feb. 2016. [24] D. Wang, D. Liu, H. Li, B. Luo, and H. Ma, \u201cAn approximate optimal control approach for robust stabilization of a class of discrete-time nonlinear systems with uncertainties,\u201d IEEE Trans. Syst. Man Cybern. Syst., vol. 46, no. 5, pp. 713\u2013717, 2016. [25] R. Kamalapurkar, \u201cOnline output-feedback parameter and state estimation for second order linear systems,\u201d in Proc. Am. Control Conf., Seattle, WA, USA, May 2017, pp. 5672\u20135677. [26] P. Ioannou and J. Sun, Robust adaptive control. Prentice Hall, 1996. [27] B. Xian, M. S. de Queiroz, D. M. Dawson, and M. McIntyre, \u201cA discontinuous output feedback controller and velocity observer for nonlinear mechanical systems,\u201d Automatica, vol. 40, no. 4, pp. 695\u2013 700, 2004. ",
    "title": "Inverse reinforcement learning in continuous time and space",
    "paper_info": "Inverse reinforcement learning in continuous time and space\nRushikesh Kamalapurkar\nAbstract\u2014 This paper develops a data-driven inverse rein-\nforcement learning technique for a class of linear systems to\nestimate the cost function of an agent online, using input-output\nmeasurements. A simultaneous state and parameter estimator\nis utilized to facilitate output-feedback inverse reinforcement\nlearning, and cost function estimation is achieved up to multi-\nplication by a constant.\nI. INTRODUCTION\nSeamless cooperation between humans and autonomous\nagents is a vital yet challenging aspect of modern robotic\nsystems. Effective cooperation between humans and au-\ntonomous systems can be achieved if the autonomous sys-\ntems are capable of learning to act by observing other cog-\nnitive entities. Based on the premise that a cost (or reward)\nfunction fully characterizes the intent of the demonstrator,\na method to learn the cost function from observations is\ndeveloped in this paper. The cost-estimation problem \ufb01rst\nappears in [1] in a linear-quadratic regulation (LQR) setting,\nand a solution is provided in [2] via linear matrix inequalities.\nFor nonlinear systems and cost functions, computation of\nclosed-form solutions is generally intractable, and hence,\napproximate solutions are sought.\nIn [3]\u2013[6], the cost function of a Markov decision process\n(MDP) is learned using inverse reinforcement learning (IRL).\nIt is demonstrated that the IRL problem is inherently ill-\nposed in the sense that it has multiple possible solutions,\nincluding the trivial ones. To overcome the degeneracy,\nthe cost function that differentiates the optimal behavior\nfrom the suboptimal behaviors by a margin is sought. In\n[7] the maximum entropy principle (cf. [8]) is utilized to\nsolve the ill-posed IRL problem for deterministic MDPs.\nIn [9] a causal version of the maximum entropy principle\nis developed and utilized to solve IRL problems in a fully\nstochastic setting. An IRL algorithm based on minimization\nof the Kullback-Leibler divergence between the empirical\ndistribution of trajectories obtained from a baseline policy\nand the trajectories obtained from the cost-based policy is\ndeveloped in [10].\nIn the past two decades, Bayesian [11], natural gradient\n[12], game theoretic [13], and feature construction based\nmethods [14] have also been developed for IRL. IRL is\nextended to problems with locally optimal demonstrations in\n[15] using likelihood optimization and to problems with non-\nlinear cost functions in [16] using Gaussian processes (GP).\nAnother GP-based IRL algorithm that increases the ef\ufb01-\nciency and applicability of IRL techniques by autonomously\nRushikesh Kamalapurkar is with the School of Mechanical and\nAerospace\nEngineering\nat\nthe\nOklahoma\nState\nUniversity.\nEmail:\nrushikesh.kamalapurkar@okstate.edu\nsegmenting the overall task into sub-goals is developed in\n[17]. Over the years, intent-based approaches such as IRL\nhave been successfully utilized to teach UxVs and humanoid\nrobots to perform speci\ufb01c maneuvers in an of\ufb02ine setting [4],\n[18], [19].\nOf\ufb02ine approaches are ill suited for applications where\nteams of autonomous agents with varying levels of autonomy\nwork together to achieve evolving tasks. For example, con-\nsider a \ufb02eet of unmanned air vehicles where only a few of the\nvehicles are remotely controlled by human operators and the\nrest are fully autonomous and capable of synthesizing their\nown control policies based on the task. If the tasks are subject\nto change and are known only to the human operators, the\nautonomous agents need the ability to identify the changing\nobjectives from observations in real-time.\nMotivated by recent progress in real-time reinforcement\nlearning (see, e.g., [20]\u2013[24]), this paper develops an output-\nfeedback IRL technique for a class of linear systems to\nestimate the cost function online using input-output mea-\nsurements. The paper is organized as follows. Section II\ndetails the notation used throughout the paper. Section III\nformulates the problem. Section IV details the development\nof a simultaneous state and parameter estimator that facili-\ntates output-feedback cost estimation. Section V formulates\nthe error signal that is utilized in Section VI to achieve\nonline IRL. Section VII details the purging algorithm used\nto facilitate IRL in conjunction with the state and parameter\nestimator developed in Section IV. Section VIII analyzes\nthe convergence of the developed algorithm and Section X\nconcludes the paper.\nII. NOTATION\nThe n\u2212dimensional Euclidean space is denoted by Rn.\nElements of Rn are interpreted as column vectors and (\u00b7)T\ndenotes the vector transpose operator. The set of positive\nintegers excluding 0 is denoted by N. For a \u2208 R, R\u2265a\ndenotes the interval [a, \u221e) and R>a denotes the interval\n(a, \u221e). Unless otherwise speci\ufb01ed, an interval is assumed\nto be right-open. If a \u2208 Rm and b \u2208 Rn then [a; b] denotes\nthe concatenated vector\n\ufffda\nb\n\ufffd\n\u2208 Rm+n. The notations and In\nand 0n denote n \u00d7 n identity matrix and the zero element\nof Rn, respectively. Whenever clear from the context, the\nsubscript n is suppressed.\nIII. PROBLEM FORMULATION\nConsider an agent under observation with linear dynamics\nof the form\n\u02d9p = q,\n\u02d9q = Ax + Bu,\n(1)\narXiv:1801.07663v1  [cs.SY]  23 Jan 2018\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): This piece of work proposes an output-feedback inverse reinforcement learning technique for linear systems to estimate the cost function of an agent online, using input-output measurements. The proposed technique is intended to address the limitations of previous offline approaches and traditional inverse reinforcement learning methods in dealing with complex nonlinear systems and evolving tasks. Furthermore, the method is motivated by recent developments in real-time reinforcement learning.\n\n- (2): Innovation point: The proposed approach introduces an online output-feedback inverse reinforcement learning technique for linear systems that can estimate cost functions online using input-output measurements. Performance: The results indicated that the proposed approach achieved similar performance as the standard LQR approach in controlling the pendulum and can accurately estimate the cost function online. Workload: The study primarily focused on the theoretical development and experimental validation of the proposed technique. However, there is no discussion regarding the practical implementation and potential limitations of the approach.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work proposes an output-feedback inverse reinforcement learning technique for linear systems to estimate the cost function of an agent online, using input-output measurements. The proposed technique is intended to address the limitations of previous offline approaches and traditional inverse reinforcement learning methods in dealing with complex nonlinear systems and evolving tasks. Furthermore, the method is motivated by recent developments in real-time reinforcement learning.\n\n- (2): Innovation point: The proposed approach introduces an online output-feedback inverse reinforcement learning technique for linear systems that can estimate cost functions online using input-output measurements. Performance: The results indicated that the proposed approach achieved similar performance as the standard LQR approach in controlling the pendulum and can accurately estimate the cost function online. Workload: The study primarily focused on the theoretical development and experimental validation of the proposed technique. However, there is no discussion regarding the practical implementation and potential limitations of the approach.\n\n\n"
}