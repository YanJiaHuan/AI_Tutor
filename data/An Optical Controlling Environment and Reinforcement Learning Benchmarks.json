{
    "Abstract": "ABSTRACT Deep reinforcement learning has the potential to address various scienti\ufb01c problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment incorporates nonconvex and nonlinear optical phenomena as well as more realistic timedependent noise. Then we provide the benchmark results of several state-of-the-art reinforcement learning algorithms on the proposed simulation environment. In the end, we discuss the dif\ufb01culty of controlling the real-world optical environment with reinforcement learning algorithms. Keywords Optical Control \u00b7 Reinforcement Learning \u00b7 Simulation 1 ",
    "Introduction": "Introduction In recent years, deep reinforcement learning (RL) has been used to solve challenging problems in various \ufb01elds [1], including self-driving car [2] and robot control [3]. Among all of the applications, deep RL made signi\ufb01cant progress in play games on a superhuman level [4, 5, 6, 7]. Beyond playing games, deep RL has the potential to strongly impact the traditional control and automation tasks in the natural science, such as control problems in chemistry [8], biology [9], quantum physics [10], optics and photonics [11]. In optics and photonics, there is particular potential for RL methods to drive the next generation of optical laser technologies [11]. This is not only because there is increasing demand for adaptive control and automation (of tuning and control) for optical systems [12], but also because many phenomena in optics are nonlinear and multidimensional [13], with noise-sensitive dynamics that are extremely challenging to model using conventional methods. RL methods are able to control multidimensional environment with nonlinear function approximation [14]. Thus, study the RL controller in optics becomes increasingly promising in optics and photonics as well as its applications in scienti\ufb01c research, medicine, and other industries [11, 15]. Traditionally, many of the control problems in optics and photonics were implemented by stochastic parallel gradient descent (SPGD) algorithm with PID controller [16, 17, 18]. The target is to maximize the reward (e.g. optical pulse energy) by adjusting and controlling the system parameters. The SPGD algorithm is one of the special cases of stochastic error descent method [16, 19]. Stochastic error descent is based on the model-free distributed learning mechanism. A parameter update rule is proposed by which each individual parameter vector perturbation contributes to a decrease in error (or increase in reward). However, SPGD is typically a convex optimization solver, and many control problems in optics are non-convex. SPGD may be failed to search the global optimum of the optics control system unless the initial state of the system is near a global optimum. Conventionally, the initial state of the optical system was adjusted by experienced experts, then utilizing SPGD to control the manually adjusted system, which becomes extremely hard with the increasing system complexity. In order to achieve ef\ufb01cient control and automation, deep RL was introduced to control optical systems [20, 21, 22, 23]. Most of the previous works implemented Deep-Q Network [4] and Deep Deterministic Policy Gradient [24], in optical control systems to achieve the comparable performance with traditional SPGD-PID control [20, 25]. But there is a lack of works on the evaluation of more RL algorithms in the more complex optical control environment. Studying and validating RL algorithms in the real-world optical system is a challenging process because its cost is expensive and requires experienced experts to implement the optical system. Instrumenting and operating RL algorithms in a simple optical system require signi\ufb01cant funds and manpower. An effective alternative to validate RL algorithms in arXiv:2203.12114v1  [cs.LG]  23 Mar 2022 An Optics Controlling Environment optics is simulation. Simulation has been used for robotics and autonomous driving since the early days of research [26, 27]. As learning-based robotics expands in both interest and application, the role of simulation may become ever more critical in driving research progress. But there is not any open sourced RL environment for optics control simulation by now. In this paper, we introduce OPS (Optical Pulse Stacking environment) - a scalable open simulator for controlling a typical optical system. The physics behind our OPS system is the same as many other optical problems, including coherent optical inference [28] and linear optical sampling [29], which can be used for precise measurement, industrial manufacturing, and scienti\ufb01c research. A typical optical pulse stacking system directly and symmetrically stacks up the input pulses to multiply the pulse energy for output stacked pulses [30, 31, 32, 33]. By providing an optical control simulation environment, we aim to encourage exploration of the application of RL on optical control tasks and furtherly explore the RL controllers in natural science. We use OPS to evaluate some important RL algorithms including twin delayed deep deterministic policy gradient (TD3, [34]), soft actor-critic (SAC, [35]), and proximal policy optimization (PPO, [36]). After reporting the results of these RL algorithms, we discuss the dif\ufb01culty of RL algorithms in the real-world optical system. With the provided simulating environment OPS and the experiments of RL algorithms, we believe that this work can promote the research on RL applications in optics as well as bene\ufb01t both the machine learning and the optics community. The code of the paper is available at https://github.com/ Walleclipse/Reinforcement-Learning-Pulse-Stacking. 2 Simulation environment 2.1 Physics of the simulation \ud835\udc38\" \ud835\udf0f\" \ud835\udf0f$ \ud835\udc38% \ud835\udc38$ \ud835\udf0f\" \ud835\udc38& \ud835\udc38' \ud835\udf0f\" \ud835\udc38( \ud835\udc38) \ud835\udf0f\" \ud835\udc38* \ud835\udc38\",% \ud835\udc38$,& \ud835\udf0f% \ud835\udc38',( \ud835\udc38),* \ud835\udf0f% \ud835\udc38\",%,$,& \ud835\udc38',(,),* 1st\tstage\tstacking 2nd\tstage\tstacking 3rd\tstage\tstacking \ud835\udc38;<= Figure 1: Illustration of the principle of optical pulse stacking. Only 3-stage pulse stacking was plotted for simplicity. The optical pulse stacking (OPS, or also called pulse combination) system recursively stacks up the optical pulses in the time domain. The dynamics of the OPS are similar to the recurrent neural networks (RNN) or Wavenet architecture [37]. We illustrate the dynamics of the OPS in RNN style as shown in \ufb01g. 1. The input of the OPS is a periodic pulse train1 with a repetition period of T. Assume the basic function of the \ufb01rst pulse at time step t is E1 = E(t), then the consecutive pulses can be described as E2 = E(t + T), E3 = E(t + 2T)... The OPS system recursively imposes the time delay on earlier pulses for every two consecutive pulse pairs. As an example, the 1st stage time-delay controller imposes the time delay \u03c41 on pulse 1 to shift the pulse 1. With the proper time delay, pulse 1 could be stacked with the next pulse E2 to create the stacked pulses E1,2 = E(t + \u03c41) + E(t + T). Similarly, pulse 3 could be stacked with pulse 4 to create E3,4 = E(t + 2T + \u03c41) + E(t + 3T) , and so on. In 2nd stage OPS, the time delay \u03c42 was further 1The periodic pulse train generally emitted by lasers. The wave function of each laser pulse is almost the same except for the time delay of a period. 2 An Optics Controlling Environment imposed to E1,2 to make it to stack up with E3,4 to create E1,2,3,4. This kind of stacking is repeated in each stage OPS controller, which stacks up the pulses in geometrical progression (recursion). An N-stage OPS system simply multiplies pulse energy by 2N times by stacking up 2N pulses, in which N time delays (\u03c41, \u03c42, ..., \u03c4N) are needed to control and stabilize. Please check the more detailed illustration and con\ufb01guration of the real-world OPS experiment in appendix A. 2.2 Control objective and noise The objective of the controlling OPS system is to maximize the \ufb01nal stacked (output) pulse energy by adjusting the time delays. For N-stage OPS system, let PN denotes the \ufb01nal energy of N times stacked pulse, and \u03c4 = [\u03c41, \u03c42, \u00b7 \u00b7 \u00b7 , \u03c4N] denote the time delays. Then the objective function for controlling N-stage OPS system is: arg max \u03c4 PN(\u03c4) = arg max\u03c41,\u03c42,...,\u03c4N PN(\u03c41, \u03c42, ..., \u03c4N) (1) If any noise were ignored, we would analyze the exact function of the \ufb01nal pulse energy PN w.r.t. the time delays \u03c4. Figure 2(a) shows the function of the pulse energy P1(\u03c41) w.r.t. the \ufb01rst time delay \u03c41 in 1-stage OPS system. And \ufb01g. 2(b) shows the function surface of P2(\u03c41, \u03c42) w.r.t. the \ufb01rst and second stages time delay (\u03c41, \u03c42) in 2-stage OPS system. As can be seen, the control function of the OPS system is non-linear and non-convex even ignoring any noises2. This is a challenging problem for any controlling algorithms to achieve the global optimum (or better local optimum) from a random initial state. 20 16 12 8 4 0 4 8 12 16 20 time delay  0.2 0.4 0.6 0.8 1.0 combined pulse energy (a) One stage OPS (1-d) 1st stage time delay  4 2 0 2 4 2nd stage time delay  4 2 0 2 4 combined pulse energy 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 (b) Two stage OPS (2-d) Figure 2: Function plot of the (a) 1-stage OPS: pulse energy P1(\u03c41) w.r.t. delay line \u03c41. (b) 2-stage OPS: pulse energy P2(\u03c41, \u03c42) w.r.t. delay lines (\u03c41, \u03c42). In general, noise can not be ignored and the system is quite noise-sensitive. That is because the wavelength of the pulse is in \u00b5m level (1\u00b5m = 10\u22126m). The noise in the environment, including vibration of optical devices and temperature drift of the atmosphere, could easily bring the shift of the time delay then change the output pulses. So the objective function in real-world practice is more complex than \ufb01g. 2, especially for higher stage (high-dimensional) OPS. Therefore, under (unpredictable) noise and the noise-sensitive complex system, the model of the system is hard to be achieved [11]. So it is hard to implement model-based controllers. In this paper, we mainly consider model-free reinforcement learning approaches. In this simulation, we mainly consider two kinds of noise. The \ufb01rst one is fast noise which comes from the vibration of devices. The noise could be formulated as a zero-mean Gaussian random noise N(0, \u03c3) by following the simulation noise of [20]. The second is slow noise \u00b5t, which comes from slow temperature drift. The in\ufb02uence of the temperature drift can be formulated as a piecewise linear function [38]. By incorporating these two kinds of noise, overall noise et can be formulated as a random process, where: E [et] = \u00b5t, VAR [et] = \u03c32 (2) 2Part of the reasons are the optical periodicity and nonlinearity of the coherent interference. 3 An Optics Controlling Environment 2.3 Reinforcement learning environment Interactions with RL agent. An RL agent interacts with the OPS environment in discrete time steps, as shown in \ufb01g. 3. At each time step t, the RL agent receives the current state of the OPS environment st. Then the RL agent chooses an action at to send the OPS environment. The environment conduct the action and moves to new state st+1. Then the reward rt, which measured by the state st+1, feedback to the RL agent. The RL agent trained with the experience (st, at, st+1, rt) to learn a policy \u03c0(a, s) which maximizes the expected cumulative reward. OPS Environment Initial pulses RL agent state \ud835\udc60 \ud835\udc38! \ud835\udc38\" \ud835\udc38# \ud835\udc38$ \ud835\udc38!,\" \ud835\udc38#,$ \ud835\udc38!,\",#,$ 1st stage time delay \ud835\udf0f! 2nd stage time delay \ud835\udf0f\" Photo detection reward \ud835\udc5f 1st stacked pulses 2nd stacked pulses action \ud835\udc4e \ud835\udf0f! \ud835\udf0f\" Figure 3: Illustration of the interaction between RL agent and OPS environment. Only 2-stage pulse stacking was plotted in OPS for simplicity. State space. State space of OPS is a continual and multidimensional vector space. The state value st could be described as the pulse amplitude measurement of the \ufb01nal stacked pulse st = Amplitude(Eout(t)). So st is the time-domain \"picture\" of the \ufb01nal stacked pulse, which directly re\ufb02ects the performance of the control. In a real-world system, pulse amplitude was detected by a photo-detector then converted to digital time-series signals. In our simulation, we also implement real-time rendering of the pulse amplitude to monitor the controlling process. Action space. Action space of N-stage OPS environment is a continual and N-dimensional vector space. At time step t, the action at is an additive time delay value \u2206\u03c4(t) for N-stage OPS environment: at = \u2206\u03c4(t) = (\u03c41(t + 1) \u2212 \u03c41(t), \u03c42(t + 1) \u2212 \u03c42(t), ..., \u03c4N(t + 1) \u2212 \u03c4N(t)) or \u03c4(t + 1) = at + \u03c4(t) . The additive time delay value a(t) was conducted by OPS environment to lead the next state. Reward. As mentioned in section 2.2, the objective of the OPS controller is maximizing the \ufb01nal stacked pulse energy PN(\u03c4). We used the reward value as normalized \ufb01nal pulse energy: r = \u2212(PN(\u03c4) \u2212 Pmax)2 (Pmin \u2212 Pmax)2 , (3) where Pmax is the maximum pulse energy achieved at the global optimum, and Pmin is the minimum pulse energy. The maximum reward 0 achieved when P(\u03c4) = Pmax (peak position of \ufb01g. 2(b)) . State transition function. The environmental noise poses direct impacts to the delay lines (including the vibration and temperature shift noise of the delay line devices). So in the state transition, real conducted delay line \u03c4real(t + 1) is a combination of the action at and noise et: \u03c4real(t + 1) = \u03c4(t + 1) + et = \u03c4(t) + at + et. (4) Then the real time delay \u03c4real(t) imposed to some selected pulses by delay line devices (the device impose additional time delay for pulses) as conduct the action. The state transition is governed by state, action and noise. The exact form of the state transition follow the principle of the coherent pulse interference. Let f is a observation function (which observe \ufb01nal output pulse by time delay value). Then the state transition can be written as: st+1 = f(\u03c4real(t + 1)) = f(\u03c4real(t) + at + et) = f(f \u22121(st) + at + et) (5) Please note that E [et] = \u00b5t is a slow-changed piecewise linear function, which changes very slowly with time. For episodic training for RL agents, the \u00b5t could be considered as a constant value for iterations within an episode. But the value of \u00b5t might differ from one episode to the next episode. In this case, we can assume \u00b5t changes very slowly, then approximate the OPS control process as a Markov decision process (MDP). If the higher accuracy is needed, one can include the noise in the state de\ufb01nition, st = [Amplitude(Eout(t)); et], then the control process can be formulated as a partially observable Markov decision process (POMDP). 4 An Optics Controlling Environment Different control dif\ufb01culty of the environment. We implemented the OPS environment for arbitrary (N \u2208 {1, 2, 3, ...}) stage of pulse stacking. With the increase of the number of stages, the control would become more and more dif\ufb01cult. In addition to the customized number of stages, we also provided three modes (easy, medium, and hard) for each stage OPS, as shown in \ufb01g. 4. The mode was determined by the initial state of the system and noise distribution: \u2022 Easy mode. The initial state of the OPS system is near the global optimum for easy mode. Figure 6(a) shows the example initial state of the easy mode of the 3-stage OPS environment. This is a case for many traditional optics control problems: the initial state of the system is tuned by \"experts\" to make it easy to control for convex controllers. \u2022 Medium mode. The initial state of the system is random, as shown in \ufb01g. 6(b), which makes the control problem becomes nonconvex. But in medium mode, the noise is time-independent and we simply set the noise distribution as et \u223c N(0, \u03c3). This is the case for many classical reinforcement learning and typical MDP settings. The noise distribution of each episode is the same. \u2022 Hard mode. Similar to medium mode and \ufb01g. 6(b), the initial state of the system is random. Different from the medium mode, the noise behavior is more complicated. The mean value of the noise distribution \u00b5t is a time-dependent variable, which slowly changes during time. This case is not a typical MDP. The hard mode is closer to real-world settings. Because in real-world applications, we always deploy the testing environment and algorithms after training, so the noise distribution of the testing environment is different from the training environment. Mode Initial state Noise easy near the  optimum time independent; \ud835\udf07! \u2261 0 medium random time independent; \ud835\udf07! \u2261 0 hard random time dependent; d\ud835\udf07!/d\ud835\udc61 \u2262 0 Figure 4: Comparison of the different game modes. from optics_env import OPS_env env = OPS_env(stage=5, mode= \"medium\") env.reset() done = False while not done: action = env.action_space.sample() observation, reward, done, info = env.step(action) env.render() Figure 5: Example code of the OPS environment. 80 60 40 20 0 20 40 60 80 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) (a) Initial state: easy mode 80 60 40 20 0 20 40 60 80 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) (b) Initial state: medium/hard mode 80 60 40 20 0 20 40 60 80 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) (c) Target optimal state Figure 6: Rendering examples of the state of (a) initial state for easy mode, (b) initial state for medium or hard mode, (c) global optimal target state in a 2-stage OPS environment. The initial state of the easy mode has almost sacked some parts of pulses, which is more closer to the target state. The initial state of medium or hard mode is almost random and might be trapped into local optimum. API & sample usage. The optical and physical principle of the simulation is based on the Nonlinear-Optics-Modeling package [39]. The OPS environment is out of the box compatible with the widely used OpenAI Gym API [40]. We show an example code of running random agent on OPS environment as \ufb01g. 5. Features of the OPS environment. We summarize the key features of the OPS environment as follows: \u2022 Open source optical control environment. To the best of our knowledge, this is the \ufb01rst open-sourced RL environment for optics control problems. The open-source licenses enable researchers to inspect the underlying code and to modify environments if required to test new research ideas. \u2022 Scalable and dif\ufb01culty-adjustable scienti\ufb01c environment. Many of the recent RL environments (e.g. Atari) are easy to solve. In our OPS environment, the dif\ufb01culty of the environment is \ufb02exible. And the dimension of the action space is easy to scalable with stage number N. If we choose quite larger N with hard mode, controlling the environment could become quite hard. If the hard scienti\ufb01c control problem could be solved effectively, which would have a broader impact on many scienti\ufb01c control problems [11, 15]. 5 An Optics Controlling Environment \u2022 Realistic noise. In the hard mode of the OPS environment, we module the noise distribution as the timedependent function. It made the noise distribution of the testing environment different from the noise distribution of the training environment. This is more realistic for noise-sensitive systems [38]. It also increases the stochasticity of the environment. \u2022 Extendable state and structural information. When \u00b5t changes very slowly, we can formulate the OPS control process as a MDP. If the higher accuracy is needed, we can include the noise in the state de\ufb01nition, then formulate the OPS control process as a POMDP. In addition, we can explore the structural information (or physical constrain) from the function of the OPS (\ufb01g. 2) and incorporate it with RL controllers. 3 Experiments 3.1 Experimental setup As a reference, we provide benchmark results for four state-of-the-art reinforcement learning algorithms: PPO [36], TD3 [34], and SAC [41]. We implement the algorithms using stable-baseline-3 [42]. The training procedure for an RL agent is divided into several episodes, each episode lasts for 200 steps. Other hyperparameters of each algorithm and training setting can be found in Appendix B.1. For each of the experimental settings, we run ten random seeds and average the results. 3.2 Results on controlling 5-stage OPS environment In this section, we mainly report the results for the 5-stage OPS system, that stacked 25 = 32 pulses. For the results of the different stage OPS system, the readers are referred to appendix B.2. In a 5-stage OPS system, we evaluate all of the four algorithms in 3 dif\ufb01culty modes of the environment: easy, medium, and hard. Training curve (plot for training reward per step w.r.t. iterations) of PPO, TD3, and SAC algorithms has been shown in \ufb01g. 7(a) for easy mode, \ufb01g. 7(b) for medium mode, and \ufb01g. 7(c) for hard mode. As can be seen, the performance of TD3 and SAC is similar and higher than PPO for all three modes. For the dif\ufb01culty mode of the environment, the convergence speed slows down and the \ufb01nal convergent value decreases with the increase of dif\ufb01culty of the environment. As an example, in easy mode, SAC converges to the reward value of \u22120.04 within 100,000 steps, but it takes 200,000 to converges to the reward value of \u22120.1 for hard mode. 0 50000 100000 150000 200000 250000 300000 Iterations 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Training Reward (per iteration) TD3 SAC PPO (a) Training curve: easy mode 0 50000 100000 150000 200000 250000 300000 Iterations 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Training Reward (per iteration) TD3 SAC PPO (b) Training curve: medium mode 0 50000 100000 150000 200000 250000 300000 Iterations 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Training Reward (per iteration) TD3 SAC PPO (c) Training curve: hard mode Figure 7: Training curve for SAC, TD3, and PPO on 5-stage OPS environment for (a) easy mode, (b) medium mode, and (c) hard mode. The dashed region shows the area within the standard deviation. After training the RL agents, we evaluated the performance in the testing environment. The \ufb01nal return (stacked pulse power PN ) under different iterations on easy mode, medium mode, and hard mode as shown in \ufb01g. 8(a), \ufb01g. 8(b), and \ufb01g. 8(c). As can be seen, although the training curve of the medium mode (\ufb01g. 7(b)) and hard mode \ufb01g. 7(c) is a bit of similar, the evaluation curve on testing environment of medium mode (\ufb01g. 8(b)) and hard mode \ufb01g. 8(c) is different. That is because the hard mode has a different noise distribution for the training and test environment. That makes the evaluation control on the testing environment for hard mode is slow to converge and achieved the lower \ufb01nal return. Please see the detailed results of the evaluation in appendix B.3. We reported the \ufb01nal return (combined pulse power PN ) of the training and testing environment on the trained policy in table 1. Please note that the training environment and testing environment for easy and medium mode is similar, just like the classical Atari environment. The performance differences are mainly caused by randomness. We showed that the performance difference between the training and testing environment is much higher for hard mode. That is because of the different noise behavior of the training and testing environment, which makes the control complicated. 6 An Optics Controlling Environment 0 25 50 75 100 125 150 175 200 Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Final Stacked Pulse Energy  TD3 SAC PPO (a) Testing curve: easy mode 0 25 50 75 100 125 150 175 200 Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Final Stacked Pulse Energy  TD3 SAC PPO (b) Testing curve: medium mode 0 25 50 75 100 125 150 175 200 Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Final Stacked Pulse Energy  TD3 SAC PPO (c) Testing curve: hard mode Figure 8: Evaluation of the stacked pulse power PN (normalized) of testing environment for (a) easy mode, (b) medium mode, and (c) hard mode. Mode Evaluation on which environment PPO TD3 SAC easy training 0.7684 \u00b1 0.0884 0.9580 \u00b1 0.0189 0.9637 \u00b1 0.0172 testing 0.7439 \u00b1 0.0463 0.9541 \u00b1 0.0177 0.9514 \u00b1 0.0231 medium training 0.6210 \u00b1 0.0828 0.9204 \u00b1 0.0351 0.8945 \u00b1 0.0501 testing 0.6182 \u00b1 0.0229 0.9106 \u00b1 0.0217 0.8833 \u00b1 0.0838 hard training 0.5473 \u00b1 0.0680 0.8524 \u00b1 0.0380 0.8515 \u00b1 0.0375 testing 0.4461 \u00b1 0.0300 0.8130 \u00b1 0.0215 0.8071 \u00b1 0.0164 Table 1: Evaluation performance of PPO, TD3, and SAC on three ( easy, medium, hard) modes. Final return PN on both the training environment and testing environment was evaluated. 3.3 Results on different stage experiments We evaluated all of the four algorithms of the different N-stage OPS environments with hard modes. Figure 9(a) shows the training curve, and \ufb01g. 9(b) shows the testing curve of TD3 on different N-stage OPS system. As can be seen, with the increase of stage number, the training convergence became slower, and the \ufb01nal return PN became smaller. 0 50000 100000 150000 200000 Iterations 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Training Reward (per iteration) 2-stage 3-stage 4-stage 5-stage (a) Training curve: TD3 with hard mode 0 25 50 75 100 125 150 175 200 Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Final Stacked Pulse Energy  2-stage 3-stage 4-stage 5-stage (b) Testing curve: TD3 with hard mode Figure 9: Comparison of the results on hard mode N-stage OPS environment with TD3 algorithms. (a) shows the training curve; (b) shows the evaluation of \ufb01nal return PN of the testing environment. We also evaluated the trained TD3 and SAC on the different N stage testing environment, as shown in \ufb01g. 10(a) and \ufb01g. 10(b). Figure 10(a) illustrated the \ufb01nal return PN under different stages OPS and different dif\ufb01culty modes. For 1-stage OPS, the \ufb01nal return PN could reach 1. That means TD3 and SAC are able to search the global optimum for 1-stage OPS. But for 5-stage OPS with hard mode, the SAC and TD3 only could achieve the 0.8 \ufb01nal return. That means the controlling algorithms were trapped into a local optimum. In the real-world experiments, this case means the 20% energy loss. Figure 10(a) illustrated the training-convergence step for different stages OPS. As the stage number increase, the number of steps to training convergence increases signi\ufb01cantly, which will slow down the training for higher stage OPS. 7 An Optics Controlling Environment 1 2 3 4 5 Stage number in OPS 0.80 0.85 0.90 0.95 1.00 Testing pulse energy TD3@hard TD3@medium TD3@easy SAC@hard SAC@medium SAC@easy (a) Testing curve (\ufb01nal return PN) 1 2 3 4 5 Stage number in OPS 0 25000 50000 75000 100000 125000 150000 175000 200000 Convergence step TD3@hard TD3@medium TD3@easy SAC@hard SAC@medium SAC@easy (b) Training convergence step Figure 10: (a) Final return PN of different stage OPS on testing environment controlled with TD3 or SAC. (b) Convergence steps for the training of TD3 and SAC on different stage OPS environments. 4 Discussion 4.1 Transfer trained policy It is possible to transfer the trained policy between different OPS environments. The major difference between the simulation and real-world environments is the different noise levels. We conduct an simulated experiment to show the transferability between different noise levels. In our simulation environment, the noise level is dependent on the dif\ufb01culty mode. Then we explore the transferability of the trained policy between \"easy\", \"medium\", and \"hard\" modes. After trained the policy on \"hard\" mode environment, we tested the trained policy on \"hard\", \"medium\", and \"easy\" environment, as shown in \ufb01g. 11 (a). Transfer results of \"medium\" trained policy and \"easy\" trained policy are shown in \ufb01g. 11 (b) and \ufb01g. 11 (c) respectively. As can be seen, We can perfectly transfer the harder mode trained policy to easier mode environments. When the easier mode training strategy is transferring to a harder mode environment, the performance may drop, and there is a jitter in pulse energy. Please see the discussion about the real-world and simulated experiments in appendix C. 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.2 0.4 0.6 0.8 1.0 Final Stacked Pulse Energy  hard -> hard hard -> medium hard -> easy (a) hard trained 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.2 0.4 0.6 0.8 1.0 Final Stacked Pulse Energy  medium -> hard medium -> medium medium -> easy (b) medium trained 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.2 0.4 0.6 0.8 1.0 Final Stacked Pulse Energy  easy -> hard easy -> medium easy -> easy (c) easy trained Figure 11: Demonstration of the transfer performance of the trained policy on (a) hard mode training env; (b) medium mode training env; (c) easy mode training env. Figure 11 shows that it is possible to transfer the trained policy between different noise levels, but it is more useful to train the policy in a harder environment than tested on the easier environment. Thus, we could explore fast and robust controlling algorithms in more harder simulation environment (with introducing more noise and more uncertainty in the simulation) then deploy the trained policy to real-world physical systems. 4.2 Connections with real-world experiment As far as we know, the previous real-world experiments of RL algorithms on complicated OPS systems are not very successful [20]. One reason is slow training in a real environment. To deploy the RL algorithm in an optics system, we need to convert optical signal to analog signal using photo-detector, then convert the analog signal to digital signal using an analog-to-digital converter. These two conversions not only cost some additional time to process the signal but also introduce some noises. Another reason is that it is hard to \ufb01nd satisfactory RL algorithms to handle such a complex and noise-sensitive system with non-stationary noise (or non-stationary state). So one of the promising approaches is 8 An Optics Controlling Environment sim2real: exploring RL algorithms in the simulation environment then deploying them to the real-world control system. Note that sim2real is not easy for optical control systems. Because of the coherent interference (which is a root for many optical control problems), there are many states with zero observation signal (coherent cancellation of the pulses with opposite phases). More of the controller failed when encountering many zero observations3. Compared to directly training on physical OPS system, sim2real relief the problem, but it still exists. So the fast training and noise-robust RL algorithms, that are able to handle non-stationary noise and non-convex control objectives, are critical to controlling tasks in optics. Which is our main concern about implementing OPS simulation environments. 5 Conclusion In this paper, we introduce OPS \u2013 an open-sourced simulator for controlling the pulse stacking system. To the best of our knowledge, this is the \ufb01rst open-sourced RL environment for optics control problems. Then we evaluated the SAC, TD3, and PPO on our proposed simulation environment. By providing an optical control simulation environment and RL benchmarks, we aim to encourage exploration of the application of RL on optical control tasks and furtherly explore the RL controllers in natural science. In the future, we will explore the sim2real experiments: training RL algorithms in the simulation environment then deploying them to the real-world control system. Another important topic is that optical systems (or other scienti\ufb01c control problems) typically provide us with much richer structural information. So it is promising to incorporate additional structural information behind the OPS into the RL controllers. Acknowledgement We thank Prof. Zhigang Zhang from Peking University for invaluable discussions about Optics part of the paper. References [1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. [2] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079, 2018. [3] Fangyi Zhang, J\u00fcrgen Leitner, Michael Milford, Ben Upcroft, and Peter Corke. Towards vision-based deep reinforcement learning for robotic motion control. arXiv preprint arXiv:1511.03791, 2015. [4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [5] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 387\u2013395, Bejing, China, 22\u201324 Jun 2014. PMLR. [6] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. [7] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017. [8] Oliver J Dressler, Philip D Howes, Jaebum Choo, and Andrew J deMello. Reinforcement learning for dynamic micro\ufb02uidic control. ACS omega, 3(8):10084\u201310091, 2018. [9] Jun Izawa, Toshiyuki Kondo, and Koji Ito. Biological arm motion through reinforcement learning. Biological cybernetics, 91(1):10\u201322, 2004. [10] Marin Bukov, Alexandre GR Day, Dries Sels, Phillip Weinberg, Anatoli Polkovnikov, and Pankaj Mehta. Reinforcement learning in different phases of quantum control. Physical Review X, 8(3):031086, 2018. [11] Go\u00ebry Genty, Lauri Salmela, John M Dudley, Daniel Brunner, Alexey Kokhanovskiy, Sergei Kobtsev, and Sergei K Turitsyn. Machine learning and applications in ultrafast photonics. Nature Photonics, pages 1\u201311, 2020. 3Under the zero observations, the system controlled by experts conventionally. 9 An Optics Controlling Environment [12] Thomas Baumeister, Steven L Brunton, and J Nathan Kutz. Deep learning and model predictive control for self-tuning mode-locked lasers. JOSA B, 35(3):617\u2013626, 2018. [13] Yuen-Ron Shen. The principles of nonlinear optics. New York, 1984. [14] Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Convergent reinforcement learning with nonlinear function approximation. In International Conference on Machine Learning, pages 1125\u20131134. PMLR, 2018. [15] Martin E Fermann and Ingmar Hartl. Ultrafast \ufb01bre lasers. Nature photonics, 7(11):868\u2013874, 2013. [16] Gert Cauwenberghs. A fast stochastic error-descent algorithm for supervised learning and optimization. Advances in neural information processing systems, 5:244\u2013251, 1993. [17] Pu Zhou, Zejin Liu, Xiaolin Wang, Yanxing Ma, Haotong Ma, Xiaojun Xu, and Shaofeng Guo. Coherent beam combining of \ufb01ber ampli\ufb01ers using stochastic parallel gradient descent algorithm and its application. IEEE Journal of Selected Topics in Quantum Electronics, 15(2):248\u2013256, 2009. [18] Abulikemu Abuduweili, Bowei Yang, and Zhigang Zhang. Modi\ufb01ed stochastic gradient algorithms for controlling coherent pulse stacking. In Conference on Lasers and Electro-Optics, page STh4P.1, 2020. [19] Amir Dembo and Thomas Kailath. Model-free distributed learning. IEEE Transactions on Neural Networks, 1(1):58\u201370, 1990. [20] Henrik T\u00fcnnermann and Akira Shirakawa. Deep reinforcement learning for coherent beam combining applications. Opt. Express, 27(17):24223\u201324230, Aug 2019. [21] Chang Sun, Eurika Kaiser, Steven L Brunton, and J Nathan Kutz. Deep reinforcement learning for optical systems: A case study of mode-locked lasers. Machine Learning: Science and Technology, 1(4):045013, 2020. [22] Abulikemu Abuduweili, Bowei Yang, and Zhigang Zhang. Control of delay lines with reinforcement learning for coherent pulse stacking. In Conference on Lasers and Electro-Optics, page JW2F.33, 2020. [23] Abulikemu Abuduweili, Jie Wang, Bowei Yang, Aimin Wang, and Zhigang Zhang. Reinforcement learning based robust control algorithms for coherent pulse stacking. Opt. Express, 29(16):26068\u201326081, Aug 2021. [24] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [25] Carlo M Valensise, Alessandro Giuseppi, Giulio Cerullo, and Dario Polli. Deep reinforcement learning control of white-light continuum generation. Optica, 8(2):239\u2013242, 2021. [26] D Pomerleau. An autonomous land vehicle in a neural network. Advances in Neural Information Processing Systems; Morgan Kaufmann Publishers Inc.: Burlington, MA, USA, 1998. [27] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013279, 2013. [28] Gordon Wetzstein, Aydogan Ozcan, Sylvain Gigan, Shanhui Fan, Dirk Englund, Marin Solja\u02c7ci\u00b4c, Cornelia Denz, David AB Miller, and Demetri Psaltis. Inference in arti\ufb01cial intelligence with deep optics and photonics. Nature, 588(7836):39\u201347, 2020. [29] C Dorrer, DC Kilper, HR Stuart, G Raybon, and MG Raymer. Linear optical sampling. IEEE Photonics Technology Letters, 15(12):1746\u20131748, 2003. [30] Henrik T\u00fcnnermann and Akira Shirakawa. Delay line coherent pulse stacking. Opt. Lett., 42(23):4829\u20134832, Dec 2017. [31] Henning Stark, Michael M\u00fcller, Marco Kienel, Arno Klenke, Jens Limpert, and Andreas T\u00fcnnermann. Electrooptically controlled divided-pulse ampli\ufb01cation. Optics express, 25(12):13494\u201313503, 2017. [32] Ignas Astrauskas, Edgar Kaksis, Tobias Fl\u00f6ry, Giedrius Andriukaitis, Audrius Pug\u017elys, Andrius Baltu\u0161ka, John Ruppe, Siyun Chen, Almantas Galvanauskas, and Tadas Bal\u02c7ci\u00afunas. High-energy pulse stacking via regenerative pulse-burst ampli\ufb01cation. Optics letters, 42(11):2201\u20132204, 2017. [33] Bowei Yang, Guanyu Liu, Abuduweili Abulikemu, Yan Wang, Aimin Wang, and Zhigang Zhang. Coherent stacking of 128 pulses from a ghz repetition rate femtosecond yb:\ufb01ber laser. In Conference on Lasers and Electro-Optics, page JW2F.28, 2020. [34] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1587\u20131596. PMLR, 2018. 10 An Optics Controlling Environment [35] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1861\u20131870. PMLR, 10\u201315 Jul 2018. [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [37] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. [38] Yoanna M Ivanova, Hannah Pallubinsky, Rick Kramer, and Wouter van Marken Lichtenbelt. The in\ufb02uence of a moderate temperature drift on thermal physiology and perception. Physiology & Behavior, 229:113257, 2021. [39] Johan Hult. A fourth-order runge\u2013kutta in the interaction picture method for simulating supercontinuum generation in optical \ufb01bers. J. Lightwave Technol., 25(12):3770\u20133775, Dec 2007. [40] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [41] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. [42] Antonin Raf\ufb01n, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019. [43] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. [44] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1\u201376, 2021. [45] Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably ef\ufb01cient q-learning with function approximation via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321, 2019. [46] Yudong Chen and Yuejie Chi. Harnessing structures in big data via guaranteed low-rank matrix estimation: Recent theory and fast algorithms via convex and nonconvex optimization. IEEE Signal Processing Magazine, 35(4):14\u201331, 2018. [47] Yuejie Chi, Yuxin Chen, and M. Yue Lu. Recent advances in nonconvex methods for high-dimensional estimation. ICASSP tutorial, 2018. [48] Sobhan Miryoose\ufb01, Kiant\u00e9 Brantley, Hal Daum\u00e9 III, Miroslav Dud\u00edk, and Robert Schapire. Reinforcement learning with convex constraints. arXiv preprint arXiv:1906.09323, 2019. [49] Qiang Du, Tong Zhou, Lawrence R Doolittle, Gang Huang, Derun Li, and Russell Wilcox. Deterministic stabilization of eight-way 2d diffractive beam combining using pattern recognition. Optics letters, 44(18):4554\u2013 4557, 2019. 11 An Optics Controlling Environment A Additional Information of Optical Pulse Stacking A.1 System con\ufb01guration Initial pulses \ud835\udc38\" \ud835\udc38# \ud835\udc38$ \ud835\udc38% \ud835\udc38\",# \ud835\udc38$,% \ud835\udc38\",#,$,% 1st stage 2nd stage Photo detection 1st stacked pulses 2nd stacked pulses time delay \ud835\udf0f\" time delay \ud835\udf0f# Laser RL Controller final stacked pulses feedback mirror Figure 12: Con\ufb01guration of optical pulse stacking (OPS) system. Only a 2-stage OPS system was plotted for simplicity. The con\ufb01guration of the optical pulse stacking (OPS) system is shown in \ufb01g. 12. The source laser delivers a train of periodic optical pulses. Given the base wave function of the laser pulse E(t) and period T, each laser pulse in \ufb01g. 12 can be described as: E1 = E(t), E2 = E(t + T), E3 = E(t + 2T), E4 = E(t + 3T). (6) Then the laser pulses were sent to the n-stage OPS system. In each OPS time delay stacking module, a time delay should be given to former pulses for every two consecutive pulse pairs. The demonstration of each OPS time delay stacking module is shown in \ufb01g. 13. Figure 13(a) shows the initial state of the two pulses before processing by this stage OPS time delay. Figure 13(b) \u223c \ufb01g. 13(e) show the (chronological) procedures of stacking two pulses by imposing additional time delay. The former pulse E1 was refracted by the splitter to undergo additional delay lines (vertical path between \"mirror\" and \"time delay controller and mirror\" in \ufb01g. 13(a)). The latter pulse directly transmitted the splitter. If the displacement of the additional delay line is d1, then the additional time delay imposed to E1 is \u03c41 = d1/c, where c is the light speed. Thus, in experimental implementation, the time delay of the pulse was imposed by additional delay line displacement. The value of the delay line displacement is controlled by the RL controller. In OPS system, the 1st stage time-delay controller imposes the time delay \u03c41 on pulse E1 to stack with pulse E2 to create the stacked pulses E1,2 = E(t + \u03c41) + E(t + T). Similarly, After imposing time delay, pulse E3 could be stacked with pulse E4 to create E3,4 = E(t + 2T + \u03c41) + E(t + 3T). In the 2nd stage OPS, the time delay \u03c42 was further imposed to E1,2 to make it stacking up with E3,4 to create E1,2,3,4: E1,2,3,4 = E(t + \u03c41 + \u03c42) + E(t + T + \u03c42) + E(t + 2T + \u03c41) + E(t + 3T). (7) If noise was ignored, when \u03c41 = T, \u03c42 = 2T, E1,2,3,4 achieved the maximum value 4E(t + 3T). Furtherly, for N-stage OPS system, 1st, 2nd, . .. , N-th time delay \u03c41, \u03c42, ..., \u03c4N matches to 20, 21, \u00b7 \u00b7 \u00b7 , 2N\u22121 times of the pulse period T, the 2N pulses will be perfectly stacked, and the power of the output pulse reaches the global maximum. In practice, noise can not be ignored, so the exact value of time delay \u03c41, \u03c42, ..., \u03c4N could be adjusted according to the feedback. A.2 Real physical system The real physical OPS system is shown in \ufb01g. 14. RL algorithm computes the value of each time delay \u03c41, \u03c42, ..., \u03c4N and sends the values to each stage delay line controller. (RL controller connected with the electric signal line of 1st, 2nd, 3rd delay line located at the bottom of the \ufb01g. 14.) Real-world OPS control experiments are quite costly and slow. 12 ",
    "Experiments": "Experiments B.1 Experimental setting We evaluated the performance of PPO, TD3, and SAC in our OPS environment. For each of PPO, TD3, and SAC, we performed hyperparameter search to achieve better performance. For the search, we trained on 5-stage OPS environment with medium dif\ufb01culty. Each of the hyperparameter sets was repeated with 3 random seeds. For each algorithm, the best hyperparameter set was decided based on the \ufb01nal performance in the testing environment. After the search, each of the best hyperparameter sets was used to run experiments with 10 different random seeds on all scenarios. The 13 An Optics Controlling Environment hyperparameter range and selected value of TD3 can be found in table 2, hyperparameter range and selected value of SAC can be found in table 3, and hyperparameter range and selected value of PPO can be found in table 4. Hyperparameter Range Best-selected Size of the replay buffer {1000,10000,100000} 10000 Step of collect transition before training {100, 1000, 10000} 1000 Unroll Length/n-step {1,10, 100} 100 Training epochs per update {1,10, 100} 100 Discount factor (\u03b3) {0.98, 0.99, 0.999} 0.98 Noise type {\u2019normal\u2019, \u2019ornstein-uhlenbeck\u2019, None} \u2019normal\u2019 Noise standard value {0.1, 0.3, 0.5, 0.7, 0.9} 0.7 Learning rate {0.0001, 0.0003, 0.001,0.003,0.01} 0.001 Policy network hidden layer {1, 2, 3} 2 Policy network hidden dimension {64, 128, 256} 256 Optimizer Adam Adam Table 2: TD3: ranges used during the hyperparameter search and the \ufb01nal selected values. Hyperparameter Range Best-selected Size of the replay buffer {1000,10000,100000} 10000 Step of collect transition before training {100, 1000, 10000} 1000 Unroll Length/n-step {1,10, 100} 1 Training epochs per update {1,10, 100} 1 Discount factor (\u03b3) {0.98, 0.99, 0.999} 0.98 Generalized State Dependent Exploration (gSDE) {True, False} True Soft update coef\ufb01cient for \"Polyak update\" (\u03c4) {0.002,0.005, 0.01, 0.02} 0.005 Learning rate {0.0001, 0.0003, 0.001,0.003,0.01} 0.001 Policy network hidden layer {1, 2, 3} 2 Policy network hidden dimension {64, 128, 256} 256 Optimizer Adam Adam Table 3: SAC: ranges used during the hyperparameter search and the \ufb01nal selected values. Hyperparameter Range Best-selected Unroll Length/n-step {128,256,512,1024,2048} 1024 Training epochs per update {1,5,10} 10 Clipping range {0.1,0.2,0.4} 0.2 Discount factor (\u03b3) {0.98, 0.99, 0.999} 0.98 Entropy Coef\ufb01cient {0, 0.001, 0.01, 0.1} 0.01 GAE (\u03bb) {0.90, 0.95, 0.98, 0.99} 0.95 Value function coef\ufb01cient {0.1,0.3,0.5,0.7,0.9} 0.5 Learning rate {0.0001, 0.0003, 0.001,0.003,0.01} 0.001 Gradient norm clipping {0.1, 0.5, 1.0, 5.0} 0.5 Policy network hidden layer {1, 2, 3} 2 Policy network hidden dimension {64, 128, 256} 256 Optimizer Adam Adam Table 4: PPO: ranges used during the hyperparameter search and the \ufb01nal selected values. B.2 Results on controlling OPS environment We reported the training curve (training reward w.r.t. iterations) and testing curve (return (stacked pulse power PN ) w.r.t. testing iterations) on the 4-stage OPS environment in \ufb01g. 15, and on the 6-stage OPS environment in \ufb01g. 16. As can be seen, the performance of TD3 and SAC is higher than PPO. Compared with \ufb01g. 15 (4-stage), and \ufb01g. 7 (5-stage) to \ufb01g. 16 (6-stage), with the increase of stage number, the training convergence became slower, and the \ufb01nal return PN became smaller, especially for medium mode and hard mode dif\ufb01culty. 14 ",
    "Discussion": "",
    "Conclusion": "Conclusion In this paper, we introduce OPS \u2013 an open-sourced simulator for controlling the pulse stacking system. To the best of our knowledge, this is the \ufb01rst open-sourced RL environment for optics control problems. Then we evaluated the SAC, TD3, and PPO on our proposed simulation environment. By providing an optical control simulation environment and RL benchmarks, we aim to encourage exploration of the application of RL on optical control tasks and furtherly explore the RL controllers in natural science. In the future, we will explore the sim2real experiments: training RL algorithms in the simulation environment then deploying them to the real-world control system. Another important topic is that optical systems (or other scienti\ufb01c control problems) typically provide us with much richer structural information. So it is promising to incorporate additional structural information behind the OPS into the RL controllers. Acknowledgement We thank Prof. Zhigang Zhang from Peking University for invaluable discussions about Optics part of the paper. ",
    "References": "References [1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. [2] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079, 2018. [3] Fangyi Zhang, J\u00fcrgen Leitner, Michael Milford, Ben Upcroft, and Peter Corke. Towards vision-based deep reinforcement learning for robotic motion control. arXiv preprint arXiv:1511.03791, 2015. [4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [5] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 387\u2013395, Bejing, China, 22\u201324 Jun 2014. PMLR. [6] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. [7] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017. [8] Oliver J Dressler, Philip D Howes, Jaebum Choo, and Andrew J deMello. Reinforcement learning for dynamic micro\ufb02uidic control. ACS omega, 3(8):10084\u201310091, 2018. [9] Jun Izawa, Toshiyuki Kondo, and Koji Ito. Biological arm motion through reinforcement learning. Biological cybernetics, 91(1):10\u201322, 2004. [10] Marin Bukov, Alexandre GR Day, Dries Sels, Phillip Weinberg, Anatoli Polkovnikov, and Pankaj Mehta. Reinforcement learning in different phases of quantum control. Physical Review X, 8(3):031086, 2018. [11] Go\u00ebry Genty, Lauri Salmela, John M Dudley, Daniel Brunner, Alexey Kokhanovskiy, Sergei Kobtsev, and Sergei K Turitsyn. Machine learning and applications in ultrafast photonics. Nature Photonics, pages 1\u201311, 2020. 3Under the zero observations, the system controlled by experts conventionally. 9 An Optics Controlling Environment [12] Thomas Baumeister, Steven L Brunton, and J Nathan Kutz. Deep learning and model predictive control for self-tuning mode-locked lasers. JOSA B, 35(3):617\u2013626, 2018. [13] Yuen-Ron Shen. The principles of nonlinear optics. New York, 1984. [14] Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed: Convergent reinforcement learning with nonlinear function approximation. In International Conference on Machine Learning, pages 1125\u20131134. PMLR, 2018. [15] Martin E Fermann and Ingmar Hartl. Ultrafast \ufb01bre lasers. Nature photonics, 7(11):868\u2013874, 2013. [16] Gert Cauwenberghs. A fast stochastic error-descent algorithm for supervised learning and optimization. Advances in neural information processing systems, 5:244\u2013251, 1993. [17] Pu Zhou, Zejin Liu, Xiaolin Wang, Yanxing Ma, Haotong Ma, Xiaojun Xu, and Shaofeng Guo. Coherent beam combining of \ufb01ber ampli\ufb01ers using stochastic parallel gradient descent algorithm and its application. IEEE Journal of Selected Topics in Quantum Electronics, 15(2):248\u2013256, 2009. [18] Abulikemu Abuduweili, Bowei Yang, and Zhigang Zhang. Modi\ufb01ed stochastic gradient algorithms for controlling coherent pulse stacking. In Conference on Lasers and Electro-Optics, page STh4P.1, 2020. [19] Amir Dembo and Thomas Kailath. Model-free distributed learning. IEEE Transactions on Neural Networks, 1(1):58\u201370, 1990. [20] Henrik T\u00fcnnermann and Akira Shirakawa. Deep reinforcement learning for coherent beam combining applications. Opt. Express, 27(17):24223\u201324230, Aug 2019. [21] Chang Sun, Eurika Kaiser, Steven L Brunton, and J Nathan Kutz. Deep reinforcement learning for optical systems: A case study of mode-locked lasers. Machine Learning: Science and Technology, 1(4):045013, 2020. [22] Abulikemu Abuduweili, Bowei Yang, and Zhigang Zhang. Control of delay lines with reinforcement learning for coherent pulse stacking. In Conference on Lasers and Electro-Optics, page JW2F.33, 2020. [23] Abulikemu Abuduweili, Jie Wang, Bowei Yang, Aimin Wang, and Zhigang Zhang. Reinforcement learning based robust control algorithms for coherent pulse stacking. Opt. Express, 29(16):26068\u201326081, Aug 2021. [24] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [25] Carlo M Valensise, Alessandro Giuseppi, Giulio Cerullo, and Dario Polli. Deep reinforcement learning control of white-light continuum generation. Optica, 8(2):239\u2013242, 2021. [26] D Pomerleau. An autonomous land vehicle in a neural network. Advances in Neural Information Processing Systems; Morgan Kaufmann Publishers Inc.: Burlington, MA, USA, 1998. [27] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013279, 2013. [28] Gordon Wetzstein, Aydogan Ozcan, Sylvain Gigan, Shanhui Fan, Dirk Englund, Marin Solja\u02c7ci\u00b4c, Cornelia Denz, David AB Miller, and Demetri Psaltis. Inference in arti\ufb01cial intelligence with deep optics and photonics. Nature, 588(7836):39\u201347, 2020. [29] C Dorrer, DC Kilper, HR Stuart, G Raybon, and MG Raymer. Linear optical sampling. IEEE Photonics Technology Letters, 15(12):1746\u20131748, 2003. [30] Henrik T\u00fcnnermann and Akira Shirakawa. Delay line coherent pulse stacking. Opt. Lett., 42(23):4829\u20134832, Dec 2017. [31] Henning Stark, Michael M\u00fcller, Marco Kienel, Arno Klenke, Jens Limpert, and Andreas T\u00fcnnermann. Electrooptically controlled divided-pulse ampli\ufb01cation. Optics express, 25(12):13494\u201313503, 2017. [32] Ignas Astrauskas, Edgar Kaksis, Tobias Fl\u00f6ry, Giedrius Andriukaitis, Audrius Pug\u017elys, Andrius Baltu\u0161ka, John Ruppe, Siyun Chen, Almantas Galvanauskas, and Tadas Bal\u02c7ci\u00afunas. High-energy pulse stacking via regenerative pulse-burst ampli\ufb01cation. Optics letters, 42(11):2201\u20132204, 2017. [33] Bowei Yang, Guanyu Liu, Abuduweili Abulikemu, Yan Wang, Aimin Wang, and Zhigang Zhang. Coherent stacking of 128 pulses from a ghz repetition rate femtosecond yb:\ufb01ber laser. In Conference on Lasers and Electro-Optics, page JW2F.28, 2020. [34] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1587\u20131596. PMLR, 2018. 10 An Optics Controlling Environment [35] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1861\u20131870. PMLR, 10\u201315 Jul 2018. [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [37] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. [38] Yoanna M Ivanova, Hannah Pallubinsky, Rick Kramer, and Wouter van Marken Lichtenbelt. The in\ufb02uence of a moderate temperature drift on thermal physiology and perception. Physiology & Behavior, 229:113257, 2021. [39] Johan Hult. A fourth-order runge\u2013kutta in the interaction picture method for simulating supercontinuum generation in optical \ufb01bers. J. Lightwave Technol., 25(12):3770\u20133775, Dec 2007. [40] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [41] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. [42] Antonin Raf\ufb01n, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019. [43] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. [44] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1\u201376, 2021. [45] Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably ef\ufb01cient q-learning with function approximation via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321, 2019. [46] Yudong Chen and Yuejie Chi. Harnessing structures in big data via guaranteed low-rank matrix estimation: Recent theory and fast algorithms via convex and nonconvex optimization. IEEE Signal Processing Magazine, 35(4):14\u201331, 2018. [47] Yuejie Chi, Yuxin Chen, and M. Yue Lu. Recent advances in nonconvex methods for high-dimensional estimation. ICASSP tutorial, 2018. [48] Sobhan Miryoose\ufb01, Kiant\u00e9 Brantley, Hal Daum\u00e9 III, Miroslav Dud\u00edk, and Robert Schapire. Reinforcement learning with convex constraints. arXiv preprint arXiv:1906.09323, 2019. [49] Qiang Du, Tong Zhou, Lawrence R Doolittle, Gang Huang, Derun Li, and Russell Wilcox. Deterministic stabilization of eight-way 2d diffractive beam combining using pattern recognition. Optics letters, 44(18):4554\u2013 4557, 2019. 11 An Optics Controlling Environment A Additional Information of Optical Pulse Stacking A.1 System con\ufb01guration Initial pulses \ud835\udc38\" \ud835\udc38# \ud835\udc38$ \ud835\udc38% \ud835\udc38\",# \ud835\udc38$,% \ud835\udc38\",#,$,% 1st stage 2nd stage Photo detection 1st stacked pulses 2nd stacked pulses time delay \ud835\udf0f\" time delay \ud835\udf0f# Laser RL Controller final stacked pulses feedback mirror Figure 12: Con\ufb01guration of optical pulse stacking (OPS) system. Only a 2-stage OPS system was plotted for simplicity. The con\ufb01guration of the optical pulse stacking (OPS) system is shown in \ufb01g. 12. The source laser delivers a train of periodic optical pulses. Given the base wave function of the laser pulse E(t) and period T, each laser pulse in \ufb01g. 12 can be described as: E1 = E(t), E2 = E(t + T), E3 = E(t + 2T), E4 = E(t + 3T). (6) Then the laser pulses were sent to the n-stage OPS system. In each OPS time delay stacking module, a time delay should be given to former pulses for every two consecutive pulse pairs. The demonstration of each OPS time delay stacking module is shown in \ufb01g. 13. Figure 13(a) shows the initial state of the two pulses before processing by this stage OPS time delay. Figure 13(b) \u223c \ufb01g. 13(e) show the (chronological) procedures of stacking two pulses by imposing additional time delay. The former pulse E1 was refracted by the splitter to undergo additional delay lines (vertical path between \"mirror\" and \"time delay controller and mirror\" in \ufb01g. 13(a)). The latter pulse directly transmitted the splitter. If the displacement of the additional delay line is d1, then the additional time delay imposed to E1 is \u03c41 = d1/c, where c is the light speed. Thus, in experimental implementation, the time delay of the pulse was imposed by additional delay line displacement. The value of the delay line displacement is controlled by the RL controller. In OPS system, the 1st stage time-delay controller imposes the time delay \u03c41 on pulse E1 to stack with pulse E2 to create the stacked pulses E1,2 = E(t + \u03c41) + E(t + T). Similarly, After imposing time delay, pulse E3 could be stacked with pulse E4 to create E3,4 = E(t + 2T + \u03c41) + E(t + 3T). In the 2nd stage OPS, the time delay \u03c42 was further imposed to E1,2 to make it stacking up with E3,4 to create E1,2,3,4: E1,2,3,4 = E(t + \u03c41 + \u03c42) + E(t + T + \u03c42) + E(t + 2T + \u03c41) + E(t + 3T). (7) If noise was ignored, when \u03c41 = T, \u03c42 = 2T, E1,2,3,4 achieved the maximum value 4E(t + 3T). Furtherly, for N-stage OPS system, 1st, 2nd, . .. , N-th time delay \u03c41, \u03c42, ..., \u03c4N matches to 20, 21, \u00b7 \u00b7 \u00b7 , 2N\u22121 times of the pulse period T, the 2N pulses will be perfectly stacked, and the power of the output pulse reaches the global maximum. In practice, noise can not be ignored, so the exact value of time delay \u03c41, \u03c42, ..., \u03c4N could be adjusted according to the feedback. A.2 Real physical system The real physical OPS system is shown in \ufb01g. 14. RL algorithm computes the value of each time delay \u03c41, \u03c42, ..., \u03c4N and sends the values to each stage delay line controller. (RL controller connected with the electric signal line of 1st, 2nd, 3rd delay line located at the bottom of the \ufb01g. 14.) Real-world OPS control experiments are quite costly and slow. 12 An Optics Controlling Environment mirror time delay controller and mirror splitter \ud835\udc38\" \ud835\udc38# (a) initial step \ud835\udc38\" \ud835\udc38# (b) step 1 \ud835\udc38\" \ud835\udc38# (c) step 2 \ud835\udc38\" \ud835\udc38# (d) step 3 \ud835\udc38\",$ (e) step 4 Figure 13: Demonstration of stacking two pulses with additional time delay. (a) shows the initial state of the 2 pulses before stacking. (b)-(e) show the (chronological) procedure of the stacking 2 pulses with imposing additional time delay. The former (latter) pulse was plotted with purple (red). The solid (transparent) plotted pulse shows the pulse position at the current (last) step. The arrow denotes the shifting value of a pulse. Figure 14: Real world optical pulse stacking system. B Additional Experiments B.1 Experimental setting We evaluated the performance of PPO, TD3, and SAC in our OPS environment. For each of PPO, TD3, and SAC, we performed hyperparameter search to achieve better performance. For the search, we trained on 5-stage OPS environment with medium dif\ufb01culty. Each of the hyperparameter sets was repeated with 3 random seeds. For each algorithm, the best hyperparameter set was decided based on the \ufb01nal performance in the testing environment. After the search, each of the best hyperparameter sets was used to run experiments with 10 different random seeds on all scenarios. The 13 An Optics Controlling Environment hyperparameter range and selected value of TD3 can be found in table 2, hyperparameter range and selected value of SAC can be found in table 3, and hyperparameter range and selected value of PPO can be found in table 4. Hyperparameter Range Best-selected Size of the replay buffer {1000,10000,100000} 10000 Step of collect transition before training {100, 1000, 10000} 1000 Unroll Length/n-step {1,10, 100} 100 Training epochs per update {1,10, 100} 100 Discount factor (\u03b3) {0.98, 0.99, 0.999} 0.98 Noise type {\u2019normal\u2019, \u2019ornstein-uhlenbeck\u2019, None} \u2019normal\u2019 Noise standard value {0.1, 0.3, 0.5, 0.7, 0.9} 0.7 Learning rate {0.0001, 0.0003, 0.001,0.003,0.01} 0.001 Policy network hidden layer {1, 2, 3} 2 Policy network hidden dimension {64, 128, 256} 256 Optimizer Adam Adam Table 2: TD3: ranges used during the hyperparameter search and the \ufb01nal selected values. Hyperparameter Range Best-selected Size of the replay buffer {1000,10000,100000} 10000 Step of collect transition before training {100, 1000, 10000} 1000 Unroll Length/n-step {1,10, 100} 1 Training epochs per update {1,10, 100} 1 Discount factor (\u03b3) {0.98, 0.99, 0.999} 0.98 Generalized State Dependent Exploration (gSDE) {True, False} True Soft update coef\ufb01cient for \"Polyak update\" (\u03c4) {0.002,0.005, 0.01, 0.02} 0.005 Learning rate {0.0001, 0.0003, 0.001,0.003,0.01} 0.001 Policy network hidden layer {1, 2, 3} 2 Policy network hidden dimension {64, 128, 256} 256 Optimizer Adam Adam Table 3: SAC: ranges used during the hyperparameter search and the \ufb01nal selected values. Hyperparameter Range Best-selected Unroll Length/n-step {128,256,512,1024,2048} 1024 Training epochs per update {1,5,10} 10 Clipping range {0.1,0.2,0.4} 0.2 Discount factor (\u03b3) {0.98, 0.99, 0.999} 0.98 Entropy Coef\ufb01cient {0, 0.001, 0.01, 0.1} 0.01 GAE (\u03bb) {0.90, 0.95, 0.98, 0.99} 0.95 Value function coef\ufb01cient {0.1,0.3,0.5,0.7,0.9} 0.5 Learning rate {0.0001, 0.0003, 0.001,0.003,0.01} 0.001 Gradient norm clipping {0.1, 0.5, 1.0, 5.0} 0.5 Policy network hidden layer {1, 2, 3} 2 Policy network hidden dimension {64, 128, 256} 256 Optimizer Adam Adam Table 4: PPO: ranges used during the hyperparameter search and the \ufb01nal selected values. B.2 Results on controlling OPS environment We reported the training curve (training reward w.r.t. iterations) and testing curve (return (stacked pulse power PN ) w.r.t. testing iterations) on the 4-stage OPS environment in \ufb01g. 15, and on the 6-stage OPS environment in \ufb01g. 16. As can be seen, the performance of TD3 and SAC is higher than PPO. Compared with \ufb01g. 15 (4-stage), and \ufb01g. 7 (5-stage) to \ufb01g. 16 (6-stage), with the increase of stage number, the training convergence became slower, and the \ufb01nal return PN became smaller, especially for medium mode and hard mode dif\ufb01culty. 14 An Optics Controlling Environment 0 50000 100000 150000 200000 250000 300000 Iterations 1.0 0.8 0.6 0.4 0.2 0.0 Training Reward (per iteration) TD3 SAC PPO (a) Easy mode: train curve 0 50000 100000 150000 200000 250000 300000 Iterations 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Training Reward (per iteration) TD3 SAC PPO (b) Medium mode: train curve 0 50000 100000 150000 200000 250000 300000 Iterations 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Training Reward (per iteration) TD3 SAC PPO (c) Hard mode: train curve 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.2 0.4 0.6 0.8 1.0 Final Stacked Pulse Energy  TD3 SAC PPO (d) Easy mode: testing curve 0 25 50 75 100 125 150 175 200 Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Final Stacked Pulse Energy  TD3 SAC PPO (e) Medium mode: testing curve 0 25 50 75 100 125 150 175 200 Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Final Stacked Pulse Energy  TD3 SAC PPO (f) Hard mode: testing curve Figure 15: 4-stage OPS experiments. Training reward was plotted for (a) easy mode, (b) medium mode, and (c) hard mode. Evaluation of the stacked pulse power P4 (normalized) of testing environment was plotted for (d) easy mode, (e) medium mode, and (f) hard mode. B.3 Demonstration of the controlling OPS environment Figure 17 shows the pulse trains on a 5-stage hard mode OPS system controlled by TD3 from the random initial state. It is seen that TD3 algorithm could achieve (local) maximum power within 40 iterations. C Discussion C.1 Real-world environment and simulation environment Deploying the RL algorithm in the real-world optics system requires converting optical signal to electrical analog signal using photo-detector (PD), then converting the analog signal to digital signal using an analog-to-digital converter (ADC). These two conversions cost some additional time to process the signal and cause feedback delay. At a conservative estimate, the regular PD and ADC processing takes 0.01s per step. Then it would be possible to implement deep reinforcement algorithms on FPGAs to create control output by the feed of digital observation signal. In a proper implementation, FPGA computing time would be less than 0.01s per step. Including signal converting, neural-network inference time, and time-delay of the optical-mirror driver (controller), the time cost per control step is in the magnitude of 0.1s 4. But in our simulation system, we could speed up the control step by at least 10 times (with GPUs). More importantly, for RL training on real-world OPS systems, it needs to manually tune the optical devices when the optical beams are totally misaligned caused by the exploring process of RL. The initial alignment of the complex OPS system is usually tuned by experts to take several hours even several days, that the time-cost is depending on the system complexity5. But in our simulation system, the initial alignment could be done by simply \"reset\" the environment. So the value of the simulation environment can be summarized as: \u2022 Faster control process than a real-world experiment. \u2022 Easy to \"reset\" (and initial align) the environment, while it takes a lot of works to reset or initial align a real-world experiment. 4With expensive high-speed PD, AD/DA cards, and optical mirror driver, as well as ef\ufb01cient FPGA implementation, the control-speed time would be reduced to 0.01s. But it will increase the budget of the devices. 5We cannot detect any stacking signal when the optical beams are totally misaligned. So the RL algorithms would fail. It needs to align manually in this scenario. 15 ",
    "Evaluation": "Evaluation of the stacked pulse power P6 (normalized) of testing environment was plotted for (d) easy mode, (e) medium mode, and (f) hard mode. \u2022 It is safer and cheaper. In real-world experiments, it has potential risk when the optical beams are totally misaligned, because refracted light is non-predictable and may shed on experimenters. C.2 Real-world Evaluation The impact of the simulation must be valued by the real measurement. Part of the correctness of our simulation has been evaluated by the simpli\ufb01ed beam combining experiments [20, 33]. Speci\ufb01cally, [20] implemented an simple real experiment and the same simulation, the authors found the simulation is valuable. Our simulation and experimental settings are complicated than [20], but the physics behind them is the same. Actually, if we set stage number =1, our simulation is almost the same as [20]. We will do detailed real experiments and justi\ufb01cation in the near future. C.3 Potential Impact and additional Related works Machine learning community. High-dimensional real-world reinforcement learning problems are extremely challenging[43]. In our simulation environment, if we choose a quite large N-stage number with hard mode, controlling the environment could become high-dimensional and dif\ufb01cult. Few recent works studied the distribution shift in RL[44, 45]. In the hard mode of the OPS environment, the noise distribution of the testing environment is different from the noise distribution of the training environment. Therefore our simulation environment is bene\ufb01cial to solve the hard and realistic reinforcement learning problems. In recent years, statistical procedures have been developed to promote low-dimensional structures using convex relaxations, rather than directly solving the nonconvex problems [46, 47]. As shown in \ufb01g. 2, we know the function of the OPS objective (if ignoring noise). The function typically provides us with much richer structural information and physical constraints. So it is possible to explore the additional information about the function of the OPS and incorporating it with RL algorithms. In many of the real-world cases, we are not interested in \u201dgeneric\u201d nonconvex problems, but rather, we focus on more speci\ufb01c nonconvex control with physical constrain or some known objective function [48]. Exploring the nonconvex and periodic objective of OPS would bene\ufb01t the real-world RL problems that including some structural information. Optics community. High pulse energy lasers can be used in laser accelerators, large-scale material processing, and medicine [15]. Optical (coherent) pulse stacking is one of the easiest and promising ways to scale the pulse energy [30]. However, the conventional control algorithms for OPS are not very effective [49]. RL methods are able to control this kind of multi-dimensional and nonlinear environment. Similar to our OPS control system, all of the optical control problems are affected by the nonlinearity and periodicity of the light inference (as shown in \ufb01g. 2), including 16 An Optics Controlling Environment 300 200 100 0 100 200 300 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) iteration=0 (a) Initial state 300 200 100 0 100 200 300 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) iteration=10 (b) After 10 iterations 300 200 100 0 100 200 300 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) iteration=20 (c) After 20 iterations 300 200 100 0 100 200 300 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) iteration=30 (d) After 30 iterations 300 200 100 0 100 200 300 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) iteration=40 (e) After 40 iterations 300 200 100 0 100 200 300 Time 0.0 0.2 0.4 0.6 0.8 1.0 Intensity (normalized) iteration=50 (f) After 50 iterations Figure 17: Demonstration of the controlling 5-stage OPS hard mode testing environment by TD3 algorithm after training. (a): initial state of pulses; (b) pulse state after 10 control iterations; (c) pulse state after 20 control iterations; (d) pulse state after 30 control iterations; (e) pulse state after 40 control iterations; (e) pulse state after 50 control iterations. coherent optical inference [28] and linear optical sampling [29], which can be used for precise measurement, industrial manufacturing, and scienti\ufb01c research. We believe our simulation is one of the important and typical optical control environments. Beyond OPS, RL methods have the potential to drive the next generation of optical laser technologies even the next generation of scienti\ufb01c control technologies[11]. This is because many phenomena in optics are nonlinear and multidimensional, with noise-sensitive dynamics that are extremely challenging to model using conventional methods. 17 ",
    "title": "",
    "paper_info": "AN OPTICAL CONTROLLING ENVIRONMENT\nAND REINFORCEMENT LEARNING BENCHMARKS\nAbulikemu Abuduweili\nRobotics Institute, Carnegie Mellon University\nChangliu Liu\nRobotics Institute, Carnegie Mellon University\nABSTRACT\nDeep reinforcement learning has the potential to address various scienti\ufb01c problems. In this paper,\nwe implement an optics simulation environment for reinforcement learning based controllers. The\nenvironment incorporates nonconvex and nonlinear optical phenomena as well as more realistic time-\ndependent noise. Then we provide the benchmark results of several state-of-the-art reinforcement\nlearning algorithms on the proposed simulation environment. In the end, we discuss the dif\ufb01culty of\ncontrolling the real-world optical environment with reinforcement learning algorithms.\nKeywords Optical Control \u00b7 Reinforcement Learning \u00b7 Simulation\n1\nIntroduction\nIn recent years, deep reinforcement learning (RL) has been used to solve challenging problems in various \ufb01elds [1],\nincluding self-driving car [2] and robot control [3]. Among all of the applications, deep RL made signi\ufb01cant progress in\nplay games on a superhuman level [4, 5, 6, 7]. Beyond playing games, deep RL has the potential to strongly impact the\ntraditional control and automation tasks in the natural science, such as control problems in chemistry [8], biology [9],\nquantum physics [10], optics and photonics [11].\nIn optics and photonics, there is particular potential for RL methods to drive the next generation of optical laser\ntechnologies [11]. This is not only because there is increasing demand for adaptive control and automation (of tuning\nand control) for optical systems [12], but also because many phenomena in optics are nonlinear and multidimensional\n[13], with noise-sensitive dynamics that are extremely challenging to model using conventional methods. RL methods\nare able to control multidimensional environment with nonlinear function approximation [14]. Thus, study the RL\ncontroller in optics becomes increasingly promising in optics and photonics as well as its applications in scienti\ufb01c\nresearch, medicine, and other industries [11, 15].\nTraditionally, many of the control problems in optics and photonics were implemented by stochastic parallel gradient\ndescent (SPGD) algorithm with PID controller [16, 17, 18]. The target is to maximize the reward (e.g. optical pulse\nenergy) by adjusting and controlling the system parameters. The SPGD algorithm is one of the special cases of\nstochastic error descent method [16, 19]. Stochastic error descent is based on the model-free distributed learning\nmechanism. A parameter update rule is proposed by which each individual parameter vector perturbation contributes to\na decrease in error (or increase in reward). However, SPGD is typically a convex optimization solver, and many control\nproblems in optics are non-convex. SPGD may be failed to search the global optimum of the optics control system\nunless the initial state of the system is near a global optimum. Conventionally, the initial state of the optical system\nwas adjusted by experienced experts, then utilizing SPGD to control the manually adjusted system, which becomes\nextremely hard with the increasing system complexity. In order to achieve ef\ufb01cient control and automation, deep RL\nwas introduced to control optical systems [20, 21, 22, 23]. Most of the previous works implemented Deep-Q Network\n[4] and Deep Deterministic Policy Gradient [24], in optical control systems to achieve the comparable performance\nwith traditional SPGD-PID control [20, 25]. But there is a lack of works on the evaluation of more RL algorithms in the\nmore complex optical control environment.\nStudying and validating RL algorithms in the real-world optical system is a challenging process because its cost is\nexpensive and requires experienced experts to implement the optical system. Instrumenting and operating RL algorithms\nin a simple optical system require signi\ufb01cant funds and manpower. An effective alternative to validate RL algorithms in\narXiv:2203.12114v1  [cs.LG]  23 Mar 2022\n",
    "GPTsummary": "- (1): The research background of this article is deep reinforcement learning applied to challenging scientific problems, with a particular focus on optics and photonics. \n\n- (2): Traditional control problems in optics and photonics have been implemented using stochastic parallel gradient descent (SPGD) with PID controller, but this method is not suitable for non-convex control problems. Deep reinforcement learning has been introduced to control optical systems with comparable performance to traditional SPGD-PID control, but more evaluation of reinforcement learning algorithms is needed in the complex optical control environment. \n\n- (3): The research methodology proposed in this paper is the implementation of an optics simulation environment called OPS (Optical Pulse Stacking environment) for reinforcement learning based controllers, incorporating nonconvex and nonlinear optical phenomena, as well as realistic time-dependent noise. The authors use OPS to evaluate several state-of-the-art reinforcement learning algorithms, including twin delayed deep deterministic policy gradient (TD3), soft actor-critic (SAC), and proximal policy optimization (PPO). \n\n- (4): The task in this paper is optical pulse stacking, which involves recursively stacking up optical pulses to multiply the pulse energy for output stacked pulses. The performance of the reinforcement learning algorithms is reported and discussed, showing promise for the application of reinforcement learning in optics control tasks and promoting research in both machine learning and optics.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in the introduction of an open-sourced simulator OPS for optical control problems, which makes it possible to evaluate the application of reinforcement learning algorithms in these tasks. The research also promotes the exploration of the combination of machine learning and scientific control problems, which has potential in various natural science fields.\n\n- (2): Innovation point: The innovation point of this article is the creation of the OPS simulator for optical control problems, which allows for the evaluation of various reinforcement learning algorithms in these tasks. Performance: The performance of the reinforcement learning algorithms evaluated in OPS, including TD3, SAC, and PPO, shows promise in the application of machine learning in optics control. Workload: The workload of this article is moderate, with a focus on the implementation of the OPS simulator and the evaluation of reinforcement learning algorithms, but further exploration of sim2real experiments and the incorporation of structural information may require more workload in the future.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in the introduction of an open-sourced simulator OPS for optical control problems, which makes it possible to evaluate the application of reinforcement learning algorithms in these tasks. The research also promotes the exploration of the combination of machine learning and scientific control problems, which has potential in various natural science fields.\n\n- (2): Innovation point: The innovation point of this article is the creation of the OPS simulator for optical control problems, which allows for the evaluation of various reinforcement learning algorithms in these tasks. Performance: The performance of the reinforcement learning algorithms evaluated in OPS, including TD3, SAC, and PPO, shows promise in the application of machine learning in optics control. Workload: The workload of this article is moderate, with a focus on the implementation of the OPS simulator and the evaluation of reinforcement learning algorithms, but further exploration of sim2real experiments and the incorporation of structural information may require more workload in the future.\n\n\n"
}