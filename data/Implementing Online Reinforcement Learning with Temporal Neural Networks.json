{
    "title": "",
    "paper_info": "January 29, 2022 \nJ. E. Smith  \nPage 16 \n \n \nFigure 15. R-TNN column.  wmax = 3. The outputs zi indicate actions.  For the cart-pole problem, \nthere are only two actions that correspond to application of forces +F and -F. \n4. Initial Simulation Study \nTo illustrate the basic design methodology, a simple system is first simulated. Two TNNs each consisting \nof a single column cooperate to balance the pole.  The frontend C-TNN feeds the backend R-TNN, and \nthere is a feedback path from the R-TNN to the C-TNN through the environment. \nFor initial simulations only one state variable is used: the pole angle. The pole angle is restricted to fall \nwithin a range of \u00b1 12o, and this range is discretized into 16 equal intervals and encoded in a 3-hot code \n(Figure 7). Codings are binarized (Figure 7b).    \nAs a matter of methodology, the first set of simulations are directed at the R-TNN back-end rather than \nthe C-TNN clustering front-end. After the R-TNN has been studied and optimized, a subsequent set of \nsimulations focuses on the C-TNN. \nEpisodes \nA simulation run consists of multiple episodes, each beginning with a random initial pole angle:                \n-2.0 \u2264  \u2220  \u2264 +2.0 and initial displacement: d = 0. Synaptic weights (and therefore learning) are preserved \nbetween episodes.  Simulation of each episode proceeds as a series of steps until there is a failure (\u2220 falls \nout of range \u00b1 12o  or  d exceeds \u00b1 2.4 meters) or until 10,000 steps are reached successfully (i.e., 200 \nseconds of simulated time).    \nA pole failure yields negative reward (R(s) = -1). In the initial simulations, the track displacement d is not \nused as a state variable, so a track failure results in zero reward (R(s) = 0). Nevertheless, a track failure \nterminates an episode.  If a simulation reaches at least 500 steps (and for every 500 steps thereafter), a \nreward R(s) = +1 is generated. In all other cases R(s) = 0.  \nFor the dimensions and parameters used here, if there is a failure, it is because the cart hits the end of the \ntrack.  The common scenario is that the pole is kept within range for long periods of time, but there is a \nslow drift in cart displacement (Figure 16).  Eventually, the cart drifts to the end of the track.  Of course, \nthis behavior is to be expected because displacement is not used as a state variable. \n0\n  \n0\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\u03b8R =2\nW\nT\nA\n\u03b8R =2\n\u03b8R =2\n\u03b8R =2\n",
    "GPTsummary": "- (1): The research background of this article is online reinforcement learning with temporal neural networks, which encode information as precise spike timing relationships and consider online localized learning via spike timing dependency (STDP) to support continual, adaptive online learning.\n\n- (2): The past methods include TNNs that rely on compute-intensive back propagation learning methods and offline supervised classification benchmarks, which do not play to the strengths of TNNs with STDP learning. The approach proposed in this article is well motivated to efficiently implement online applications, especially those that employ online reinforcement learning.\n\n- (3): The research methodology proposed in this paper is a TNN architecture for implementing efficient online reinforcement learning, composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning employing biologically plausible neo-Hebbian three-factor learning rules.\n\n- (4): The proposed approach is studied via simulation with a prototype implementation of the cart-pole problem (balancing an inverted pendulum), achieving good performance that supports their goals of efficient online reinforcement learning.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in proposing a TNN architecture for efficient online reinforcement learning, which enables continual, adaptive online learning and encodes information as precise spike timing relationships via STDP. The proposed approach achieves good performance in simulation with a prototype implementation of the cart-pole problem.\n\n- (2): Innovation point: The proposed TNN architecture with a frontend for unsupervised clustering and a backend for reinforcement learning via neo-Hebbian three-factor learning rules is a novel approach for efficient online reinforcement learning. Performance: The simulation results demonstrate good performance in achieving the goal of efficient online reinforcement learning. Workload: While the proposed approach aims to reduce the workload by implementing efficient online learning, further studies are required to investigate the scalability and generalizability of this approach to more complex tasks.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing a TNN architecture for efficient online reinforcement learning, which enables continual, adaptive online learning and encodes information as precise spike timing relationships via STDP. The proposed approach achieves good performance in simulation with a prototype implementation of the cart-pole problem.\n\n- (2): Innovation point: The proposed TNN architecture with a frontend for unsupervised clustering and a backend for reinforcement learning via neo-Hebbian three-factor learning rules is a novel approach for efficient online reinforcement learning. Performance: The simulation results demonstrate good performance in achieving the goal of efficient online reinforcement learning. Workload: While the proposed approach aims to reduce the workload by implementing efficient online learning, further studies are required to investigate the scalability and generalizability of this approach to more complex tasks.\n\n\n"
}