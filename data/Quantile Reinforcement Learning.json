{
    "Abstract": "Abstract In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire. Keywords: Reinforcement learning, Quantile, Ordinal Decision Model, Two-Timescale Stochastic Approximation 1. ",
    "Introduction": "Introduction Markov decision process and reinforcement learning are powerful frameworks for building autonomous agents (physical or virtual), which are systems that make decisions without human supervision in order to perform a given task. Examples of such systems abound: expert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al., 2001), acrobatic helicopter \ufb02ight (Abbeel et al., 2010) or human-level video game player (Mnih et al., 2015). However, the standard framework assumes that rewards are numeric, scalar and additive and that policies are evaluated with the expectation criterion. In practice, it may happen that such numerical rewards are not available, for instance, when the agent interacts with a human who generally gives ordinal feedback (e.g., \u201cexcellent\u201d, \u201cgood\u201d, \u201cbad\u201d and so on). Besides, even when this numerical information is available, one may want to optimize a criterion di\ufb00erent than the expectation, for instance in one-shot decision-making. Several works considered the case where preferences are qualitative. Markov decision processes with ordinal reward have been investigated (Weng, 2012, 2011) and di\ufb00erent ordinal decision criteria have been proposed in that context. More generally, preferencebased reinforcement learning (Akrour et al., 2012; F\u00a8urnkranz et al., 2012; Busa-Fekete et al., 2013, 2014) has been proposed to tackle situations where the only available preferential information concerns pairwise comparisons of histories. c\u20dd 2016 H. Gilbert & P. Weng. arXiv:1611.00862v1  [cs.LG]  3 Nov 2016 ",
    "Related Work": "Related Work A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria di\ufb00erent to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered di\ufb00erent cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the su\ufb03ciency of working in a state space augmented with the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria that are variance-penalized versions of the standard ones. They formulated the obtained optimization problem as a non-linear program. Yu et al. (1998) optimized the probability that the total reward becomes higher than a certain threshold. Additionally, in the arti\ufb01cial intelligence community, Liu and Koenig (2005, 2006) also investigated the use of EU as a decision criterion in MDPs. To optimize it, they proposed a 1. Or 0.01%-quantile, depending on whether the problem is expressed in terms of costs or rewards. 2 ",
    "Background": "Background We provide in this section the background information necessary to present our algorithm to learn a policy optimal for the quantile criterion. 3 Gilbert Weng 3.1. Markov Decision Process Markov Decision Processes (MDPs) o\ufb00er a powerful formalism to model and solve sequential decision-making problems (Puterman, 1994). A \ufb01nite horizon MDP is formally de\ufb01ned as a tuple MT = (S, A, P, R, s0) where: \u2022 T is a \ufb01nite horizon, \u2022 S is a \ufb01nite set of states, \u2022 A is a \ufb01nite set of actions, \u2022 P : S \u00d7 A \u00d7 S \u2192 R is a transition function with P(s, a, s\u2032) being the probability of reaching state s\u2032 when action a is performed in state s, \u2022 R : S \u00d7 A \u2192 R is a bounded reward function and \u2022 s0 \u2208 S is an initial state. In this model, starting from initial state s0, an agent chooses at every time step t an action at to perform in her current state st, which she can observe. This action results in a new state st+1 \u2208 S according to probability distribution P(st, at, .), and a reward signal R(st, at), which penalizes or reinforces the choice of this action. We will call t-history ht a succession of t state-action pairs starting from state s0 (e.g., ht = (s0, a1, s1, . . . , st\u22121, at\u22121, st)). The action choices of the agent is guided by a policy. More formally, a policy \u03c0 at an horizon T is a sequence of T decision rules (\u03b4T , . . . , \u03b41). Decision rules prescribe which action the agent should perform at a given time step. They can be Markovian if they only depend on the current state. Besides, a decision rule is either deterministic if it always selects the same action in a given situation or randomized if it prescribes a probability distribution over possible actions. Consequently, a policy can be Markovian, deterministic or randomized according to the type of its decision rules. Lastly, a policy is stationary if it applies the same decision rule at every time step, i.e., \u03c0 = (\u03b4, \u03b4, . . .). Policies can be compared with respect to di\ufb00erent decision criteria. The usual criterion is the expected (discounted) sum of rewards, for which an optimal deterministic Markovian policy is known to exist for any horizon T. This criterion is de\ufb01ned as follows. First, the value of a history ht = (s0, a1, s1, . . . , st\u22121, at, st) is described as the (possibly discounted) sum of rewards obtained along it, i.e., r(ht) = t \ufffd i=1 \u03b3i\u22121R(si\u22121, ai) where \u03b3 \u2208 [0, 1] is a discount factor. Then, the value of a policy \u03c0 = (\u03b4T , . . . , \u03b41) in a state s is set to be the expected value of the histories that can be generated by \u03c0 from s. This value, given by the value function v\u03c0 T : S \u2192 R can be computed iteratively as follows: v\u03c0 0 (s) = 0 v\u03c0 t (s) = R(s, \u03b4t(s)) + \u03b3 \ufffd s\u2032\u2208S P(s, \u03b4t(s), s\u2032)v\u03c0 t\u22121(s\u2032) (1) 4 Quantile RL The value v\u03c0 t (s) is the expectation of cumulated rewards obtained by the agent if she performs action \u03b4t(s) in state s at time-step t and continues to follow policy \u03c0 thereafter. The higher the values of v\u03c0 t (s) are, the better. Therefore, value functions induce a preference relation \u227f\u03c0 over policies in the following way: \u03c0 \u227f\u03c0 \u03c0\u2032 \u21d4 \u2200s \u2208 S, v\u03c0 T (s) \u2265 v\u03c0\u2032 T (s) A solution to an MDP is a policy, called optimal policy, that ranks the highest with respect to \u227f\u03c0. Such a policy can be found by solving the following equations, which yields the value function of an optimal policy: v\u2217 0(s) = 0 v\u2217 t (s) = max a\u2208A R(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032)v\u2217 t\u22121(s\u2032) 3.2. Reinforcement Learning In the reinforcement learning (RL) setting, the assumption of the knowledge of the environment is relaxed: both dynamics through the transition function and preferences via the reward function are not known anymore. While interacting with its environment, an RL agent tries to learn a good policy by trial and error. To make \ufb01nite horizon MDPs learnable, we assume the decision process is repeated in\ufb01nitely many times. That is, when horizon T is reached, we assume that the agent automatically returns to the initial state and the problem starts over. A simple algorithm to solve such an RL problem is the Q-learning algorithm (see Algorithm 1), which estimates the Q-function: Qt(s, a) = R(s, a) + \u03b3 \ufffd s\u2032\u2208S P(s, a, s\u2032)Vt\u22121(s\u2032) And obviously, we have: Vt(s) = max a\u2208A Qt(s, a). In Algorithm 1, Line 1 generally depends on the Qt\u22121(s, \u00b7) and possibly on iteration n. A simple strategy to perform this choice is called \u03b5-greedy where the best action dictated by Qt\u22121(s, \u00b7) is chosen with probability 1 \u2212 \u03b5 (with \u03b5 a small positive value) or a random action is chosen otherwise. A schedule can be de\ufb01ned so that parameter \u03b5 tends to zero as n tends to in\ufb01nity. Besides, \u03b1n(s, a) \u2208 (0, 1) on Line 2 is a learning rate. In the general case, it depends on iteration n, state s and action a, although in practice it is often chosen as a constant. 3.3. Limits of standard criteria The standard decision criteria used in MDPs, which are based on expectation, may not be reasonable in some situations. Firstly, unfortunately, in many cases, the reward function R is not known. In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013). However, even for an expert user, the elicitation of the reward function can reveal burdensome. In inverse reinforcement learning (Ng and Russell, 2000), the expert is assumed to know an 5 Gilbert Weng Data: MT = (S, A, G, P, R, s0) MDP Result: Q begin Q0(s, a) \u2190\u2212 0, \u2200(s, a) \u2208 S \u00d7 A s \u2190\u2212 s0 t \u2190\u2212 1 for n = 1 to N do 1 a \u2190\u2212 choose action r, s\u2032 \u2190\u2212 perform action a in state s 2 Qt(s, a) \u2190\u2212 Qt(s, a) + \u03b1n(s, a) \ufffd r + \u03b3 maxa\u2032\u2208A Qt\u22121(s\u2032, a\u2032) \u2212 Qt(s, a) \ufffd 3 if t = T then s \u2190\u2212 s0 t \u2190\u2212 1 else s \u2190\u2212 s\u2032 t \u2190\u2212 t + 1 end end end Algorithm 1: Q-learning optimal policy, which is rarely true in practice. In interactive settings (Regan and Boutilier, 2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters. In this paper, we address this problem by only assuming that a strict weak ordering over histories is known. Secondly, for numerous applications, the expectation of cumulated reward, as used in Equation 1, may not be the most appropriate criterion (even when a numeric reward function is de\ufb01ned). For instance, in case of high variance or when a policy is known to be only applied a few times, the solution given by this criterion may not be satisfying for risk-averse agent. Moreover, in some domains (e.g., web industry or more generally service industry), decisions about performance are often based on the minimal quality of 99% of the possible outcomes. Therefore, in this article we aim at using a quantile (de\ufb01ned in Section 3.5) as a decision criterion to solve an MDP. 3.4. MDP with End States In this paper, we work with episodic MDPs with end states. Such an MDP is formally de\ufb01ned as a tuple MT = (S, A, G, P, s0) where S, A, P, s0 are de\ufb01ned as previously, G \u2286 S is a \ufb01nite set of end states and T is a \ufb01nite maximal horizon (i.e., an end state is attained after at most T time steps.). We call episode a history starting from s0 and ending in a \ufb01nal state of G. We assume that a preference relation is de\ufb01ned over end states: We write g\u2032 \u227a g if end state g is preferred to end state g\u2032. Without loss of generality, we assume that G = {g1, . . . , gn} and end states are ordered with increasing preference, i.e., g1 \u227a g2 \u227a . . . \u227a gn. The weak relation of \u227a is denoted \u2aaf. 6 Quantile RL Note that a \ufb01nite horizon MDP can be reformulated as an MDP with end states by state augmentation. Although the resulting MDP may have a large-sized state space, the two models are formally equivalent. We focus on episodic MDPs with end states to simplify the presentation of our approach. 3.5. Quantile Criterion We de\ufb01ne quantiles of distributions over end states of G, which are ordered by \u2aaf. Let \u03c4 \u2208 [0, 1] be a \ufb01xed parameter. Intuitively, the \u03c4-quantile of a distribution of end states, is the value q \u2208 G such that the probability of getting an end state equal or lower than q is \u03c4 and that of getting an end state equal or greater than q is 1 \u2212 \u03c4. The 0.5-quantile, also known as median, can be seen as the ordinal counterpart of the mean. The 0-quantile (resp. 1-quantile) is the minimum (resp. maximum) of a distribution. More generally, quantiles, which have been axiomatically characterized by Rostek (2010), de\ufb01ne decision criteria that have the nice property of not requiring numeric valuations, but only an order. The formal de\ufb01nition of quantiles can be stated as follows. Let p\u03c0 denote the probability distribution over end states induced by a policy \u03c0 from initial state s0, the cumulative distribution induced by p\u03c0 is then de\ufb01ned as F \u03c0 where F \u03c0(g) = \ufffd g\u2032\u2aafg p\u03c0(g\u2032) is the probability of getting an end state not preferred to g when applying policy \u03c0. Similarly, the decumulative distribution induced by p\u03c0 is de\ufb01ned as G\u03c0(g) = \ufffd g\u2aafg\u2032 p\u03c0(g\u2032) is the probability of getting an end state not lower than g. These two notions of cumulative and decumulative enable us to de\ufb01ne two kinds of criteria. First, given a policy \u03c0, we de\ufb01ne the lower \u03c4-quantile for \u03c4 \u2208 (0, 1] as: q\u03c0 \u03c4 = min{g \u2208 G | F \u03c0(g) \u2265 \u03c4} (2) where the min operator is with respect to \u2aaf. Then, given a policy \u03c0, we de\ufb01ne the upper \u03c4-quantile for \u03c4 \u2208 [0, 1) as: q\u03c0 \u03c4 = max{g \u2208 G | G\u03c0(g) \u2265 1 \u2212 \u03c4} (3) where the max operator is with respect to \u2aaf. If \u03c4 = 0 or \u03c4 = 1 only one of q\u03c0 \u03c4 or q\u03c0 \u03c4 is de\ufb01ned and we de\ufb01ne the \u03c4-quantile q\u03c0 \u03c4 as that value. When both are de\ufb01ned, by construction, we have q\u03c0 \u03c4 \u2aaf q\u03c0 \u03c4 . If those two values are equal, q\u03c0 \u03c4 is de\ufb01ned as equal to them. For instance, this is always the case in continuous settings for continuous distributions. However, in our discrete setting, it could happen that those values di\ufb00er, as shown by Example 1. Example 1 Consider an MDP where G = {g1 \u227a g2 \u227a g3}. Let \u03c0 be a policy that attains each end state with probabilities 0.5, 0.2 and 0.3 respectively. It is easy to check that q\u03c0 0.5 = g1 whereas q\u03c0 0.5 = g2. When the lower and the upper quantiles di\ufb00er, one may de\ufb01ne the quantile as a function of the lower and upper quantiles (Weng, 2012). For simplicity, in this paper, we focus on optimizing directly the lower and the upper quantiles. The quantile criterion is di\ufb03cult to optimize, even when a numerical reward function is given and the quality of an episode is de\ufb01ned as the cumulative of rewards received along the episode. This di\ufb03culty comes notably from two related sources: 7 Gilbert Weng \u2022 The quantile criterion is non-linear: for instance, the \u03c4-quantile q\u02dc\u03c0 \u03c4 of the mixed policy \u02dc\u03c0 that generates an episode using policy \u03c0 with probability p and \u03c0\u2032 with probability 1 \u2212 p is not equal to pq\u03c0 \u03c4 + (1 \u2212 p)q\u03c0\u2032 \u03c4 . \u2022 The quantile criterion is non-dynamically consistent: A sub-policy at time step t of an optimal policy for horizon T may not be optimal for horizon T \u2212 t. In decision theory (McClennen, 1990), three approaches have been considered for such kinds of decision criteria: 1. Consequentialist approach: at each time step t, follow an optimal policy for the problem with horizon T \u2212 t and initial state st even if the resulting policy is not optimal at horizon T; 2. Resolute choice approach: at time step t = 0, apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it; 3. Sophisticated resolute choice approach (Ja\ufb00ray, 1998; Fargier et al., 2011): apply a policy \u03c0 (chosen at the beginning) that trades o\ufb00 between how much \u03c0 is optimal for all horizons T, T \u2212 1, . . . , 1. With non-dynamically consistent preferences, it is debatable to adopt a consequentialist approach, as the sequence of decisions may lead to dominated results. In this paper, we adopt a resolute choice point of view. We leave the third approach for future work. 4. Quantile-based Reinforcement Learning In this section, we \ufb01rst state the problem solved in this paper and some useful properties. Then, we present our algorithm called Quantile Q-learning (or QQ-learning for short), which is an extension of Q-learning and exploits a two-timescale stochastic approximation technique. 4.1. Problem Statement In this paper, we aim at learning a policy that is optimal for the quantile criterion from a \ufb01xed initial state. We assume that the underlying MDP is an episodic MDP with end states. Let \u03c4 \u2208 (0, 1) be a \ufb01xed parameter. Formally, the problem of determining a policy optimal for the lower/upper \u03c4-quantile can be stated as follows: \u03c0\u2217 = arg max \u03c0 q\u03c0 \u03c4 or \u03c0\u2217 = arg max \u03c0 q\u03c0 \u03c4 (4) We focus on learning a policy that is deterministic and Markovian. The optimal lower/upper quantiles satisfy the following lemmas: Lemma 1 The optimal lower \u03c4-quantile q\u2217 \u03c4 satis\ufb01es: q\u2217 \u03c4 = min{g : F \u2217(g) \u2265 \u03c4} (5) F \u2217(g) = min \u03c0 F \u03c0(g) \u2200g \u2208 G (6) 8 Quantile RL and the optimal upper \u03c4-quantile q\u2217 \u03c4 satis\ufb01es: q\u2217 \u03c4 = max{g : G\u2217(g) \u2265 1 \u2212 \u03c4} (7) G\u2217(g) = max \u03c0 G\u03c0(g) \u2200g \u2208 G (8) Then, if the optimal lower quantile (q\u2217 \u03c4) or upper quantile (q\u2217 \u03c4) were known, the problem would be relatively easy to solve. By Lemma 1, an optimal policy for the lower quantile could be obtained as follows: \u03c0\u2217 = arg min \u03c0 F \u03c0(q\u2212 \u03c4 ) (9) where q\u2212 \u03c4 = g1 if q\u2217 \u03c4 = g1 and q\u2212 \u03c4 = gi if q\u2217 \u03c4 = gi+1. The reason one needs to use q\u2212 \u03c4 instead of the optimal lower quantile is that otherwise it may happen that the cumulative distribution of a non-optimal policy \u03c0 is smaller than or equal to that of an optimal policy at the lower quantile q\u2217 \u03c4 (but greater below q\u2217 \u03c4 as \u03c0 is not optimal). In that case, the lower quantile of \u03c0 might not be q\u2217 \u03c4. Such a thing cannot happen for the upper quantile. In that sense, determining an optimal policy for the upper quantile is easier. By Lemma 1, it is simply given by: \u03c0\u2217 = arg max \u03c0 G\u03c0(q\u2217 \u03c4) (10) In practice, if the lower/upper quantiles were known, those policies could be computed by solving a standard MDP with the following reward functions: R\u03b8(s) = \uf8f1 \uf8f2 \uf8f3 0 \u2200s \u0338\u2208 G 0 \u2200s = gi \u2208 G, \u03b8 \u2265 i 1 \u2200s = gi \u2208 G, \u03b8 \u2264 i \u2212 1 for the lower quantile with \u03b8 = k if q\u2212 \u03c4 = gk and R\u03b8(s) = \uf8f1 \uf8f2 \uf8f3 0 \u2200s \u0338\u2208 G 1 \u2200s = gi \u2208 G, \u03b8 \u2264 i 0 \u2200s = gi \u2208 G, \u03b8 \u2265 i + 1 for the upper quantile with \u03b8 = k if q\u2217 \u03c4 = gk. Note that R\u03b8 can be rewritten to depend on the optimal lower quantile: R\u03b8(s) = \uf8f1 \uf8f2 \uf8f3 0 \u2200s \u0338\u2208 G 0 \u2200s = gi \u2208 G, \u03b8 \u2265 i + 1 1 \u2200s = gi \u2208 G, \u03b8 \u2264 i with \u03b8 = k if q\u2217 \u03c4 = gk. Solving an MDP with R\u03b8 amounts to minimizing the probability of ending in a \ufb01nal state strictly less preferred than q\u2217 \u03c4, which solves Equation 9. Similarly, solving an MDP with R\u03b8 amounts to maximizing the probability of ending in a \ufb01nal state at least as preferred as q\u2217 \u03c4, which solves Equation 10. Now, the issue here is that the lower and upper quantiles are not known. We show in the next subsection that this problem can be overcome with a two-timescale stochastic approximation technique. 9 Gilbert Weng Data: MT = (S, A, G, P, s0) begin Initialize \u03b8 to a random value for n = 1, 2, . . . do 4 Solve MDP MT with reward function R\u03b8 if V \u2217 \u03b8 (s0) < 1 \u2212 \u03c4 then \u03b8 \u2190\u2212 \u03b8 \u2212 1/n else \u03b8 \u2190\u2212 \u03b8 + 1/n end end end Algorithm 2: Simple strategy for \ufb01nding the optimal upper quantile 4.2. QQ-learning As the lower and upper quantiles are not known, we let parameter \u03b8 vary in R+ during the learning steps and we re\ufb01ne the de\ufb01nition of the previous reward functions to make sure they are both well-de\ufb01ned for all \u03b8 \u2208 R+ and smooth in \u03b8: R\u03b8(s) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 0 \u2200s \u0338\u2208 G \u22121 \u2200s = gi \u2208 G, \u03b8 \u2265 i + 1 0 \u2200s = gi \u2208 G, \u03b8 \u2264 i i \u2212 \u03b8 else R\u03b8(s) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 0 \u2200s \u0338\u2208 G 1 \u2200s = gi \u2208 G, \u03b8 \u2264 i 0 \u2200s = gi \u2208 G, \u03b8 \u2265 i + 1 i + 1 \u2212 \u03b8 else In the remaining of the paper, we present how to solve for the upper quantile. A similar approach can be developed for the lower quantile. In order to \ufb01nd the optimal upper quantile, one could use the strategy described in Algorithm 2. Value V \u2217 \u03b8 (s0) approximates the probability of reaching an end state whose index is at least as high as \u03b8. If that value is smaller than 1 \u2212 \u03c4, it means \u03b8 is too high and should be decreased. Otherwise \u03b8 is too small and should be increased. Parameter \u03b8 will then converge to the index of the optimal upper quantile, which is the maximal value for \u03b8 such that V \u2217 \u03b8 (s0) \u2265 1 \u2212 \u03c4. The optimal policy for V \u2217 \u03b8 is an optimal policy for the upper \u03c4-quantile. In a reinforcement learning setting, the solve MDP part (line 4 in Algorithm 2) could be replaced by an RL algorithm such as Q-learning. The problem is that such algorithm is only guaranteed to converge to the solution when n \u2192 \u221e. It would be therefore di\ufb03cult to integrate Q-learning in Algorithm 2. Instead, a good policy can be learned while searching for the correct value of \u03b8. To that aim, we use a two-timescale technique (Borkar, 1997, 2008) in which Q-learning and the update of parameter \u03b8 are run concurrently but at di\ufb00erent speeds (i.e., at two di\ufb00erent timescales). For this to work, parameter \u03b8 needs to be seen as 10 ",
    "Experimental Results": "Experimental Results To demonstrate the soundness of our approach, we evaluate our algorithm on the domain, Who wants to be a millionaire. We present the experimental results below. 5.1. Domain In this popular television game show, a contestant needs to answer a maximum of 15 multiple-choice questions (with four possible answers) of increasing di\ufb03culty, for increasingly large sums, roughly doubling the pot at each question. At each time step, the contestant may decide to walk away with the money currently won. If she answers incorrectly, then all winnings are lost except what has been earned at a \u201cguarantee point\u201d (questions 11 ",
    "Results": "Results We plot the results (see Figure 1) obtained for two di\ufb00erent learning runs, one with 1 million learning steps and the other with 10 million learning steps. The \u03b8-updates were interleaved with a Q-learning phase using an \u03b5-greedy exploration strategy with \u03b5 = 0.01 and with learning rate \u03b1(n) = 1/(n + 1)11/20. One can check that Equation 11 is satis\ufb01ed. We optimize the upper quantile with \u03c4 = 0.3. During the learning process we maintain a vector f of frequencies with which each \ufb01nal state has been attained. We de\ufb01ne the quantity score as the cross product between f and the vector of rewards obtained when attaining each \ufb01nal state given the current value of \u03b8. Put another way, score is the value of the non-stationary policy that has been played since the beginning of the learning process. Moreover, at each iteration we compute V \u2217 \u03b8 (s0), the optimal value in s0 given the current value of \u03b8. On Figures 1(a) and 1(b) we observe the evolution of V \u2217 \u03b8 (s0) as the number of iterations increases. We observe that V \u2217 \u03b8 (s0) converges towards 1\u2212\u03c4 = 0.7 as the number of iterations increases with oscillations of decreasing amplitude that are due to the ever changing \u03b8 value. Figures 1(c) and 1(d) show the evolution of the score as the number of iterations increases. Score converges towards a value of 0.7 but inferior. This is due to the exploration of the Q-learning algorithm. Lastly, 1(e) and 1(f ) plot the evolution of \u03b8 as the number of iteration increases. The value of \u03b8 converges towards a value of 4 which the one for which V \u2217 \u03b8 (s0) = 1 \u2212 \u03c4 = 0.7. 6. ",
    "Conclusion": "Conclusion We have presented an algorithm for learning a policy optimal for the quantile criterion in the reinforcement learning setting, when the MDP has a special structure, which corresponds to repeated episodic decision-making problems. It is based on stochastic approximation with two timescales (Borkar, 2008). Our proposition is experimentally validated on the domain, Who wants to be millionaire. As future work, it would be interesting to investigate how to choose the learning rate \u03b1n in order to ensure a fast convergence. Moreover, our approach could be extended to other settings than episodic MDPs. Besides, it would also be interesting to explore whether gradient-based algorithms could be developed for the optimization of quantiles, based on the fact that a quantile is solution of an optimization problem where the objective function is piecewise linear (Koenker, 2005). 12 Quantile RL (a) (b) (c) (d) (e) (f ) Figure 1: Evolution of V \u2217 \u03b8 (s0), score and \u03b8 for 1 million learning steps (left) and 10 million learning steps (right) 13 ",
    "References": "References Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobatics through apprenticeship learning. International Journal of Robotics Research, 29(13):1608\u20131639, 2010. R. Akrour, M. Schoenauer, and M. S\u00b4ebag. April: Active preference-learning based reinforcement learning. In ECML PKDD, Lecture Notes in Computer Science, volume 7524, pages 116\u2013131, 2012. Nicole B\u00a8auerle and Jonathan Ott. Markov decision processes with average value-at-risk criteria. Mathematical Methods of Operations Research, 74(3):361\u2013379, 2011. D.F. Benoit and D. Van den Poel. Bene\ufb01ts of quantile regression for the analysis of customer lifetime value in a contractual setting: An application in \ufb01nancial services. Expert Systems with Applications, 36:10475\u201310484, 2009. D. Blackwell. An analog of the minimax theorem for vector payo\ufb00s. Paci\ufb01c Journal of Mathematics, 6(1):1\u20138, 1956. V. Borkar and Rahul Jain. Risk-constrained Markov decision processes. IEEE Transactions on Automatic Control, 59(9):2574\u20132579, 2014. Vivek S. Borkar. Stochastic approximation : a dynamical systems viewpoint. Cambridge university press New Delhi, Cambridge, 2008. ISBN 978-0-521-51592-4. URL http://opac.inria.fr/ record=b1132816. V.S. Borkar. Stochastic approximation with time scales. Systems & Control Letters, 29(5):291\u2013294, 1997. M. Boussard, M. Bouzid, A.I. Mouaddib, R. Sabbadin, and P. Weng. Markov Decision Processes in Arti\ufb01cial Intelligence, chapter Non-Standard Criteria, pages 319\u2013359. Wiley, 2010. V\u00b4eronique Bruy`ere, Emmanuel Filiot, Mickael Randour, and Jean-Fran\u00b8cois Raskin. Meet your expectations with guarantees: beyond worst-case synthesis in quantitative games. In STACS, 2014. R. Busa-Fekete, B. Sz\u00a8orenyi, P. Weng, W. Cheng, and E. H\u00a8ullermeier. Preference-based reinforcement learning. In European Workshop on Reinforcement Learning, Dagstuhl Seminar, 2013. Robert Busa-Fekete, Balazs Szorenyi, Paul Weng, Weiwei Cheng, and Eyke H\u00a8ullermeier. Preferencebased Reinforcement Learning: Evolutionary Direct Policy Search using a Preference-based Racing Algorithm. Machine Learning, 97(3):327\u2013351, 2014. Alexandra Carpentier and Michal Valko. Extreme bandits. In NIPS, 2014. Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in MDPs. In NIPS, 2014. G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. Dynamo: amazon\u2019s highly available key-value store. ACM SIGOPS Operating Systems Review, 41(6):205\u2013220, 2007. E. Delage and S. Mannor. Percentile optimization in uncertain Markov decision processes with application to e\ufb03cient exploration. In ICML, pages 225\u2013232, 2007. 14 Quantile RL H\u00b4el`ene Fargier, Gildas Jeantet, and Olivier Spanjaard. Resolute choice in sequential decision problems with multiple priors. In IJCAI, 2011. Jerzy A. Filar. Percentiles and Markovian decision processes. Operations Research Letters, 2(1): 13 \u2013 15, 1983. ISSN 0167-6377. doi: http://dx.doi.org/10.1016/0167-6377(83)90057-3. URL http://www.sciencedirect.com/science/article/pii/0167637783900573. Jerzy A. Filar, L. C. M. Kallenberg, and Huey-Miin Lee. Variance-penalized Markov decision processes. Mathematics of Operations Research, 14:147\u2013161, 1989. P.C. Fishburn. An axiomatic characterization of skew-symmetric bilinear functionals, with applications to utility theory. Economics Letters, 8(4):311\u2013313, 1981. J. F\u00a8urnkranz, E. H\u00a8ullermeier, W. Cheng, and S.H. Park. Preference-based reinforcement learning: A formal framework and a policy iteration algorithm. Machine Learning, 89(1):123\u2013156, 2012. Hugo Gilbert, Olivier Spanjaard, Paolo Viappiani, and Paul Weng. Solving MDPs with skew symmetric bilinear utility functions. In IJCAI, pages 1989\u20131995, 2015. Hugo Gilbert, Bruno Zanuttini, Paolo Viappiani, Paul Weng, and Esther Nicart. Model-free reinforcement learning with skew-symmetric bilinear utilities. In International Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016. Hugo Gimbert. Pure stationary optimal strategies in Markov decision processes. In STACS, 2007. Jean-Yves Ja\ufb00ray. Implementing resolute choice under uncertainty. In UAI, 1998. Philippe Jorion. Value-at-Risk: The New Benchmark for Managing Financial Risk. McGraw-Hill, 2006. D. Kalathil, V.S. Borkar, and R. Jain. A learning scheme for blackwell\u2019s approachability in mdps and stackelberg stochastic games. Arxiv, 2014. R. Koenker. Quantile Regression. Cambridge university press, 2005. Y. Liu and S. Koenig. Risk-sensitive planning with one-switch utility functions: Value iteration. In AAAI, pages 993\u2013999. AAAI, 2005. Y. Liu and S. Koenig. Functional value iteration for decision-theoretic planning with general utility functions. In AAAI, pages 1186\u20131193. AAAI, 2006. E. McClennen. Rationality and dynamic choice: Foundational explorations. Cambridge university press, 1990. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015. A.Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In ICML. Morgan Kaufmann, 2000. F. Perea and J. Puerto. Dynamic programming analysis of the TV game who wants to be a millionaire? European Journal of Operational Research, 183(2):805 \u2013 811, 2007. ISSN 0377-2217. doi: http://dx.doi.org/10.1016/j.ejor.2006.10.041. URL http://www.sciencedirect.com/science/ article/pii/S0377221706010320. 15 Gilbert Weng M.L. Puterman. Markov decision processes: discrete stochastic dynamic programming. Wiley, 1994. Mickael Randour, Jean-Fran\u00b8cois Raskin, and Ocan Sankur. Percentile queries in multi-dimensional Markov decision processes. CoRR, abs/1410.4801, 2014. URL http://arxiv.org/abs/1410. 4801. K. Regan and C. Boutilier. Regret based reward elicitation for Markov decision processes. In UAI, pages 444\u2013451. Morgan Kaufmann, 2009. M.J. Rostek. Quantile maximization in decision theory. Review of Economic Studies, 77(1):339\u2013371, 2010. Bal\u00b4azs Sz\u00a8or\u00b4enyi, R\u00b4obert Busa-Fekete, Paul Weng, and Eyke H\u00a8ullermeier. Qualitative multi-armed bandits: A quantile-based approach. In ICML, pages 1660\u20131668, 2015. Gerald Tesauro. Temporal di\ufb00erence learning and td-gammon. Communications of the ACM, 38(3): 58\u201368, 1995. P. Weng. Markov decision processes with ordinal rewards: Reference point-based preferences. In ICAPS, volume 21, pages 282\u2013289. AAAI, 2011. P. Weng. Ordinal decision models for Markov decision processes. In ECAI, volume 20, pages 828\u2013833. IOS Press, 2012. P. Weng and B. Zanuttini. Interactive value iteration for Markov decision processes with unknown rewards. In IJCAI, 2013. D. J. White. Utility, probabilistic constraints, mean and variance of discounted rewards in Markov decision processes. OR Spektrum, 9:13\u201322, 1987. R. Wolski and J. Brevik. QPRED: Using quantile predictions to improve power usage for private clouds. Technical report, UCSB, 2014. Jia Yuan Yu and Evdokia Nikolova. Sample complexity of risk-averse bandit-arm selection. In IJCAI, 2013. Stella X. Yu, Yuanlie Lin, and Pingfan Yan. Optimization models for the \ufb01rst arrival target distribution function in discrete time. Journal of mathematical analysis and applications, 225:193\u2013223, 1998. B. Zhang, Q. Cai, J. Mao, E Chang, and B. Guo. Spoken dialogue management as planning and acting under uncertainty. In 7th European Conference on Speech Communication and Technology, 2001. 16 ",
    "title": "Quantile Reinforcement Learning",
    "paper_info": "JMLR: Workshop and Conference Proceedings 60 (2016) 1\u201316\nACML 2016\nQuantile Reinforcement Learning\nHugo Gilbert\nhugo.gilbert@lip6.fr\nSorbonnes Universit\u00b4es\nUPMC Univ Paris 06\nCNRS, LIP6 UMR 7606\nParis, France\nPaul Weng\npaweng@cmu.edu\nSchool of Electronics and Information Technology\nSYSU-CMU Joint Institute of Engineering\nSYSU-CMU Shunde Joint Research Institute\nGuangzhou, 510006, PR China\nAbstract\nIn reinforcement learning, the standard criterion to evaluate policies in a state is the expec-\ntation of (discounted) sum of rewards. However, this criterion may not always be suitable,\nwe consider an alternative criterion based on the notion of quantiles. In the case of episodic\nreinforcement learning problems, we propose an algorithm based on stochastic approxima-\ntion with two timescales. We evaluate our proposition on a simple model of the TV show,\nWho wants to be a millionaire.\nKeywords: Reinforcement learning, Quantile, Ordinal Decision Model, Two-Timescale\nStochastic Approximation\n1. Introduction\nMarkov decision process and reinforcement learning are powerful frameworks for building\nautonomous agents (physical or virtual), which are systems that make decisions without\nhuman supervision in order to perform a given task. Examples of such systems abound:\nexpert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al., 2001), acrobatic\nhelicopter \ufb02ight (Abbeel et al., 2010) or human-level video game player (Mnih et al., 2015).\nHowever, the standard framework assumes that rewards are numeric, scalar and additive\nand that policies are evaluated with the expectation criterion. In practice, it may happen\nthat such numerical rewards are not available, for instance, when the agent interacts with\na human who generally gives ordinal feedback (e.g., \u201cexcellent\u201d, \u201cgood\u201d, \u201cbad\u201d and so on).\nBesides, even when this numerical information is available, one may want to optimize a\ncriterion di\ufb00erent than the expectation, for instance in one-shot decision-making.\nSeveral works considered the case where preferences are qualitative. Markov decision\nprocesses with ordinal reward have been investigated (Weng, 2012, 2011) and di\ufb00erent\nordinal decision criteria have been proposed in that context. More generally, preference-\nbased reinforcement learning (Akrour et al., 2012; F\u00a8urnkranz et al., 2012; Busa-Fekete et al.,\n2013, 2014) has been proposed to tackle situations where the only available preferential\ninformation concerns pairwise comparisons of histories.\nc\u20dd 2016 H. Gilbert & P. Weng.\narXiv:1611.00862v1  [cs.LG]  3 Nov 2016\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant because it proposes an alternative criterion based on the notion of quantiles in reinforcement learning, which can be useful when numerical rewards are not always available or when optimizing a different criterion than the expectation. The proposed algorithm based on stochastic approximation with two timescales for episodic reinforcement learning problems achieves promising results on a simulated domain.\n\n- (2): Innovation point: The article proposes a novel approach based on the quantile criterion in reinforcement learning to optimize a different criterion than the expectation. Performance: The algorithm achieves promising results on a simple model of the TV show Who Wants to be a Millionaire, supporting the proposed alternative criterion based on quantiles. Workload: Although the proposed approach is shown to be effective, future work is needed to explore how to choose the learning rate and extend the approach to other settings than episodic MDPs.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This piece of work is significant because it proposes an alternative criterion based on the notion of quantiles in reinforcement learning, which can be useful when numerical rewards are not always available or when optimizing a different criterion than the expectation. The proposed algorithm based on stochastic approximation with two timescales for episodic reinforcement learning problems achieves promising results on a simulated domain.\n\n- (2): Innovation point: The article proposes a novel approach based on the quantile criterion in reinforcement learning to optimize a different criterion than the expectation. Performance: The algorithm achieves promising results on a simple model of the TV show Who Wants to be a Millionaire, supporting the proposed alternative criterion based on quantiles. Workload: Although the proposed approach is shown to be effective, future work is needed to explore how to choose the learning rate and extend the approach to other settings than episodic MDPs.\n\n\n"
}