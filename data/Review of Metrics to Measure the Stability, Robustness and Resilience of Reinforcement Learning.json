{
    "Abstract": "Abstract and Keywords  Theo   Theoretical  US    United States  WoS   Web of Science      References    Resilience    [1]  Behzadan, V., and Munir, A. 2018. Adversarial exploitation of emergent  dynamics in smart cities. Proceedings of the 2018 IEEE International Smart  Cities Conference, doi 10.1109/ISC2.2018.8656789.  [2]  Enjalbert, S., and Vanderhaegen, F. 2017. A hybrid reinforced learning system to  estimate resilience indicators. Engineering Applications of Artificial Intelligence  64:295-301.  [3]  Bunyakitanon, M., Vasilakos, X., Nejabati, R., and Simeonidou D. 2020. End-toEnd Performance-Based Autonomous VNF Placement with Adopted  Reinforcement Learning. IEEE Transactions on Cognitive Communications and  Networking 6(2):534-547. doi 10.1109/TCCN.2020.2988486.    Stability    [4]  Dong, Zhe, X. Huang, Y. Dong, and Z. Zhang. 2020. Multilayer perception based  reinforcement learning supervisory control of energy systems with application to  a nuclear steam supply system. Journal of Applied Energy 259, Feb. 2020. doi  10.1016/j.apenergy.2019.114193.  [5]  Wen, Guoxing, C.L. Philip Chen, Shuzhi Sam Ge, Hongli Yang, and Xiaoguang  Liu. 2019. Optimized adaptive nonlinear tracking control using actor-critic  reinforcement learning strategy. IEEE Transactions on Industrial Informatics  15(9):4969-4977.  PREPRINT    40  [6]  Muneeswari, B., and M.S.K. Manikandan. 2019. Energy efficient clustering and  secure routing using reinforcement learning for three-dimensional mobile ad hoc  networks. IET Communications 13(12):1828-1839.  [7]  de Oliveira, Bernardo A.G., Carlos A.P. da S. Martins, Flavia Magalhaes, Luis  Fabricio, and W. Goes. 2019. Difference based metrics for deep reinforcement  learning algorithms. IEEE Access 7:159141-159149.  [8]  Zhang, Kaiqing, Alec Koppel, Hao Zhu, and Tamer Bas. 2019. Policy search in  infinite-horizon discounted reinforcement learning: advances through connections  to non-convex optimization. Proceedings: 53rd Annual Conference on  Information Sciences and Systems (CISS), Baltimore, MD, Mar 20-22.  [9]  Du, Zhijiang, Wei Wang, Zhiyuan Yan, Wei Dong, and Weidong Wang. 2017.  Variable admittance control based on fuzzy reinforcement learning for minimally  invasive surgery manipulator, Sensors 17(4).  [10]  Jiang, He, Huaguang Zhang, Yanhong Luo, and Junyi Wang. 2016. Optimal  tracking control for completely unknown nonlinear discrete-time Markov jump  systems using data-based reinforcement learning method. Neurocomputing  194:176-182.  [11]  Abdallah, Sherief. 2009. Why global performance is a poor metric for verifying  convergence of multi-agent learning. arXiv:0904.2320v1 [cs.MA] 15 April 2009.   [12]  Talele, Nihar, and Katie Byl. 2019. Mesh-based tools to analyze deep  reinforcement learning policies for underactuated biped locomotion.  arXiv:1903.12311v2 [cs.RO] 1 November 2019.  [13]  Tuan, Yi-Lin, Jinzhi Zhang, Yujia Li, and Hung-yi Lee. 2018. Proximal policy  optimization and its dynamic version for sequence generation.  arXiv:1808.07982v1 [cs.CL] 24 August 2018.  [14]  Serhani, Abdellatif, Najib Naja, and Abdellah Jamali. 2020. AQ-Routing:  mobility-, stability-aware adaptive routing protocol for data routing in MANET\u2013 IoT systems. Cluster Computing 23:13\u201327. doi 10.1007/s10586-019-02937-x.  [15]  Dong, Zhe, Xiaojin Huang, Yujie Dong, and Zuoyi Zhang. 2020. Multilayer  perception based reinforcement learning supervisory control of energy systems  with application to a nuclear steam supply system. Applied Energy 259 (2020)  114193. doi 10.1016/j.apenergy.2019.114193.  [16]  Zhang, Huaguang, Kun Zhang, Yuliang Cai, and Jian Han. 2019. Adaptive fuzzy  fault-tolerant tracking control for partially unknown systems with actuator faults  PREPRINT    41  via integral reinforcement learning method. IEEE Transactions on Fuzzy Systems  27(10). doi 10.1109/TFUZZ.2019.2893211.  [17]  Cohen, Daniel, Scott M. Jordan, and W. Bruce Croft. 2019. Learning a Better  Negative Sampling Policy with Deep Neural Networks for Search. In the 2019  ACM SIGIR International Conference on the Theory of Information Retrieval  (ICTIR \u201919), October 2\u20135, 2019, Santa Clara, CA, USA. ACM, New York, NY,  USA. doi 10.1145/3341981.3344220.  [18]  Mu, Chaoxu, Qian Zhao, Zhongke Gao, and Changyin Sun. 2019. Q-learning  solution for optimal consensus control of discrete-time multiagent systems using  reinforcement learning. Journal of the Franklin Institute 356:6946\u20136967. doi  10.1016/j.jfranklin.2019.06.0070016-0032.  [19]  Tang, Xiaocheng, Zhiwei (Tony) Qin, Fan Zhang, Zhaodong Wang, Zhe Xu,  Yintai Ma, Hongtu Zhu, and Jieping Ye. 2019. A deep value-network based  approach for multi-driver order dispatching. KDD 19, August 4\u20138, 2019,  Anchorage, AK, USA. doi 10.1145/3292500.3330724.  [20]  Abouheaf, Mohammed, and Wail Gueaieb. 2019. Model-free adaptive control  approach using integral reinforcement learning. IEEE International Symposium  on Robotic and Sensors Environments \u2013 Proceedings.  [21]  Seo, Donghyeon, Harin Kim, and Donghan Kim. 2019. Push recovery control for  humanoid robot using reinforcement learning. Third IEEE International  Conference on Robotic Computing (IRC). doi 10.1109/IRC.2019.00102.  [22]  Lv, Y., X. Ren, and J. Na. 2019. Online Nash-optimization tracking control of  multi-motor driven load system with simplified RL scheme. ISA Transactions.  doi 10.1016/j.isatra.2019.08.025.  [23]  Tang, Li, Yan-Jun Liu, and C. L. Philip Chen. 2018. Adaptive critic design for  pure-feedback discrete-time MIMO systems preceded by unknown backlashlike  hysteresis. IEEE Transactions on Neural Networks and Learning Systems.  29(11), November. doi 10.1109/TNNLS.2018.2805689.  [24]  Mertikopoulos, Panayotis, and William H. Sandholm. 2018. Riemannian game  dynamics. Journal of Economic Theory (177):315-364. doi  10.1016/j.jet.2018.06.002.  [25]  Liu, D., and G.-H. Yang. 2018. Model-free adaptive control design for nonlinear  discrete-time processes with reinforcement learning techniques. International  PREPRINT    42  Journal of Systems Science 49(11):2298-2308. doi  10.1080/00207721.2018.1498557.  [26]  Bentaleb, Abdelhak, Ali C. Begen, and Roger Zimmermann. 2018. ORL-SDN:  Online reinforcement learning for SDN-enabled HTTP adaptive streaming. ACM  Trans. Multimedia Comput. Commun. Appl. 14(3) Article 71 (August), 28 pages.  doi 10.1145/3219752.  [27]  Hu Y., and B. Si. 2018. A reinforcement learning neural network for robotic  manipulator control. Neural Computation 30(7):1983-2004. doi  10.1162/neco_a_01079.  [28]  Mei, Y., G.-Z. Tan, Z.-T. Liu, and H. Wu. 2018. Chaotic time series prediction  based on brain emotional learning model and self-adaptive genetic algorithm.  Acta Physica Sinica 67(8). doi 10.7498/aps.67.20172104.  [29]  Hong, Zhang-Wei, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and ChunYi Lee. 2018. A deep policy inference Q-network for multi-agent systems. In  Proc. of the 17th International Conference on Autonomous Agents and  Multiagent Systems (AAMAS 2018), Stockholm, Sweden, July 10\u201315, 2018.   [30]  Xiong, Yanhai, Haipeng Chen, Mengchen Zhao, and Bo An. 2018. HogRider:  Champion agent of Microsoft Malmo collaborative AI challenge. The ThirtySecond AAAI Conference on Artificial Intelligence (AAAI-18), pp. 4767-4774.  [31]  Wu, Weiguo, and Liyang Gao. 2017. Posture self-stabilizer of a biped robot based  on training platform and reinforcement learning. Robotics and Autonomous  Systems 98:42-55. doi 10.1016/j.robot.2017.09.001.  [32]  Boushaba, Mustapha, Abdelhakim Hafid, and Michel Gendreau. 2017. Node  stability-based routing in wireless mesh networks. Journal of Network and  Computer Applications 93:1\u201312. doi 10.1016/j.jnca.2017.02.010.  [33]  Chasparis, Georgios C. 2017. Stochastic stability analysis of perturbed learning  Automata with constant step-size in strategic-form games. American Control  Conference (ACC), Seattle, WA, 2017, pp. 4607-4612.  [34]  Prins, Noeline W., Justin C. Sanchez and Abhishek Prasad. 2017. Feedback for  reinforcement learning based brain-machine interfaces using confidence metrics.  Journal of Neural Engineering 14 036016. doi 10.1088/1741-2552/aa6317.  [35]  Song, Ruizhuo, Frank L. Lewis, and Qinglai Wei. 2017. Off-policy integral  reinforcement learning method to solve nonlinear continuous-time multiplayer  PREPRINT    43  nonzero-sum games. IEEE Transactions on Neural Networks and Learning  Systems 28(3). doi 10.1109/TNNLS.2016.2582849.  [36]  Yousefian, Reza, Amireza Sahami, and Sukumar Kamalasadan. 2017. Hybrid  transient energy function-based real-time optimal wide-area damping controller.  IEEE Transactions on Industry Applications 53(2). doi  10.1109/TIA.2016.2624264.  [37]  Tatari, Farzaneh, Mohammad-Bagher Naghibi-Sistani, and Kyriakos G.  Vamvoudakis. 2015. Distributed learning algorithm for non-linear differential  graphical games. Transactions of the Institute of Measurement and Control 1\u201310.   [38]  Lu, C., J. Huang, and J. Gong. 2016. Reinforcement learning for ramp control: an  analysis of learning parameters. Promet \u2013 Traffic & Transportation 28(4):371381.  [39]  Vamvoudakis, Kyriakos G. 2016. Optimal trajectory output tracking control with  a Q-learning algorithm. Proceedings of the American Control Conference, pp.  5752-5757. doi 10.1109/ACC.2016.7526571.  [40]  R\u00eago, Patr\u00edcia Helena Moraes, Jo\u00e3o Viana da Fonseca Neto, and Ernesto M.  Ferreira. 2013. Convergence of the standard RLS method and UDUT  factorisation of covariance matrix for solving the algebraic Riccati equation of the  DLQR via heuristic approximate dynamic programming. International Journal of  Systems Science. doi 10.1080/00207721.2013.844283.  [41]  Liu, Derong, and Qinglai Wei. 2014. Policy iteration adaptive dynamic  programming algorithm for discrete-time nonlinear systems. IEEE Transactions  on Neural Networks and Learning Systems 25(3).  [42]  Alharbi, Amal, Abdullah Al-Dhalaan, and Miznah Al-Rodhaan. 2014. Q-routing  in cognitive packet network routing protocol for MANETs. In Proceedings of the  International Conference on Neural Computation Theory and Applications  (NCTA-2014), pp. 234-243. doi 10.5220/0005082902340243.  [43]  Yousefian, Reza, and Sukumar Kamalasadan. 2014. An approach for real-time  tuning of cost functions in optimal system-centric wide area controller based on  adaptive critic design. IEEE Power and Energy Society General Meeting,  October 2014. doi 10.1109/PESGM.2014.6939224.  [44]  Dong, Bo, and Yuanchun Li. 2013. Decentralized reinforcement learning robust  optimal tracking control for time varying constrained reconfigurable modular  PREPRINT    44  robot based on ACI and \ud835\udc44-function. Mathematical Problems in Engineering 2013,  Article 387817, 16 pages. doi 10.1155/2013/387817.  [45]  Teixeira, Carlos, Lino Costa, and Cristina Santos. 2014. Biped locomotion -  improvement and adaptation. IEEE International Conference on Autonomous  Robot Systems and Competitions (ICARSC), May 14-15, Espinho, Portugal.   [46]  Luy, N.T., N.T. Thanh, H.M. Tri. 2014. Reinforcement learning-based intelligent  tracking control for wheeled mobile robot. Transactions of the Institute of  Measurement and Control 36(7):868\u2013877. doi 10.1177/0142331213509828.  [47]  Hager, Louw vS., Kenneth R. Uren, and George van Schoor. 2014. Series-Parallel  Approach to On-line Observer based Neural Control of a Helicopter System.  Proceedings of the 19th World Congress the International Federation of  Automatic Control, Cape Town, South Africa. August 24-29, 2014.   [48]  Wei, Qinglai, and Derong Liu. 2014. Data-driven neuro-optimal temperature  control of water-gas shift reaction using stable iterative adaptive dynamic  programming. IEEE Transactions on Industrial Electronics 61(11).  [49]  Zhao, D., B. Wang, and D. Liu. 2013. A supervised Actor-Critic approach for  adaptive cruise control. Soft Comput 17:2089\u20132099. doi 10.1007/s00500-013-1110y.  [50]  Kashki, M., M.A. Abido, and Y.L. Abdel-Magid. 2013. Power system dynamic  stability enhancement using optimum design of PSS and static phase shifter  based stabilizer. Arab J Sci Eng 38:637\u2013650. doi 10.1007/s13369-012-0325-z.  [51]  Li, Cai, Robert Lowe, and Tom Ziemke. 2013. Humanoids learning to walk: a  natural CPG-actor-critic architecture. Frontiers in Neurorobotics 7, Article 5. doi  10.3389/fnbot.2013.00005.  [52]  Vamvoudakis, K.G., F.L. Lewis, and G.R. Hudas. 2012. Multi-agent differential  graphical games: Online adaptive learning solution for synchronization with  optimality. Automatica 48(8):1598-1611. doi 10.1016/j.automatica.2012.05.074.  [53]  Moradi, P., M.E. Shiri, A.A. Rad, A. Khadivi, and M. Hasler. 2012. Automatic  skill acquisition in reinforcement learning using graph centrality measures.  Intelligent Data Analysis 16(1):113-135. doi 10.3233/IDA-2011-0513.  [54]  Bhasin, Shubhendu, Nitin Sharma, Parag Patre, and Warren Dixon. 2011.  Asymptotic tracking by a reinforcement learning-based adaptive critic controller.  J Control Theory Appl 9(3):400-409. doi 10.1007/s11768-011-0170-8.  PREPRINT    45  [55]  Hafner, Roland, and Martin Riedmiller. 2011. Reinforcement learning in feedback  control: Challenges and benchmarks from technical process control. Machine  Learning 84:137-169. doi 10.1007/s10994-011-5235-x.  [56]  Luy, Nguyen Tan. 2012. Reinforcement learning-based tracking control for  wheeled mobile robot. IEEE International Conference on Systems, Man, and  Cybernetics, October 14-17, 2012, Seoul, Korea.  [57]  Shih, Peter, Brian C. Kaul, Sarangapani Jagannathan, and James A. Drallmeier.  2009. Reinforcement-learning-based output-feedback control of nonstrict  nonlinear discrete-time systems with application to engine emission control. IEEE  Transactions on Systems, Man, and Cybernetics\u2014Part B: Cybernetics 39(5).  [58]  Boada, M.J.L., B.L. Boada, A.G. Bab\u00e9, J.A. Calvo Ramos, and V.D. L\u00f3pez.  2009. Active roll control using reinforcement learning for a single unit heavy  vehicle. International Journal of Heavy Vehicle Systems 16(4):412-430. doi  10.1504/IJHVS.2009.027413.  [59]  Guo, L., Y. Zhang, and J.-L. Hu. 2007. Adaptive HVDC supplementary (lamping  controller based on reinforcement learning. Electric Power Automation  Equipment 27(10):87-91.  [60]  Lin, C.-K. 2003. A reinforcement learning adaptive fuzzy controller for robots.  Fuzzy Sets and Systems 137(3):339-352. doi 10.1016/S0165-0114(02)00299-3.  [61]  Jagannathan, S. 2002. Adaptive critic neural network-based controller for  nonlinear systems. Proceedings of the 2002 IEEE International Symposium on  Intelligent Control, Vancouver, Canada, October 27-30, 2002.  [62]  Kaygisiz, Burak H., Aydan M. Erkmen, and Ismet Erkmen. 2002. Smoothing  stability roughness of fractal boundaries using reinforcement learning. IFAC  Proceedings Volumes 15(1):481-485.  [63]  Li, J.N., J.L. Ding, T.Y. Chai, and F.L. Lewis. 2020. Nonzero-Sum Game  Reinforcement Learning for Performance Optimization in Large-Scale Industrial  Processes. IEEE Transactions on Cybernetics 50(9):4132-4145. Doi  10.1109/TCYB.2019.2950262.  [64]  Zhang, K., H.G. Zhang, Y.L. Cai, and R. Su. 2020. Parallel Optimal Tracking  Control Schemes for Mode-Dependent Control of Coupled Markov Jump Systems  via Integral RL Method. IEEE Transactions on Automation Science and  Engineering 17(3):1332-1342. Doi 10.1109/TASE.2019.2948431.  PREPRINT    46  [65]  Zhang, Qian, Kui Wu, and Yang Shi. 2020. Route Planning and Power  Management for PHEVs With Reinforcement Learning. IEEE Transactions on  Vehicular Technology 69(5):4751-4762. doi 10.1109/TVT.2020.2979623.  [66]  Serhani, Abdellatif, Najib Naja, and Abdellah Jamali. 2020. AQ-Routing:  mobility-, stability-aware adaptive routing protocol for data routing in MANETIoT systems. Cluster Computing 23(1):13-27. Doi 10.1007/s10586-019-02937-x.  [67]  Dong, Zhe, Xiaojin Huang, Yujie Dong, and Zuoyi Zhang. 2020. Multilayer  perception based reinforcement learning supervisory control of energy systems  with application to a nuclear steam supply system. Applied Energy 259. doi  10.1016/j.apenergy.2019.114193.  [68]  Wang, Qingling. 2020. Integral Reinforcement Learning Control for a Class of  High-Order Multivariable Nonlinear Dynamics with Unknown Control  Coefficients. IEEE Access 8:86223-86229. Doi 10.1109/ACCESS.2020.2993265.  [69]  Zhang J., Z. Peng, J. Hu, Y. Zhao, R. Luo, B.K. Ghosh. 2020. Internal  reinforcement adaptive dynamic programming for optimal containment control of  unknown continuous-time multi-agent systems. Neurocomputing 413:85-95. doi  10.1016/j.neucom.2020.06.106.  [70]  Mitriakov, A., P. Papadakis, S.M. Nguyen, and S. Garlatti. 2020. Staircase  traversal via reinforcement learning for active reconfiguration of assistive robots.  Proceedings IEEE International Conference on Fuzzy Systems. doi  10.1109/FUZZ48607.2020.9177581.  [71]  Lv, Y., X. Ren, and J. Na. 2020. Online Nash-optimization tracking control of  multi-motor driven load system with simplified RL scheme. ISA Transactions  98:251-262. doi 10.1016/j.isatra.2019.08.025.  [72]  Thornton, C.E., M.A. Kozy, R.M. Buehrer, A.F. Martone, and K.D. Sherbondy.  2020. Deep Reinforcement Learning Control for Radar Detection and Tracking in  Congested Spectral Environments. IEEE Transactions on Cognitive  Communications and Networking. doi 10.1109/TCCN.2020.3019605.  [73]  Prasanna, John Deva, John Aravindhar, P. Sivasankar, and K. Perumal. 2020.  Reinforcement learning based virtual backbone construction in manet using  connected dominating sets. Journal of Critical Reviews 7(9):146-152. doi  10.31838/jcr.07.09.28.  PREPRINT    47  [74]  Pongfai, J., X. Su, H. Zhang, and W. Assawinchaichote. 2020. PID controller  autotuning design by a deterministic Q-SLP algorithm. IEEE Access 8:5001050021. doi 10.1109/ACCESS.2020.2979810.  [75]  Hoppe, Sabrina, and Marc Toussaint. 2020. Q graph-bounded Q-learning:  Stabilizing Model-Free O-Policy Deep Reinforcement Learning.  arXiv:2007.07582v1 [cs.LG] 15 Jul 2020.  [76]  Osinenko, Pavel, Lukas Beckenbach, Thomas G\u00f6hrt, and Stefan Streif. 2020. A  reinforcement learning method with closed-loop stability guarantee.  arXiv:2006.14034v1 [math.OC] 24 Jun 2020.  [77]  Han, Minghao, Lixian Zhang, Jun Wang, and Wei Pan. 2020. Actor-Critic  Reinforcement Learning for Control with Stability Guarantee.  arXiv:2004.14288v3 [cs.RO] 15 Jul 2020.  [78]  Khader, Shahbaz A., Hang Yin, Pietro Falco and Danica Kragic. 2020. StabilityGuaranteed Reinforcement Learning for Contact-rich Manipulation.  arXiv:2004.10886v2 [cs.RO] 27 Sep 2020.  [79]  Han, Minghao, Yuan Tian, Lixian Zhang, Jun Wang, and Wei Pan. 2020. H  infinity Model-free Reinforcement Learning with Robust Stability Guarantee.  arXiv:1911.02875v3 [cs.LG] 25 Jul 2020.  [80]  Tessler, Chen, Nadav Merlis, and Shie Mannor. Stabilizing Deep Reinforcement  Learning with Conservative Updates. arXiv:1910.01062v2 [cs.LG] 9 Feb 2020.    Robustness    [81]  Abuzainab, Nof, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Yi Shi,  Sharon J. Mackey, Mitesh Patel, Frank Panettieri, Muhammad A. Qureshi,  Volkan Isler, and Aylin Yener. 2019. QoS and jamming-aware wireless  networking using deep reinforcement learning. arXiv:1910.05766v1 [cs.NI] 13  October 2019.  [82]  Ahn, Michael. 2019. ROBEL: Robotics benchmarks for learning with low-cost  robots. arXiv:1909.11639v3 [cs.RO] 16 Dec 2019.  [83]  Dhiman, Vikas, Shurjo Banerjee, Brent Griffin, Jeffrey M Siskind, and Jason J  Corso. 2019. A critical investigation of deep reinforcement learning for  navigation. arXiv:1802.02274v2 [cs.RO] 4 Jan 2019.  PREPRINT    48  [84]  Naderializadeh, Navid, Jaroslaw Sydir, Meryem Simsek, Hosein Nikopour, and  Shilpa Talwar. 2019. When multiple agents learn to schedule: a distributed radio  resource management framework. arXiv:1906.08792v1 [cs.LG] 20 Jun 2019.  [85]  Nguyen, Khanh, Hal Daum\u00e9 III, and Jordan Boyd-Graber. 2017. Reinforcement  learning for bandit neural machine translation with simulated human feedback.  arXiv:1707.07402v4 [cs.CL] 11 Nov 2017.  [86]  Talele, Nihar, and Katie Byl. 2019. Mesh-based tools to analyze deep  reinforcement learning policies for underactuated biped locomotion.  arXiv:1903.12311v2 [cs.RO] 1 Nov 2019.  [87]  Turchetta, Matteo, Andreas Krause, and Sebastian Trimpe. 2019. Robust modelfree reinforcement learning with multi-objective Bayesian optimization.  arXiv:1910.13399v1 [cs.RO] 29 Oct 2019.  [88]  Yuan, Ye, and Kris Kitani. 2019. Ego-pose estimation and forecasting as realtime PD control. arXiv:1906.03173v2 [cs.CV] 4 Aug 2019.  [89]  Muneeswari, B., and M.S.K. Manikandan. 2019. Energy efficient clustering and  secure routing using reinforcement learning for three-dimensional mobile ad hoc  networks. IET Communications 13(12):1828-1839. doi 10.1049/iet-com.2018.6150.  [90]  Zhao, B., D. Wang, G. Shi, D.R. Liu, and Y.C. Li. 2018. Decentralized control  for large-scale nonlinear systems with unknown mismatched interconnections via  policy iteration. IEEE Transactions on Systems Man Cybernetics-Systems 48(10).  [91]  Zhang, Y., Y. Yang, S.X. Ding, and L.L. Li. 2016. Optimal design of residualdriven dynamic compensator using iterative algorithms with guaranteed  convergence. IEEE Transactions on Systems, Man, and Cybernetics: Systems  46(4). doi 10.1109/TSMC.2015.2450203.  [92]  Tokic, M. 2010. Adaptive epsilon-greedy exploration in reinforcement learning  based on value differences. Lecture Notes in Artificial Intelligence, 33rd Annual  German Conference on Artificial Intelligence, Karlsruhe, Germany, Sep 21-24,  2010.  [93]  Xiong, Y., L. Guo, Y. Huang, and L. Chen. 2020. Intelligent thermal control  strategy based on reinforcement learning for space telescope. Journal of  Thermophysics and Heat Transfer 34(1):37-44.  [94]  Isa-Jara, R.F., G.J. Meschino, and V.L. Ballarin. 2020. A comparative study of  reinforcement learning algorithms applied to medical image registration. IFMBE  Proceedings, pp. 281-289.  PREPRINT    49  [95]  Guo, F., X. Zhou, J. Liu, Y. Zhang, D. Li, and H. Zhou. 2019. A reinforcement  learning decision model for online process parameters optimization from offline  data in injection molding. Applied Soft Computing Journal 85. doi  10.1016/j.asoc.2019.105828.  [96]  Li, S., C. He, M. Liu, Y. Wan, Y. Gu, J. Xie, S. Fu, and K. Lu. 2019. Design and  implementation of aerial communication using directional antennas: Learning  control in unknown communication environments. IET Control Theory and  Applications 13(17):2906-2916. doi 10.1049/iet-cta.2018.6252.  [97]  Tang, X., Z. Qin, F. Zhang, Z. Wang, Z. Xu, Y. Ma, H. Zhu, and J. Ye. 2019. A  deep value-network based approach for multi-driver order dispatching.  Proceedings of the ACM SIGKDD International Conference on Knowledge  Discovery and Data Mining, pp. 1780-1790. doi 10.1145/3292500.3330724.  [98]  Chowdhury, A., S.A. Raut, and H.S. Narman. 2019. DA-DRLS: Drift adaptive  deep reinforcement learning based scheduling for IoT resource management.  Journal of Network and Computer Applications 138:51-65. doi  10.1016/j.jnca.2019.04.010.  [99]  Wang, X., C. Li, L. Yu, L. Han, X. Deng, E. Yang, and P. Ren. 2019. UAV first  view landmark localization with active reinforcement learning. Pattern  Recognition Letters 125:549-555. doi 10.1016/j.patrec.2019.03.011.  [100] L\u00fctjens, B., M. Everett, and J.P. How. 2019. Safe reinforcement learning with  model uncertainty estimates. Proceedings - IEEE International Conference on  Robotics and Automation, pp. 8662-8668. doi 10.1109/ICRA.2019.8793611.  [101] Balakrishnan, A., and J.V. Deshmukh. 2019. Structured reward functions using  STL. Proceedings of the 2019 22nd ACM International Conference on Hybrid  Systems: Computation and Control, pp. 270-271. doi 10.1145/3302504.3313355.  [102] Tang, C., W. Zhu, and X. Yu. 2019. Deep hierarchical strategy model for multisource driven quantitative investment. IEEE Access 7:79331-79336. doi  10.1109/ACCESS.2019.2923267.  [103] Cheng, Q., X. Wang, Y. Niu, and L. Shen. 2019. Reusing source task knowledge  via transfer approximator in reinforcement transfer learning. Symmetry 11(1). doi  10.3390/sym11010025.  [104] Jeon, Y.-S., H. Lee, and N. Lee. 2018. Robust MLSD for wideband SIMO systems  with one-bit ADCs: reinforcement-learning approach. Proceedings - 2018 IEEE  PREPRINT    50  International Conference on Communications Workshops, ICC Workshops, pp.  1-6. doi 10.1109/ICCW.2018.8403665.  [105] Yang, X., and H. He. 2018. Self-learning robust optimal control for continuoustime nonlinear systems with mismatched disturbances. Neural Networks 99:19-30.  doi 10.1016/j.neunet.2017.11.022.  [106] Jiang, H., H. Zhang, Y. Cui, and G. Xiao. 2018. Robust control scheme for a  class of uncertain nonlinear systems with completely unknown dynamics using  data-driven reinforcement learning method. Neurocomputing 273:68-77. doi  10.1016/j.neucom.2017.07.058.  [107] Shayeghi, H., and A. Younesi. 2017. An online Q-learning based multi-agent LFC  for a multi-area multi-source power system including distributed energy  resources. Iranian Journal of Electrical and Electronic Engineering 13(4):385398. doi 10.22068/IJEEE.13.4.385.  [108] Zhao, D., Y. Ma, Z. Jiang, and Z. Shi. 2017. Multiresolution airport detection via  hierarchical reinforcement learning saliency model. IEEE Journal of Selected  Topics in Applied Earth Observations and Remote Sensing 10(6):2855-2866. doi  10.1109/JSTARS.2017.2669335.  [109] Tow, A.W., S. Shirazi, J. Leitner., N. S\u00fcnderhauf, M. Milford, and B. Upcroft.  2016. A robustness analysis of deep Q networks. Australasian Conference on  Robotics and Automation, pp. 116-125.  [110] Hatami, E., and H. Salarieh. 2015. Adaptive critic-based neuro-fuzzy controller  for dynamic position of ships. Scientia Iranica 22(1):272-280.  [111] Xiang, J., and Z. Chen. 2015. Adaptive traffic signal control of bottleneck  subzone based on grey qualitative reinforcement learning algorithm. ICPRAM  2015 - 4th International Conference on Pattern Recognition Applications and  Methods, Proceedings, 2:295-301.  [112] Padmanabhan, R., N. Meskin, and W.M. Haddad. 2015. Closed-loop control of  anesthesia and mean arterial pressure using reinforcement learning. Biomedical  Signal Processing and Control 22:54-64. doi 10.1016/j.bspc.2015.05.013.  [113] Bruno, R., A. Masaracchia, and A. Passarella. 2014. Robust adaptive modulation  and coding (AMC) selection in LTE systems using reinforcement learning. IEEE  Vehicular Technology Conference. doi 10.1109/VTCFall.2014.6966162.  PREPRINT    51  [114] Jamali, N., P. Kormushev, S.R. Ahmadzadeh, and D.G. Caldwell. 2014.  Covariance analysis as a measure of policy robustness. OCEANS 2014 \u2013 Taipei.  doi 10.1109/OCEANS-TAIPEI.2014.6964339.  [115] Tati, S., S. Silvestri, T. He, and T.L. Porta. 2014. Robust network tomography  in the presence of failures. Proceedings - International Conference on Distributed  Computing Systems, pp. 481-492. doi 10.1109/ICDCS.2014.56.  [116] Luy, N.T., N.T. Thanh, and H.M. Tri. 2013. Reinforcement learning-based robust  adaptive tracking control for multi-wheeled mobile robots synchronization with  optimality. Proceedings of the 2013 IEEE Workshop on Robotic Intelligence in  Informationally Structured Space, pp. 74-81. doi 10.1109/RiiSS.2013.6607932.  [117] Kashki, M., M.A. Abido, and Y.L. Abdel-Magid. 2013. Power system dynamic  stability enhancement using optimum design of PSS and static phase shifter  based stabilizer. Arabian Journal for Science and Engineering 38(3):637-650. doi  10.1007/s13369-012-0325-z.  [118] Lopes, M., T. Lang, M. Toussaint, and P.-Y. Oudeyer. 2012. Exploration in  model-based reinforcement learning by empirically estimating learning progress.  Advances in Neural Information Processing Systems 1:206-214.  [119] Llorente, M.S., and S.E. Guerrero. 2012. Increasing retrieval quality in  conversational recommenders. IEEE Transactions on Knowledge and Data  Engineering 24(10):1876-1888. doi 10.1109/TKDE.2011.116.  [120] Maes, F., L. Wehenkel, and D. Ernst. 2012. Learning to play K-armed bandit  problems. Proceedings of the 4th International Conference on Agents and  Artificial Intelligence, pp. 74-81.  [121] Bhasin, S., N. Sharma, P. Patre, and W. Dixon. 2011. Asymptotic tracking by a  reinforcement learning-based adaptive critic controller. Journal of Control Theory  and Applications 9(3):400-409. doi 10.1007/s11768-011-0170-8.  [122] Tjahjadi, A., S. Sendari, S. Mabu, and K. Hirasawa. 2010. Robustness analysis of  genetic network programming with reinforcement learning. Proceedings Joint 5th  International Conference on Soft Computing and Intelligent Systems and 11th  International Symposium on Advanced Intelligent Systems, pp. 594-601.  [123] Kulkarni, S.A., and G.R. Rao. 2010. Vehicular ad hoc network mobility models  applied for reinforcement learning routing algorithm. Communications in  Computer and Information Science, pp. 230-240. doi 10.1007/978-3-642-148255_20.  PREPRINT    52  [124] Molina, C., N.B. Yoma, F. Huenup\u00e1n, C. Garret\u00f3n, and J. Wuth. 2010.  Maximum entropy-based reinforcement learning using a confidence measure in  speech recognition for telephone speech. IEEE Transactions on Audio, Speech  and Language Processing 18(5):1041-1052. doi 10.1109/TASL.2009.2032618.  [125] Luy, N.T., N.D. Thanh, N.T. Thanh, and N.T.P. Ha. 2010. Robust reinforcement  learning-based tracking control for wheeled mobile robot. The 2nd International  Conference on Computer and Automation Engineering, pp. 171-176. doi  10.1109/ICCAE.2010.5451973.  [126] Heidrich-Meisner, Verena, and Christian Igel. 2009. Hoeffding and Bernstein  races for selecting policies in evolutionary direct policy search. Proceedings of the  26th International Conference on Machine Learning, pp. 401-408.  [127] Satoh, H. 2008. A nonlinear approach to robust routing based on reinforcement  learning with state space compression and adaptive basis construction. IEICE  Transactions on Fundamentals of Electronics, Communications and Computer  Sciences 7:1733-1740. doi 10.1093/ietfec/e91-a.7.1733.  [128] Conn, K., and R.A. Peters. 2007. Reinforcement learning with a supervisor for a  mobile robot in a real-world environment. Proceedings of the 2007 IEEE  International Symposium on Computational Intelligence in Robotics and  Automation, pp. 73-78. doi 10.1109/CIRA.2007.382878.  [129] Wang, X.-S., Y.-H. Cheng, and W. Sun. 2007. A proposal of adaptive PID  controller based on reinforcement learning. Journal of China University of  Mining and Technology 17(1):40-44. doi 10.1016/S1006-1266(07)60009-1.  [130] Leem, JoonBum, and Ha Young Kim. 2020. Action-specialized expert ensemble  trading system with extended discrete action space using deep reinforcement  learning. PLOS One 15(7). doi 10.1371/journal.pone.0236178.  [131] Xiong, Yan, Liang Guo, Yong Huang, and Liheng Chen. 2020. Intelligent  Thermal Control Strategy Based on Reinforcement Learning for Space Telescope.  Journal of Thermophysics and Heat Transfer 34(1):37-44. doi 10.2514/1.T5774.  [132] Balakrishnan, Anand, and Jyotirmoy V. Deshmukh. 2019. Structured Reward  Functions using STL. Proceedings of the 2019 22nd ACM International  Conference on Hybrid Systems: Computation and Control (HSCC '19), pp. 270271. doi 10.1145/3302504.3313355.  PREPRINT    53  [133] Chen, G., S. Yao, J. Ma, L. Pan, Y. Chen, P. Xu, J. Ji, and X. Chen. 2020.  Distributed Non-Communicating Multi-Robot Collision Avoidance via MapBased Deep Reinforcement Learning. Sensors 20(17). doi 10.3390/s20174836.  [134] Sun, C., X. Li, and C. Belta. 2020. Automata Guided Semi-Decentralized MultiAgent Reinforcement Learning. Proceedings of the American Control  Conference, pp. 3900-3905. doi 10.23919/ACC45564.2020.9147704.  [135] Wang, X., and X. Ye. 2020. Optimal Robust Control of Nonlinear Uncertain  System via Off-Policy Integral Reinforcement Learning. Proceedings of the  Chinese Control Conference, pp. 1928-1933. doi  10.23919/CCC50068.2020.9189626.  [136] Yan, Z., J. Ge, Y. Wu, L. Li, and T. Li. 2020. Automatic virtual network  embedding: A deep reinforcement learning approach with graph convolutional  networks. IEEE Journal on Selected Areas in Communications 38(6):1040-1057.  doi 10.1109/JSAC.2020.2986662.  [137] Alhazmi, K., and S.M. Sarathy. 2020. Continuous Control of Complex Chemical  Reaction Network with Reinforcement Learning. Proceedings of the European  Control Conference 2020, ECC 2020, pp. 1066-1068.  [138] Ghasemkhani, A., A. Darvishi, I. Niazazari, A. Darvishi, H. Livani, and L. Yang.  2020. DeepGrid: Robust Deep Reinforcement Learning-based Contingency  Management. Proceedings of the 2020 IEEE Power and Energy Society  Innovative Smart Grid Technologies Conference, ISGT 2020. doi  10.1109/ISGT45199.2020.9087633.  [139] Pitti, A., M. Quoy, C. Lavandier, and S. Boucenna. 2020. Gated spiking neural  network using Iterative Free-Energy Optimization and rank-order coding for  structure learning in memory sequences (INFERNO GATE). Neural Networks  121:242-258. doi 10.1016/j.neunet.2019.09.023.  [140] Vecerik, Mel, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile  Pevceviciute, Thomas Roth\u00f6rl, Christopher Schuster, Raia Hadsell, Lourdes  Agapito, and Jonathan Scholz. 2020. S3K: Self-Supervised Semantic Keypoints  for Robotic Manipulation via Multi-View Consistency. arXiv:2009.14711.  [141] Jiao, Yusheng, Feng Ling, Sina Heydari, Nicolas Heess, Josh Merel, and Eva  Kanso. 2020. Learning to swim in potential flow. arXiv:2009.14280.  PREPRINT    54  [142] Alm\u00e1si, P\u00e9ter, R\u00f3bert Moni, and B\u00e1lint Gyires-T\u00f3th. 2020. Robust  Reinforcement Learning-based Autonomous Driving Agent for Simulation and  Real World. arXiv:2009.11212.  [143] Ding, Wenhao, Baiming Chen, Bo Li, Kim Ji Eun, and Ding Zhao. 2020.  Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms  Evaluation. arXiv:2009.08311.  [144] Schamberg, Gabe, Marcus Badgeley, and Emery N. Brown. 2020. Controlling  Level of Unconsciousness by Titrating Propofol with Deep Reinforcement  Learning. arXiv:2008.12333.  [145] Pang, Bo, and Zhong-Ping Jiang. 2020. Robust Reinforcement Learning: A Case  Study in Linear Quadratic Regulation. arXiv:2008.11592.  [146] Kobayashi, Taisuke, and Wendyam Eric Lionel Ilboudo. 2020. t-Soft Update of  Target Network for Deep Reinforcement Learning. arXiv:2008.10861.  [147] Zavoli, Alessandro, and Lorenzo Federici. 2020. Reinforcement Learning for  Low-Thrust Trajectory Design of Interplanetary Missions. arXiv:2008.08501.  [148] Limoyo, Oliver, Bryan Chan, Filip Mari\u0107, Brandon Wagstaff, Rupam Mahmood,  and Jonathan Kelly. 2020. Heteroscedastic Uncertainty for Robust Generative  Latent Dynamics. arXiv:2008.08157.  [149] Zhao, Wenshuai, Jorge Pe\u00f1a Queralta, Li Qingqing, and Tomi Westerlund. 2020.  Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot  Deep Reinforcement Learning. arXiv:2008.07875.  [150] Qu, Xinghua, Yew-Soon Ong, Abhishek Gupta, and Zhu Sun. 2020. Defending  Adversarial Attacks without Adversarial Attacks in Deep Reinforcement  Learning. arXiv:2008.06199.  [151] Swazinna, Phillip, Steffen Udluft, and Thomas Runkler. 2020. Overcoming Model  Bias for Robust Offline Deep Reinforcement Learning. arXiv:2008.05533.  [152] Ahmed, Ibrahim, Hamed Khorasgani, and Gautam Biswas. 2020. Comparison of  Model Predictive and Reinforcement Learning Methods for Fault Tolerant  Control. arXiv:2008.04403.  [153] Kova\u010d, Grgur, Adrien Laversanne-Finot, and Pierre-Yves Oudeyer. 2020.  GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep  Reinforcement Learning. arXiv:2008.04388.  PREPRINT    55  [154] Zhu, Jianxin Li, Hao Peng, Senzhang Wang, Philip S. Yu, and Lifang He. 2020.  Adversarial Directed Graph Embedding. arXiv:2008.03667.  [155] Ma, Xiao, Siwei Chen, David Hsu, and Wee Sun Lee. 2020. Contrastive  Variational Model-Based Reinforcement Learning for Complex Observations.  arXiv:2008.02430.  [156] Oikarinen, Tuomas, Tsui-Wei Weng, and Luca Daniel. 2020. Robust Deep  Reinforcement Learning through Adversarial Loss. arXiv:2008.01976.  [157] Vinitsky, Eugene, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and  Alexandre Bayen. 2020. Robust Reinforcement Learning using Adversarial  Populations. arXiv:2008.01825.  [158] Park, Hwangpil, Ri Yu, Yoonsang Lee, Kyungho Lee, and Jehee Lee. 2020.  Understanding the Stability of Deep Control Policies for Biped Locomotion.  arXiv:2007.15242.  [159] Steverson, Kai, Jonathan Mullin, and Metin Ahiskali. 2020. Adversarial  Robustness for Machine Learning Cyber Defenses Using Log Data.  arXiv:2007.14983.  [160] Chen, Xinwei, Tong Wang, Barrett W. Thomas, and Marlin W. Ulmer. 2020.  Same-Day Delivery with Fairness. arXiv:2007.09541.  [161] Chen, Xin, Yawen Duan, Zewei Chen, Hang Xu, Zihao Chen, Xiaodan Liang,  Tong Zhang, and Zhenguo Li. 2020. CATCH: Context-based Meta Reinforcement  Learning for Transferrable Architecture Search. arXiv:2007.09380.  [162] Zhang, Lin, Hao Xiong, Ou Ma, and Zhaokui Wang. 2020. Multi-robot  Cooperative Object Transportation using Decentralized Deep Reinforcement  Learning. arXiv:2007.09243.  [163] Tan, Kai Liang, Yasaman Esfandiari, Xian Yeow Lee, Aakanksha, and Soumik  Sarkar. 2020. Robustifying Reinforcement Learning Agents via Action Space  Adversarial Training. arXiv:2007.07176.  [164] Stooke, Adam, Joshua Achiam, and Pieter Abbeel. 2020. Responsive Safety in  Reinforcement Learning by PID Lagrangian Methods. arXiv:2007.03964.  [165] Abe, Kenshi, and Yusuke Kaneko. 2020. Off-Policy Exploitability-Evaluation and  Equilibrium-Learning in Two-Player Zero-Sum Markov Games. arXiv:2007.02141.  [166] Wang, Xiao, Saasha Nair, and Matthias Althoff. 2020. Falsification-Based Robust  Adversarial Reinforcement Learning. arXiv:2007.00691.  PREPRINT    56  [167] Lee, Heunchul, Maksym Girnyk, and Jaeseong Jeong. 2020. Deep reinforcement  learning approach to MIMO precoding problem: Optimality and Robustness.  arXiv:2006.16646.  [168] Xu, Duo, Mohit Agarwal, Ekansh Gupta, Faramarz Fekri, and Raghupathy  Sivakumar. 2020. Accelerating Reinforcement Learning Agent with EEG-based  Implicit Human Feedback. arXiv:2006.16498.  [169] Yu, Liang, Yi Sun, Zhanbo Xu, Chao Shen, Dong Yue, Tao Jiang, and Xiaohong  Guan. 2020. Multi-Agent Deep Reinforcement Learning for HVAC Control in  Commercial Buildings. arXiv:2006.14156.  [170] Gleave, Adam, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. 2020.  Quantifying Differences in Reward Functions. arXiv:2006.13900.  [171] Raileanu, Roberta, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob  Fergus. 2020. Automatic Data Augmentation for Generalization in Deep  Reinforcement Learning. arXiv:2006.12862.  [172] Liu, Haotian, and Wenchuan Wu. 2020. Online Multi-agent Reinforcement  Learning for Decentralized Inverter-based Volt-VAR Control. arXiv:2006.12841.  [173] Zou, Yayi, and Xiaoqi Lu. 2020. Gradient-EM Bayesian Meta-learning.  arXiv:2006.11764.  [174] Panaganti, Kishan, and Dileep Kalathil. 2020. Model-Free Robust Reinforcement  Learning with Linear Function Approximation. arXiv:2006.11608.  [175] Zhang, Amy, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey  Levine. 2020. Learning Invariant Representations for Reinforcement Learning  without Reconstruction. arXiv:2006.10742.  [176] Rahman, Arrasy, Niklas Hopner, Filippos Christianos, and Stefano V. Albrecht.  2020. Open Ad Hoc Teamwork using Graph-based Policy Learning.  arXiv:2006.10412.  [177] Jeong, Heejin, Hamed Hassani, Manfred Morari, Daniel D. Lee, and George J.  Pappas. 2020. Learning to Track Dynamic Targets in Partially Known  Environments. arXiv:2006.10190.  [178] Ning, Kun-Peng, and Sheng-Jun Huang. 2020. Reinforcement Learning with  Supervision from Noisy Demonstrations. arXiv:2006.07808.  [179] Dou, Yingtong, Guixiang Ma, Philip S. Yu, and Sihong Xie. 2020. Robust  Spammer Detection by Nash Reinforcement Learning. arXiv:2006.06069.  PREPRINT    57  [180] Huang, Xiaoshui, Fujin Zhu, Lois Holloway, and Ali Haidar. 2020. Causal  Discovery from Incomplete Data using An Encoder and Reinforcement Learning.  arXiv:2006.05554.  [181] Chow, Yinlam, Brandon Cui, MoonKyung Ryu, and Mohammad Ghavamzadeh.  2020. Variational Model-based Policy Optimization. arXiv:2006.05443.  [182] Jafferjee, Taher, Ehsan Imani, Erin Talvitie, Martha White, and Micheal  Bowling. 2020. Hallucinating Value: A Pitfall of Dyna-style Planning with  Imperfect Environment Models. arXiv:2006.04363.  [183] Tian, Yuan, Manuel Arias Chao, Chetan Kulkarni, Kai Goebel, and Olga Fink.  2020. Real-Time Model Calibration with Deep Reinforcement Learning.  arXiv:2006.04001.  [184] Kallus, Nathan, and Masatoshi Uehara. 2020. Efficient Evaluation of Natural  Stochastic Policies in Offline Reinforcement Learning. arXiv:2006.03886.  [185] Hou, Linfang, Liang Pang, Xin Hong, Yanyan Lan, Zhiming Ma, and Dawei Yin.  2020. Robust Reinforcement Learning with Wasserstein Constraint.  arXiv:2006.00945.  [186] Zhi, Jixuan, and Jyh-Ming Lien. 2020. Learning to Herd Agents Amongst  Obstacles: Training Robust Shepherding Behaviors using Deep Reinforcement  Learning. arXiv:2005.09476.  [187] Chandak, Yash, Georgios Theocharous, Shiv Shankar, Martha White, Sridhar  Mahadevan, and Philip S. Thomas. 2020. Optimizing for the Future in NonStationary MDPs. arXiv:2005.08158.  [188] Ding, Yiming, Ignasi Clavera, and Pieter Abbeel. 2020. Mutual Information  Maximization for Robust Plannable Representations. arXiv:2005.08114.  [189] Totaro, Simone, Ioannis Boukas, Anders Jonsson, and Bertrand Corn\u00e9lusse. 2020.  Lifelong Control of Off-grid Microgrid with Model Based Reinforcement  Learning. arXiv:2005.08006.  [190] Xie, Zhaoming, Hung Yu Ling, Nam Hee Kim, and Michiel van de Panne. 2020.  ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills.  arXiv:2005.04323.  [191] Singh, Rahul, Qinsheng Zhang, and Yongxin Chen. 2020. Improving Robustness  via Risk Averse Distributional Reinforcement Learning. arXiv:2005.00585.  PREPRINT    58  [192] Kostrikov, Ilya, Denis Yarats, and Rob Fergus. 2020. Image Augmentation Is All  You Need: Regularizing Deep Reinforcement Learning from Pixels.  arXiv:2004.13649.  [193] Chen, Jerry Zikun. 2020. Reinforcement Learning Generalization with Surprise  Minimization. arXiv:2004.12399.  [194] Ngo, Phuong D., and Fred Godtliebsen. 2020. Data-Driven Robust Control Using  Reinforcement Learning. arXiv:2004.07690.  [195] Everett, Michael, Bjorn Lutjens, and Jonathan P. How. 2020. Certified  Adversarial Robustness for Deep Reinforcement Learning. arXiv:2004.06496.  [196] Koren, Mark, and Mykel J. Kochenderfer. 2020. Adaptive Stress Testing without  Domain Heuristics using Go-Explore. arXiv:2004.04292.  [197] Anahtarci, Berkay, Can Deha Kariksiz, and Naci Saldi. 2020. Q-Learning in  Regularized Mean-field Games. arXiv:2003.12151.  [198] Lindenberg, Bj\u00f6rn, Jonas Nordqvist, and Karl-Olof Lindahl. 2020. Distributional  Reinforcement Learning with Ensembles. arXiv:2003.10903.   [199] Shen, Qianli, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao. 2020. Deep  Reinforcement Learning with Robust and Smooth Policy. arXiv:2003.09534.  [200] Zhang, Huan, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning,  and Cho-Jui Hsieh. 2020. Robust Deep Reinforcement Learning against  Adversarial Perturbations on State Observations. arXiv:2003.08938.  [201] Guo, Xin, Anran Hu, Renyuan Xu, and Junzi Zhang, 2020. A General  Framework for Learning Mean-Field Games. arXiv:2003.06069.  [202] Touati, Ahmed, Adrien Ali Taiga, and Marc G. Bellemare. 2020. Zooming for  Efficient Model-Free Reinforcement Learning in Metric Spaces. arXiv:2003.04069.  [203] Gao, Shen, Peihao Dong, Zhiwen Pan, and Geoffrey Ye Li. 2020. Reinforcement  Learning Based Cooperative Coded Caching under Dynamic Popularities in  Ultra-Dense Networks. arXiv:2003.03758.  [204] Lin, Jieyu, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and  Nicolas Papernot. 2020. On the Robustness of Cooperative Multi-Agent  Reinforcement Learning. arXiv:2003.03722.  [205] Derman, Esther, and Shie Mannor. 2020. Distributional Robustness and  Regularization in Reinforcement Learning. arXiv:2003.02894.  PREPRINT    59  [206] Spooner, Thomas, and Rahul Savani. Robust Market Making via Adversarial  Reinforcement Learning. arXiv:2003.01820.  [207] Chanc\u00e1n, Marvin, and Michael Milford. 2020. MVP: Unified Motion and Visual  Self-Supervised Learning for Large-Scale Robotic Navigation. arXiv:2003.00667.  [208] Ilboudo, Wendyam Eric Lionel, Taisuke Kobayashi, and Kenji Sugimoto. 2020.  TAdam: A Robust Stochastic Gradient Optimizer. arXiv:2003.00179.  [209] Tschantz, Alexander, Beren Millidge, Anil K. Seth, and Christopher L. Buckley.  2020. Reinforcement Learning through Active Inference. arXiv:2002.12636.  [210] Kuutti, Sampo, Saber Fallah, and Richard Bowden. 2020. Training Adversarial  Agents to Exploit Weaknesses in Deep Control Policies. arXiv:2002.12078.  [211] Nguyen, Ngoc Duy, Thanh Thi Nguyen, and Saeid Nahavandi. 2020. A Visual  Communication Map for Multi-Agent Deep Reinforcement Learning.  arXiv:2002.11882.  [212] Yang, Chao-Han Huck, Jun Qi, Pin-Yu Chen, Yi Ouyang, I-Te Danny Hung,  Chin-Hui Lee, and Xiaoli Ma. 2020. Enhanced Adversarial Strategically-Timed  Attacks against Deep Reinforcement Learning. arXiv:2002.09027.  [213] Sun, Tao, Han Shen, Tianyi Chen, and Dongsheng Li. 2020. Adaptive Temporal  Difference Learning with Linear Function Approximation. arXiv:2002.08537.  [214] Naderializadeh, Navid, Jaroslaw Sydir, Meryem Simsek, and Hosein Nikopour.  2020. Resource Management in Wireless Networks via Multi-Agent Deep  Reinforcement Learning. arXiv:2002.06215.  [215] Kamalaruban, Parameswaran, Yu-Ting Huang, Ya-Ping Hsieh, Paul Rolland,  Cheng Shi, and Volkan Cevher. 2020. Robust Reinforcement Learning via  Adversarial training with Langevin Dynamics. arXiv:2002.06063.  [216] Kallus, Nathan, and Masatoshi Uehara. 2020. Statistically Efficient Off-Policy  Policy Gradients. arXiv:2002.04014.  [217] Lee, Gilwoo, Brian Hou, Sanjiban Choudhury, and Siddhartha S. Srinivasa. 2020.  Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement  Learning with Clairvoyant Experts. arXiv:2002.03042.  [218] Pacelli, Vincent, and Anirudha Majumdar. 2020. Learning Task-Driven Control  Policies via Information Bottlenecks. arXiv:2002.01428.  [219] Yao, Jiahao, Marin Bukov, and Lin Lin. 2020. Policy Gradient based Quantum  Approximate Optimization Algorithm. arXiv:2002.01068.  PREPRINT    60  [220] Nishio, Daichi, Daiki Kuyoshi, Toi Tsuneda, and Satoshi Yamane. 2020.  Discriminator Soft Actor Critic without Extrinsic Rewards. arXiv:2001.06808.  [221] Dai, Tianhong, Kai Arulkumaran, Tamara Gerbert, Samyakh Tukra, Feryal  Behbahani, and Anil Anthony Bharath. 2020. Analysing Deep Reinforcement  Learning Agents Trained with Domain Randomisation. arXiv:1912.08324.  [222] Zhang, Xinglong, Jiahang Liu, Xin Xu, Shuyou Yu, and Hong Chen. 2020.  Learning-based Predictive Control for Nonlinear Systems with Unknown  Dynamics Subject to Safety Constraints. arXiv:1911.09827.  [223] Lykouris, Thodoris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. 2020.  Corruption robust exploration in episodic reinforcement learning.  arXiv:1911.08689.  [224] Salter, Sasha, Dushyant Rao, Markus Wulfmeier, Raia Hadsell, and Ingmar  Posner. 2020. Attention-Privileged Reinforcement Learning. arXiv:1911.08363.  [225] Han, Minghao. Yuan Tian, Lixian Zhang, Jun Wang, and Wei Pan. 2020. H\u221e  Model-free Reinforcement Learning with Robust Stability Guarantee.  arXiv:1911.02875.  [226] L\u00fctjens, Bj\u00f6rn, Michael Everett, and Jonathan P. How. 2020. Certified  Adversarial Robustness for Deep Reinforcement Learning. arXiv:1910.12908.  [227] Uehara, Masatoshi, Jiawei Huang, and Nan Jiang. 2020. Minimax Weight and QFunction Learning for Off-Policy Evaluation. arXiv:1910.12809.  [228] Li, Shuo, and Osbert Bastani. 2020. Robust Model Predictive Shielding for Safe  Reinforcement Learning with Stochastic Dynamics. arXiv:1910.10885.  [229] Slaoui, Reda Bahi, William R. Clements, Jakob N. Foerster, and S\u00e9bastien Toth.  2020. Robust Visual Domain Randomization for Reinforcement Learning.  arXiv:1910.10537.  [230] Zhang, Kaiqing, Bin Hu, and Tamer Ba\u015far. 2020. Policy Optimization for H2  Linear Control with H\u221e Robustness Guarantee: Implicit Regularization and  Global Convergence. arXiv:1910.09496.  [231] Liu, Zhuang, Xuanlin Li, Bingyi Kang, and Trevor Darrell. 2020. Regularization  Matters in Policy Optimization. arXiv:1910.09191.  [232] Yang, Jiachen, Brenden Petersen, Hongyuan Zha, and Daniel Faissol. 2020.  Single Episode Policy Transfer in Reinforcement Learning. arXiv:1910.07719.  PREPRINT    61  [233] Chen, Shuhang, Adithya M. Devraj, Fan Lu, Ana Bu\u0161i\u0107, and Sean P. Meyn.  2020. Zap Q-Learning with Nonlinear Function Approximation.  arXiv:1910.05405.  [234] Schwartz, Erez, Guy Tennenholtz, Chen Tessler, and Shie Mannor. 2020.  Language is Power: Representing States Using Natural Language in  Reinforcement Learning. arXiv:1910.02789.  [235] Yarats, Denis, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and  Rob Fergus. 2020. Improving Sample Efficiency in Model-Free Reinforcement  Learning from Images. arXiv:1910.01741.  [236] Kalweit, Gabriel, Maria Huegle, and Joschka Boedecker. 2020. Composite Qlearning: Multi-scale Q-function Decomposition and Separable Optimization.  arXiv:1909.13518.  [237] Ryu, Moonkyung, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, and  Craig Boutilier. 2020. CAQL: Continuous Action Q-Learning. arXiv:1909.12397.  [238] Li, Jiachen, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Iskov  Christensen, and Hao Su. 2020. Multi-task Batch Reinforcement Learning with  Metric Learning. arXiv:1909.11373.  [239] Shen, Macheng, and Jonathan P. How. 2020. Robust Opponent Modeling via  Adversarial Ensemble Reinforcement Learning in Asymmetric ImperfectInformation Games. arXiv:1909.08735.  [240] Kallus, Nathan, Masatoshi Uehara. 2020. Double Reinforcement Learning for  Efficient Off-Policy Evaluation in Markov Decision Processes. arXiv:1908.08526.  [241] Roy, Julien, Paul Barde, F\u00e9lix G. Harvey, Derek Nowrouzezahrai, and  Christopher Pal. 2020. Promoting Coordination through Policy Regularization in  Multi-Agent Deep Reinforcement Learning. arXiv:1908.02269.  [242] Urakami, Yusuke, Alec Hodgkinson, Casey Carlin, Randall Leu, Luca Rigazio,  and Pieter Abbeel. 2020. DoorGym: A Scalable Door Opening Environment and  Baseline Agent. arXiv:1908.01887.  [243] Wang, Qisheng, Keming Feng, Xiao Li, and Shi Jin. 2020. PrecoderNet: Hybrid  Beamforming for Millimeter Wave Systems with Deep Reinforcement Learning.  arXiv:1907.13266.  [244] Bogdanovic, Miroslav, Majid Khadiv, Ludovic Righetti. 2020. Learning Variable  Impedance Control for Contact Sensitive Tasks. arXiv:1907.07500.  PREPRINT    62  [245] Mankowitz, Daniel J., Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas  Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and  Martin Riedmiller. 2020. Robust Reinforcement Learning for Continuous Control  with Model Misspecification. arXiv:1906.07516.  [246] Li, Alexander C., Carlos Florensa, Ignasi Clavera, and Pieter Abbeel. Sub-policy  Adaptation for Hierarchical Reinforcement Learning. arXiv:1906.05862.  [247] Assran, Mahmoud, Joshua Romoff, Nicolas Ballas, Joelle Pineau, and Michael  Rabbat. 2020. Gossip-based Actor-Learner Architectures for Deep Reinforcement  Learning. arXiv:1906.04585.  [248] Gravell, Benjamin, Peyman Mohajerin Esfahani, and Tyler Summers. 2020.  Learning robust control for LQR systems with multiplicative noise via policy  gradient. arXiv:1905.13547.  [249] Francis, Anthony, Aleksandra Faust, Hao-Tien Lewis Chiang, Jasmine Hsu, J.  Chase Kew, Marek Fiser, and Tsang-Wei Edward Lee. 2020. Long-Range Indoor  Navigation with PRM-RL. arXiv:1902.09458.  [250] Wang, Jingkang, Yang Liu, and Bo Li. 2020. Reinforcement Learning with  Perturbed Rewards. arXiv:1810.01032.    Other    [251] Moher D, Liberati A, Tetzlaff J, Altman DG, The PRISMA Group. 2009.  Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The  PRISMA Statement. PLoS Med 6(7): e1000097. doi  10.1371/journal.pmed1000097  [252] Liu, Y., P. Ramachandran, Q. Liu, and J. Peng, Stein Variational Policy  Gradient. arXiv:1704.02399v1 [cs.LG] 7 April 2017.  [253] 5. Liang, G., Zhu, X., Zhang, C. An Empirical Study of Bagging Predictors for  Different Learning Algorithms. AAAI, pp. 1802\u20131803 (2011)  [254] Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas  and Schulman, John and Tang, Jie and Zaremba, Wojciech. Openai gym,  arXiv preprint arXiv:1606.01540, 2016.    ",
    "title": "Review of Metrics to Measure the Stability, Robustness and  Resilience of Reinforcement Learning ",
    "paper_info": "PREPRINT \n \n1 \nReview of Metrics to Measure the Stability, Robustness and \nResilience of Reinforcement Learning \n \nLaura L. Pullum, DSc \nComputer Science and Mathematics Division, Oak Ridge National Laboratory \npullumll@ornl.gov \n \nAbstract \n \nReinforcement learning (RL) has received significant interest in recent years, due \nprimarily to the successes of deep reinforcement learning at solving many challenging \ntasks such as playing Chess, Go and online computer games. However, with the \nincreasing focus on RL, applications outside of gaming and simulated environments \nrequire understanding the robustness, stability and resilience of RL methods. To this \nend, we characterize the available literature on these three behaviors as they pertain to \nRL. We classify the quantification approaches used, determine the objectives of the \ndesired behaviors, and provide a decision tree for selecting metrics to quantify the \nbehaviors. \n \n1.  Introduction \n \nRecent literature on the robustness of machine learning models has focused almost \nentirely on the robustness of deep neural networks for imaging applications. However, at \nthe time of this study, there are no published surveys on robustness of RL. With RL use \nincreasing, especially in control systems contexts, we pursued this review. Included \nalong with robustness are stability and resilience. Stability is included because the term \nhas been used interchangeably with robustness and resilience is included because the \nterm has been used as a state beyond robustness. \n \nRL involves agents which take actions in an environment and experience at a reward for \nthose actions. The agent is to learn a policy that maximizes the cumulative reward. \nFormally, consider an agent operating over time \ud835\udc61\t \u2208\t{1, \u2026 , \ud835\udc47}. At time t, the agent is in \nenvironment state st and produces an action \ud835\udc4e! \u2208 \ud835\udc34. The agent then observes a new \nstate st+1 and receive a reward \ud835\udc5f! \u2208 \ud835\udc45. The set of possible actions A can be discrete or \ncontinuous. The goal of reinforcement learning is to find a policy \ud835\udf0b(\ud835\udc4e!|\ud835\udc60!) for choosing \nan action in state st to maximize a utility function or (expected return). [252] \n \n",
    "GPTsummary": "- (1): The background of this article is to understand the robustness, stability and resilience of reinforcement learning in applications outside of gaming and simulated environments. \n\n- (2): The past methods have focused almost entirely on the robustness of deep neural networks for imaging applications, and there are no published surveys on robustness of reinforcement learning. The approach is well motivated due to the increasing use of RL, especially in control systems contexts. \n\n- (3): The research methodology proposed in this paper is a comprehensive literature review to characterize the available literature on these three behaviors as they pertain to reinforcement learning. The authors classify the quantitative and theoretical approaches used to indicate or measure robustness, stability, and resilience behaviors. \n\n- (4): The methods in this paper do not achieve any new performance on a specific task, but rather provide a decision tree useful for selecting metrics to quantify the behaviors of stability, robustness, and resilience in reinforcement learning. The comprehensive review of the literature supports their goals.\n\n\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to provide a comprehensive literature review to understand the behaviors of stability, robustness, and resilience in reinforcement learning. It aims to fill the gap in the literature on the concepts of robustness in reinforcement learning beyond gaming and simulated environments. \n\n- (2): The innovation point of this article lies in the comprehensive literature review and decision tree provided for selecting metrics to quantify stability, robustness, and resilience in reinforcement learning. However, the article does not achieve any new performance on a specific task. As for workload, the article involves a substantial amount of literature review and classification, which supports their goals.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to provide a comprehensive literature review to understand the behaviors of stability, robustness, and resilience in reinforcement learning. It aims to fill the gap in the literature on the concepts of robustness in reinforcement learning beyond gaming and simulated environments. \n\n- (2): The innovation point of this article lies in the comprehensive literature review and decision tree provided for selecting metrics to quantify stability, robustness, and resilience in reinforcement learning. However, the article does not achieve any new performance on a specific task. As for workload, the article involves a substantial amount of literature review and classification, which supports their goals.\n\n\n"
}