{
    "Abstract": "ABSTRACT Background: Deep reinforcement learning is actively used for training autonomous and adversarial car policies in a simulated driving environment. Due to the large availability of various reinforcement learning algorithms and the lack of their systematic comparison across different driving scenarios, we are unsure of which ones are more effective for training and testing autonomous car software in single-agent as well as multi-agent driving environments. Aims: A benchmarking framework for the comparison of deep reinforcement learning in a vision-based autonomous driving will open up the possibilities for training better autonomous car driving policies. Furthermore, autonomous cars trained on deep reinforcement learning-based algorithms are known for being vulnerable to adversarial attacks. To guard against adversarial attacks, we can train autonomous cars on adversarial driving policies. However, we lack the knowledge of which deep reinforcement learning algorithms would act as good adversarial agents able to effectively test autonomous cars. Method: To address these challenges, we provide an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous and adversarial driving in a single- and multi-agent environment. Using the framework, we perform a comparative study of five discrete and two continuous action space deep reinforcement learning algorithms. We run the experiments in a vision-only high fidelity urban driving simulated environments. Results: The results indicate that only some of the deep reinforcement learning algorithms perform consistently better across single and multi-agent scenarios when trained in a multi-agentonly setting. For example, A3C- and TD3-based autonomous cars perform comparatively better in terms of more robust actions and minimal driving errors in both single and multi-agent scenarios. PPO IMPALA from a discrete action space and DDPG TD3 from a continuous action space show to be effective algorithms for training adversarial agents and exposing failure scenarios in autonomous car software. Conclusions: We conclude that different DRL algorithms exhibit different driving and testing performance in different scenarios, which underlines the need for their systematic comparative analysis. The benchmarking framework proposed in this paper facilitates such a comparison. KEYWORDS deep reinforcement learning, multi-agent systems, autonomous cars, autonomous driving, adversarial reinforcement learning 1 ",
    "Introduction": "INTRODUCTION Autonomous cars (ACs) are complex decision-making systems that are unfortunately prone to errors [16]. They commonly use machine/deep learning algorithms as part of decision-making software, which are known to be difficult to validate [29, 40]. Therefore, ACs need comprehensive training and evaluation in order to minimize risk to the public. For the same reason, autonomous driving (AD) research is nowadays performed within simulated driving environments (also used by the state-of-the-art industrial solutions like Tesla [44] and Comma ai [43]), as they provide flexibility for testing and validating AD without posing any danger to the real world. However, we observe three specific challenges in training and validating ACs in the existing simulation environments. First, while the majority of AD research is focused on using deep reinforcement learning (DRL) for training ACs in a simulated urban driving scenario [35][47][6][4][39], there is a lack of comparison among DRL algorithms for vision-based urban driving scenarios. Having such a benchmark of commonly used DRL algorithms can be useful for understanding why some algorithms perform worse than others in specific driving scenarios, which can lead to the improvements of the state-of-art DRL algorithms for AD. Second, the majority of existing research trains ACs as noncommunicating and independent single intelligent agents [52][42], therefore treating the ACs as a single-agent driving problem. However, in the near future AD will be a complex multi-agent problem [20]. By bringing more than one AC into a multi-agent environment we can evaluate how a non-stationary driving scenario affects AC\u2019s control decisions when interacting with other ACs. Existing comparative analyses of RL algorithms for AD are still limited to single agent driving environments [46][48], and there is no systematic study performed yet on which DRL models work best for AD in a multi-agent environment. Third, a vast portion of existing research is actively focused on testing ACs trained on vision-based end-to-end systems. One of the ways to test their control behavior is using adversarial RL (ARL). In fact, DRL is proved to be vulnerable multiple times against adversarial attacks [45], and one of the solutions to this problem is to train the adversary as a separate physical driving model using DRL [8]. ARL can be used to not only find failure scenarios in ACs but also to improve their driving policies through retraining [49]. However, at the moment, we lack the support for training and systematically evaluating ARL algorithms to decide which ones are more effective in creating better adversarial AD agents able to find errors in ACs. arXiv:2112.11947v2  [cs.AI]  27 May 2022 ",
    "Related Work": "RELATED WORK In AD research, there are only few benchmarks for evaluating the performance of RL-based AD models. Vinitsky [48] proposes a benchmark for DRL in mixed-autonomy traffic. While the benchmark involves four scenarios: the figure eight network, the merge network, the grid, and the bottleneck, it evaluates a limited number of RL algorithm (two gradient-based and two gradient-free). In addition, the authors do not consider ARL in their work, although dealing with adversarial attacks is a great challenge in AD research. Finally, the proposed benchmark is specific to connected AD research. Stang [46] proposes another benchmark for RL algorithms in a simulated AD environment. As a limitation, this benchmark focuses on a simple lane-tracking task, and furthermore, evaluates only off-policy RL algorithms. In contrast, our work evaluates both on-policy and off-policy algorithms, and further allows comparing the performance of DRL algorithms in a complex urban environment. As another limitation, both [48] and [46] only support DRL benchmarking for single-agent AD environments. Li [24] introduces a driving simulation framework called MetaDrive and performs a benchmarking of RL algorithms for AD. While the authors use five different driving scenarios, they only evaluate two RL algorithms (PPO and SAC). The proposed work also lacks any research on dealing with adversarial attacks on driving policies. The work could also benefit from using realistic visual rendering as provided by the CARLA framework in our work. Palanisamy [36] proposes a multi-agent urban driving framework in which one can train more than one AC. Using IMPALA, a connected AC policy is trained within the CARLA simulator. However, as a limitation, the work is restricted to connected AD problem only. Furthermore, there are frameworks proposed for training and testing autonomous vehicles. For example, F1TENTH framework [33] with three racing scenarios and baselines for testing and evaluating autonomous vehicles. However, the framework does not support dealing with ARL. Han [17] proposes an off-road simulated environment for AD with realistic off-road scenes, such as mountains, deserts, snowy fields, and highlands. While realistic environments are useful for evaluating the generalization abilities of AD models, the work is limited to single-agent AD environments. 3 DEEP REINFORCEMENT LEARNING FOR AUTONOMOUS DRIVING Reinforcement learning (RL) is mainly modeled as a formulation of the Markov Decision Process (MDP), where the desired goal of the agents in a certain environment is to learn an optimal policy. This goal is achieved by maximizing cumulative reward after interacting with an environment. The MDP model consists of \ud835\udc40(\ud835\udc46,\ud835\udc34, \ud835\udc43, \ud835\udc45,\ud835\udefe), where \ud835\udc46 is a set of agent\u2019s state and \ud835\udc34 is a set of discrete or continuous actions. \ud835\udc45 : \ud835\udc46 \u00d7 \ud835\udc34 \u21a6\u2192 R is a reward function value returned against every action \ud835\udc34, whereas \ud835\udefe is the discount rate applied to the future reward values. Lastly, the MDP model consists of a \ud835\udc43 : \ud835\udc46 \u00d7 \ud835\udc34 \u21a6\u2192 \ud835\udc46 as the transition probability which describes the stochastic probability distribution of the next state \ud835\udc60\u2032 \ud835\udf16 \ud835\udc46 given actions. Following the basics of the MDP model, the agent is dependent on the previous state only to make the next decision. Such a system obeys Markov property during RL control decisions. Reinforcement learning has achieved great results over the past few years due to the advancements in DRL. In DRL, the MDP model is solved by using deep neural networks to learn weight parameters \ud835\udf03 over the time span of environment exploration. DRL models consist of a policy \ud835\udf0b which is responsible for taking an action given a state \ud835\udf0b(\ud835\udc4e|\ud835\udc60) and a value function \ud835\udc63\ud835\udf0b\ud835\udc60 for estimating maximum reward given the current policy \ud835\udc60 \ud835\udf16 \ud835\udc46. Traditional RL fails at solving high-dimensional state space problems and therefore deep neural networks enable the possibility of learning a function approximation over large input and action state spaces to solve complex problems. The policy and value functions in DRL are therefore learned using the defined deep net models to estimate future actions. In our work, the DRL algorithms we choose to benchmark are based on a model-free approach. In model-free RL, the policy \ud835\udf0b and value function \ud835\udc63\ud835\udf0b\ud835\udc60 is learned directly by interacting with the environment without taking model dynamics and the transition probability function of every state space into consideration. Next, we provide a brief descriptions of the DRL models we use for the training and validation of AD in a simulated urban environment. These models are chosen on the basis of i) popularity in the DRL-based AD research community and ii) coverage of discrete as well as continuous action space (for multi-agent testing purposes). 3.1 DRL Algorithms for Autonomous Driving 3.1.1 Discrete Action Space. Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment Proximal Policy Optimization (PPO):. PPO [41] is a DRL algorithm, which also serves as one of the extensions to the policy gradient (PG) algorithms. Vanilla PG faces the problem of high gradient variance, and therefore PPO solves it by adding constraints like clipped surrogate objective and the KL penalty coefficient. Such improvements made PPO a very easy choice in the domain of DRL over the past few years. In terms of vision-based AD, the authors in [35] use PPO as an RL algorithm to train a driving policy using synthetic simulated RGB images. The trained policy is transferred to real-world testing experiments for analyzing the perception and control perspective of the AC. Another authors in [18] uses PPO for proposing a road detecting algorithm in an urban driving environment. They carry out experiments in a Udacity racing game simulator [3] as well as in a small OpenAI gym carracing-v0 environment [7]. Advantage actor-critic (A2C):. A2C [23] is a synchronous version of the A3C RL algorithm. A2C is designed to solve problems associated with A3C regarding how individual actors can update the global parameters and thus it can lead to non-optimal solutions. In A2C, a coordinator is introduced in order to bring consistency to the actors so that all the actors can work with the same policy in the next iterations. Jaafra proposed [19] to addresses an AD problem using CARLA autonomous simulator for training their DRL agent. Using an actorcritic architecture, the driving policy takes RGB images as input and learns to take action outputs within an urban driving environment. Asynchronous Advantage Actor-Critic (A3C):. A3C [30] is a well-known gradient descent-based RL algorithm. Following the on-policy technique, it focuses on using two neural networks - actor and critic. An actor is responsible for making actions while athe critic aims for learning a value function. A3C takes this approach by keeping a global critic model while making multiple actor models for parallel training. A3C has been used in [51] for training an AC policy in a virtual environment that can work in a real-world situation as well. By using synthetic images as an input, the policy learns to drive in an urban scenario using the proposed DRL method. Another work by the authors in [47] also uses A3C by combining RL with image semantic segmentation to train an AC driving policy. Using the TORCS [1] racing simulator, their goal is to lower the gap between virtual and real models while training an RL agent. A3C is also used in [21] where the authors created an end-to-end driving approach without detailed perception tasks on input images. Importance Weighted Actor-Learner Architecture (IMPALA):. IMPALA [13] uses actor-critic technique, but with the twist of decoupling actors. Following a V-trace off-policy approach, IMPALA\u2019s main objective is to scale up the DRL training capability by adding multiple independent actors. The actors, in this case, are meant to generate experience trajectories for the policy to learn, while the learner tries to optimize not only the policy but also the value function. An author in [36] proposes a multi-agent urban driving framework in which one can train more than one AC. Using IMPALA, the author performs training of connected AC policy within the CARLA simulator. Deep Q Networks (DQN):. DQN [31] fall into the category of value-based methods, where the goal is to learn the policy function by going through the optimal action-value function. DQN uses off-policy and temporal differences (TD) to learn state-action pairs by collecting episodes as experience replay. Using the episodic data, samples from the replay memory are used in order to learn Q-values, as a neural network function approximation. DQN is one of the foundation models in the upbringing of DRL and there are many improved versions of DQN implemented by overcoming the existing flaws. DQN is used in [14] for training an AC in a Unity-based urban driving simulator. The authors use camera and laser as their input sensors for training the driving policies. Another work in [6] uses DQN for first training a driving agent in simulation to test its navigation capabilities in both simulated and real-world driving situations. DQN is also utilized in [50] for learning to steer a vehicle within a realistic physics simulator. Given the camera input feeds, their DQN based agent is aiming for following lanes with minimal offroad steering errors. 3.1.2 Continuous Action Space. Deep Deterministic Policy Gradient (DDPG):. When it comes to a continuous action space, DDPG [25] is the most widely used algorithm in the DRL research. DDPG is a model-free and off-policy approach falling under the actor-critic algorithms. It extends DQN by learning a deterministic policy in a continuous action space using actor-critic framework. DPG is extensively used in the field of training autonomous vehicles within simulated driving scenarions [4]. Authors in [39] uses DDPG to construct a DRL model for learning to avoid collisions and steering. DDPG is also used in [22] for learning a lane following policy within driving simulation using continuous action space. Twin Delayed DDPG (TD3): TD3 [15] extends the idea of DDPG in a continuous action space algorithms by tackling issues regarding overestimation of the Q-learning function found in the value-based and actor-critic methods. This results in both improving the learning speed and model performance of DDPG within a continuous action setting. TD3 is extensively used in urban driving scenarios for training AC agents. As a model-free approach, TD3 is used in [10][9] for learning an AD policy within an urban simulated environment. The authors proposed a framework for learning complex driving scenarios using a visual encoding to capture low-level latent variables. Another work in [28] also uses TD3 in to overcome the challenges of driving in an urban simulated environments. 3.2 Multi-agent Autonomous Driving While introducing multi-agent AD agents, we need to consider an environment where agents do not have access to all the states at each time step. Such types of environments are found in the field of robotics and ACs where an agent is limited to the sensory information gathered by its hardware. Therefore, the existing MDP can be termed as a Partially Observable Markov decision process (POMDP) [34]. Furthermore, the current formulation of POMDP can be reformulated as Partially Observable Stochastic Games (POSG) [12] by defining a DRL control problem as a tuple Aizaz Sharif and Dusica Marijan (\ud835\udc3c,\ud835\udc46,\ud835\udc34,\ud835\udc42, \ud835\udc43, \ud835\udc45). In POSG, we can incorporate multi-agent scenarios using Markov Games [27] where multiple agents are interacting with the environment. An actor \ud835\udc56 \ud835\udf16 \ud835\udc3c receives its partial observations from a joint of observation state \ud835\udc5c\ud835\udc56 \ud835\udf16 \ud835\udc42\ud835\udc56 at each time step \ud835\udc61. Following the traditional MDP approach, each actor uses its learned policy function \ud835\udf0b\ud835\udc56 : \ud835\udc42\ud835\udc56 \u21a6\u2192 \ud835\udc34\ud835\udc56 to perform actions \ud835\udc4e\ud835\udc56 \ud835\udf16 \ud835\udc34\ud835\udc56. As a return, each actor gets a desired reward value \ud835\udc5f\ud835\udc56 \ud835\udf16 \ud835\udc45\ud835\udc56. 3.3 Adversarial Reinforcement Learning ARL is a new branch of RL where adversarial algorithms are trained such that they create perturbation attacks against a victim. The desired objective of ARL is to find failure outputs in the victim when the victim is exposed to such adversaries. The victim ACs are obtained from experimental evaluation as described in Section 5.2.1 and 5.2.2. In our work, the adversarial agents are trained as physical driving agents, as opposed to victim ACs in a competitive urban driving environment. Such driving adversaries have no white box access to the input, output, or model weights of victim ACs. By following a zero-sum game strategy, they learn a policy that produces adversarial output actions that appear as natural observations for victim ACs, thus fooling them into errors. 4 END-TO-END BENCHMARKING FRAMEWORK In this section, we provide the details of the DRL benchmarking framework architecture as well as an explanation of the reward function, hyperparameters, and driving policies. The implementation of the framework is available at 1. 4.1 Driving Policies We divide our driving policies into \ud835\udf0b\ud835\udc34\ud835\udc36 and \ud835\udf0b\ud835\udefc. \ud835\udf0b\ud835\udc34\ud835\udc36 represents the policy of AC driving agents that are trained using one of the 7 selected DRL algorithms, whereas \ud835\udf0b\ud835\udefc is the policy for adversarial driving models on the same DRL algorithms. Policies and their associated algorithms are mentioned in Table 1. Their usage is thoroughly explained in Section 5.2. Table 1: Driving Policies and their associated symbols for ACs and adversaries. DRL Algorithm AC Policy Adversarial Policy PPO \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 \ud835\udf0b\ud835\udefc\u2212\ud835\udc43\ud835\udc43\ud835\udc42 A2C \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 \ud835\udf0b\ud835\udefc\u2212\ud835\udc342\ud835\udc36 A3C \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 \ud835\udf0b\ud835\udefc\u2212\ud835\udc343\ud835\udc36 IMPALA \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 \ud835\udf0b\ud835\udefc\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 DQN \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc44\ud835\udc41 DDPG \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a TD3 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 \ud835\udf0b\ud835\udefc\u2212\ud835\udc47\ud835\udc373 1https://anonymous.4open.science/r/Benchmarking-DRL-ESEM-20228B47/README.md 4.2 Deep Neural Network Model A pictorial description of the DRL benchmarking framework can be seen in Figure 1. Each driving agent, whether an AC or adversary, receives partial input state observation of 84x84x3 dimension images through the front camera sensors. Cameras are mounted as part of the driving agents, and during each time step of the simulation environment, cameras capture the input state observations which serve as an input layer to the DRL model. The input layer is then passed to the convolutions and connected hidden layers for extracting important features before they are passed to the output layer of the architecture. ACs as well the adversary driving policies predict the control actions at the output layer based on the 3-dimensional input images at each time step. Since our seven selected DRL models work on a discrete as well as a continuous action space, the output layer of the architecture predicts 9 distinct action values for the discrete action space and 2 float values for the continuous action space policies. These output actions can be summed into three vehicle control commands: Steer, Throttle, and Brake. Reverse and hand brakes are disabled for our experiments. 4.3 Reward functions As described in Section 3, each agent in a POMDP setting is following MDP. Thus, the driving policies receive reward \ud835\udc45 at each time step of the simulated environment while collecting trajectories of the tuple (\ud835\udc46, \ud835\udc45,\ud835\udc34). \ud835\udc45 is the reward value returned while performing action \ud835\udc34 on the current state observations \ud835\udc46 which helps in improving the driving policies \ud835\udf0b. For our experiments, we define two different types of reward functions. The first reward function \ud835\udc45\ud835\udc34\ud835\udc36 is used by the DRL AC policies \ud835\udf0b\ud835\udc34\ud835\udc36 during the training phase. As opposed to that, we define \ud835\udc45\ud835\udefc as the second reward function which is used by the DRL adversarial policies \ud835\udf0b\ud835\udefc. \ud835\udc45\ud835\udc34\ud835\udc36 aims to maximize the driving performance of an AC agent by keeping safety in check. \ud835\udc45\ud835\udc34\ud835\udc36 can be formulated as: \ud835\udc45\ud835\udc34\ud835\udc36 = (\ud835\udc37\ud835\udc61\u22121 \u2212 \ud835\udc37\ud835\udc61) + (\ud835\udc39\ud835\udc61)/10 \u2212 100.0(\ud835\udc36\ud835\udc49\ud835\udc61 + \ud835\udc36\ud835\udc42\ud835\udc61) \u2212 0.5(\ud835\udc3c\ud835\udc42\ud835\udc61) + \ud835\udefd where \ud835\udc37 is the distance covered and \ud835\udc39 is the speed of the AD agent. \ud835\udc36\ud835\udc49 and \ud835\udc36\ud835\udc42 represent a boolean value for collision with another vehicle and road objects respectively. Lastly, \ud835\udc3c\ud835\udc42 refers to the boolean value for offroad steering of the AC model. The \ud835\udefd at the end of the reward functions is a constant used to encourage AC models in driving ground truth road. Ground truth in our case refers to the start and end coordinates of the environment which are assigned to each driving agent during training and testing phase. It is clear from the equation that AC driving policies are penalized against collisions and offroad steering decisions in policy training. On the other hand, the adversary reward function \ud835\udc45\ud835\udefc can be defined as: \ud835\udc45\ud835\udefc = (\ud835\udc37\ud835\udc61\u22121 \u2212 \ud835\udc37\ud835\udc61) + (\ud835\udc39\ud835\udc61)/10 + 5.0(\ud835\udc36\ud835\udc49\ud835\udc61 + \ud835\udc36\ud835\udc42\ud835\udc61) + 0.05(\ud835\udc3c\ud835\udc42\ud835\udc61) + \ud835\udefd where \ud835\udc45\ud835\udefc is designed to aim for maximizing a positive reward against collision and offroad steering errors in ACs while training adversarial driving policies. Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment Autonomous Driving   Deep RL Agents  Observations  1 Action  2 Steer  Throttle  Reward  3 Multi-agent Autonomous   Driving Environment  Discrete Action  Space  Continuous Action  Space  Discrete (9)  Box (2)  Brake  Figure 1: End-to-end DRL benchmarking framework for AC and adversary agents. An agent receives an input observation image of 84x84x3 which is passed to a DRL model. Actions are selected at the output layer of every agent and are performed in the next time step of the simulation in order to obtain a reward and a new observation state. 4.4 Hyperparameters We select hyperparameters by going through the literature of the best implementations for DRL in AD. Since we are using seven different DRL implementations for training their driving policies, each of the algorithms requires separate hyperparameter tuning. The details of the hyperparameters for all the AC and adversary agents are provided in the anonymous repository 2. There are some common hyperparameter configurations used for defining the training and testing scenarios of the driving policies. The hyperparameters used in the training phase of the ACs and adversaries are shown in Table 2. Based on a DRL algorithm and its interaction with the environment, the number of episodes and steps per training iteration varies a lot. During the testing phase, as explained in Section 6, we run 50 total episodes, each having 2000 simulation steps in one environment setting and 5000 simulation steps in another environment setting per driving agent. Table 2: Hyperparameters for the training of AC the adversarial driving agents. Hyperparameter AC Adversarial Total Training Iterations 200 100 Training Steps per episode 2048 2048 Total Training steps 40000000 20000000 Learning Rate 0.0005 0.0005 Batch Size 128 128 Optimizer Adam Adam 5 COMPARATIVE STUDY In this section, we present a comparative study of the performance of seven DRL algorithms for AD, including the description of the simulation frameworks for the training and testing of AC agents. 2https://anonymous.4open.science/r/Benchmarking-DRL-ESEM-20228B47/README.md The research questions in our work evaluate: RQ1: Which DRL-based ACs act better (or worse) in a singleagent as well as multi-agent competitive environment when trained in a multi-agent-only scenario? RQ2: Which DRL-based adversarial agent is more effective in finding failure scenarios for victim ACs while driving against the best performing ACs? We evaluate the driving performance of victim ACs using four Driving Performance Metrics: \u2022 CC: the amount of collision with another driving vehicle \u2022 CO: the amount of collision with road objects \u2022 OS: offroad steering percentage from its driving lane \u2022 SPEED: the forward speed of the AC agent. The first three metrics are used together for showing the overall driving safety capabilities of an AC policy, while the fourth metric is used for an in-depth understanding of the driving behavior of specific AC agents. 5.1 Driving Environment To train our AC agents in a partially observable urban environment we use Town03 map from Carla with Python API [5]. This environment is configured for training multi-agent ACs and testing the same ACs in a single and multi-agent scenario. The same environment is also used for training the adversaries against the best-performing AC. The details of the training and testing configurations for our experiments are mentioned in Section 5.2. 5.1.1 Driving Environment 1 (env_1). We use four-way intersection as our first driving environment from the Town03 map. The driving setting is perfect for validating multi-agent AC policies in a single scenario. All the independent non-communicating driving agents are spawned closer to the intersection in all three scenarios as mentioned in Section 5.2. 5.1.2 Driving Environment 2 (env_2). The second environment has independent non-communicating agents spawned close to the Aizaz Sharif and Dusica Marijan T-intersection throughout the testing scenarios. The choice of Tintersection as a driving scenario is based on its higher complexity for an AC agent in AD research. This driving environment is only used in the testing scenarios for AD policies as explained in Section 5.2. Figure 2: Illustration of Town03 Carla urban driving environment. Red highlighted area on the map represents env_1 (four-way intersection) and green highlighted area displays env_2 (T-intersection). 5.2 Experimental Setup 5.2.1 Scenario 1: Training and testing of multi-agent ACs. First, we train AC policies \ud835\udf0b\ud835\udc34\ud835\udc36 in a multi-agent driving environment. There are DRL-based as well as a few auto-controlled cars driving around each of the AC agents representing human drivers from real-life scenarios. We train \ud835\udf0b\ud835\udc34\ud835\udc36 policies using the reward function \ud835\udc45\ud835\udc34\ud835\udc36 described in Section 4.3. The performance of AC policies that are trained in the first scenario is shown in Figure 3. By training the \ud835\udf0b\ud835\udc34\ud835\udc36 policies for a fixed number of iterations, almost every AC policy converges faster except the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 based policy. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 based DRL policies get to a state stable max episodic reward after a few training episodes. 20 40 60 80 100 \u22121500 \u22121000 \u2212500 0 500 PPO A2C A3C Training Iterations Mean Episodic Reward (a) 20 40 60 80 100 \u22121500 \u22121000 \u2212500 0 500 1000 IMPALA DQN Training Iterations (b) 10 20 30 40 50 60 \u22121000 \u2212500 0 500 1000 1500 DDPG TD3 Training Iterations (c) Figure 3: Convergence of DRL AC policies in Scenario 1. Plot (a) shows mean episodic reward for PPO, A2C, and A3C from a discrete action space. Plot (b) shows the same for IMPALA, and DQN algorithms. Plot (c) illustrates DDPG and TD3 algorithms from a continuous action space. Next, for testing and validating ACs in a multi-agent Scenario 1 all seven DRL-based ACs drive next to each other, as well as around auto-controlled cars, therefore acting as independent noncommunicating competitive driving agents, illustrated in Figure 4(a). The driving performance of each AC is evaluated against the four Driving Performance Metrics. 5.2.2 Scenario 2: Testing of single-agent ACs. After training all seven DRL-based ACs for a number of episodes in a multi-agent Scenario 1, we validate the driving performance of each AC policy in a single-agent driving environment, illustrated in Figure 4(b). Testing in a single-agent scenario is important in order to validate the driving capability of AC policies in similar driving situations but with no cars around. Just like in Scenario 1, ACs\u2019 driving performance is validated based on the four Driving Performance Metrics. 5.2.3 Scenario 3: Training and testing of driving adversaries against the best performing AC. In the third scenario, we introduce adversarial driving agents to our experiments, illustrated in Figure 4(c). For training the adversarial policy, we select the best performing DRL-based AC policies from Scenario 1 and 2. We train every policy \ud835\udf0b\ud835\udefc by keeping its weights constant during the adversarial training. The number of training iterations for each adversarial agent is kept lower than the number of iterations used for AC policy training. As mentioned in Section 4.3 we use \ud835\udc45\ud835\udefc as a reward function for training the seven adversaries. Finally, we use the seven trained adversaries to test the driving behavior and control decisions of the best performing AC agents (evaluated in Scenario 1 and Scenario 2) in a competitive multi-agent scenario. The goal is to see which DRL-based adversarial policy is the most effective in leading victim ACs into failure states. All three scenarios are illustrated in Figure 4. 5.3 Simulation Setup The proposed benchmarking framework uses the following open libraries/frameworks: CARLA [11] is an urban driving simulation framework designed for training and validating autonomous vehicles. CARLA is famous for its highly integrated Python API and access to high-fidelity urban driving environments. We use the 0.9.4 version. RLlib [32] is a very fine-tuned and scalable DRL framework library. RLlib gives the opportunity to access more than one DRL policy graph and its hyperparameters for creating a non-shared multi-agent system. We use versions 0.8.0 for IMPALA and 0.8.5 for rest of the 6 DRL algorithms. Macad-gym [37] is an open-source framework that connects CARLA, RLlib, and Open AI\u2019s Gym toolkit [7]. We have modified the framework by adding adversarial training and competitive multiagent driving functionalities required for our experiments. We use the 0.1.4 of version. Tensorflow [2] is one of the leading frameworks used to create deep learning-based algorithms. We use version 2.2.0 within the RLlib library. 6 RESULTS & ANALYSIS In this section, we discuss the experimental results for testing DRL AC policies. For collecting the results, we run 50 testing episodes in both single and multi-agent scenarios to take an average among the Driving Performance Metrics explained in Section 5. In each Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment Single-agent policy   ( ) \u03c0AC ot Action Observation  Car Autonomous Driving Environment Victim Car Observation 1  Observation 2 Action 1 Action 2 Autonomous Driving Environment Adversary Car   ( ) \u03c0AC ot ( ) \u03c0 \u03b1 ot (b) (a) (c) Observation 1  Observation 2 Action 1 Action 2 Autonomous Driving Environment   ( ) \u03c0AC ot Non-shared Multiagent policies   ( ) \u03c0AC ot Observation n Car n Car 1 Car 2   ( ) \u03c0AC ot Action n Non-shared Multiagent policies Figure 4: Illustration of three different scenarios for the testing phase. The top figure (a) shows a competitive multiagent testing environment, while the middle figure (b) displays a single-agent driving scenario for testing every DRLbased AC agent individually. Bottom figure (c) shows the third scenario used for training and testing adversarial RL agents against the victim AC policies. episode, we use 2000 simulation steps in env_1 and 5000 steps in env_2 in order to test the performance of every AC and adversarial driving policy. All experimental results are available at 3. 6.1 RQ1: Testing AC policies in a single and multi-agent scenario We first look into the performance comparison among the 5 discrete and 2 continuous action space DRL AC policies. Each of the AC 3https://anonymous.4open.science/r/Benchmarking-DRL-ESEM-20228B47/README.md agents is trained with a fixed number of episodes and their model convergence performance has been discussed in Section 5.2.1. Now we use the same training environment and analyze their driving behavior in both single and multi-agent scenarios. The evaluation results of the driving performance of DRL-based AC agents are presented in Table 3 using three Driving Performance Metrics: CC, CO, and OS. AC policies having values closer to 0 are driving error-free, while those near to 1 have a higher failure state. The results of these three metrics should be analyzed jointly with the fourth metric: SPEED. This is because a car could have no collision and offroad steering errors due to being stationary. Therefore, we provide Figure 5, which shows an AC\u2019s driving speed per timestep in a testing simulation. For displaying all the episodic results, we took an average of both Scenario 1 and 2 for every DRL AC policy across each driving environment setting. 6.1.1 env_1: From Table 3, it is clear that in a multi-agent scenario, A3C algorithm-based policy \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 performs better with no collision with other vehicles and zero offroad steering errors. We can also see in Figure 5 that A3C overall performs consistently throughout the testing episodes with both high and low speed values. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 although have no collision and offroad steering percentage in the table, they perform the worst among DRL based policies when we analyze the results in Figure 5. In the figure, we can see that A2C and IMPALA-based AC policies did not move forward after the starting number of steps and therefore stayed at one position during the entire testing phase. This results from \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 avoiding any collision and offroad steering errors by standing still, making it the worst-performing agents during our experiments. The rest of the 2 discrete action space AC driving policies work almost the same, dropping the performance after going halfway through the testing episodes as shown in the figure. The table also shows that \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 based AC policies had no collision with DRL and auto-controlled vehicles, but were driving offroad most of the testing phase resulting in collisions with footpaths and other road objects. In the same scenario using continuous action space, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 performs much better than \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a with less collision percentage and no offroad steering error states. We see supporting behavior in the Figure 5 as well for Scenario 1 where \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 covers more distance \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a while performing multi-agent testing. To answer RQ1 in-depth, DRL AC policies trained in a multiagent scenario are now brought in a single-agent scenario. The goal is to see how much does driving in a single-agent urban setting affect the performance capabilities of each AC agent. Using the same four Driving Performance Metrics we display our results using Table 3 and Figure 5. The driving performance of AC policies drops significantly when tested in a single-agent environment. Comparatively, A3C-based policy \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 performs better than the rest of the AC policies in a single-agent setting, which is consistent with the results from a multi-agent scenario. Similarly, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 joins \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 as the worst-performing AC policies while driving in a single-agent scenario. Both policies have zero collision and offroad steering error since they were unable to perform control decisions as shown in Figure 5. Aizaz Sharif and Dusica Marijan Table 3: Comparison of the behavior of AC driving agents in terms of collision and offroad steering error percentage when tested both in a single (Scenario 1) and multi-agent (Scenario 2) environment. Scenario 1 Scenario 2 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 env_1 CC 0.0 0.0 0.0 0.0 0.0 0.0356 0.0 CO 0.6069 0.0 0.015 0.0 0.3402 0.0 0.1361 0.925 0.0 0.4741 0.0 0.7347 0.0 0.064 OS 0.697 0.0 0.0 0.0 0.8986 0.783 0.0 0.987 0.0 0.6605 0.0 0.9809 0.5773 0.4979 env_2 CC 0.0 0.0 0.21012 0.0 0.37552 0.0 0.03048 CO 0.8788 0.0 0.0248 0.0 0.28824 0.37988 0.16052 0.9414 0.0 0.20548 0.0 0.76772 0.18632 0.1912 OS 0.73056 0.0 0.12004 0.0 0.7804 0.98672 0.77764 0.39412 0.0 0.98476 0.0 0.99472 0.78556 0.47312 0 500 1000 1500 2000 Steps 0 2 4 6 Forward Speed (a) PPO env_1 0 500 1000 1500 2000 Steps 0.000 0.002 0.004 0.006 (b) A2C env_1 0 500 1000 1500 2000 Steps 0 2 4 (c) A3C env_1 0 500 1000 1500 2000 Steps 0.000 0.001 0.002 0.003 0.004 (d) IMPALA env_1 0 500 1000 1500 2000 Steps 0 2 4 (e) DQN env_1 0 500 1000 1500 2000 Steps 0 1 2 (f) DDPG env_1 0 500 1000 1500 2000 Steps 0.0 0.5 1.0 (g) TD3 env_1 0 2000 4000 Steps 0 2 4 6 Forward Speed (h) PPO env_2 0 2000 4000 Steps 0.0000 0.0002 0.0004 (i) A2C env_2 0 2000 4000 Steps 0 2 4 (j) A3C env_2 0 1000 2000 3000 4000 5000 Steps 0.0000 0.0002 0.0004 0.0006 (k) IMPALA env_2 0 2000 4000 Steps 0 2 4 (l) DQN env_2 0 2000 4000 Steps 0 2 4 6 (m) DDPG env_2 0 2000 4000 Steps 0 2 4 (n) TD3 env_2 Figure 5: Comparison of the behavior of AC driving agents in terms of forward speed when tested both in a single and multiagent scenario. The first row represents AC policies tested in env_1 and the second row represents the same policies tested in env_2. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 based AC agents drop their driving performance after taking a few steps in the testing episodes as plotted in the figure. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 started off well but after a few simulation steps, its driving decision capabilities dropped when getting closer to a four-way intersection. On the other hand, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 had more offroad steering errors than \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a, but it kept moving forward towards the destination unlike \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a which stopped at an earlier episodic steps of testing. 6.1.2 env_2: To test the driving robustness of AC policies in an unseen environment we use env_2. In the multi-agent setting (Scenario 1), for \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36, and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 , the overall factor of collision and offroad steering increased compared to the results in env_1. Within discrete action space algorithms, once again \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 performs better than the rest of the AC policies. The same pattern applies to continuous action space algorithms when tested in the multi-agent scenario of env_2. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 although performs better in comparison, but just like \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a, it results in more failure trajectories while performing episodic runs. In a single-agent testing Scenario 2, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 faced many offroad steering trajectories compared to the multiagent scenario. Whereas \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 also had the same effect of many offroad steering errors, in comparison, it had fewer road objects collision rates. For continuous algorithms, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 performed slightly better in a single-agent unseen driving. Just like in env_1, \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 yields fewer failure driving states while covering more distance and speed in comparison with \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a. By combining the results of Scenarios 1 and 2 for env_2, we can visually see that the driving performance of \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc43\ud835\udc43\ud835\udc42 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc37\ud835\udc44\ud835\udc41 remained almost the same as in env_1. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc342\ud835\udc36 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 similarly fail to perform in env_2. In summary, the experimental results of testing AC policies in a single and multi-agent scenario indicate that the A3C and TD3based driving agents perform better than the rest of the DRL agents in both single and multi-agent scenarios within two driving environment settings, answering RQ1. Next, we use \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 policies for training our adversarial driving policies \ud835\udf0b\ud835\udefc in Scenario 3, as described in 5.2.3. The same A3C and TD3-based victim AC agents are evaluated using the trained adversaries in 6.2 . 6.2 RQ2: Effectiveness of adversarial driving policies in finding failure driving scenarios for victim ACs We now compare which of the seven DRL algorithms perform better as adversaries. A successful adversarial driving agent is the one that is able to disturb the driving behavior of a victim AC, thus resulting in exploring more failure cases for the victim AC. The victim ACs are \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 for a discrete and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 for a continuous action space. For evaluating the effectiveness of each adversarial policy \ud835\udf0b\ud835\udefc we use the same four Driving Performance Metrics. Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment Table 4 describes the collision and offroad steering error percentage of \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 victim AC agents against each of the seven DRL-based adversarial agents. Figure 6 further shows the driving speed of these two victim AC policies under adversarial attacks. 6.2.1 env_1: The PPO-based adversarial policy ends up finding most of the failure states in the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36-based victim AC compared to the rest of the adversaries. \ud835\udf0b\ud835\udefc\u2212\ud835\udc43\ud835\udc43\ud835\udc42 adversary affects the driving performance of \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 by pushing it into colliding with other vehicles in the environment. \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 did not have a single vehicle collision while testing without adversaries, as shown in Table 3. This shows that the PPO algorithm is a good choice for training adversarial agents which can later be used for testing and validating AC agents. \ud835\udf0b\ud835\udefc\u2212\ud835\udc43\ud835\udc43\ud835\udc42 adversary also affected the driving performance of SUT by discovering a few cases of offroad steering failure states. In Figure 6a, we can see the effects of the \ud835\udf0b\ud835\udefc\u2212\ud835\udc43\ud835\udc43\ud835\udc42-based adversarial policy on speed of the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36-based victim AC in env_1. The speed of \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 driving agent on average drops after passing halfway through the testing episodes. This implies that the PPObased adversarial agent was able to disturb the control decisions of the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36-based victim AC, leading it into error states, hence a decline in speed of the victim agent. The rest of the four DRL-based adversaries from a discrete action space perform somewhat weaker compared to \ud835\udf0b\ud835\udefc\u2212\ud835\udc43\ud835\udc43\ud835\udc42, where other than a few cases by \ud835\udf0b\ud835\udefc\u2212\ud835\udc343\ud835\udc36 none of them forced the victim AC to a vehicle collision failure state. Moving towards the continuous space, TD3 itself became a better adversary in a two-player game against the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373-based victim AC. Within env_1 it drove the victim AC policy more towards failure cases compared to the \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a-based adversary. The presence of adversarial policy within the observation state of \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373-based victim AC resulted in more offroad steering errors and collisions in the victim AC policy actions. Figure 6b also depicts that, i.e. due to the adversarial attacks of \ud835\udf0b\ud835\udefc\u2212\ud835\udc47\ud835\udc373, there is an increase of unstable driving performance of the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373-based victim AC, leading toward no speed control decisions for the rest of the episode steps. 6.2.2 env_2: We observe slightly different results while using the same adversarial agents against the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36-based victim AC in a new driving environment env_2. Among all the discrete algorithms, \ud835\udf0b\ud835\udefc\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 performs most effectively as an adversarial driving agent. Figure 6c shows the impact of adversary \ud835\udf0b\ud835\udefc\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 on the the driving speed of the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36-based victim AC, where the speed of the victim AC policy drops with time during the episodic testing. In the continuous action space, env_2 brings new findings for the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373-based victim AC against both adversaries. As shown in Table 4 \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a works better as adversarial driving policy in the new driving environment against the victim AC agent during testing, which in turn leads to higher collision and offroad steering percentage. Figure 6d also shows the same effects where the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373-based AC policy fails to overcome the \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a natural observational attacks. \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a forces the \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373-based AC policy into higher collision and offroad steering error states, resulting in no speed control decisions from the AC policy in majority of the episode steps. In summary, the experimental results of evaluating the effectiveness of adversarial driving policies in findings errors in victim ACs indicate that PPO and IMPALA in discrete action space as well as TD3 and DDPG in continuous action space algorithms are the most effective adversarial driving agents, answering RQ2. These adversarial agents were able for expose failure states within AC policies across two different driving environments. Table 4: Collision and offroad steering error percentage of the best-performing victim AC policies (\ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373) when tested against the seven adversarial cars. Scenario 3 AC-A3C AC-TD3 \ud835\udf0b\ud835\udefc\u2212\ud835\udc43\ud835\udc43\ud835\udc42 \ud835\udf0b\ud835\udefc\u2212\ud835\udc342\ud835\udc36 \ud835\udf0b\ud835\udefc\u2212\ud835\udc343\ud835\udc36 \ud835\udf0b\ud835\udefc\u2212\ud835\udc3c\ud835\udc40\ud835\udc43\ud835\udc34\ud835\udc3f\ud835\udc34 \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc44\ud835\udc41 \ud835\udf0b\ud835\udefc\u2212\ud835\udc37\ud835\udc37\ud835\udc43\ud835\udc3a \ud835\udf0b\ud835\udefc\u2212\ud835\udc47\ud835\udc373 env_1 CC 0.3518 0.0 0.0625 0.0 0.0 0.0 0.1756 CO 0.0408 0.0271 0.0933 0.0973 0.0 0.2643 0.2844 OS 0.1817 0.1893 0.2502 0.2033 0.04 0.1903 0.7193 env_2 CC 0.0 0.0 0.0 0.1526 0.0 0.56716 0.19304 CO 0.0 0.0 0.09764 0.26436 0.15868 0.63288 0.813 OS 0.3078 0.20948 0.08472 0.17104 0.1176 0.7952 0.78364 0 250 500 750 1000 1250 1500 1750 2000 Steps 0.0 0.5 1.0 1.5 Forward Speed (a) A3C Against AdvPPO env_1 0 250 500 750 1000 1250 1500 1750 2000 Steps 0.0 0.5 1.0 1.5 2.0 2.5 (b) TD3 Against AdvTD3 env_1 0 1000 2000 3000 4000 5000 Steps 0 1 2 3 4 Forward Speed (c) A3C Against AdvIMPALA env_2 0 1000 2000 3000 4000 5000 Steps 0 1 2 3 4 (d) TD3 Against AdvDDPG env_2 Figure 6: Driving speed of the best-performing victim AC agents \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc343\ud835\udc36 and \ud835\udf0b\ud835\udc34\ud835\udc36\u2212\ud835\udc47\ud835\udc373 when tested against adversarial cars in env_1 (1st row) and env_2 environments (2nd row). 7 DISCUSSION AND OPEN RESEARCH DIRECTIONS This section provides our observations and thoughts on the effectiveness and robustness of the seven evaluated DRL algorithms for their use in AD research, specifically, as AC agents and adversarial agents used for testing the driving agents. PPO as AC, in general, did not perform better than the rest of the algorithms while driving in a single or multi-agent scenario. Standard PPO policies are often stuck at non-optimal actions while learning, since they are very sensitive to sparse high rewards. However, the same PPO algorithm with the adversarial reward function ",
    "Conclusion": "CONCLUSION In this work, we compare the robustness of DRL-based policies while training ACs and adversarial agents. By first training DRL AC policies in a multi-agent environment, we test their driving performance in both single and multi-agent scenarios. We analyze the robustness of AC policies using four evaluation metrics described in Section 6. In order to find more vulnerabilities in AC policies, the same DRL algorithms are used for training adversarial agents that can help in finding failure-driving scenarios for victim ACs. We use the best performing AC agents from our experiments as victim ACs to evaluate the effectiveness of adversarial policies in driving victim ACs into collision and offroad driving errors. ",
    "References": "REFERENCES [1] 2007. TORCS, The Open Racing Car Simulator. https://sourceforge.net/projects/ torcs/ [2] 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. https://www.tensorflow.org/ Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment [3] 2017. A self-driving car simulator built with Unity. In Udacity. https://github. com/udacity/self-driving-car-sim [4] 2018. Deep Reinforcement Learning for Autonomous Driving. (2018). [5] 2021. 3rd Maps and navigation. In CARLA. https://carla.readthedocs.io/en/latest/ core_map/ [6] P\u00e9ter Alm\u00e1si, R\u00f3bert Moni, and B\u00e1lint Gyires-T\u00f3th. 2020. Robust Reinforcement Learning-based Autonomous Driving Agent for Simulation and Real World. (2020). [7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. (2016). [8] Behdad Chalaki, Logan E. Beaver, Ben Remer, Kathy Jang, Eugene Vinitsky, Alexandre M. Bayen, and Andreas A. Malikopoulos. 2020. Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning. In IEEE 16th International Conference on Control Automation (ICCA). [9] Jianyu Chen, Shengbo Eben Li, and Masayoshi Tomizuka. 2021. Interpretable Endto-End Urban Autonomous Driving With Latent Deep Reinforcement Learning. IEEE Transactions on Intelligent Transportation Systems (2021). [10] Jianyu Chen, Bodi Yuan, and Masayoshi Tomizuka. 2019. Model-free Deep Reinforcement Learning for Urban Autonomous Driving. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC). [11] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017. CARLA: An Open Urban Driving Simulator. In 1st Annual Conference on Robot Learning. [12] R. Emery-Montemerlo, S. Thrun, G. Gordon, and J. Schneider. 2004. Approximate Solutions for Partially Observable Stochastic Games with Common Payoffs. In International Joint Conference on Autonomous Agents and Multiagent Systems. IEEE Computer Society. [13] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. 2018. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. In Proceedings of the 35th International Conference on Machine Learning (PMLR). [14] Abdur R. Fayjie, Sabir Hossain, Doukhi Oualid, and Deok-Jin Lee. 2018. Driverless Car: Autonomous Driving Using Deep Reinforcement Learning in Urban Environment. In 15th International Conference on Ubiquitous Robots (UR). [15] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation Error in Actor-Critic Methods. [16] Joshua Garcia, Yang Feng, Junjie Shen, Sumaya Almanee, Yuan Xia, Chen, and Qi Alfred. 2020. A Comprehensive Study of Autonomous Vehicle Bugs. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. [17] Isaac Han, Dong-Hyeok Park, and Kyung-Joong Kim. 2021. A New Open-Source Off-Road Environment for Benchmark Generalization of Autonomous Driving. IEEE Access 9 (2021), 136071\u2013136082. https://doi.org/10.1109/ACCESS.2021. 3116710 [18] Martin Holen, Rupsa Saha, Morten Goodwin, Christian W. Omlin, and Knut Eivind Sandsmark. 2020. Road Detection for Reinforcement Learning Based Autonomous Car. In Proceedings of the The 3rd International Conference on Information Science and System (ICISS). Association for Computing Machinery. [19] Yesmina Jaafra, Jean Luc Laurent, Aline Deruyver, and Mohamed Saber Naceur. 2019. Robust Reinforcement Learning for Autonomous Driving. In ICLR Workshop. [20] Kathy Jang, Eugene Vinitsky, Behdad Chalaki, Ben Remer, Logan Beaver, Andreas A. Malikopoulos, and Alexandre Bayen. 2019. Simulation to Scaled City: Zero-Shot Policy Transfer for Traffic Control via Autonomous Vehicles. In 10th ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS). [21] Maximilian Jaritz, Raoul de Charette, Marin Toromanoff, Etienne Perot, and Fawzi Nashashibi. 2018. End-to-End Race Driving with Deep Reinforcement Learning. In IEEE International Conference on Robotics and Automation (ICRA). [22] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, JohnMark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. 2019. Learning to Drive in a Day. In International Conference on Robotics and Automation (ICRA). [23] Yonghwi Kwon, Brendan Saltaformaggio, I Luk Kim, Kyu Hyung Lee, Xiangyu Zhang, and Dongyan Xu. 2017. A2C: Self Destructing Exploit Executions via Input Perturbation. In Proceedings of the 24th Annual Network and Distributed System Security Symposium (NDSS. [24] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. 2021. MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning. https://arxiv.org/abs/2109.12674 [25] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. [26] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep reinforcement learning. CoRR abs/1509.02971 (2016). [27] Michael L. Littman. 1994. Markov Games as a Framework for Multi-Agent Reinforcement Learning. In 11th International Conference on International Conference on Machine Learning. [28] Haochen Liu, Zhiyu Huang, and Chen Lv. 2021. Improved Deep Reinforcement Learning with Expert Demonstrations for Urban Autonomous Driving. [29] Dusica Marijan and Arnaud Gotlieb. 2020. Software Testing for Machine Learning. Proceedings of the AAAI Conference on Artificial Intelligence 34, 09 (Apr. 2020), 13576\u201313582. https://doi.org/10.1609/aaai.v34i09.7084 [30] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of The 33rd International Conference on Machine Learning (PMLR). [31] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. (2013). [32] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, William Paul, Michael I. Jordan, and Ion Stoica. 2017. Ray: A Distributed Framework for Emerging AI Applications. (2017). [33] Matthew O\u2019Kelly, Hongrui Zheng, Dhruv Karthik, and Rahul Mangharam. 2020. F1TENTH: An Open-source Evaluation Environment for Continuous Control and Reinforcement Learning. In Proceedings of the NeurIPS 2019 Competition and Demonstration Track (Proceedings of Machine Learning Research), Hugo Jair Escalante and Raia Hadsell (Eds.), Vol. 123. PMLR, 77\u201389. [34] Frans A. Oliehoek. 2012. Decentralized POMDPs. In Springer Berlin Heidelberg. [35] B\u0142a\u017cej Osi\u0144ski, Adam Jakubowski, Pawe\u0142 Zi\u0119cina, Piotr Mi\u0142o\u015b, Christopher Galias, Silviu Homoceanu, and Henryk Michalewski. 2020. Simulation-Based Reinforcement Learning for Real-World Autonomous Driving. In IEEE International Conference on Robotics and Automation (ICRA). [36] Praveen Palanisamy. 2020. Multi-Agent Connected Autonomous Driving using Deep Reinforcement Learning. In 2020 International Joint Conference on Neural Networks (IJCNN). [37] P. Palanisamy. 2020. Multi-Agent Connected Autonomous Driving using Deep Reinforcement Learning. In International Joint Conference on Neural Networks (IJCNN). [38] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V. Albrecht. 2019. Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning. (2019). [39] Horia Porav and Paul Newman. 2018. Imminent Collision Mitigation with Reinforcement Learning and Vision. In 21st International Conference on Intelligent Transportation Systems (ITSC). [40] V. Riccio, G. Jahangirova, and A. et al. Stocco. 2020. Testing machine learning based systems: a systematic mapping. Empir Software Eng (2020), 5193. [41] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. (2017). [42] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora, Sertac Karaman, and Daniela Rus. 2019. Social behavior for autonomous vehicles. Proceedings of the National Academy of Sciences (2019). [43] Harald Sch\u00e4fer. 2021. End-to-end lateral planning. In comma.ai. [44] David Silver. 2021. AI Day Showcases The Breadth Of Tesla\u2019s Ambition. In Forbes. https://www.forbes.com/sites/davidsilver/2021/08/20/ai-day-showcasesthe-breadth-of-teslas-ambition/ [45] Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, and Prateek Mittal. 2018. DARTS: Deceiving Autonomous Cars with Toxic Signs. In arXiv. [46] Marco Stang, Daniel Grimm, Moritz Gaiser, and Eric Sax. 2020. Evaluation of Deep Reinforcement Learning Algorithms for Autonomous Driving. In 2020 IEEE Intelligent Vehicles Symposium (IV). 1576\u20131582. https://doi.org/10.1109/IV47402. 2020.9304792 [47] Bowen Tan, Nayun Xu, and Bingyu Kong. 2018. Autonomous Driving in Reality with Reinforcement Learning and Image Translation. (2018). [48] Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Fangyu Wu, Richard Liaw, Eric Liang, and Alexandre M. Bayen. 2018. Benchmarks for reinforcement learning in mixed-autonomy traffic. In CoRL. [49] Akifumi Wachi. 2019. Failure-Scenario Maker for Rule-Based Agent using Multiagent Adversarial Reinforcement Learning and its Application to Autonomous Driving. In Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI. [50] Peter Wolf, Christian Hubschneider, Michael Weber, Andr\u00e9 Bauer, Jonathan H\u00e4rtl, Fabian D\u00fcrr, and J. Marius Z\u00f6llner. 2017. Learning how to drive in a real world simulation with deep Q-Networks. In IEEE Intelligent Vehicles Symposium (IV). [51] Yurong You, Xinlei Pan, Ziyan Wang, and Cewu Lu. 2017. Virtual to Real Reinforcement Learning for Autonomous Driving. (2017). [52] Ming et al. Zhou. 2021. SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous Driving. PMLR. ",
    "title": "Evaluating the Robustness of Deep Reinforcement Learning for",
    "paper_info": "Evaluating the Robustness of Deep Reinforcement Learning for\nAutonomous and Adversarial Policies in a Multi-agent Urban\nDriving Environment\nAizaz Sharif\naizaz@simula.no\nSimula Research Laboratory\nNorway\nDusica Marijan\ndusica@simula.no\nSimula Research Laboratory\nNorway\nABSTRACT\nBackground: Deep reinforcement learning is actively used for\ntraining autonomous and adversarial car policies in a simulated\ndriving environment. Due to the large availability of various re-\ninforcement learning algorithms and the lack of their systematic\ncomparison across different driving scenarios, we are unsure of\nwhich ones are more effective for training and testing autonomous\ncar software in single-agent as well as multi-agent driving environ-\nments. Aims: A benchmarking framework for the comparison of\ndeep reinforcement learning in a vision-based autonomous driving\nwill open up the possibilities for training better autonomous car\ndriving policies. Furthermore, autonomous cars trained on deep\nreinforcement learning-based algorithms are known for being vul-\nnerable to adversarial attacks. To guard against adversarial attacks,\nwe can train autonomous cars on adversarial driving policies. How-\never, we lack the knowledge of which deep reinforcement learning\nalgorithms would act as good adversarial agents able to effectively\ntest autonomous cars. Method: To address these challenges, we\nprovide an open and reusable benchmarking framework for sys-\ntematic evaluation and comparative analysis of deep reinforcement\nlearning algorithms for autonomous and adversarial driving in a\nsingle- and multi-agent environment. Using the framework, we\nperform a comparative study of five discrete and two continuous\naction space deep reinforcement learning algorithms. We run the\nexperiments in a vision-only high fidelity urban driving simulated\nenvironments.\nResults: The results indicate that only some of the deep rein-\nforcement learning algorithms perform consistently better across\nsingle and multi-agent scenarios when trained in a multi-agent-\nonly setting. For example, A3C- and TD3-based autonomous cars\nperform comparatively better in terms of more robust actions and\nminimal driving errors in both single and multi-agent scenarios.\nPPO IMPALA from a discrete action space and DDPG TD3 from a\ncontinuous action space show to be effective algorithms for training\nadversarial agents and exposing failure scenarios in autonomous\ncar software. Conclusions: We conclude that different DRL algo-\nrithms exhibit different driving and testing performance in different\nscenarios, which underlines the need for their systematic compara-\ntive analysis. The benchmarking framework proposed in this paper\nfacilitates such a comparison.\nKEYWORDS\ndeep reinforcement learning, multi-agent systems, autonomous\ncars, autonomous driving, adversarial reinforcement learning\n1\nINTRODUCTION\nAutonomous cars (ACs) are complex decision-making systems that\nare unfortunately prone to errors [16]. They commonly use ma-\nchine/deep learning algorithms as part of decision-making software,\nwhich are known to be difficult to validate [29, 40]. Therefore, ACs\nneed comprehensive training and evaluation in order to minimize\nrisk to the public. For the same reason, autonomous driving (AD)\nresearch is nowadays performed within simulated driving envi-\nronments (also used by the state-of-the-art industrial solutions\nlike Tesla [44] and Comma ai [43]), as they provide flexibility for\ntesting and validating AD without posing any danger to the real\nworld. However, we observe three specific challenges in training\nand validating ACs in the existing simulation environments.\nFirst, while the majority of AD research is focused on using deep\nreinforcement learning (DRL) for training ACs in a simulated urban\ndriving scenario [35][47][6][4][39], there is a lack of comparison\namong DRL algorithms for vision-based urban driving scenarios.\nHaving such a benchmark of commonly used DRL algorithms can\nbe useful for understanding why some algorithms perform worse\nthan others in specific driving scenarios, which can lead to the\nimprovements of the state-of-art DRL algorithms for AD.\nSecond, the majority of existing research trains ACs as non-\ncommunicating and independent single intelligent agents [52][42],\ntherefore treating the ACs as a single-agent driving problem. How-\never, in the near future AD will be a complex multi-agent prob-\nlem [20]. By bringing more than one AC into a multi-agent envi-\nronment we can evaluate how a non-stationary driving scenario\naffects AC\u2019s control decisions when interacting with other ACs.\nExisting comparative analyses of RL algorithms for AD are still\nlimited to single agent driving environments [46][48], and there\nis no systematic study performed yet on which DRL models work\nbest for AD in a multi-agent environment.\nThird, a vast portion of existing research is actively focused\non testing ACs trained on vision-based end-to-end systems. One\nof the ways to test their control behavior is using adversarial RL\n(ARL). In fact, DRL is proved to be vulnerable multiple times against\nadversarial attacks [45], and one of the solutions to this problem is\nto train the adversary as a separate physical driving model using\nDRL [8]. ARL can be used to not only find failure scenarios in ACs\nbut also to improve their driving policies through retraining [49].\nHowever, at the moment, we lack the support for training and\nsystematically evaluating ARL algorithms to decide which ones are\nmore effective in creating better adversarial AD agents able to find\nerrors in ACs.\narXiv:2112.11947v2  [cs.AI]  27 May 2022\n",
    "GPTsummary": "- (1): The research background of this article is to address the challenges of training and validating autonomous cars (ACs) using deep reinforcement learning (DRL) in a simulated environment, where the lack of systematic comparison among various reinforcement learning algorithms makes it difficult to determine which ones are more effective for testing and training autonomous car software in single-agent and multi-agent driving environments.\n\n- (2): The past methods have focused on using DRL for training ACs in a simulated urban driving scenario, treating the ACs as a single-agent driving problem, and testing ACs trained on vision-based end-to-end systems using adversarial RL. However, there is a lack of comparison among DRL algorithms for vision-based urban driving scenarios, no systematic study performed yet on which DRL models work best for AD in a multi-agent environment, and we lack support for training and systematically evaluating ARL algorithms to decide which ones are more effective in creating better adversarial AD agents able to find errors in ACs. The approach is well motivated by the need for comprehensive training and evaluation of ACs in order to minimize risk to the public.\n\n- (3): The research methodology proposed in this paper is an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous and adversarial driving in a single- and multi-agent environment. The comparative study of five discrete and two continuous action space deep reinforcement learning algorithms is performed using the framework and experiments are run in a vision-only high fidelity urban driving simulated environments.\n\n- (4): The methods in this paper are evaluated on the task of evaluating the robustness of deep reinforcement learning for autonomous and adversarial policies in a multi-agent urban driving environment. The results indicate that only some of the deep reinforcement learning algorithms perform consistently better across single and multi-agent scenarios when trained in a multi-agent-only setting. For example, the A3C- and TD3-based autonomous cars perform comparatively better in terms of more robust actions and minimal driving errors in both single and multi-agent scenarios. PPO IMPALA from a discrete action space and DDPG TD3 from a continuous action space show to be effective algorithms for training adversarial agents and exposing failure scenarios in autonomous car software. The performance achieved supports their goals of providing a benchmarking framework for the comparative analysis of deep reinforcement learning algorithms for autonomous and adversarial driving in a single- and multi-agent environment.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work is to provide a comprehensive evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous and adversarial driving in a single- and multi-agent environment, addressing the challenges of training and validating autonomous cars (ACs) using deep reinforcement learning (DRL). The authors propose an open and reusable benchmarking framework for systematic evaluation and comparative analysis of DRL algorithms for ACs, which can minimize risks to the public.\n\n- (2): Innovation point: The article proposes a comprehensive evaluation and comparative analysis of DRL algorithms for ACs in a multi-agent environment, which can provide better solutions and improve the robustness of ACs. Performance: The experiments demonstrate that some of the DRL algorithms perform consistently better across single and multi-agent scenarios when trained in a multi-agent-only setting. However, some algorithms are more effective for training adversarial agents and exposing failure scenarios in autonomous car software. Workload: The approach is well motivated and provides a systematic study on DRL algorithms, which requires significant effort in the design and implementation of the benchmarking framework and experiments.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to provide a comprehensive evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous and adversarial driving in a single- and multi-agent environment, addressing the challenges of training and validating autonomous cars (ACs) using deep reinforcement learning (DRL). The authors propose an open and reusable benchmarking framework for systematic evaluation and comparative analysis of DRL algorithms for ACs, which can minimize risks to the public.\n\n- (2): Innovation point: The article proposes a comprehensive evaluation and comparative analysis of DRL algorithms for ACs in a multi-agent environment, which can provide better solutions and improve the robustness of ACs. Performance: The experiments demonstrate that some of the DRL algorithms perform consistently better across single and multi-agent scenarios when trained in a multi-agent-only setting. However, some algorithms are more effective for training adversarial agents and exposing failure scenarios in autonomous car software. Workload: The approach is well motivated and provides a systematic study on DRL algorithms, which requires significant effort in the design and implementation of the benchmarking framework and experiments.\n\n\n"
}