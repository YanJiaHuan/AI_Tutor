{
    "Abstract": "",
    "Introduction": "Introduction 2 Finding solutions to complex problems with deep reinforcement learning can still be slow and there is sometimes no gurantee for a standard algorithm to succeed solving a previously unseen task, especially if there is no time to carefully choose the hyperparameters of the algorithm. Chapter 4 describes ideas to train models that preprocess the available information in advance and thus simplify the reinforcement learning task. We call this strategy pretraining. One goal of this thesis is to investigate the concepts of chapter 4 and their practical applicability to algorithms of chapter 3. We also combine DDPG (deep deterministic policy gradient; Lillicrap et al. 2015) and asynchronous methods (Mnih, Badia, et al. 2016), which both are commonly used reinforcement learning methods, to form two new reinforcement learning algorithms, which will be investigated. We call these algorithms distributed and asynchronous DDPG. The intention of combining two inventions, that were recently succesful in the field, is to combine their respective benefits. Asynchronous methods enable fast training and generalize well to unseen situations. The DDPG algorithm is a good choice for learning a continuous action space. This means, that a discrete choice between several options is not satisfactory, but complex continuous actions need to be generated for a correct behavior. Chapter 5 introduces these algorithms, but also describes a simulated robotic environment that was used for all evaluations and some interesting implementation details. We implemented the distributed and asynchronous DDPG algorithms and reused a DDPG implementation as baseline. Chapter 6 explains the experimental setups and states the obtained results. We evaluate all variants of DDPG and compare the respective scores, but also combine the DDPG baseline with differently pretrained models for comparing the quality of various pretraining techniques. Chapter 7 concludes with a discussion. Chapter 2 Different Types of Learning 2.1 Supervised Learning A large number of machine learning tasks like regression, classification or pattern recognition aim to approximate a function \ud835\udc66 = \ud835\udc53(\ud835\udc65) given a set of training examples (\ud835\udc65(\ud835\udc56), \ud835\udc66(\ud835\udc56)). These tasks can be viewed as approximating the probability distribution of the given data \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(\ud835\udc66|\ud835\udc65). The formulation is taken from Goodfellow, Bengio, and Courville (2016). While other formulations are possible, estimating a probability distribution is very general and has the pleasing property that many learning algorithms can be derived easily, for example using maximum likelihood estimation to match the distribution of the model to the data distribution. As an example, performing maximum likelihood estimation in a simple supervised learning model with only one output is equal to minimizing the mean squared error (MSE) between the outputs of the model and the targets \ud835\udc66(\ud835\udc56), if the distribution of the model is assumed to be gaussian with fixed variance and mean ^\ud835\udc66 specified by the model with learnable parameters \ud835\udf03 (Goodfellow, Bengio, and Courville 2016). Maximum likelihood estimation is performed by maximizing the conditional log-likelihood over the training examples (\ud835\udc65(\ud835\udc56), \ud835\udc66(\ud835\udc56)) to turn the possibly large product of probabilities \u220f\ufe00 \ud835\udc56 \ud835\udc5d\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59(\ud835\udc66(\ud835\udc56)|\ud835\udc65(\ud835\udc56); \ud835\udf03) into a sum and overcome issues like numerical underflow. The training examples are assumed to be i.i.d. to make this a valid conversion. ^\ud835\udc66(\ud835\udc56) = \ud835\udc53\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59(\ud835\udc65(\ud835\udc56); \ud835\udf03) (2.1.1) \ud835\udc5d\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59(\ud835\udc66|\ud835\udc65; \ud835\udf03) = \ud835\udca9(\ud835\udc66; ^\ud835\udc66, \ud835\udf0e2) (2.1.2) 3 2. Different Types of Learning 4 \ud835\udf03\ud835\udc40\ud835\udc3f = arg max \ud835\udf03 \u2211\ufe01 \ud835\udc56 log \ud835\udc5d\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59(\ud835\udc66(\ud835\udc56)|\ud835\udc65(\ud835\udc56); \ud835\udf03) (2.1.3) = arg max \ud835\udf03 \u2211\ufe01 \ud835\udc56 log( 1 \u221a\ufe00 2\ud835\udf0b\ud835\udf0e2 \ud835\udc52 \u2212 (\ud835\udc66(\ud835\udc56)\u2212^\ud835\udc66(\ud835\udc56))2 2\ud835\udf0e2 ) (2.1.4) = arg max \ud835\udf03 \u2211\ufe01 \ud835\udc56 \u2212 log \ud835\udf0e \u2212 1 2 log(2\ud835\udf0b) \u2212 (\ud835\udc66(\ud835\udc56) \u2212 ^\ud835\udc66(\ud835\udc56))2 2\ud835\udf0e2 (2.1.5) Removing all terms that do not depend on the parameters \ud835\udf03 yields a formula very similar to MSE. Usually it is not possible to directly estimate \ud835\udf03\ud835\udc40\ud835\udc3f due to computational limitations. Therefore, it is necessary to perform gradient descent to optimize the parameters step by step. \ud835\udf03\ud835\udc40\ud835\udc3f = arg min \ud835\udf03 \u2211\ufe01 \ud835\udc56 (\ud835\udc66(\ud835\udc56) \u2212 ^\ud835\udc66(\ud835\udc56))2 (2.1.6) 2.2 Unsupervised Learning When there is no output specified, machine learning models can still approximate a distribution \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(\ud835\udc65) and thus learn to find structures inherent in the data. This can be useful for tasks like denoising, clustering or pretraining some parameters of a model for a supervised learning task. Vincent et al. (2008) and Kingma and Welling (2013) provide examples of successfully applied unsupervised learning. 2.3 Reinforcement Learning Many real world applications like robotic tasks or playing video games require learning to perform a series of actions \ud835\udc4e\ud835\udc61, for instance pressing buttons or moving a robotic arm to a target by controlling motors, while the state of the environment \ud835\udc60\ud835\udc61 can be observed between the actions often forming a trajectory over time: \ud835\udf0f = {\ud835\udc601, \ud835\udc4e1, \ud835\udc602, \ud835\udc4e2...\ud835\udc60\ud835\udc47 }. Each complete trajectory starting from an initial state forms an episode, which can be ended after a predefined number of time steps or after reaching a terminal state. Besides trajectory centric or episodic reinforcement learning, there are continuing environments that do not use trajectories of fixed length, but visit every possible state infinitely often with a certain probability (Sutton and Barto 1998). A common property of reinforcement learning tasks is that there is no previously known solution to the problem but an implicit aim in form of a reward, which could be the score in a video game or a distance measure stating how well a robot performed the task of moving to a specific position. Learning from this kind of signal 2. Different Types of Learning 5 is biologically plausible, because some brain areas like the basal ganglias are supposed to follow the same strategy (Doya 2000). Reinforcement learning enables intelligent algorithms to learn, when the objective is not to directly model a specific probability distribution like discussed above, but to maximize a scalar reward signal \ud835\udc5f\ud835\udc61+1 = \ud835\udc5f(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) assigned to each pair of state and action over time. The reward at each time step depends on both the current state and the chosen action. At each time step the action \ud835\udc4e\ud835\udc61 is sampled from a policy function \ud835\udc5d(\ud835\udc4e\ud835\udc61) = \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc61|\ud835\udc60\ud835\udc61) with learned parameters \ud835\udf03, which can also take the deterministic form \ud835\udc4e\ud835\udc61 = \ud835\udf07\ud835\udf03(\ud835\udc60\ud835\udc61). The next state \ud835\udc60\ud835\udc61+1 is then generated by the dynamics of the environment, that are restricted to satisfy the markov property \ud835\udc5d(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc5d(\ud835\udc60\ud835\udc61+1|\ud835\udc601, \ud835\udc4e1...\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). The trajectories are thus sampled from a markov decision process (MDP). The state \ud835\udc60\ud835\udc61 sometimes can be only partially observed (Hausknecht and Stone 2015), which yields a partially observable markov decision process (POMDP). Chapter 3 Variants of Reinforcement Learning The goal of episodic reinforcement learning algorithms is to maximize the expected accumulated reward or return \ud835\udc451 = E\ud835\udf0b(\u2211\ufe00\ud835\udc47\u22121 \ud835\udc56=1 \ud835\udefe\ud835\udc56\u22121\ud835\udc5f\ud835\udc56+1) for all initial states \ud835\udc601, where T denotes the end of the episode, \ud835\udc5f\ud835\udc61+1 the reward for state \ud835\udc60\ud835\udc61 and action \ud835\udc4e\ud835\udc61, and \ud835\udefe \u2208 [0, 1] is a discounting factor for future rewards. At each time step, the direct reward depends on the current state and the action chosen by the model and thus on the executed policy. Theoretically, many sampled trajectories including all possible states and actions at different positions in time would be needed to infer a policy that reliably maximizes the expected return over all possible trajectories. For continuing environments, an estimate of the average reward per time step denoted as \u2211\ufe00 \ud835\udc60\u2208\ud835\udc46 \ud835\udc5d\ud835\udf0b(\ud835\udc60) \u2211\ufe00 \ud835\udc4e\u2208\ud835\udc34 \ud835\udf0b(\ud835\udc4e|\ud835\udc60)\ud835\udc5f(\ud835\udc60, \ud835\udc4e) can be used as a measure of the policy quality, where \ud835\udc46 and \ud835\udc34 are the respective sets of all states and actions and \ud835\udc5d\ud835\udf0b(\ud835\udc60) is the probability of arriving at state \ud835\udc60 when following policy \ud835\udf0b (Sutton and Barto 1998). While we focus on episodic reinforcement learning in the following, it is possible to apply most of the discussed algorithms also to continuing environments. 3.1 Common Reinforcement Learning Issues Many recent improvements in the field of reinforcement learning are caused by the difficulties that occur, when trying to apply standardized algorithms in realistic environments. In the following, we will first name some of the main difficulties reinforcement learning algorithms are confronted with and then present various methods researchers have invented to overcome these limitations, starting with relatively simple tabular methods that have been used for a long time and continuing with recent approaches to improve training performance and especially enable algorithms to deal with highly complex environments. 6 3. Variants of Reinforcement Learning 7 Exploring the state space: In contrast to randomly exploring the state space, especially deterministic policies, when they are used to sample trajectories, do not visit large parts of the state space and thus lead to algorithms converging to suboptimal solutions. During sampling it is required to deliberately include suboptimal actions to explore unseen parts of the state space. Whether to greedily choose the assumed best action or a random action is known as the exploitationexploration tradeoff (Kaelbling, Littman, and Moore 1996; Woergoetter and Porr 2008). Complex state spaces: In practice, reinforcement learning algorithms should be able to deal with large state spaces like pixel images that could be observations of a camera (Mnih, Kavukcuoglu, Silver, Graves, et al. 2013). This imposes the difficulty of learning a policy even when only a small subset of all possible states will be visited during training. The fact that any image generated by a random generator almost never looks like any realistic scene, although there is a non-zero probability for it to do so, suggests that comparatively few examples of an image of given size correspond to natural images (Goodfellow, Bengio, and Courville 2016). Furthermore, some of the possible states might not be reached during training due to restricted training time. Images from cameras or other complex state representations might also appear noisy, which again complicates the task of learning a good policy. Continuous action spaces: For realistic environments it is often not sufficient to design a policy that deterministically outputs a discrete action or stochastically models a probability distribution over a set of discrete actions. In robotic applications, motor torques must be produced that can take any continuous value and must be exact to make the robot move correctly. Although it is sometimes possible to successfully discretize a continuous action space, using this approach always means losing flexibility (Lillicrap et al. 2015). For problems with multiple continuous actions, like producing multiple motor torques in parallel, the number of corresponding discrete actions rises exponentially, which quickly makes following this solution impossible. High variance of the trained estimator: While the policy estimation should normally converge to an optimum when the number of training examples is large, practical environments induce strong correlations between samples that are temporally close to each other. A robot might be expected to behave very similarly in close situations and the state representation might not change significantly. Thus, the variance of the trained estimator will be high, as the samples from the environment change slowly and the learned model will always tend to overfit the training data currently presented and forget important past transitions (Lin 1992). This means it will more likely fail to generalize to other data. The lack of generalization will at least negatively affect the training performance or the algorithm will not be able to learn a resonable policy at all. 3. Variants of Reinforcement Learning 8 Partially observable environments: For some environments it might be impossible to observe the complete state \ud835\udc60\ud835\udc61. A robotic camera for instance might not be able to capture the whole scene with all objects relevant for the task, but only parts of it. The impact of the action that has to be generated by the policy depends on the system dynamics, which base on \ud835\udc60\ud835\udc61. That makes it necessary to gather more information by memorizing multiple observations to predict a reasonable action (Hausknecht and Stone 2015). 3.2 Algorithms that Optimize Value Functions 3.2.1 Reinforcement Learning with Tabular Value Functions The expected return for the initial state with respect to the policy is the target for optimization, like discussed above, as it spans the whole trajectory. However, only estimating \ud835\udc451 is not sufficient to derive an optimal policy, because the policy also needs to output an optimal action for every intermediate state. If both the state and action space are discrete, the straightforward way to find an optimal policy and thus maximize the expected return over trajectories is to either estimate the optimal state-value function \ud835\udc49 * or the optimal action-value function \ud835\udc44* (Sutton and Barto 1998). The state-value function specifies the expected return of a state \ud835\udc60\ud835\udc61 when following an optimal policy, whereas the action-value function specifies the expected return when choosing action \ud835\udc4e\ud835\udc61 in state \ud835\udc60\ud835\udc61 and following an optimal policy subsequently. \ud835\udc49 *(\ud835\udc60\ud835\udc61) = max \ud835\udc4e\ud835\udc61 E[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc49 *(\ud835\udc60\ud835\udc61+1)] (3.2.1) \ud835\udc44*(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = E[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe max \ud835\udc4e\ud835\udc61+1 \ud835\udc44*(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1)] (3.2.2) With good estimates of all \ud835\udc49 *(\ud835\udc60\ud835\udc61), an optimal greedy policy can be easily derived by always choosing the action which most probably leads to the best rated states. To estimate the states the environment might take after executing an action, it is required to know the dynamics of the system. For system dynamics \ud835\udc5d(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) action \ud835\udc4e\ud835\udc61 would be chosen according to: arg max \ud835\udc4e\ud835\udc61 \u2211\ufe01 \ud835\udc60\ud835\udc61+1 \ud835\udc5d(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)\ud835\udc49 *(\ud835\udc60\ud835\udc61+1) (3.2.3) 3. Variants of Reinforcement Learning 9 With good estimates of all \ud835\udc44*(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) it is no longer necessary to know the dynamics of the system, because the action-value function implicitly captures the transition probabilities. Many learning algorithms thus focus on approximating \ud835\udc44* rather than \ud835\udc49 *. These algorithms can be applied to a broad range of environments with different dynamics without the need to train a model of the environment and are thus called model-free (Kaelbling, Littman, and Moore 1996). The optimal action with respect to \ud835\udc44* can be chosen as simply as: arg max \ud835\udc4e\ud835\udc61 \ud835\udc44*(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) (3.2.4) To form a learning algorithm, the action-value function is redefined with respect to an arbitrary policy \ud835\udf0b. The value of \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) corresponds to the expected return when taking action \ud835\udc4e\ud835\udc61 in step \ud835\udc60\ud835\udc61 and following policy \ud835\udf0b subsequently. The calculated estimates can then be used to improve the policy. Intuitively, for optimal \ud835\udf0b, the action-value function with respect to the policy converges to \ud835\udc44*(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). It is possible to estimate \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) for example using a Monte-Carlo estimate of the expected return after the end of each episode. This strategy is called offline learning, because it must wait until an episode has finished. However, it is much more practical to store estimates of \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) in an array \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) and reuse existing estimates for future states, which is called bootstrapping and yields an online learning rule (Sutton and Barto 1998): \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) += \ud835\udefc(\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)) (3.2.5) The transitions between states are sampled from the system dynamics and the actions from the policy \ud835\udf0b. Continuously updating the estimates of \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) while improving the policy to follow the maximal Q-values leads to the SARSA algorithm presented in Listing 3.1. Listing 3.1: SARSA (State-Action-Reward-State-Action), reproduced in essence from Sutton and Barto (1998). 1 Initialize Q(s,a) arbitrarily 2 Repeat (for each episode): 3 Initialize \ud835\udc601 4 Choose \ud835\udc4e1 from \ud835\udc601 using policy derived from \ud835\udc44 5 Repeat (for each \ud835\udc61 of episode): 6 Take action \ud835\udc4e\ud835\udc61, observe \ud835\udc5f\ud835\udc61, \ud835\udc60\ud835\udc61+1 7 Choose \ud835\udc4e\ud835\udc61+1 from \ud835\udc60\ud835\udc61+1 using policy derived from \ud835\udc44 8 \ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2190 \ud835\udc44(\ud835\udc60, \ud835\udc4e) + \ud835\udefc[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)] 9 \ud835\udc60\ud835\udc61 \u2190 \ud835\udc60\ud835\udc61+1; \ud835\udc4e\ud835\udc61 \u2190 \ud835\udc4e\ud835\udc61+1; SARSA performs on-policy bootstrapping, because the expected return \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e\ud835\udc61+1) of the next state depends on the policy \ud835\udf0b. It is also possible to alter the update rule to bootstrap using the best action in the next state yielding the popular Q-learning algorithm 3. Variants of Reinforcement Learning 10 introduced by Watkins and Dayan (1992), which is presented in Listing 3.2. This algorithm can be viewed as directly approximizing the action-value function of an optimal policy, as the policy used for bootstrapping is directly given by the algorithm and does not necessarily match the policy \ud835\udf0b used for sampling. Therefore Q-learning is called an off-policy algorithm. The Q-values in Q-learning are updated as follows: \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) += \ud835\udefc(\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc4e\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)) (3.2.6) Listing 3.2: Q-Learning, reproduced in essence from Sutton and Barto (1998). 1 Initialize Q(s,a) arbitrarily 2 Repeat (for each episode): 3 Initialize \ud835\udc601 4 Repeat (for each \ud835\udc61 of episode): 5 Choose \ud835\udc4e\ud835\udc61 from \ud835\udc60\ud835\udc61 using policy derived from \ud835\udc44 6 Take action \ud835\udc4e\ud835\udc61, observe \ud835\udc5f\ud835\udc61, \ud835\udc60\ud835\udc61+1 7 \ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2190 \ud835\udc44(\ud835\udc60, \ud835\udc4e) + \ud835\udefc[\ud835\udc5f\ud835\udc61+1 + \ud835\udefe max\ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)] 8 \ud835\udc60\ud835\udc61 \u2190 \ud835\udc60\ud835\udc61+1; 3.2.2 Using Stochastic Policies to Improve Exploration One beneficial property of an off-policy learning approach like Q-learning (Listing 3.2) is that any policy can be used for sampling, while the learning rule is not affected. It is in particular possible to design a policy for sampling that encourages exploration and thus leads to a better search in the state space as stated above, while the learned policy is still greedy. An exploring policy needs to be based on some kind of randomization to detect unknown or less frequently visited regions in state space. Policies of that kind do not deterministically predict an action to execute but rather model a probability distribution from which the actions will be sampled. With off-policy training algorithms it is possible to train a deterministic policy while using a stochastic policy for sampling. A popular choice for a policy that can be used to encourage exploration during sampling, when in most cases the best action known so far (greedy action) should be executed is the \ud835\udf16-greedy policy (Sutton and Barto 1998). The parameter \ud835\udf16 \u2208 [0, 1] specifies the probability of selecting a completely random action. In all other cases the assumed best action will be executed: \ud835\udc4e\ud835\udc61 = {\ufe03 a random action with probability \ud835\udf16 arg max\ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e) otherwise (3.2.7) 3. Variants of Reinforcement Learning 11 This strategy however ignores the fact, that there might be large differences between the Qvalues of the non-optimal actions, which are all chosen with equal probability. The softmax policy (Sutton and Barto 1998) consideres this fact and assigns a different probability to each action that depends on the Q-value of the action: \ud835\udc5d(\ud835\udc4e\ud835\udc61)\ud835\udc60\ud835\udc61 = \ud835\udc52\ud835\udc44(\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61)/\ud835\udf0f \u2211\ufe00 \ud835\udc4e \ud835\udc52\ud835\udc44(\ud835\udc60\ud835\udc61,\ud835\udc4e)/\ud835\udf0f (3.2.8) The parameter \ud835\udf0f is called the temperature and regularizes the amount of randomization induced. When \ud835\udf0f decreases, the policy becomes more and more deterministic or greedy, while increasing \ud835\udf0f encourages exploration. 3.2.3 Using Eligibility Traces to Improve Bootstrapping The bootstrapping idea presented in section 3.2.1 only supports direct updates of the value function estimates with respect to the estimated value of the following action. This can considerably slow down learning, especially if there are long chains of consecutive actions. A robot might need to appoach a target for a long time before reaching it and thus might get a reward only after executing many steps beforehand. In this case, the reward associated with the last action is known as a delayed reward (Watkins 1989). To obtain positive value estimates for the first actions in the chain, many episodes are needed, because during the first episode only the value of the last action is updated, then the value of the second last action due to bootstrapping and so on. This problem is also known as the credit assignment problem (Woergoetter and Porr 2008), as present actions might be essential for future rewards, but do not directly benefit from it. With Monte Carlo estimates it is not necessary to repeatedly visit the same consecutive actions multiple times to obtain valid value estimates at the beginning of the chain, because the accumulated return over all future actions is directly used to form the estimates. Using Monte Carlo methods however causes other issues like high variance. It is possible to overcome this tradeoff by using n-step bootstrapping, that spans over multiple steps and can also include weights to raise the impact of rewards to temporally close actions (Sutton and Barto 1998). N-step bootstrapping provides an intermediate solution that combines the advantages of Monte Carlo estimates and direct bootstrapping from the next action-value. The Q-Learning algorithm can be viewed as comparing the expected return by choosing a specific action to the current action-value estimate. The expected return is obtained using bootstrapping from the next action value, called one-step bootstrapping. The update of the current Q-value with a learning rate \ud835\udefc is then given by: 3. Variants of Reinforcement Learning 12 \u0394\ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udefc(\ud835\udc45(1) \ud835\udc61 \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61)) (3.2.9) \ud835\udc45(1) \ud835\udc61 = \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc4e\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e) (3.2.10) The term \ud835\udc45(1) \ud835\udc61 is called the one-step return. In contrast, the Monte Carlo estimate starting from state t would accumulate the rewards till the end of the episode without any bootstrapping: \ud835\udc45\ud835\udc61 = \ud835\udc47\u22121 \u2211\ufe01 \ud835\udc56=\ud835\udc61 \ud835\udefe\ud835\udc56\u2212\ud835\udc61\u22121\ud835\udc5f\ud835\udc56+1 (3.2.11) The n-step return mixes both views and bootstraps from the value of the action executed n timesteps after t. If the episode ends earlier, the n-step return is equivalent to the Monte Carlo estimate \ud835\udc45\ud835\udc61. \ud835\udc45(\ud835\udc5b) \ud835\udc61 = \ud835\udefe\ud835\udc5b\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc4e\ud835\udc44(\ud835\udc60\ud835\udc61+\ud835\udc5b, \ud835\udc4e) + \ud835\udc61+\ud835\udc5b\u22121 \u2211\ufe01 \ud835\udc56=\ud835\udc61 \ud835\udefe\ud835\udc56\u2212\ud835\udc61\u22121\ud835\udc5f\ud835\udc56+1 (3.2.12) As the best value of n is hard to predict, but it is straightforward to see that temporally closer actions should have a greater effect on each other, we can accumulate all n-step returns with a decaying weighting factor \ud835\udf06(\ud835\udc5b\u22121): \ud835\udc45(\ud835\udf06) \ud835\udc61 = (1 \u2212 \ud835\udf06) \u221e \u2211\ufe01 \ud835\udc5b=1 \ud835\udf06\ud835\udc5b\u22121\ud835\udc45(\ud835\udc5b) \ud835\udc61 (3.2.13) It is even possible to formulate an online learning algorithm using this idea, by memorizing actions executed in the past. The eligibility trace (Sutton 1988) assigns a value to each pair of state and action that is reset to 1, when the action is executed in the respective state and then decays by \ud835\udefe\ud835\udf06 after every time step. Q-Learning with eligibility traces is called Q(\ud835\udf06) learning and updates all Q-values at every time step as follows: \u2200\ud835\udc60,\ud835\udc4e : \ud835\udc44(\ud835\udc60, \ud835\udc4e) = \ud835\udc44(\ud835\udc60, \ud835\udc4e) + \ud835\udefc\ud835\udeff\ud835\udc61\ud835\udc52(\ud835\udc60, \ud835\udc4e) (3.2.14) \ud835\udeff\ud835\udc61 = \ud835\udc5f\ud835\udc61+1 + \ud835\udefe max \ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) (3.2.15) \u2200\ud835\udc60,\ud835\udc4e : \ud835\udc52(\ud835\udc60, \ud835\udc4e) = {\ufe03 1 \ud835\udc56\ud835\udc53(\ud835\udc60, \ud835\udc4e) = (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \ud835\udefe\ud835\udf06\ud835\udc52(\ud835\udc60, \ud835\udc4e) \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 (3.2.16) 3. Variants of Reinforcement Learning 13 The term \ud835\udeff\ud835\udc61 is known as the TD-error for step t and \ud835\udc52(\ud835\udc60, \ud835\udc4e) is the eligibility trace assigned to state s and action a. It is straightforward to see, that eligibility traces mix the ideas of Monte Carlo estimates and one-step bootstrapping, as the update rules are equivalent to Monte Carlo estimates for \ud835\udf06 = 1 and to one-step bootstrapping for \ud835\udf06 = 0. In practice, further adjustments might be necessary to regard the case, when exploration happens (see section 3.2.2) and thus the chain of assumed optimal actions is broken (Watkins 1989). It is also possible to accumulate the value of eligibility traces, when a pair of state and action is visited often in a short length of time. Figure 3.1 compares replacing traces used in equation 3.2.16 to accumulating traces. Figure 3.1: Accumulating eligibility traces do not have a fixed maximum value, but can grow larger than 1, when a pair of state and action is visited often. Figure adapted from Sutton and Barto (1998). 3.2.4 Using Neural Networks to Approximate the Q-function The tabular methods described in the sections above do not apply to large discrete or continuous state spaces, which require a specific function approximator to estimate the Q-function. Estimating the Q-function can be done by a supervised learning algorithm with the targets for training given by the reinforcement learning algorithm. Therefore a loss function is introduced that drives the function approximator to output the correct Q-values, where \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e; \ud835\udf03) is a function, parametrized by learned parameters \ud835\udf03: \u2112(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61+1, \ud835\udc60\ud835\udc61+1, \ud835\udf03) = (\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc4e\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e; \ud835\udf03) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61; \ud835\udf03))2 (3.2.17) Neural networks have proven to be effective for function approximation and supervised learning tasks. Architectures solving supervised learning tasks, like estimating the Qvalues in the reinforcement learning setting, usually take the form of feed-forward neural networks. This means, the neurons are organized in layers, that are ordered and connected 3. Variants of Reinforcement Learning 14 in only one direction from the input layer to the output layer. Normally, there are no connections between neurons in a single layer. Deep feed-forward neural networks with multiple intermediate layers (hidden layers) between input and output layer are able to deal with complex state spaces and can generalize to unknown states that were never observed during training (Levine, Finn, et al. 2016). A neural network with only feedforward connections, non-linear activation functions and one or more hidden layers is also known as a multi-layer perceptron (MLP). The architecture of a simple Q-network is depicted in figure 3.2. The network takes the state representation as input and outputs the Q-values for all actions using seperate output neurons. The action space thus still needs to be discrete to enable the network to learn the Q-values for all actions. states multilayer neural network Q value 1 Q value 2 Q value 3 Figure 3.2: Architecture of a simple deep Q-network with 3 discrete actions. 3.2.5 Using Convolutional Layers to Process Pixel Images To extract useful information from pixel images, it is often not sufficient to only use fully connected feed-forward layers, where every neuron of one layer is connected to every unit of the next layer. Like required by other supervised learning tasks, in the reinforcement learning setup, neural networks should be able to perform a generalization task and reduce the complexity of the state space to being able to correctly predict the Q-values later. While it is theoretically possible to capture the information presented in an image by purely using fully connected layers, network structures like these induce several problems. First of all, a great number of neurons and thus even more connections would be needed, because each pixel in an image must be represented by a single neuron or even multiple 3. Variants of Reinforcement Learning 15 Figure 3.3: Convolutional layers can be used to detect features in images and effectively eliminate background noise. (a) Sample activations of the input layer of a neural network. (b) Corresponding activations of neurons in the first hidden layer, produced by a filter that detects mainly low level features like edges. (c) Corresponding activations of neurons in the second hidden layer, produced by a filter that detects high level features like the position of the star in the image. neurons for multiple color channels. To store the associated parameters, massive amounts of memory would be required and it would also take comparatively long time to calculate the outputs of the network for given inputs. Because of the large amount of parameters, fully connected layers converge slowly and can even completely fail to capture the relevant information of larger pixel images, if not carefully designed. Downsampling of the images reduces the complexity of the network, but also discards a lot information. Convolutional networks (CNNs) were successfully applied by Krizhevsky, Sutskever, and G. E. Hinton (2012) to win the ImageNet challenge with an exceptional good score. The convolution operation applied to neural networks deliberately restricts the connections between two layers to be local. This is a reasonable assumption, as the pixels in an image, that are close to each other, are normally much stronger correlated. Each unit of the first hidden layer is thus a function of only a small patch of the input image. Furthermore, the connections between each image patch and the corresponding unit in the next layer are restricted to be equal over the whole image, which again reduces the amount of parameters needed. Figure 3.3 shows a simple example of convolutions applied to identify an object in an image. The filter function that slides over the image to produce the next neural layer is called a kernel and output of the convolution operation a feature map of the input (Goodfellow, Bengio, and Courville 2016). Most of the time, an additional activation function is executed after the convolution operation to transform the elements of the feature map to the activations of the next layer. Multiple learned kernels can be used to produce multiple feature maps for detecting different features in the image. Stacking convolutional layers 3. Variants of Reinforcement Learning 16 enables the network to learn a hierarchical form of dependencies between pixels in distant regions of the original input image. Especially for applications that are only interested in extracting features from images and not in where these features occur in the image, it is useful to downsample the intermediate feature maps. This operation is called pooling (Scherer, M\u00fcller, and Behnke 2010). Fully connected layers can then be used to further process the output after several convolutional layers, which has greatly reduced dimensionality. Convolutional layers can also be transposed to reconstruct image data from a lower dimensional representation. Each activation in a feature map produced by a convolutional layer is a function of the convolution kernel and the input. For X being the two-dimensional input image, K the kernel and Y the elements of the feature map, the convolution operation (denoted as *) can be applied to transform X to Y: \ud835\udc4c = \ud835\udc4b * \ud835\udc3e (3.2.18) The convolution operation flips the kernel to obtain a commutative operation. If a twodimensional matrix kernel is used, that has odd height m and width n to be centered on one pixel whose coordinates in the kernel are defined to be (0,0), equation 3.2.18 decomposes to: \ud835\udc4c (\ud835\udc56, \ud835\udc57) = (\ud835\udc4b * \ud835\udc3e)(\ud835\udc56, \ud835\udc57) = \ud835\udc5a\u22121 2 \u2211\ufe01 \ud835\udc601= \u2212\ud835\udc5a+1 2 \ud835\udc5b\u22121 2 \u2211\ufe01 \ud835\udc602= \u2212\ud835\udc5b+1 2 \ud835\udc4b(\ud835\udc56 \u2212 \ud835\udc601, \ud835\udc57 \u2212 \ud835\udc602)\ud835\udc3e(\ud835\udc601, \ud835\udc602) (3.2.19) 3.2.6 Deep Q-network (DQN) Mnih, Kavukcuoglu, Silver, Graves, et al. (2013) showed that deep learning with convolutional layers can enable reinforcement learning algorithms to successfully learn to play Atari 2600 games. An improved version of this approach was presented later as deep Q-network (Mnih, Kavukcuoglu, Silver, Rusu, et al. 2015), that was able to use direct training from pixels to actions to play 49 different Atari games without the need to change the hyperparameters of the network or make any other modifications for a specific game. Zhang et al. (2015) showed how to train a robotic arm with the DQN approach to perform reaching tasks while only observing camera images. The performance on Atari games is comparatively impressive, as the learned policies were often able to outperform human players and only the pixel images and the game score were used as input to the training algorithm, which means that there was no domain knowledge available to the algorithm. 3. Variants of Reinforcement Learning 17 While some Atari games can be directly modeled as fully observable MDPs as discussed in section 2.3, it is not possible to infer properties like the velocity of objects from a single image, which is necessary to establish effective strategies for some games. Therefore, a sequence of four frames is passed into the network to provide the missing information to the network and approximately satisfy the markov property. The success of DQN however does not only rely on the usage of a neural network function approximator. There are some problems that would practically prevent neural networks as nonlinear function approximators from converging. First, the loss function given in equation 3.2.17 includes the parameters \ud835\udf03 twice, which arguably makes learning instable. The foresight into the future \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e; \ud835\udf03) to be used for bootstrapping should not directly depend on \ud835\udf03 to stabilize learning and reduce the variance of the approximated function. The DQN training method therefore introduces a target Q-network, that copies the parameters from the trained Q-network only after several hundred or thousand training steps and thus does not change rapidly and enables the algorithm to learn stable long term dependencies (Mnih, Kavukcuoglu, Silver, Rusu, et al. 2015). With parameters \ud835\udf03\u2212 of the target network, the loss function changes to: \u2112\ud835\udc37\ud835\udc44\ud835\udc41(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61+1, \ud835\udc60\ud835\udc61+1, \ud835\udf03, \ud835\udf03\u2212) = (\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc4e\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e; \ud835\udf03\u2212) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61; \ud835\udf03))2 (3.2.20) Unfortunately, simple gradient descent on the loss function with target network can still lead to high variance of the function estimator due to the inherent structure of the learned data. Especially if collecting more training examples is costly as it is with data generated by a real robotic system, other ways need to be found to encourage generalization of the trained network. The technique used for training the DQN is essentially equivalent to stochastic gradient descent on a memory of past transitions of the reinforcement learning environment called the experience replay memory (Lin 1992). The idea behind stochastic gradient descent is to use random samples of relatively few training examples to estimate the expectation of the true training error. When the examples are sampled from very different time steps and were generated under different conditions, they can be sufficient to provide a good estimate of the true trainig error with relatively low variance. The experience replay memory stores transitions between states sampled in the past, even for multiple episodes, and also memorizes the corresponding actions and rewards to being able to correctly calculate the loss at every time step in the future. Thus, the memory consists of samples (\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56+1, \ud835\udc60\ud835\udc56+1) for each recorded time step. 3. Variants of Reinforcement Learning 18 Listing 3.3: Deep Q-Network (DQN) with experience replay and target network, adapted from Mnih, Kavukcuoglu, Silver, Rusu, et al. (2015). 1 Initialize replay memory \ud835\udc37 2 Initialize action-value function \ud835\udc44 with random weights \ud835\udf03 3 Initialize target action-value function ^\ud835\udc44 with weights \ud835\udf03\u2212 = \ud835\udf03 4 for episode = 1 to \ud835\udc40 do 5 Initialize sequence \ud835\udc601 = {\ud835\udc651} and preprocessed sequence \ud835\udf111 = \ud835\udf11(\ud835\udc601) 6 for \ud835\udc61 = 1 to \ud835\udc47 do 7 Select \ud835\udc4e\ud835\udc61 = {\ufe03 a random action with probability \ud835\udf16 arg max\ud835\udc4e \ud835\udc44(\ud835\udf11(\ud835\udc60\ud835\udc61), \ud835\udc4e; \ud835\udf03) otherwise 8 9 Execute action \ud835\udc4e\ud835\udc56 in emulator and observe reward \ud835\udc5f\ud835\udc61 and image \ud835\udc65\ud835\udc61+1 10 Set \ud835\udc60\ud835\udc61+1 = \ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc65\ud835\udc61+1 and preprocess \ud835\udf11\ud835\udc61+1 = \ud835\udf11(\ud835\udc60\ud835\udc61+1) 11 Store transition (\ud835\udf11\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udf11\ud835\udc61+1) in \ud835\udc37 12 13 // sample from experience replay memory 14 Sample random minibatch of transitions (\ud835\udf11\ud835\udc57, \ud835\udc4e\ud835\udc57, \ud835\udc5f\ud835\udc57, \ud835\udf11\ud835\udc57+1) from \ud835\udc37 15 Set \ud835\udc66\ud835\udc57 = {\ufe03 \ud835\udc5f\ud835\udc57 if episode terminates at step \ud835\udc57 + 1 \ud835\udc57 + \ud835\udefe max\ud835\udc4e\u2032 ^\ud835\udc44(\ud835\udf11\ud835\udc57+1, \ud835\udc4e\u2032; \ud835\udf03\u2212) otherwise 16 Perform a gradient descent step on (\ud835\udc66\ud835\udc57 \u2212 \ud835\udc44(\ud835\udf11\ud835\udc57, \ud835\udc4e\ud835\udc57; \ud835\udf03))2 w.r.t. to the network parameters \ud835\udf03 17 18 // update target network 19 Every \ud835\udc36 steps reset ^\ud835\udc44 = \ud835\udc44, that means, set \ud835\udf03\u2212 = \ud835\udf03 20 end for 21 end for Let \ud835\udc5a denote the number of all examples stored in the experience replay memory and \ud835\udc5a\u2032 a relatively small number of examples, which are supposed to be randomly sampled from the memory, called the batch size, where \ud835\udc5a\u2032 \u226a \ud835\udc5a. The goal is to minimize the loss for all examples \ud835\udc3d(\ud835\udf03) = 1 \ud835\udc5a \u2211\ufe00\ud835\udc5a \ud835\udc56=1 \ud835\udc3f(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56+1, \ud835\udc60\ud835\udc56+1, \ud835\udf03). For simplicity, the uninteresting dependency of the loss function on \ud835\udf03\u2212 is left out here, but could be easily added. The sum of all losses can be approximized as follows: \ud835\udc3d(\ud835\udf03) = 1 \ud835\udc5a \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \u2112(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56+1, \ud835\udc60\ud835\udc56+1, \ud835\udf03) \u2248 1 \ud835\udc5a\u2032 \ud835\udc5a\u2032 \u2211\ufe01 \ud835\udc56=1 \u2112(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56+1, \ud835\udc60\ud835\udc56+1, \ud835\udf03) (3.2.21) Gradient descent can then be intuitively performed by shifting the parameters in the direction of the negative gradient: \ud835\udc54 = \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) \u2248 1 \ud835\udc5a\u2032 \u2207\ud835\udf03 \ud835\udc5a\u2032 \u2211\ufe01 \ud835\udc56=1 \u2112(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56+1, \ud835\udc60\ud835\udc56+1, \ud835\udf03) (3.2.22) 3. Variants of Reinforcement Learning 19 \ud835\udf03 \u2212= \ud835\udefc\ud835\udc54 (3.2.23) Listing 3.3 shows the DQN algorithm of the original publication in pseudocode including all main concepts discussed so far, namely: using neural networks as function approximators for the action-value function (Q-function), including target networks for bootstrapping of the action-value function, and using experience replay as a variant of stochastic gradient descent. The parameter \ud835\udefc in equation 3.2.23 denotes the learning rate. In fact, simple gradient descent like this can work reasonably well for a learning rate that is appropriate for the optimized problem, but also very likely fails to converge for a randomly chosen and fixed learning rate. While it is possible to find a suitable learning rate by searching the the hyperparameter space, this would be computationally expensive as many training steps need be done to evaluate a single learning rate. Gradient descent can be seen as navigating through the hyperplane spanned by the unified loss \ud835\udc3d(\ud835\udf03) (error function) and all parameters \ud835\udf03\ud835\udc56 being the coordinates. Learning with any fixed learning rate thus can be slow, because it takes long time to move across flat regions, where the gradient is very small. When gradient descent arrives at a specific point of the hyperplane, it is possible for the error function to rapidly decrease in only one or few directions of the parameter space while moving in other directions leaves the error nearly unchanged (Goodfellow, Bengio, and Courville 2016). This motivates the use of learning algorithms that adapt to the shape of the parameter space. Figure 3.4: With the additional momentum term, gradient descent arrives faster at the minimum of the cost function without wasting too much time for oscillation. Figure adapted from Goodfellow, Bengio, and Courville (2016). 3. Variants of Reinforcement Learning 20 The RMSProp algorithm introduced by Tieleman and G. Hinton (2012) is able to sidestep the issue of a fixed learning rate by using a seperate learning rate for each direction of the parameter space and automatically adapting these learning rates to the magnitude of the gradient for the respective direction. It is often beneficial to include an additional momentum term in the parameter update that plays the same role like velocity in physics. This term is for example good for reducing the probability of oscillation, when the hyperplane for which the minimum should be found looks like a valley with steep sides. An example of such a situation is depicted in figure 3.4. The Adam algorithm (Kingma and Ba 2014) is a modification of RMSProp, that includes a momentum term by default. 3.2.7 Improvements to DQN Double DQN (D-DQN) as proposed by Van Hasselt, Guez, and Silver (2016) was again able to improve the performance of DQN applied to Atari games by a minor modification of the training target. The loss function with target network given in equation 3.2.20 is considered problematic, because it tends to overestimate the future action values. Both the selection and evaluation of future actions depend on the parameters of the target network \ud835\udf03\u2212. Equation 3.2.20 can thus be rewritten as: \u2112\ud835\udc37\ud835\udc44\ud835\udc41(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61+1, \ud835\udc60\ud835\udc61+1, \ud835\udf03, \ud835\udf03\u2212) = (\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, arg max \ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e; \ud835\udf03\u2212); \ud835\udf03\u2212) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61; \ud835\udf03))2 (3.2.24) The loss function used by D-DQN disentangles the action selection from the evaluation of the selected action by using the trained parameters \ud835\udf03 to select future actions instead of those of the target network: \u2112\ud835\udc37\u2212\ud835\udc37\ud835\udc44\ud835\udc41(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61+1, \ud835\udc60\ud835\udc61+1, \ud835\udf03, \ud835\udf03\u2212) = (\ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, arg max \ud835\udc4e \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udc4e; \ud835\udf03); \ud835\udf03\u2212) \u2212 \ud835\udc44(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61; \ud835\udf03))2 (3.2.25) Another popular approach to improve DQN called a dueling network architecture is proposed by Wang et al. (2015). The authors do not directly use the neural network to predict the Q-function. The network itself predicts two functions of the input with respect to the policy currently followed: A state-value function \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) and an advantage function \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) defined as: \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) (3.2.26) 3. Variants of Reinforcement Learning 21 Both outputs are unified to form an estimate of the Q-value which is then used for training. The loss to be optimized can therefore include all previously made improvements and for instance take the form of equation 3.2.25. It is also possible to use experience replay and a target network exactly like discussed above. The straightforward way to derive \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) from \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) and \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) would be to simply reorganize equation 3.2.26: \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) + \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) (3.2.27) As the learning algorithm only optimizes the Q-function and does not know anything about the underlying components, optimizing equation 3.2.27 almost never leads to \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) or \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) converging to good estimates of a state-value or advantage function. To force gradient descent on \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) to properly estimate these functions, a correction term is introduced: \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) = \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 max \ud835\udc4e \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e) + \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) (3.2.28) When the assumed optimal action \ud835\udc4e* = arg max\ud835\udc4e \ud835\udc34\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e) = arg max\ud835\udc4e \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e) is chosen in state \ud835\udc60\ud835\udc61, all terms including the advantage function cancel out and the state-value function is optimized to be equal to the action-value function. This is a valid optimization, as the value of a state corresponds to the value of the best action to be chosen in that state, formally: \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) = max\ud835\udc4e \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e) = \ud835\udc44\ud835\udf0b(\ud835\udc60\ud835\udc61, \ud835\udc4e*). As \ud835\udc49 \ud835\udf0b(\ud835\udc60\ud835\udc61) is tied to be a valid estimate of the state-value function, the other terms naturally approximate the advantage function defined as the difference between state-value and action-value function. Other forms of the correction term like averaging over all possible actions are able to achieve a similar effect and yield better results in practice (Wang et al. 2015). 3. Variants of Reinforcement Learning 22 3.3 Algorithms that Follow the Policy Gradient Section 3.2 showed that it is possible to derive reasonably performing policies from good estimates of value functions, especially useful are estimates of action-values provided by the Q-function. However, because policies derived from value functions search over a discrete number of Q-values to find the best action, it is not possible to directly obtain policies that output continuous actions using one of the methods described above. Policy gradient methods directly parametrize the policy as a probability function \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc61|\ud835\udc60\ud835\udc61) that can be completely described by the parameters \ud835\udf03 and thus provide maximal freedom to learn any action-generating function. To evaluate different policies, the expected return following \ud835\udf0b over all trajectories conditioned by the policy, formally \ud835\udf0f \u223c \ud835\udc5d\ud835\udf0b(\ud835\udf0f) = \ud835\udc5d(\ud835\udf0f|\ud835\udf03), is used. The return over a single trajectory \ud835\udc5f(\ud835\udf0f) is equal to the measure introduced at the beginning of chapter 3 and takes the form: \ud835\udc5f(\ud835\udf0f) = \ud835\udc47\u22121 \u2211\ufe01 \ud835\udc56=1 \ud835\udefe\ud835\udc56\u22121\ud835\udc5f\ud835\udc56+1 (3.3.1) The term \ud835\udc5f\ud835\udc56+1 is the reward given to action \ud835\udc4e\ud835\udc56 executed in state \ud835\udc60\ud835\udc56 of the respective trajectory. The probability distribution over trajectories \ud835\udc5d(\ud835\udf0f|\ud835\udf03) decomposes as follows: \ud835\udc5d(\ud835\udf0f|\ud835\udf03) = \ud835\udc5d({\ud835\udc601, \ud835\udc4e1..\ud835\udc60\ud835\udc47 , \ud835\udc4e\ud835\udc47 }|\ud835\udf03) = \ud835\udc5d(\ud835\udc601) \ud835\udc47\u22121 \u220f\ufe01 \ud835\udc56=1 \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc56|\ud835\udc60\ud835\udc56)\ud835\udc5d(\ud835\udc60\ud835\udc56+1|\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56) (3.3.2) With these definitions, the target for optimization can be defined: \ud835\udc3d(\ud835\udf03) = \u2211\ufe01 \ud835\udf0f\u2208\ud835\udc47 \ud835\udc5d(\ud835\udf0f|\ud835\udf03)\ud835\udc5f(\ud835\udf0f) (3.3.3) While other methods like using evolutionary algorithms or random search are possible (Deisenroth, Neumann, Peters, et al. 2013), it is straightforward to optimize equation 3.3.3 using gradient ascend: \ud835\udf03 += \ud835\udefc\u2207\ud835\udf03\ud835\udc3d (3.3.4) According to Peters and Bagnell (2011), following the policy gradient to solve reinforcement learning tasks only slightly modifies the parameters of the policy in contrast to value based methods, where large jumps between two estimated policies are possible. This property 3. Variants of Reinforcement Learning 23 arguably improves training stability and convergence towards an optimal policy. In the following, several methods to estimate the gradient of the true expected return with respect to the parameters of the policy will be discussed. 3.3.1 Finite-Difference Methods The main difficulty imposed by equation 3.3.4 is to derive an appropriate estimate of \u2207\ud835\udf03\ud835\udc3d. Analytically calculating the gradient is impossible, as it would be necessary to sum over possibly infinitely many trajectories. The dynamics of the environment \ud835\udc5d(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) might also be unknown and not differentiable anyway. Finite-difference methods (FDM) are simple numerical methods to estimate first order gradients. FDM use the first two terms of the taylor series expansion and rearrange them to get an approximation of the true gradient. Among others, Peters and Bagnell (2011) provide a compact description of how to apply FDM to find the policy gradient in the reinforcement learning setup. Let \ud835\udf03\ud835\udc56 denote the i-th element of the parameter vector \ud835\udf03 and \ud835\udf03 + \ud835\udefc\ud835\udc62\ud835\udc56 a small perturbation of the i-th component of \ud835\udf03. The taylor series expansion approximates \ud835\udc3d(\ud835\udf03 + \ud835\udefc\ud835\udc62\ud835\udc56) by using its partial derivatives: \ud835\udc3d(\ud835\udf03 + \ud835\udefc\ud835\udc62\ud835\udc56) = \ud835\udc3d(\ud835\udf03) + \ud835\udefc\ud835\udf15\ud835\udc3d(\ud835\udf03) \ud835\udf15\ud835\udf03\ud835\udc56 + \ud835\udefc2 2 \ud835\udf152\ud835\udc3d(\ud835\udf03) \ud835\udf15\ud835\udf03\ud835\udc56 2 + ... + \ud835\udefc\ud835\udc5b \ud835\udc5b! \ud835\udf15\ud835\udc5b\ud835\udc3d(\ud835\udf03) \ud835\udf15\ud835\udf03\ud835\udc56 \ud835\udc5b + \ud835\udc45\ud835\udc5b(\ud835\udf03 + \ud835\udefc\ud835\udc62\ud835\udc56) (3.3.5) The partial derivative of \ud835\udc3d with respect to \ud835\udf03\ud835\udc56 can thus be approximated as follows: \ud835\udf15\ud835\udc3d(\ud835\udf03) \ud835\udf15\ud835\udf03\ud835\udc56 \u2248 \ud835\udc3d(\ud835\udf03 + \ud835\udefc\ud835\udc62\ud835\udc56) \u2212 \ud835\udc3d(\ud835\udf03) \ud835\udefc (3.3.6) To obtain the gradient for all components of the parameter vector, all partial derivatives must be approximated. The FDM approach has the beneficial property that it can be used for arbitrary policies, even for not differentiable policies. While the estimate of the gradient can be improved by averaging multiple estimated gradients for different perturbations, badly chosen perturbation can still make learning instable or cause it to fail (Peters and Bagnell 2011). For realistic applications, the used policy can be assumed to be a differentiable function, which enables other estimations to work that are less error-prone and noisy. 3.3.2 Likelihood-Ratio Methods Let \ud835\udc5d(\ud835\udf0f|\ud835\udf03) = \ud835\udc5d\ud835\udf03(\ud835\udf0f) be the probability of trajectory \ud835\udf0f under policy \ud835\udf0b\ud835\udf03. The likelihood ratio trick, best known for its application in the REINFORCE algorithm introduced by Williams 3. Variants of Reinforcement Learning 24 (1992), rewrites the gradient in equation 3.3.4 using the property \u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03(\ud835\udf0f) = \u2207\ud835\udf03\ud835\udc5d\ud835\udf03(\ud835\udf0f) \ud835\udc5d\ud835\udf03(\ud835\udf0f) as follows: \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) = \u2211\ufe01 \ud835\udf0f\u2208\ud835\udc47 \ud835\udc5d\ud835\udf03(\ud835\udf0f)\u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03(\ud835\udf0f)\ud835\udc5f(\ud835\udf0f) (3.3.7) = E\ud835\udf0f\u223c\ud835\udc5d(\ud835\udf0f|\ud835\udf03)[\u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03(\ud835\udf0f)\ud835\udc5f(\ud835\udf0f)] (3.3.8) The expectation of equation 3.3.8 is useful to estimate the gradient of \ud835\udc3d\ud835\udf03 while avoiding the sum over all trajectories, which is intractable. The inner term \u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03(\ud835\udf0f)\ud835\udc5f(\ud835\udf0f) still depends on the possibly unknown or not differentiable system dynamics, which now can be easily excluded using equation 3.3.2, because they do not depend on the parameters \ud835\udf03: \u2207\ud835\udf03 log \ud835\udc5d\ud835\udf03(\ud835\udf0f) = \u2207\ud835\udf03 log \ud835\udc5d(\ud835\udc601) + \ud835\udc47\u22121 \u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc56|\ud835\udc60\ud835\udc56) + \ud835\udc47\u22121 \u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf03 log \ud835\udc5d(\ud835\udc60\ud835\udc56+1|\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56) (3.3.9) = \ud835\udc47\u22121 \u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc56|\ud835\udc60\ud835\udc56) (3.3.10) This means, all knowledge about the dynamics of the environment can be easily discarded to form a model-free estimate of the parameter gradient. In fact, the gradient can be approximated by sampling trajectories from the reinforcement learning environment to form a Monte-Carlo estimate yielding the REINFORCE learning rule (Williams 1992). For any differentiable stochastic policy, it is straightforward to obtain an unbiased estimate of the gradient using this technique. Therefore, equation 3.3.1 and 3.3.10 are incorporated into equation 3.3.8 with m being the number of sampled trajectories and \ud835\udc47\ud835\udc56 the length of i-th trajectory. \ud835\udc60\ud835\udc56 \ud835\udc57 is the j-th state of the i-th trajectory, \ud835\udc4e\ud835\udc56 \ud835\udc57 the j-th action of the i-th trajectory and \ud835\udc5f\ud835\udc56 \ud835\udc57+1 the reward associated to both: \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) \u2248 1 \ud835\udc5a \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc47\ud835\udc56\u22121 \u2211\ufe01 \ud835\udc57=1 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc56 \ud835\udc57|\ud835\udc60\ud835\udc56 \ud835\udc57)\ud835\udefe\ud835\udc57\u22121\ud835\udc5f\ud835\udc56 \ud835\udc57+1 (3.3.11) The original REINFORCE algorithm additionally uses a baseline term to reduce the variance of the gradient estimation. Williams (1992) show, that the baseline term does not introduce a bias, if it is chosen independently from the selected actions. According to 3. Variants of Reinforcement Learning 25 Degris, Pilarski, and Sutton (2012), a reasonable choice for the baseline term is to use an estimate of the state value \ud835\udc49\ud835\udf0b(\ud835\udc60\ud835\udc61). Equation 3.3.11 with incorporated baseline becomes to: \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) \u2248 1 \ud835\udc5a \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc47\ud835\udc56\u22121 \u2211\ufe01 \ud835\udc57=1 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc56 \ud835\udc57|\ud835\udc60\ud835\udc56 \ud835\udc57)(\ud835\udefe\ud835\udc57\u22121\ud835\udc5f\ud835\udc56 \ud835\udc57+1 \u2212 \ud835\udc4f(\ud835\udc60\ud835\udc56 \ud835\udc57)) (3.3.12) 3.3.3 Actor-Critic Methods Section 3.3.2 has shown how to use the likelihood-ratio to estimate the gradient of the target function \ud835\udc3d. Sutton, McAllester, et al. (2000) generalize this insight to the form of the policy gradient theorem: \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) = E{\ud835\udc601,\ud835\udc4e1..\ud835\udc60\ud835\udc47 ,\ud835\udc4e\ud835\udc47 }\u223c\ud835\udc5d(\ud835\udf0f|\ud835\udf03) [\ufe01 \ud835\udc47\u22121 \u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf03 log \ud835\udf0b\ud835\udf03(\ud835\udc4e\ud835\udc56|\ud835\udc60\ud835\udc56)\ud835\udc44\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56) ]\ufe01 (3.3.13) Because there are many well studied methods to approximate value functions, for instance those described in section 3.2, it seems natural to use a second function estimator to also approximate a suitable value function to be used as training target for the policy gradient method. One advantage of this dual training approach in comparision to purely value based methods is, that the policy is still parametrized independently and thus can be used to output continuous actions. Other advantages of policy gradient based methods over value based methods, like making small updates to the parameters of the policy, also still apply when using estimates of a value function as training target. Methods that learn a value function, which is used as training target for an independently parametrized policy are called actor-critic architectures (Sutton and Barto 1998). The actor is the function estimator for the policy discussed so far. The critic is a second function estimator, that estimates a value function. Both parts can be modeled by neural networks. The interactions of the two function estimators are depicted in figure 3.5. A straightforward way to derive an actor-critic like training method from the REINFORCE algorithm discussed in section 3.3.2 is to use the state-value function as baseline, which obviously requires a seperate estimate of a value function. Degris, Pilarski, and Sutton (2012) compare different actor-critic methods and show that they can be applied to solve a robotic task. By approximizing the gradient of the true expected return by the gradient of a value function, a bias might be introduced. Sutton, McAllester, et al. (2000) show that under certain conditions, the gradient is still exact. This formulation is known as the policy gradient theorem with function approximation. 3. Variants of Reinforcement Learning 26 states actor/policy critic action Q-value Figure 3.5: Simple Actor-Critic architecture, where the critic estimates the action-value function. In order to reduce the variance of the critic, it is very sensible not to approximize the Q-function, but another meaningful function, that can be used as target for policy optimization. For example, subtracting the state-value function \ud835\udc49\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc61) from \ud835\udc44\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) leaves the policy gradient theorem intact, as \ud835\udc49\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc61) is independent from the selected action and thus does not alter the gradient with respect to the parameters of the policy. The term \ud835\udc44\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) \u2212 \ud835\udc49\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc61), which is known as the advantage function \ud835\udc34\ud835\udf0b\ud835\udf03(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) from section 3.2.7, has very reduced variance however, as it only models the impact of the currently selected action on the expected return. 3.3.4 Deep Deterministic Policy Gradient (DDPG) While it is possible to derive the gradient of a stochastic policy via the policy gradient theorem (equation 3.3.13), it can be beneficial to use a deterministic policy for calculating the gradient. Silver, Lever, et al. (2014) show that the deterministic policy gradient is the expected gradient of the action-value function. They also prove, that the deterministic policy gradient is a special case of the gradient of many stochastic policies, when variance approaches zero. Let \ud835\udc5d(\ud835\udc60|\ud835\udf07\ud835\udf03) denote the probability of arriving in state s, when following the policy \ud835\udf07\ud835\udf03. Because the policy is now a deterministic function, no trick is needed for differentiation and the policy gradient can be decomposed as follows: \u2207\ud835\udf03\ud835\udc3d(\ud835\udf03) = E\ud835\udc60\u223c\ud835\udc5d(\ud835\udc60|\ud835\udf07\ud835\udf03) [\ufe00 \u2207\ud835\udf03\ud835\udc44\ud835\udf07\ud835\udf03(\ud835\udc60, \ud835\udf07\ud835\udf03(\ud835\udc60)) ]\ufe00 (3.3.14) = E\ud835\udc60\u223c\ud835\udc5d(\ud835\udc60|\ud835\udf07\ud835\udf03) [\ufe00 \u2207\ud835\udf03\ud835\udf07\ud835\udf03(\ud835\udc60)\u2207\ud835\udf07\ud835\udf03(\ud835\udc60)\ud835\udc44\ud835\udf07\ud835\udf03(\ud835\udc60, \ud835\udf07\ud835\udf03(\ud835\udc60)) ]\ufe00 (3.3.15) As the expectation is taken only with respect to the states, it can be estimated more effectively than in the stochastic case, where the expectation depends on both the states and actions (see equation 3.3.13 for comparision). Obviously, the learning algorithm uses the gradient of the action-value function with respect to the action to improve the policy. Each training step modifies the policy in the way, that it\u2019s outputs are pushed in the direction 3. Variants of Reinforcement Learning 27 of the positive gradient of the action-value function. Especially for continuous actions, this strategy is very effective, as it directly pushes the generated actions towards the assumed best action with respect to the action-value estimations. For a stochastic policy the same procedure would require a more exhaustive search in the action space to find the assumed best action. Lillicrap et al. (2015) apply these insights to problems with complex continuous action spaces and successfully combine the deterministic policy gradient with a deep Q-network (DQN) to obtain the deep deterministic policy gradient (DDPG) algorithm, that is shown in listing 3.4. To encourage exploration, a stochastic policy is still used to generate the training samples, which yields an off-policy training algorithm. Like DQN, the DDPG algorithm uses target networks for both the actor and the critic and experience replay. In contrast to DQN, the target networks are updated after each gradient step to slowly replicate the changes made to the trained networks. Listing 3.4: DDPG algorithm. Reproduced from Lillicrap et al. (2015). 1 Randomly initialize critic network \ud835\udc44(\ud835\udc60, \ud835\udc4e|\ud835\udf03\ud835\udc44) and actor \ud835\udf07(\ud835\udc60|\ud835\udf03\ud835\udf07) with weights \ud835\udf03\ud835\udc44 and \ud835\udf03\ud835\udf07 2 Initialize target network \ud835\udc44\u2032 and \ud835\udf07\u2032 with weights \ud835\udf03\ud835\udc44\u2032 \u2190 \ud835\udf03\ud835\udc44, \ud835\udf03\ud835\udf07\u2032 \u2190 \ud835\udf03\ud835\udf07 3 Initialize replay buffer \ud835\udc45 4 for episode = 1, M do 5 Initialize a random process \ud835\udca9 for action exploration 6 Receive initial observation state \ud835\udc601 7 for t = 1, T do 8 Select action \ud835\udc4e\ud835\udc61 = \ud835\udf07(\ud835\udc60\ud835\udc61|\ud835\udf03\ud835\udf07) + \ud835\udca9\ud835\udc61 according to the current policy and exploration noise 9 Execute action \ud835\udc4e\ud835\udc61 and observe reward \ud835\udc5f\ud835\udc61 and observe new state \ud835\udc60\ud835\udc61+1 10 Store transition (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc60\ud835\udc61+1) in \ud835\udc45 11 Sample a random minibatch of \ud835\udc41 transitions (\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56, \ud835\udc5f\ud835\udc56, \ud835\udc60\ud835\udc56+1) from \ud835\udc45 12 Set \ud835\udc66\ud835\udc56 = \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc44\u2032(\ud835\udc60\ud835\udc56+1, \ud835\udf07\u2032(\ud835\udc60\ud835\udc56+1|\ud835\udf03\ud835\udf07\u2032 )|\ud835\udf03\ud835\udc44\u2032 ) 13 Update critic by minimizing the loss: \ud835\udc3f = 1 \ud835\udc41 \u2211\ufe00 \ud835\udc56(\ud835\udc66\ud835\udc56 \u2212 \ud835\udc44(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56|\ud835\udf03\ud835\udc44))2 14 Update the actor policy using the sampled policy gradient: 15 16 \u2207\ud835\udf03\ud835\udf07\ud835\udc3d \u2248 1 \ud835\udc41 \u2211\ufe01 \ud835\udc56 \u2207\ud835\udc4e\ud835\udc44(\ud835\udc60, \ud835\udc4e|\ud835\udf03\ud835\udc44)|\ud835\udc60=\ud835\udc60\ud835\udc56,\ud835\udc4e=\ud835\udf07(\ud835\udc60\ud835\udc56)\u2207\ud835\udf03\ud835\udf07\ud835\udf07(\ud835\udc60|\ud835\udf03\ud835\udf07)|\ud835\udc60=\ud835\udc60\ud835\udc56 17 18 Update the target networks: 19 \ud835\udf03\ud835\udc44\u2032 \u2190 \ud835\udf0f\ud835\udf03\ud835\udc44 + (1 \u2212 \ud835\udf0f)\ud835\udf03\ud835\udc44\u2032 20 \ud835\udf03\ud835\udf07\u2032 \u2190 \ud835\udf0f\ud835\udf03\ud835\udf07 + (1 \u2212 \ud835\udf0f)\ud835\udf03\ud835\udf07\u2032 21 end for 22 end for 3. Variants of Reinforcement Learning 28 3.3.5 Asynchronous Advantage Actor-Critic (A3C) The asynchronous advantage actor-critic algorithm (A3C), introduced by Mnih, Badia, et al. (2016) among other algorithms, is a popular recent implementation of an actor-critic model, that improves state-of-the-art performance on many experiments. The authors show for instance, that their algorithm is able to perform better than the deep Q-network, that is discussed in section 3.2.6, on the task of playing many different Atari games. Levine, Pastor, et al. (2016) and Gu, Holly, et al. (2017) show successful applications of asynchronous updates similar to those of A3C in robotics, where the algorithm is able to generalize to different robotic hardware. Another important benefit of the A3C algorithm is, that it can be efficiently implemented and is therefore faster than many other methods, that achieve comparable performance. The key idea that motivated asynchronous algorithms is that learning can be parallelized using different threads, that independently collect experience. The independent execution of multiple different environments reduces the variance of the trained estimators, because it provides the learning algorithm with many decorrelated training examples at one time. Techniques like experience replay used for training the DQN are thus no longer necessary. By choosing different starting conditions and exploration rates for the threads, it can be ensured that the training examples produced at one time are sufficiently varying. Figure 3.6 depicts the main components of the A3C algorithm. Asynchronous advantage actor-critic and other similar algorithms like asynchronous n-step Q-learning (Mnih, Badia, et al. 2016) use a mix of explicitly computed n-step returns. Unlike algorithms that rely on eligibility traces, which were discussed in section 3.2.3, A3C directly executes a sequence of steps with fixed length n. After that, the one-step return is used to obtain the gradient update for the last pair of state and action, the two-step return for the second last and so on. Each asynchronous thread independently computes updates for the parameters of both networks using the gradient of equation 3.3.13 to determine the update of the actor and the respective n-step update for updating the critic. Although there is a chance of overriding changes made by other threads, the gradient updates can be synchronized without any locks, if the learning rate is sufficiently small. This updating mechanism is known as Hogwild! style updating (Recht et al. 2011). Each thread maintains a local copy of the two global networks to being able to compute the updates independently from all other threads. After each update, the locally copied networks are updated. In comparision to other state-of-theart methods, A3C is very fast, because of the possibility to massively parallelize it with minimal overhead for synchronization. Listing 3.5 pictures the algorithm in pseudocode. 3. Variants of Reinforcement Learning 29 Local actor & critic Global actor & critic network sync Environment 2 Computing thread 2 train simulate Local actor & critic sync Environment 1 Computing thread 1 train simulate Target actor & critic network update Figure 3.6: Asynchronous advantage actor-critic (A3C) still uses target netorks for stability, but no experience replay. Many threads, each with a separate instance of the environment, train local instances of the actor and critic network in parallel, while only two threads are exemplary displayed. The local updates are synchronized with the global networks at regular intervals. Listing 3.5: A3C algorithm for each learner thread. The threads repeatedly synchronize their respective weight updates. Reproduced from Mnih, Badia, et al. (2016). 1 // Assume global shared parameter vectors \ud835\udf03 and \ud835\udf03\ud835\udc63 and global shared counter \ud835\udc47 = 0 2 // Assume thread-specific parameter vectors \ud835\udf03\u2032 and \ud835\udf03\u2032 \ud835\udc63 3 Initialize thread step counter \ud835\udc61 \u2190 1 4 repeat 5 Reset gradients: \ud835\udc51\ud835\udf03 \u2190 0 and \ud835\udc51\ud835\udf03\ud835\udc63 \u2190 0 6 Synchronize thread-specific parameters \ud835\udf03\u2032 = \ud835\udf03 and \ud835\udf03\u2032 \ud835\udc63 = \ud835\udf03\ud835\udc63 7 \ud835\udc61\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 = \ud835\udc61 8 Get state \ud835\udc60\ud835\udc61 9 repeat 10 Perform \ud835\udc4e\ud835\udc61 according to policy \ud835\udf0b(\ud835\udc4e\ud835\udc61|\ud835\udc60\ud835\udc61; \ud835\udf03\u2032) 11 Receive reward \ud835\udc5f\ud835\udc61 and new state \ud835\udc60\ud835\udc61+1 12 \ud835\udc61 \u2190 \ud835\udc61 + 1 13 \ud835\udc47 \u2190 \ud835\udc47 + 1 14 until terminal \ud835\udc60\ud835\udc61 or \ud835\udc61 \u2212 \ud835\udc61\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61 == \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 15 \ud835\udc45 = {\ufe03 0 for terminal \ud835\udc60\ud835\udc61 \ud835\udc49 (\ud835\udc60\ud835\udc61, \ud835\udf03\u2032 \ud835\udc63) for non-terminal \ud835\udc60\ud835\udc61 // Bootstrap from last state 16 for \ud835\udc56 \u2208 {\ud835\udc61 \u2212 1, ..., \ud835\udc61\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61} do 17 \ud835\udc45 \u2190 \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc45 18 Accumulate gradients wrt \ud835\udf03\u2032 : \ud835\udc51\ud835\udf03 \u2190 \ud835\udc51\ud835\udf03 + \u2207\ud835\udf03\u2032 log \ud835\udf0b(\ud835\udc4e\ud835\udc56|\ud835\udc60\ud835\udc56; \ud835\udf03\u2032)(\ud835\udc45 \u2212 \ud835\udc49 (\ud835\udc60\ud835\udc56; \ud835\udf03\u2032 \ud835\udc63)) 19 Accumulate gradients wrt \ud835\udf03\u2032 \ud835\udc63 : \ud835\udc51\ud835\udf03\ud835\udc63 \u2190 \ud835\udc51\ud835\udf03\ud835\udc63 + \ud835\udf15(\ud835\udc45 \u2212 \ud835\udc49 (\ud835\udc60\ud835\udc56; \ud835\udf03\u2032 \ud835\udc63))/\ud835\udf15\ud835\udf03\u2032 \ud835\udc63 20 end for 21 Perform asynchronous update of \ud835\udf03 using \ud835\udc51\ud835\udf03 and of \ud835\udf03\ud835\udc63 using \ud835\udc51\ud835\udf03\ud835\udc63 22 until \ud835\udc47 > \ud835\udc47\ud835\udc5a\ud835\udc4e\ud835\udc65 Chapter 4 Extensions to Reinforcement Learning Recent research often focuses on improving model-free reinforcement learning algorithms to being able to solve a wide variety of different challenging tasks with the same neural network structure and minimal or no changes to the used learning algorithms or hyperparameters (Mnih, Badia, et al. 2016). These methods, some of which were discussed at the end of section 2.3, have in common that they directly train a policy to solve the given task with one learning algorithm. The technique of learning a function approximator that directly predicts the desired output from the given input data without intermediate stages can be named end-to-end learning (Levine, Finn, et al. 2016), mostly from pixels to actions. The execution of the policy during test time is straightforward and minimal or no preprocessing of the input data is required. While the obtained results are impressive, it is still reasonable to assume that incorporating information about the model of the environment or preprocessing the input data can improve the learned behavior. As a practical example, Levine, Pastor, et al. (2016) use a training objective, which is similar to that of reinforcement learning, to predict the probability of successfully grasping an object, when the position of a robotic gripper is known. For testing the learned behavior, a sampling algorithm is used, that samples movements of the robot and evaluates them according to the predicted probability of a successful grasp. This approach incorporates knowledge of the environment into the training and testing process and thereby improves the performance of the robot after training and is able to generalize to different robotic hardware. In the following, three possible ways to extend reinforcement learning algorithms will be discussed. These ideas require at least some knowledge about the environment or process the given information in several stages to produce the desired output and thus cannot be described as end-to-end learning algorithms. Nevertheless, we assume that some of 30 4. Extensions to Reinforcement Learning 31 the gained insights can be transfered to other similar tasks or environments with minor adaptions. 4.1 Pretraining a State Model Using the Physical States Sometimes additional information can be observed during training for tasks that should be carried out while only observing pixel data in the test case. A robotic system for instance might have access to the physical states of the important components like positions of objects or parts of the robot during training. Reinforcement learning only on the physical states is usually much more effective than learning directly from pixels, because the possibly complicated task of extracting the necessary information from the high dimensional pixel data is no longer required. In addition, it is also possible to learn a policy that is able to act on pixels while incorporating knowledge about the physical states to enhance the training performance. Levine, Finn, et al. (2016) use a dual training approach to train a neural network to directly predict robotic motor torques using pixel images as input and thus are able to form an endto-end learning algorithm. The training procedure however does include the physical states of the robot and other objects to force the network to learn useful features. We suppose that introducing a model, that is trained to predict the physical states from a pixel image using a supervised learning algorithm (Figure 4.1), shortens training time and simplifies the reinforcement learning task. We call this model an internal model. The term internal model sometimes refers to models learning the dynamics of the system (Kawato 1999), while we label those inverse- or forward-models. The internal model can be trained in parallel to a second model, that uses reinforcement learning to predict actions based on the physical states. The resulting hybrid of the two parallel trained models cannot be described as an end-to-end model anymore, but it is still able to fulfill the same task like a model trained end-to-end from pixels to actions during test time as it predicts actions from pixels with an intermediate step in between but without any additional information required. 4.2 Pretraining a State Model with a Deep Autoencoder Without the knowledge of the physical states, it is more difficult to pretrain a model, that transforms the input pixels into an intermediate representation to be used as input for a reinforcement learning algorithm. Although no supervised learning target is available, unsupervised learning techniques can be applied to extract information inherent in the provided data. 4. Extensions to Reinforcement Learning 32 pixel image internal model physical states Figure 4.1: The internal model predicts the physical states from a pixel image and thus enables any function estimator trained with reinforcement learning on the physical states to work on pixels. A convenient structure for learning to extract useful information from high dimensional data in an unsupervised manner is an autoencoder (Figure 4.2), which is trained to reproduce its input through an internal representation or latent code (Goodfellow, Bengio, and Courville 2016). In our case, the internal representation serves as input to the reinforcement learning process to form a hybrid training approach similar to the combination of internal model and reinforcement learning in section 4.1. The part of the autoencoder, that generates the latent code from the input is known as the encoder, while the other part, that generates the reconstruction from the hidden code is named the decoder. Both parts can be complicated function estimators, usually neural networks with multiple layers including convolutional layers and transposed convolutional layers. To make the autoencoder learn something useful, it must be discouraged from simply copying the input to the output, which would make it useless. Building the latent code in fact must be tied to learning the important variations of the data. The autoencoder thus might not be able to recover the input exactly, but will focus on its main aspects. Autoencoders can be viewed as a feed forward network from the input to the reconstruction. Therefore, they can be trained by simple gradient descent similar to a supervised training procedure, where the input and the target are equal. The loss function to be optimized, that measures the difference of the input to the reconstruction in any way, is called the reconstruction error. Goodfellow, Bengio, and Courville (2016) summarize several ways to train autoencoders. An autoencoder that is forced to learn useful information inherent in the data by making the dimension of its latent code smaller than the dimension of the input is called an undercomplete autoencoder. However, various regularization strategies enable autoencoders to still learn useful features of the data, when they are not necessarily undercomplete. This can be for example a sparsity contraint (Lee et al. 2007) imposed on the latent code to form a sparse autoencoder. Denoising autoencoders add noise to the input and thus force the autoencoder to remove it and learn to distinguish realistic data from noise (Vincent et al. 2008). Another strategy is to penalize the derivatives of the 4. Extensions to Reinforcement Learning 33 pixel image latent representation reconstructed image Figure 4.2: Any autoencoder maps an input vector to a reconstruction through a latent internal representation. In the special case of using pixel images, the autoencoder is trained to reconstruct a given input image as precisely as possible. latent code with respect to the input to form a contractive autoencoder, which learns locally stable features (Rifai et al. 2011). Recent extensions to the general idea of learning from the task of reconstruction include deep autoencoders, that use deep neural networks to build the encoder and decoder functions. Another recent innovation in this field are autoencoders, that generalize the encoder and decoder functions to stochastic mappings. As popular examples of these probabilistic models, the adversarial autoencoder (Makhzani et al. 2015) and the variational autoencoder (Kingma and Welling 2013) were successfully applied to a range of tasks including denoising, compression or semi-supervised classification, that is able to work on partially labled training data. Let \ud835\udc5d(\ud835\udc65) denote the data distribution, \ud835\udf03 the parameters of the encoder and \ud835\udf11 the parameters of the decoder. The reconstruction error for both the encoder \ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f(\ud835\udc67|\ud835\udc65) = \ud835\udc5e\ud835\udf03(\ud835\udc67|\ud835\udc65) and the decoder \ud835\udc5d\ud835\udc51\ud835\udc52\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f(\ud835\udc65|\ud835\udc67) = \ud835\udc5d\ud835\udf11(\ud835\udc65|\ud835\udc67) being stochastic can be expressed as the negative log-likelihood of reconstructing x, when x is the input: \u2112(\ud835\udc65, \ud835\udf03, \ud835\udf11)\ud835\udc5f\ud835\udc52\ud835\udc50 = E\ud835\udc67\u223c\ud835\udc5e\ud835\udf03(\ud835\udc67|\ud835\udc65) [\ufe00 \u2212 log(\ud835\udc5d\ud835\udf11(\ud835\udc65|\ud835\udc67)) ]\ufe00 (4.2.1) The variational autoencoder (Kingma and Welling 2013) imposes a prior probability distribution on the latent code to regularize it and present the learned features in an appealing form that can be used to solve tasks like classification. The imposed prior usually is a multivariate gaussian disribution. The mean and variance of each variable in the latent code are modeled seperately and forced to match the prior distribution by adding a second term to the loss function beside the reconstruction error. The newly introduced term 4. Extensions to Reinforcement Learning 34 measures the difference between the actual probability distribution of the latent variables and the imposed prior \ud835\udc5d(\ud835\udc67) using the Kullback\u2013Leibler divergence: \u2112(\ud835\udc65, \ud835\udf03, \ud835\udf11) = \u2112(\ud835\udc65, \ud835\udf03, \ud835\udf11)\ud835\udc5f\ud835\udc52\ud835\udc50 + \ud835\udc37\ud835\udc3e\ud835\udc3f[\ud835\udc5e\ud835\udf03(\ud835\udc67|\ud835\udc65)||\ud835\udc5d(\ud835\udc67)] (4.2.2) In addition to a training a deep autoencoder only on pixel data, in the case when physical states are available, they might be incorporated into the training procedure of the autoencoder to help extracting sensible features. The structure of the autoencoder can be extended to not only predict the reconstructed image from the latent variables, but also the physical states (Figure 4.3). The training objective is to minimize the sum of the two losses for the reconstructed image and the predicted physical states respectively. To enhance the regularization effect, the latent variables of the autoencoder can be forced to become a linear function of the physical states, by drawing only simple linear connections between the two layers. When the dimension of the latent code is chosen to be larger than the dimension of the physical states, this learning technique might lead to a better estimation of the true physical states, because reducing the high dimensional pixel image to very few physical states can be error-prone and inexact. pixel image latent representation reconstructed image physical states Figure 4.3: A network structure similar to that of an autoencoder can be trained jointly to predict the reconstructed image and the physical states. The learned latent representation ideally encodes the physical states with less error than the purely supervised model of section 4.1. 4. Extensions to Reinforcement Learning 35 4.3 Training Inverse- or Forward-Models Reinforcement learning algorithms often do not require to directly learn a model of the system dynamics. Nevertheless they must maintain an implicit understanding of those dynamics to being able to act. Especially for end-to-end training approaches it is not easy to tell what kind of knowledge the trained model has aquired about its environment. Most of the time, only the actions taken and thus the behavior can be evaluated. Other recent innovations that are only loosely coupled to reinforcement learning but solve very similar problems directly attempt to learn the dynamics of the environment. There are two main possibilities to learn the system dynamics \ud835\udc5d(\ud835\udc60\ud835\udc61+1|\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61). While looking into the future, forward models (Figure 4.4) can be trained to predict the next state \ud835\udc60\ud835\udc61+1. Often it is also helpful to estimate the action that was executed between two states. Models of this kind are called inverse models (Figure 4.5). Kawato (1999) introduce both types of models and discuss possible applications to motor control problems. Both approaches usually rely on neural networks as function approximators, when they are used in settings similar to the reinforcement learning tasks described so far. Dosovitskiy and Koltun (2016) train a forward model to being able to compare a set of discrete actions by evaluating their respective impact in the context of the 3D game Doom. Agrawal et al. (2016) jointly train a forward and an inverse model in a robotic environment to intuitively learn about the dynamics of different physical objects by displacing or rotating them. state s1 neural network state s2 action Figure 4.4: Abstract architecture of a simple forward model. 4. Extensions to Reinforcement Learning 36 state s1 neural network action state s2 Figure 4.5: Abstract architecture of a simple inverse model. Chapter 5 Methods An important property of many recent inventions in the field of reinforcement learning is, that the underlying ideas can be transferred to many different environments with minimal changes. In contrast to this, to evaluate the different reinforcement learning methods discussed so far and extensions to them, we focus on a single simulated robotic task for two reasons. First, we assume that the relative scores obtained by training a single robotic task with different algorithms are still meaningful and can be used to compare the algorithms, while training on a larger set of tasks would require a more sophisticated design of the function estimators beforehand and thus would slow down the experiments. Second, solving a single task allows to incorporate information about the model of the environment into the training algorithm, as discussed in section 4. An important point to investigate is the influence of this additional knowledge on the training performance. Recently published ideas, that analyse the benefits of directly modeling the underlying system dynamics, for instance the algorithm proposed by Dosovitskiy and Koltun (2016), are often tested on a very small set of similar environments too. In the following, we will first describe the single simulated robotic task used for all experiments and its variations. Then follows a discussion of the software components used for conducting the experiments. Finally we summarize the different algorithms that were used. Two new algorithms, that we call distributed and asynchronous DDPG will be presented. With these training approaches, we aim to combine asynchronous methods and DDPG to form asynchronous learning methods, which are able to effectively learn to predict continuous actions. 37 5. Methods 38 5.1 Training Environments We trained all experiments on a single robotic task, that is very similar to the OpenAI Gym reacher task1. OpenAI Gym2 is an open source platform for comparing reinforcement learning algorithms. It is used to obtain benchmark results for algorithms trained on many different environments like video games, robotic tasks or board games through a unified programming interface. The reacher task consists of a simulated arm with two degrees of freedom, where the gripper of the robotic arm, which is the endpoint of the last arm segment, should be guided to reach a target. While it would be possible to transfer the reacher problem to a 3D space with more degrees of freedom, we restrict to the 2D space for all experiments and only use two arm segments. The original reacher task provides the learning algorithm with the physical states, which resemble the angles of the arm segments and the position of the target, during training and test time. In contrast to this, we try to learn a policy, that is able to predict actions from pixel data. We therefore provide most experimentally learned policies only with screenshots of the simulated reacher task during test time. This task is much harder, as the state space is very much larger than before and the tested algorithm needs to find a way to extract useful information from pixels. Ideally, we would like to be able to train this kind of policy only on pixel data and thus leave out the physical states completely. Nevertheless, many experiments still include the information about the physical states as guide for the algorithm to help it extracting useful information from the pixels. 5.1.1 Simulation with Matplotlib As the OpenAI Gym reacher environment relies on the commercial mujoco physics engine, we decided to replicate the simulation in essence using matplotlib3. Figure 5.1 shows five images, that were randomly generated using the matplotlib simulation. To make the vision task harder, which means making it harder to extract useful information from pixels, noise can be added in the background. The noise is however static and thus it looks always the same on every pixel image. The rendered images have 64 pixels in both dimensions. The environment outputs the physical states in parallel, which consist of the two angles of the arm segments and the coordinates of the goal. Figure 5.2 shows a screenshot of the environment with the physical states highlighted. Executed actions are tuples with two components specifying the desired 1https://gym.openai.com/envs/Reacher-v1, last downloaded 2017-09-14 2https://gym.openai.com/, last downloaded 2017-09-14 3https://matplotlib.org/, last downloaded 2017-09-14 5. Methods 39 Figure 5.1: Five visualizations of the matplotlib reacher at different random states. The first two images are rendered without background noise, while the other images include static noise in the background. rotation of both arm segments. Each arm can rotate maximally by 2 degrees in each direction. The components of the actions are clipped to lie in the interval between -1 and 1: \ud835\udc4e \u2208 [\u22121, 1]2. Action (-1,-1) for instance means to rotate both segments left by two degrees and action (1,-1) means rotating only the first segment right by two degrees and the second segment left by two degrees. The reward returned by the environment for each step is designed to guide the gripper to the target as fast as possible. We therefore introduced a distance related part of the reward function, that depends on the distance between the gripper and the target: \ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61 = \ud835\udc52\u2212|\ud835\udc65\ud835\udc54\ud835\udc5f\ud835\udc56\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5f\u2212\ud835\udc65\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61|. Early experiments showed that this kind of reward leads to a strategy, where the gripper circles around the target without ever reaching it, while collecting a large amount of reward, because the distance is very small. To overcome this problem, we multiplied the distance based reward with a control term, that only depends on the currently executed action: \ud835\udc5f\ud835\udc50\ud835\udc61\ud835\udc5f\ud835\udc59 = |\ud835\udc65\ud835\udc54\ud835\udc5f\ud835\udc56\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5f\u2212\ud835\udc5c\ud835\udc59\ud835\udc51 \u2212 \ud835\udc65\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61| \u2212 |\ud835\udc65\ud835\udc54\ud835\udc5f\ud835\udc56\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5f \u2212 \ud835\udc65\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61|, which forces the action to be taken in the direction of the target. As the total reward \ud835\udc5f = \ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\u00b7\ud835\udc5f\ud835\udc50\ud835\udc61\ud835\udc5f\ud835\udc59 is the product of both terms, actions that are taken in the wrong direction, when being close to the target, will be severely punished. The total reward is normalized to lie in the interval [-1,1]. When \ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61 falls below 0.1 the episode is considered successfully terminated. This distance is sufficiently small for reaching the target, as both axes of the simulated environment range from -1 to 1. The episode fails, when 1000 steps are executed without reaching the target. 5.1.2 More Realistic Simulation with Dart The same task of reaching a target with a robotic arm, that has two degrees of freedom, has been replicated as an extension to OpenAI Gym using the open source physics engine Dart4. We also experimented with this visualization to obtain more realistic pixel images. The simulation with Dart has however not been designed to allow training from pixel data. 4https://github.com/DartEnv/dart-env, last downloaded 2017-09-14 5. Methods 40 Figure 5.2: Visualization of the reacher environment with highlighted and labeled physical states. The simulation relies on matplotlib. The arm segments are simple lines with dots in between. The target has the shape of a star. Noise in the background can be added in form of white dots. Figure 5.3: Screenshot of the original visualization of the Dart environment, which simulates the reacher task in 3D, where the arm is only allowed to move on a two-dimensional plane. The target point is already enlarged here. Hence, we had to apply preprocessing to the pixel images, which means that we enlarged the target point to make it better recognizable by the training algorithm. Then we cropped the important region of the image, where the arm and the target are shown and resized the cropped images to 64x64 pixels. The physical states were adjusted to be similar to those of the matplotlib simulation for better comparision. Instead of the 11 physical states originally returned by the Dart environment, we used these to calculate four variables representing the angles and the target position, that are finally passed to the learning algorithm. 5. Methods 41 Figure 5.4: Five visualizations of the Dart reacher at different random states. The images have been cropped to the important region and resized to 64x64 pixels. 5.2 Implementation Details 5.2.1 Used Software and Hardware All implementations purely consist of python code, while numpy is used for general data processing. We used tensorflow5 for neural network training and keras6 to build the architecture of the respective networks. Keras can be seen as an easy interface to tensorflow, while it also supports other backends. For drawing plots and statistics we relied on matplotlib7 and tensorboard, which is a part of the tensorflow framework and provides easy to use visualization tools for tensorflow training and neural network training architectures (graphs). With tensorflow it is relatively easy to set up an asynchronous training algorithm, as the framework itself heavily supports multithreading and is even able to run on a GPU without any changes to the code of the algorithm. When executed without any restrictions, Tensorflow occupies all computing resources it can find, which means it creates a computing thread for every CPU and also reserves the whole memory of all available GPUs. For CPU training, tensorflow distinguishes between intra-op-parallelism and inter-op-parallelism. For a single computing operation, the threads of the intra-opparallelism pool are used to execute this operation in parallel, while the threads in the inter-op-parallelism pool execute multiple operations at one time. For asynchronous training, especially the inter-op-parallelism is important, as multiple threads independently compute gradients, which corresponds to executing multiple gradient operations in parallel. The intra_op_parallelism_threads variable and the inter_op_parallelism_threads variable control the size of the respective thread pools. Listing 5.1 shows a sample configuration of both variables. 5https://www.tensorflow.org/, last downloaded 2017-09-14 6https://keras.io/, last downloaded 2017-09-14 7https://matplotlib.org/, last downloaded 2017-09-14 5. Methods 42 Listing 5.1: An example of how to specify the size of the tensorflow thread pools at the beginning of the python program. Both sizes are exemplary set to 4. The newly created tensorflow session should be set as default session for the backend of the keras library. 1 import tensorflow as tf 2 from keras import backend as K 3 4 def main(_): 5 intra_threads = 4 6 inter_threads = 4 7 sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=intra_threads, inter_op_parallelism_threads=inter_threads)) 8 with sess.as_default(): 9 K.set_session(sess) 10 # do calculations ... To restrict tensorflow to use only one GPU of a cluster, the CUDA_VISIBLE_DEVICES switch is necessary to specify the indices of the visible GPUs. For most cases it makes sense to train on a single GPU, while multiple indices could be set by using a colon as delimiter. The switch can be set for each run separately, e.g. CUDA_VISIBLE_DEVICES=0 python main.py for a file named main.py. Listing 5.2 depicts how to rewrite the main function to support passing the GPU index as command line argument for easier use. The equivalent to directly using the switch would then be the shorter form: main.py - -gpu=0. Listing 5.2: Initializing the CUDA_VISIBLE_DEVICES switch in code from a command line parameter is a practical solution to prevent tensorflow from unnecessarily blocking the memory of all GPUs. If an invalid index (e.g. -1) is specified, tensorflow runs only on the CPUs. 1 import tensorflow as tf 2 3 flags = tf.app.flags 4 flags.DEFINE_integer('gpu', 0, 'index of GPU to use') 5 FLAGS = flags.FLAGS 6 7 def main(_): 8 os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(FLAGS.gpu) It is also possible to assign a specific device to a subset of tensorflow operations. The block, where the operations are defined has to start with a wrapping call to tf.device(device). Listing 5.3 shows this method for a simple example. We were however unable to achieve any performance improvements when directly placing operations on devices and thus decided to let tensorflow automatically assign the operations to the available hardware for all experiments. 5. Methods 43 Listing 5.3: Tensorflow operations can be directly assigned to hardware units, for example the CPU with index 1. 1 import tensorflow as tf 2 3 with tf.device('/cpu:1'): 4 # all operations defined here will run on CPU 1 For training on CPUs we used a computing server with 4x8 Intel Xeon E5-4650 CPUs and 256 GB RAM. The GPU based training was performed on a second machine with a shared-memory system, 16 CPU cores, 8 Tesla K20m GPUs and 126 GB RAM. The reinforcement learning algorithms were run only on the CPUs without exception. As they require frequent interactions with the environment, which requires communication with the CPU, there were no speed gains observed when using a GPU. Especially asynchronous implementations can be efficiently run on a parallel CPU system, because execution can be carried out by multiple parallel threads, that only require minimal communication. All different forms of pretraining, which do not involve reinforcement learning, were entirely performed on the GPUs and heavily benefited from the speedup. Especially implementations that use large amounts of training data at once can be efficiently parallelized on a GPU. 5.2.2 Asynchronously Executing Multiple Environments For asynchronous training, we identified the execution of the environment being a major bottleneck, when using a single threaded instance of the environment for asynchronous training algorithms. Tensorflow takes care of parallelizing the network training operations, but does not parallelize the executions of the environment, which run synchronously due to pythons global interpreter lock8. We thus decided to use the multiprocessing module to create separate processes for all instances of the environment. Listing 5.4 shows the snippet where a new simulation process is initialized. Figure 5.5 shows how the AsyncEnvironment class is used, that provides an interface for creating and managing asynchronous environments. For easy usage, this class exposes the same function like a local environment, but sends the commands across process boundaries instead of directly executing them. It is important to notice, that the creation mode of the new process must be set to spawn. The standard setting is fork on Unix/Linux platforms, which copies the entire memory of the computing process to all simulating processes and leads to explosion of memory. This happens especially, when the parameters of the neural networks, which are stored for the computing process, already consume large amounts of memory. 8https://wiki.python.org/moin/GlobalInterpreterLock, last downloaded 2017-09-14 5. Methods 44 tensorflow parallelization env. parallelization Computing thread 1 AsyncEnvironment 1 Computing thread 2 AsyncEnvironment 2 Environment 1 reset / step Environment 2 reset / step pixels / ph. states pixels / ph. states Figure 5.5: The execution of different instances of the environment cannot be automatically parallelized by tensorflow. We therefore create a separate process for each instance of the environment and attach it to the computing thread. Another problem was that we were unable to simulate the Dart environment discussed in section 5.1.2 on a computing server, because the simulation is carried out with OpenGL and needs an active display to function properly. To separate the simulation from the training algorithm, we decided to set up a TCP server for simulation. Especially for running asynchronous algorithms with multiple independent instances of the environment, it is crucial for the server to be able to effectively manage multiple client connections in parallel. Each client requests a unique id, which is bound to a single instance of the environment on the server side. The client then sends the actions to be executed and a reset command at the start of each episode to the server, while receiving the current pixel image and the physical states in return. The architecture of the simulation server is thus very similar to the asynchronously simulated environments depicted in figure 5.5. The only difference is, that the communication with the environments is now TCP-based and carried out over the network. 5. Methods 45 Listing 5.4: The class AsynchronousEnvironment encapsulates the creation of separate processes with multiprocessing. The _ _init_ _ method starts a new process without copying the memory of the current process to it. The processes use pipes to communicate. 1 import multiprocessing 2 3 class AsynchronousEnvironment: 4 def __init__(self, env_name): 5 ctx = multiprocessing.get_context('spawn') 6 sim_pipe, self.pipe = ctx.Pipe() 7 self.proc = ctx.Process( 8 name=\"data_generator\", 9 target=func_proc, 10 args=(sim_pipe,env_name)) 11 self.proc.start() 5.3 Training Algorithms As the robotic task we were aiming to solve uses continuous actions, we mainly focused on the DDPG algorithm (Lillicrap et al. 2015), that was introduced in section 3.3.4, and deterministic policies. We use the DDPG implementation from keras-rl9 as a basline for our own experiments, but also provide an own implementation of two extended DDPG algorithms, that combine the ideas of asynchronous training algorithms like A3C (Mnih, Badia, et al. 2016; see section 3.3.5) with the deterministic policy gradient. As this combination is a novel approach, one aim of the conducted experiments was to evaluate the performance of the extended DDPG algorithms. We first investigated a variant of DDPG with one-step updates, that directly bootstrap from the next state (see section 3.2.3). This algorithm does not use Hogwild! style updates (Recht et al. 2011) to synchronize the updates of the different threads, but locks the weights of the networks during training for other threads. The algorithm executes five steps before performing a gradient update and stores the experience collected so far in a very small local memory. We therefore suggest to view this method as implementing a distributed experience replay memory rather than a fully asynchronous algorithm and call it distributed DDPG. This idea can be seen as an intermediate step between using experience replay and fully asynchronous updates. We still used target networks and updated them in the same way like proposed for plain DDPG (Lillicrap et al. 2015). Subsequent experiments also included a variant of DDPG with lock-free Hogwild! style updates, that we call asynchronous DDPG. We also switched to using the same mix of explicitly computed n-step returns like A3C (see section 3.3.5). 9https://github.com/matthiasplappert/keras-rl, last downloaded 2017-09-14 ",
    "Methods": "Methods 46 Listing 5.5: Distributed DDPG algorithm for each actor thread with globally shared counter T and globally shared parameter vectors \ud835\udf03\ud835\udf07, \ud835\udf03\ud835\udc44, \ud835\udf03\u2212 \ud835\udf07 and \ud835\udf03\u2212 \ud835\udc44. 1 while \ud835\udc47 < \ud835\udc47\ud835\udc5a\ud835\udc4e\ud835\udc65 do 2 t = 0 3 Get state \ud835\udc60\ud835\udc61 4 repeat 5 Execute action \ud835\udc4e\ud835\udc61 according to policy \ud835\udf07(\ud835\udc60\ud835\udc61; \ud835\udf03\ud835\udf07) + \ud835\udf16\ud835\udca9 6 Receive reward \ud835\udc5f\ud835\udc61+1 and observe new state \ud835\udc60\ud835\udc61+1 7 Compute \ud835\udc45\ud835\udc61 = {\ufe03 \ud835\udc5f\ud835\udc61+1 + \ud835\udefe\ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udf07(\ud835\udc60\ud835\udc61+1; \ud835\udf03\u2212 \ud835\udf07 ); \ud835\udf03\u2212 \ud835\udc44) if not terminal \ud835\udc5f\ud835\udc61+1 otherwise. 8 Store (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc45\ud835\udc61) in buffer 9 if \ud835\udc61 % 5 == 0 or terminal then 10 Update critic using gradient: 5 \u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf03\ud835\udc44(\ud835\udc45\ud835\udc56 \u2212 \ud835\udc44(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56; \ud835\udf03\ud835\udc44))2 11 Update actor using gradient: 5 \u2211\ufe01 \ud835\udc56=1 \u2207\ud835\udf07(\ud835\udc60\ud835\udc56;\ud835\udf03\ud835\udf07)\ud835\udc44(\ud835\udc60\ud835\udc56, \ud835\udf07(\ud835\udc60\ud835\udc56; \ud835\udf03\ud835\udf07); \ud835\udf03\ud835\udc44)\u2207\ud835\udf03\ud835\udf07\ud835\udf07(\ud835\udc60\ud835\udc56; \ud835\udf03\ud835\udf07) 12 Update target critic: \ud835\udf03\u2212 \ud835\udc44 \u2190 \ud835\udf0f\ud835\udf03\ud835\udc44 + (1 \u2212 \ud835\udf0f)\ud835\udf03\u2212 \ud835\udc44 13 Update target actor: \ud835\udf03\u2212 \ud835\udf07 \u2190 \ud835\udf0f\ud835\udf03\ud835\udf07 + (1 \u2212 \ud835\udf0f)\ud835\udf03\u2212 \ud835\udf07 14 Empty buffer 15 end if 16 \ud835\udc61 = \ud835\udc61 + 1 17 \ud835\udc47 = \ud835\udc47 + 1 18 until \ud835\udc61 > 1000 or terminal Listing 5.6: Asynchronous DDPG algorithm for each actor thread with globally shared parameter vectors \ud835\udf03\ud835\udf07, \ud835\udf03\ud835\udc44, \ud835\udf03\u2212 \ud835\udc44, \ud835\udf03\u2212 \ud835\udf07 and counter T. \ud835\udf03\u2032 \ud835\udf07 and \ud835\udf03\u2032 \ud835\udc44 are thread-specific copies. 1 while \ud835\udc47 < \ud835\udc47\ud835\udc5a\ud835\udc4e\ud835\udc65 do 2 t = 0 3 Get state \ud835\udc60\ud835\udc61 4 repeat 5 Execute action \ud835\udc4e\ud835\udc61 according to policy \ud835\udf07(\ud835\udc60\ud835\udc61; \ud835\udf03\u2032 \ud835\udf07) + \ud835\udf16\ud835\udca9 6 Receive reward \ud835\udc5f\ud835\udc61+1 and observe new state \ud835\udc60\ud835\udc61+1 7 Store (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc5f\ud835\udc61+1) in buffer 8 \ud835\udc61 = \ud835\udc61 + 1 9 \ud835\udc47 = \ud835\udc47 + 1 10 if \ud835\udc61 % 5 == 0 or terminal then 11 \ud835\udc45 = {\ufe03 0 for terminal \ud835\udc60\ud835\udc61 \ud835\udc44(\ud835\udc60\ud835\udc61+1, \ud835\udf07(\ud835\udc60\ud835\udc61+1; \ud835\udf03\u2212 \ud835\udf07 ); \ud835\udf03\u2212 \ud835\udc44) for non-terminal \ud835\udc60\ud835\udc61 12 for \ud835\udc56 \u2208 {\ud835\udc61 \u2212 1, ..., \ud835\udc61 \u2212 5} do 13 \ud835\udc45 \u2190 \ud835\udc5f\ud835\udc56 + \ud835\udefe\ud835\udc45 14 Accumulate gradients wrt \ud835\udf03\u2032 \ud835\udc44 and \ud835\udf03\u2032 \ud835\udf07 : 15 \ud835\udc51\ud835\udf03\ud835\udc44 \u2190 \u2207\ud835\udf03\u2032 \ud835\udc44(\ud835\udc45 \u2212 \ud835\udc44(\ud835\udc60\ud835\udc56, \ud835\udc4e\ud835\udc56; \ud835\udf03\u2032 \ud835\udc44))2 16 \ud835\udc51\ud835\udf03\ud835\udf07 \u2190 \u2207\ud835\udf07(\ud835\udc60\ud835\udc56;\ud835\udf03\u2032 \ud835\udf07)\ud835\udc44(\ud835\udc60\ud835\udc56, \ud835\udf07(\ud835\udc60\ud835\udc56; \ud835\udf03\u2032 \ud835\udf07); \ud835\udf03\u2032 \ud835\udc44)\u2207\ud835\udf03\u2032 \ud835\udf07\ud835\udf07(\ud835\udc60\ud835\udc56; \ud835\udf03\u2032 \ud835\udf07) 17 end for 18 Perform asynchronous update of \ud835\udf03\ud835\udc44 using \ud835\udc51\ud835\udf03\ud835\udc44 and of \ud835\udf03\ud835\udf07 using \ud835\udc51\ud835\udf03\ud835\udf07 19 Update target critic: \ud835\udf03\u2212 \ud835\udc44 \u2190 \ud835\udf0f\ud835\udf03\ud835\udc44 + (1 \u2212 \ud835\udf0f)\ud835\udf03\u2212 \ud835\udc44 20 Update target actor: \ud835\udf03\u2212 \ud835\udf07 \u2190 \ud835\udf0f\ud835\udf03\ud835\udf07 + (1 \u2212 \ud835\udf0f)\ud835\udf03\u2212 \ud835\udf07 21 Reset gradients: \ud835\udc51\ud835\udf03\ud835\udc44 \u2190 0 and \ud835\udc51\ud835\udf03\ud835\udf07 \u2190 0 22 Synchronize thread-specific parameters \ud835\udf03\u2032 \ud835\udc44 \u2190 \ud835\udf03\ud835\udc44 and \ud835\udf03\u2032 \ud835\udf07 \u2190 \ud835\udf03\ud835\udf07 23 Empty buffer 24 end if 25 until \ud835\udc61 > 1000 or terminal Chapter 6 Experimental Results The experimental analysis aims to evaluate two different ideas. First, we compare the distributed and asynchronous DDPG algorithms to a standard DDPG baseline. Second, we investigate the effect of various ways to incorporate knowlege of the environment into the training process. The final goal is to build a training algorithm, that works only on pixel data during training and test time, although this algorithm does not necessarily have to be an end-to-end reinforcement learning algorithm. In the last section of the experiental analysis we will focus on ideas to apply preprocessing to pixel images without any knowledge of the physical states. The sections before will include the physical states either only during the training process or for both training and testing. Due to computational limitations, it was not possible to perform an extensive hyperparameter search for any of the provided experiments. We therefore reused many hyperparameters and some elements of the network structure from Lillicrap et al. (2015) and Mnih, Badia, et al. (2016). For comparing the different variants of the DDPG algorithm, the same hyperparameters and network structure were used to obtain a reliable relative performance. We used the following hyperparameters for all experiments: parameter value update rate for target networks \ud835\udf0f 0.001 learning rate critic 0.0001 learning rate actor 0.0001 discount factor \ud835\udefe 0.97 weight penalty l2 (for critic weights to output neuron) 0.02 maximum number of steps per episode 1000 Table 6.1: Overview of the hyperparameters used for all experiments. 47 6. Experimental Results 48 6.1 Using the Physical States for Training and Testing At first, we compared our extended DDPG variants and the DDPG baseline, while learning directly from the physical states of the matplotlib simulation. This obviously requires the physical states also during test time. Figure 6.1 depicts the network structure we used for the three experiments. We trained each algorithm for 1.600.000 steps and repeated the experiment 15 times. The asynchronous and distributed version both used 16 parallel threads. After each training phase, the trained model was tested for 100 episodes and the percentage of the successfully solved episodes was monitored. An episode is considered successful, when it takes less than 1000 steps for the gripper to reach the target. We state the mean and standard derivation for all independent tests and also the highest and the lowest of the 15 scores. In addition to the experiments using DDPG and its adapted variants, we also tested a simple inverse model, which predicts the action that is needed to guide the gripper of the robotic arm to a desired target position. This model was trained by simply executing random actions and training the model to output the executed actions while it observes the angles of the robotic arm at the state before the action was executed and the position of the gripper after the execution. The inverse model, that is shown in figure 6.2, was trained on 250.000 batches of size 1.000 drawn from an experience replay memory consisting of 1.000 episodes with random starting conditions and length 1.000 each. During evaluation we trained the inverse model three times from scratch and then tested each of these models 5 times on 100 episodes to obtain an evaluation similar to the DDPG experiments. The inverse model then does not receive an imagined next position of the gripper, but the true target position, that is most of the times out of reach. Experiment Score Mean SD Max Min inverse model 98.20% 1.28% 100.00% 97.20% DDPG baseline 49.40% 45.49% 100.00% 0.00% distributed DDPG 68.60% 12.53% 86.00% 46.00% asynchronous DDPG 74.67% 22.30% 96.00% 9.00% Table 6.2: Summary of the experiments conducted using only the physical states. The inverse model performs best, while the asynchronous DDPG algorithm outperforms the baseline and the distributed version. The results as depicted in table 6.2 show that asynchronous DDPG outperforms both other variants comparing the mean scores and is able to replicate the best results of the DDPG baseline. Although the asynchronous DDPG algorithm uses Hogwild! style updates, that only approximate the true gradient and induce a chance of one thread overriding the updates of another thread, this method seems to improve generalization and eventually yields better test results. Both modifications of DDPG reduce the training variance and do not depend 6. Experimental Results 49 Actor Critic Physical States (4) Fully (200, relu) Fully (200, relu) Actions (2, tanh) Physical States (4) Fully (200, relu) Fully (200, linear) Add Fully (200, relu) Actions (2) Fully (200, linear) Fully (200, relu) Q-value (1, linear) Figure 6.1: Network structure used for all variants of DDPG to learn on the physical states. A weight penalty is added to the output neuron of the critic to prevent too fast rising Q-values. on the starting conditions as heavily as the baseline. The distributed DDPG algorithm has very low variance, but is not able to replicate the best results of the baseline. Figure 6.3 compares all variants of DDPG and shows the relative amount of successfully completed episodes during training. The modified DDPG algorithms both outperform the baseline and usually converge fast to better scores. The distributed DDPG algorithm converges best but lacks generalization as the test scores above showed. We also demonstrated that asynchronous training and DDPG can be combined in general to obtain a working algorithm. The training of our asynchronous DDPG implementation is more than five times faster than the DDPG baseline, because of the parallelization. 6. Experimental Results 50 Current angles (2) Concatenate Fully (25, relu) Target gripper pos (2) Fully (50, relu) Fully (25, relu) Actions (2, linear) Figure 6.2: Inverse model, which is trained to predict the action that moves the gripper to a target position, when the current angles of the arm segments are given. The best results however were obtained by using the inverse model, which is very stable and achieves 100% success rate most of the time. The inverse model was solely trained on small movements of the robotic arm. Each arm segment is allowed to maximally move by 2 degrees in any direction, while the true target that is used as desired position for the gripper is often far away and requires many succeeding actions to be reached. The inverse model is still able to predict very good actions at every time step, although most of the data seen during testing has certainly never been observed during training. We like to see this result as evidence, that the inverse model is able to generalize very well and learns a good understanding of the system dynamics. Because of its very good performance we also tested the inverse model on the physical states of the Dart environment (see section 5.1.2), where one inverse model was trained and tested three times for 500 episodes. The results are comparable to those before. We achieved an average score of 99.00% with a standard derivation of 0.28%. These findings demonstrate the ability of the inverse model to learn dynamics under different physical conditions. 6. Experimental Results 51 0 20 40 60 80 100 120 140 160 num episode (*10^4) 0.0 0.2 0.4 0.6 0.8 1.0 successful episodes ratio asynchronous DDPG distributed DDPG DDPG baseline Figure 6.3: Comparing asynchronous DDPG and distributed DDPG to the DDPG baseline. The plots state the relative amount of successful episodes over the last 25 episodes at the respective time. Every algorithm was trained 15 times for 1.600.000 steps and the scores were averaged. 6.2 Using the Physical States for Training and Pixel Images for Testing We tested different options to include the physical states in the training process. As pixel images should be used later, it is required to base the reinforcement learning process on a representation that can be directly derived from pixels. We use the physical states during training to regularize a pretraining process, that learns to extract useful information from the pixels. In a second step, we use reinforcement learning to predict actions based on the outputs of the pretrained model. In the following, we will describe three different ways to extract information from pixels: 1. Internal model: The easiest way to make use of the physical states is pretraining a model that directly predicts the physical states from pixels, which we call an internal model. The concept has been described in section 4.1. The detailed internal model is shown in figure 6.4. 2. Autoencoder, that additionally predicts physical states: An autoencoder is normally used to simply reconstruct images. We extended an autoencoder structure to also predict the physical states like described in section 4.2. The detailed network structure 6. Experimental Results 52 Pixel Image (64x64) Convolution (8 filters, 3x3 kernel, relu) Convolution (32 filters, 5x5 kernel, relu) Fully (200, relu) Physical states (4, linear) Figure 6.4: Detailed architecture of the internal model, that consists of two convolutional layers and one fully connected hidden layer. of the extended autoencoder is depicted in figure 6.5. The physical states as additional training target are supposed to help the autoencoder finding more useful features and better encoding the pixel data. 3. Forward model with physical states as output: We trained a simple forward model (see section 4.3 for comparision), that predicts the physical states of the following system state given the pixel image of the current state and the executed action in between. The last layer, that only depends on the pixel image of the current state, is used as state representation for the reinforcement learning algorithm. By forcing the model to learn about the dynamics of the system, this learned representation is expected to extract useful features for reinforcement learning. Figure 6.6 states the structure of the forward model. The evaluation results of the internal model, the autoencoder with physical states and the forward model with physical states are given in table 6.3. We tested every method with a static background included in all images and also without a background. Every model was trained two times on 100.000 batches of size 250, that were randomly sampled from a memory consisting of 1.000 episodes with 1.000 steps each. All actions executed during the episodes were randomly sampled. We used the DDPG baseline for evaluation and ran reinforcement learning three times for each pretrained model. This means, we conducted six evaluations for every combination of model architecture and background, as each model was pretrained twice. The actor-critic architecture used for reinforcement learning is the same like depicted in figure 6.1, while only the size of the state input has been adapted for 6. Experimental Results 53 Pixel Image (64x64) Convolution (8 filters, 3x3 kernel, relu) Convolution (32 filters, 5x5 kernel, relu) Fully (200, relu) Latent Variables (15, sigmoid) Fully (200, relu) Physical States (4, linear) Fully (58 * 58 * 32, relu) Transposed Convolution (8 filters, 5x5 kernel, relu) Transposed Convolution (1 filter, 3x3 kernel, sigmoid) = Reconstruction Figure 6.5: Detailed architecture of the extended autoencoder, that also includes the physical states. We suppose the latent code of the autoencoder to become more useful, when the training process forces the physical states to be a linear function of it. The latent code is used as state input to the reinforcement learning algorithm. the autoencoder and the forward model. Every final actor was tested for 100 episodes and the relative amount of successfully completed episodes was recorded. As the variance for all experiments is high and due to long training time only few independent experiments could be performed for one pretrained model, we only state the maximum scores for comparision in table 6.3. The raw scores are listed in appendix A. 6. Experimental Results 54 Current Pixel Image (64x64) Convolution (8 filters, 3x3 kernel, relu) Convolution (32 filters, 5x5 kernel, relu) Fully (200, relu) Fully (10, sigmoid) Concatenate Next Physical States (4, linear) Actions (2) Figure 6.6: Detailed architecture of the simple forward model. Learning the dynamics of the underlying physical system is expected to help finding useful features. The last layer, that only depends on the current pixels (sigmoid layer with 10 units) is fed to the reinforcement learning algorithm and can be predicted without knowledge of the physical states or actions. The best results are obtained by using the autoencoder that predicts both images and physical states, when no background is included. The latent variables of the autoencoder thus provide a representation of the system state that is less error-prone than the physical states. The same argumentation applies to the performance of the forward model, which also outperforms the internal model in the case, when no background is included. The autoencoder gets heavily distracted by additive noise in the background of the images and the maximum score decreases. We therefore trained another version of this kind of extended autoencoder using noisy images, but performing mean removal before further processing them. This approach effectively removed most of the noise and again increased the score, which supports the assumption, that generating images with background noise is comparatively hard. 6. Experimental Results 55 Experiment background enabled Max. Score Internal model yes 73% Internal model no 63% Autoencoder w. physical states yes 62% Autoencoder w. physical states no 90% Autoencoder w. physical states yes (mean removal) 89% Forward model w. physical states yes 60% Forward model w. physical states no 82% Table 6.3: Summary of the experiments conducted using the physical states during training and only pixel data for testing. The additive noise makes image generation harder for the autoencoder, but has a weaker impact, when images are only used as input and convolutional layers can be used to remove the noise. For the inverse model, the scores for images with background are even a little bit higher. 6.3 Using Pixel Images for Training and Testing We trained an autoencoder structure, that is shown in figure 6.7 with the mean-squarederror function and mean removal to reconstruct pixel images. The trained autoencoder was able to reconstruct the input images almost perfectly for the matplotlib environment and even for the more sophisticated Dart simulation, as depicted in figure 6.8, with only minor modifications of the architecture to being able to process colored images. Reinforcement learning using the latent code of this autoencoder however produced poor results. The best of six independently trained actor-critic architectures, that were trained using the latent code of converged autoencoders as input and tested for 100 episodes each, was only able to solve 25% of all episodes. We trained the autoencoder two times on the matplotlib environment and ran reinforcement learning three times for each pretrained model to obtain an evaluation similar to section 6.2. A fully random policy might also be able to achieve that score by randomly hitting the target sometimes. We also trained a variational autoencoder by replacing the 30 latent variables with 2x30 neurons that model the variance and mean of a multivariate gaussian distribution with 30 variables. The variational autoencoder did not converge. Furthermore, we tested several variants of inverse- and forward models only on pixels, but these did not help to improve the previously obtained score, or also completely failed to converge. We were however able to use our implementation of the asynchronous DDPG algorithm to obtain a converging end-to-end learning algorithm. The architecture of the actor-critic network is very similar to figure 6.1. Two convolutional layers were added to process 6. Experimental Results 56 Pixel Image (64x64) Convolution (8 filters, 3x3 kernel, relu) Convolution (32 filters, 5x5 kernel, relu) Fully (200, relu) Fully (30, sigmoid) Fully (200, relu) Fully (58 * 58 * 32, relu) Transposed Convolution (8 filters, 5x5 kernel, relu) Transposed Convolution (1 filter, 3x3 kernel, sigmoid) = Reconstruction Figure 6.7: Detailed architecture of the autoencoder, that simply reconstructs pixel images. While the model converges very well, it does not provide a useful state representation in the latent code and reinforcement learning fails to find a good policy. the visual information both for the actor and the critic. These layers replace the input of the physical states. Because of the long training time, when the gradient must be backpropagated through many additional weights, that are added with the convolutional layers, we only trained the end-to-end model once and tested it for 500 episodes. The model was able to solve 87% of all episodes. The distributed DDPG algorithm and the DDPG baseline repeatedly failed the end-to-end learning task while using the same hyperparameters. ",
    "Experimental Results": "Experimental ",
    "Results": "Results 57 Figure 6.8: Input to the converged autoencoder and reconstructed images, exemplary shown for the Dart environment: We apply mean removal to all images before passing them to the autoencoder and add the mean to the reconstructed images. The reconstruction looks very similar to the input. The two rows represent two different random inputs. The columns from left to right depict the following: 1 - input image, 2 - input image minus mean image, 3 reconstruction of the autoencoder, 4 - mean image, 5 - reconstruction plus mean image. Chapter 7 Discussion After describing the main concepts of deep reinforcement learning, we introduced two novel algorithms, that combine the deep deterministic policy gradient (DDPG) with asynchronous methods in different ways. We first evaluated these algorithms using a simple robotic task, while the true physical states of the environment were given, and compared both to a DDPG baseline. The most important finding of these experiments is, that the variant of DDPG using asynchronous lock-free gradient updates generalizes better than the variant with locks and also converges more often than a DDPG baseline, when executing multiple runs. The decision to use a lock free approach as introduced by Mnih, Badia, et al. (2016) was mostly motivated by performance considerations and not compared to a variant without locks. We suggest that lock-free updates might be beneficial not only to shorten training time but also for improving the generalization of many algorithms using asynchronous updates. We also showed, that the combination of DDPG and asynchronous updates can be applied to solve an end-to-end learning task. Another advantage of our asynchronous DDPG implementation is, that it is about five times faster than the DDPG baseline due to the parallelization. We also investigated the effect of different pretraining techniques and successfully implemented multiple forms of pretraining, that use the physical states of the environment during training, but do not require them for testing. Levine, Finn, et al. (2016) show that real world robotic tasks sometimes provide access to the true physical states during training, but later require the trained model to act, while only observing camera images. The hybrid pretraining approach is much faster than reinforcement learning on pixel data, because pretraining is a standard deep learning task, that can be heavily parallelized and performed entirely on the GPU. In contrast to deep reinforcement learning, where it is necessary to regularly call the environment and thus communicate with the CPU, large data files can be prepared, which only need to be loaded once into memory. The simplified reinforcement learning task that follows after pretraining is also much faster than reinforcement learning 58 7. Discussion 59 on pixels, as the preprocessed state vector is comparatively small and the actor-critic model thus is far less complex. The conducted experiments show, that it is more helpful to train a custom state representation than just predicting the physical states. This can be accomplished by adding an image reconstruction target similar to that of a deep autoencoder or modeling the system dynamics with inverse-/forward models. Referring to Agrawal et al. (2016), we were able to show for a very simple experiment, that an inverse model can generalize well, while learning the system dynamics (see section 6.1). An important question, that still remains open with our work, is how this generalization effect can be transfered to effectively process pixel data. The internal model of section 6.2 performs better on images with background noise, which at first might seem irrational. While this effect could be random variance and caused by the small number of collected scores, noisy images in fact sometimes proved useful for learning to detect features. Vincent et al. (2008) showed that it can be beneficial to include additional noise in input images, because the trained model needs to find ways to distinguish noise from important features and thus learns a better representation of the important variations in the images. The same effect might apply here. For both other architectures, noisy images negatively influence the test scores. Learning to reconstruct images or learning the system dynamics presumably improves the detection of features in other ways and thus adding noise has an impact that is contrary to the positive effect we observed with the internal model. It might still be interesting to investigate the ability of all pretrained models to benefit from background noise, for example by using noisy images as input to the autoencoder and images without noise as reconstruction target. This is the principle of the denoising autoencoder (Vincent et al. 2008). In section 6.2 we also showed, that a major problem of pretraining pixel based models, is the need to generate pixel images. This is for instance the case when using an autoencoder network structure. Agrawal et al. (2016) showed that a combined inverse-/forward model can generalize well on pixel data, while they sidestep the complicated problem of generating images by first transforming the pixel images to a learned latent representation. Thereby, they establish a model, that combines the advantages of autoencoders with those of inverseand forward-models. During our experiments, we tested both concepts independently. The convolutional layers, that carry out the transformation to the latent space are jointly trained with the combined inverse-/forward model. This joint training approach cannot be applied to our experiment, because the model only learns to detect objects in the images it is able to physically interact with. The arm of our simulated reacher task however does not physically interact with the target, which thus would have never been recognized. Both of our simulated reacher experiments are similar to the OpenAI Gym reacher task1, which we did not directly use, as it requires the proprietary mujoco library. The Dart environment has been compared to the OpenAI Gym reacher task and it has been shown, 1https://gym.openai.com/envs/Reacher-v1/, last downloaded 2017-09-14 7. Discussion 60 that learned policies can be transfered between the two environments2. We however concentrated on the simple matplotlib simulation for the most experiments, which makes learning arguably easier, because it does not include realistic physics and the visualization is very simple even when additional noise is added. The experiments we conducted are therefore not very comprehensive. For comparing to state-of-the-art methods in the field of deep reinforcement learning, most researchers evaluate their algortihms on many video games and simulated robotic tasks. It has become a quasi-standard to state the scores for all Atari games and use the mujoco simulations for robotic experiments (Mnih, Badia, et al. 2016; Lillicrap et al. 2015). An interesting next step would therefore be to test the two novel variants of DDPG on a range of these experiments to being able to compare the respective scores and reliably assess the quality of these algorithms, while we showed that both algorithms can learn reasonable policies. Our distributed and asyncronous variants of DDPG are mainly motivated by the A3C algorithm. Both A3C and DDPG enjoy a high popularity and the underlying concepts are still used and further refined (O\u2019Donoghue et al. 2016; Gu, Lillicrap, et al. 2016) or applied to various real world tasks (e.g. Gu, Holly, et al. 2017). Hence, we think that deterministic policies and asynchronous learning in general still provide a good starting point for future research. The DDPG baseline we used has very high variance, but was still used for all experiments with pretrained models, because we did not want to mix up the effects of the modifications to DDPG with the effects of pretraining a state model. We observed that the variance increased when we did not train on the physical states directly, but on an intermediate representation obtained by executing a pretrained model. This can be caused by the fact, that we used two different pretrained models of the same kind for all experiments in section 6.2 and one model probably converged better than the other. Because the whole training process with pretraining and repeatedly predicting the intermediate representation during reinforcement learning is already relatively slow, we also collected less data than for the experiments on physical states (see section 6.1). We also suppose, that learning on the intermediate representation is in general harder and thus the DDPG baseline fails more often. For the end-to-end learning algorithm, we were only able to train one model with asynchronous DDPG as training the convolutional layers was still very slow, despite the use of asynchronous updates. We were generally able to provide a good solution with a score above 85% for all three main categories of experiments: reinforcement learning directly on the physical states, pretraining a state model using the physical states supplementary to pixels and learning only from pixels. An unsuccesful approach was to pretrain a state model only from pixel data. Goodfellow, Bengio, and Courville (2016) state, that unsupervised pretraining might be outdated for many applications, where end-to-end learning is possible. The reinforcement signal is supposed to provide helpful information to the algorithm and thus enhances the detection of features in images, that are useful for the task to solve. Purely unsupervised 2https://github.com/DartEnv/dart-env/wiki/OpenAI-Gym-Environments, last downloaded 2017-09-14 ",
    "Discussion": "Discussion 61 pretraining like training an autoencoder does not have this information and is therefore less efficient. Recent innovations like the variational autoencoder detect features in images very well, but are designed to generalize to small changes in position, orientation or shape of objects (Doersch 2016). Robotic applications however often require exact knowledge of positions of objects or angles like in our simulated example (see section 5.1). Nevertheless, it is possible to still make use of inverse- or forward models, as they require the model to learn the dynamics of the system. Dosovitskiy and Koltun (2016) use a forward model to solve a control task without reinforcement learning, but only focus on discrete actions. We suppose that under these conditions, unsupervised pretraining can work, but might be more useful in other scenarios with different environements than ours. The model capacity of our tested models for unsupervised pretraining should be sufficient, as a very similar model was able to solve an end-to-end learning task, but regularization strategies like batch normalization (Ioffe and Szegedy 2015) or others might be added. Mean removal seems to be crucial for the success of all models, that need to generate images. We also think, that the success of the simple inverse model of section 6.1 is mainly caused by its very fast training speed and the ability to train it on many millions of example transitions in few hours. It thus might be worthwhile to further investigate especially inverse- and forward models, that only have access to pixel data, and train them for a very long time. None of our eperiments included any form of recurrence or memory in the network structure. Recurrent neural networks like the LSTM (Hochreiter and Schmidhuber 1997) have been successfully used by many researchers to solve reinforcement learning problems, where only parts of the environment can be observed and the whole process forms a POMDP (e.g. Mnih, Badia, et al. 2016). Partially observed environments were shortly mentioned in section 3.1, while we did not consider them in any experiment throughout this thesis. Nevertheless, we suppose that many of the used network structures can be augmented with recurrent layers and thus can also be applied to partially observable environments. Bibliography Abbeel, Pieter et al. (2007). \u201cAn application of reinforcement learning to aerobatic helicopter flight\u201d. In: Advances in neural information processing systems, pp. 1\u20138 (cit. on p. 1). Agrawal, Pulkit et al. (2016). \u201cLearning to poke by poking: Experiential learning of intuitive physics\u201d. In: Advances in Neural Information Processing Systems, pp. 5074\u20135082 (cit. on pp. 35, 59). Degris, Thomas, Patrick M Pilarski, and Richard S Sutton (2012). \u201cModel-free reinforcement learning with continuous action in practice\u201d. In: American Control Conference (ACC), 2012. IEEE, pp. 2177\u20132182 (cit. on p. 25). Deisenroth, Marc Peter, Gerhard Neumann, Jan Peters, et al. (2013). \u201cA survey on policy search for robotics\u201d. Foundations and Trends in Robotics 2.1\u20132, pp. 1\u2013142 (cit. on p. 22). Doersch, Carl (2016). \u201cTutorial on variational autoencoders\u201d. arXiv preprint arXiv:1606.05908 (cit. on p. 61). Dosovitskiy, Alexey and Vladlen Koltun (2016). \u201cLearning to act by predicting the future\u201d. arXiv preprint arXiv:1611.01779 (cit. on pp. 35, 37, 61). Doya, Kenji (2000). \u201cComplementary roles of basal ganglia and cerebellum in learning and motor control\u201d. Current opinion in neurobiology 10.6, pp. 732\u2013739 (cit. on p. 5). Goodfellow, Ian, Yoshua Bengio, and Aaron Courville (2016). Deep learning. MIT press (cit. on pp. 1, 3, 7, 15, 19, 32, 60). Gu, Shixiang, Ethan Holly, et al. (2017). \u201cDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates\u201d. In: International Conference on Robotics and Automation (ICRA). IEEE, pp. 3389\u20133396 (cit. on pp. 28, 60). Gu, Shixiang, Timothy Lillicrap, et al. (2016). \u201cQ-prop: Sample-efficient policy gradient with an off-policy critic\u201d. arXiv preprint arXiv:1611.02247 (cit. on p. 60). Hausknecht, Matthew and Peter Stone (2015). \u201cDeep recurrent q-learning for partially observable mdps\u201d. CoRR, abs/1507.06527 (cit. on pp. 5, 8). Hochreiter, Sepp and J\u00fcrgen Schmidhuber (1997). \u201cLong short-term memory\u201d. Neural computation 9.8, pp. 1735\u20131780 (cit. on p. 61). 62 Bibliography 63 Ioffe, Sergey and Christian Szegedy (2015). \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift\u201d. In: International Conference on Machine Learning, pp. 448\u2013456 (cit. on p. 61). Kaelbling, Leslie Pack, Michael L Littman, and Andrew W Moore (1996). \u201cReinforcement learning: A survey\u201d. Journal of artificial intelligence research 4, pp. 237\u2013285 (cit. on pp. 7, 9). Kawato, Mitsuo (1999). \u201cInternal models for motor control and trajectory planning\u201d. Current opinion in neurobiology 9.6, pp. 718\u2013727 (cit. on pp. 31, 35). Kingma, Diederik and Jimmy Ba (2014). \u201cAdam: A method for stochastic optimization\u201d. arXiv preprint arXiv:1412.6980 (cit. on p. 20). Kingma, Diederik and Max Welling (2013). \u201cAuto-encoding variational bayes\u201d. arXiv preprint arXiv:1312.6114 (cit. on pp. 4, 33). Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton (2012). \u201cImagenet classification with deep convolutional neural networks\u201d. In: Advances in neural information processing systems, pp. 1097\u20131105 (cit. on p. 15). Lee, Honglak et al. (2007). \u201cEfficient sparse coding algorithms\u201d. In: Advances in neural information processing systems, pp. 801\u2013808 (cit. on p. 32). Levine, Sergey, Chelsea Finn, et al. (2016). \u201cEnd-to-end training of deep visuomotor policies\u201d. Journal of Machine Learning Research 17.39, pp. 1\u201340 (cit. on pp. 14, 30, 31, 58). Levine, Sergey, Peter Pastor, et al. (2016). \u201cLearning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\u201d. The International Journal of Robotics Research (cit. on pp. 28, 30). Lillicrap, Timothy et al. (2015). \u201cContinuous control with deep reinforcement learning\u201d. arXiv preprint arXiv:1509.02971 (cit. on pp. 2, 7, 27, 45, 47, 60). Lin, Long-H (1992). \u201cSelf-improving reactive agents based on reinforcement learning, planning and teaching\u201d. Machine learning 8.3/4, pp. 69\u201397 (cit. on pp. 7, 17). Makhzani, Alireza et al. (2015). \u201cAdversarial autoencoders\u201d. arXiv preprint arXiv:1511.05644 (cit. on p. 33). Mnih, Volodymyr, Adria Puigdomenech Badia, et al. (2016). \u201cAsynchronous methods for deep reinforcement learning\u201d. In: International Conference on Machine Learning, pp. 1928\u20131937 (cit. on pp. 2, 28\u201330, 45, 47, 58, 60, 61). Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, et al. (2013). \u201cPlaying atari with deep reinforcement learning\u201d. arXiv preprint arXiv:1312.5602 (cit. on pp. 7, 16). Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, et al. (2015). \u201cHumanlevel control through deep reinforcement learning\u201d. Nature 518.7540, pp. 529\u2013533 (cit. on pp. 16\u201318). O\u2019Donoghue, B. et al. (2016). \u201cCombining policy gradient and Q-learning\u201d. ArXiv e-prints arXiv:1611.01626 (cit. on p. 60). Peters, Jan and J Andrew Bagnell (2011). \u201cPolicy gradient methods\u201d. In: Encyclopedia of Machine Learning. Springer, pp. 774\u2013776 (cit. on pp. 22, 23). Bibliography 64 Recht, Benjamin et al. (2011). \u201cHogwild: A lock-free approach to parallelizing stochastic gradient descent\u201d. In: Advances in neural information processing systems, pp. 693\u2013701 (cit. on pp. 28, 45). Rifai, Salah et al. (2011). \u201cContractive auto-encoders: Explicit invariance during feature extraction\u201d. In: Proceedings of the 28th international conference on machine learning (ICML-11), pp. 833\u2013840 (cit. on p. 33). Scherer, Dominik, Andreas M\u00fcller, and Sven Behnke (2010). \u201cEvaluation of pooling operations in convolutional architectures for object recognition\u201d. Artificial Neural Networks\u2013ICANN 2010, pp. 92\u2013101 (cit. on p. 16). Silver, David, Aja Huang, et al. (2016). \u201cMastering the game of Go with deep neural networks and tree search\u201d. Nature 529.7587, pp. 484\u2013489 (cit. on p. 1). Silver, David, Guy Lever, et al. (2014). \u201cDeterministic policy gradient algorithms\u201d. In: Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 387\u2013395 (cit. on p. 26). Sutton, Richard S (1988). \u201cLearning to predict by the methods of temporal differences\u201d. Machine learning 3.1, pp. 9\u201344 (cit. on p. 12). Sutton, Richard S and Andrew G Barto (1998). Reinforcement learning: An introduction. MIT press Cambridge (cit. on pp. 4, 6, 8\u201311, 13, 25). Sutton, Richard S, David A McAllester, et al. (2000). \u201cPolicy gradient methods for reinforcement learning with function approximation\u201d. In: Advances in neural information processing systems, pp. 1057\u20131063 (cit. on p. 25). Tieleman, Tijmen and Geoffrey Hinton (2012). \u201cLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude\u201d. COURSERA: Neural networks for machine learning 4.2, pp. 26\u201331 (cit. on p. 20). Van Hasselt, Hado, Arthur Guez, and David Silver (2016). \u201cDeep Reinforcement Learning with Double Q-Learning.\u201d In: AAAI, pp. 2094\u20132100 (cit. on p. 20). Vincent, Pascal et al. (2008). \u201cExtracting and composing robust features with denoising autoencoders\u201d. In: Proceedings of the 25th international conference on Machine learning. ACM, pp. 1096\u20131103 (cit. on pp. 4, 32, 59). Wang, Ziyu et al. (2015). \u201cDueling network architectures for deep reinforcement learning\u201d. arXiv preprint arXiv:1511.06581 (cit. on pp. 20, 21). Watkins, Christopher John Cornish Hellaby (1989). \u201cLearning from delayed rewards\u201d. PhD thesis. King\u2019s College, Cambridge (cit. on pp. 11, 13). Watkins, Christopher John Cornish Hellaby and Peter Dayan (1992). \u201cQ-learning\u201d. Machine learning 8.3-4, pp. 279\u2013292 (cit. on p. 10). Williams, Ronald J (1992). \u201cSimple statistical gradient-following algorithms for connectionist reinforcement learning\u201d. Machine learning 8.3-4, pp. 229\u2013256 (cit. on pp. 23, 24). Woergoetter, Florentin and Bernd Porr (2008). \u201cReinforcement learning\u201d. Scholarpedia 3.3, p. 1448 (cit. on pp. 7, 11). Zhang, Fangyi et al. (2015). \u201cTowards vision-based deep reinforcement learning for robotic motion control\u201d. arXiv preprint arXiv:1511.03791 (cit. on p. 16). ",
    "Experiment": "Experiment background enabled Raw Scores Internal model yes 73%, 65%, 62%, 52%, 48%, 2% Internal model no 63%, 61%, 55%, 2%, 1%, 1% Autoencoder w. physical states yes 62%, 59%, 34%, 28%, 1%, 0% Autoencoder w. physical states no 90%, 3%, 2%, 2%, 0%, 0% Autoencoder w. physical states yes (mean removal) 89%, 85%, 82%, 56%, 1%, 0% Forward model w. physical states yes 60%, 60%, 52%, 0%, 0%, 0% Forward model w. physical states no 82%, 50%, 23%, 1%, 0%, 0% Table A.1: Raw scores for the experiments conducted using the physical states during training and only pixel data for testing. Experiment Raw Scores Full Autoencoder 25%, 12%, 8%, 5%, 3%, 2% Table A.2: Raw scores for the experiments conducted using pixel images during training and testing. 65 ",
    "title": "Contents Abstract Introduction Different Types of Learning Variants of Reinforcement Learning Extensions to Reinforcement Learning Methods Experimental Results Discussion Bibliography Raw Scores",
    "paper_info": "Appendix A\nRaw Scores\nExperiment\nbackground enabled\nRaw Scores\nInternal model\nyes\n73%, 65%, 62%, 52%, 48%, 2%\nInternal model\nno\n63%, 61%, 55%, 2%, 1%, 1%\nAutoencoder w. physical states\nyes\n62%, 59%, 34%, 28%, 1%, 0%\nAutoencoder w. physical states\nno\n90%, 3%, 2%, 2%, 0%, 0%\nAutoencoder w. physical states\nyes (mean removal)\n89%, 85%, 82%, 56%, 1%, 0%\nForward model w. physical states\nyes\n60%, 60%, 52%, 0%, 0%, 0%\nForward model w. physical states\nno\n82%, 50%, 23%, 1%, 0%, 0%\nTable A.1: Raw scores for the experiments conducted using the physical states during\ntraining and only pixel data for testing.\nExperiment\nRaw Scores\nFull Autoencoder\n25%, 12%, 8%, 5%, 3%, 2%\nTable A.2: Raw scores for the experiments conducted using pixel images during training\nand testing.\n65\n",
    "GPTsummary": "- (1): The research uses deep reinforcement learning to address the problem of robotic control in continuous action spaces and high-dimensional state spaces.\n\n- (2): Two new algorithms are proposed, distributed and asynchronous DDPG, by combining two commonly used reinforcement learning methods in response to the limitations of standard algorithms for reinforcement learning problems.\n\n- (3): The proposed methodology focuses on preprocessing information to simplify the reinforcement learning task and reduce training time. The algorithms are evaluated using a simulated robotic\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant in addressing the problem of robotic control in continuous action spaces and high-dimensional state spaces using deep reinforcement learning. The proposed methodologies of preprocessing information and combining two reinforcement learning methods contribute to the improvement of training efficiency and task performance.\n\n- (2): Innovation point: The paper proposes two new algorithms called distributed and asynchronous DDPG that combine two commonly used reinforcement learning methods to handle continuous action spaces and high-dimensional state spaces. Performance: The proposed algorithms show significantly better performance than the baseline algorithms in the simulated robotic environment, demonstrating the effectiveness of the proposed approach in handling complex robotic control problems. Workload: However, the workload required for implementing the proposed approach may be high, as it involves preprocessing of information and coordination of multiple agents.\n\n\n",
    "GPTmethods": "- (1): The research uses deep reinforcement learning to address the problem of robotic control in continuous action spaces and high-dimensional state spaces.\n\n- (2): Two new algorithms are proposed, distributed and asynchronous DDPG, by combining two commonly used reinforcement learning methods in response to the limitations of standard algorithms for reinforcement learning problems.\n\n- (3): The proposed methodology focuses on preprocessing information to simplify the reinforcement learning task and reduce training time. The algorithms are evaluated using a simulated robotic\n\n\n\n\n\n8. Conclusion: \n\n- (1): This piece of work is significant in addressing the problem of robotic control in continuous action spaces and high-dimensional state spaces using deep reinforcement learning. The proposed methodologies of preprocessing information and combining two reinforcement learning methods contribute to the improvement of training efficiency and task performance.\n\n- (2): Innovation point: The paper proposes two new algorithms called distributed and asynchronous DDPG that combine two commonly used reinforcement learning methods to handle continuous action spaces and high-dimensional state spaces. Performance: The proposed algorithms show significantly better performance than the baseline algorithms in the simulated robotic environment, demonstrating the effectiveness of the proposed approach in handling complex robotic control problems. Workload: However, the workload required for implementing the proposed approach may be high, as it involves preprocessing of information and coordination of multiple agents.\n\n\n",
    "GPTconclusion": "- (1): This piece of work is significant in addressing the problem of robotic control in continuous action spaces and high-dimensional state spaces using deep reinforcement learning. The proposed methodologies of preprocessing information and combining two reinforcement learning methods contribute to the improvement of training efficiency and task performance.\n\n- (2): Innovation point: The paper proposes two new algorithms called distributed and asynchronous DDPG that combine two commonly used reinforcement learning methods to handle continuous action spaces and high-dimensional state spaces. Performance: The proposed algorithms show significantly better performance than the baseline algorithms in the simulated robotic environment, demonstrating the effectiveness of the proposed approach in handling complex robotic control problems. Workload: However, the workload required for implementing the proposed approach may be high, as it involves preprocessing of information and coordination of multiple agents.\n\n\n"
}