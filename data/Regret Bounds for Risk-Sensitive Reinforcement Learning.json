{
    "Abstract": "Abstract In safety-critical applications of reinforcement learning such as healthcare and robotics, it is often desirable to optimize risk-sensitive objectives that account for tail outcomes rather than expected reward. We prove the \ufb01rst regret bounds for reinforcement learning under a general class of risk-sensitive objectives including the popular CVaR objective. Our theory is based on a novel characterization of the CVaR objective as well as a novel optimistic MDP construction. 1 ",
    "Introduction": "Introduction There has been recent interest in risk-sensitive reinforcement learning, which replaces the usual expected reward objective with one that accounts for variation in possible outcomes. One of the most popular risk-sensitive objectives is the conditional value-at-risk (CVaR) objective [1, 2, 3, 4], which is the average risk at some tail of the distribution of returns (i.e., cumulative rewards) under a given policy [5, 6]. More generally, we consider a broad class of objectives in the form of a weighted integral of quantiles of the return distribution, of which CVaR is a special case. A key question is providing regret bounds for risk-sensitive reinforcement learning. While there has been some work studying this question, it has focused on a speci\ufb01c objective called the entropic risk measure [7, 8], leaving open the question of bounds for more general risk-sensitive objectives. There has also been work on optimistic exploration for CVaR [9], but without any regret bounds. We provide the \ufb01rst regret bounds for risk-sensitive reinforcement learning with objectives of form \u03a6(\u03c0) = \ufffd 1 0 F \u2020 Z(\u03c0)(\u03c4) \u00b7 dG(\u03c4), (1) where Z(\u03c0) is the random variable encoding the return of policy \u03c0, FZ(\u03c0) is its quantile function (roughly speaking, the inverse CDF), and G is a weighting function over the quantiles. This class captures a broad range of useful objectives, and has been studied in prior work [10, 4]. We focus on the episodic setting, where the agent interacts with the environment, modeled by a Markov decision process (MDP), over a \ufb01xed sequence of episodes. Its goal is to minimize the regret\u2014i.e., the gap between the objective value it achieves compared to the optimal policy. Our approach is based on the upper con\ufb01dence bound strategy [11, 12], which makes decisions according to an optimistic estimate of the MDP. We prove that this algorithm (denoted A) has regret regret(A) = \u02dcO \ufffd T 2 \u00b7 LG \u00b7 |S|3/2 \u00b7 |A| \u00b7 \u221a K \ufffd , Preprint. Under review. arXiv:2210.05650v1  [cs.LG]  11 Oct 2022 ",
    "Problem Formulation": "Problem Formulation Markov decision process. We consider a Markov decision process (MDP) M = (S, A, D, P, P, T), with \ufb01nite state space S, \ufb01nite action space A, initial state distribution D(s), \ufb01nite time horizon T, transition probabilities P(s\u2032 | s, a), and reward measure PR(s,a); without loss of generality, we assume r \u2208 [0, 1] with probability one. A history is a sequence \u03be \u2208 Z = T\ufffd t=1 Zt where Zt = (S \u00d7 A \u00d7 R)t\u22121 \u00d7 S Intuitively, a history captures the interaction between an agent and M up to step t. We consider stochastic, time-varying, history-dependent policies \u03c0t(at | \u03bet), where t is the time step. Given \u03c0, the history \u039e(\u03c0) t generated by \u03c0 up to step t is a random variable with probability measure P\u039e(\u03c0) t (\u03bet) = \ufffd D(s1) if t = 1 P\u039e(\u03c0) t\u22121(\u03bet\u22121) \u00b7 \u03c0t(at | \u03bet\u22121) \u00b7 PR(st,at)(rt) \u00b7 P(st+1 | st, at) otherwise, where for all \u03c4 \u2208 [T] we use the notation \u03be\u03c4 = ((s1, a1, r1), ..., (s\u03c4\u22121, a\u03c4\u22121, r\u03c4\u22121), s\u03c4). 2 Finally, an episode (or rollout) is a history \u03be \u2208 ZT of length T generated by a given policy \u03c0. Bellman equation. The return of \u03c0 on step t is the random variable (Z(\u03c0) t (\u03bet))(\u03beT ) = \ufffdT \u03c4=t rt, where \u03beT \u223c P\u039e(\u03c0) T (\u00b7 | \u039e(\u03c0) t = \u03bet)\u2014i.e., it is the reward from step t given that the current history is \u03bet. De\ufb01ning Z(\u03c0) T +1(\u03be, s) = 0, the distributional Bellman equation [14, 9] is FZ(\u03c0) t (\u03be)(x) = \ufffd a\u2208A \u03c0t(a | \u03be) \ufffd s\u2032\u2208S P(s\u2032 | S(\u03be), a) \ufffd FZ(\u03c0) t+1(\u03be\u25e6(a,r,s\u2032))(x \u2212 r) \u00b7 dPR(s,a)(r), where S(\u03be) = s for \u03be = (..., s) is the current state in history \u03be, and FX is the cumulative distribution function (CDF) of random variable X. Finally, the cumulative return of \u03c0 is Z(\u03c0) = Z(\u03c0) 1 (\u03be), where \u03be = (s) \u2208 Z1 for s \u223c D is the initial history; in particular, we have FZ(\u03c0)(x) = \ufffd FZ(\u03c0) 1 (\u03be)(x) \u00b7 dD(s). Risk-sensitive objective. The quantile function of a random variable X is F \u2020 X(\u03c4) = inf {x \u2208 R | FX(x) \u2265 \u03c4} . Note that if FX is strictly monotone, then it is invertible and we have F \u2020 X(\u03c4) = F \u22121 X (\u03c4). Now, our objective is given by the Riemann-Stieljes integral \u03a6M(\u03c0) = \ufffd 1 0 F \u2020 Z(\u03c0)(\u03c4) \u00b7 dG(\u03c4), where G(\u03c4) is a given CDF over quantiles \u03c4 \u2208 [0, 1]. This objective was originally studied in [15] for the reinforcement learning setting. For example, choosing G(\u03c4) = min{\u03c4/\u03b1, 1} (i.e., the CDF of the distribution Uniform([0, \u03b1])) for \u03b1 \u2208 [0, 1] yields the \u03b1-conditional value at risk (CVaR) objective; furthermore, taking \u03b1 = 1 yields the usual expected cumulative reward objective. In addition, choosing G(\u03c4) = 1(\u03c4 \u2264 \u03b1) for \u03b1 \u2208 [0, 1] yields the \u03b1 value at risk (VaR) objective. Other risk sensitive-objectives can also be captured in this form, for example the Wang measure [16], and the cumulative probability weighting (CPW) metric [17]. We call any policy \u03c0\u2217 M \u2208 arg max \u03c0 \u03a6M(\u03c0). an optimal policy\u2014i.e., it maximizes the given objective for M. Assumptions. First, we have the following assumption on the quantile function for Z(\u03c0): Assumption 2.1. F \u2020 Z(\u03c0)(1) = T. Since T is the maximum reward attainable in an episode, this assumption says that the maximum reward is attained with some nontrivial probability. This assumption is very minor; for any given MDP M, we can modify M to include a path achieving reward T with arbitrarily low probability. Assumption 2.2. G is LG-Lipschitz continuous for some LG \u2208 R>0, and G(0) = 0. For example, for the \u03b1-CVaR objective, we have LG = 1/\u03b1. Assumption 2.3. We are given an algorithm for computing \u03c0\u2217 M for a given MDP M. For CVaR objectives, existing algorithms [13] can compute \u03c0\u2217 M with any desired approximation error. For completeness, we give a formal description of the procedure in Appendix D. When unambiguous, we drop the dependence on M and simply write \u03c0\u2217. Finally, our goal is to learn while interacting with the MDP M across a \ufb01xed number of episodes K. In particular, at the beginning of each episode k \u2208 [K], our algorithm chooses a policy \u03c0(k) = A(Hk), where Hk = {\u03beT,\u03ba}k\u22121 \u03ba=1 is the random set of episodes observed so far, to use for the duration of episode k. Then, our goal is to design an algorithm A that aims to minimize regret, which measures the expected sub-optimality with respect to \u03c0\u2217: regret(A) = E \uf8ee \uf8f0 \ufffd k\u2208[K] \u03a6(\u03c0\u2217) \u2212 \u03a6(\u03c0(k)) \uf8f9 \uf8fb . Finally, for simplicity, we assume that the initial state distribution D is known; in practice, we can remove this assumption using a standard strategy. 3 3 Optimal Risk-Sensitive Policies In this section, we characterize properties of the optimal risk-sensitive policy \u03c0\u2217 M. First, we show that it suf\ufb01ces to consider policies dependent on the current state and the cumulative rewards obtained so far, rather than the entire history. Second, the cumulative reward is a continuous quantity, making it dif\ufb01cult to compute the optimal policy; we prove that discretizing this component does not signi\ufb01cantly reduce the objective value. For CVaR objectives, these results imply that existing algorithms can be used to compute the optimal risk-sensitive policy [13]. Augmented state space. We show there exists an optimal policy \u03c0\u2217 t (at | yt, st) that only depends on the current state st and cumulative reward yt = J(\u03bet) = \ufffdt\u22121 \u03c4=1 r\u03c4 obtained so far. To this end, let Zt(y, s) = {\u03be \u2208 Zt | J(\u03bet) \u2264 y \u2227 st = s} be the set of length t histories \u03be with cumulative reward at most y so far, and current state s. For any history-dependent policy \u03c0, de\ufb01ne the alternative policy \u02dc\u03c0 by \u02dc\u03c0t(at | \u03bet) = E\u039e(\u03c0) t \ufffd \u03c0t(at | \u039e(\u03c0) t ) \ufffd\ufffd\ufffd \u039e(\u03c0) t \u2208 Zt(J(\u03bet), st) \ufffd . Note that \u02dc\u03c0 only depends on \u03bet through yt = J(\u03bet) and st, we can de\ufb01ne \u02dc\u03c0t(at | \u03bet) = \u02dc\u03c0t(at | yt, st). Theorem 3.1. For any policy \u03c0, we have \u03a6(\u02dc\u03c0) = \u03a6(\u03c0). We give a proof in Appendix A. In particular, given any optimal policy \u03c0\u2217, we have \u03a6(\u02dc\u03c0\u2217) = \u03a6(\u03c0\u2217); thus, we have \u02dc\u03c0\u2217 \u2208 arg max\u03c0 \u03a6(\u03c0). Finally, we note that this result has already been shown for CVaR objectives [13]; our theorem generalizes the existing result to any risk-sensitive objective that can be expressed as a weighted integral of the quantile function. Augmented MDP. As a consequence of Theorem 3.1, it suf\ufb01ces to consider the augmented MDP \u02dc M = ( \u02dcS, A, \u02dcD, \u02dcP, \u02dcP, T). First, \u02dcS = S \u00d7 R is the augmented state space; for a state (s, y) \u2208 \u02dcS, the \ufb01rst component encodes the current state and the second encodes the cumulative rewards so far. The initial state distribution is a probability measure \u02dcD((s, y)) = D(s) \u00b7 \u03b40(y), where \u03b40 is the Dirac delta measure placing all probability mass on y = 0 (i.e., the cumulative reward so far is initially zero). The transitions are given by the product measure \u02dcP((s\u2032, y\u2032) | (s, y), a) = P(s\u2032 | s, a) \u00b7 PR(s,a)(y\u2032 \u2212 y), i.e., the second component of the state space is incremented as y\u2032 = y + r, where r is the reward achieved in the original MDP. Finally, the rewards are now only provided on the \ufb01nal step: PRt((s,y),a)(r) = \ufffd\u03b4y(r) if t = T 0 otherwise, i.e., the reward at the end of a rollout is simply the cumulative reward so far, as encoded by the second component of the state. By Theorem 3.1, it suf\ufb01ces to compute the optimal policy for \u02dc M over history-independent policies \u03c0t(at | \u02dcst): max \u03c0\u2208\u03a0ind \u03a6 \u02dc M(\u03c0) = max \u03c0 \u03a6M(\u03c0), where \u03a0ind is the set of history-independent policies. Once we have \u03c0\u2217 \u02dc M, we can use it in M by de\ufb01ning \u03c0M(a | \u03be, s) = \u03c0\u2217 \u02dc M(a | J(\u03be), s). Discretized augmented MDP. Planning over \u02dc M is complicated by the fact that the second component of its state space is continuous. Thus, we consider an \u03b7-discretization of \u02dc M, for some \u03b7 \u2208 R>0. To this end, we modify the reward function so that it only produces rewards in \u03b7 \u00b7N = {\u03b7 \u00b7n | n \u2208 N}, by always rounding the reward up. Then, sums of these rewards are contained in \u03b7 \u00b7 N, so we can replace the second component of \u02dcS with \u03b7 \u00b7 N. In particular, we consider the discretized MDP \u02c6 M = ( \u02c6S, A, \u02dcD, \u02c6P, \u02dcP, T), where \u02c6S = S \u00d7 (\u03b7 \u00b7 N), and transition probability measure \u02c6P((s\u2032, y\u2032) | (s, y), a) = P(s\u2032 | s, a) \u00b7 (PR(s,a) \u25e6 \u03c6\u22121)(y\u2032 \u2212 y) 4 where \u03c6(r) = \u03b7 \u00b7 \u2308r/\u03b7\u2309. That is, PR(s,a) is replaced with the pushforward measure PR(s,a) \u25e6 \u03c6\u22121, which gives reward \u03b7 \u00b7 i with probability PR(s,a)[\u03b7 \u00b7 (i \u2212 1) < r \u2264 \u03b7 \u00b7 i]. Now, we prove that the optimal policy \u03c0\u2217 \u02c6 M for the discretized augmented MDP \u02c6 M achieves objective value close to the optimal policy \u03c0\u2217 M for the original MDP M. Importantly, we want to consider measure performance of both policies based on the objective \u03a6M of the original MDP M. To do so, we need a way to use \u03c0\u2217 \u02c6 M in M. Note that \u03c0\u2217 \u02c6 M depends only on the state \u02c6s = (s, y), where s \u2208 S is a state of the original MDP M, and y \u2208 \u03b7 \u00b7 N is a discretized version of the cumulative reward obtained so far. Thus, we can run \u03c0\u2217 \u02c6 M in M by simply rounding the reward rt at each step t up to the nearest value \u02c6rt \u2208 \u03b7 \u00b7 N at each step\u2014i.e., \u02c6rt = \u03c6(rt); then, we increment the internal state as yt = yt\u22121 + \u02c6rt. We call the resulting policy \u03c0M the version of \u03c0\u2217 \u02c6 M adapted to M. Then, our next result says that the performance of \u03c0M is not too much worse than the performance of \u03c0\u2217 M. Theorem 3.2. Let \u03c0\u2217 \u02c6 M be the optimal policy for the discretized augmented MDP \u02c6 M, let \u03c0M be the policy \u03c0\u2217 \u02c6 M adapted to the original MDP M, and let \u03c0\u2217 M be the optimal (history-dependent) policy for the original MDP M. Then, we have \u03a6M(\u03c0M) \u2265 \u03a6M(\u03c0\u2217 M) \u2212 \u03b7. We give a proof in Appendix B. Note that we can set \u03b7 to be suf\ufb01ciently small to achieve any desired error level (i.e., choose \u03f5/T, where \u03f5 is the desired error). The only cost is in computation time. Note that the number of states in \u02c6 M is still in\ufb01nite; however, since the cumulative return satis\ufb01es y \u2208 [0, H], it suf\ufb01ces to take \u02c6S = S \u00d7 (\u03f5 \u00b7 [\u2308H/\u03b7\u2309]); then, \u02c6 M has | \u02c6S| = |S| \u00b7 \u2308H/\u03b7\u2309 states. 4 Upper Con\ufb01dence Bound Algorithm Here, we present our upper con\ufb01dence bound (UCB) algorithm (summarized in Algorithm 1). At a high level, for each episode, our algorithm constructs an estimate M(k) of the underlying MDP M based on the prior episodes i \u2208 [k \u2212 1]; to ensure exploration, it optimistically in\ufb02ates the estimate of the reward probability measure P. Then, it plans in M(k) to obtain an optimistic policy \u03c0(k) = \u03c0\u2217 M(k), and uses this policy to act in the MDP for episode k. Optimistic MDP. We de\ufb01ne M(k). Without loss of generality, we assume S includes a distinguished state s\u221e with rewards FR(s\u221e,a)(r) = 1(r \u2265 1) (i.e., achieve the maximum reward r = 1 with probability one), and transitions P(s\u221e | s, a) = 1(s = s\u221e) and P(s\u2032 | s\u221e, a) = 1(s\u2032 = s\u221e) (i.e., inaccessible from other states and only transitions to itself). Our construction of \u02c6 M(k) uses s\u221e for optimism. Now, let \u02dc M(k) be the MDP using the empirical estimates of the transitions and rewards: \u02dcP (k)(s\u2032 | s, a) = Nk,t(s, a, s\u2032) Nk,t(s, a) F \u02dc R(k)(s,a)(r) = 1 Nk,t(s, a) k\u22121 \ufffd i=1 T \ufffd t=1 1(r \u2264 ri,t) \u00b7 1 (si,t = s \u2227 ai,t = a) . Then, let \u02c6 M(k) be the optimistic MDP; in particular, its transitions \u02c6P (k)(s\u2032 | s, a) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1(s\u2032 = s\u221e) if s = s\u221e 1 \u2212 \ufffd s\u2032\u2208S\\{s\u221e} \u02dcP (k)(s\u2032 | s, a) if s\u2032 = s\u221e max \ufffd \u02dcP (k)(s\u2032 | s, a) \u2212 \u03f5(k) R (s, a), 0 \ufffd otherwise transition to the optimistic state s\u221e when uncertain, and its rewards F \u02c6 R(k)(s,a)(r) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1(r \u2265 1) if s = s\u221e 1 if r \u2265 1 max \ufffd F \u02dc R(k)(s,a)(r) \u2212 \u03f5(k) R (s, a), 0 \ufffd otherwise optimistically shift the reward CDF downwards. Here, \u03f5(k) P (s, a) and \u03f5(k) R (s, a) are de\ufb01ned in Section 5; intuitively, they are high-probability upper bounds on the errors of the empirical estimates \u02dcP (k)(\u00b7 | s, a) and F \u02dc R(k)(s,a) of the transitions and rewards, respectively. 5 Algorithm 1 Upper Con\ufb01dence Bound Algorithm 1: for k \u2208 [K] do 2: Compute M(k) and \u03c0(k) = \u03c0\u2217 M(k) using prior episodes {\u03be(i) | i \u2208 [k \u2212 1]} 3: Execute \u03c0(k) in the true MDP M and observe episode \u03be(k) = [(sk,t, ak,t, rk,t)]T t=1 \u222a [sk,T +1] 4: end for Theoretical guarantees. We have the following upper bound on the regret of Algorithm 1. Theorem 4.1. Denote Algorithm 1 by A. For any \u03b4 \u2208 (0, 1], with probability at least 1 \u2212 \u03b4, we have regret(A) \u2264 4T 3/2 \u00b7 LG \u00b7 |S| \u00b7 \ufffd 5|S| \u00b7 |A| \u00b7 K \u00b7 log \ufffd4|S| \u00b7 |A| \u00b7 K \u03b4 \ufffd = \u02dcO( \u221a K). We brie\ufb02y compare our bound to existing ones in the setting of expected return objectives. The dependence on the number of episodes K matches existing bounds [11, 12]; since this is optimal in the setting of expected return, and expected return is a special case of our setting (with G(\u03c4) = \u03c4), our bound is also optimal in K. In terms of the dependence on the number of states |S|, our bound has an extra \ufffd |S| factor compared to the UCRL2 algorithm [11], and an extra |S| factor compared to the improved bound of the UCBVI algorithm [12]. One extra \ufffd |S| comes from down-shifting transitions uniformly in the construction of the optimistic MDP \u02c6 M(k). This \ufffd |S| may be removed by a more careful construction of the optimistic MDP. Another extra \ufffd |S| compared to UCBVI comes from bounding the estimation error of the reward distribution. We believe it may be possible to remove this \ufffd |S| through a more careful treatment of the estimation error, similar to the one in UCBVI. We leave both of these potential re\ufb01nements to future work. In terms of the dependence on the number of actions |A|, our bound matches the order of \ufffd |A| in both UCRL2 and UCBVI. Our dependence on the horizon length T is T 3/2, compared to the same order of T 3/2 in UCBVI and T in a variant of UCBVI [12] utilizing a carefully designed variance-based bonus. 5 Proof of Theorem 4.1 We prove Theorem 4.1; we defer proofs of several lemmas to Appendix C. At a high level, the proof proceeds in three steps. First, we prove our key Lemma 5.1, which expresses the objective \u03a6 in terms of an integral of the weighted CDF of the return. This lemma allows us to translate bounds on the difference between CDFs of the estimated return \u02c6Z(\u03c0) and the true return Z(\u03c0) into bounds on the difference between corresponding objective values. The proof of this lemma is divided into three parts that deal with different sets of points in the domain of the quantile function F \u2020 Z(\u03c0): (i) discontinuous; (ii) continuous and strictly monotone; (iii) continuous and non-strictly monotone. This result is used throughout the remainder of the proof. Second, we de\ufb01ne E to be the event where the optimistic estimated MDP \u02c6 M(k) falls into a certain con\ufb01dence set around the true MDP M for each k \u2208 [K]; in Lemma 5.2, we prove that E holds with high probability. Then, in Lemma 5.6, we prove that under event E, the objective values of \u02c6 M(k) and M are close. To prove this lemma, we separately show that (i) the objective values of the estimated MDP \u02dc M(k) (estimated without optimism) and M are close (Lemma 5.4), and (ii) the objective values of \u02c6 M(k) and \u02dc M(k) are close (Lemma 5.5). Third, in Lemma 5.7, we prove that under event E, the MDP \u02c6 M(k) is indeed optimistic. Together, these results imply the regret bound using the standard UCB proof strategy. We proceed with the proof. First, we have our key result providing an equivalent expression for \u03a6: Lemma 5.1. We have \u03a6(\u03c0) = T \u2212 \ufffd R G(FZ(\u03c0)(x)) \u00b7 dx. 6 Proof. First, note that by integration by parts, we have \u03a6(\u03c0) = \ufffd 1 0 F \u2020 Z(\u03c0)(\u03c4) \u00b7 dG(\u03c4) = \ufffd F \u2020 Z(\u03c0)(\u03c4) \u00b7 G(\u03c4) \ufffd1 0 \u2212 \ufffd 1 0 G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) = T \u2212 \ufffd 1 0 G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4), where the last line follows by Assumptions 2.1 & 2.2. Thus, it suf\ufb01ces to show that \ufffd 1 0 G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) = \ufffd R G(FZ(\u03c0)(x)) \u00b7 dx. The quantile function F \u2020 Z(\u03c0) is monotonically increasing and left-continuous [18], so this integral is equivalently a Lebesgue-Stieltjes integral [19]. Dividing the unit interval I = [0, 1] into disjoint sets I(1) = {\u03c4 \u2208 I | F \u2020 Z(\u03c0)(\u03c4) is discontinuous} I(2) = {\u03c4 \u2208 I | F \u2020 Z(\u03c0)(\u03c4) is continuous and strictly monotone} I(3) = {\u03c4 \u2208 I | F \u2020 Z(\u03c0)(\u03c4) is continuous and non-strictly monotone}, then we have \ufffd 1 0 G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) = \ufffd I(1) G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) + \ufffd I(2) G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) + \ufffd I(3) G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4). We consider each of the three terms separately and then combine them to \ufb01nish the proof. First term. Note that I(1) = {\u03c4 (1) i }\u221e i=1 is countable since monotone functions can have countably many discontinuities. Also, for each i \u2208 N, the measure assigned to \u03c4 (1) i by dF \u2020 Z(\u03c0) is dF \u2020 Z(\u03c0)({\u03c4 (1) i }) = lim \u03c4\u2192\u03c4 (1) i + F \u2020 Z(\u03c0)(\u03c4) \u2212 F \u2020 Z(\u03c0)(\u03c4 (1) i ) =: x(1)+ i \u2212 x(1) i . Thus, we have \ufffd I(1) G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) = \u221e \ufffd i=1 G(\u03c4 (1) i ) \u00b7 (x(1)+ i \u2212 x(1) i ) = \u221e \ufffd i=1 G(\u03c4 (1) i ) \u00b7 \ufffd x(1)+ i x(1) i dx = \u221e \ufffd i=1 \ufffd x(1)+ i x(1) i G(FZ(\u03c0)(x)) \u00b7 dx = \u221e \ufffd i=1 \ufffd F \u22121 Z(\u03c0)({\u03c4 (1) i }) G(FZ(\u03c0)(x)) \u00b7 dx = \ufffd F \u22121 Z(\u03c0)(I(1)) G(FZ(\u03c0)(x)) \u00b7 dx. On the second line, we have used the fact that FZ(\u03c0)(x) = \u03c4 (1) i for all x \u2208 [x(1) i , x(1)+ i ). To see this fact, note that since x \u2265 x(1) i , by monotonicity of FZ(\u03c0), we have FZ(\u03c0)(x) \u2265 FZ(\u03c0)(x(1) i ) = \u03c4 (1) i . Furthermore, if FZ(\u03c0)(x) > \u03c4 (1) i , then we would have x(1)+ i = lim \u03c4\u2192\u03c4 (1) i + F \u2020 Z(\u03c0)(\u03c4) = lim \u03c4\u2192\u03c4 (1)+ i inf{x\u2032 \u2208 R | FZ(\u03c0)(x\u2032) \u2265 \u03c4} \u2264 inf{x\u2032 \u2208 R | FZ(\u03c0)(x\u2032) \u2265 FZ(\u03c0)(x)} \u2264 x, where the \ufb01rst inequality follows since FZ(\u03c0)(x) \u2265 \u03c4 for \u03c4 suf\ufb01ciently close to \u03c4 (1) i , and the second since x \u2208 {x\u2032 \u2208 R | FZ(\u03c0)(x\u2032) \u2265 FZ(\u03c0)(x)}. Since we have assumed x < x(1)+ i , we have a 7 contradiction, so FZ(\u03c0)(x) \u2264 \u03c4 (1) i . Thus, it follows that FZ(\u03c0)(x) = \u03c4 (1) i , as claimed. The third line follows since F \u22121 Z(\u03c0)({\u03c4 (1) i }) = [x(1) i , x(1)+ i ) or F \u22121 Z(\u03c0)({\u03c4 (1) i }) = [x(1) i , x(1)+ i ]. In particular, for any x \u2208 F \u22121 Z(\u03c0)({\u03c4 (1) i }), we have FZ(\u03c0)(x) = \u03c4 (1) i , so x \u2265 inf{x \u2208 R | FZ(\u03c0)(x) \u2265 \u03c4 (1) i } = x(1) i . Conversely, we have x(1)+ i = lim \u03c4\u2192\u03c4 (1) i + F \u2020 Z(\u03c0)(\u03c4) = lim \u03c4\u2192\u03c4 (1)+ i inf{x\u2032 \u2208 R | FZ(\u03c0)(x\u2032) \u2265 \u03c4} \u2265 x since FZ(\u03c0)(x\u2032) \u2264 \u03c4 (1) i < \u03c4 for all x\u2032 \u2264 x so the in\ufb01mum must be \u2265 x. These two arguments show that F \u22121 Z(\u03c0)({\u03c4 (1) i }) \u2286 [x(1) i , x(1)+ i ]. The fact that [x(1) i , x(1)+ i ) \u2286 F \u22121 Z(\u03c0)({\u03c4 (1) i }) follows by the same argument as for the second line. The claim follows. Finally, the fourth line follows since the sets F \u22121 Z(\u03c0)({\u03c4 (1) i }) are disjoint. Second term. For any \u03c4 \u2208 I(2), then F \u22121 Z(\u03c0) exists at \u03c4, and we have F \u2020 Z(\u03c0)(\u03c4) = F \u22121 Z(\u03c0)(\u03c4). Thus, by a substitution \u03c4 = FZ(\u03c0)(x), we have \ufffd I(2) G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) = \ufffd F \u22121 Z(\u03c0)(I(2)) G(FZ(\u03c0)(x)) \u00b7 dx. Third term. We can divide I(3) into a union of disjoint intervals I(3) = \ufffd\u221e i=1 I(3) i , where I(3) i = {\u03c4 \u2208 [0, 1] | F \u2020 Z(\u03c0)(\u03c4) = x(3) i } for some x(3) i \u2208 R; there are only be countably many such intervals (since each one contains a distinct rational number). Then, we have \ufffd I(3) G(\u03c4) \u00b7 dF \u2020 Z(\u03c0)(\u03c4) = 0 = \ufffd F \u22121 Z(\u03c0)(I(3)) G(FZ(\u03c0)(x)) \u00b7 dx, since F \u22121 Z(\u03c0)(I(3)) = {x(3) i }\u221e i=1 has measure zero according to the Lebesgue measure dx. Final proof. Finally, note that F \u22121 Z(\u03c0)(I(1)), F \u22121 Z(\u03c0)(I(2)), and F \u22121 Z(\u03c0)(I(3)) cover R and are disjoint except possibly on a set of measure zero, so \ufffd F \u22121 Z(\u03c0)(I(1)) G(FZ(\u03c0)(x)) \u00b7 dx + \ufffd F \u22121 Z(\u03c0)(I(2)) G(FZ(\u03c0)(x)) \u00b7 dx + \ufffd F \u22121 Z(\u03c0)(I(3)) G(FZ(\u03c0)(x)) \u00b7 dx = \ufffd R G(FZ(\u03c0)(x)) \u00b7 dx. The claim follows. Next, given \u03b4 \u2208 R>0, de\ufb01ne E to be the event where the following hold: \u2225 \u02dcP (k)(\u00b7 | s, a) \u2212 P(\u00b7 | s, a)\u22251 \u2264 \ufffd 2|S| N (k)(s, a) log \ufffd6|S| \u00b7 |A| \u00b7 K \u03b4 \ufffd =: \u03f5(k) P (s, a) (\u2200s \u2208 S, a \u2208 A) \u2225F \u02dc R(k)(s,a) \u2212 FR(s,a)\u2225\u221e \u2264 \ufffd 1 2N (k)(s, a) log \ufffd6|S| \u00b7 |A| \u00b7 K \u03b4 \ufffd =: \u03f5(k) R (s, a) (\u2200s \u2208 S, a \u2208 A) \u2225 \u02dcP (k)(\u00b7 | s, a) \u2212 P(\u00b7 | s, a)\u2225\u221e \u2264 \ufffd 1 2N (k)(s, a) log \ufffd6|S| \u00b7 |A| \u00b7 K \u03b4 \ufffd = \u03f5(k) R (s, a) (\u2200s \u2208 S, a \u2208 A). Lemma 5.2. We have P[E | {N (k)(s, a)}k\u2208[K],s\u2208S,a\u2208A] \u2265 1 \u2212 \u03b4. 8 Next, let \u02dcZ(k,\u03c0) and \u02c6Z(k,\u03c0) be the returns for policy \u03c0 for \u02dc M(k) and \u02c6 M(k), respectively, let \u03a6 = \u03a6M, \u02dc\u03a6(k) = \u03a6 \u02dc M(k), and \u02c6\u03a6(k) = \u03a6 \u02c6 M(k), and let \u03c0\u2217 = \u03c0\u2217 M, \u02dc\u03c0(k) = \u03c0\u2217 \u02dc M(k), and \u02c6\u03c0(k) = \u03c0\u2217 \u02c6 M(k). Now, we prove two key results: (i) \u02c6\u03a6(k) is close to \u03a6, and (ii) \u02c6\u03a6(k) is optimistic compared to \u03a6. To this end, we have the following key lemma; its proof depends critically on Lemma 5.1. Lemma 5.3. Consider MDPs M = (S, A, D, P, P, T) and M\u2032 = (S, A, D, P \u2032, P\u2032, T), such that \u2225P \u2032(\u00b7 | s, a) \u2212 P(\u00b7 | s, a)\u22251 \u2264 \u03f5P (s, a) and \u2225FR\u2032(s,a) \u2212 FR(s,a)\u2225\u221e \u2264 \u03f5R(s, a). Then, we have |\u03a6\u2032(\u03c0) \u2212 \u03a6(\u03c0)| \u2264 T \u00b7 LG \u00b7 B(\u03c0) (\u2200k \u2208 [K], \u03c0), where B(\u03c0) = E\u039e(\u03c0) T \ufffd T \ufffd t=1 \u03f5P (st, at) + \u03f5R(st, at) \ufffd . Our next lemma characterizes the connection between \u02dc\u03a6(k) and \u03a6. Lemma 5.4. On event E and conditioned on {N (k)(s, a)}k\u2208[K],s\u2208S,a\u2208A, we have |\u02dc\u03a6(k)(\u03c0) \u2212 \u03a6(\u03c0)| \u2264 T \u00b7 LG \u00b7 B(k)(\u03c0) (\u2200k \u2208 [K], \u03c0), where B(k)(\u03c0) = E\u039e(\u03c0) T \ufffd T \ufffd t=1 \u03f5(k) P (st, at) + \u03f5(k) R (st, at) \ufffd\ufffd\ufffd\ufffd {N (k)(s, a)}s\u2208S,a\u2208A \ufffd . Proof. The result follows since on event E and conditioned on {N (k)(s, a)}k\u2208[K],s\u2208S,a\u2208A, \u02dc M(k) and M satisfy the conditions of Lemma 5.3 for all k \u2208 [K]. Our next lemma characterizes the connection between \u02c6\u03a6(k) and \u02dc\u03a6(k). Lemma 5.5. For each k \u2208 [K] and any policy \u03c0, we have |\u02c6\u03a6(k)(\u03c0) \u2212 \u02dc\u03a6(\u03c0)| \u2264 T \u00b7 LG \u00b7 \ufffd |S| \u00b7 B(k)(\u03c0). Proof. The result follows since by de\ufb01nition of \u02c6 M(k), \u02c6 M(k) and \u02dc M(k) satisfy the condition of Lemma 5.3 with \u03f5P (s, a) = 2|S| \u00b7 \u03f5(k) R (s, a) \u2264 \ufffd |S| \u00b7 \u03f5(k) P (s, a) and \u03f5R(s, a) = \u03f5(k) R (s, a) for all k \u2208 [K]. Now, we prove the \ufb01rst key claim\u2014i.e., \u02c6\u03a6(k) is close to \u03a6. Lemma 5.6. On event E, for all k \u2208 [K] and any policy \u03c0, we have |\u02c6\u03a6(k)(\u03c0) \u2212 \u03a6(\u03c0)| \u2264 2T \u00b7 LG \u00b7 \ufffd |S| \u00b7 B(k)(\u03c0). Proof. Note that |\u02c6\u03a6(k)(\u03c0) \u2212 \u03a6(\u03c0)| \u2264 |\u02c6\u03a6(k)(\u03c0) \u2212 \u02dc\u03a6(k)(\u03c0)| + |\u02dc\u03a6(k)(\u03c0) \u2212 \u03a6(\u03c0)| \u2264 2T \u00b7 LG \u00b7 \ufffd |S| \u00b7 B(k)(\u03c0), where the second inequality follows by Lemmas 5.4 & 5.5. Now, we prove the second key claim\u2014i.e., \u02c6\u03a6(k) is optimistic compared to \u03a6. Lemma 5.7. On event E, we have \u02c6\u03a6(k)(\u03c0) \u2265 \u03a6(\u03c0) for all k \u2208 [K] and all policies \u03c0. With these two key claims, the proof of Theorem 4.1 follows by a standard upper con\ufb01dence bound argument; we give the proof in Appendix C.4. 9 ",
    "Experiments": "Experiments We consider a classic frozen lake problem with a \ufb01nite horizon. The agent moves to a block next to its current state at each timestep t and has a slipping probability of 0.1 in its moving direction if the next state is an ice block. The objective is to maximize the cumulative reward without falling into holes. The agent needs to choose among paths which correspond to different levels of risk and rewards. In other words, the agent should account for the tradeoff between the cumulative reward and risk of slipping into holes. We use a map with four paths of the same lengths that have different rewards at the end and different levels of risk of falling into holes. We consider \u03b1 \u2208 {0.40, 0.33, 0.25, 0.01}, which correspond to optimal policies of choosing paths with best possible returns of {6, 4, 2, 1} and success probabilities of {0.729, 0.81, 0.9, 1}, respectively (failure corresponds to zero return). Figure 1 (left) shows the comparison in cumulative regret between our algorithm, UCBVI (which maximizes expected returns, not our risk-sensitive objective), and the an algorithm that optimizes our risk-sensitive objective but explores in a greedy way (i.e., use the best policy for the current estimated MDP without any optimism), for \u03b1 = 0.33. The regret is measured in terms of the CVaR objective with respect to the optimal policy for the same CVaR objective. While UCBVI outperforms greedy, neither of them converge; in contrast, our algorithm converges within 40 episodes. Figure 1 (right) compares the regret between our algorithm under different values of \u03b1 using the CVaR objective. Note that smaller values of \u03b1 tend to lead our algorithm to converge more slowly; this result matches our theory since smaller \u03b1 corresponds to larger LG. Intuitively, more samples are needed to get a good estimate of the objective as \u03b1 becomes small since the CVaR objective is the average return over a tiny fraction of samples, causing high variance in our estimate of the objective. 7 ",
    "Conclusion": "Conclusion We have proposed a novel regret bound for risk sensitive reinforcement learning that applies to a broad class of objective functions, including the popular conditional value-at-risk (CVaR) objective. Our results recover the usual \u221a K dependence on the number of episodes, and also highlights dependence on the Lipschitz constant LG of the integral of the weighting function G used to de\ufb01ne the objective. Future work includes extending these ideas to the setting of function approximation and understanding whether alternative exploration strategies such as Thompson sampling are applicable. Acknowledgments and Disclosure of Funding This work is funded in part by NSF Award CCF-1910769, NSF Award CCF-1917852, and ARO Award W911NF-20-1-0080. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. 10 ",
    "References": "References [1] R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions. Journal of banking & \ufb01nance, 26(7):1443\u20131471, 2002. [2] Yichuan Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov. Worst cases policy gradients. arXiv preprint arXiv:1911.03618, 2019. [3] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070\u20136120, 2017. [4] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative of\ufb02ine distributional reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. [5] Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. In Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence, 2015. [6] Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps. Advances in neural information processing systems, 27, 2014. [7] Yingjie Fei, Zhuoran Yang, Yudong Chen, and Zhaoran Wang. Exponential bellman equation and improved regret bounds for risk-sensitive reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. [8] Yingjie Fei, Zhuoran Yang, and Zhaoran Wang. Risk-sensitive reinforcement learning with function approximation: A debiasing approach. In International Conference on Machine Learning, pages 3198\u20133207. PMLR, 2021. [9] Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pages 4436\u20134443, 2020. [10] Will Dabney, Mark Rowland, Marc Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 32, 2018. [11] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21, 2008. [12] Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017. [13] Nicole B\u00e4uerle and Jonathan Ott. Markov decision processes with average-value-at-risk criteria. Mathematical Methods of Operations Research, 74(3):361\u2013379, 2011. [14] Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449\u2013458. PMLR, 2017. [15] Will Dabney, Georg Ostrovski, David Silver, and R\u00e9mi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pages 1096\u20131105. PMLR, 2018. [16] Shaun S Wang. A class of distortion operators for pricing \ufb01nancial and insurance risks. Journal of risk and insurance, pages 15\u201336, 2000. [17] Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5(4):297\u2013323, 1992. [18] Paul Embrechts and Marius Hofert. A note on generalized inverses. Mathematical Methods of Operations Research, 77(3):423\u2013432, 2013. [19] Elias M Stein and Rami Shakarchi. Real analysis. In Real Analysis. Princeton University Press, 2009. 11 A Proof of Theorem 3.1 In this section, we prove Theorem 3.1, which says that it suf\ufb01ces to the augmented state space (y, s) rather than the whole history \u03be. First, we have the following lemma. Lemma A.1. For any y \u2208 R and s \u2208 S, we have E\u039e(\u03c0) t \ufffd \u03c0t(a | \u039e(\u03c0) t ) \u00b7 1 \ufffd \u039e(\u03c0) t \u2208 Zt(y, s) \ufffd\ufffd = E\u039e(\u03c0) t \ufffd \u02dc\u03c0t(a | \u039e(\u03c0) t ) \u00b7 1 \ufffd \u039e(\u03c0) t \u2208 Zt(y, s) \ufffd\ufffd . Proof. Note that E\u039e(\u03c0) t \ufffd \u02dc\u03c0t(a | \u039e(\u03c0) t ) \ufffd\ufffd\ufffd \u039e(\u03c0) t \u2208 Zt(y, s) \ufffd = E\u039e(\u03c0) t \ufffd E\u02dc\u039e(\u03c0) t \ufffd \u03c0t(at | \u02dc\u039e(\u03c0) t ) \ufffd\ufffd\ufffd \u02dc\u039e(\u03c0) t \u2208 Zt(J(\u039e(\u03c0) t ), st) \ufffd \ufffd\ufffd\ufffd \u039e(\u03c0) t \u2208 Zt(y, s) \ufffd = E\u039e(\u03c0) t \ufffd E\u02dc\u039e(\u03c0) t \ufffd \u03c0t(at | \u02dc\u039e(\u03c0) t ) \ufffd\ufffd\ufffd \u02dc\u039e(\u03c0) t \u2208 Zt(y, s) \ufffd \ufffd\ufffd\ufffd \u039e(\u03c0) t \u2208 Zt(y, s) \ufffd = E\u02dc\u039e(\u03c0) t \ufffd \u03c0t(at | \u02dc\u039e(\u03c0) t ) \ufffd\ufffd\ufffd \u039e(\u03c0) t \u2208 Zt(y, s) \ufffd . The claim follows by replacing \u02dc\u039e(\u03c0) t with \u039e(\u03c0) t and multiplying by P\u039e(\u03c0) t [\u039e(\u03c0) t \u2208 Zt(y, s)]. Next, let D(\u03c0) t (y, s) = P\u039e(\u03c0) t \ufffd J(\u039e(\u03c0) t ) \u2264 y \u2227 S(\u039e(\u03c0) t ) = s \ufffd be the probability of a history achieving current cumulative return at most y and ending in state s. Lemma A.2. We have D(\u03c0) t = D(\u02dc\u03c0) t . Proof. We prove by induction. The base case t = 1 follows trivially. For the inductive case, note that D(\u03c0) t+1(y, s\u2032\u2032) = \ufffd 1(\u03be\u2032 \u2208 Zt+1(y, s\u2032\u2032)) \u00b7 dP\u039e(\u03c0) t+1(\u03be\u2032) = \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S 1(\u03be \u25e6 (a, r, s\u2032) \u2208 Zt+1(y, s\u2032\u2032)) \u00b7 P(s\u2032 | S(\u03be), a) \u00b7 dPR(s,a)(r) \u00b7 \u03c0(a | \u03be) \u00b7 dP\u039e(\u03c0) t (\u03be) = \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S 1(J(\u03be) + r \u2264 y) \u00b7 1(s\u2032 = s\u2032\u2032) \u00b7 P(s\u2032 | S(\u03be), a) \u00b7 dPR(s,a)(r) \u00b7 \u03c0(a | \u03be) \u00b7 dP\u039e(\u03c0) t (\u03be) = \ufffd \ufffd a\u2208A 1(J(\u03be) + r \u2264 y) \u00b7 P(s\u2032\u2032 | S(\u03be), a) \u00b7 dPR(s,a)(r) \u00b7 \u03c0(a | \u03be) \u00b7 dP\u039e(\u03c0) t (\u03be), where the \ufb01rst line follows by de\ufb01nition of D(\u03c0) t+1, the second by the inductive formula for P\u039e(\u03c0) t+1, the third since J(\u03be \u25e6 (a, r, s\u2032)) = J(\u03be) + r, and the fourth by summing over s\u2032. Continuing, we have D(\u03c0) t+1(y, s\u2032\u2032) = \ufffd s\u2208S \ufffd a\u2208A \ufffd \u03c0(a | \u03be) \u00b7 1(J(\u03be) + r \u2264 y) \u00b7 1(S(\u03be) = s) \u00b7 dP\u039e(\u03c0) t (\u03be) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd s\u2208S \ufffd a\u2208A \ufffd \u03c0(a | \u03be) \u00b7 1(\u03be \u2208 Zt(y \u2212 r, s)) \u00b7 dP\u039e(\u03c0) t (\u03be) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd s\u2208S \ufffd a\u2208A \ufffd \u02dc\u03c0(a | \u03be) \u00b7 1(\u03be \u2208 Zt(y \u2212 r, s)) \u00b7 dP\u039e(\u03c0) t (\u03be) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r), 12 where the \ufb01rst line follows by introducing 1(S(\u03be) = s) and rearranging, the second by de\ufb01nition of Zt, and the third by Lemma A.1. Continuing, we have D(\u03c0) t+1(y, s\u2032\u2032) = \ufffd s\u2208S \ufffd a\u2208A \ufffd 1(\u03be \u2208 Zt(y \u2212 r, s)) \u00b7 dP\u039e(\u03c0) t (\u03be) \u00b7 \u02dc\u03c0(a | y \u2212 r, s) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd s\u2208S \ufffd a\u2208A \ufffd D(\u03c0) t (y \u2212 r, s) \u00b7 \u02dc\u03c0(a | y \u2212 r, s) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd s\u2208S \ufffd a\u2208A \ufffd D(\u02dc\u03c0) t (y \u2212 r, s) \u00b7 \u02dc\u03c0(a | y \u2212 r, s) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd s\u2208S \ufffd a\u2208A \ufffd 1(\u03be \u2208 Zt(y \u2212 r, s)) \u00b7 dP\u039e(\u02dc\u03c0) t (\u03be) \u00b7 \u02dc\u03c0(a | y \u2212 r, s) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r), where the \ufb01rst line follows since \u02dc\u03c0 is independent of \u03be and by rearranging, the second by de\ufb01nition of D(\u03c0) t , the third by induction, and the fourth by de\ufb01nition of D(\u02dc\u03c0) t . Continuing, we have D(\u03c0) t+1(y, s\u2032\u2032) = \ufffd s\u2208S \ufffd a\u2208A \ufffd \u02dc\u03c0(a | \u03be) \u00b7 1(\u03be \u2208 Zt(y \u2212 r, s)) \u00b7 dP\u039e(\u02dc\u03c0) t (\u03be) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd s\u2208S \ufffd a\u2208A \ufffd \u02dc\u03c0(a | \u03be) \u00b7 1(J(\u03be) + r \u2264 y) \u00b7 1(S(\u03be) = s) \u00b7 dP\u039e(\u02dc\u03c0) t (\u03be) \u00b7 P(s\u2032\u2032 | s, a) \u00b7 dPR(s,a)(r) = \ufffd a\u2208A \ufffd 1(J(\u03be) + r \u2264 y) \u00b7 P(s\u2032\u2032 | S(\u03be), a) \u00b7 dPR(s,a)(r) \u00b7 \u02dc\u03c0(a | \u03be) \u00b7 dP\u039e(\u02dc\u03c0) t (\u03be) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd 1(J(\u03be) + r \u2264 y) \u00b7 1(s\u2032 = s\u2032\u2032) \u00b7 P(s\u2032 | S(\u03be), a) \u00b7 dPR(s,a)(r) \u00b7 \u02dc\u03c0(a | \u03be) \u00b7 dP\u039e(\u02dc\u03c0) t (\u03be) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd 1(\u03be \u25e6 (a, r, s\u2032) \u2208 Zt+1(y, s\u2032\u2032)) \u00b7 P(s\u2032 | S(\u03be), a) \u00b7 dPR(s,a)(r) \u00b7 \u02dc\u03c0(a | \u03be) \u00b7 dP\u039e(\u02dc\u03c0) t (\u03be) = \ufffd 1(\u03be\u2032 \u2208 Zt+1(y, s\u2032\u2032)) \u00b7 dP\u039e(\u03c0) t+1(\u03be\u2032) = D(\u02dc\u03c0) t+1(y, s\u2032\u2032), where the \ufb01rst line follows by de\ufb01nition of \u02dc\u03c0, the second by de\ufb01nition of Zt, the third by summing over s and rearranging, the fourth by introducing 1(s\u2032 = s\u2032\u2032), the \ufb01fth by de\ufb01nition of Zt+1, the sixth by the inductive formula for P\u039e(\u03c0) t+1, and the seventh by the de\ufb01nition of D(\u02dc\u03c0) t+1. The claim follows. Now, we prove Theorem 3.1. By Lemma A.2, we have FZ(\u03c0)(x) = \ufffd 1(J(\u03be) \u2264 x) \u00b7 dP\u039e(\u03c0) T (\u03be) = \ufffd s\u2208S \ufffd 1(J(\u03be) \u2264 x) \u00b7 1(S(\u03be) = s) \u00b7 dP\u039e(\u03c0) T (\u03be) = \ufffd s\u2208S \ufffd 1(\u03be \u2208 ZT (x, s)) \u00b7 dP\u039e(\u03c0) T (\u03be) = \ufffd s\u2208S \ufffd 1(\u03be \u2208 ZT (x, s)) \u00b7 dP\u039e(\u02dc\u03c0) T (\u03be) = \ufffd 1(J(\u03be) \u2264 x) \u00b7 dP\u039e(\u02dc\u03c0) T (\u03be) = FZ(\u02dc\u03c0)(x). Theorem 3.1 follows straightforwardly from this result. 13 B Proof of Theorem 3.2 We construct a sequence of MDPs M0, M1, ..., MT , such that M0 = \u02dc M and MT = \u02c6 M, and where we can bound the incremental errors \u03a6 \u02dc M(\u03c0\u2217 M\u03c4 ) \u2212 \u03a6 \u02c6 M(\u03c0\u2217 M\u03c4\u22121), noting a policy for one of the MDPs can be used in all the other MDPs. For each \u03c4 \u2208 [T], the MDP M\u03c4 discretizes the reward assigned on the tth step of M\u03c4\u22121\u2014more precisely, it discretizes the transitions since the rewards are only assigned on the last step based on the cumulative reward recorded in the second component of the state space. Formally, M\u03c4 is identical to \u02dc M, except it uses the (time-varying) transition probability measure \u02c6P (\u03c4) de\ufb01ned by \u02c6P (\u03c4) t ((s\u2032, y\u2032) | (s, y), a) = \ufffdP(s\u2032 | s, a) \u00b7 (PR(s,a) \u25e6 \u03c6\u22121)(y\u2032 \u2212 y) if t \u2264 \u03c4 P(s\u2032 | s, a) \u00b7 PR(s,a)(y\u2032 \u2212 y) otherwise. Then, M\u03c4 is identical to M\u03c4\u22121 except PR(s,a) is replaced with PR(s,a) \u25e6 \u03c6\u22121 on step \u03c4. We prove three lemmas showing a lower bound on the value of a policy \u03c0 for M\u03c4 when adapted to M\u03c4\u22121. Lemma B.1. Given \u03c4 \u2208 [T], let M = M\u03c4\u22121 and \u02c6 M = M\u03c4 (so compared to M, \u02c6 M replaces PR(s,a) with PR(s,a) \u25e6 \u03c6\u22121 on step \u03c4 in its transitions). Given any policy \u02c6\u03c0 for \u02c6 M, de\ufb01ne the policy \u03c0t(a | s, y, \u03b1) = \u02c6\u03c0t(a | s, y + \u03b1) for M, where we initialize the (extra) policy internal state \u03b11 = 0, and we update \u03b1\u03c4+1 = \u03c6(r\u03c4)\u2212r\u03c4 on step \u03c4 and \u03b1t+1 = \u03b1t otherwise. Then, for all x, y, \u03b1 \u2208 R, for t > \u03c4, we have FZ(\u03c0) t (s,y,\u03b1)(x) = F \u02c6 Z(\u02c6\u03c0) t (s,y+\u03b1)(x), and for t \u2264 \u03c4, we have FZ(\u03c0) t (s,y,0)(x) \u2264 F \u02c6 Z(\u02c6\u03c0) t (s,y)(x + \u03b7), where Z(\u03c0) t (resp., \u02c6Z(\u02c6\u03c0) t ) is the return of M (resp., \u02c6 M) from step t for policy \u03c0 (resp., \u02c6\u03c0). Proof. We prove by backwards induction on t. The base case t = T follows by de\ufb01nition (and since the reward measure does not change from M to \u02c6 M). For t > \u03c4, we have FZ(\u03c0) t (s,y,\u03b1)(x) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y, \u03b1) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r,\u03b1)(x \u2212 r) \u00b7 dPR(s,a)(r) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y + \u03b1) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03b1+r)(x \u2212 r) \u00b7 dPR(s,a)(r) = F \u02c6 Z(\u02c6\u03c0) t (s,y+\u03b1)(x), where the second line follows by induction and by the de\ufb01nition of \u03c0. Next, for t = \u03c4, we have FZ(\u03c0) t (s,y,0)(x) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y, 0) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r,\u03c6(r)\u2212r)(x \u2212 r) \u00b7 dPR(s,a)(r) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c6(r))(x \u2212 r) \u00b7 dPR(s,a)(r) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c6(r))(x \u2212 \u03c6(r) + \u03c6(r) \u2212 r) \u00b7 dPR(s,a)(r) \u2264 \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c6(r))(x \u2212 \u03c6(r) + \u03b7) \u00b7 dPR(s,a)(r) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c1)(x \u2212 \u03c1 + \u03b7) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) = F \u02c6 Z(\u02c6\u03c0) t (s,y)(x + \u03b7), 14 where the \ufb01rst line uses the update from \u03b1t = 0 to \u03b1t+1 = r \u2212 \u03c6(r) on this step, the second line follows by induction and by the de\ufb01nition of \u03c0, the fourth line follows by monotonicity of F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+r), and the \ufb01fth line follows by a change of variables \u03c1 = \u03c6(r). For t < \u03c4, we have FZ(\u03c0) t (s,y,0)(x) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y, 0) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r,0)(x \u2212 r) \u00b7 dPR(s,a)(r) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r,0)(x \u2212 r) \u00b7 dPR(s,a)(r) \u2264 \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+r,0)(x \u2212 r + \u03b7) \u00b7 dPR(s,a)(r) = F \u02c6 Z(\u02c6\u03c0) t (s\u2032,0)(x + \u03b7), where the second line follows by the de\ufb01nition of \u03c0, and the third line follows by induction. The claim follows. Lemma B.2. For any monotonically increasing F, we have F \u2020(F(x)) \u2264 x, and F(F \u2020(\u03c4)) \u2265 \u03c4. Proof. See Proposition 1 in [18]. Lemma B.3. Let F, G : R \u2192 R be monotonically increasing. If F(x) \u2264 G(x + \u03b7) for all x \u2208 R, then we have F \u2020(\u03c4) \u2265 G\u2020(\u03c4) \u2212 \u03b7 for all \u03c4 \u2208 R. Proof. By assumption, G(x) \u2265 F(x \u2212 \u03b7). Substituting x = F \u2020(\u03c4) + \u03b7 into this formula, we obtain G(F \u2020(\u03c4) + \u03b7) \u2265 F(F \u2020(\u03c4)) \u2265 \u03c4, where the second inequality follows by Lemma B.2. Also by Lemma B.2, since G is monotonically increasing, so is G\u2020, so we can apply G\u2020 to each side of the inequality to obtain G\u2020(\u03c4) \u2264 G\u2020(G(F \u2020(\u03c4) + \u03b7)) \u2264 F \u2020(\u03c4) + \u03b7, where the second inequality follows by Lemma B.2. The claim follows. Lemma B.4. Consider the same setup as in Lemma B.1. Let \u02c6\u03c0 be a policy for \u02c6 M, and let \u03c0 be the policy de\ufb01ned in Lemma B.1 that adapts \u02c6\u03c0 to M. Then, we have \u03a6(\u03c0) \u2265 \u02c6\u03a6(\u02c6\u03c0) \u2212 \u03b7, where \u03a6 is the objective for M and \u02c6\u03a6 is the objective for \u02c6 M. Proof. Let \u02c6Z = \u02c6Z(\u02c6\u03c0) 1 (s1, 0) and Z = Z(\u03c0) 1 (s1, 0, 0). Applying Lemma B.3 to the inequality in Lemma B.1, we have F \u2020 Z(\u03c4) \u2265 F \u2020 \u02c6 Z(\u03c4) \u2212 \u03b7. Integrating this inequality, we have \u03a6(\u03c0) = \ufffd F \u2020 Z(\u03c4) \u00b7 dG(\u03c4) \u2265 \ufffd \ufffd F \u2020 \u02c6 Z(\u03c4) \u2212 \u03b7 \ufffd \u00b7 dG(\u03c4) = \u02c6\u03a6(\u02c6\u03c0) \u2212 \u03b7, as claimed. Lemma B.5. Consider the same setup as in Lemma B.1. Given any policy \u03c0 for M, de\ufb01ne the policy \u02c6\u03c0t(a | s, y, \u03b1) = \u03c0t(a | s, y + \u03b1) for \u02c6 M, where we initialize \u03b11 = 0, and we update \u03b1\u03c4+1 = r on step \u03c4, where r is a random variable with probability measure PR(s,a)(r | \u03c6(r) = \u03c1), and \u03b1t+1 = \u03b1t otherwise. Then, for all x, y, \u03b1 \u2208 R, for t > \u03c4, we have F \u02c6 Z(\u02c6\u03c0) t (s,y,\u03b1)(x) = FZ(\u03c0) t (s,y+\u03b1)(x), and for t \u2264 \u03c4, we have F \u02c6 Z(\u02c6\u03c0) t (s,y,0)(x) \u2264 FZ(\u03c0) t (s,y)(x). 15 Proof. We prove by backwards induction on T. The base case t = T follows by de\ufb01nition. For t > \u03c4, we have F \u02c6 Z(\u02c6\u03c0) t (s,y,\u03b1)(x) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y, \u03b1) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u03c0) t+1(s\u2032,y+\u03c1,\u03b1)(x \u2212 \u03c1) \u00b7 dPR(s,a)(\u03c1) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y + \u03b1) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+\u03b1+\u03c1)(x \u2212 \u03c1) \u00b7 dPR(s,a)(\u03c1) = FZ(\u03c0) t (s,y+\u03b1)(x), where the second line follows by induction and by the de\ufb01nition of \u03c0. Next, for t = \u03c4, we have F \u02c6 Z(\u02c6\u03c0) t (s,c,0)(x) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y, 0) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c1,r\u2212\u03c1)(x \u2212 \u03c1) \u00b7 dPR(s,a)(r | \u03c6(r) = \u03c1) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 \u03c1) \u00b7 dPR(s,a)(r | \u03c6(r) = \u03c1) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) \u2264 \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dPR(s,a)(r | \u03c6(r) = \u03c1) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dPR(s,a)(r) = FZ(\u03c0) t (s,y)(x), where the second line uses the update from \u03b1t = 0 to \u03b1t+1 = r \u2212 \u03c1 on this step, the third line follows by induction and by the de\ufb01nition of \u02c6\u03c0, the fourth line follows by monotonicity of FZ(\u03c0) t+1(s\u2032,y+r), and the \ufb01fth line follows by the de\ufb01nition of conditional probability\u2014in particular, \ufffd FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dPR(s,a)(r | \u03c6(r) = \u03c1) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) = \ufffd \ufffd FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 1(r \u2208 \u03c6\u22121(\u03c1)) \u00b7 dPR(s,a)(r) \ufffd 1(r\u2032 \u2208 \u03c6\u22121(\u03c1)) \u00b7 dPR(s,a)(r\u2032) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) = \ufffd FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \ufffd \u00b71(r \u2208 \u03c6\u22121(\u03c1)) \ufffd 1(r\u2032 \u2208 \u03c6\u22121(\u03c1)) \u00b7 dPR(s,a)(r\u2032) \u00b7 dPR(s,a) \u25e6 \u03c6\u22121(\u03c1) \u00b7 dPR(s,a)(r) = \ufffd FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u221e \ufffd i=1 \u00b71(r \u2208 Bi) P(R(s, a) \u2208 Bi) \u00b7 P(R(s, a) \u2208 Bi) \u00b7 dPR(s,a)(r) = \ufffd FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dRR(s,a)(r), where in the third line, Bi = (\u03b7 \u00b7 (i \u2212 1), \u03b7 \u00b7 i]. For t < \u03c4, we have F \u02c6 Z(\u02c6\u03c0) t (s,y,0)(x) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u02c6\u03c0t(a | s, y, 0) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c1,0)(x \u2212 \u03c1) \u00b7 dPR(s,a)(\u03c1) = \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 F \u02c6 Z(\u02c6\u03c0) t+1(s\u2032,y+\u03c1,0)(x \u2212 \u03c1) \u00b7 dPR(s,a)(\u03c1) \u2264 \ufffd a\u2208A \ufffd s\u2032\u2208S \ufffd \u03c0t(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+\u03c1,0)(x \u2212 \u03c1) \u00b7 dPR(s,a)(\u03c1) = FZ(\u03c0) t (s\u2032,y,0)(x), where the second line follows by the de\ufb01nition of \u03c0, and the third line follows by induction. The claim follows. 16 Next, we prove two lemmas showing a converse\u2014namely, a lower bound on the value of a policy \u03c0 for M\u03c4\u22121 when adapted to M\u03c4. Lemma B.6. Consider the same setup as in Lemma B.1. Letting \u03c0 be a policy for M, and \u02c6\u03c0 be the policy de\ufb01ned in Lemma B.5 that adapts \u03c0 to \u02c6 M. Then, we have \u02c6\u03a6(\u02c6\u03c0) \u2265 \u03a6(\u03c0), where \u02c6\u03a6 is the objective for \u02c6 M and \u03a6 is the objective for M. Proof. Let Z = Z(\u03c0) 1 (s1, 0) and \u02c6Z = Z(\u02c6\u03c0) 1 (s1, 0, 0). Applying Lemma B.3 to the inequality in Lemma B.5, we have F \u2020 \u02c6 Z(\u03c4) \u2265 F \u2020 Z(\u03c4). Integrating this inequality, we have \u02c6\u03a6(\u02c6\u03c0) = \ufffd F \u2020 \u02c6 Z(\u03c4) \u00b7 dG(\u03c4) \u2265 \ufffd F \u2020 Z(\u03c4) \u00b7 dG(\u03c4) = \u03a6(\u03c0), as claimed. Finally, we prove Theorem 3.2. Let \u03c0T T be the optimal policy for MT , and let \u03c0T \u03c4 be the policy de\ufb01ned in Lemma B.4 adapting \u03c0T \u03c4 from M\u03c4 to M\u03c4\u22121 for each \u03c4 \u2208 [T]. \u03a60(\u03c0T 0 ) \u2265 \u03a61(\u03c0T 1 ) \u2212 \u03b7 \u2265 \u03a62(\u03c0T 2 ) \u2265 ... \u2265 \u03a6T (\u03c0T T ) \u2212 T \u00b7 \u03b7, where each inequality follows by Lemma B.4. Similarly, let \u03c00 0 be the optimal policy for M0, and let \u03c00 \u03c4 be the policy de\ufb01ned in Lemma B.6 adapting \u03c00 \u03c4\u22121 from M\u03c4\u22121 to M\u03c4. Then, we have \u03a6T (\u03c00 T ) \u2265 \u03a6T \u22121(\u03c00 T \u22121) \u2265 ... \u2265 \u03a60(\u03c00 0), where each inequality follows by Lemma B.6. Furthermore, by optimality of \u03c0T T for \u03a6T , we also have \u03a6T (\u03c0T T ) \u2265 \u03a6T (\u03c00 T ); together, these three inequalities imply \u03a60(\u03c0T 0 ) \u2265 \u03a60(\u03c00 0) \u2212 T \u00b7 \u03b7. Finally, note that \u03c00 0 = \u03c0\u2217 \u02dc M is the optimal policy for \u02dc M = M0, and \u03c00 T = \u03c0 \u02c6 M is \u03c00 0 adapted to \u02c6 M; also, \u03a60 = \u03a6 \u02dc M is the objective for \u02dc M. Thus, we have \u03a6 \u02dc M(\u03c0 \u02c6 M) \u2265 \u03a6 \u02dc M(\u03c0\u2217 \u02dc M) \u2212 T \u00b7 \u03b7. By Theorem 3.1, the optimal policy for \u02dc M equals the optimal history-dependent policy for the original MDP M, so the claim follows. C Proof of Lemmas for Section 5 C.1 Proof of Lemma 5.2 Proof. First, by the Dvoretzky\u2013Kiefer\u2013Wolfowitz (DKW) inequality and a union bound, for each k \u2208 [K], conditioned on {N (k)(s, a)}s\u2208S,a\u2208A, with probability at least 1 \u2212 \u03b4/(3K), we have \u2225F \u02dc R(k)(s,a) \u2212 FR(s,a)\u2225\u221e \u2264 \u03f5(k) R (s, a) (\u2200s \u2208 S, a \u2208 A). Similarly, by Hoeffding\u2019s inequality, an \u21131 concentration bound for multinomial distribution, and a union bound, for each k \u2208 [K], conditioned on {N (k)(s, a)}s\u2208S,a\u2208A, we have \u2225 \u02dcP (k)(\u00b7 | s, a) \u2212 P(\u00b7 | s, a)\u22251 \u2264 \u03f5(k) P (s, a) (\u2200s \u2208 S, a \u2208 A) \u2225 \u02dcP (k)(\u00b7 | s, a) \u2212 P(\u00b7 | s, a)\u2225\u221e \u2264 \u03f5(k) R (s, a) (\u2200s \u2208 S, a \u2208 A). each holding with probability at least 1 \u2212 \u03b4/(3K), respectively. Thus, both of these bounds hold for all k \u2208 [K] with probability at least 1 \u2212 \u03b4. The claim follows. 17 C.2 Proof of Lemma 5.3 Proof. First, we prove that for all policies \u03c0, we have \u2225FZ\u2032(\u03c0) \u2212 FZ(\u03c0)\u2225\u221e \u2264 B(\u03c0). To this end, let G(r) = FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r); note that G(\u2212\u221e) = 1 and G(\u221e) = 0. Then, by integration by parts, we have FZ(\u03c0) t (s,y)(x) = \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 G(r) \u00b7 dPR(s,a)(r) = \u2212 \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FR(s,a)(r) \u00b7 dG(r), and similarly for FZ \u2032(\u03c0) t (s,y)(x). Next, note that sup x\u2208R |FZ \u2032(\u03c0) t (s,y)(x) \u2212 FZ(\u03c0) t (s,y)(x)| = sup x\u2208R \ufffd\ufffd\ufffd\ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) (P \u2032(s\u2032 | s, a) \u2212 P(s\u2032 | s, a)) \ufffd FZ \u2032(\u03c0) t+1 (s\u2032,y+r)(x \u2212 r)dP\u2032 R\u2032(s,a)(r) + \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y)P(s\u2032 | s, a) \ufffd \ufffd FZ \u2032(\u03c0) t+1 (s\u2032,y+r)(x \u2212 r) \u2212 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \ufffd dP\u2032 R\u2032(s,a)(r) \ufffd\ufffd\ufffd\ufffd \u2212 \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y)P(s\u2032 | s, a) \ufffd \ufffd FR\u2032(s,a)(r) \u2212 FR(s,a)(r) \ufffd dFZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u2264 sup x\u2208R \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 |P \u2032(s\u2032 | s, a) \u2212 P(s\u2032 | s, a)| + \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \ufffd sup x\u2032\u2208R |FZ \u2032(\u03c0) t+1 (s\u2032,y+r)(x\u2032) \u2212 FZ(\u03c0) t+1(s\u2032,y+r)(x\u2032)| \u00b7 dP\u2032 R\u2032(s,a)(r) + \ufffd a\u2208A \u03c0(a | s, y) \u00b7 sup r\u2032\u2208R |FR\u2032(s,a)(r\u2032) \u2212 FR(s,a)(r\u2032)| \u2264 E \ufffd \u03f5P (s, a) + \u03f5R(s, a) + sup x\u2032\u2208R |FZ \u2032(\u03c0) t+1 (s\u2032,y+r)(x\u2032) \u2212 FZ(\u03c0) t+1(s\u2032,y+r)(x\u2032)| \ufffd . Thus, we have \u03f5(\u03c0) t := E \ufffd sup x\u2208R |FZ \u2032(\u03c0) t (s,y)(x) \u2212 FZ(\u03c0) t (s,y)(x)| \ufffd = E \ufffd \u03f5(k,P ) s,a + \u03f5(k,R) s,a + sup x\u2032\u2208R |FZ \u2032(\u03c0) t+1 (s\u2032,y+r)(x\u2032) \u2212 FZ(\u03c0) t+1(s\u2032,y+r)(x\u2032)| \ufffd \u2264 E [\u03f5P (s, a) + \u03f5R(s, a)] + \u03f5(k,\u03c0) t+1 = E \ufffd T \ufffd \u03c4=t \u03f5P (s\u03c4, a\u03c4) + \u03f5R(s\u03c4, a\u03c4) \ufffd , where the last step follows by induction. Finally, we have |\u03a6\u2032(\u03c0) \u2212 \u03a6(\u03c0)| = \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd T 0 (G(FZ\u2032(\u03c0)(x)) \u2212 G(FZ(\u03c0)(x))) \u00b7 dx \ufffd\ufffd\ufffd\ufffd\ufffd \u2264 LG \ufffd T 0 |FZ\u2032(\u03c0)(x) \u2212 FZ(\u03c0)(x)| \u00b7 dx \u2264 T \u00b7 LG \u00b7 \u03f5(\u03c0) 1 , where the \ufb01rst line follows by Lemma 5.1. The claim follows since \u03f5(\u03c0) 1 equals the desired bound. 18 C.3 Proof of Lemma 5.7 Proof. First, we prove that F \u02c6 Z(k,\u03c0) t (s,y)(x) \u2264 FZ(\u03c0) t (s,y)(x). The case s = s\u221e is straightforward, since its transitions and rewards are equal in M and \u02c6 M, and it only transitions to itself. For s \u0338= s\u221e, we prove by induction on t. The base case t = T follows by de\ufb01nition. Then, we have F \u02c6 Z(k,\u03c0) t (s,y)(x) = \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \u02c6P (k)(s\u2032 | s, a) \u00b7 F \u02c6 Z(k,\u03c0) t+1 (s\u2032,y+r)(x \u2212 r) \u00b7 d\u02c6P \u02c6 R(k)(s,a)(r) \u2264 \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \u02c6P (k)(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 d\u02c6P \u02c6 R(k)(s,a)(r) = \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \u02c6P (k)(s\u2032 | s, a) \u00b7 F \u02c6 R(k)(s,a)(x\u2032 \u2212 x) \u00b7 dFZ(\u03c0) t+1(s\u2032,y+r)(x\u2032) \u2264 \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \u02c6P (k)(s\u2032 | s, a) \u00b7 FR(s,a)(x\u2032 \u2212 x) \u00b7 dFZ(\u03c0) t+1(s\u2032,y+r)(x\u2032) = \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \u02c6P (k)(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dPR(s,a)(r), (2) where the second line follows by induction, the third by integration by parts and substituting x\u2032 = x\u2212r, the fourth since FR(s,a)(r) = 1 = F \u02c6 R(k)(s,a)(r) for r \u2265 1, and for r < 1, on event E, we have FR(s,a)(r) \u2265 max \ufffd F \u02dc R(k)(s,a)(r) \u2212 \u03f5(k) R (s, a), 0 \ufffd = F \u02c6 R(k)(s,a)(r), and the \ufb01fth by integration by parts and substituting r = x\u2032 \u2212 x. Next, since s \u0338= s\u221e, we have \u02c6P (k)(s\u221e | s, a) = 1 \u2212 \ufffd s\u2032\u2208S\\{s\u221e} \u02c6P (k)(s\u2032 | s, a) = \ufffd s\u2032\u2208S\\{s\u221e} P(s\u2032 | s, a) \u2212 \u02c6P (k)(s\u2032 | s, a), so we can decompose the summand \u02c6P (k)(s\u221e | s, a) \u00b7 FZ(\u03c0) t+1(s\u221e,y+r)(x \u2212 r) (i.e., s\u2032 = s\u221e) in (2) and distribute it across the other summands; in particular, the summands s\u2032 \u0338= s\u221e become \u02c6P (k)(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) + \ufffd P(s\u2032 | s, a) \u2212 \u02c6P (k)(s\u2032 | s, a) \ufffd \u00b7 FZ(\u03c0) t+1(s\u221e,y+r)(x \u2212 r) \u2264 \u02c6P (k)(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) + \ufffd P(s\u2032 | s, a) \u2212 \u02c6P (k)(s\u2032 | s, a) \ufffd \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) = P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r), (3) where the second line follows since FZ(\u03c0) t+1(s\u221e,y+r)(x \u2212 r) \u2264 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) for all s\u2032 \u0338= s\u221e, and since P(s\u2032 | s, a) \u2212 \u02c6P (k)(s\u2032 | s, a) \u2265 0 on event E. Continuing from (2), we have F \u02c6 Z(k,\u03c0) t (s,y)(x) \u2264 \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S \u03c0(a | s, y) \u00b7 \u02c6P (k)(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dPR(s,a)(r) \u2264 \ufffd \ufffd a\u2208A \ufffd s\u2032\u2208S\\{s\u221e} \u03c0(a | s, y) \u00b7 P(s\u2032 | s, a) \u00b7 FZ(\u03c0) t+1(s\u2032,y+r)(x \u2212 r) \u00b7 dPR(s,a)(r) = FZ(\u03c0) t (s,y)(x), (4) where the second line follows by distributing the summand s\u2032 = s\u221e and applying (3). Since M and \u02c6 M have the same initial state distribution, we have F \u02c6 Z(k,\u03c0)(x) \u2264 FZ(\u03c0)(x). By Lemma 5.1, we have \u02c6\u03a6(k)(\u03c0) = T \u2212 \ufffd T 0 G(F \u02c6 Z(k,\u03c0)(x)) \u00b7 dx \u2265 T \u2212 \ufffd T 0 G(FZ(\u03c0)(x)) \u00b7 dx = \u03a6(\u03c0), where the inequality follows from (4) and since G is monotone. The claim follows. 19 C.4 Proof of Theorem 4.1 Proof. We prove Theorem 4.1. Note that on event E, we have regret(A) = K \ufffd k=1 \u03a6(\u03c0\u2217) \u2212 \u03a6(\u02c6\u03c0(k)) \u2264 K \ufffd k=1 \u02c6\u03a6(k)(\u03c0\u2217) \u2212 \u03a6(\u02c6\u03c0(k)) \u2264 K \ufffd k=1 \u02c6\u03a6(k)(\u02c6\u03c0(k)) \u2212 \u03a6(\u02c6\u03c0(k)) \u2264 K \ufffd k=1 2T \u00b7 LG \u00b7 \ufffd |S| \u00b7 B(k)(\u02c6\u03c0(k)) = 2TLG \ufffd 5|S|2 log \ufffd4|S| \u00b7 |A| \u00b7 K \u03b4 \ufffd \u00b7 E \u039e(\u02c6\u03c0(1:K)) T \ufffd K \ufffd k=1 T \ufffd t=1 1 \ufffd N (k)(st, at) \ufffd\ufffd\ufffd\ufffd {N (k)(s, a)}k\u2208[K],s\u2208S,a\u2208A \ufffd , where the \ufb01rst inequality follows by Lemma 5.7, the second follows by optimality of \u02c6\u03c0(k) for \u02c6\u03a6(k), and the third by Lemma 5.6. Furthermore, note that K \ufffd k=1 T \ufffd t=1 1 \ufffd N (k)(st, at) \u2264 K \ufffd k=1 T \ufffd t=1 1(N (k)(st, at) \u2264 T) + K \ufffd k=1 T \ufffd t=1 1(N (k)(st, at) > T) 1 \ufffd N (k)(st, at) . The event (st, at) = (s, a) and (N (k)(s, a) \u2264 T) can happen fewer than 2T times per state action pair. Therefore, \ufffdK k=1 \ufffdT t=1 1(N (k)(st, at) \u2264 T) \u2264 2TSA. Now suppose N (k)(s, a) > T. Then for any t \u2208 Wk, we have N (k) t (s, a) \u2264 N (k)(s, a) + T \u2264 2N (k)(s, a). Thus, we have K \ufffd k=1 T \ufffd t=1 1(N (k)(st, at) > T) \ufffd N (k)(st, at) \u2264 K \ufffd k=1 T \ufffd t=1 \ufffd 2 N (k) t (st, at) = \u221a 2 K \ufffd k=1 T \ufffd t=1 \ufffd s\u2208S \ufffd a\u2208A 1((st, at) = (s, a)) \ufffd N (k) t (s, a) \u2264 \u221a 2 \ufffd s\u2208S \ufffd a\u2208A N (K+1)(s,a) \ufffd j=1 j\u22121/2 \u2264 \u221a 2 \ufffd s\u2208S \ufffd a\u2208A \ufffd N (K+1)(s,a) x=0 x\u22121/2dx \u2264 \ufffd 2|S| \u00b7 |A| \u00b7 \ufffd s\u2208S \ufffd a\u2208A N (K+1)(s, a) = \ufffd 2|S| \u00b7 |A| \u00b7 KT. The claim follows. D The Optimal Policy for CVaR Objectives In this section, we describe how to compute the optimal policy for the CVaR objective when the MDP is known; this approach is described in detail in [13]. Following this work, we consider the setting where we are trying to minimize cost rather than maximize reward. In particular, consider an MDP M = (S, A, D, P, P, T), and our goal is to compute a policy \u03c0 that maximizes its CVaR objective. 20 Step 1: CVaR objective. We begin by rewriting the CVaR objective in a form that is more amenable to optimization. First, we have the following key result (see [13] for a proof): Lemma D.1. For any random variable Z, we have CVaR\u03b1(Z) = inf \u03c1\u2208R \ufffd \u03c1 + 1 1 \u2212 \u03b1 \u00b7 EZ \ufffd (Z \u2212 \u03c1)+\ufffd\ufffd , where the minimum is achieved by \u03c1\u2217 = VaR(Z). As a consequence of this lemma, we have min \u03c0\u2208\u03a0 CVaR(Z(\u03c0)) = min \u03c0\u2208\u03a0 inf \u03c1\u2208R \ufffd \u03c1 + 1 1 \u2212 \u03b1 \u00b7 EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1)+\ufffd\ufffd = inf \u03c1\u2208R \ufffd \u03c1 + 1 1 \u2212 \u03b1 \u00b7 min \u03c0\u2208\u03a0 EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1)+\ufffd\ufffd . Thus, we have \u03c0\u2217 = arg min \u03c0\u2208\u03a0 EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1\u2217)+\ufffd , where \u03c1\u2217 = arg inf \u03c1\u2208R J(\u03c1) where J(\u03c1) = \u03c1 + 1 1 \u2212 \u03b1 \u00b7 max \u03c0\u2208\u03a0 EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1)+\ufffd . The main challenge is evaluating the minimum over \u03c0 \u2208 \u03a0 in J(\u03c1). To do so, we construct another MDP whose objective is EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1)+\ufffd for the appropriate choice of initial state distribution. Step 2: Construct alternative MDP. The MDP we construct is \u02dc M = ( \u02dcS, A, \u02dcD, \u02dcP, \u02dcR, T), where the states are \u02dcS = S \u00d7 R, the (time-varying, deterministic) rewards \u02dcR : \u02dcS \u00d7 [T] \u2192 R are \u02dcR((s, r), t) = \ufffdmax{r, 0} if t = T 0 otherwise, and the transitions are \u02dcP((s\u2032, r\u2032) | (s, r), a) = P(s\u2032 | s, a) \u00d7 PR(s,a)(r\u2032 \u2212 r), noting that \u02dcP is a (conditional) probability measure since the state space \u02dcS includes a continuous component; in practice, we discretize the continuous component of the state space. Step 3: Value iteration. Letting S1 be the random initial state of the original MDP M (with distribution D), we have EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1)+\ufffd = ES1 \ufffd \u02dcV (\u03c0) 1 ((S1, \u2212\u03c1)) \ufffd , where \u02dcV (\u03c0) 1 is the value function of policy \u03c0 for MDP \u02dc M on step t = 1. Thus, we have min \u03c0\u2208\u03a0 EZ(\u03c0) \ufffd (Z(\u03c0) \u2212 \u03c1)+\ufffd = ES1 \ufffd \u02dcV \u2217 1 ((S1, \u2212\u03c1)) \ufffd , where \u02dcV \u2217 1 is the value function of the optimal policy for \u02dc M. Intuitively, this strategy works because the augmented component of the state space r captures the cumulative reward so far plus its initial value \u2212\u03c1; then, by the de\ufb01nition of \u02dcR, the reward is r+, which implies that \u02dcV (\u03c0) 1 ((s, \u2212\u03c1)) is the expectation of the random variable (Z(\u03c0) \u2212 \u03c1)+. Thus, we can compute min\u03c0\u2208\u03a0 EZ(\u03c0)[(Z(\u03c0) \u2212 \u03c1)+] by performing value iteration on \u02dc M to compute \u02dcV (\u03c0) 1 . In particular, we have \u02dcV \u2217 T ((s, r)) = max{r, 0}, and \u02dcV \u2217 t ((s, r)) = min a\u2208A \ufffd \u02dcV \u2217 t+1((s\u2032, r\u2032)) \u00b7 d \u02dcP((s\u2032, r\u2032) | (s, r), a) for all t \u2208 {1, ..., T \u2212 1}. Then, given an initial state s1, we construct state \u02dcs1 = (s1, \u2212\u03c1\u2217), where \u03c1\u2217 = arg inf \u03c1\u2208R \ufffd \u03c1 + 1 1 \u2212 \u03b1 \u00b7 \u02dcV (\u03c0) 1 ((s, \u2212\u03c1)) \ufffd , and then acting optimally in \u02dc M according to \u02dcV \u2217 t . 21 ",
    "title": "Regret Bounds for Risk-Sensitive",
    "paper_info": "Regret Bounds for Risk-Sensitive\nReinforcement Learning\nOsbert Bastani\nUniversity of Pennsylvania\nobastani@seas.upenn.edu\nYecheng Jason Ma\nUniversity of Pennsylvania\njasonyma@seas.upenn.edu\nEstelle Shen\nUniversity of Pennsylvania\npixna@sas.upenn.edu\nWanqiao Xu\nStanford University\nwanqiaox@stanford.edu\nAbstract\nIn safety-critical applications of reinforcement learning such as healthcare and\nrobotics, it is often desirable to optimize risk-sensitive objectives that account for\ntail outcomes rather than expected reward. We prove the \ufb01rst regret bounds for\nreinforcement learning under a general class of risk-sensitive objectives including\nthe popular CVaR objective. Our theory is based on a novel characterization of the\nCVaR objective as well as a novel optimistic MDP construction.\n1\nIntroduction\nThere has been recent interest in risk-sensitive reinforcement learning, which replaces the usual\nexpected reward objective with one that accounts for variation in possible outcomes. One of the\nmost popular risk-sensitive objectives is the conditional value-at-risk (CVaR) objective [1, 2, 3, 4],\nwhich is the average risk at some tail of the distribution of returns (i.e., cumulative rewards) under a\ngiven policy [5, 6]. More generally, we consider a broad class of objectives in the form of a weighted\nintegral of quantiles of the return distribution, of which CVaR is a special case.\nA key question is providing regret bounds for risk-sensitive reinforcement learning. While there has\nbeen some work studying this question, it has focused on a speci\ufb01c objective called the entropic risk\nmeasure [7, 8], leaving open the question of bounds for more general risk-sensitive objectives. There\nhas also been work on optimistic exploration for CVaR [9], but without any regret bounds.\nWe provide the \ufb01rst regret bounds for risk-sensitive reinforcement learning with objectives of form\n\u03a6(\u03c0) =\n\ufffd 1\n0\nF \u2020\nZ(\u03c0)(\u03c4) \u00b7 dG(\u03c4),\n(1)\nwhere Z(\u03c0) is the random variable encoding the return of policy \u03c0, FZ(\u03c0) is its quantile function\n(roughly speaking, the inverse CDF), and G is a weighting function over the quantiles. This class\ncaptures a broad range of useful objectives, and has been studied in prior work [10, 4].\nWe focus on the episodic setting, where the agent interacts with the environment, modeled by a\nMarkov decision process (MDP), over a \ufb01xed sequence of episodes. Its goal is to minimize the\nregret\u2014i.e., the gap between the objective value it achieves compared to the optimal policy. Our\napproach is based on the upper con\ufb01dence bound strategy [11, 12], which makes decisions according\nto an optimistic estimate of the MDP. We prove that this algorithm (denoted A) has regret\nregret(A) = \u02dcO\n\ufffd\nT 2 \u00b7 LG \u00b7 |S|3/2 \u00b7 |A| \u00b7\n\u221a\nK\n\ufffd\n,\nPreprint. Under review.\narXiv:2210.05650v1  [cs.LG]  11 Oct 2022\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing a novel approach for optimizing risk-sensitive objectives in reinforcement learning, and providing regret bounds for risk-sensitive reinforcement learning with objectives of form \u03a6(\u03c0). This is important for safety-critical applications such as healthcare and robotics.\n\n- (2): Innovation point: The approach proposed in the paper for regret bounds in risk-sensitive reinforcement learning with objectives of form \u03a6(\u03c0) is well motivated, and achieves a regret rate of O(sqrt(T)) on a broad class of objectives including CVaR, which supports their claims of providing the first regret bounds for risk-sensitive reinforcement learning under a general class of risk-sensitive objectives. \n\nPerformance: The paper provides regret bounds for risk-sensitive reinforcement learning that can be applied to a broad class of objective functions, including the popular CVaR objective. However, future work may need to extend these ideas to the setting of function approximation and explore alternative exploration strategies such as Thompson sampling. \n\nWorkload: The workload required for reviewing this paper is moderate, due to the technical nature of the content and the need to have knowledge of reinforcement learning and risk-sensitive objectives.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing a novel approach for optimizing risk-sensitive objectives in reinforcement learning, and providing regret bounds for risk-sensitive reinforcement learning with objectives of form \u03a6(\u03c0). This is important for safety-critical applications such as healthcare and robotics.\n\n- (2): Innovation point: The approach proposed in the paper for regret bounds in risk-sensitive reinforcement learning with objectives of form \u03a6(\u03c0) is well motivated, and achieves a regret rate of O(sqrt(T)) on a broad class of objectives including CVaR, which supports their claims of providing the first regret bounds for risk-sensitive reinforcement learning under a general class of risk-sensitive objectives. \n\nPerformance: The paper provides regret bounds for risk-sensitive reinforcement learning that can be applied to a broad class of objective functions, including the popular CVaR objective. However, future work may need to extend these ideas to the setting of function approximation and explore alternative exploration strategies such as Thompson sampling. \n\nWorkload: The workload required for reviewing this paper is moderate, due to the technical nature of the content and the need to have knowledge of reinforcement learning and risk-sensitive objectives.\n\n\n"
}