{
    "Abstract": "Abstract Meta reinforcement learning (meta-RL) algorithms leverage experience from learning previous tasks to learn how to learn new tasks quickly. However, this process requires a large number of meta-training tasks to be provided for metalearning. In effect, meta-RL shifts the human burden from algorithm to task design. In this work we automate the process of task design, devising a meta-learning algorithm that does not require manual design of meta-training tasks. We propose a family of unsupervised meta-RL algorithms based on the insight that task proposals based on mutual information can be used to train optimal meta learners. Experimentally, our unsupervised meta-RL algorithm, which does not require manual task design, substantially improves on learning from scratch, and is competitive with supervised meta-RL approaches on benchmark tasks. 1. ",
    "Introduction": "Introduction Reusing past experience for faster learning of new tasks is a key challenge for machine learning. Meta-learning methods achieve this by using past experience to explicitly optimize for rapid adaptation (Mishra et al., 2017; Snell et al., 2017; Schmidhuber, 1987; Finn et al., 2017a; Gupta et al., 2018; Wang et al., 2016; Al-Shedivat et al., 2017). In the context of reinforcement learning (RL), meta-reinforcement learning (meta-RL) algorithms can learn to solve new RL tasks more quickly through experience on past tasks (Duan et al., 2016b; Gupta et al., 2018; Finn et al., 2017a). Typical meta-RL algorithms assume the ability to sample from a pre-speci\ufb01ed task distribution, and these algorithms learn to solve new tasks drawn from this distribution very quickly. However, specifying a task distribution is tedious and requires a signi\ufb01cant amount of supervision (Finn et al., 2017b; Duan et al., 2016b) that may be dif\ufb01cult to provide for large, realworld problem settings. The performance of meta-learning algorithms critically depends on the meta-training task distribution, and meta-learning algorithms generalize best to *Equal contribution 1UC Berkeley 2Carnegie Mellon University 3Stanford University. Correspondence to: Abhishek Gupta <abhigupta@eecs.berkeley.edu>. new tasks which are drawn from the same distribution as the meta-training tasks (Finn & Levine, 2018). In effect, meta-RL of\ufb02oads the design burden from algorithm design to task design. While meta-RL acquires representations for fast adaptation to the speci\ufb01ed task distribution, specifying this task distribution is often tedious and challenging. Can we automate the process of task design, thereby doing away with human supervision entirely? In this paper, we take a step towards unsupervised metaRL: meta-learning from a task distribution that is acquired automatically, rather than requiring manual design of the meta-training tasks. While unsupervised meta-RL does not make any assumptions about the reward functions on which it will be evaluated at test time, it does assume that the environment dynamics remain the same. This allows an unsupervised meta-RL agent to utilize environment interactions to meta-train a model that is optimized to be effective for learning from previously unseen reward functions in that environment at meta-test time. Our method can also be thought of as automatically acquiring an environmentspeci\ufb01c learning procedure for deep neural network policies, somewhat related to data-driven initialization procedures explored in supervised learning (Kr\u00a8ahenb\u00a8uhl et al., 2015; Hsu et al., 2018). The primary contribution of our work is a framework for unsupervised meta-RL. We describe a family of unsupervised meta-RL algorithms and provide analysis to show that unsupervised meta-RL methods based on mutual information can be optimal, in a minimax sense. Our experiments shows that, for a variety of robotic control tasks, unsupervised meta-RL can effectively acquire RL procedures. These procedures not only learn faster than standard RL approaches that learn from scratch, but also outperform prior methods that do pure exploration and then \ufb01ne-tuning at test time. Our results even approach the performance of an oracle method that relies on hand-designed task distributions. 2. ",
    "Related Work": "Related Work Our work lies at the intersection of meta-RL, goal generation, and unsupervised exploration. Meta-learning algorithms use data from multiple tasks to learn how to learn, acquiring rapid adaptation procedures from experience (Schmidhuber, 1987; Naik & Mammone, 1992; Thrun & Pratt, 1998; Bengio et al., 1992; Hochreiter et al., 2001; arXiv:1806.04640v3  [cs.LG]  30 Apr 2020 Unsupervised Meta-Learning for Reinforcement Learning environment Unsupervised Meta-RL Meta-learned  environment-specific  RL algorithm reward-maximizing  policy reward  function Unsupervised  Task Acquisition Meta-RL Fast  Adaptation Figure 1. Unsupervised meta-reinforcement learning: Given an environment, unsupervised meta-RL produces an environmentspeci\ufb01c learning algorithm that quickly acquire new policies that maximize any task reward function. Santoro et al., 2016; Andrychowicz et al., 2016; Ravi & Larochelle, 2017; Finn et al., 2017a; Munkhdalai & Yu, 2017). These approaches have been extended into the setting of RL (Duan et al., 2016b; Wang et al., 2016; Finn et al., 2017a; Sung et al., 2017; Gupta et al., 2018; Mendonca et al., 2019; Houthooft et al., 2018; Stadie et al., 2018; Rakelly et al., 2019; Nagabandi et al., 2018a). In practice, the performance of meta-learning algorithms depends on the user-speci\ufb01ed meta-training task distribution. We aim to lift this limitation and provide a general recipe for avoiding manual task engineering for meta-RL. A handful of prior meta-learning methods have used self-proposed task distributions for learning supervised learning procedures (Hsu et al., 2018; Antoniou & Storkey, 2019; Lin et al., 2019; Ji et al., 2019). In contrast, our work deals with the RL setting, where the environment dynamics provides a rich inductive bias that our meta-learner can exploit. In the RL setting, task distributions can be obtained in a variety of ways, including adversarial goal generation (Sukhbaatar et al., 2017; Held et al., 2017), information-theoretic methods (Gregor et al., 2016; Eysenbach et al., 2018; Co-Reyes et al., 2018; Achiam et al., 2018). The most similar work is Jabri et al. (2019), which also considers the unsupervised application of meta-learning to RL tasks. We build upon this work by proving that an optimal meta-learner can be acquired using mutual information-based task proposal. Exploration methods that seek out novel states are also closely related to goal generation methods (Pathak et al., 2017; Schmidhuber, 2009; Bellemare et al., 2016; Osband et al., 2016; Stadie et al., 2015), but do not by themselves aim to generate new tasks or learn to adapt more quickly to new tasks, only to achieve wide coverage of the state space. Model-based RL methods (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Srinivas et al., 2018; Nagabandi et al., 2018b; Finn & Levine, 2017b; Atkeson & Santamaria, 1997) use unsupervised experience to learn a dynamics model but do not learn how to ef\ufb01ciently use this model to explore to solve new tasks. Goal-conditioned RL (Schaul et al., 2015; Andrychowicz et al., 2017; Pong et al., 2018) is also related to our work, and our analysis will study this special case \ufb01rst before generalizing to the general case of arbitrary tasks. As we discuss in Section 3.4, goal-reaching itself is not enough, as goal-reaching agents are not optimized to ef\ufb01ciently explore to determine which goal they should reach, relying instead on a hand-speci\ufb01ed goal parameterization that doesn\u2019t allow these algorithms to work with arbitrary reward functions. 3. Unsupervised Meta-RL We consider the problem of learning a reinforcement learning algorithm that can quickly solve new tasks in a given environment. This meta-RL process could, for example, tune the hyperparameters of another RL algorithm, or could replace the RL update rule itself with a learned update rule. Unlike prior work, we aim to do so without depending on any human supervision or information about the tasks that will be provided for meta-testing. A task reward is provided at meta-test time, and the learned RL procedure should adapt to this task reward as quickly as possible. We assume that all test-time tasks have the same dynamics, and differ only in their reward functions. Our algorithm will therefore need to utilize unsupervised environment interaction to learn an RL algorithm. In effect, the dynamics themselves will be the supervision for our learning algorithm. We formalize the meta-training setting as a controlled Markov process (CMP) \u2013 a Markov decision process without a reward function, C = (S, A, P, \u03b3, \u03c1), with state space S, action space A, transition dynamics P, discount factor \u03b3 and initial state distribution \u03c1. The CMP, along with a reward function r, produces a Markov decision processes M = (S, A, P, \u03b3, \u03c1, r). We de\ufb01ne a learning algorithm f : D \u2192 \u03c0 as a function that takes as input a dataset of experience from the MDP, D = {(si, ai, ri, s\u2032 i)} \u223c M, and outputs a policy \u03c0(a | s). Evaluation of the learning procedure f is carried out over a handful of episodes. In episode i, the learning procedure f observes all previous data {\u03c41, \u00b7 \u00b7 \u00b7 , \u03c4i\u22121} and outputs a policy to be used in iteration i. We evaluate the learning procedure f by summing its cumulative reward across iterations: R(f, rz) = \ufffd i E\u03c0=f({\u03c41,\u00b7\u00b7\u00b7 ,\u03c4i\u22121}) \u03c4\u223c\u03c0 \ufffd\ufffd t rz(st, at) \ufffd Our aim is to take this CMP and produce an environmentspeci\ufb01c learning algorithm f that can quickly learn an optimal policy \u03c0\u2217 r(a | s) for any reward function r. We refer to this problem as unsupervised meta-RL, and illustrate the problem setting in Fig. 1. We now sketch a recipe for unsupervised meta-RL, analyze when this recipe is optimal, and then instantiate a practical approximation to this theoretically-motivated approach by building upon known meta-learning algorithms and unsupervised exploration methods. 3.1. A General Recipe To construct an unsupervised meta-RL algorithm, we leverage the insight that, to acquire a fast learning algorithm without task supervision, we can simply leverage standard meta-learning techniques, but with unsupervised task proposal mechanisms. Our unsupervised meta-RL framework Unsupervised Meta-Learning for Reinforcement Learning therefore consists of a task proposal mechanism and a metalearning method. For reasons that will become more apparent later, we will de\ufb01ne the task distribution as a mapping from a latent variable z \u223c p(z) to a reward function rz(s, a) : S \u00d7 A \u2192 R1. That is, for each value of the random variable z, we have a different reward function rz(s, a). Under this formulation, learning a task distribution amounts to optimizing a parametric form for the reward function rz(s, a) that maps each z \u223c p(z) to a different reward function. The choice of this parametric form represents an important design decision for an unsupervised meta-learning method, and the resulting set of tasks is often referred to as a task or goal proposal procedure. In the following section, we will discuss a theoretical framework that allows us to make this choice in the following section so as to minimize worst case regret of the subsequently meta-learned learning algorithm f. The second component is the meta-learning algorithm, which takes the family of reward functions induced by p(z) and rz(s, a), along with the associated CMP, and metalearns an RL algorithm f that can quickly adapt to any task from the task distribution de\ufb01ned by p(z) and rz(s, a) in the given CMP. The meta-learned algorithm f can then learn new tasks quickly at meta-test time, when a user-speci\ufb01ed reward function is actually provided. Fig. 1 summarizes this generic design for an unsupervised meta-RL algorithm. The \u201cno free lunch theorem\u201d (Wolpert et al., 1995; Whitley & Watson, 2005) might lead us to expect that a truly generic approach to proposing a task distribution would not yield a learning procedure f that is effective on any real tasks. However, the assumption that the dynamics remain the same across tasks affords us an inductive bias with which we pay for our lunch. In the following sections, we will discuss how to formulate acquiring the optimal unsupervised learning procedure, which minimizes regret on new meta-test tasks in the absence of any prior knowledge. Since our analysis will focus on a restricted class of learning procedures, our results are lower bounds for the performance of general learning procedures. We \ufb01rst de\ufb01ne an optimal meta-learner and then show how we can train one without requiring task distributions to be hand-speci\ufb01ed. 3.2. Optimal Meta-Learners We begin our analysis by considering the optimal learning procedure when the task distribution is known. For a task distribution p(rz), the optimal learning procedure f \u2217 is given by f \u2217 \u225c arg max f Ep(rz) [R(f, rz)] . Other learning procedures f may achieve lower reward, and we de\ufb01ne the regret incurred by using a suboptimal learning 1In most cases p(z) is chosen to be a uniform categorical so it is not challenging to specify procedure as the difference in expected reward, compared with the optimal learning procedure: REGRET(f, p(rz)) \u225c Ep(rz) [R(f \u2217, rz)]\u2212Ep(rz) [R(f, rz)] . Minimizing this regret is equivalent to maximizing the expected reward objective used by most meta-RL methods (Finn et al., 2017a; Duan et al., 2016b). Note that different task distributions p(rz) will have different optimal learning procedures f \u2217. For example, the optimal behavior for manipulation tasks involves moving a robot\u2019s arms, while the optimal behavior for locomotion tasks involves moving a robot\u2019s legs. Therefore, f \u2217 depends on p(rz). We next de\ufb01ne the notion of an optimal unsupervised metalearner, which does not require prior knowledge of p(rz). In unsupervised meta-reinforcement learning, the reward distribution p(rz) is unknown. In this setting, we evaluate a learning procedure f based on its regret against the worstcase task distribution for CMP C: REGRETWC(f, C) = max p(rz) REGRET(f, p(rz)). (1) For a CMP C, we de\ufb01ne the optimal unsupervised learning procedure as follows: De\ufb01nition 1. The optimal unsupervised learning procedure f \u2217 C for a CMP C is de\ufb01ned as f \u2217 C \u225c arg min f REGRETWC(f, C). Note the optimal unsupervised learning procedure may be different for different CMPs. We can also de\ufb01ne the optimal unsupervised meta-learning algorithm F\u2217, which takes as input a CMP C and returns the optimal unsupervised learning procedure f \u2217 C for that CMP: De\ufb01nition 2. The optimal unsupervised meta-learner F\u2217(C) = f \u2217 C is a function that takes as input a CMP C and outputs the corresponding optimal unsupervised learning procedure f \u2217 C: F\u2217 \u225c arg min F REGRETWC(F(C), C) Note that the optimal unsupervised meta-learner F\u2217 is universal \u2013 it does not depend on any particular task distribution, or any particular CMP. The next sections discuss how to \ufb01nd the minimax learning procedure, which minimizes the worst-case regret (Eq. 1). 3.3. Special Case: Goal-Reaching Tasks We start by deriving an optimal unsupervised meta-learner for the special case where all tasks are assumed to be goal state reaching tasks, and then generalize this approach to solve arbitrary tasks in Section 3.4. We restrict our analysis to CMPs with deterministic dynamics, and consider episodes with \ufb01nite horizon T and a discount factor of Unsupervised Meta-Learning for Reinforcement Learning \u03b3 = 1. Each tasks corresponds to reaching a goal states sg at the last time step in the episode, so the reward function is rg(st) \u225c 1(t = T) \u00b7 1(st = g). We \ufb01rst derive the optimal learning procedure for the case where p(sg) is known, and then derive the optimal procedure for the case where p(sg) is unknown. 3.3.1. THE OPTIMAL LEARNING PROCEDURE FOR KNOWN p(sg) In the case of goal reaching tasks, the optimal fast learning procedure f searches through potential goal states until it \ufb01nds the goal and then navigates to that goal state in all subsequent episodes. De\ufb01ne f\u03c0 as the learning procedure that uses policy \u03c0 to explore until the goal is found, and then always returns to the goal state. We will restrict our attention to the set of learning procedures F\u03c0 \u225c {f\u03c0} constructed in this fashion, so our theoretical results will be lower bound on the performance of arbitrary learning procedures. The learning procedure f\u03c0 incurs one unit of regret for each step before it has found the goal, and zero regret afterwards. The expected cumulative regret is therefore the expectation of the hitting time. To compute the expected hitting time, we de\ufb01ne \u03c1T \u03c0 (s) as the probability that policy \u03c0 visits state s at time step t = T. If sg is the true goal, then the event that the policy \u03c0 reaches sg at the \ufb01nal step of an episode is a Bernoulli random variable with parameter p = \u03c1T \u03c0 (sg). Thus, the expected hitting time of this goal state is HITTINGTIME\u03c0(sg) = 1 \u03c1T\u03c0 (sg). The regret of the learning procedure f\u03c0 is REGRET(f\u03c0, p(rg)) = \ufffd HITTINGTIME\u03c0(sg)p(sg)dsg = \ufffd p(sg) \u03c1T\u03c0 (sg)dsg. (2) To now compute the optimal learning procedure f\u03c0, we can minimize the regret in Equation 2 w.r.t. the marginal distribution \u03c1T \u03c0 . Using the calculus of variations (for more details refer to Appendix C in Lee et al. (2019)), the exploration policy for the optimal meta-learner, \u03c0\u2217, satis\ufb01es: \u03c1T \u03c0\u2217(sg) = \ufffd p(sg) \ufffd \ufffd p(s\u2032g)ds\u2032g . (3) Thus, when the goal sampling distribution p(sg) is known, the optimal learning procedure is obtained by \ufb01nding \u03c0\u2217 satisfying Eq. 3 and then using f\u03c0\u2217 as the learning procedure. The next section considers the case where p(sg) is not known. 3.3.2. THE OPTIMAL UNSUPERVISED LEARNING PROCEDURE FOR GOAL REACHING TASKS In the case of goal-reaching tasks where the goal distribution p(sg) is not known, the optimal unsupervised learning procedure can be constructed from a policy with a uniform marginal state distribution (proof in Appendix A): Lemma 1. Let \u03c0 be a policy for which \u03c1T \u03c0 (s) is uniform. Then f\u03c0 is has lowest worst-case regret among learning procedures in F\u03c0. One route for constructing this optimal unsupervised learning procedure is to \ufb01rst acquire a policy \u03c0 for which \u03c1T \u03c0 (s) is uniform and then return f\u03c0. However, \ufb01nding such a policy \u03c0 is challenging, especially in high-dimensional state spaces and in the absense of resets. Instead, we will take an alternate route, acquiring f\u03c0 directly without every computing \u03c0. In addition to sidestepping the requirement of computing \u03c0, this approach will also have the bene\ufb01t of generalizing beyond goal-reaching tasks to arbitrary task distributions. Our approach for directly computing the optimal unsupervised learning procedure hinges on the observation that the optimal unsupervised learning procedure is the optimal (supervised) learning procedure for goals proposed from a uniform distribution. Thus, the optimal unsupervised learning procedure will come not as a result of a careful construction, but rather as the output of the an optimization procedure (i.e., meta-learning). Thus, we can obtain the optimal unsupervised learning procedure by applying a meta-learning algorithm to a task distribution that samples goals uniformly. To ensure that the resulting learning procedure f lies within the set F\u03c0, we will only consider \u201cmemoryless\u201d meta-learning algorithms that maintain no internal state before the true goal is found.2 While sampling goals uniform is itself a challenging problem, we can use the same trick as before: instead of constructing this uniform goal distribution directly, we instead \ufb01nd an optimization problem for which the solution is to sample goals uniformly. The optimization problem that we use will involve two latent variables, the \ufb01nal state sT and an auxiliary latent variable z sampled from a prior \u00b5(z). The optimization problem will be to \ufb01nd a conditional distribution \u00b5(sT | z) such that the mutual information between z and sT is optimized: max \u00b5(sT |z) I\u00b5(sT ; z) (4) The conditional distribution \u00b5(sT | z) that optimizes Equation 4 is one with a uniform marginal distribution over terminal states (proof in Appendix A): Lemma 2. Assume there exists a conditional distribution \u00b5(sT | z) satisfying the following two properties: 1. The marginal distribution over terminal states is uniform: \u00b5(sT ) = \ufffd \u00b5(sT | z)\u00b5(z)dz = UNIF(S); and 2MAML satis\ufb01es this requirement, as the internal parameters are updated by policy gradient, which is zero because the reward is zero before the true goal is found. Unsupervised Meta-Learning for Reinforcement Learning 2. The conditional distribution \u00b5(sT | z) is a Dirac: \u2200z, sT \u2203sz s.t. \u00b5(sT | z) = 1(sT = sz). Then any solution \u00b5(sT | z) to the mutual information objective (Eq. 4) satis\ufb01es the following: \u00b5(sT ) = UNIF(S) and \u00b5(sT | z) = 1(sT = sz). 3.3.3. OPTIMIZING MUTUAL INFORMATION To optimize the above mutual information objective, we note that a conditional distribution \u00b5(sT | z) can be de\ufb01ned implicitly via a latent-conditioned policy \u00b5(a | s, z). This policy is not a meta-learned model, but rather will become part of the task proposal mechanism. For a given prior \u00b5(z) and latent-conditioned policy \u00b5(a | s, z), the joint likelihood is \u00b5(\u03c4, z) = \u00b5(z)p(s1) \ufffd t p(st+1 | st, at)\u00b5(at | st, z), and the marginal likelihood is simply given by \u00b5(sT , z) = \ufffd \u00b5(\u03c4, z)ds1a1 \u00b7 \u00b7 \u00b7 aT \u22121. The purpose of our repeated indirection now becomes clear: prior work (Eysenbach et al., 2018; Achiam et al., 2018) has proposed ef\ufb01cient algorithms for maximizing the mutual information objective (Eq. 4) when the conditional distribution \u00b5(sT | z) is de\ufb01ned implicitly in terms of a latent-conditioned policy. At this point, we \ufb01nally can sample goals uniformly, by sampling z \u223c \u00b5(z) followed by sT \u223c \u00b5(sT | z). Recall that we wanted to obtain a uniform goal distribution so that we could apply meta-learning to obtain the optimal learning procedure. However, the input to meta-learning procedures is not a distribution over goals but a distribution over reward functions. We then de\ufb01ne our task proposal distribution p(rz) by sampling z \u223c p(z) and using the corresponding reward function rz(sT , aT ) \u225c log p(sT | z), resulting in a uniform distribution as described in Lemma 2. 3.4. General Case: Trajectory-Matching Tasks To extend the analysis in the previous section to the general case, and thereby derive a framework for optimal unsupervised meta-learning, we will consider \u201ctrajectory-matching\u201d tasks. These tasks are a trajectory-based generalization of goal reaching: while goal reaching tasks only provide a positive reward when the policy reaches the goal state, trajectory-matching tasks only provide a positive reward when the policy executes the optimal trajectory. The trajectory matching case is more general because, while trajectory matching can represent different goal-reaching tasks, it can also represent tasks that are not simply goal reaching, such as reaching a goal while avoiding a dangerous region or reaching a goal in a particular way. Moreover, the trajectory matching case is actually also a generalization of the typical reinforcement learning case with Markovian rewards, because any such task can be represented by a trajectory reaching objective as well. Please refer to Section 3.4.3 for a more complete discussion of the same. As before, we will restrict our attention to CMPs with deterministic dynamics. These non-Markovian tasks essentially amount to a problem where an RL algorithm must \u201cguess\u201d the optimal policy, and only receives a reward if its behavior is perfectly consistent with that optimal policy. We will show that optimizing the mutual information between z and trajectories to obtain a task proposal distribution, and subsequently optimizing a meta-learner for this distribution will give us the optimal unsupervised meta-learner for this class of reward functions. We subsequently show that unsupervised meta-learning for the trajectory-matching task is at least as hard as unsupervised meta-learning for general tasks. As before, let us begin within an analysis of optimal meta-learners in the case where the distribution over trajectory matching tasks p(\u03c4 \u2217) is known, and subsequently direct our attention to formulating an optimal unsupervised meta-learner. 3.4.1. OPTIMAL META-LEARNER FOR KNOWN p(\u03c4 \u2217) Formally, we de\ufb01ne a distribution of trajectory-matching tasks by a distribution over desired trajectories, p(\u03c4 \u2217). For each goal trajectory \u03c4 \u2217, the corresponding trajectory-level reward function is r\u2217 \u03c4(\u03c4) \u225c 1(\u03c4 = \u03c4 \u2217) Analysis from Section 3.3 can be repurposed here. As before, restrict our attention to learning procedures f\u03c0 \u2208 F\u03c0. After running the exploration policy to discover trajectories that obtain reward, the policy will deterministically keep executing the desired trajectory. We can de\ufb01ne the hitting time as the expected number of episodes to match the target trajectory: HITTINGTIME\u03c0(\u03c4 \u2217) = 1 \u03c0(\u03c4 \u2217) We then de\ufb01ne regret as the expected hitting time: REGRET(f\u03c0, p(r\u03c4)) = \ufffd HITTINGTIME\u03c0(\u03c4)p(\u03c4)d\u03c4) = \ufffd p(\u03c4) \u03c0(\u03c4)d\u03c4. (5) This de\ufb01nition of regret allows us to optimize for an optimal learning procedure, and we obtain an exploration policy for the optimal learning procedure satisfying the requirement \u03c0\u2217(\u03c4) = \ufffd p(\u03c4) \ufffd \ufffd p(\u03c4 \u2032)d\u03c4 \u2032 . Unsupervised Meta-Learning for Reinforcement Learning 3.4.2. OPTIMAL UNSUPERVISED LEARNING PROCEDURE FOR TRAJECTORY-MATCHING TASKS As described in Section 3.2, obtaining such a policy requires knowing the trajectory distribution p(\u03c4), and we must resort to optimizing the worst-case regret. As argued in Lemma 1, the solution to this min-max optimization is a learning procedure which has an exploration policy that is uniform distribution over trajectories. Lemma 3. Let \u03c0 be a policy for which \u03c0(\u03c4) is uniform. Then f\u03c0 has lowest worst-case regret among learning procedures in F\u03c0. We can acquire an unsupervised meta-learner of this form by proposing and meta-learning on a task distribution that is uniform over trajectories. How might we actually propose a task distribution that is uniform over trajectories? As argued for the goal reaching case, we can do so by optimizing a trajectory-level mutual information objective: I(\u03c4; z) = H[\u03c4] \u2212 H[\u03c4 | z] The optimal policy for this objective has a uniform distribution over trajectories that, conditioned on a particular latent z, deterministically produces a single trajectory in a deterministic CMP. The analysis for the case of stochastic dynamics is more involved and is left to future work. By optimizing a task proposal distribution that maximizes trajectory-level mutual information, and subsequently performing meta-learning on the proposed tasks, we can acquire the optimal unsupervised meta-learner for trajectory matching tasks, under the de\ufb01nition in Section 3.2. 3.4.3. RELATIONSHIP TO GENERAL REWARD MAXIMIZING TASKS Now that we have derived the optimal meta-learner for trajectory-matching tasks, we observe that trajectorymatching is a super-set of the problem of optimizing any possible Markovian reward function at test-time. For a given initial state distribution, each reward function is optimized by a particular trajectory. However, trajectories produced by a non-Markovian policy (i.e., a policy with memory) are not necessarily the unique optimum for any Markovian reward function. Let R\u03c4 denote the set of trajectory-level reward functions, and Rs,a denote the set of all state-action level reward functions. Bounding the worst-case regret on R\u03c4 minimizes an upper bound on the worst-case regret on Rs,a: min r\u03c4 \u2208R\u03c4 E\u03c0 [r\u03c4(\u03c4)] \u2264 min r\u2208Rs,a E\u03c0 \ufffd\ufffd t r(st, at) \ufffd \u2200\u03c0. This inequality holds for all policies \u03c0, including the policy that maximizes the LHS. While we aim to maximize the RHS, we only know how to maximize the LHS, which gives us a lower bound on the RHS. This inequality holds for all policies \u03c0, so it also holds for the policy that maximizes the LHS. In general, this bound is loose, because the set of all Markovian reward functions is smaller than the set of all trajectory-level reward functions (i.e., trajectory-matching tasks). However, this bound becomes tight when considering meta-learning on the set of all possible (non-Markovian) reward functions. In the discussion of meta-learning thus far, we have restricted our attention to tasks where the reward is provided at the last time step T of each episode and to the set of learning procedures F\u03c0 that maintain no internal state before the true goal or trajectory is found. In this restricted setting case, the best that an optimal meta-learner can do is go directly to a goal or execute a particular trajectory at every episode according to the optimal exploration policy as discussed previously, essentially performing a version of posterior sampling. In the more general case with arbitrary reward functions and arbitrary learning procedures, intermediate rewards along a trajectory may be informative, and the optimal exploration strategy may be different from posterior sampling (Rothfuss et al., 2019; Duan et al., 2016b; Wang et al., 2016). Nonetheless, the analysis presented in this section provides us insight into the behavior of optimal meta-learning algorithms and allows us to understand the qualities desirable for unsupervised task proposals. The general proposed scheme for unsupervised meta-learning has a signi\ufb01cant bene\ufb01t over standard universal value function and goal reaching style algorithms: it can be applied to arbitrary reward functions going beyond simple goal reaching, and doesn\u2019t require the goal to be known in a parametric form beforehand. 3.5. Summary of Analysis Through our analysis, we introduced the notion of optimal meta-learners and analyze their exploration behavior and regret on a class of goal reaching problems. We showed that on these problems, when the test-time task distribution is unknown, the optimal meta-training task distribution for minimizing worst-case test-time regret is uniform over the space of goals. We also showed that this optimal task distribution can be acquired by a simple mutual information maximization scheme. We subsequently extend the analysis to the more general case of matching arbitrary trajectories, as a proxy for the more general class of arbitrary reward functions. In the following section, we will discuss how we can derive a practical algorithm for unsupervised metalearning from this analysis. 3.6. A Practical Algorithm Following the derivation in the previous section, we can instantiate a practical unsupervised meta-RL algorithm by constructing a task proposal mechanism based on a mutual information objective. A variety of different mutual ",
    "Evaluation": "Evaluation In our experiments, we aim to understand whether unsupervised meta-learning as described in Section 3.1 can provide us with an accelerated RL procedure on new tasks. Whereas standard meta-learning requires a hand-speci\ufb01ed task distribution at meta-training time, unsupervised meta-learning learns the task distribution through unsupervised interaction with the environment. A fair baseline that likewise uses requires no reward supervision at training time, and only uses rewards at test time, is learning via RL from scratch without any meta-learning. As an upper bound, we include the unfair comparison to a standard meta-learning approach, where the meta-training distribution is manually designed. This method has access to a hand-speci\ufb01ed task distribution that is not available to our method. We evaluate two variants of our approach: (a) task acquisition based on DIAYN followed by meta-learning using MAML, and (b) task acquisition using a randomly initialized discriminator followed by meta-learning using MAML. 4.1. Tasks and Implementation Details Our experiments study three simulated environments of varying dif\ufb01culty: 2D point navigation, 2D locomotion using the \u201cHalfCheetah,\u201d and 3D locomotion using the \u201cAnt,\u201d with the latter two environments are modi\ufb01cations of popular RL benchmarks (Duan et al., 2016a). While the 2D navigation environment allows for direct control of position, HalfCheetah and Ant can only control their center of mass via feedback control with high dimensional actions (6D for HalfCheetah, 8D for Ant) and observations (17D for HalfCheetah, 111D for Ant). The evaluation tasks, shown in Figure 5, are similar to prior work (Finn et al., 2017a; Pong et al., 2018): 2D navigation and ant require navigating to goal positions, while the half cheetah must run at different goal velocities. These tasks are not accessible to our algorithm during meta-training. Please refer to Appendix C for details about hyperparameters for both MAML and DIAYN. 4.2. Fast Adaptation after Unsupervised Meta RL The comparison between the two variants of unsupervised meta-learning and learning from scratch is shown in Figure 2. We also add a comparison to VIME (Houthooft et al., 2016), a standard novelty-based exploration method, where we pretrain a policy with the VIME reward and then \ufb01netune it on the meta-test tasks. In all cases, the UMLDIAYN variant of unsupervised meta-learning produces an RL procedure that outperforms RL from scratch and VIME-init, suggesting that unsupervised interaction with the environment and meta-learning is effective in producing environment-speci\ufb01c but task-agnostic priors that accelerate learning on new, previously unseen tasks. The comparison with VIME shows that the speed of learning is not just about exploration but is indeed about fast adaptation. In our experiments thus far, UML-DIAYN always performs better than learning from scratch, although the bene\ufb01t varies across tasks depending on the actual performance of DIAYN. We also perform signi\ufb01cantly better than a baseline of simply Unsupervised Meta-Learning for Reinforcement Learning 2D navigation Half-Cheetah Ant Figure 2. Unsupervised meta-learning accelerates learning: After unsupervised meta-learning, our approach (UML-DIAYN and UMLRANDOM) quickly learns a new task signi\ufb01cantly faster than learning from scratch, especially on complex tasks. Learning the task distribution with DIAYN helps more for complex tasks. Results are averaged across 20 evaluation tasks, and 3 random seeds for testing. UML-DIAYN and random also signi\ufb01cantly outperform learning with DIAYN initialization or VIME. 2D Navigation Half-Cheetah Ant Navigation Figure 3. Comparison with handcrafted tasks: Unsupervised meta-learning (UML-DIAYN) is competitive with meta-training on handcrafted reward functions (i.e., an oracle). A misspeci\ufb01ed, handcrafted meta-training task distribution often performs worse, illustrating the bene\ufb01ts of learning the task distribution. initializing from a DIAYN trained contextual policy, and then \ufb01netuning the best skill with the actual task reward. Interestingly, in many cases (in Figure 3) the performance of unsupervised meta-learning with DIAYN matches that of the hand-designed task distribution. We see that on the 2D navigation task, while handcrafted meta-learning is able to learn very quickly initially, it performs similarly after 100 steps. For the cheetah environment as well, handcrafted meta-learning is able to learn very quickly to start off, but is quickly matched by unsupervised meta-RL with DIAYN. On the ant task, we see that hand-crafted meta-learning does do better than UML-DIAYN, likely because the task distribution is challenging, and a better unsupervised task proposal algorithm would improve performance. The comparison between the two unsupervised metalearning variants is also illuminating: while the DIAYNbased variant of our method generally achieves the best performance, even the random discriminator is often able to provide a suf\ufb01cient diversity of tasks to produce meaningful acceleration over learning from scratch in the case of 2D navigation and ant. This result has two interesting implications. First, it suggests that unsupervised meta-learning is an effective tool for learning an environment prior. Although the performance of unsupervised meta-learning can be improved with better coverage using DIAYN (as seen in Figure 2), even the random discriminator version provides competitive advantages over learning from scratch. Second, the comparison provides a clue for identifying the source of the structure learned through unsupervised meta-learning: though the particular task distribution has an effect on performance, simply interacting with the environment (without structured objectives, using a random discriminator) already allows meta-RL to learn effective adaptation strategies in a given environment. 5. Discussion and Future Work We presented an unsupervised approach to meta-RL, where meta-learning is used to acquire an ef\ufb01cient RL procedure without requiring hand-speci\ufb01ed task distributions. This approach accelerates RL without relying on the manual supervision required for conventional meta-learning algorithms. We provide a theoretical derivation that argues that task proposals based on mutual information maximization can provide a minimum worst-case regret meta-learner, under certain assumptions. Our experiments indicate unsupervised meta-RL can accelerate learning on a range of tasks. Our approach also opens a number of questions about unsupervised meta-learning algorithms. One limitation of our analysis is that it only considers deterministic dynamics, and only considers task distributions where posterior sampling is optimal. Extending our analysis to stochastic dynamics and more realistic task distributions may allow unsupervised meta-RL to acquire learning algorithms that can more effectively solve real-world tasks. ",
    "References": "References Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery algorithms. arXiv preprint arXiv:1807.10299, 2018. Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. arXiv preprint arXiv:1710.03641, 2017. Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Neural Information Processing Systems (NIPS), 2016. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017. Antreas Antoniou and Amos Storkey. Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation. arXiv preprint arXiv:1902.09884, 2019. Christopher G Atkeson and Juan Carlos Santamaria. A comparison of direct and model-based reinforcement learning. In Proceedings of International Conference on Robotics and Automation, volume 4, pp. 3557\u20133564. IEEE, 1997. Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R\u00b4emi Munos. Unifying count-based exploration and intrinsic motivation. CoRR, abs/1606.01868, 2016. URL http://arxiv.org/abs/1606.01868. Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Optimality in Arti\ufb01cial and Biological Neural Networks, 1992. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pp. 4754\u20134765, 2018. John D Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. arXiv preprint arXiv:1806.02813, 2018. Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-ef\ufb01cient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465\u2013472, 2011. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329\u20131338, 2016a. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. CoRR, abs/1710.11622, 2017a. URL http://arxiv.org/abs/1710.11622. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 2786\u20132793. IEEE, 2017b. Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. International Conference on Learning Representations, 2018. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017a. Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via metalearning. CoRR, abs/1709.04905, 2017b. URL http:// arxiv.org/abs/1709.04905. Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016. Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-reinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245, 2018. David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017. Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Arti\ufb01cial Neural Networks, 2001. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: variational information maximizing exploration. In Advances in Neural Information Processing Systems, 2016. Rein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. arXiv preprint arXiv:1802.04821, 2018. Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. arXiv preprint arXiv:1810.02334, 2018. Allan Jabri, Kyle Hsu, Abhishek Gupta, Ben Eysenbach, Sergey Levine, and Chelsea Finn. Unsupervised curricula for visual meta-reinforcement learning. In Advances in Neural Information Processing Systems, pp. 10519\u201310530, 2019. Zilong Ji, Xiaolong Zou, Tiejun Huang, and Si Wu. Unsupervised few-shot learning via self-supervised training. arXiv preprint arXiv:1912.12178, 2019. Philipp Kr\u00a8ahenb\u00a8uhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations of convolutional neural networks. arXiv preprint arXiv:1511.06856, 2015. Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhutdinov. Ef\ufb01cient exploration via state marginal matching. CoRR, abs/1906.05274, 2019. URL http://arxiv.org/abs/1906.05274. Unsupervised Meta-Learning for Reinforcement Learning Jianxin Lin, Yijun Wang, Yingce Xia, Tianyu He, and Zhibo Chen. Learning to transfer: Unsupervised meta domain translation. arXiv preprint arXiv:1906.00181, 2019. Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Guided meta-policy search. CoRR, abs/1904.00956, 2019. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In NIPS 2017 Workshop on Meta-Learning, 2017. Tsendsuren Munkhdalai and Hong Yu. Meta networks. International Conference on Machine Learning (ICML), 2017. Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. arXiv preprint arXiv:1803.11347, 2018a. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free \ufb01ne-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 7559\u20137566. IEEE, 2018b. Devang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In International Joint Conference on Neural Netowrks (IJCNN), 1992. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. CoRR, abs/1602.04621, 2016. URL http://arxiv.org/abs/ 1602.04621. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, 2017. Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep rl for modelbased control. arXiv preprint arXiv:1802.09081, 2018. Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Ef\ufb01cient off-policy meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254, 2019. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017. Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal meta-policy search. In International Conference on Learning Representations, ICLR, 2019. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memoryaugmented neural networks. In International Conference on Machine Learning (ICML), 2016. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pp. 1312\u20131320, 2015. J\u00a8urgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universit\u00a8at M\u00a8unchen, 1987. J\u00a8urgen Schmidhuber. Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. In Computational Creativity: An Interdisciplinary Approach, 12.07. 17.07.2009, 2009. URL http://drops.dagstuhl.de/ opus/volltexte/2009/2197/. Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4080\u20134090, 2017. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018. Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015. Bradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. CoRR, abs/1803.01118, 2018. URL http://arxiv.org/ abs/1803.01118. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017. Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic networks for sample ef\ufb01cient learning. arXiv preprint arXiv:1706.09529, 2017. Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 1998. Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018. Darrell Whitley and Jean Paul Watson. Complexity theory and the no free lunch theorem, 2005. David H Wolpert, William G Macready, et al. No free lunch theorems for search. Technical report, Technical Report SFITR-95-02-010, Santa Fe Institute, 1995. Unsupervised Meta-Learning for Reinforcement Learning A. Proofs Lemma 1 Let \u03c0 be a policy for which \u03c1T \u03c0 (s) is uniform. Then \u03c0 has lowest worst-case regret. Proof of Lemma 1. To begin, we note that all goal distributions p(sg) have equal regret for policies where \u03c1T \u03c0 (s) = 1/|S| is uniform: REGRETp(\u03c0) = \ufffd p(sg) \u03c1T\u03c0 (sg)dsg = \ufffd p(sg) 1/|S|dsg = |S| Now, consider a policy \u03c0\u2032 for which \u03c1T \u03c0 (s) is not uniform. For simplicity, we will assume that the argmin is unique, though the proof holds for non-unique argmins as well. The worst-case goal distribution will choose the state s\u2212 where that the policy is least likely to visit: p\u2212(sg) \u225c 1(sg = arg min s \u03c1T \u03c0 (s)) Thus, the worst-case regret for policy \u03c0\u2032 is strictly greater than the regret for a uniform \u03c0: max p REGRETp(\u03c0) = REGRETp\u2212(\u03c0) = \ufffd 1(sg = arg mins \u03c1T \u03c0 (s)) \u03c1T\u03c0 (sg) dsg = 1 mins \u03c1T \u03c0\u2032(s) > |S| (6) Thus, a policy \u03c0\u2032 for which \u03c1T \u03c0 is non-uniform cannot be minimax, so the optimal policy has a uniform marginal \u03c1T \u03c0 . Lemma 2: Mutual information I(sT ; z) is maximized by a task distribution p(sg) which is uniform over goal states. Proof of Lemma 2. We de\ufb01ne a latent variable model, where we sample a latent variable z from a uniform prior p(z) and sample goals from a conditional distribution p(sT | z). To begin, note that the mutual information can be written as a difference of entropies: Ip(sT ; z) = Hp[sT ] \u2212 Hp[sT | z] The conditional entropy Hp[sT | z] attains the smallest possible value (zero) when each latent variable z corresponds to exactly one \ufb01nal state, sz. In contrast, the marginal entropy Hp[sT ] attains the largest possible value (log |S|) when the marginal distribution p(sT ) = \ufffd p(sT | z)p(z)dz is uniform. Thus, a task uniform distribution p(sg) maximizes I(sT ; z). Note that for any non-uniform task distribution q(sT ), we have Hq[sT ] < Hp[sT ]. Since the conditional entropy Hp[sT | z] is zero, no distribution can achieve a smaller conditional entropy. This, for all non-uniform task distributions q, we have Iq(sT ; z) < Ip(sT ; z). Thus, the optimal task distribution must be uniform. B. Ablations Figure 4. Analysis of effect of additional meta-training on metatest time learning of new tasks. For larger iterations of meta-trained policies, we have improved test time performance, showing that additional meta-training is bene\ufb01cial. To understand the method performance more clearly, we also add an ablation study where we compare the meta-test performance of policies at different iterations along metatraining. This shows the effect that additional meta-training has on the fast learning performance for new tasks. This comparison is shown in Figure 4. As can be seen here, at iteration 0 of meta-training the policy is not a very good initialization for learning new tasks. As we move further along the meta-training process, we see that the meta-learned initialization becomes more and more effective at learning new tasks. This shows a clear correlation between additional meta-training and improved meta test-time performance. B.1. Analysis of Learned Task Distributions We can analyze the tasks discovered through unsupervised exploration and compare them to tasks we evaluate on at meta-test time. Figure 5 illustrates these distributions using scatter plots for 2D navigation and the Ant, and a histogram for the HalfCheetah. Note that we visualize dimensions of the state that are relevant for the evaluation tasks \u2013 positions and velocities \u2013 but these dimensions are not speci\ufb01ed in any way during unsupervised task acquisition, which operates on the entire state space. Although the tasks proposed via unsupervised exploration provide fairly broad coverage, they are clearly quite distinct from the meta-test tasks, suggesting the approach can tolerate considerable distributional shift. Qualitatively, many of the tasks proposed via unsupervised exploration such as jumping and falling that are not relevant for the evaluation tasks. Our choice of the evaluation tasks was largely based on prior work, and therefore not tailored to this exploration procedure. The results for unsupervised Unsupervised Meta-Learning for Reinforcement Learning 2D navigation Ant Half-Cheetah Figure 5. Learned meta-training task distribution and evaluation tasks: We plot the center of mass for various skills discovered by point mass and ant using DIAYN, and a blue histogram of goal velocities for cheetah. Evaluation tasks, which are not provided to the algorithm during meta-training, are plotted as red \u2018x\u2019 for ant and pointmass, and as a green histogram for cheetah. While the meta-training distribution is broad, it does not fully cover the evaluation tasks. Nonetheless, meta-learning on this learned task distribution enables ef\ufb01cient learning on a test task distribution. meta-RL therefore suggest quite strongly that unsupervised task acquisition can provide an effective meta-training set, at least for MAML, even when evaluating on tasks that do not closely match the discovered task distribution. C. Hyperparameter Details Half-Cheetah Ant Figure 6. Environments: (Left) Half-Cheetah and (Right) Ant For all our experiments, we used DIAYN to acquire the task proposals using 20 skills for half-cheetah and for ant and 50 skills for the 2D navigation. We illustrate these half cheetah and ant in Fig. 6. We ran the domains using the standard DIAYN hyperparameters described in https://github. com/ben-eysenbach/sac to acquire task proposals. These proposals were then fed into the MAML algorithm https://github.com/cbfinn/maml_rl, with inner learning rate 0.1, meta learning rate 0.01, inner batch size 40, outer batch size, path length 100, using 2 layer networks with 300 units each with ReLu nonlinearities. We vary the meta-batch size according to the number of skills: 50 for pointmass, 20 for cheetah, and 20 ant. The test time learning is done with the same parameters for the UMRL variants, and done using REINFORCE with the Adam optimizer for the comparison with learning from scratch. We swept over learning rates for learning from scratch via vanilla policy gradient, and found that using ADAM with adaptive step size is the most stable and quick at learning. ",
    "title": "Unsupervised Meta-Learning for Reinforcement Learning",
    "paper_info": "Unsupervised Meta-Learning for Reinforcement Learning\nAbhishek Gupta * 1 Benjamin Eysenbach * 2 Chelsea Finn 3 Sergey Levine 1\nAbstract\nMeta reinforcement learning (meta-RL) algo-\nrithms leverage experience from learning previ-\nous tasks to learn how to learn new tasks quickly.\nHowever, this process requires a large number\nof meta-training tasks to be provided for meta-\nlearning. In effect, meta-RL shifts the human bur-\nden from algorithm to task design. In this work\nwe automate the process of task design, devising\na meta-learning algorithm that does not require\nmanual design of meta-training tasks. We propose\na family of unsupervised meta-RL algorithms\nbased on the insight that task proposals based on\nmutual information can be used to train optimal\nmeta learners. Experimentally, our unsupervised\nmeta-RL algorithm, which does not require man-\nual task design, substantially improves on learn-\ning from scratch, and is competitive with super-\nvised meta-RL approaches on benchmark tasks.\n1. Introduction\nReusing past experience for faster learning of new tasks is a\nkey challenge for machine learning. Meta-learning methods\nachieve this by using past experience to explicitly optimize\nfor rapid adaptation (Mishra et al., 2017; Snell et al., 2017;\nSchmidhuber, 1987; Finn et al., 2017a; Gupta et al., 2018;\nWang et al., 2016; Al-Shedivat et al., 2017). In the context of\nreinforcement learning (RL), meta-reinforcement learning\n(meta-RL) algorithms can learn to solve new RL tasks more\nquickly through experience on past tasks (Duan et al., 2016b;\nGupta et al., 2018; Finn et al., 2017a). Typical meta-RL\nalgorithms assume the ability to sample from a pre-speci\ufb01ed\ntask distribution, and these algorithms learn to solve new\ntasks drawn from this distribution very quickly. However,\nspecifying a task distribution is tedious and requires a sig-\nni\ufb01cant amount of supervision (Finn et al., 2017b; Duan\net al., 2016b) that may be dif\ufb01cult to provide for large, real-\nworld problem settings. The performance of meta-learning\nalgorithms critically depends on the meta-training task dis-\ntribution, and meta-learning algorithms generalize best to\n*Equal contribution\n1UC Berkeley 2Carnegie Mellon Univer-\nsity 3Stanford University. Correspondence to: Abhishek Gupta\n<abhigupta@eecs.berkeley.edu>.\nnew tasks which are drawn from the same distribution as\nthe meta-training tasks (Finn & Levine, 2018). In effect,\nmeta-RL of\ufb02oads the design burden from algorithm design\nto task design. While meta-RL acquires representations for\nfast adaptation to the speci\ufb01ed task distribution, specifying\nthis task distribution is often tedious and challenging. Can\nwe automate the process of task design, thereby doing away\nwith human supervision entirely?\nIn this paper, we take a step towards unsupervised meta-\nRL: meta-learning from a task distribution that is acquired\nautomatically, rather than requiring manual design of the\nmeta-training tasks. While unsupervised meta-RL does not\nmake any assumptions about the reward functions on which\nit will be evaluated at test time, it does assume that the\nenvironment dynamics remain the same. This allows an\nunsupervised meta-RL agent to utilize environment interac-\ntions to meta-train a model that is optimized to be effective\nfor learning from previously unseen reward functions in\nthat environment at meta-test time. Our method can also\nbe thought of as automatically acquiring an environment-\nspeci\ufb01c learning procedure for deep neural network policies,\nsomewhat related to data-driven initialization procedures\nexplored in supervised learning (Kr\u00a8ahenb\u00a8uhl et al., 2015;\nHsu et al., 2018).\nThe primary contribution of our work is a framework for un-\nsupervised meta-RL. We describe a family of unsupervised\nmeta-RL algorithms and provide analysis to show that unsu-\npervised meta-RL methods based on mutual information can\nbe optimal, in a minimax sense. Our experiments shows that,\nfor a variety of robotic control tasks, unsupervised meta-RL\ncan effectively acquire RL procedures. These procedures\nnot only learn faster than standard RL approaches that learn\nfrom scratch, but also outperform prior methods that do pure\nexploration and then \ufb01ne-tuning at test time. Our results\neven approach the performance of an oracle method that\nrelies on hand-designed task distributions.\n2. Related Work\nOur work lies at the intersection of meta-RL, goal gen-\neration, and unsupervised exploration. Meta-learning al-\ngorithms use data from multiple tasks to learn how to\nlearn, acquiring rapid adaptation procedures from experi-\nence (Schmidhuber, 1987; Naik & Mammone, 1992; Thrun\n& Pratt, 1998; Bengio et al., 1992; Hochreiter et al., 2001;\narXiv:1806.04640v3  [cs.LG]  30 Apr 2020\n",
    "GPTsummary": "                    - (1): The background of this research is the challenge of quickly learning new tasks by leveraging past experience in the field of machine learning.\n\n                    - (2): Past methods in meta-reinforcement learning require a pre-specified task distribution, which is challenging to provide for large-scale real-world problems. This paper proposes unsupervised meta-learning, which uses an automatic task distribution rather than manual design of meta-training tasks. The approach is well motivated as it potentially eliminates the need for human supervision entirely.\n\n                    - (3): The research methodology proposed in this paper is a family of unsupervised meta-learning algorithms for reinforcement learning based on mutual information. The algorithms utilize task proposals based on mutual information to train optimal meta-learners, and do not require manual task design.\n\n                    - (4): The methods in this paper achieve improved performance on a variety of robotic control tasks, with the unsupervised methods outperforming standard reinforcement learning from scratch and prior methods that combine exploration and fine-tuning. The results approach the performance of an oracle method that relies on hand-designed task distributions. These results support the goals of unsupervised meta-learning for reinforcement learning.\n\n\n\n\n\n\n\n8. Conclusion: \n- (1): This piece of work proposes an unsupervised meta-learning algorithm for reinforcement learning that uses an automatic task distribution to quickly learn new tasks. The significance of this work is that it potentially eliminates the need for human supervision in task design, which is challenging for large-scale real-world problems. \n\n- (2): Innovation point: The unsupervised meta-learning algorithm proposed in this paper is innovative in that it uses an automatic task distribution rather than manual design of meta-training tasks, potentially eliminating the need for human supervision entirely. Performance: The methods in this paper achieve improved performance on a variety of robotic control tasks, with the unsupervised methods outperforming standard reinforcement learning from scratch and prior methods that combine exploration and fine-tuning. The results approach the performance of an oracle method that relies on hand-designed task distributions. Workload: The workload for implementing the proposed unsupervised meta-learning algorithm is not explicitly discussed in this paper, but it does require significant computational resources to train the optimal meta-learner. \n\nNote: Proper nouns have been marked in English, and the statements are concise and academic as required.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (2): Innovation point: The unsupervised meta-learning algorithm proposed in this paper is innovative in that it uses an automatic task distribution rather than manual design of meta-training tasks, potentially eliminating the need for human supervision entirely. Performance: The methods in this paper achieve improved performance on a variety of robotic control tasks, with the unsupervised methods outperforming standard reinforcement learning from scratch and prior methods that combine exploration and fine-tuning. The results approach the performance of an oracle method that relies on hand-designed task distributions. Workload: The workload for implementing the proposed unsupervised meta-learning algorithm is not explicitly discussed in this paper, but it does require significant computational resources to train the optimal meta-learner. \n\nNote: Proper nouns have been marked in English, and the statements are concise and academic as required.\n\n\n"
}