{
    "Abstract": "",
    "Introduction": "INTRODUCTION Reinforcement learning is a technique which automatically learns a strategy to solve a task by interacting with the environment and learning from its mistakes. By combining reinforcement learning and deep learning to extract features from the input, a wide variety of tasks such as Atari 2600 games [14] are efciently solved. However, these techniques applied to 2D domains struggle in complex environments such as three-dimensional virtual worlds resulting a prohibitive training time and an inefcient learned policy. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permited. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. SAC 2018, Pau, France \u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5191-1/18/04...$15.00 DOI: 10.1145/3167132.3167165 A powerful recent idea to tackle the problem of computational expenses is to modularise the models into an ensemble of experts [10]. Since each expert focuses on learning a stage of the task, the reduction of the actions to consider leads to a shorter learning period. Although this approach is conceptually simple, it does not handle very complex environments and environments with a large set of actions. A similar idea of extending the information extracted from lowlevel architectural modules [11] with high-level ones have been previously used in the area of cognitive systems [15] but does not directly relies on RL and was limited to a supervised classifcation problem. Te idea was to leverage information about videos with external ontologies to detect events in videos. Another technique is called Hierarchical Learning [20][1] and is used to solve complex tasks, such as \u201dsimulating human brain\u201d [9]. It is inspired by human learning which uses previous experiences to face new situations. Instead of learning directly the entire task, diferent sub-tasks are learned by the agent. By reusing knowledge acquired from the previous sub-tasks, the learning is faster and easier. Some limitations are the necessity to re-train the model which is time consuming and problems related to catastrophic forgeting of knowledge on previous tasks. In this paper, our approach focuses on combining deep reinforcement learning and external knowledge. Using external knowledge is a way to supervise the learning and enhance information given to the agent by introducing human expertise. We augment the input of a reinforcement learning model whose input is raw pixels by adding high-level information created from simple knowledge about the task and recognized objects. We combine this model with a knowledge based decision algorithm using Q-learning [24]. In our experiments, we demonstrate that our framework successfully learns in real time to solve a food gathering task in a 3D partially observable environment by only using visual inputs. We evaluate our technique on the Malmo platform built on top of a 3D virtual environment, Minecraf. Our model is especially suitable for tasks involving navigation, orientation or exploration, in which we can easily provide external knowledge. Te paper is organized as follows. Section 2 gives an overview of reinforcement learning and most recent models. Te environment is presented in Section 3. Te main contribution of the paper is described in Sections 4. Results are presented in Section 5. Section 6 presents the main conclusions drawn from the work. 2 ",
    "Related Work": "RELATED WORK Below we give a brief introduction to reinforcement learning and the models used into our system architecture. arXiv:1712.04101v1  [cs.LG]  12 Dec 2017 SAC 2018, April 9\u201313, 2018, Pau, France N. Bougie et al. 2.1 Reinforcement Learning Reinforcement learning consists of an agent learning a policy by interacting with an environment. At each time-step the agent receives an observation st and choose an action at . Te agent gets a feedback from the environment called a reward rt . Given this reward and the observation, the agent can update its policy to improve the future rewards. Given a discount factor \u03b3, the future discounted reward, called return Rt , is defned as follows : Rt = T \ufffd t\u2032=t \u03b3 t\u2032\u2212trt\u2032 (1) Te goal of reinforcement learning is to learn to select the action with the maximum return Rt achievable for a given observation [19]. From Equation (1), we can defne the action value Q\u03c0 at a time t as the expected reward for selecting an action a for a given state st and following a policy \u03c0. Q\u03c0 (s,a) = E [Rt | st = s,a] (2) Te optimal policy is defned as selecting the action with the optimal Q-value, the highest expected return, followed by an optimal sequence of actions. Tis obeys the Bellman optimality equation: Q\u2217(s,a) = E \ufffd r + \u03b3 max a\u2032 Q\u2217(s \u2032,a \u2032) | s,a \ufffd (3) When the state space or the action space is too large to be represented, it is possible to use an approximator to estimate the actionvalue Q\u2217(s,a): Q\u2217(s,a) \u2248 Q(s,a;\u03b8) (4) Neural networks are a common way to approximate the actionvalue. Te parameters of the neural network \u03b8 can be optimized to minimize a loss function Li defned as the expected temporal diference error of Equation (3): Li(\u03b8i) = Es,a,r,s\u2032 \ufffd (yi \u2212 Q(s,a;\u03b8i))2\ufffd (5) where yt = rt + \u03b3 maxa\u2032 Q\u03b8tar\u0434et (st+1,a \u2032) Te gradient of the loss function with respect to the weights is the following : \u2207\u03b8iLi(\u03b8i) = Es,a,r,s\u2032 \ufffd (r + \u03b3 maxa\u2032 Q(s \u2032,a \u2032;\u03b8i\u22121) \u2212 Q(s,a;\u03b8i))\u2207\u03b8iQ(s,a;\u03b8i) \ufffd (6) Mnih et al. (2013) used this idea and created the famous method called Deep Q-learning (DQN) [14]. However, the learning may be slow due to the propagation of the reward to the previous states and actions. Similarly, the value function V \u03c0 (s) which represents the expected return for a state s following a policy \u03c0 is defned as follows: V \u03c0 (s) = E [Rt | st = s] (7) Some reinforcement learning models such as Actor-Critic or Dueling Network decompose the Q-values Q(s,a) into two more fundamental values, the value functionV (s) and the advantage functionA(a,s) which is the beneft of taking an action compared to the others. Figure 1: Actor-critic model A(s,a) = Q(s,a) \u2212 V (s) (8) 2.2 Asynchronous Advantage Actor-Critic (A3C) It was shown that combining methods of deep learning and reinforcement learning is very unstable. To deal with this challenge, many solutions store the agent\u2019s data into a memory, then the data can be batched from the memory. It is done because sequences of data are highly correlated and can lead to learn from its mistakes resulting in a worse and worse policy. A3C [13] avoids computational and memory problems by using asynchronous learning. It allows the usage of on-policy reinforcement learning algorithms such as Q-learning [24] or advantage actor-critic. Te learning is stabilized without using experience replay and the training time is reduced linearly in the number of learners. Te learners of A3C which use their own copy of the environment are trained in parallel. Each process will learn a diferent policy and hence will explore the environment in a diferent way leading to a much more efcient exploration of the environment than with a replay memory. A process updates its own policy based on an advantage actor-critic model [8] (Figure 1). Te actor-critic model is composed by an actor which acts out a policy and a critic which evaluates the policy. Te main thread is updated periodically using the accumulated gradients of the diferent processes. Te critic takes as input the state and the reward and outputs a score to criticize the current policy. In the case of advantage actor-critic model, the critic estimates the advantage function which requires to estimate V and Q. Te actor does not have access to the reward but only to the state and the advantage value outputed by the critic. Contrary to the critic which is value based, the actor directly works into the policy space and changes the policy towards the best direction estimated by the critic. Optimization techniques such as stochastic gradient descent are used to fnd \u03b8 that maximizes the policy objective function J(\u03b8). Te policy gradient objective function \u2207\u03b8 J(\u03b8) is defned as follows: \u2207\u03b8 J(\u03b8) = E\u03c0,\u03b8 \ufffd \u2207\u03b8lo\u0434\u03c0\u03b8 (s,a)Aw(s,a) \ufffd (9) where Aw(s,a) is a long term estimation of the reward to allow the actor to go in the direction that the critic considers the best. Deep Reinforcement Learning Boosted by External Knowledge SAC 2018, April 9\u201313, 2018, Pau, France Figure 2: Dueling network architecture 2.3 Dueling Network Te idea is to separately compute the advantage function and the value function and combine these two values at the fnal layer (Figure 2). Te dueling network [23] may not need to care about both values and the advantage at any given time. Te estimation of a state value is more robust by decoupling it from the necessity of being atached to a specifc action. Tis is particularly useful in states where its actions do not afect the environment in any relevant way. For example, moving lef or right only maters when a collision is upcoming. Te second stream, which estimates the advantage function values, is relevant when the model needs to make a choice over the actions in a state. Te Bellman\u2019s equation (3) becomes now: Q(s,a;\u03b8,\u03b1, \u03b2) = V (s,\u03b8, \u03b2)+(A(s,a;\u03b8,\u03b1)\u2212 max a\u2032 \u2208A A(s,a \u2032;\u03b8,\u03b1)) (10) And by changing the max by a mean: Q(s,a;\u03b8,\u03b1, \u03b2) = V (s,\u03b8, \u03b2) + (A(s,a;\u03b8,\u03b1) \u2212 1 |A| \ufffd a\u2032 A(s,a \u2032;\u03b8,\u03b1)) (11) With \u03b8 the shared parameters of the neural network, \u03b1 the parameters of the stream of the advantage function A and \u03b2 the parameters of the stream of the value function V. Since the output of the two streams produces a Q function, it can be trained with many existing algorithms such as Double Deep Q-learning (DDQN) [22] or SARSA [18]. Te main advantage is that for each update of the Q-values, the value function is updated whereas with traditional Q-learning only one action-value is updated. 3 TASK & ENVIRONMENT We built an environment on the top of the Malmo platform [5] to evaluate our idea. Malmo is an open-source platform that allows us to create scenarios with Minecraf engine. To test our model, we trained an agent to collect foods in a feld with obstacles. Te agent can only receive partial information of the environment from his viewpoint. We only use image frames to solve the scenario. An example of screenshot with the object recognition results is shown in Figure 3. Te goal of the agent is to learn to have a healthy diet. It involves to recognize the objects and learn to navigate into a 3D environment. Te task consists in picking up food from the ground for 30 seconds. Food is randomly spread across the environment and four obstacles are randomly generated. Each of the 20 kinds of food Figure 3: Screenshot of the environment has an associated reward when the agent picks it up. Tis reward is a number between +2 (healthy) and -2 (unhealthy). Tey are distributed equitably, meaning that a random agent should get a reward of 0. Te setings were: window size: 400 \u00d7 400 pixels, actions: turn lef, turn right, crouch, jump, move straight and move back , number of objects: 200, number of obstacles: 4. Te actions turn lef and turn right are continuous actions to make the learning smoother as consecutive frames are more similar. 4 SYSTEM ARCHITECTURE 4.1 General Idea Figure 4 describes the global architecture of our new framework called DRL-EK. It consists of four modules: an Object Recognition Module, a Reinforcement Learning Module, a Knowledge Based Decision Module, and an Action Selection Module. Te object recognition module identifes the objects within the current image and generates high-level features. Tese features of the environment are then used to augment the raw image input to the reinforcement learning module. In parallel, the knowledge based decision module selects another action by combining external knowledge and the object recognition module outputs. To manage the trade-of between these two sources of decision we use an action selection module. Te chosen action is then acted by the agent and the modules are updated from the obtained reward. 4.2 Object Recognition Module Injecting external knowledge requires to understand the scene at a high-level in order to be interpreted by a human. Te easiest way to understand an image is to identify the objects. For example, it is intuitive to give more importance to the actions turn or jump than the action move strai\u0434ht when an obstacle is in front of the agent. To recognize the objects, the module uses You Only Look Once (YOLO) [16][17] library which is based on a deep convolutional neural network. As input, we use an RGB image of size 400\u00d7400 pixels. YOLO predicts in real time the bounding boxes, the labels and confdence scores between 0 and 100 of the objects. An example SAC 2018, April 9\u201313, 2018, Pau, France N. Bougie et al. Objects Knowledge Based Decision Module Action1 Reinforcement Learning Module Action 2 Image Yolo Yolo Meta-features learning model A3C Q-learning Long time planning model Object Recognition Module Final action Action Selection Module Figure 4: Global architecture of DRL-EK is shown in Figure 3. We trained YOLO on a dataset of 25 000 images with twenty diferent classes corresponding to the food that is presented in the environment. Te model is trained of-line before starting the learning into the environment. Te neural network architecture is adapted from the one proposed by Redmon et al. (2016) for the Pascal VOC dataset [17]. In order to recognize small objects, the size of cells is decreased from 7 to 5 pixels and the number of bounding boxes for each cell is increased from 2 to 4. In addition to the identifed objects, the module creates feature information about the current frame. To generate these high-level abstraction features we combine the recognized objects and external knowledge. Tey are then used as input by the reinforcement learning module and the knowledge based decision module. We designed two types of features presence of objects and important area. 4.2.1 Presence Of Objects Features. Te frst type of features is a vector of booleans which indicates whether an object appears or not within the current image. Te size of this vector is the number of diferent objects in the environment. Since some objects are not helpful to solve the task, we can decide to only take some of the objects into account based on our knowledge about the task. 4.2.2 Important Area Features. As the position of objects is important, we encode information about objects within each area of Figure 5: Important areas of an image Inputs Figure 6: Injection of new features into the reinforcement learning module (A3C) the image. We split the image into k rectangles vertically and horizontally. So, the number of areas is k2 and for each one we compute a score (Figure 5). Te score of an area is the sum of the score of the objects within this area. External knowledge can be introduced by shaping the score of the objects. From our knowledge about the task, we manually defned the scores to indicate whether or not an object is important to solve the task. To tackle problems with partially observable environments, we keep track of recent information by concatenating the array of scores of the current frame with the arrays of the two previous frames. In our experiments, the top half of the images only contains the sky so we computed the important area features on the half botom of the images. We gave a score of -15/+5 to foods we think is unhealthy (cake,cookie) / healthy (meat, fruit) and 0 for the others. Tat way, if an area contains a healthy food such as a fruit and a sweet food, then the score of the area will be lower than an area containing only a fruit or no object. We set the number of rectangles to 3 (9 areas in total: 3\u00d73). We found that with a higher number of areas the amount of encoded information is bigger but information quality of each area is worse than with 3 areas. 4.3 Reinforcement Learning Module For a computer, learning from an image is difcult and requires a lot of training steps. To deal with it, the entry point of most of the reinforcement learning models is a recurrent convolutional neural network [3] to extract temporal and spatial features of the image. Deep Reinforcement Learning Boosted by External Knowledge SAC 2018, April 9\u201313, 2018, Pau, France We trained a deep reinforcement learning model to perform policy learning and we modifed the neural network structure to incorporate external knowledge. In addition to the image input, we injected presence of objects or important area features which are created by the object recognition module. In the neural network, we give to a Long Short Term Memory (LSTM) [4] the output of the last convolutional layer concatenated with the new features (Figure 6). Te next layers of the neural network are two separated fully-connected layers to estimate the value function V (s) and the policy \u03c0(a|st ). Te purpose is to help the model at the beginning of the training to recognize and focus on objects. Te new features augment the raw image input to the reinforcement learning model by adding high-level information. For example, from presence of objects features the model can decide which actions are allowed or not. If a door is detected some of the actions may become irrelevant such as jumpin\u0434. Te choice of the reinforcement learning model highly depends on the environment. Since the model at each time-step takes an input and outputs an action, we can easily substitute most of the reinforcement learning techniques such as Deep Q-learning (DQN) [14], Deep Deterministic Gradient Policy (DDPG) [12], Dueling Network [23] or Asynchronous Actor-Critic Agents(A3C) [13] by using a recurrent convolutional neural network as state approximator. A3C is the most suitable model to solve our task. We tested and empirically searched the best parameters such as a good convolutional neural network architecture and the choice of the optimizer of this model. It provides a baseline to evaluate the importance of each module of our architecture on the fnal policy. Working directly with 400 \u00d7 400 pixel images is too computationally demanding. We apply image preprocessing before training A3C. Te raw frame is resized to 200 \u00d7 200 pixels. To decrease the storage cost of the images we convert the image scale from 0 \u2212 255 to 0 \u2212 1. We set the number of workers of A3C to 3 and a convolutional recurrent neural network is used to approximate the states. Te reason why we use a recurrent neural network is because the environment is partially observable. Te input of the neural network of A3C estimator consists in a 200\u00d7200\u00d73 image. Te 4 frst layers convolve with the following parameters (flter: 32,32,32,32, kernel size: 8\u00d78,4\u00d74,3\u00d73,2\u00d72, stride size: 2,2,2,1) and apply a rectifer nonlinearity. It is followed by a LSTM layer of size 128 to incorporate the temporal features of the environment. Two separate fully connected layers predict the value function and a policy function, a distribution of probability over the actions. We use RMSProp [21] as optimization technique with \u03f5 = 10\u22126 and minibatches of size 32 for training. 4.4 Knowledge Based Decision Module We believe that the agent is not able to accurately understand and take into account the objects of the environment. A human can easily understand and make a decision from high-level features such as the utility or name of an object. Geting this level of abstraction is difcult but we can help the machine by giving it less low-level information such as color of pixels but more high-level information such as the importance of an area of the image. Moreover, when the reinforcement learning module is fed with the images and the presence of objects or important areas features, the training time is long due to the size of state space. Te knowledge based decision module is able to select an action using external knowledge and high-level features generated by the object recognition module and without direct access to the image. We propose two diferent approaches, a long time planning model or a meta-feature learning model. 4.4.1 Long Time Planning Model. Our approach to solving the task is based on planning a sequence of actions. We designed and developed a long time planning model without learning which combines traditional test-case algorithms and planning. Te model takes as input the probabilities and the bounding boxes of the objects detected within the current frame. We store in an array the sequence of planned actions. At each time-step, the model checks if the previously planned sequence of actions is still the optimal one and if it is not the case (for example the next action is jump but there is no obstacle) the algorithm updates it, otherwise the frst action in the array is returned. To update or plan a sequence of actions, the model uses the information about the objects and manually created rules. First, a test-case verifcation selects the possible actions. An example of simple rule is, if the object cookie is on the lef of the image then the action turn lef is forbidden. Ten, to decide of the action among the remaining actions we use a priority list. If the selected action is related to the movement, the model estimates the best angle and the necessary number of steps to perform it. Finally, the frst of the planned actions is returned. To decrease the number of rules we discretized the image space into four areas: center, lef, right, other. We designed 43 rules to prevent the agent from going in the direction of the food we think is dangerous. To avoid static behaviour, we give more priority to the actions turn lef, turn right, move straight than the others in the priority list. 4.4.2 Meta-feature Learning Model. In our previous approach, we manually create rules to reason on high-level features. To automatically learn the rules and select the optimal action from them, we use a deep reinforcement learning model such as DQN or dueling network. Unlike the reinforcement learning module which uses the image, the only input is high-level features such as important areas or presence of objects. As the input is much smaller than an image, a simple neural network can be trained to approximate the states. Te smaller number of parameters leads to a faster learning than a model trained from visual information. In experiments, we trained a dueling network combined with a double deep Q-learning (DDQN). It empirically gives a smoother learning than most of the other reinforcement learning models. A neural network approximates the states. It consists in 3 fully connected layers of size 100 with a rectifer nonlinearity activation function. Network was trained using the Adam algorithm [7], learning rate of 10\u22123 and minibatches of size 32. As input, we used a slightly modifed version of the important area features outputed by the object recognition module. To create important area features, we fltered the objects too far and the objects with a confdence score less than 0.25. Taking into account an object such as grass is ",
    "Experiments": "EXPERIMENTS We conducted several experiments for evaluating our architecture. In all our experiments, we set the discount factor to 1.0. According to our diferent tests, on average the best reward that a perfect agent can get in 30 seconds is 9. 5.1 Object Recognition We evaluated our object recognition module for understanding the correctness of obtained object information in the environment. Figure 7 reports the object recognition module performance. In this experiment, we measured the mean average precision (mAP) as the error metric. Te results are similar to the results presented by the authors (Redmon et al., 2017) [17] on the Pascal VOC. dataset [2]. We obtained a mean average precision of 53.47. Although other libraries could ofer higher performance, the real-time detection was the main criterion for selecting YOLO. We noticed that most of the errors are false positives (68.3%) whereas the false negatives (31.7%) are uncommon. It leads to an agent with a policy more greedy and safer. As shown in the fgure, the average precision is similar for every class. Te performance of YOLO is slightly afected by the complexity of the objects such as their shape, color or size. Figure 7: Average precision over all the classes obtained by the object recognition module Figure 8: Evolution of the reward of the long time planning model 5.2 Long Time Planning Model Next, we tested the long time planning model in the knowledge based decision module to evaluate the efectiveness of this approach. We only utilized the object recognition module and the knowledge based decision module in our framework. Te long time planning model performs much beter than a random agent with an average reward of 3.3 (Figure 8), since expected reward of random agent is 0. Te most likely cause of the wide variance is the difculty to handle all possible cases with manually created rules. For instance, the agent has difculty in gathering food near obstacles. Since there is no learning, the quality of the agent only depends of the quality of the rules and is not able to converge. On the other hand, from the frst episode the average reward is much higher than any other learning based models. ",
    "Evaluation": "Evaluation Finally, we report the average reward of our whole framework trained using the injection of important areas features into the reinforcement learning module and a meta-feature learning model ",
    "Conclusion": "CONCLUSION We proposed a new architecture to combine reinforcement learning with external knowledge. We demonstrated its ability to solve complex tasks in 3D partially observable environments with image as input. Our central thesis is enhancing the image by generating high-level features of the environment. Further benefts stem from efciently combining two sources of decision. Moreover, our approach can be easily adapted to solve new tasks with a very limited amount of human work. We have demonstrated the efcacy of our architecture to decrease the training time and to learn a beter and more efcient policy. In the future, a promising research area is building an agent that incorporates human feedback. Another challenge is how to integrate complex and structured external knowledge such as ontologies or textual data into our model. Finally, we are interested in extending our experiments to new environments such as VizDoom [6]. ",
    "References": "REFERENCES [1] Andrew G. Barto and Sridhar Mahadevan. 2003. Recent Advances in Hierarchical Reinforcement Learning. Discrete Event Dynamic Systems 13, 4 (2003), 341\u2013379. [2] Mark Everingham, Luc J. Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. 2010. Te Pascal Visual Object Classes (VOC) Challenge. International Journal of Computer Vision 88, 2 (June 2010), 303\u2013338. [3] Mathew J. Hausknecht and Peter Stone. 2015. Deep Recurrent Q-Learning for Partially Observable MDPs. CoRR abs/1507.06527 (2015). [4] Sepp Hochreiter and J\u00a8urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (Nov. 1997), 1735\u20131780. [5] Mathew Johnson, Katja Hofmann, Tim Huton, and David Bignell. 2016. Te Malmo Platform for Artifcial Intelligence Experimentation. In Proceedings of the Twenty-Fifh International Joint Conference on Artifcial Intelligence, New York, NY, USA, 9-15 July 2016, Subbarao Kambhampati (Ed.). IJCAI/AAAI Press, 4246\u20134247. [6] Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u00b4skowski. 2016. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In Proceedings of IEEE conference on Computational Intelligence and Games (CIG). IEEE, 1\u20138. [7] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. CoRR (2014). [8] Vijay R. Konda and John N. Tsitsiklis. 1999. Actor-Critic Algorithms. In Proceedings of Neural Information Processing Systems, Sara A. Solla, Todd K. Leen, and Klaus-Robert M\u00a8uller (Eds.). Te MIT Press, 1008\u20131014. [9] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. 2016. Building Machines Tat Learn and Tink Like People. Behavioral and Brain Sciences (2016), 1\u2013101. [10] Guillaume Lample and Devendra Singh Chaplot. 2017. Playing FPS Games with Deep Reinforcement Learning. In Proceedings of AAAI. 2140\u20132146. [11] Antonio Lieto, Christian Lebiere, and Alessandro Oltramari. 2017. Te knowledge level in cognitive architectures: Current limitations and possible developments. Cognitive Systems Research (2017). [12] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. ArXiv e-prints (Sept. 2015). arXiv:cs.LG/1509.02971 [13] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of International Conference on Machine Learning. 1928\u20131937. [14] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. ArXiv e-prints (Dec. 2013). arXiv:cs.LG/1312.5602 [15] Oltramari and Lebiere. 2012. Using Ontologies in a Cognitive-Grounded System: Automatic Action Recognition in Video-Surveillance. (2012). [16] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2015. You Only Look Once: Unifed, Real-Time Object Detection. CoRR abs/1506.02640. htp://arxiv.org/abs/1506.02640 [17] Joseph Redmon and Ali Farhadi. 2016. YOLO9000: Beter, Faster, Stronger. CoRR abs/1612.08242 (2016). [18] G. A. Rummery and M. Niranjan. 1994. On-Line Q-Learning Using Connectionist Systems. Technical Report. University of Cambridge. [19] Richard S Suton and Andrew G Barto. 1998. Reinforcement learning: An introduction. MIT press Cambridge. [20] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. 2017. A Deep Hierarchical Approach to Lifelong Learning in Minecraf. In Proceedings of AAAI Conference on Artifcial Intelligence. 1553\u20131561. [21] Tijmen Tieleman and Geofrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning (April 2012). [22] Hado van Hasselt, Arthur Guez, and David Silver. 2015. Deep Reinforcement Learning with Double Q-learning. CoRR abs/1509.06461 (2015). [23] Ziyu Wang, Tom Schaul, Mateo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In Proceedings of the 33rd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). 1995\u20132003. [24] Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-learning. Machine Learning 8, 3 (01 May 1992), 279\u2013292. ",
    "title": "Deep Reinforcement Learning Boosted by External Knowledge",
    "paper_info": "Deep Reinforcement Learning Boosted by External Knowledge\nNicolas Bougie\nNational Institute of Informatics\n2-1-2 Hitotsubashi, Chiyoda\nTokyo, Japan 101-8430\nnicolas-bougie@nii.ac.jp\nRyutaro Ichise\nNational Institute of Informatics\n2-1-2 Hitotsubashi, Chiyoda\nTokyo, Japan 101-8430\nichise@nii.ac.jp\nABSTRACT\nRecent improvements in deep reinforcement learning have allowed\nto solve problems in many 2D domains such as Atari games. How-\never, in complex 3D environments, numerous learning episodes\nare required which may be too time consuming or even impossible\nespecially in real-world scenarios. We present a new architecture\nto combine external knowledge and deep reinforcement learning\nusing only visual input. A key concept of our system is augmenting\nimage input by adding environment feature information and com-\nbining two sources of decision. We evaluate the performances of\nour method in a 3D partially-observable environment from the Mi-\ncrosof Malmo platform. Experimental evaluation exhibits higher\nperformance and faster learning compared to a single reinforcement\nlearning model.\nCCS CONCEPTS\n\u2022Computing methodologies \u2192 Reasoning about belief and\nknowledge; Sequential decision making; Neural networks; Partially-\nobservable Markov decision processes;\nKEYWORDS\nReinforcement Learning, Object Recognition, External Knowledge,\nDeep Learning, Knowledge Reasoning\nACM Reference format:\nNicolas Bougie and Ryutaro Ichise. 2018. Deep Reinforcement Learning\nBoosted by External Knowledge. In Proceedings of SAC 2018: Symposium on\nApplied Computing , Pau, France, April 9\u201313, 2018 (SAC 2018), 8 pages.\nDOI: 10.1145/3167132.3167165\n1\nINTRODUCTION\nReinforcement learning is a technique which automatically learns\na strategy to solve a task by interacting with the environment and\nlearning from its mistakes. By combining reinforcement learning\nand deep learning to extract features from the input, a wide variety\nof tasks such as Atari 2600 games [14] are efciently solved. How-\never, these techniques applied to 2D domains struggle in complex\nenvironments such as three-dimensional virtual worlds resulting a\nprohibitive training time and an inefcient learned policy.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permited. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specifc permission\nand/or a fee. Request permissions from permissions@acm.org.\nSAC 2018, Pau, France\n\u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n978-1-4503-5191-1/18/04...$15.00\nDOI: 10.1145/3167132.3167165\nA powerful recent idea to tackle the problem of computational\nexpenses is to modularise the models into an ensemble of experts\n[10]. Since each expert focuses on learning a stage of the task,\nthe reduction of the actions to consider leads to a shorter learning\nperiod. Although this approach is conceptually simple, it does not\nhandle very complex environments and environments with a large\nset of actions.\nA similar idea of extending the information extracted from low-\nlevel architectural modules [11] with high-level ones have been\npreviously used in the area of cognitive systems [15] but does not\ndirectly relies on RL and was limited to a supervised classifcation\nproblem. Te idea was to leverage information about videos with\nexternal ontologies to detect events in videos.\nAnother technique is called Hierarchical Learning [20][1] and is\nused to solve complex tasks, such as \u201dsimulating human brain\u201d [9].\nIt is inspired by human learning which uses previous experiences\nto face new situations. Instead of learning directly the entire task,\ndiferent sub-tasks are learned by the agent. By reusing knowledge\nacquired from the previous sub-tasks, the learning is faster and\neasier. Some limitations are the necessity to re-train the model\nwhich is time consuming and problems related to catastrophic\nforgeting of knowledge on previous tasks.\nIn this paper, our approach focuses on combining deep reinforce-\nment learning and external knowledge. Using external knowledge\nis a way to supervise the learning and enhance information given\nto the agent by introducing human expertise. We augment the\ninput of a reinforcement learning model whose input is raw pixels\nby adding high-level information created from simple knowledge\nabout the task and recognized objects. We combine this model with\na knowledge based decision algorithm using Q-learning [24]. In\nour experiments, we demonstrate that our framework successfully\nlearns in real time to solve a food gathering task in a 3D partially\nobservable environment by only using visual inputs. We evaluate\nour technique on the Malmo platform built on top of a 3D virtual\nenvironment, Minecraf. Our model is especially suitable for tasks\ninvolving navigation, orientation or exploration, in which we can\neasily provide external knowledge.\nTe paper is organized as follows. Section 2 gives an overview of\nreinforcement learning and most recent models. Te environment\nis presented in Section 3. Te main contribution of the paper is\ndescribed in Sections 4. Results are presented in Section 5. Section\n6 presents the main conclusions drawn from the work.\n2\nRELATED WORK\nBelow we give a brief introduction to reinforcement learning and\nthe models used into our system architecture.\narXiv:1712.04101v1  [cs.LG]  12 Dec 2017\n",
    "GPTsummary": "\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work is to propose a method that combines external knowledge and deep reinforcement learning to improve the performance in complex 3D environments.\n                     \n- (2): Innovation point: The article innovatively proposes a method that augments the input of deep reinforcement learning models with high-level information about the environment and recognized objects, improving their performance. Performance: The proposed method shows higher performance and faster learning compared to a single RL model in a 3D partially-observable environment. Workload: The article suggests that their approach can be easily adapted to solve new tasks with a limited amount of human work. However, there is a limitation in how to integrate complex and structured external knowledge such as ontologies or textual data into the model.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to propose a method that combines external knowledge and deep reinforcement learning to improve the performance in complex 3D environments.\n                     \n- (2): Innovation point: The article innovatively proposes a method that augments the input of deep reinforcement learning models with high-level information about the environment and recognized objects, improving their performance. Performance: The proposed method shows higher performance and faster learning compared to a single RL model in a 3D partially-observable environment. Workload: The article suggests that their approach can be easily adapted to solve new tasks with a limited amount of human work. However, there is a limitation in how to integrate complex and structured external knowledge such as ontologies or textual data into the model.\n\n\n"
}