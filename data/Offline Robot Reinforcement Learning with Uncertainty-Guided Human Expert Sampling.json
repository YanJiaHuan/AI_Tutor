{
    "Abstract": "Abstract Recent advances in batch (of\ufb02ine) reinforcement learning have shown promising results in learning from available of\ufb02ine data and proved of\ufb02ine reinforcement learning to be an essential toolkit in learning control policies in a model-free setting. An of\ufb02ine reinforcement learning algorithm applied to a dataset collected by a suboptimal non-learning-based algorithm can result in a policy that outperforms the behavior agent used to collect the data. Such a scenario is frequent in robotics, where existing automation is collecting operational data. Although of\ufb02ine learning techniques can learn from data generated by a sub-optimal behavior agent, there is still an opportunity to improve the sample complexity of existing of\ufb02ine reinforcement learning algorithms by strategically introducing human demonstration data into the training process. To this end, we propose a novel approach that uses uncertainty estimation to trigger the injection of human demonstration data and guide policy training towards optimal behavior while reducing overall sample complexity. Our experiments show that this approach is more sample ef\ufb01cient when compared to a naive way of combining expert data with data collected from a sub-optimal agent. We augmented an existing of\ufb02ine reinforcement learning algorithm Conservative Q-Learning with our approach and performed experiments on data collected from MuJoCo and OffWorld Gym learning environments. 1 ",
    "Introduction": "Introduction The \ufb01eld of of\ufb02ine reinforcement learning has emerged in the past few years as a way of training reinforcement learning (RL) agents from previously collected experiences and without the need for an interactive feedback loop with the environment [20]. Since the \ufb01rst breakthrough in the \ufb01eld of deep reinforcement learning [24], this \ufb01eld has seen considerable progress over the years, achieving super-human performance in computer games [23, 32, 31, 35], solving robotics control problem [2, 19, 11], improving recommendation systems [4, 13], healthcare [12], autonomous driving [25], and process optimization [27]. However, one of the limitations of online reinforcement learning is that it relies on interaction with a dynamical system or environment, and such online interactions can be expensive, especially in domains such as real-world robotics. Similar to supervised learning, where an algorithm trains a model by performing batch model updates, of\ufb02ine reinforcement learning performs model updates by sampling batches from a dataset of state, action, reward and next state (SARS) tuples. The model is then trained towards the standard reinforcement learning objective of maximizing expected future discounted reward, but also with a secondary objective of either keeping the learned policy close to the distribution of the behavior policy that was used to collect the data [9, 16]. We propose a method that allows to reduce the overall of\ufb02ine learning sample complexity by tracking model uncertainty measured from an ensemble of Q-networks. High level of uncertainty shows that the agent has entered an unexplored part of the state-action space indicating that introducing human demonstrations at this moment would have the most impact on learning. The algorithm arXiv:2212.08232v1  [cs.LG]  16 Dec 2022 ",
    "Related Work": "Related Work Our work is most effective when applied with an algorithm that addresses some of the inherent issues of of\ufb02ine reinforcement learning like extrapolation error, q-value overestimation, and others. Extrapolation error is caused when there is a mismatch between the distribution of the dataset collected by a behavior agent and the state-action visitation of the policy being trained. Fujimoto et al. [9] introduced an algorithm called Batch-Constrained Q-Learning where they propose a solution to reduce the extrapolation error in reinforcement learning by training a policy such that it is constrained to stay close to the behavior policy. Techniques like BEAR-QL [16] prevent overestimation of q-value by bootstrapping error reduction, while CQL, and algorithm introduced by Kumar et al. [17], introduced a lower bound on the q-value to prevent over-estimation. This work assumes that this algorithm is applied on a static dataset. To the best of our knowledge, the current state of the art algorithms like CQL and Implicit Q-Learning (IQL) [14] have been tested on static datasets only and there is no research work that demonstrates the application of of\ufb02ine reinforcement learning techniques on dynamic datasets. Over the recent years, signi\ufb01cant contributions have been made to the \ufb01eld of of\ufb02ine reinforcement learning towards solving such inherent challenges with the technique as deadly-triad issue [33], the issue with q-value function overestimation for out-of-distribution (OOD) state-action pairs [9], over\ufb01tting and under\ufb01tting issues [6], to name a few. To solve the problem of q-value function overestimation for OOD state-action pair, algorithms like conservative Q-learning (CQL) implement a learning objective that favors conservative estimates of the q-value function. The deadly-triad problem was solved in various works, viz. Fujimoto et al. [9], Lillicrap et al. [22] and Haarnoja et al. [11]. The formulation of over\ufb01tting and under\ufb01tting has been established in work by Kumar et al. [18], where several approaches to overcome these problems have been identi\ufb01ed. Since this work is based on mixing data generated by different behavior agents to create a single dataset, we rely on the results of Schweighofer et al. [28], Fu et al. [7], and Gulcehre et al. [10] that demonstrate the feasibility of training an of\ufb02ine RL model on a mixed dataset that mixes SARS tuples obtained by different behavior policies. In the online RL setting, uncertainty estimation has been leveraged for ef\ufb01cient exploration [5], and for improving exploration by reducing the uncertainty of learned implicit reward function [21]. In the of\ufb02ine RL setting, the work by An et al. [1] quanti\ufb01es the uncertainty of Q-value estimates by using an ensemble of Q-networks. However, in this work, uncertainty estimation is leveraged as a penalization term in Q-learning, while in our proposed method uncertainty acts as an indicator of the learning progress. Wu et al. [36] proposed the Uncertainty Weighted Actor-Critic algorithm that down-weights the contribution of OOD state-action pairs in training. This work achieves good performance gains on a dataset with narrow human demonstrations, however, in their work, data mixing is done by mixing the human demonstration data with data generated by imitating the human data. Some recent work [29], [8] have explored sampling strategies in of\ufb02ine RL using rank-based sampling or sampling prioritized experience replay where priority is given to sample transitions with lower epistemic uncertainty. Such methods may over-sample good samples. 2 Preliminaries Reinforcement learning is a machine learning technique through which an agent learns to solve a task by learning from interactions with an environment. In the domain of robotic control and behavior most of the algorithms are based on the Markov Decision Process. A Markov Decision Process or MDP is de\ufb01ned as a tuple comprising of seven elements \u2013 (S, A, T , r, \u03b3, S0, H), where S is the state space, A is the action space, T is the state transition probability function T = P(st+1|st, at), r is the environment reward function r: S \u00d7 A \u2192 R1, \u03b3 is the discount value, S0 is the start state distribution 2 and H is the horizon length. In this work, we consider an MDP setting with a non-zero time horizon and discounted cumulative rewards. A reinforcement learning algorithm interacts with an environment to learn a policy \u03c0, which maximizes the reinforcement learning objective J (\u03c0)=E[ H \ufffd t=0 \u03b3trt]. The two important functions that help estimate the value of a state or a state and action pair are the value function V\u03c0(s)=E[ H \ufffd t=0 \u03b3trt|s0=s] and the Q-function Q\u03c0(s,a)=E[ H \ufffd t=0 \u03b3trt|s0=s,a0=a]. There are two broad categories of online reinforcement learning algorithms, viz. on-policy and off-policy. In an on-policy setting, the algorithm generates an action for a particular state based on the current policy. The recorded trajectory data is then used to update the current policy. Whereas in an off-policy setting there is a concept of a replay buffer D, that accumulates data from different policies and the policy updates are made by sampling from the replay buffer instead of generating actions from the current policy. The standard Q-learning objective is to the minimize the Bellman error which is de\ufb01ned as L(\u03b8)=(Q\u03b8(st,at)\u2212(rt+\u03b3 maxa Q(st+1,a))) [33]. In of\ufb02ine reinforcement learning, D represents the entire replay dataset and in Q-learning setting the objective is to minimize the following loss function based on Bellman error: L(\u03b8) = E(s,a,r,s\u2032)\u223cD[(Q\u03b8(s, a) \u2212 (r + \u03b3Ea\u2032\u223c\u03c0(\u00b7|s\u2032)[Q\u03b8\u2032(s\u2032, a\u2032)]))2] (1) where \u03b8\u2032 are the parameters of the target Q-network, softly updated for algorithm stability, while \u03b8 are the parameters of the running Q-network. In addition to the general reinforcement learning objective or the Q-learning objective, the algorithms usually have a secondary objective of keeping the learned policy close to the state-action distribution of the behavior policy. This is done to avoid overestimation of the Q-value, which happens when the Q-estimate error propagates during the bootstrapping, as shown in Equation 1. Over-estimation of Q-values can destabilize the training. Sub-optimal agent (SOA) is represented by a behavior policy trained by QR-DQN learning algorithm to approximately 60% of optimal performance. While we assume that a human is an optimal (or close to) agent, the SOA represents a data source that has lower performance guarantees, but is easier to obtain. 3 Uncertainty-Guided Sampling in Of\ufb02ine RL The problem we are addressing in this work is the reduction of sample complexity of existing of\ufb02ine RL algorithms. In robotics, solving a task with a minimal number of samples is a goal most practitioners would appreciate. In many cases, there is either an absence of an optimal approach or a human is an expert, but creating a human demonstration dataset is prohibitively expensive. Combining data from expert and non-expert demonstrations into a mixed dataset can signi\ufb01cantly reduce the overall sample complexity [28], however in many practical applications the amount of human demonstrations necessary to reach close to optimal behavior remains prohibitively high. In this work, we propose a method to strategically choose the order in which we pick samples from two replay buffers, one with data collected by a sub-optimal agent (DSOA) and another with data collected from human demonstrations (DH). Both are available ahead of time. This allows us to boost the speed of learning twice and reduce the overall sample complexity by 5 times, minimizing the required total sample size of both replay buffers required to reach close-to-optimal performance. 3.1 Sample complexity in of\ufb02ine RL setting Of\ufb02ine RL algorithms learn a policy by performing policy updates on data sampled from an existing dataset. A dataset consists of trajectories collected by performing rollouts in an environment using a behavior policy \u00b5. A dataset is usually of the form: [(s1 0, a1 0, r1 0, . . . , s1 H, a1 H, r1 H), . . . , (s1 0, a1 0, r1 0, . . . , sN H, aN H, rN H)], where N is the total number of episodes. A sample-ef\ufb01cient algorithm requires a smaller dataset than an algorithm with high sample complexity. Sample complexity bounds are usually computed based on MDP attributes such as the horizon length H, the state space S, the action space A, etc. We will make the single policy concentrability assumption about the behavior policy as de\ufb01ned in [26, 37]. 3 The single policy concentrability assumption is de\ufb01ned as follows: given a reference policy \u00b5 and an optimal policy \u03c0\u2217, \u00b5 is said to satisfy the assumption when max t\u2208[1..H],(s,a)\u2208S\u00d7A d\u03c0\u2217 t (s, a) d\u00b5 t (s, a) \u2264 C\u2217, \u2200s \u2208 S, a \u2208 A (2) for some deterministic optimal policy \u03c0\u2217 and coef\ufb01cient C\u2217, where d is the state-action distribution of a policy. Intuitively the assumption requires there is a constant C\u2217 (concentrability coef\ufb01cient) such that for every possible state-action pair the ratio of probabilities of said state-action being in d\u03c0 and d\u00b5 is not higher than the constant C\u2217. The concentrability coef\ufb01cient C\u2217 \u2208 [1, \u221e) is the smallest value that satis\ufb01es Equation 2. When the reference policy is exactly equal to the optimal policy \u00b5 = \u03c0\u2217 the concentrability coef\ufb01cient C\u2217 = 1. C\u2217 estimates the difference between the state-action distribution density under of reference (behavior) policy and the optimal policy. Recent works in the \ufb01eld of sample complexity analysis of of\ufb02ine RL algorithms are based on the pessimism principle for value iterations [30, 37] and express the sample complexity of an of\ufb02ine RL algorithm as a function of S, A, H and C\u2217, where the upper bound on the number of episodes in the dataset D is O(f(S, A, H, C\u2217)). The theoretical foundation of our approach relies on two assumptions. First, the algorithms need to satisfy the single policy concentrability assumption, so that their sample complexity could be expressed as a function of S, A, H and C\u2217. Second, we assume that the human demonstrator is an expert and the behavior policy \u00b5H corresponding to human policy is very close to an optimal policy \u03c0\u2217 such that the following condition is satis\ufb01ed, C\u2217 \u00b5H < C\u2217 \u00b5, \u2200\u00b5 \u2208 M (3) where M is a family of non-expert behavior policies. Intuitively, inequality 3 states that if an algorithm\u2019s sample complexity can be expressed in terms of concentrability, and the assumption that human expert is close to optimal is satis\ufb01ed, then the concentrability coef\ufb01cient is lower when the algorithm is trained on human data. 3.2 Uncertainty estimation In online reinforcement learning, uncertainty estimation has been leveraged for effective exploration strategies such as upper con\ufb01dence bound exploration via Q-ensembles [5]. In an of\ufb02ine reinforcement learning setting, uncertainty estimation has been leveraged to act conservatively, and select paths with low uncertainty [3]. Our approach de\ufb01nes uncertainty in of\ufb02ine reinforcement learning as estimated variance of the Q-estimate of a given (s, a) pair over an ensemble of M Q-networks trained on the same data. In an ensemble of M Q-networks that are trained to minimize the objective de\ufb01ned in Equation 1 we de\ufb01ne point estimators for the mean \u00b5= 1 M \ufffdM i=1[Q\u03b8i(s,a)] and variance \u03c32= 1 M M \ufffd i=1 (Q\u03b8i(s,a)\u2212\u00b5(s,a))2 of the Q-estimates. The variance \u03c32 characterizes the uncertainty in Qfunction estimates and re\ufb02ects the state of the training. At the start of the training, the Q-function estimates are expected to be noisy and have high variance. As the training progresses, the Q-network estimates become more accurate in the parts of the state space that were visited by the learning algorithm as the critic or Q-estimator improves. 3.3 Of\ufb02ine reinforcement learning with Uncertainty-Guided Expert Sampling Introducing expert demonstrations into the training dataset helps reduce the sample complexity and also helps improve performance when compared to a mixed dataset. Having de\ufb01ned sample complexity analysis and uncertainty estimation in the previous sections, we propose the UncertaintyGuided Expert Sampling (UGES) algorithm (see Supplementary Algorithm 1) that leads to more ef\ufb01cient use of available human data in conjunction with of\ufb02ine data collected from a sub-optimal behavior policy. Above-threshold uncertainty in the model ensemble triggers the algorithm to sample the next SARS tuple from the replay buffer DH containing human demonstrations. Strategic sampling allows to keep the buffer of human demonstrations small while providing the same bene\ufb01t to the overall learning process as a naive sampling on a larger human dataset would. 4 ",
    "Experimental Results": "Experimental ",
    "Results": "Results We experimentally validate that uncertainty-based sampling strategy allows for a signi\ufb01cant reduction in overall sample complexity and explore the bene\ufb01ts of combining the two sources of of\ufb02ine data. We compare two ways of mixing the data during learning: a naive (random) combination strategy is compared against the UGES method in terms of speed of learning, resulting agent performance, overall sample complexity, and amount of human data required to reach close-to-optimal performance. (a) MuJoCo Cheetah (b) MuJoCo Ant (c) OffWorld Gym Monolith Figure 1: Learning environments we used to test the uncertainty-guided expert sampling approach. The experiments were performed using datasets generated from three learning tasks in simulated environments shown on Figure 1. In addition to the standard MuJoCo tasks Cheetah and Ant (D4RL benchmark datasets [7]), UGES was tested in a simulated version of the OffWorld Gym Monolith environment that contains a vision-based task: a mobile robot explores the environment and gets rewarded when it reaches a monolith (goal) in the center of the \ufb01eld. A sparse reward of +1 is given when the robot is within a small distance from the goal with no step penalty. To facilitate a fair comparison, we use the number of successful trajectories as the characteristic of a size of a dataset. A successful trajectory is de\ufb01ned as one that leads to a positive reward (see Supplementary Figure 3a for a comparison between training on human demonstrations (expert data) versus on data collected by a sub-optimal agent. In all experiment we maintain the 5:1 ratio of suboptimal to expert data. In Cheetah and Ant experiments the dataset was 320, 000 successful trajectories from the suboptimal agent and 80, 000 expert ones. In the Monolith environment the of\ufb02ine data consisted of 10, 000 SOA trajectories and 2, 500 expert human demonstrations. In our 5 ",
    "Conclusion": "Conclusion Generating a large dataset for of\ufb02ine RL can could be time-consuming, and algorithms that reduce sample requirements can make a difference. Human demonstrations provide great learning signal for Of\ufb02ine RL, but collecting such data is prohibitively expensive, especially in real-world robotics. These constraints lead us to consider using a combination of two sources of of\ufb02ine data: a limited amount of \u201cexpensive\" human demonstrations with a dataset of \u201ccheap\" autonomously collected experiences. In this work we show how uncertainty estimation can guide strategic introduction of the human samples into the learning process leading to signi\ufb01cant reduction in overall sample complexity. Limitations The theoretical analysis in Section 3.1 is based on the assumption that the sample complexity of the CQL algorithm (and other of\ufb02ine RL algorithms that are being used in practice) can be expressed in terms of a set of MDP characteristics and the concentrability coef\ufb01cient. However, to our knowledge, this assumption has not been explicitly proven for the CQL algorithm. Due to a wide range of experimental conditions, we relied on data generated by interactions with a simulated environment, while our main aim is to make of\ufb02ine RL sample complexity suf\ufb01ciently low to facilitate learning in the real physical world. The robotic benchmark learning environment chosen 6 ",
    "References": "References [1] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based of\ufb02ine reinforcement learning with diversi\ufb01ed q-ensemble. Advances in Neural Information Processing Systems, 34, 2021. [2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020. [3] Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in \ufb01xed-dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020. [4] Jia-Wei Chang, Ching-Yi Chiou, Jia-Yi Liao, Ying-Kai Hung, Chien-Che Huang, Kuan-Cheng Lin, and Ying-Hung Pu. Music recommender using deep embedding-based features and behavior-based reinforcement learning. Multimedia Tools and Applications, 80(26):34037\u201334064, 2021. [5] Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-ensembles. arXiv preprint arXiv:1706.01502, 2017. [6] Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Of\ufb02ine reinforcement learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919, 2021. [7] Justin Fu, Aviral Kumar, O\ufb01r Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. [8] Yuwei Fu, Di Wu, and Benoit Boulet. Benchmarking sample selection strategies for batch reinforcement learning. 2021. [9] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pages 2052\u20132062. PMLR, 2019. [10] Caglar Gulcehre, Sergio G\u00f3mez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021. [11] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018. [12] Pierre Humbert, Julien Audiffren, Cl\u00e9ment Dubost, and Laurent Oudre. Learning from an expert. In Porceedings of 30th Conference on Neural Information Processing Systems (NIPS), pages 1\u20135, 2016. [13] Wacharawan Intayoad, Chayapol Kamyod, and Punnarumol Temdee. Reinforcement learning for online learning recommendation system. In 2018 Global Wireless Summit (GWS), pages 167\u2013170. IEEE, 2018. [14] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Of\ufb02ine reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021. [15] Ashish Kumar, Toby Buckley, John B Lanier, Qiaozhi Wang, Alicia Kavelaars, and Ilya Kuzovkin. Offworld gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research. arXiv preprint arXiv:1910.08639, 2019. [16] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019. 7 [17] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for of\ufb02ine reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020. [18] Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A work\ufb02ow for of\ufb02ine model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021. [19] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International journal of robotics research, 37(4-5):421\u2013436, 2018. [20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Of\ufb02ine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [21] Xinran Liang, Katherine Shu, Kimin Lee, and Pieter Abbeel. Reward uncertainty for exploration in preference-based reinforcement learning. arXiv preprint arXiv:2205.12401, 2022. [22] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015. [25] Daniel Chi Kit Ngai and Nelson Hon Ching Yung. A multiple-goal reinforcement learning method for complex vehicle overtaking maneuvers. IEEE Transactions on Intelligent Transportation Systems, 12(2): 509\u2013522, 2011. [26] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging of\ufb02ine reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34, 2021. [27] Manuel Schneckenreither and Stefan Haeussler. Reinforcement learning methods for operations research applications: The order release problem. In International Conference on Machine Learning, Optimization, and Data Science, pages 545\u2013559. Springer, 2018. [28] Kajetan Schweighofer, Markus Hofmarcher, Marius-Constantin Dinu, Philipp Renz, Angela Bitto-Nemling, Vihang Patil, and Sepp Hochreiter. Understanding the effects of dataset characteristics on of\ufb02ine reinforcement learning. arXiv preprint arXiv:2111.04714, 2021. [29] Yichen Shen, Zhou Fang, Yunkun Xu, Yu Cao, and Jiangcheng Zhu. A rank-based sampling framework for of\ufb02ine reinforcement learning. In 2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI), pages 197\u2013202. IEEE, 2021. [30] Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for of\ufb02ine reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890, 2022. [31] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. [32] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017. [33] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. [34] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012. [35] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019. 8 [36] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for of\ufb02ine reinforcement learning. arXiv preprint arXiv:2105.08140, 2021. [37] Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy \ufb01netuning: Bridging sampleef\ufb01cient of\ufb02ine and online reinforcement learning. Advances in neural information processing systems, 34, 2021. 9 Supplementary Materials Tables and Figures Behavior policy Successful trajectories Time Steps Human 200 5,000 Suboptimal agent 300 13,500 Suboptimal agent 500 21,000 Suboptimal agent 900 40,000 Suboptimal agent 1,400 60,000 Suboptimal agent 1,600 70,000 Table 1: Correspondence between the number of successful trajectories and the total number of time steps in the dataset collected from the OffWorld Gym environment. (a) With the same number of successful trajectories in the dataset, the trajectories provided by a human expert lead to faster learning and higher \ufb01nal performance. (b) In OffWorld Gym Monolith experiment 5x human expert data were required for a naive sampling strategy to reaches a level of performance similar to UGES. Figure 3: Supplementary experiments. (a) Comparing learning performance using human data versus relying on sub-optimal agent data only. (b) We conducted a set of experiment gradually increasing the amount of of\ufb02ine data available to the naive strategy to mark the moment when the performance reaches that of UGES. Parameters The table 2 contains the values of various parameters used in our MuJoCo experiments. Parameters used in the Monolith environment are in table 3. 10 Parameter Value Number of Ensemble, M 10 Uncertainty threshold, \u03f5 16 Actor learning rate, \u03b7\u03c0 0.0001 Critic learning rate, \u03b7Q 0.0003 Batch Size 256 Number of iterations, E 36 Gamma, \u03b3 0.99 RL Algorithm Soft-Actor Critic Hidden layer sizes [256, 256] Table 2: Values of various parameters used in experiments. Parameter Value Number of Ensemble, M 10 Uncertainty threshold, \u03f5 16 Critic learning rate, \u03b7Q 0.0001 Batch Size 32 Number of iterations, E 36 Gamma, \u03b3 0.99 RL Algorithm QR-DQN Table 3: Values of various parameters used in experiments. 11 ",
    "title": "Supplementary Materials",
    "paper_info": "Supplementary Materials\nTables and Figures\nBehavior policy\nSuccessful trajectories\nTime Steps\nHuman\n200\n5,000\nSuboptimal agent\n300\n13,500\nSuboptimal agent\n500\n21,000\nSuboptimal agent\n900\n40,000\nSuboptimal agent\n1,400\n60,000\nSuboptimal agent\n1,600\n70,000\nTable 1: Correspondence between the number of successful trajectories and the total number of time\nsteps in the dataset collected from the OffWorld Gym environment.\n(a) With the same number of successful trajectories in\nthe dataset, the trajectories provided by a human expert\nlead to faster learning and higher \ufb01nal performance.\n(b) In OffWorld Gym Monolith experiment 5x\nhuman expert data were required for a naive sam-\npling strategy to reaches a level of performance\nsimilar to UGES.\nFigure 3: Supplementary experiments. (a) Comparing learning performance using human data versus\nrelying on sub-optimal agent data only. (b) We conducted a set of experiment gradually increasing\nthe amount of of\ufb02ine data available to the naive strategy to mark the moment when the performance\nreaches that of UGES.\nParameters\nThe table 2 contains the values of various parameters used in our MuJoCo experiments.\nParameters used in the Monolith environment are in table 3.\n10\n",
    "GPTsummary": "- (1): This paper focuses on the task of offline robot reinforcement learning, which aims to train control policies from previously collected experiences without the need for an interactive feedback loop with the environment. Offline reinforcement learning has seen considerable progress over the years but still suffers from high sample complexity. \n\n- (2): In the past, offline reinforcement learning algorithms have been applied to datasets collected by a suboptimal non-learning-based algorithm, resulting in a policy that outperforms the behavior agent used to collect the data. However, there is still an opportunity to improve the sample complexity of existing offline reinforcement learning algorithms by introducing human demonstration data into the training process. The proposed approach in this paper uses uncertainty estimation to trigger the injection of human demonstration data and guide policy training towards optimal behavior while reducing overall sample complexity. This approach is shown to be more sample efficient compared to a naive way of combining expert data with data collected from a sub-optimal agent. \n\n- (3): The proposed approach uses an existing offline reinforcement learning algorithm, Conservative Q-Learning, and augments it with uncertainty estimation to guide the injection of human demonstration data. High levels of uncertainty are tracked through an ensemble of Q-networks, which indicate that the agent has entered an unexplored part of the state-action space, indicating that introducing human demonstrations at this point would have the most impact on learning.\n\n- (4): Experiments were performed on data collected from MuJoCo and OffWorld Gym learning environments, and the results showed that the proposed approach achieved better performance and was more sample-efficient compared to existing methods. The performance supports the goal of reducing sample complexity and improving the effectiveness of offline robot reinforcement learning.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work lies in proposing an approach for offline robot reinforcement learning that utilizes uncertainty estimation to guide the injection of human demonstration data, reducing the overall sample complexity while improving the effectiveness of policy training.\n                     \n- (2): Innovation point: The proposed approach introduces uncertainty estimation to guide the injection of human demonstration data, which is a novel use of uncertainty for offline reinforcement learning. Performance: Experiments on datasets from MuJoCo and OffWorld Gym learning environments show that the proposed approach achieves better performance and is more sample-efficient compared to existing methods. Workload: The limitations of the work include the assumption made in the theoretical analysis and the reliance on simulated environments. However, the proposed approach provides a promising direction for reducing sample complexity in offline robot reinforcement learning.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work lies in proposing an approach for offline robot reinforcement learning that utilizes uncertainty estimation to guide the injection of human demonstration data, reducing the overall sample complexity while improving the effectiveness of policy training.\n                     \n- (2): Innovation point: The proposed approach introduces uncertainty estimation to guide the injection of human demonstration data, which is a novel use of uncertainty for offline reinforcement learning. Performance: Experiments on datasets from MuJoCo and OffWorld Gym learning environments show that the proposed approach achieves better performance and is more sample-efficient compared to existing methods. Workload: The limitations of the work include the assumption made in the theoretical analysis and the reliance on simulated environments. However, the proposed approach provides a promising direction for reducing sample complexity in offline robot reinforcement learning.\n\n\n"
}