{
    "Introduction": "Introduction Combinatorial optimization (discrete optimization), as opposed to continuous optimization is the focus of this paper. Discrete optimization is searching for an optimal solution in a \ufb01nite or countably in\ufb01nite set of potential solutions. Optimality is de\ufb01ned with respect to some criterion function, which is to be minimized or maximized. This paper discusses combinatorial optimization applied to the Quadratic Assignment Problem (QAP), and a special case which is the well-known Traveling Salesperson Problem (TSP). The QAP was introduced by Koopmans and Beckman in 1957(Koopmans, and Beckmann. 1957) in the context of locating \u201dindivisible economic activities\u201d(Anstreicher 2003). The QAP consists of two sets of interrelated objects, the solution of the problem is the optimal assignment among the objects. From an economic perspective, the objective of the QAP is to assign a set of facilities to a set of locations in such a way as to minimize the total assignment cost (C\u00b8ela 1998). 1 Yang and Whinston: Reinforcement Learning for Combinatorial Optimization 2 Article submitted to Management Science; manuscript no. The QAP is known to be an NP-Hard problem (Rainer 2013). There is no polynomial-time solution to the problem. However, there are many approximation algorithms for this problem in order to reduce the computational complexity. Recently, the idea of machine learning is broadly used in computation. Currently, many researchers try to apply reinforcement learning and neural networks for solving the QAP. It is an important way which we will discuss in the later part of the paper. One of the most common example of the QAP is TSP. The TSP is also an NP-Complete problem in combinatorial optimization(Papadimitriou 1977), commonly studied in theoretical computer science and operations research. The approximation of the TSP is an important topic. Recently, some researchers are focusing on applying reinforcement learning algorithms on approximating the solution to the TSP. This paper is focusing on comparing the di\ufb00erent reinforcement learning algorithms that generate approximate solutions to the TSP. Reinforcement learning (RL) is an area of machine learning that develops approximate methods for solving dynamic optimization problems. The main concern of reinforcement learning is how software agents ought to take actions in an environment in order to maximize the concept of cumulative reward or minimize the cost/penalty. The environment is typically stated in the form of a Markov decision process (Van Otterlo, Wiering 2012). Because of the nature of reinforcement learning, it is one of the relatively e\ufb03cient learning techniques. In recent years, the concept of Deep RL was introduced and largely applied in the machine learning \ufb01elds. Deep reinforcement learning is a combination of RL and deep learning(Francois-Lavet 2018). Deep RL utilizes a deep neural network structure to manipulate high-dimensional data. It typically generates outputs based on the probabilistic outcomes. In recent years, there are some Deep RL algorithm was also introduced in approximating the TSP, as well as other combinatorial optimization problems. 2. Motivation With the evolution of the computing power, reinforcement learning techniques can be applied to various problems. Unlike supervised learning, reinforcement learning does not need labelled data to adjust the network base on the loss function. Instead, it focuses on \ufb01nding balance between exploration and exploitation(Kaelbling, Littman, Moore 1996). A special case of the QAP- TSP is NP-hard, which is known to be challenging to \ufb01nd the optimal solution. While reinforcement learning is not developed to \ufb01nd the optimal solution but to approximate the optimal solution. Therefore, compare to approximating TSP arithmetically, reinforcement learning is a new direction that is worth to be explored. This paper will discuss the performance di\ufb00erence between the algorithm introduced in 1960s and the modern reinforcement learning algorithms, as well as how the limitations of computing powers would a\ufb00ect the performance of these algorithms. Yang and Whinston: Reinforcement Learning for Combinatorial Optimization Article submitted to Management Science; manuscript no. 3 3. Historical Timeline Although Reinforcement Learning is a relatively new \ufb01eld in machine learning, there were some researchers in the past who introduced the idea of using Reinforcement Learning to solve the TSP. The idea of Reinforcement Learning can be traced back to mid 20th Century, which provided the theoretical support for modern Reinforcement Learning. For example, the \u201dprototype\u201d of Reinforcement Learning on Combinatorial Optimization was introduced in 1970. 3.1. Bellman Equation (1957) Bellman Equation, also known as Dynamic Programming Equation, was introduced by Richard Bellman(Bellman 1957) and has been used in Dynamic Programming. It divides a dynamic optimization problem into a sequence of simpler sub-problems(Kirk 1970). The Bellman Equation is able to deal with the majority of the discrete-time problems that relate to Optimal Control Theory. However, the Bellman Equation is not feasible in solving the large scale NP-hard problems such as the TSP. Reinforcement Learning provides a way to approximate the Bellman Equation and solve the TSP. The Bellman Equation is commonly used as the starting point of the Reinforcement Learning approach. To learn the optimal policy \u03c0 in Reinforcement Learning(Graves 2017), there are two types of value functions: the state value function V (s), and the action value function Q(s,a). The state value function returns the value of a state s according to the policy \u03c0 (a function or method to generate outputs). V \u03c0(s) = E\u03c0[Rt|st = s] (1) De\ufb01ne \u2118a ss\u2032 = Pr[st+1 = s\u2032|st = s,at = a] and \u211ca ss\u2032 = E[rt+1|st = s,st+1 = s\u2032,at = a]. Where \u2118 is the transition probability and \u211c is the expected or average reward when starting in state s, taking action a, and moving toward state s\u2032. Then, derive the Bellman Equation, the state value function can be wrote as V \u03c0(s) = E\u03c0[ \u221e \ufffd k=0 \u03b3krt+k+1|st = s] = E\u03c0[\u03b3t+1 + \u03b3 \u221e \ufffd k=0 \u03b3krt+k+2|st = s] (2) The equation above describes the expected return value if start from state s and follow policy \u03c0. Then, inserting \u2118 and \u211c that are de\ufb01ned above to this equation and use the fact that E\u03c0[rt+1|st = s] = \ufffd a \u03c0(s,a)\ufffd s\u2032 \u2118a ss\u2032\u211ca ss\u2032 E\u03c0[\u03b3 \u221e \ufffd k=0 \u03b3krt+k+2|st = s] = \ufffd a \u03c0(s,a) \ufffd s\u2032 \u2118a ss\u2032\u03b3 E\u03c0[ \u221e \ufffd k=0 \u03b3krt+k+2|st+1 = s\u2032] (3) Eventually, the state value function V (s) can be rewrite as V \u03c0(s) = \ufffd a \u03c0(s,a) \ufffd s\u2032 \u2118a ss\u2032(\u211ca ss\u2032 + \u03b3V \u03c0(s\u2032)) (4) Yang and Whinston: Reinforcement Learning for Combinatorial Optimization 4 Article submitted to Management Science; manuscript no. The action value function can also be derived using the Bellman Equation: Q\u03c0(s,a) = E\u03c0[ \u221e \ufffd k=0 \u03b3krt+k+1|st = s,at = a] = E\u03c0[\u03b3t+1 + \u03b3 \u221e \ufffd k=0 \u03b3krt+k+2|st = s,at = a] = \ufffd s\u2032 \u2118a ss\u2032(\u211ca ss\u2032 + \u03b3 E\u03c0[ \u221e \ufffd k=0 \u03b3krt+k+2|st+1 = s\u2032]) = \ufffd s\u2032 \u2118a ss\u2032(\u211ca ss\u2032 + \u03b3 \ufffd a\u2032 \u03c0(s\u2032,a\u2032)Q\u03c0(s\u2032,a\u2032)) The action value function can be rewrite as: Q\u03c0(s,a) = E\u03c0[Rt|st = s,at = a] (5) Therefore, the Bellman Equation enables expressing values of a speci\ufb01c state as values of other states. This makes the calculation of values between states become much simpler. This opens a lot of possibilities for iterative approaches for calculating the value for each state(Graves 2017). So, the Bellman Equation plays an important role in the inception of RL. 3.2. Graves and Whinston\u2019s Algorithm (1970)- A Prototype of RL Graves and Whinston\u2019s Algorithm for solving the TSP was introduced in 1970(Graves and Whinston 1970). This algorithm used the Bellman Equation and statistical properties of the criterion function. It could be considered as the prototype of Reinforcement Learning, which calculated the mean and variance to serve as a value function. Graves and Whinston\u2019s Algorithm achieved one of the shortest distance on the TSP among the existing algorithms in that period. 3.2.1. Computational Scheme The TSP is considered as an optimal permutation problem. The algorithm is attempting to discover the optimal mapping between the set S and R, where S consists of variables (x1,...,xn) and R consists of integers from (1,...,n). The process is stated below: 1. Initialize by setting k = 1 and determine S1 and R1 according to a speci\ufb01c rule (eg. feasibility). 2. If Rk is empty, go to step 8. 3. Take elements i\u2217 \u2208 Sk and j\u2217 \u2208 Rk arbitrarily. Let ik = i\u2217, jk = j\u2217, and Rk = Rk\\{j\u2217}. 4. If k = n, go to step 12. 5. Increment k by 1, k = k + 1. 6. Determine Sk and Rk according to some speci\ufb01ed rule. 7. Repeat from step 2. Yang and Whinston: Reinforcement Learning for Combinatorial Optimization Article submitted to Management Science; manuscript no. 5 8. If k = 1, stop 9. Decrement k by 1, k = k \u2212 1. 10. If Rk is empty, go back to step 8. 11. Take an arbitrary element j\u2217 \u2208 Rk, let jk = j\u2217 to go with the current ik and set Rk = Rk\\{j\u2217}. Then go back to step 4 12. Record the current mapping, and go to step 10. In this algorithm, as a criterion for selecting a path, is the value of the associated mean completion. E[\u03c6(p), p \u2208 C(i1,...ik)] For the kth selection, the algorithm will pick the path to the city which has the smallest mean among all unvisited cities. The mean can be calculated as follow: M = 1 n(\ufffd i\u2208Nai)(\ufffd j\u2208Nbj) And the variance can be written as: 1 n! \ufffd p[(\ufffd j ajbp(j))(\ufffd i aibp(i))] = (n\u22122)! n! [(\ufffd i ai)2 \u2212 \ufffd t a2 t][(\ufffd k bk)2 \u2212 \ufffd t b2 t] + (n\u22121)! n! (\ufffd i a2 i )(\ufffd j b2 j) Repeated examinations with the selection procedure indicates that while an optimal solution does not generally result from the \ufb01rst complete assignment, a very good solution is achieved. Then, the algorithm turns to the explicit computation of the mean and variance value of the completion of a k-partial map. 3.2.2. Implicit Enumeration De\ufb01ne \u03a8(S) is a set that consists all the subsets of the set S. Let L\u03c6 : \u03a8(S) \u2192 R and L\u03c6(E) \u2264 \u03c6(i),\u2200i \u2208 E where E \u2282 S. The functions give a lower bound for \u03c6(i) over a subset E. Then, let A denote value of the current best completed assignment, where A = \u221e if there is no complete assignment yet. If L\u03c6(C(i1,...,ik)) \u2265 A, there is no k-partial mapping from i to j consists a better solution than the current mapping. The completion class C(i1,...,ik) is implicitly enumerated. To embed implicit enumeration into the algorithm, replace Step 11 that is presented in Section 3.2.1 by the following: 11. Take an arbitrary element j\u2217 \u2208 Rk, let jk = j\u2217 to go with the current ik and set Rk = Rk\\{j\u2217}. Compute L\u03c6(C(i1,...,ik)). If L\u03c6(C(i1,...,ik)) \u2265 A go to step 9, otherwise go back to step 4. De\ufb01ne G\u03c6 : P(S) \u2192 R be \u03b1-probabilistic lower bound set function for any given \u03c6 and \u03b1. In terms of the overall algorithm, the \u03b1-probabilistic lower bound functions G\u03c6 are employed as the lower bound functions L\u03c6 in Step 11 presented above. Which means, if G\u03c6(C(i1,...,ik)) \u2265 A Yang and Whinston: Reinforcement Learning for Combinatorial Optimization 6 Article submitted to Management Science; manuscript no. then completion class C(i1,...,ik) is implicitly enumerated at the \u03b1 con\ufb01dence level. The computational experience presented in the paper indicates that the \u03b1-probabilistic lower bound function gives much greater cutting power than the lower bound functions without substantial risk of overlooking the true minimum. It has been proved to achieve a complete con\ufb01dence level enumeration with large practical problems. 3.2.3. Evaluation The algorithm is illustrated from three classical TSP from Karg and Thompson (Karg, Thompson 1964). The three problems are 33-city, 42-city and 57-city, which are considered large scale problems. The outcomes of Grave and Whinston\u2019s Algorithm and the optimal solution of the problems are presented in the table below. The optimal solution is retrieved from Karg and Thompson\u2019s paper. The results show that the approximations are within 15% away from the optimal solution, moreover, the approximation will move closer to the optimal solution when the problem scale up. 33-city 42-city 57-city Grave and Whinston\u2019s Approximation 12,406 707 13,159 Optimal Solution 10,861 699 12,995 3.2.4. Analysis The Grave and Whinston\u2019s Algorithm reveals some characteristics of Reinforcement Learning. The algorithm utilizes the Bellman Equation for approximating the optimal solution of the partial TSP. Bellman equation is the basic block of solving reinforcement learning and is omnipresent in modern Reinforcement Learning(Tanwar 2019). In Grave and Whinston\u2019s Algorithm, the Bellman Equation is employed to calculate the optimal solution of the prevailing k-partial TSP. The calculation is built upon the recorded optimal solution of previous (k - 1)partial TSP. Then, the determination of k-partial mapping will serve as the \u201denvironment\u201d for the subsequent (k+1)-partial mapping. The mean and variance in Section 3.2.2 serve as the value function in the modern reinforcement learning architecture. The key to solving the TSP is to minimize the mean distance of each k-partial mapping. Therefore, a large mean value will diminish the probability of taking a speci\ufb01c choice at the current step. Unmistakably, there exist many distinctions between the approach from Grave and Whinston and modern reinforcement learning, as a result of the inadequacy of computing power in the 1960s. There is no adjustment on the network in the training procedure in Grave and Whinston\u2019s architecture. The \u201dlearning\u201d component in the model is not signi\ufb01cant. 3.3. Ant-Q (1995)- A Classical RL Approach The Ant-Q Algorithm was introduced by Luca M. Gambardella and Marco Dorigo in 1995, which presents many similarities with Q-learning(Watkins 1989). This algorithm was inspired by ant system(Dorigo, Maniezzo,and Colorni 1996), which is a distributed algorithm for combinatorial optimization. It was a notable algorithm that applied reinforcement learning on the Yang and Whinston: Reinforcement Learning for Combinatorial Optimization Article submitted to Management Science; manuscript no. 7 TSP. The Ant-Q algorithm gained competitive results on both symmetric and asymmetric TSP(Gambardella and Dorigo 1995). 3.3.1. Ant System (AS) is the \ufb01rst Ant Colony Optimization algorithm, developed by Marco Dorigo(Gambardella and Dorigo 1995). The Ant Colony Optimization algorithm (ACO), also introduced by Dorigo, is a probabilistic technique for solving computational problems which can be reduced to \ufb01nding good paths through graphs. Ant System uses an arti\ufb01cial ant- a computational agent- to \ufb01nd good solutions to graph-related optimization problems. The optimization problem that ACO can apply on is equivalent to the TSP, which \ufb01nds the shortest path on a weighted graph. In the ACO algorithm, each arti\ufb01cial ant selects a path to constructs a solution arbitrarily for every iteration. Then, the solutions generated by the ants are compared and evaluated. The network will be adjusted based on the evaluations. 3.3.2. Q-Learning is a reinforcement learning algorithm, which learns and records policies and takes action to maximize the expectation. The letter \u2019Q\u2019 stands for the quality of actions that are taken. Q-Learning is model-free, so that it does not require to build environmental networks. The algorithm centralizes on the quality of state-action combinations: Q : S \u00d7 A \u2192 R In the beginning, Q is initialized randomly. At each time step t, action at associates with a transformation cost/reward to the next stage st+1, the cost/reward is denoted as rt. The quality Q is updated by utilizing the Bellman Equation (Quality Value Function presented in Section 3.1): Qnew(st,at) = Qt(st,at) + \u03b1(rt + \u03bb \u00b7 max{Q(st+1,a)} \u2212 Q(st,at)) where \u03b1 is the learning rate and \u03bb is the discount factor. 3.3.3. Ant-Q Algorithm can be described as follows. For each iteration, there are four steps: 1. Initialize AQ-values, each agent k is placed on a city rk1 according to some policy. Initialize a set of to-be-visited cities Jk(rk1). 2. Each agent makes a move and updates AQ(r,s) if the move discounts the next state evaluation. The agents repeat moving and updating AQ-values until they are back to the starting city. 3. The length Lk of the tour done by agent k is computed, and Lk is used to calculate the delayed reinforcements \u2206AQ(r,s). Then, update the AQ-values base on \u2206AQ(r,s). 4. Check whether the pre-de\ufb01ned termination condition is met. Return the approximated shortest path Lk. The AQ-values are updates by the following rule: AQ(r,s) = (1 \u2212 \u03b1) \u00b7 AQ(r,s) + \u03b1 \u00b7 (\u2206AQ(r,s) + \u03bb \u00b7 Maxz\u2208Jk(s)AQ(s,z)) where Jk(s) is a function of the previous history of agent k. And \u2206AQ(r,s) is calculated as follow: \u2206AQ(r,s) = \ufffd W Lk if (r,s) \u2208 tour done by agent k 0 otherwise ",
    "Approach": "Approach REINFORCE presents an idea to learn a heuristic for combinatorial optimization problems. The model is trained using a simple baseline based on a deterministic greedy rollout, which we \ufb01nd is more e\ufb03cient than using a value function(Kool 2019). This algorithm signi\ufb01cantly improve the performance on the TSP up to 100 cities. 3.4.1. Attention Model takes graph structure into account by a masking procedure. The attention based encoder-decoder model de\ufb01nes a stochastic policy p(\u03c0|s) for selecting a solution \u03c0 given a problem instance s. It is factorized and parameterized by \u03b8: p\u03b8(\u03c0|s) = \ufffdn t=1 p\u03b8(\u03c0t|s,\u03c01:t\u22121) Yang and Whinston: Reinforcement Learning for Combinatorial Optimization Article submitted to Management Science; manuscript no. 9 The encoder produces embeddings of all input nodes. Input nodes are embedded and processed by N sequential layers, each consisting of a multi-head attention layer and node-wise feed-forward sub-layer. The node embedding and graph embedding are produced as the inputs of the decoder. The decoder produces the sequence \u03c0 of input nodes. To produce the solution, a context vector that consists of the graph embedding of \ufb01rst, last and unvisited cities will be given to a decoder. The decoder will calculate the probability distribution of unvisited cities and output the next city to be visited. 3.4.2. Algorithm The Attention Model obtains a solution (path) \u03c0|s from a probability distribution p\u03b8(\u03c0|s). To train the model, the loss is de\ufb01ned as L(\u03b8|s) = Ep\u03b8[L(\u03c0)] The loss is adjusted by gradient descent, using the REINFORCE gradient estimator with baseline b(s)(Williams 1992): \u2206L(\u03b8|s) = Ep\u03b8[(L(\u03c0) \u2212 b(s))\u2206log p\u03b8(\u03c0|s)] With the greedy rollout as baseline b(s), the function L(\u03c0)\u2212b(s) is negative if the sampled solution \u03c0 is better than the greedy rollout, causing actions to be reinforced. This way the model is trained to improve over itself. 3.4.3. Evaluation For the TSP, the algorithm is compared with Nearest Insertion, Random Insertion, Farthest Insertion, as well as Nearest Neighbor. \u2022 Nearest Insertion inserts the node i that is nearest to the tour: i\u2217 = argmini/\u2208S(minj /\u2208S(dij)) \u2022 Farthest Insertion inserts the node i so that the distance of the tour is maximized: i\u2217 = argmaxi/\u2208S(minj /\u2208S(dij)) \u2022 Random Insertion inserts a random node. \u2022 Nearest Neighbor heuristic represents the partial solution as a path with a starting and ending node. They are compared using 20, 50, and 100-city TSP, while the REINFORCE algorithm obtains the best performance among all the selected algorithms within a relatively short period of time. 3.4.4. Analysis This algorithm presents an approach to utilizing RL techniques on the selftraining of the attention model, or graph attention network. The attention model provides a probabilistic approach to estimating the path base on the given environment. The attention model is focusing on solving sub-problems and merging the outcomes in a probabilistic manner to form the \ufb01nal result. Thus it can reduce the overall computation complexity by adjusting the parameters in the attention network to set concentrations (sub-graph with high probability scores). ",
    "Discussion": "Discussion Through analyzing three RL approaches to the TSP, we argue that RL is a good technique to solve combinatorial optimization problems. All of the three algorithms we reviewed in Section 3 achieved good performance among the algorithms developed in those time periods. Taking into account the selection bias when comparing algorithms, we could at least argue that the RL approach in combinatorial optimization achieves above-average performance. In addition to the performance, the RL approach has its own strength. In the modern RL algorithms that are introduced in the recent \ufb01ve years, there is no human knowledge required by those models. Which means the RL model starts from completely arbitrary values/states. After deep RL is widely used in approximating combinatorial optimization, the quality and capability of the RL approach are raised. The potential of the RL approach approximation is greater than approximating arithmetically or algorithmically. With the rapid expansion of computing power, the training time and the quantity of training data will be less taken into consideration. Then, the performance and capability of the RL approach approximation will be enhanced continuously. This trend may be signi\ufb01cant when quantum computing is widely used in machine learning. 4.1. Future Research With the expansion of computing power, the RL network for combinatorial optimization gradually will grow more intricate in order to advance the performance. We intend to obtain a performancecomplexity balance, where a comparatively good solution is found with less computation e\ufb00ort. In the Graves and Whinston\u2019s Algorithm presented in Section 3.2, they made decisions based on the combination of mean and variance (the calculation is shown in Section 3.2.1) and their decision on the next city would minimize the combination of mean and variance. A possible enhancement is embedding a simple feed-forward neural network. The value function of Graves and Whinston\u2019s algorithm could be updated to the following form: ",
    "References": "References Anstreicher, K.M. 2003. Recent advances in the solution of quadratic assignment problems.. Mathematical Programming Series B 97, 27 - 42. Koopmans, T. C. and M. J. Beckmann. 1957. Assignment problems and the location of economic activities.. Econometrica 25, 53 - 76. C\u00b8ela, E. 1998. The Quadratic Assignment Problem: Theory and Algorithms.. Kluwer Academic Publishers, Dordrecht. Rainer E. burkard. 2013. The Quadratic Assignment Problem: Theory and Algorithms.. Handbook of Combinatorial Optimization pp 2741-2814. Panel Christos H.Papadimitriou. 1977. The Euclidean travelling salesman problem is NP-complete.. Theoretical Computer Science, Volume 4, Issue 3, Pages 237-244 Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau. 2018. An Introduction to Deep Reinforcement Learning. arXiv preprint at arXiv:1811.12560 [cs.LG]. Van Otterlo, M.; Wiering, M. 2012. Reinforcement learning and markov decision processes. Reinforcement Learning. Adaptation, Learning, and Optimization. 12. pp. 3\u201342. doi:10.1007/978-3-64227645-3 1. ISBN 978-3-642-27644-6. Kaelbling, Leslie P.; Littman, Michael L.; Moore, Andrew W. (1996). Reinforcement Learning: A Survey. Journal of Arti\ufb01cial Intelligence Research. 4: 237\u2013285. arXiv:cs/9605103. doi:10.1613/jair.301. Archived from the original on 2001-11-20. Bellman, R.E. 1957. Dynamic Programming. Princeton University Press, Princeton, NJ. Republished 2003: Dover, ISBN 0-486-42809-5. Kirk, Donald E. (1970). Optimal Control Theory: An Introduction. Prentice-Hall. p. 55. ISBN 0-13638098-0. Yang and Whinston: Reinforcement Learning for Combinatorial Optimization 12 Article submitted to Management Science; manuscript no. Graves, Josh. 2017. Understanding RL: The Bellman Equations [Blog post]. Retrieved from joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/ G.W.Graves, A.B.Whinston. 1970. An Algorithm For The Quadratic Assignment Problem. Management Science. Vol 17. pg 453. Robert L. Karg and Gerald L. Thompson. 1964. A Heuristic Approach to Solving Travelling Salesman Problems.Management Science. Vol. 10, No. 2 (Jan., 1964), pp. 225-248 Sanchit Tanwar. 2019. Bellman Equation and dynamic programming. Medium [Blog post]. Retrieved from medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7 Luca M. Gambardella, Marco Dorigo. 1995. Ant-Q: A Reinforcement Learning approach to the TSP. Proceedings of the Twelfth International Conference on Machine Learning, Tahoe City, California, July 9\u201312, 1995. Pages 252-260 Christopher J. C. H. Watkins. 1989. Learning with delayed rewards. Ph. D. dissertation, Psychology Department, University of Cambridge, England. M. Dorigo ; V. Maniezzo ; A. Colorni. 1996. Ant system: optimization by a colony of cooperating agents. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) ( Volume: 26 , Issue: 1 , Feb 1996 ). Page 29 - 41 Arulkumaran, K.; Deisenroth, M. P.; Brundage, M.; Bharath, A. A. November 2017. Deep Reinforcement Learning: A Brief Survey. IEEE Signal Processing Magazine. 34 (6): 26\u201338. Carr, Roger. 2002. \u201dSimulated Annealing.\u201d From MathWorld\u2013A Wolfram Web Resource, created by Eric W. Weisstein. https://mathworld.wolfram.com/SimulatedAnnealing.html Or Rivlin. 2019. Reinforcement Learning for Combinatorial Optimization. Towards Data Science [Blog post]. Retrieved from https://towardsdatascience.com/reinforcement-learning-for-combinatorialoptimization-d1402e396e91 Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya1, Sayan Ranu, Ambuj Singh. 2020. Learning Heuristics over Large Graphs via Deep Reinforcement Learning. arXiv:1903.03332v3 Wouter Kool, Herke van Hoof, Max Welling. 2019. ATTENTION, LEARN TO SOLVE ROUTING PROBLEMS! ICLR 2019. Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256. ",
    "title": "A Survey on Reinforcement Learning for",
    "paper_info": "arXiv:2008.12248v2  [cs.LG]  23 Dec 2020\nSubmitted to Management Science\nmanuscript\nA Survey on Reinforcement Learning for\nCombinatorial Optimization\nYunhao Yang\nDepartment of Computer Science, University of Texas at Austin, Austin, TX 78705, yunhaoyang234@utexas.edu\nAndrew Whinston\nDepartment of Information, Risk and Operations Management, University of Texas at Austin, Austin, TX 78705,\nabw@uts.cc.utexas.edu\nThis paper gives a detailed review of reinforcement learning in combinatorial optimization, introduces the\nhistory of combinatorial optimization starting in the 1960s, and compares it with the reinforcement learning\nalgorithms in recent years. We explicitly look at a famous combinatorial problem known as the Traveling\nSalesperson Problem (TSP). We compare the approach of the modern reinforcement learning algorithms on\nTSP with an approach published in 1970. Then, we discuss the similarities between these algorithms and\nhow the approach of reinforcement learning changes due to the evolution of machine learning techniques\nand computing power. We also mention the deep learning approach on the TSP, which is named Deep\nReinforcement Learning. We argue that deep learning is a generic approach that can be integrated with\ntraditional reinforcement learning algorithms and optimize the outcomes of the TSP.\nKey words : reinforcement learning, combinatorial optimization, dynamic programming, machine learning,\ntraveling salesperson\n1.\nIntroduction\nCombinatorial optimization (discrete optimization), as opposed to continuous optimization is the\nfocus of this paper. Discrete optimization is searching for an optimal solution in a \ufb01nite or countably\nin\ufb01nite set of potential solutions. Optimality is de\ufb01ned with respect to some criterion function,\nwhich is to be minimized or maximized. This paper discusses combinatorial optimization applied\nto the Quadratic Assignment Problem (QAP), and a special case which is the well-known Traveling\nSalesperson Problem (TSP).\nThe QAP was introduced by Koopmans and Beckman in 1957(Koopmans, and Beckmann. 1957)\nin the context of locating \u201dindivisible economic activities\u201d(Anstreicher 2003). The QAP consists of\ntwo sets of interrelated objects, the solution of the problem is the optimal assignment among the\nobjects. From an economic perspective, the objective of the QAP is to assign a set of facilities to\na set of locations in such a way as to minimize the total assignment cost (C\u00b8ela 1998).\n1\n",
    "GPTsummary": "\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in proposing a deep reinforcement learning approach to the Traveling Salesperson Problem (TSP), which achieves state-of-the-art performance compared with other selected algorithms. This approach highlights the potential for reinforcement learning to optimize outcomes for TSP, a difficult combinatorial optimization problem with many possible solutions and few reliable heuristics.\n\n- (2): In terms of innovation point, this article proposes using deep reinforcement learning to solve TSP, which builds on the evolution of machine learning techniques and computing power. The proposed attention-based encoder-decoder model is also a new approach to selecting solutions for TSP. In terms of performance, the proposed approach achieves state-of-the-art performance on TSP compared with other selected algorithms. However, the workload required to train and optimize the model may be a limitation for practical applications, which requires further research to improve efficiency.\n\n\n",
    "GPTmethods": "\n\n\n\n8. Conclusion:\n\n- (1): The significance of this work lies in proposing a deep reinforcement learning approach to the Traveling Salesperson Problem (TSP), which achieves state-of-the-art performance compared with other selected algorithms. This approach highlights the potential for reinforcement learning to optimize outcomes for TSP, a difficult combinatorial optimization problem with many possible solutions and few reliable heuristics.\n\n- (2): In terms of innovation point, this article proposes using deep reinforcement learning to solve TSP, which builds on the evolution of machine learning techniques and computing power. The proposed attention-based encoder-decoder model is also a new approach to selecting solutions for TSP. In terms of performance, the proposed approach achieves state-of-the-art performance on TSP compared with other selected algorithms. However, the workload required to train and optimize the model may be a limitation for practical applications, which requires further research to improve efficiency.\n\n\n",
    "GPTconclusion": "- (1): The significance of this work lies in proposing a deep reinforcement learning approach to the Traveling Salesperson Problem (TSP), which achieves state-of-the-art performance compared with other selected algorithms. This approach highlights the potential for reinforcement learning to optimize outcomes for TSP, a difficult combinatorial optimization problem with many possible solutions and few reliable heuristics.\n\n- (2): In terms of innovation point, this article proposes using deep reinforcement learning to solve TSP, which builds on the evolution of machine learning techniques and computing power. The proposed attention-based encoder-decoder model is also a new approach to selecting solutions for TSP. In terms of performance, the proposed approach achieves state-of-the-art performance on TSP compared with other selected algorithms. However, the workload required to train and optimize the model may be a limitation for practical applications, which requires further research to improve efficiency.\n\n\n"
}