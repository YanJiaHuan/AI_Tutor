{
    "Abstract": "Abstract\u2014Reinforcement learning algorithms, just like any other Machine learning algorithm pose a serious threat from adversaries. The adversaries can manipulate the learning algorithm resulting in non-optimal policies. In this paper, we analyze the Multi-task Federated Reinforcement Learning algorithms, where multiple collaborative agents in various environments are trying to maximize the sum of discounted return, in the presence of adversarial agents. We argue that the common attack methods are not guaranteed to carry out a successful attack on Multi-task Federated Reinforcement Learning and propose an adaptive attack method with better attack performance. Furthermore, we modify the conventional federated reinforcement learning algorithm to address the issue of adversaries that works equally well with and without the adversaries. Experimentation on different small to mid-size reinforcement learning problems show that the proposed attack method outperforms other general attack methods and the proposed modi\ufb01cation to federated reinforcement learning algorithm was able to achieve near-optimal policies in the presence of adversarial agents. Keywords\u2014Adversaries, MT-FedRL, FedRL, Attack I. ",
    "Introduction": "INTRODUCTION In the past decade, Reinforcement Learning (RL) has gained wide popularity in solving complex problems in an online fashion for various problem sets such as game playing [1], autonomous navigation [2], [3], robotics [4] and network security [5]. In most of the real-life cases where we do not have complete access to the system dynamics, conventional control theory fails to provide optimum solutions. Model-free RL on the other hand uses heuristics to explore and exploit the environment to achieve the underlying goal. With a boom in Internet of Things (IoT) devices [6], we have a lot of compute power at our disposal. The problem, however, is that the compute is distributed. Distributed algorithms have been studied to take advantage of these distributed compute agents. Conventional method consist of using these IoT as datacollectors and then using a centralized server to train a network on the collected data. Federated Learning, introduced by Google [7]\u2013[9] is a distributed approach to machine learning tasks enabling model training on large sets of decentralized data by individual agents. The key idea behind federated learning is to preserve the privacy of the data to the local node responsible for generating it. The training data is assumed to be local only, the agents however, can share the model parameter that is learned. This model sharing serves two purpose. Primarily it ensures the privacy of the data being Server \u2026 Agent 0 Environment 0 Update Policy (s, a, s\u2019, r) action Agent 1 Environment 1 Update Policy (s, a, s\u2019, r) action Agent n-1 Environment n-1 Update Policy (s, a, s\u2019, r) action RL Return RL Return RL Return Fig. 1: Federated RL - The idea is to learn a common uni\ufb01ed policy without sharing the local training data that works good enough for all the environments generated locally, secondly in some of the cases the size of the model parameter might be much smaller than the size of the local data, hence sharing the model parameter instead of the data might save up on the communication cost involved. Federated learning has also been considered in the context of Reinforcement learning problem for both multi-agent RL [10]\u2013[13] and multi-task RL [14]\u2013[16] where multiple RL agents either in a single or multiple environments try to jointly maximize the collective or sum of individual discounted returns, respectively. While ML algorithms have proven to provide superior accuracy over conventional methods, they pose a threat from adversarial manipulations. Adversaries can use a variety of attack models to manipulate the model either in the training or the inference phase leading to decreased accuracy or poor policies. Common attack methods include data-poisoning and model poisoning where the adversary tries to manipulate the input data or directly the learned model respectively. In this paper, we propose and analyze a model-poisoning attack for the Multi-task Federated RL (MT-FedRL) problem and modify the conventional Federated RL approach to provide protection from model poisoning attacks. The contributions of this paper are as follows \u2022 We carry out a detailed study on the multi-task federated RL (MT-FedRL) with model-poisoning adversaries on medium and large size problems of grid-world (GridWorld) and drone autonomous navigation(AutoNav). \u2022 We argue that the general adversarial methods are not good enough to create an effective attack on MT-FedRL, arXiv:2103.06473v1  [cs.LG]  11 Mar 2021 ",
    "Related Work": "RELATED WORK The effects of adversaries in Machine learning algorithms were \ufb01rst discovered in [17] where it was observed that a small lp norm perturbation to the input of a trained classi\ufb01er model resulted in con\ufb01dently misclassifying the input. These lp norm perturbations were visually imperceptible to humans. The adversary here acts in the form of speci\ufb01cally creating adversarial inputs to produce erroneous outputs to a learned model [18]\u2013[23]. For supervised learning problems, such as a classi\ufb01cation task, where the network model has already been trained, attacking the input is the most probable choice for an adversary to attack through. In RL, there is no clear boundary between the training and test phase. The adversary can act either in the form of data-poisoning attacks, such as creating adversarial examples [24], [25], or can directly attack the underlying learned policy [26]\u2013[29] either in terms of malicious falsi\ufb01cation of reward signals, or estimating the RL dynamics from a batch data set and poisoning the policy. Authors in [30], try to attack an RL agent by selecting an adversarial policy acting in a multi-agent environment as a result of creating observations that are adversarial in nature. Their results on a two-player zero-sum game show that an adversarial agent can be trained to interact with the victim winning reliably against it. In federated RL, alongside the data-poisoning and policy-poisoning attacks, we also have to worry about the model-poisoning attacks. Since we have more than one learning agents, a complete agent can take up the role of an adversary. The adversarial agent can feed in false data to purposely corrupt the global model. In model poisoning attacks the adversary, instead of poisoning the input, tries to adversely modify the learned model parameters directly by feeding false information purposely poisoning the global model [31], [32]. Since federated learning uses an average operator to merge the local model parameters learned by individual agents, such attacks can severely affect the performance of the global model. Adversarial training can be used to mitigate the effects of such adversaries. [33] showed that the classi\ufb01cation model can be made much robust against the adversarial examples by feature de-noising. The robustness of RL policies has also been analyzed by the adversarial training [34]\u2013[37]. [34], [37] show that the data-poisoning can be made a part of RL training to learn more robust policies. They feed perturbed observations during RL training for the trained policy to be more robust to dynamically changing conditions during test time. [38] shows that the data-poisoning attacks in federated learning can be resolved by modifying the federated aggregation operator based on induced ordered weighted averaging operators [39] and \ufb01ltering out possible adversaries. To the best of our knowledge, there is no detailed research carried out on MTFedRL in the presence of adversaries. In this paper, we address the effects of model poisoning attacks on the MT-FedRL problem. III. MULTI-TASK FEDERATED REINFORCEMENT LEARNING (MT-FEDRL) We consider a Multi-task Federated Reinforcement Learning (MT-FedRL) problem with n number of agents. Each agent operates in its own environment which can be characterized by a different Markov decision process (MDP). Each agent only acts and makes observations in its own environment. The goal of MT-FedRL is to learn a uni\ufb01ed policy, which is jointly optimal across all of the n environments. Each agent shares its information with a centralized server. The state and action spaces do not need to be the same in each of these n environments. If the state spaces are disjoint across environments, the joint problem decouples into a set of n independent problems. Communicating information in the case of N-independent problems does not help. We consider policy gradient methods for RL. The MDP at each agent i can be described by the tuple Mi = (Si, Ai, Pi, Ri, \u03b3i) where Si is the state space, Ai is the action space, Pi is the MDP transition probabilities, Ri : Si \u00d7 Ai \u2192 R is the reward function, and \u03b3i \u2208 (0, 1) is the discount factor. Let V \u03c0 i be the value function, induced by the policy \u03c0, at the state s in the i-th environment, then we have V \u03c0 i (s) = E \ufffd \u221e \ufffd k=0 \u03b3k i Ri(sk i , ak i ) | s0 i = s \ufffd , ak i \u223c \u03c0(\u00b7|sk i ). (1) Similarly, we have the Q-function Q\u03c0 i and advantage function A\u03c0 i for the i-th environment as follows Q\u03c0 i (si, ai) = E \ufffd \u221e \ufffd k=0 \u03b3k i R(sk i , ak i ) | s0 i = si, a0 i = ai \ufffd , A\u03c0 i (si, ai) = Q\u03c0 i (si, ai) \u2212 V \u03c0 i (si). (2) We denote by \u03c1i the initial state distribution over the action space of i-th environment., The goal of the MT-FedRL problem is to \ufb01nd a uni\ufb01ed policy \u03c0\u2217 that maximizes the sum of long-term discounted return for all the environment i i.e. 2 max \u03c0 V (\u03c0; \u03c1) \u225c n\u22121 \ufffd i=0 Esi\u223c\u03c1iV \u03c0 i (si), \u03c1 = \uf8ee \uf8ef\uf8f0 \u03c10 ... \u03c1n\u22121 \uf8f9 \uf8fa\uf8fb (3) Solving the above equation will yield a uni\ufb01ed \u03c0\u2217 resulting in a balanced performance across all the environments. We use the parameter \u03b8 to model the family of policies \u03c0\u03b8(a|s), considering both the tabular method (for simpler problems) and neural network-based function approximation (for complex problems). The goal of the MT-FedRL problem then is to \ufb01nd \u03b8\u2217 satisfying \u03b8\u2217 = arg max \u03b8 V (\u03b8; \u03c1) \u225c n\u22121 \ufffd i=0 Esi\u223c\u03c1iV \u03c0\u03b8 i (si). (4) In tabular method, gradient ascent methods are utilized to solve (3) over a set of randomized stationary policies {\u03c0\u03b8 : \u03b8 \u2208 R|S|\u00d7|A|}, where \u03b8 uses the softmax parameterization \u03c0\u03b8(a | s) = exp (\u03b8s,a) \ufffd a\u2032\u2208A exp(\u03b8s,a\u2032)\u00b7 (5) For a simpler problem where the size of state-space and actionspace is limited, this table is easier to maintain. For more complex problems with a larger state/action space, usually neural network-based function approximation {\u03c0\u03b8 : S \u2192 A} is used, where \u03b8 are the trainable weights of a pre-de\ufb01ned neural network structure. One approach to solving this uni\ufb01ed-policy problem is by sharing the data Mi observed by each agent in its environment to a centralized server. The centralised server then can train a single policy parameter \u03b8 based on the collective data M = \u222an\u22121 i=0 Mi. This, however, comes with the cost of reduced privacy as the agent needs to share its data with the server. In MT-FedRL, however, the data tuple Mi is not shared with the server due to privacy concerns. The data remains at the local agent and instead, the policy parameter \u03b8i is shared with the server. Each agent i utilizes its locally available data Di to train the policy parameter \u03b8i by maximizing its own local value function V \u03c0 i through SGD. We assume policy gradient methods for RL training. After the completion of each episode k, the agents share their policy parameter \u03b8k\u2212 i with a centralized server. The server carries out a smoothing average and generates N new sets of parameters \u03b8k+ i , one for each agent, using the following expression. \u03b8k+ i = \u03b1k\u03b8k\u2212 i + \u03b2k \ufffd j\u0338=i \u03b8k\u2212 j (7) where \u03b1k, \u03b2k = 1\u2212\u03b1 n\u22121 \u2208 (0, 1) are non-negative smoothing average weights. The goal of this smoothing average is to achieve a consensus among the agents\u2019 parameters, i.e. lim k\u2192\u221e \u03b8k+ i \u2192 \u03b8\u2217 \u2200i \u2208 {0, n \u2212 1} (8) As the training proceeds, the smoothing average constants converge to \u03b1k, \u03b2k \u2192 1 n. The conditions on \u03b1k, \u03b2k to guarantee the convergence of Algorithm 1 can be found in Algorithm 1: Multi-task Federated Reinforcement Learning (MT-FedRL) with smoothing average Initialization: \u03b80+ i \u2208 Rd, step sizes \u03b4k, smoothing average threshold iteration t %Server Executes for k=1,2,3,... do Calculate smoothing average parameters \u03b1k = 1 n max(1, k/t), \u03b2k = 1 \u2212 \u03b1k n \u2212 1 for each agent i in parallel do Receive updated policy parameter from clients \u03b8(k+1)\u2212 i \u2190 ClientUpdate \ufffd i, \u03b8k+ i \ufffd end for each agent i do Policy update: \u03b8(k+1)+ i = \u03b1k\u03b8(k+1)\u2212 i + \u03b2k \ufffd i\u0338=j \u03b8(k+1)\u2212 j Send updated policy parameter \u03b8(k+1)+ i back to client i end end Function ClientUpdate(i, \u03b8): 1) Compute the gradient of the local value function \u2202V \u03c0\u03b8 i (\u03c1i) \u2202\u03b8si,ai based on the local data 2) Update the policy parameter \u03b8\u2212 = \u03b8 + \u03b4k \u2202V \u03c0\u03b8 i (\u03c1i) \u2202\u03b8si,ai (6) return \u03b8\u2212 [16]. The complete algorithm of multi-task federated RL can be found in Alg. 1 IV. MT-FEDRL WITH ADVERSARIES MT-FedRL has proven to converge to a uni\ufb01ed policy that performs jointly optimal on each environment [16]. This jointly optimal policy yields near-optimal policies when evaluated on each environment if the agents\u2019 goals are positively correlated. If the agent\u2019s goals are not positively correlated, the uni\ufb01ed policy might not result in a nearoptimal policy for individual environments. This is exactly what happens to MT-FedRL in the presence of an adversary. Let L denote the set of adversarial agents in a n \u2212 agent MTFedRL problem. The smoothing average at the server can be decomposed based on the adversarial and nonadversarial agent as follows \u03b8k+ i = \u03b1k\u03b8k\u2212 i + \u03b2k \ufffd j\u0338=i,j /\u2208L \u03b8k\u2212 j + \u03b2k \ufffd l\u2208L \u03b8k\u2212 l (9) where i /\u2208 L. \u03b8k+ i is the updated policy parameter for agent i calculated by the server at iteration k. This update incorporates the knowledge from other environments and as the training proceeds, these updated policy parameters for all the agents converge to a uni\ufb01ed parameter \u03b8\u2217. In 3 a non-adversarial MT-FedRL problem, this uni\ufb01ed policy parameter ends up achieving a policy that maximizes the sum of discounted returns for all the environments. In an adversarial MT-FedRL problem, the goal of the adversarial agent is to prevent the MT-FedRL from achieving this uni\ufb01ed \u03b8\u2217 by purposely providing an adversarial policy parameter \u03b8k\u2212 l . Parameters that effect learning: Using gradient ascent, each agent updates its own set of policy parameter locally according to the following equation, \u03b8k\u2212 i = \u03b8(k\u22121)+ i + \u03b4i\u2207\u03b8iV \u03c0\u03b8i i (\u03c1i) (10) where \u03b4i is the learning rate for agent i. Using Eq. 10 in the smoothing average Eq. 7 yields \u03b8k+ i = \uf8eb \uf8ed\u03b1k\u03b8(k\u22121)+ i + \u03b2k \ufffd j\u0338=i,j /\u2208L \u03b8(k\u22121)+ j \uf8f6 \uf8f8 + \uf8eb \uf8ed\u03b1k\u03b4i\u2207\u03b8iV \u03c0\u03b8i i (\u03c1i) + \u03b2k \ufffd j\u0338=i,j /\u2208L \u03b4j\u2207\u03b8jV \u03c0\u03b8j j (\u03c1j) \uf8f6 \uf8f8 + \ufffd \u03b2k \ufffd l\u2208L \u03b8k\u2212 l \ufffd (11) The server update of the policy parameter can be decomposed into three parts. \u2022 The weighted sum of the previous set of policy parameters \u03b8(k\u22121)+ i shared by the server with the respective agents. \u2022 The agent\u2019s local update, which tries to shift the policy parameter distribution towards the goal direction. \u2022 The adversarial policy parameter which aims at shifting the uni\ufb01ed policy parameter away from achieving the goal. If the update carried out by the adversarial agent is larger than the sum of each agent\u2019s policy gradient update, the policy parameter will start moving away from the desired consensus \u03b8\u2217. The success of the adversarial attack hence depends on, \u2022 The nature of adversarial policy parameter \u03b8k\u2212 l \u2022 Non-adversarial agent\u2019s local learning rate \u03b4i \u2022 The number of non-adversarial agents n \u2212 |L| An adversarial attack is more likely to be successful if the local learning rate of non-adversarial agents \u03b4i is small and the number of adversarial agents |L| is large. Threat Model: For an adversarial agent to be successful in its attack, it needs to shift the convergence from \u03b8\u2217 in Eq. 8 to \u03b8\u2032 such that the resultant policy \u03c0\u2032 follows Esi\u223c\u03c1iV \u03c0\u2032 i (si) << Esi\u223c\u03c1iV \u03c0\u2217 i (si), \u2200i /\u2208 L (12) The only way an adversarial agent can control this convergence is through the policy parameter \u03b8k\u2212 l that it shares with the server. The adversarial agent needs to share the policy parameter that moves the distribution of the smoothing average of non-adversarial agents either to a uniform distribution or Server \u2026 Agent 0 Environment 0 Update Policy (s, a, s\u2019, r) action Agent l Environment l Update Policy (s, a, s\u2019, r) action Agent n-1 Environment n-1 Update Policy (s, a, s\u2019, r) action RL Return RL Return RL Return Communicate every n iterations Fig. 2: Adversaries can negatively impact the uni\ufb01ed policy by providing adversarial policies to the server. This results in negatively impacting the achieved discounted return on the environments in the direction that purposely yields bad actions (Fig. 3). Generally shifting the distribution to uniform distribution will require less energy than to shift it to a non-optimal action distribution. This requires that the adversary cancel out the information gained by all the other non-adversarial agents hence not being able to differentiate between good and bad actions, leaving all the actions equally likely. Threat Model: We will assume the following threat model. At iteration k, each adversarial agent l shares the following policy parameter with the server \u03b8k\u2212 l = \u03bbk\u03b8k adv (13) Hence the threat model is de\ufb01ned by the choice of the attack model \u03b8adv and \u03bbk \u2208 R which is a non-negative iterationdependant scaling factor that will be used to control the norm of the adversarial attack. To make the scaling factor more meaningful, we will assume that \u2225\u03b8adv\u22252 \u2248 1 (n \u2212 |L|) \ufffd i/\u2208L \u2225\u03b8i\u22252 The relative difference in the norm of the policy parameter between the adversarial agent and non-adversarial agent will be captured in the scaling factor term \u03bbk. One thing we need to be mindful of is the value of the scaling factor. If the scaling factor is large enough, almost any random (non-zero) values for the adversarial policy parameter \u03b8k l will result in a successful attack. We will quantify the relative performance of the threat models by \u2022 How effective they are in attacking, either in terms of the achieved discounted return under the threat induced uni\ufb01ed policy or in terms of a more problem-speci\ufb01c goal parameter (more in the experimentation section). \u2022 If two threat models achieve the same attacking performance, the one with a smaller scaling factor \u03bbk will be considered better. The smaller the norm of the adversarial policy parameter, the better the chances for the threat model to go unnoticed by the server. V. COMMON ATTACK MODELS In this section, we will discuss a few common attack models \u03b8adv and propose an adaptive attack model. For the rest of the 4 Fig. 3: The objective of an adversarial agent is to shift the policy distribution that yields poor actions section, we will focus on the single-agent adversarial model |L| = 1. The extension of these threat models to multiple adversarial agents is straight forward. A. Random Policy Attack (Rand) This attack will be used as a baseline for the other attack methods. In a Random policy attack, the adversarial agent maintains a set of random policy parameter sampled from a Gaussian distribution with mean 0 and standard deviation \u03c3 \u2208 R i.e. for each element \u03b8adv,j of \u03b8adv \u03b8adv,j \u223c N(0, \u03c3) (14) This attack assumes that the adversary has no knowledge to estimate the best attack method from. If the scaling factor \u03bbk is large enough, this attack method can shift the distribution of the policy towards a random distribution. B. Opposite Goal Policy Attack (OppositeGoal) This attack method assumes that a sample environment is available for the agent to devise the attack. In this attack method, the adversary l learns a policy \u03c0OG \u03b8adv utilizing its local environment with the goal of minimizing (instead of maximizing) the long term discounted return i.e. the objective function to maximize is J(\u03b8adv) = \u2212V \u03c0\u03b8adv l (\u03c1l) (15) With the completion of an episode k, the adversary updates its policy parameter \u03b8adv locally by maximizing Eq.15 and shares the scaled version of the updated policy parameter with the server. The OppositeGoal attack method can either shift the policy to a uniform distribution, or to a distribution which prefers actions that yield opposite goal. For the agent to shift the distribution to uniform, the following constraints need to hold. 1) All the N environments should be similar enough that they generate policies that are close enough i.e. 1 |S| \ufffd s\u2208S KL(\u03c0\u03b8i(.|s), \u03c0\u03b8j(.|s)) \u2264 \u03f5 (16) or equivalently if the learning rate and initialization is the same for each agent then \ufffd\ufffd\ufffd\ufffd \u03b8i \u2225\u03b8i\u22252 \u2212 \u03b8j \u2225\u03b8j\u22252 \ufffd\ufffd\ufffd\ufffd 2 \u2264 \u03f5 (17) 2) Action selection based on minimum probability action for OppositeGoal policy should be close enough to action selection based on maximum probability for NormalGoal Policy 1 |S| \ufffd s\u2208S KL(1 \u2212 \u03c0OG \u03b8i (.|s), \u03c0\u03b8j(.|s)) \u2264 \u03f5 (18) or equivalently if the learning rate is same initialization is zero \u03b80 i = 0 \ufffd\ufffd\ufffd\ufffd \u03b8i \u2225\u03b8i\u22252 + \u03b8j \u2225\u03b8j\u22252 \ufffd\ufffd\ufffd\ufffd 2 \u2264 \u03f5 (19) In short, all the N environments should be similar enough such that training on an opposite goal will yield a policy that when combined with a policy learned to maximize the actual goal will yield a complete information loss. Most of the time, these assumptions will not hold as they are too strict (the difference in environment dynamics, initialization of policy parameter, the existence of multiple local minima, etc.). Instead, if the scaling factor is large, the OppositeGoal attack will shift the distribution of the consensus to an opposite goal policy. Since we are taking into account the environment dynamics, this attack will however be better than the random policy attack. C. Adversarial Attack by Minimizing Information Gain (AdAMInG) Even though the adversarial choice of opposite goal makes an intuitive sense as the best attack method, we will see in the results section that it\u2019s not. Hence, we propose an attack method that takes into account the nature of MT-FedRL smoothing averaging and devises the best attack given the information available locally. The goal of AdAMInG is to devise an attack that uses a single adversarial agent with a small scaling factor by forcing the server to forget what it learns from the non-adversarial agents. For the smoothing average at the server to lose all the information gained by other non-adversarial agents we should have \u03b8k\u2212 l = \u2212 1 \u03b2k|L| \uf8eb \uf8ed\u03b1k\u03b8k\u2212 i + \u03b2k \ufffd j\u0338=i,l \u03b8k\u2212 j \uf8f6 \uf8f8 (20) Using the above equation in Eq. 9 will result \u03b8k+ i = 0, hence losing the information gained by \u03b8k\u2212 i . The problem in the above equation is that the adversarial agents do not have access to the policy parameter shared by non-adversarial agents \u03b8k\u2212 i , \u2200i \u0338= l and hence the quantity in the parenthesis (smoothing average of the non-adversarial agents) is unknown. The attack model then is to estimate the smoothing average of the non-adversarial agents. The adversarial agent has the following information available to it \u2022 The last set of policy parameter shared by the adversarial agent to the server \u03b8(k\u22121)\u2212 l \u2022 The federated policy parameter shared by the server to the adversarial agent \u03b8(k\u22121)+ l 5 The adversarial agent can estimate the smoothing average of the non-adversarial agents from these quantities. The AdAMInG attack shares the following policy parameter \u03b8k\u2212 l = \u03bbk \ufffd \u03b1k\u03b8(k\u22121)+ l \u2212 \u03b8(k\u22121)\u2212 l \u03b2k \ufffd (21) The smoothing average at the server for i \u2208 {0, n\u22121}, i \u0338= l becomes \u03b8k+ i = \u03b1k\u03b8k\u2212 i + \u03b2k \ufffd i\u0338=j,l \u03b8k\u2212 j + \u03b2k\u03b8k\u2212 l = \u03b1k\u03b8k\u2212 i + \u03b2k \ufffd i\u0338=j,l \u03b8k\u2212 j \u2212 \u03bbk n \u2212 1\u03b1k \ufffd \u03b8(k\u22121)+ l \u2212 \u03b8(k\u22121)\u2212 l \ufffd = \u03b1k\u03b8k\u2212 i + \u03b2k \ufffd i\u0338=j,l \u03b8k\u2212 j \u2212 \u03bbk n \u2212 1\u03b2k \ufffd j\u0338=l \u03b8(k\u22121)\u2212 j = \ufffd \u03b1k\u03b8k\u2212 i \u2212 \u03bbk n \u2212 1\u03b2k\u03b8(k\u22121)\u2212 i \ufffd + \ufffd j\u0338=i,l \ufffd \u03b2k\u03b8k\u2212 j \u2212 \u03b2k\u03bbk n \u2212 1\u03b8(k\u22121)\u2212 j \ufffd We want \u03b8k+ i \u2192 0, \u2200i \u2208 {0, n\u22121}, i \u0338= l. This means forcing the two terms inside the parenthesis to 0. If the initialization of all the agents are same, i.e. \u03b80\u2212 i = \u03b80 = 0 \u2200i and the learning rate is small, we have \u2225\u03b8k\u2212 i \u2212 \u03b8(k\u22121)\u2212 i \u2225 < \u03f5. Hence \u03b8k+ i \u2192 0 can be achieved by the following scaling factor \u03bbk\u2217 = argmin\u03bbk g(\u03bbk, n) where g(\u03bbk, n) = \ufffd\ufffd\ufffd\ufffd\u03b1k \u2212 \u03b2k \u03bbk n \u2212 1 \ufffd\ufffd\ufffd\ufffd + \ufffd\ufffd\ufffd\ufffd\u03b2k \ufffd 1 \u2212 \u03bbk n \u2212 1 \ufffd (n \u2212 2) \ufffd\ufffd\ufffd\ufffd For simplicity we have not shown the dependence of \u03b1k, \u03b2k in the expression g(\u03bbk, n) as they directly depend on k. Solving this optimization problem yields \u03bb\u2217 = n \u2212 1, n \u2265 3 (22) This means that the scaling factor should be equal to the number of non-adversarial agents and is independent of the iteration k. For \u03bbk < \u03bb\u2217 we still can achieve a successful attack if the learning rate \u03b4 is not too high. As the training proceeds, the values of the smoothing constants \u03b1k, \u03b2k approach their steady-state value of 1 n. At that point, the steady-state value of g(\u03bbk, n) de\ufb01ned as gss(\u03bbk, n) is given by gss(\u03bbk, n) = n \u2212 1 \u2212 \u03bb n (23) The steady-state value gss(\u03bbk, n) signi\ufb01es how effective/successful the AdAMInG attack will be for the selected parameters (\u03bbk, n). A steady-state value of 0 signi\ufb01es a perfect attack, where the policy parameter shared by the server loses all the information gained by the non-adversarial agents. On the other hand, a steady-state value of 1 indicates a completely 0 20 40 60 80 100 Number of agents (n) 0.4 0.5 0.6 0.7 0.8 0.9 1.0 gss(\u03bb, n) Fig. 4: g \ufffd \u03bbk, n \ufffd as a function of \u03bbk = 1 and n 0 20 40 60 80 100 Scaling factor (\u03bb) 0.0 0.2 0.4 0.6 0.8 1.0 gss(\u03bb, n) Fig. 5: g \ufffd \u03bbk, n \ufffd as a function of \u03bbk and n = 100 unsuccessful attack. The smaller the gss(\u03bbk, n), the better the AdAMInG attack. Fig. 4 plots g(\u03bbk, n) as a function of the number of agents n for a scaling factor of 1 (\u03bbk = 1). It can be seen that as the number of agents increases, the steady-state value gss becomes closer to 1 making it dif\ufb01cult for AdAMInG to have a successful attack with a scaling factor of 1. As the number of agents increases, the update carried out by the non-adversarial agent has a more signi\ufb01cant impact on the smoothing average than the adversarial agent making it harder for the adversarial agent to attack. Fig. 5 plots g(\u03bbk, n) as a function of the scaling factor \u03bbk for n = 100. It can be seen that the scaling factor has a linear impact on the success of the AdAMInG attack. The performance of the AdAMInG attack increases linearly with the increase in the scaling factor. The best AdAMInG attack is achieved when \u03bbk = n\u22121 which is consistent with Eq. 22. In the experimentation section, we will see that a non-zero steady-state value can still result in a successful attack for a small learning rate \u03b4. It is safe to assume that if we do not change the learning rate (and it is small enough), we can \ufb01nd the scaling factor required to achieve the same attacking performance by increasing the number of agents n. The steady-state relationship between n and \u03bb in Eq. 23 lets us analyze the relative attacking performances by varying the number of agents n. Let\u2019s say 6 Evaluated Policy Environment Cumulative reward Non-adv Non-adv High Non-adv Adv Low (Secondary Attack) Adv Non-adv Low Adv Adv Low (Secondary Attack) TABLE I: Cross-evaluation of policies in ComA-FedRL in terms of cumulative return that for n1 number of agents and a given learning rate that is small, we were able to achieve a successful attack with \u03bb1. Now to achieve the same successful attack for n2 number of agents we need \u03bb2 = n2(1 + \u03bb1) n1 \u2212 1 (24) Unlike the OppositeGoal attack, we can guarantee that the AdAMInG attack will yield a successful attack if the scaling factor is equal to the number of non-adversarial agents. We will see in the results section that the scaling factor needs no to be this high if the learning rate \u03b4 is not high. We will be able to achieve a good enough attack even if \u03bbk < n \u2212 1. The only down-side with the AdAMInG attack method is that it requires twice the amount of memory as compared to that of OppositeGoal or Rand attack method. AdAMInG attack method needs to store the both the adversary shared policy parameter \u03b8(k\u22121)\u2212 l and the server shared policy parameter \u03b8(k\u22121)+ l from the previous iteration to compute the new set of policy parameters to be shared \u03b8k\u2212 l as shown in Eq. 21. However, as opposed to the OppositeGoal attack method, the AdAMInG attack method does not require to learn from the data sampled from environment saving up much on the compute cost. VI. DETECTING ATTACKS - COMA-FEDRL We will see in Sec. VII that the FedRL algorithm under the presence of an adversary can severely affect the performance of the uni\ufb01ed policy. Hence, we propose Communication Adaptive Federated RL (ComA-FedRL) to address the adversarial attacks on a Federated RL algorithm. Instead of communicating the policy parameter from all agents at a \ufb01xed communication interval, we assign different communication intervals to agents based on the con\ufb01dence of them being an adversary. An agent, with higher con\ufb01dence of being an adversary, is assigned a large communication interval and vice-versa. Communicating less frequently with an adversary agent can greatly mitigate its effects on the learned uni\ufb01ed policy. Since we can\u2019t guarantee that a certain agent is an adversary or not, we can\u2019t just cut off the communication with an agent we think would be an adversary. Moreover, an adversary can fake being a non-adversarial agent to get away with being marked as an adversarial agent. Hence, we don\u2019t mark agents as adversary or non-adversary, rather we adaptively vary the communication interval between the server and the agents based on how good, on average, does the policy of the agent performs in other environments. The complete algorithm of ComA-FedRL can be seen in Algo. 2. Algorithm 2: Communication Aware Federated RL Initialization: Initialize number agents n, \u03b80 i \u2208 Rd, step size \u03b4k, base_comm \u2208 R, comm[i] = base_comm \u2200i \u2208 {0, n \u2212 1}, wait_comm \u2208 R for k=1,2,3,... do % Pre-train phase if k \u2264 wait_comm then if k%base_comm = 0 then for each agent i in parallel do \u03b8(k+1)\u2212 i \u2190 ClientUpdate \ufffd i, \u03b8k+ i \ufffd end r \u2190 CrossEvalPolicies \ufffd r, \u03b8(k+1)\u2212\ufffd end end else Calculate smoothing average parameters \u03b1k, \u03b2k comm \u2190 UpdateCommInt(r, comm) for each agent i in parallel do if k%comm[i] = 0 then \u03b8(k+1)\u2212 i \u2190 ClientUpdate \ufffd i, \u03b8k+ i \ufffd end end num_active_agents = 0 for each agent i do if k%comm[i] = 0 then num_active_agents+=1 \u03b8(k+1)+ i = \u03b1k\u03b8(k+1)\u2212 i + \u03b2k \ufffd i\u0338=j \u03b8(k+1)\u2212 j Send \u03b8(k+1)+ i back to client i end end if num_active_agents = n then r \u2190 CrossEvalPolicies \ufffd r, \u03b8(k+1)\u2212\ufffd end end end Function CrossEvalPolicies(r, \u03b8): for each agent i in parallel do Randomly assign each agent i another agent j without replacement rj.append(ClientEvaluate(i, \u03b8j)) end return r Function ClientUpdate(i,\u03b8): 1) Compute the gradient of the local value function \u2202V \u03c0\u03b8 i (\u03c1i) \u2202\u03b8si,ai based on the local data; 2) Update the policy parameter \u03b8\u2212 = \u03b8 + \u03b4k \u2202V \u03c0\u03b8 i (\u03c1i) \u2202\u03b8si,ai return \u03b8\u2212 Function ClientEvaluate(i, \u03b8j): Evaluate the policy \u03b8j on agent i and return the cumulative reward ret return ret 7 Fig. 6: [GridWorld] The 12 environments used ComA-FedRL begins with a pre-train phase, where each agent tries to learn a locally optimistic policy independent of others. These locally optimistic policies are expected to perform better than a random policy on other agents\u2019 environments. After every certain number of episodes, the server randomly assigns a policy to all the environments without replacement for evaluation, and the cumulative reward achieved by this policy is recorded. Based on the nature of the policy and the environment it is cross-evaluated in, we have four cases as shown in Table I. When the policy locally learned by a non-adversarial agent is evaluated in the environment of a non-adversarial agent, it generally performs better than a random policy because of the correlation of the underlying tasks. Hence we get a slightly higher cumulative reward compared to other cases. On the other hand, if an adversarial policy is cross-evaluated on a non-adversarial agent\u2019s environment, it generally performs worse because of the inherent nature of the adversary, giving a low cumulative reward. When the policies are evaluated on the adversarial agent\u2019s environment, the adversary can present a secondary attack in faking the cumulative reward. It intentionally reports a low cumulative return with the hopes of confusing the server to mistake a non-adversarial agent with an adversarial one. Since the adversarial agent has no way of knowing if the policy shared by the server belongs to an adversarial or a non-adversarial agent, it always shares a low cumulative return. At the end of the pre-train phase, the cumulative rewards are averaged out for a given policy and are compared to a threshold. If the averaged cumulative reward of the policy is below (above) this threshold, the policy is marked as possibly-adversarial (possibly-non-adversarial). The possiblyadversarial agents are assigned a higher communication interval (less frequent communication), while possibly-nonadversarial agents are assigned a smaller communication interval (more frequent communication). The agents are constantly re-evaluated after a certain number of iterations and the categories associated with the agents are updated. After re-evaluation, if an already marked possible-adversary agent is re-marked as possibly-adversary, the agent\u2019s communication interval is doubled, signifying a higher probability of it being an adversary and making it contribute even lesser towards the server smoothing average. Hence as the training proceeds, the adversarial agent\u2019s contribution to the server smoothing average becomes smaller and smaller. Further details on the variables and functions used in Alg. 2 can be found in the Appendix section A. VII. EXPERIMENTATION For the entire experimentation section, we focus on singleadversary MT-FedRL and hence |L| = 1. We report the experimental results from a simpler tabular-based RL problem (GridWorld) to a more complex neural network-based RL problem (AutoNav). In both cases, we use policy gradient based RL methods. A. GridWorld - Tabular RL Problem Description: We begin our experimentation with a simple problem of GridWorld. The environments are grid world mazes of size 10 \u00d7 10 as seen in Fig. 6. Each cell in the maze is characterized into one of the following 4 categories: hell-cell (red), goal-cell (yellow), source-cell (green), and free-cell (white). The agent is initialized at the source-cell and is required to reach the goal-cell avoiding getting into the hell-cell. The free-cells in the maze can be occupied without any consequences. The agent can take one of the following 4 actions A = {move-up, move-down, move-right, move-left} which corresponds to the agent moving one cell in the respective direction. At each iteration, the agent observes a one-step SONAR-based state s \u2208 R4 which corresponds to the nature of the four cells (up, down, right, left) surrounding the agent. If the corresponding cell is a hell-cell, goal-cell, or free-cell, the corresponding state element is -1, 1, or 0 respectively. Hence, we have |S| = 81. Based on the nature of the environment, only a subset of these states will be available for each environment. At each iteration, the agent samples an action from the action space and based on the next state, observes a reward. The reward is -1, 1, 0.1, or -0.1 if the agent crashed into hell-cell, reached the goal, moved closer to or away from the goal respectively. The effectiveness of the MT-FedRL-achieved uni\ufb01ed policy is quanti\ufb01ed by the win ratio (WR) de\ufb01ned by WR = 1 n \u2212 1 \ufffd i\u0338=l # of times agent i reached goal state total # of attempts in environment i In this 12-agent MT-FedRL system, agent 0 is assigned the adversarial role (l = 0). The goal for agent 0 is to decrease this win ratio. We will characterize the performance of the 8 \u03bb = n \u2212 1 \u03bb = 1 Scaling factor \u03bb 0 20 40 60 80 100 psa(%) AdAMInG OppositeGoal Random Fig. 7: [GridWorld] Probability of successful attack psa(%) under different attack models. The greater the psa the better the performance of the adversary. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Learning rate \u03b4 0 20 40 60 80 100 psa(%) AdAMInG OppositeGoal Random Fig. 8: [GridWorld] Effect of learning rate (\u03b4) on the performance of attack methods with \u03bb = 1 and n = 12. adversarial attack by the probability of successful attack psa given by psa = 1 \u2212 WRadv WRno\u2212adv where WRadv is the win ratio with an adversary, while WRno\u2212adv is the win ratio without any adversary. An attack method is said to be successful with probability psa if it is able to defeat the system psa% of the time compared to a non-adversarial MT-FedRL. The greater the psa the better the attack performance of the adversary. Effect of Adversaries We begin the experimentation by analyzing the effect of the common attack models mentioned in Sec. V. Fig. 7 reports the psa for the three attack methods with the scaling factor of n \u2212 1 and 1 (and a learning rate \u03c3 = 0.2). With the optimal scaling factor of n \u2212 1, it can be seen that all the three attack methods were able to achieve a good enough attack (psa > 96%). For a scaling factor of 1, however, only AdAMInG attack method was able to achieve a successful attack (psa = 98%). Both the random policy attack and OppositeGoal attack were only half as good as the AdAMInG attack method with OppositeGoal being only slightly better than a random policy attack. As mentioned in section IV, for a scaling factor of 1, the performance of the attack method depends on the learning rate (\u03b4) and the number of non-adversarial agents (n \u2212 |L|). Fig. 8 reports psa of the attack methods with varying learning 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Learning rate \u03b4 0 20 40 60 80 100 psa(%) AdAMInG (\u03bb = 1) OppositeGoal (\u03bb = 2) Fig. 9: [GridWorld] Comparing attack performance for n = 12 between AdAMInG with \u03bb = 1 and OppositeGoal with \u03bb = 2. n=4 n=8 n=12 Number of agents n 0 20 40 60 80 100 psa(%) AdAMInG OppositeGoal Random Fig. 10: [GridWorld] Effect of number of agents (n) on the performance of attack methods with \u03bb = 1 and \u03b4 = 0.2 rates. It can be seen that the greater the learning rate, the poorer the performance of the attack method in terms of psa. For a higher learning rate, the local update for each agent\u2019s policy parameter has more effect than the update of the server carried out with the adversary, and hence poorer the performance of the attack. Another thing to observe is that as the learning rate increases the relative performance of the OppositeGoal attack compared to the Random policy attack becomes poorer even becoming worse than the Random policy attack. The reason behind this is that the observable states across environments are non-overlapping. The environment available to the adversary for devising OppositeGoal attack from might not have access to the states observable in other environments. Hence the OppositeGoal policy attack can not modify the policy parameter related to those states. OppositeGoal policy attack method either require a large scaling factor or more than one adversary to attack the MTFedRL with performance similar to AdAMInG with singleadversary and unity scaling factor \u03bb. This can be seen in Fig. 9. A similar trend can be observed with varying the number of non-adversarial agents. It can be seen in Fig. 10 that for a smaller number of non-adversarial agents (equivalently smaller number of total agents if the number of the adversarial agents is \ufb01xed), it is easier for the adversary to attack with a high psa. The reason behind this is that the local update in Eq. 11 is proportional to the number of non-adversarial agents. With 9 0 250 500 750 1000 1250 1500 1750 2000 Number of episodes k 0 2 4 6 8 10 12 1 n\u22121 \ufffd i\u0338=l \u2225\u03b8i\u22252 . \u03b4 . 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Fig. 11: [GridWorld] Based on the learning rate, the consensus gets converged to an intermediate value 0 250 500 750 1000 1250 1500 1750 2000 Number of episodes k \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 1 n\u22121 \ufffd i\u0338=l \ufffd\ufffd\u221e j rj \ufffd . \u03b4 . 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Fig. 12: [GridWorld] Cumulative return (moving average of 60) for different learning rate (\u03b4) and n = 12 a smaller number of non-adversarial agents, the local update is smaller compared to the update by the adversary. Among the three attack methods, AdAMInG is the most resilient to these two parameters (\u03bb, n), hence making it a better choice for an adversarial attack in an MT-FedRL setting. Analyzing AdAMInG Attack: We carry out a detailed analysis of the AdAMInG attack method. The smoothing average (Eq. 11) in the presence of an adversary carries out two updates - the local update which moves the policy parameter in a direction to maximize the collective goal, and the adversarial update which tries to move the policy parameter away from the consensus. When the training begins, the initial set of policy parameters \u03b8i is farther away from the consensus \u03b8\u2217. Gradient descent \ufb01nds a direction from the current set of policy parameters to the consensus. This direction has a higher magnitude when the distance between the current policy parameter and the consensus is high. As the system learns, the current policy parameter gets closer to the consensus, and hence the magnitude of the direction of update decreases. So even if we have a static learning rate \u03b4, the magnitude of local update \u03b4j\u2207\u03b8jV \u03c0\u03b8j j (\u03c1j) in Eq. 11 will, in general, decrease as the system successfully learns. There will be a point in training where the local update will become equal but opposite to the update being carried out by the AdAMInG adversary. From that point onwards, the current policy parameter won\u2019t change much. This can be seen in Fig. 11. The greater the learning rate \u03b4, the earlier in training we will get to the equilibrium point, and hence 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Learning rate \u03b4 0.0 0.1 0.2 0.3 0.4 std (\u03b8\u2217) AdAMInG Fig. 13: [GridWorld] Standard deviation of the consensus policy parameter Con\ufb01guration Learning rate \u03b4 psa% std \u03bb = 1, n = 8 0.2 99.75% 0.036 \u03bb = 2, n = 12 0.2 99.49% 0.031 TABLE II: [GridWorld] Relationship between \u03bb and n for same attack performance with AdAMInG poorer the attack performance which can be seen in terms of the achieved discounted return in Fig. 12. A greater standard deviation of the consensus policy parameter indicates a better differentiation between good and bad actions for a given state. Fig. 13 plots the standard deviation of the consensus policy parameter for different learning rates \u03b4. It can be seen that for higher learning rates, the consensus has a higher standard deviation hence being able to perform better than the consensus achieved under lower learning rates. We also compare the performance of the AdAMInG attack in relation to the scaling factor \u03bb and the number of agents n. According to Eq. 24 an increase in the number of agents can be compensated by increasing the scaling factor \u03bb to achieve the same attacking performance. We analyse the AdAMInG attack for the following two con\ufb01gurations: (\u03bb = 1, n = 8) and (\u03bb = 2, n = 12). Table II reports the psa and the standard deviation of the consensus policy parameter \u03b8\u2217. It can be seen that both the con\ufb01gurations generate similar numbers. The same trend can be observed temporally, in Fig. 14, for the achieved discounted return during each episode in training. Resolving adversaries: We implement the N-agent singleadversary MT-FedRL problem using ComA-FedRL to address the high psa of the conventional FedRL algorithm. Fig. 15 compares the performance of FedRL and ComA-FedRL for different attack methods. By assigning a higher communication interval to the probable adversary, ComA-FedRL was able to decrease the probability of successful attack psa in the presence of adversary to as low as < 10%. The mean communication interval for adversarial and non-adversarial agents is plotted in Fig. 16. It can be seen that Random policy attack has a slightly higher communication interval. The reason behind this is one of the non-adversarial agents was incorrectly marked as a probable adversarial agent at the beginning of training, but later that was self-corrected to a possibly-non-adversarial agent. 10 0 250 500 750 1000 1250 1500 1750 2000 Number of episodes k \u22121.00 \u22120.75 \u22120.50 \u22120.25 0.00 0.25 0.50 0.75 1 n\u22121 \ufffd i\u0338=l \ufffd\ufffd\u221e j rj \ufffd \u03bb = 2, n = 12 \u03bb = 1, n = 8 Fig. 14: [GridWorld] Relationship between \u03bb and n for same AdAMInG attack performance. (\u03bb = 1, n = 8) and (\u03bb = 2, n = 12) follows the same discounted return across episodes which is in accordance with Eq. 24 FedRL Coma-FedRL 0 20 40 60 80 100 psa(%) Adaming OppositeGoal Random No Adversary Fig. 15: [GridWorld] Comparison of probability of successful attack psa(%) under different attack models for FedRL and ComA-FedRL. The effect of adversarial agent is greatly reduced with ComA-FedRL. B. AutoNav - NN based RL Problem Description: We also experiment on a more complex problem of drone autonomous navigation in 3D realistic environments. We use PEDRA [40] as the drone navigation platform. The drone is initialized at a starting point and is required to navigate across the hallways of the environments. There is no goal position, and the drone is required to \ufb02y avoiding the obstacles as long as it can. At each iteration t, the drone captures an RGB monocular image from the frontfacing camera which is taken as the state st \u2208 R(320\u00d7180\u00d73) of the RL problem. Based on the state st, the drone takes an action at \u2208 A. We consider a perception based probabilistic action space with 25 actions (|A| = 25). A depth-based reward function is used to encourage the drone to stay away from the obstacles. We use neural network-based function approximation to estimate the action probabilities based on states. The C3F2 network used is shown in Fig. 17. We consider 4 indoor environments (indoor-twist, indoor-frogeyes, indoor-pyramid, and indoor-complex) hence we have n = 4. These environments can be seen in Fig. 18. The effectiveness of MT-FedRL-achieved uni\ufb01ed policy is AdAMInG OppositeGoal Random No Adversary Attack method 0 100 200 300 400 500 600 Communication Intervals Adv agents Non-Adv agents Fig. 16: [GridWorld] Average communication intervals for adversarial and non adversarial agents in ComA-FedRL Conv Layer MaxPool Fully Connected Layer Input Feature Map 103 103 12 12 96 7 7 3 3 5 5 2 2 3 3 3 64 1024 1024 25 4 4 Conv1 Conv2 Conv3 FC1 FC2 Fig. 17: [AutoNav] C3F2 neural network used to map states to action probabilities quanti\ufb01ed by Mean Safe Flight (MSF) de\ufb01ned as MSF = 1 n \u2212 1E \uf8ee \uf8f0\ufffd i\u0338=l di \uf8f9 \uf8fb where di is the distance traveled by the agent in the environment i before crashing. In this 4-agent MT-FedRL system, the agent in the environment indoor-complex is assigned the adversarial role (l = 3). The goal for the adversarial agent is to decrease this MSF. We will characterize the performance of the adversarial attack by the probability of successful attack psa given by psa = 1 \u2212 MSFadv MSFno\u2212adv where MSFadv is the mean safe \ufb02ight of the MT-FedRL system in the presence of the adversary, while MSFno\u2212adv is the mean safe \ufb02ight of the MT-FedRL system in the absence of the adversary. The greater the psa the better the attack method in achieving its goal. Effect of Adversaries: For each experiment, the MT-FedRL problem is trained for 4000 episodes using the REINFORCE algorithm with a learning rate of 1e-4 and \u03b3 = 0.99. Training hyper-parameters are listed in the appendix section A in detail. Table III reports the MSF achieved by the AutoNav problem for various attack methods. It can be seen that except for the AdAMInG attack, the rest of the attack methods achieve MSF comparable to the one achieved in the absence of an adversary (\u223c 1000m). Fig. 19 plots the psa for the different 11 ",
    "Conclusion": "CONCLUSION In this paper we analyse Multi-task Federated Reinforcement Learning algorithm with an adversarial perspective. We analyze the attacking performance of some general attack methods and propose an adaptive attack method AdAMInG that devises an attack taking into account the aggregation operator of federated RL. The AdAMinG attack method is formulated and its effectiveness is studied. Furthermore, to address the issue of adversaries in MT-FedRL problem, we propose a communication adaptive modi\ufb01cation to conventional federated RL algorithm, ComA-FedRL, that varies the communication frequency for the agents based on their probability of being an adversary. Results on the problems of GridWorld (maze solving) and AutoNav (drone autonomous navigation) show that the AdAMInG attack method outperforms other attack methods almost every time. Moreover, ComA-FedRL was able to recover from the adversarial attack resulting in nearoptimal policies. IX. ACKNOWLEDGEMENTS This work was supported in part by C-BRIC, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. ",
    "References": "REFERENCES [1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., \u201cHuman-level control through deep reinforcement learning,\u201d nature, vol. 518, no. 7540, pp. 529\u2013533, 2015. [2] M. A. Anwar and A. Raychowdhury, \u201cNavren-rl: Learning to \ufb02y in real environment via end-to-end deep reinforcement learning using monocular images,\u201d in 2018 25th International Conference on Mechatronics and Machine Vision in Practice (M2VIP). IEEE, 2018, pp. 1\u20136. [3] C. Wang, J. Wang, Y. Shen, and X. Zhang, \u201cAutonomous navigation of uavs in large-scale complex environments: A deep reinforcement learning approach,\u201d IEEE Transactions on Vehicular Technology, vol. 68, no. 3, pp. 2124\u20132136, 2019. [4] J. Kober, J. A. Bagnell, and J. Peters, \u201cReinforcement learning in robotics: A survey,\u201d The International Journal of Robotics Research, vol. 32, no. 11, pp. 1238\u20131274, 2013. [5] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, and M. Guizani, \u201cSecurity in mobile edge caching with reinforcement learning,\u201d IEEE Wireless Communications, vol. 25, no. 3, pp. 116\u2013122, 2018. [6] S. Li, L. Da Xu, and S. Zhao, \u201cThe internet of things: a survey,\u201d Information Systems Frontiers, vol. 17, no. 2, pp. 243\u2013259, 2015. [7] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Kone\u02c7cn`y, S. Mazzocchi, H. B. McMahan et al., \u201cTowards federated learning at scale: System design,\u201d arXiv preprint arXiv:1902.01046, 2019. [8] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth, \u201cPractical secure aggregation for privacy-preserving machine learning,\u201d in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. 1175\u20131191. 12 [9] J. Kone\u02c7cn`y, H. B. McMahan, D. Ramage, and P. Richt\u00e1rik, \u201cFederated optimization: Distributed machine learning for on-device intelligence,\u201d arXiv preprint arXiv:1610.02527, 2016. [10] S. Kumar, P. Shah, D. Hakkani-Tur, and L. Heck, \u201cFederated control with hierarchical multi-agent deep reinforcement learning,\u201d arXiv preprint arXiv:1712.08266, 2017. [11] H. H. Zhuo, W. Feng, Q. Xu, Q. Yang, and Y. Lin, \u201cFederated reinforcement learning,\u201d arXiv preprint arXiv:1901.08277, 2019. [12] G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani, \u201cLenient multiagent deep reinforcement learning,\u201d arXiv preprint arXiv:1707.04402, 2017. [13] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, \u201cA survey and critique of multiagent deep reinforcement learning,\u201d Autonomous Agents and Multi-Agent Systems, vol. 33, no. 6, pp. 750\u2013797, 2019. [14] H.-K. Lim, J.-B. Kim, J.-S. Heo, and Y.-H. Han, \u201cFederated reinforcement learning for training control policies on multiple iot devices,\u201d Sensors, vol. 20, no. 5, p. 1359, 2020. [15] B. Liu, L. Wang, and M. Liu, \u201cLifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems,\u201d IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 4555\u20134562, 2019. [16] S. Zeng, A. Anwar, T. Doan, J. Romberg, and A. Raychowdhury, \u201cA decentralized policy gradient approach to multi-task reinforcement learning,\u201d arXiv preprint arXiv:2006.04338, 2020. [17] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv preprint arXiv:1312.6199, 2013. [18] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572, 2014. [19] A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial machine learning at scale,\u201d arXiv preprint arXiv:1611.01236, 2016. [20] F. Tram\u00e8r, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, \u201cEnsemble adversarial training: Attacks and defenses,\u201d arXiv preprint arXiv:1705.07204, 2017. [21] A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial examples in the physical world,\u201d arXiv preprint arXiv:1607.02533, 2016. [22] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, \u201cDeepfool: a simple and accurate method to fool deep neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2574\u20132582. [23] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, \u201cThe limitations of deep learning in adversarial settings,\u201d in 2016 IEEE European symposium on security and privacy (EuroS&P). IEEE, 2016, pp. 372\u2013387. [24] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel, \u201cAdversarial attacks on neural network policies,\u201d arXiv preprint arXiv:1702.02284, 2017. [25] J. Kos and D. Song, \u201cDelving into adversarial attacks on deep policies,\u201d arXiv preprint arXiv:1705.06452, 2017. [26] Y. Huang and Q. Zhu, \u201cDeceptive reinforcement learning under adversarial manipulations on cost signals,\u201d in International Conference on Decision and Game Theory for Security. Springer, 2019, pp. 217\u2013237. [27] Y. Ma, X. Zhang, W. Sun, and J. Zhu, \u201cPolicy poisoning in batch reinforcement learning and control,\u201d in Advances in Neural Information Processing Systems, 2019, pp. 14 570\u201314 580. [28] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and M. Sun, \u201cTactics of adversarial attack on deep reinforcement learning agents,\u201d arXiv preprint arXiv:1703.06748, 2017. [29] V. Behzadan and A. Munir, \u201cThe faults in our pi stars: Security issues and open challenges in deep reinforcement learning,\u201d arXiv preprint arXiv:1810.10369, 2018. [30] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell, \u201cAdversarial policies: Attacking deep reinforcement learning,\u201d arXiv preprint arXiv:1905.10615, 2019. [31] P. Blanchard, R. Guerraoui, J. Stainer et al., \u201cMachine learning with adversaries: Byzantine tolerant gradient descent,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 119\u2013129. [32] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, \u201cAnalyzing federated learning through an adversarial lens,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 634\u2013643. [33] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, \u201cFeature denoising for improving adversarial robustness,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 501\u2013509. [34] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, \u201cRobust adversarial reinforcement learning,\u201d arXiv preprint arXiv:1703.02702, 2017. [35] A. Mandlekar, Y. Zhu, A. Garg, L. Fei-Fei, and S. Savarese, \u201cAdversarially robust policy learning: Active construction of physicallyplausible perturbations,\u201d in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 3932\u20133939. [36] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary, \u201cRobust deep reinforcement learning with adversarial attacks,\u201d arXiv preprint arXiv:1712.03632, 2017. [37] C. Tessler, Y. Efroni, and S. Mannor, \u201cAction robust reinforcement learning and applications in continuous control,\u201d arXiv preprint arXiv:1901.09184, 2019. [38] N. Rodr\u00edguez-Barroso, E. Mart\u00ednez-C\u00e1mara, M. Luz\u00f3n, G. G. Seco, M. \u00c1. Veganzones, and F. Herrera, \u201cDynamic federated learning model for identifying adversarial clients,\u201d arXiv preprint arXiv:2007.15030, 2020. [39] R. R. Yager and D. P. Filev, \u201cInduced ordered weighted averaging operators,\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 29, no. 2, pp. 141\u2013150, 1999. [40] A. Anwar and A. Raychowdhury, \u201cAutonomous navigation via deep reinforcement learning for resource constraint edge nodes using transfer learning,\u201d IEEE Access, vol. 8, pp. 26 549\u201326 560, 2020. Aqeel Anwar received his Bachelor\u2019s degree in Electrical Engineering from the University of Engineering and Technology (UET), Lahore, Pakistan, and Masters degree in Electrical and Computer Engineering from Georgia Institute of Technology, Atlanta, GA, USA in 2012 and 2017 respectively. Currently, he is pursuing his Ph.D. in Electrical and Computer Engineering from the Georgia Institute of Technology under the supervision of Dr. Arijit Raychowdhury. His research interests lie at the junction of machine learning and hardware design. He is working towards shifting Machine Learning (ML) from cloud to edge nodes by improving the energy ef\ufb01ciency of current state-of-the-art ML algorithms and designing ef\ufb01cient DNN accelerators. Arijit Raychowdhury (Senior Member, IEEE) received the Ph.D. degree in electrical and computer engineering from Purdue University, West Lafayette, IN, USA, in 2007. His industry experience includes \ufb01ve years as a Staff Scientist with the Circuits Research Laboratory, Intel Corporation, Portland, OR, USA, and a year as an Analog Circuit Researcher with Texas Instruments Inc., Bengaluru, India. He joined the Georgia Institute of Technology, Atlanta, GA, USA, in 2013, where he is currently an Associate Professor with the School of Electrical and Computer Engineering and also holds an ON Semiconductor Junior Professorship. He holds more than 25 U.S. and international patents and has published over 100 papers in journals and refereed conferences. His research interests include low-power digital- and mixed-signal circuit design, device\u2013circuit interactions, and novel computing models and hardware realizations. Dr. Raychowdhury was a recipient of the Dimitris N. Chorafas Award for Outstanding Doctoral Research in 2007, the Intel Labs Technical Contribution Award in 2011, the Best Thesis Award from the College of Engineering, Purdue University, in 2007, the Intel Early Faculty Award in 2015, the NSF CISE Research Initiation Initiative Award (CRII) in 2015, and multiple best paper awards and fellowships. 13 APPENDIX A. Training details Policy gradient methods for RL is used to train both the GridWorld and AutoNav RL problems. For ComA-FedRL, we use a base communication base_comm. In the pre-train phase, the communication interval for each agent is assigned this base communication i.e. comm[i] = base_comm \u2200i \u2208 {0, n \u2212 1} This means that in the pre-train phase, the agents learn only on local data, and after every base_comm number of episodes, the locally learned policies are shared with the server for crossevaluation. This cross-evaluation runs n policies, each on a randomly selected environment and the cumulative reward is recorded. We also take into account the fact that the adversarial agent can present a secondary attack in terms of faking the cumulative reward that it return when evaluating a policy. In the ComA-FedRL implementation, we assume that the adversarial agent returns a cumulative reward of \u22121, meaning that it fakes the policy being evaluated as adversarial. At the end of the pre-train phase, the cross evaluated rewards are used to assign communication intervals to all the agents. There are various choices for the selection of this mapping. The underlying goal is to assign a higher communication interval for agents whose policy performs poorly when cross-evaluated and vice versa. We use the mapping shown in Alg. 3. A reward threshold rth is used to assign agents different communication intervals. If the cumulative reward of a policy in an environment is below rth, it is assigned a high communication interval of high_comm episodes (marked as a possible adversary), otherwise it is assigned a low communication interval of low_comm episodes (marked as a possible non-adversary). The assigned communication interval also depends on the one-step history of communication intervals. If an agent was previously assigned a higher communication interval and is again marked as a possible adversary, the communication interval assigned to such an agent is doubled. The complete list of hyperparameters used for GridWorld and AutoNav can be seen in Table IV. Algorithm 3: Update Communication Intervals Function UpdateCommInt(rm\u00d7n, comm): Initialize low_comm, high_comm, rth for each agent i do Average the rewards across episodes ravg \u2190 1 m m\u22121 \ufffd j=0 r[:, i] if ravg \u2265 rth then comm[i] = low_comm end else if ravg < rth then if comm[i] \u0338= low_comm then comm[i] = 2 \u2217 comm[i] end else comm[i] = high_comm end end end return comm HyperParameter GridWorld AutoNav Functional Mapping Tabular Neural Network Number of agents 4, 8, 12 4 Algorithm REINFORCE REINFORCE Max Episodes 1000 4000 Gamma 0.95 0.99 Learning rate Variable 1e-4 base_comm 8 8 wait_train 600 1000 Gradient clipping norm None 0.1 Optimizer type ADAM ADAM Entropy Regularizer Scalar None 0.5 Training Workstation GTX1080 GTX1080 Training Time 9 hours 35 hours TABLE IV: Training hyper-parameters for GridWorld and AutoNav 14 ",
    "title": "Multi-Task Federated Reinforcement Learning with",
    "paper_info": "a non-adversarial MT-FedRL problem, this uni\ufb01ed policy\nparameter ends up achieving a policy that maximizes the\nsum of discounted returns for all the environments. In an\nadversarial MT-FedRL problem, the goal of the adversarial\nagent is to prevent the MT-FedRL from achieving this uni\ufb01ed\n\u03b8\u2217 by purposely providing an adversarial policy parameter\n\u03b8k\u2212\nl\n.\nParameters that effect learning: Using gradient ascent, each\nagent updates its own set of policy parameter locally according\nto the following equation,\n\u03b8k\u2212\ni\n= \u03b8(k\u22121)+\ni\n+ \u03b4i\u2207\u03b8iV\n\u03c0\u03b8i\ni\n(\u03c1i)\n(10)\nwhere \u03b4i is the learning rate for agent i. Using Eq. 10 in\nthe smoothing average Eq. 7 yields\n\u03b8k+\ni\n=\n\uf8eb\n\uf8ed\u03b1k\u03b8(k\u22121)+\ni\n+ \u03b2k\n\ufffd\nj\u0338=i,j /\u2208L\n\u03b8(k\u22121)+\nj\n\uf8f6\n\uf8f8 +\n\uf8eb\n\uf8ed\u03b1k\u03b4i\u2207\u03b8iV\n\u03c0\u03b8i\ni\n(\u03c1i) + \u03b2k\n\ufffd\nj\u0338=i,j /\u2208L\n\u03b4j\u2207\u03b8jV\n\u03c0\u03b8j\nj\n(\u03c1j)\n\uf8f6\n\uf8f8 +\n\ufffd\n\u03b2k \ufffd\nl\u2208L\n\u03b8k\u2212\nl\n\ufffd\n(11)\nThe server update of the policy parameter can be decom-\nposed into three parts.\n\u2022 The weighted sum of the previous set of policy param-\neters \u03b8(k\u22121)+\ni\nshared by the server with the respective\nagents.\n\u2022 The agent\u2019s local update, which tries to shift the policy\nparameter distribution towards the goal direction.\n\u2022 The adversarial policy parameter which aims at shifting\nthe uni\ufb01ed policy parameter away from achieving the\ngoal.\nIf the update carried out by the adversarial agent is larger\nthan the sum of each agent\u2019s policy gradient update, the policy\nparameter will start moving away from the desired consensus\n\u03b8\u2217. The success of the adversarial attack hence depends on,\n\u2022 The nature of adversarial policy parameter \u03b8k\u2212\nl\n\u2022 Non-adversarial agent\u2019s local learning rate \u03b4i\n\u2022 The number of non-adversarial agents n \u2212 |L|\nAn adversarial attack is more likely to be successful if the\nlocal learning rate of non-adversarial agents \u03b4i is small and\nthe number of adversarial agents |L| is large.\nThreat Model: For an adversarial agent to be successful in\nits attack, it needs to shift the convergence from \u03b8\u2217 in Eq. 8\nto \u03b8\u2032 such that the resultant policy \u03c0\u2032 follows\nEsi\u223c\u03c1iV \u03c0\u2032\ni (si) << Esi\u223c\u03c1iV \u03c0\u2217\ni\n(si),\n\u2200i /\u2208 L\n(12)\nThe only way an adversarial agent can control this conver-\ngence is through the policy parameter \u03b8k\u2212\nl\nthat it shares with\nthe server. The adversarial agent needs to share the policy\nparameter that moves the distribution of the smoothing average\nof non-adversarial agents either to a uniform distribution or\nServer\n\u2026\nAgent 0\nEnvironment 0\nUpdate Policy\n(s, a, s\u2019, r)\naction\nAgent l\nEnvironment l\nUpdate Policy\n(s, a, s\u2019, r)\naction\nAgent n-1\nEnvironment n-1\nUpdate Policy\n(s, a, s\u2019, r)\naction\nRL Return\nRL Return\nRL Return\nCommunicate\nevery n iterations\nFig. 2: Adversaries can negatively impact the uni\ufb01ed policy by\nproviding adversarial policies to the server. This results in negatively\nimpacting the achieved discounted return on the environments\nin the direction that purposely yields bad actions (Fig. 3).\nGenerally shifting the distribution to uniform distribution will\nrequire less energy than to shift it to a non-optimal action\ndistribution. This requires that the adversary cancel out the\ninformation gained by all the other non-adversarial agents\nhence not being able to differentiate between good and bad\nactions, leaving all the actions equally likely.\nThreat Model: We will assume the following threat model.\nAt iteration k, each adversarial agent l shares the following\npolicy parameter with the server\n\u03b8k\u2212\nl\n= \u03bbk\u03b8k\nadv\n(13)\nHence the threat model is de\ufb01ned by the choice of the attack\nmodel \u03b8adv and \u03bbk \u2208 R which is a non-negative iteration-\ndependant scaling factor that will be used to control the norm\nof the adversarial attack. To make the scaling factor more\nmeaningful, we will assume that\n\u2225\u03b8adv\u22252 \u2248\n1\n(n \u2212 |L|)\n\ufffd\ni/\u2208L\n\u2225\u03b8i\u22252\nThe relative difference in the norm of the policy parameter\nbetween the adversarial agent and non-adversarial agent will\nbe captured in the scaling factor term \u03bbk. One thing we need\nto be mindful of is the value of the scaling factor. If the\nscaling factor is large enough, almost any random (non-zero)\nvalues for the adversarial policy parameter \u03b8k\nl will result in a\nsuccessful attack. We will quantify the relative performance\nof the threat models by\n\u2022 How effective they are in attacking, either in terms of\nthe achieved discounted return under the threat induced\nuni\ufb01ed policy or in terms of a more problem-speci\ufb01c\ngoal parameter (more in the experimentation section).\n\u2022 If two threat models achieve the same attacking perfor-\nmance, the one with a smaller scaling factor \u03bbk will be\nconsidered better. The smaller the norm of the adversarial\npolicy parameter, the better the chances for the threat\nmodel to go unnoticed by the server.\nV. COMMON ATTACK MODELS\nIn this section, we will discuss a few common attack models\n\u03b8adv and propose an adaptive attack model. For the rest of the\n4\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n- (1): This piece of work proposes an adaptive attack method and a modified federated reinforcement learning algorithm to address the issue of adversaries in multi-task federated reinforcement learning (MT-FedRL). The paper analyzes the problem of MT-FedRL algorithms in the presence of adversarial manipulations and provides a well-motivated approach to address it.\n                     \n- (2): Innovation point: The approach proposed in this paper addresses the issue of adversaries in MT-FedRL, which is an important yet challenging problem. The adaptive attack method and the communication adaptive modification to the federated RL algorithm are innovative solutions to this problem. Performance: The proposed method outperforms other general attack methods, and the modified federated RL algorithm achieves near-optimal policies in the presence of adversarial agents. The results on GridWorld and AutoNav problems demonstrate the effectiveness of the proposed approach. Workload: The paper provides a comprehensive analysis of the MT-FedRL problem in the presence of adversarial agents and proposes a well-motivated approach to address it. However, further studies are required to evaluate the proposed approach on large-scale problems and in complex distributed settings.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "\n"
}