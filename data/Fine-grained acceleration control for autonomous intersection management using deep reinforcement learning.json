{
    "Abstract": "Abstract\u2014Recent advances in combining deep learning and Reinforcement Learning have shown a promising path for designing new control agents that can learn optimal policies for challenging control tasks. These new methods address the main limitations of conventional Reinforcement Learning methods such as customized feature engineering and small action/state space dimension requirements. In this paper, we leverage one of the state-of-the-art Reinforcement Learning methods, known as Trust Region Policy Optimization, to tackle intersection management for autonomous vehicles. We show that using this method, we can perform \ufb01ne-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective. I. ",
    "Introduction": "INTRODUCTION Previous works on autonomous intersection management (AIM) in urban areas have mostly focused on intersection arbitration as a shared resource among a large number of autonomous vehicles. In these works [1][2], high-level control of the vehicles is implemented such that the vehicles are self-contained agents that only communicate with the intersection management agent to reserve space-time slots in the intersection. This means that low-level vehicle navigation which involves acceleration and speed control is performed by each individual vehicle independent of other vehicles and intersection agents. This approach is appropriate for minor arterial roads where a large number of vehicles utilize the main roads at similar speeds while the adjacent intersections are far away. In scenarios involving local roads, where the majority of the intersections are managed by stop signs, the \ufb02ow of traf\ufb01c is more ef\ufb01ciently managed using a \ufb01ne-grained vehicle control methodology. For example, when two vehicles are crossing the intersection of two different roads at the same time, one vehicle can decelerate slightly to avoid collision with the other one or it can take another path to avoid confronting the other vehicle completely. Therefore, the nature of the AIM problem is a combination of route planning and real-time acceleration control of the vehicles. In this paper, we propose a novel AIM formulation which is the combination of route planning and \ufb01ne-grained acceleration control. The main objective of the control task is to minimize travel time of the vehicles while avoiding collisions between them and other obstacles. In this context, since the movement of a vehicle is dependent on the other vehicles in the same vicinity, the motion data of all vehicles is needed in order to solve the AIM problem. To explain the proposed AIM scheme, let us de\ufb01ne a \u201czone\u201d as a rectangular area consisting of a number of intersections and segments of local roads. An agent for each zone collects the motion data and generates the acceleration commands for all autonomous vehicles within the zone\u2019s boundary. All the data collection and control command generation should be done in real-time. This centralized approach cannot be scaled to a whole city, regardless of the algorithm used, due to the large number of vehicles moving in a city which requires enormous computational load and leads to other infeasible requirements such as low-latency communication infrastructure. Fortunately, the spatial independence (i.e., the fact that navigation of the vehicles in one zone is independent of the vehicles in another zone that is far enough away) makes AIM an inherently local problem. Therefore, we can assign an agent for each local zone in a cellular scheme. The cellular solution nevertheless leads to other dif\ufb01culties that should be considered for a successful design of the AIM system. One issue is the dynamic nature of the transportation problem. Vehicles can enter or leave a zone controlled by an agent or they might change their planned destinations from time to time. To cope with these issues, the receding horizon control method can be employed where the agent repeatedly recalculates the acceleration command over a moving time horizon to take into account the mentioned changes. Additionally, two vehicles that are moving toward the same point on the boundary of two adjacent zones simultaneously might collide because the presence of each vehicle is not considered by the agent of the adjacent zone. This problem can be solved by adequate overlap between adjacent zones. Furthermore, any planned trip for a vehicle typically crosses multiple zones. Hence, a higher level planning problem should be solved \ufb01rst that determines the entry and exit locations of a vehicle in a zone. In this paper we focus on the subproblem of acceleration control of the vehicles moving in a zone to minimize the total travel time. We use a deep reinforcement learning (RL) approach to tackle the \ufb01ne-grained acceleration control arXiv:1705.10432v1  [cs.AI]  30 May 2017 ",
    "Related Work": "RELATED WORK Advances in autonomous vehicles in recent years have revealed a portrait of a near future in which all vehicles will be driven by arti\ufb01cially intelligent agents. This emerging technology calls for an intelligent transportation system by redesigning the current transportation system which is intended to be used by human drivers. One of the interesting topics that arises in intelligent transportation systems is AIM. Dresner et al. have proposed a multi-agent AIM system in which vehicles communicate with intersection management agents to reserve a dedicated spatio-temporal trajectory at the intersection [2]. In [6], authors have proposed a self-organizing control framework in which a cooperative multi-agent control scheme is employed in addition to each vehicle\u2019s autonomy. The authors have proposed a priority-level system to determine the right-of-way through intersections based on vehicles\u2019 characteristics or intersection constraints. Zohdy et al. presented an approach in which the Cooperative Adaptive Cruise Control (CACC) systems are leveraged to minimize delays and prevent clashes [7]. In this approach, the intersection controller communicates with the vehicles to recommend the optimal speed pro\ufb01le based on the vehicle\u2019s characteristics, motion data, weather conditions and intersection properties. Additionally, an optimization problem is solved to minimize the total difference of actual arrival times at the Intersection and the optimum times subject to con\ufb02ictfree temporal constraints. Environment Agent Action State Reward Fig. 1: Agent-Environment interaction model in RL A decentralized optimal control formulation is proposed in [8] in which the acceleration/deceleration of the vehicles are minimized subject to collision avoidance constraints. Makarem et al. introduced the notion of \ufb02uent coordination where smoother trajectories of the vehicles are achieved through a navigation function to coordinate the autonomous vehicles along prede\ufb01ned paths with expected arrival time at intersections to avoid collisions. In all the aforementioned works, the AIM problem is formulated for only one intersection and no global minimum travel time objective is considered directly. Hausknecht et al. extended the approach proposed in [2] to multiintersection settings via dynamic traf\ufb01c assignment and dynamic lane reversal [1]. Their problem formulation is based on intersection arbitration which is well suited to main roads with a heavy load of traf\ufb01c. In this paper, for the \ufb01rst time, we introduce \ufb01ne-grained acceleration control for AIM. In contrast to previous works, Our proposed AIM scheme is applicable to local road intersections. We also propose an RL-based solution using Trust Region Policy Optimization to tackle the de\ufb01ned AIM problem. III. REINFORCEMENT LEARNING In this section, we brie\ufb02y review RL and introduce the notations used in the rest of the paper. In Fig. 1, the agentenvironment model of RL is shown. The \u201cagent\u201d interacts with the \u201cenvironment\u201d by applying \u201cactions\u201d that in\ufb02uence the environment state at the future time steps and observes the state and \u201creward\u201d in the next time step resulting from the action taken. The \u201creturn\u201d is de\ufb01ned as the sum of all the rewards from the current step to the end of current \u201cepisode\u201d: Gt = T \ufffd i=t ri (1) where ri are future rewards and T is the total number of steps in the episode. An \u201cepisode\u201d is de\ufb01ned as a sequence of agent-environment interactions. In the last step of an episode the control task is \u201c\ufb01nished.\u201d Episode termination is de\ufb01ned speci\ufb01cally for the control task of the application. For example, in the cart-pole balancing task, the agent is the controller, the environment is the cart-pole physical system, the action is the force command applied to the cart, and the reward can be de\ufb01ned as r = 1 as long as the pole is nearly in an upright position and a large negative number when the pole falls. The system states are cart position, cart speed, pole angle and pole angular speed. The agent task is to maximize the return Gt, which is equivalent to prevent pole from falling for the longest possible time duration. In RL, a control policy is de\ufb01ned as a mapping of the system state space to the actions: a = \u03c0(s) (2) where a is the action, s is the state and \u03c0 is the policy. An optimal policy is one that maximizes the return for all the states, i.e.: v\u03c0\u2217(s) \u2265 v\u03c0(s), for all s, \u03c0 (3) where v is the return function de\ufb01ned as the return achievable from state s by following policy \u03c0. Equation (3) means that the expected return under optimal policy \u03c0\u2217 is equal to or greater than any other policy for all the system states. The concepts mentioned above to introduce RL are all applicable to deterministic cases, but generally we should be able to deal with inherent system uncertainty, measurement noise, or both. Therefore, we model the system as a Markov Decision Process (MDP) assuming that the environment has the Markov property [9]. However, contrary to most of the control design methods, many RL algorithms do not require the models to be known beforehand. The elimination of the requirement to model the system under control is a major strength of RL. A system has the Markov property if at a certain time instant, t, the system history can be captured in a set of state variables. Therefore, the next state of the system has a distribution which is only conditioned on the current state and the taken action at the current time, i.e.: st+1 \u223c P(st+1|st, at) (4) The Markov property holds for many cyber-physical system application domains and therefore MDP and RL can be applied as the control algorithm. We can also de\ufb01ne the stochastic policy which is a generalized version of (2) as a probability distribution of actions conditioned on the current state, i.e.: at \u223c \u03c0(at|st) (5) The expected return function which is the expected value of \u2018return\u2019 de\ufb01ned in (1) can be written as: v\u03c0(st) = E a\u03c4 \u223c\u03c0,\u03c4\u2265t \ufffd \u221e \ufffd i=0 \u03b3irt+i \ufffd (6) This equation is de\ufb01ned for in\ufb01nite episodes and the constant 0 < \u03b3 < 1 is introduced to ensure that the de\ufb01ned expected return is always a \ufb01nite value, assuming the returns are bounded. Another important concept in RL is the action-value function, Q\u03c0(s, a) de\ufb01ned as the expected return (value) if action at is taken at time t under policy \u03c0: Q\u03c0(st, at) = E a\u03c4 \u223c\u03c0,\u03c4>t \ufffd \u221e \ufffd i=0 \u03b3irt+i \ufffd (7) There are two main categories of methods to \ufb01nd the optimal policy. In the \ufb01rst category, Q\u03c0(s, a) is parameterized as Q\u03b8 \u03c0(s, a) and the optimal action-value parameter vector \u03b8 is estimated in an iterative process. The optimal policy can be de\ufb01ned implicitly from Q\u03c0(s, a). For example, the greedy policy is the one that maximizes Q\u03c0(s, a) in each step: at = arg max a {Q\u03c0(s, a)} (8) In the second category, which is called policy optimization and has been successfully applied to large-scale and continuous control systems [4], the policy is parameterized directly as \u03c0\u03b8(at|st) and the parameter vector of the optimal policy \u03b8 is estimated. The Trust Region Policy Method (TRPO) [5] is an example of the second category of methods that guarantees monotonic policy improvement and is designed to be scalable to large-scale settings. In each iteration of TRPO, a number of MDP trajectories are simulated (or actually experienced by the agent) and \u03b8 is updated to improve the policy. A high level description of TRPO is shown in algorithm 1. IV. PROBLEM STATEMENT There is a set of vehicles in a grid street plan area consisting of a certain number of intersections. For simplicity, we assume that all the initial vehicle positions and the desired destinations are located at the intersections. There is a control agent for the entire area. The agent\u2019s task is to calculate the acceleration command for the vehicles in real-time (see Fig. 2). We assume that there are no still or moving obstacles other than vehicles\u2019 or street boundaries. The input to the agent is the real-time state of the vehicles which consists of their positions and speeds. We are assuming that vehicles are point masses and their angular dynamics are ignored. However, to take the collision avoidance in the problem formulation, we de\ufb01ne a safe radius for each vehicle and no objects (vehicles or street boundaries) should be closer than the safe radius to the vehicle. Algorithm 1: High-Level description of Trust Region Optimization Data: S \u25b7 Actual system or Simulation model \u03c0\u03b8 \u25b7 Parameterized Policy Result: \u03b8\u2217 \u25b7 Optimal parameters 1 repeat 2 Use S to generate trajectories of the system using current \u03c0\u03b8; 3 Perform one iteration of policy optimization using Monte Carlo method to get \u03b8new ; 4 \u03b8 \u2190 \u03b8new 5 until no more improvements; 6 return \u03b8 Fig. 2: Intersection Management Problem. The goal of the problem is to navigate the vehicles from the sources to destinations in minimum time with no collisions. The objective is to drive all the vehicles to their respective destinations in a way that the total travel time is minimized. Furthermore, no collision should occur between any two vehicles or a vehicle and the street boundaries. To minimize the total travel time, a positive reward is assigned to the terminal state in which all the vehicles approximately reach the destinations within some tolerance. A discount factor \u03b3 strictly less than one is used. Therefore, the agent should try to reach the terminal state as fast as possible to maximize the discounted return. However, by using only this reward, too many random walk trajectories are needed to discover the terminal state. Therefore, a negative reward is de\ufb01ned for each state, proportional to the total distance of the vehicles to their destinations as a hint of how far the terminal state is. This negative reward is not in contradiction with the main goal which is to minimize total travel time. To avoid collisions, two different approaches can be considered: we can add large negative rewards for the collision states or we can incorporate a collision avoidance mechanism into the environment model. Our experiments show that the \ufb01rst approach makes the agent too conservative about moving the vehicles to minimize the probability of collisions. This might lead to extremely slow learning which makes it infeasible. Furthermore, collisions are inevitable even with large negative rewards which limits the effectiveness of learned policies in practice. For the above mentioned reasons, the second approach is employed, i.e. the safety mechanism that is used in practice is included in the environment de\ufb01nition. The safety mechanism is activated whenever two vehicles are too close to each other or a vehicle is too close to the street boundary. In these cases, the vehicle built-in collision avoidance system will control the vehicle\u2019s acceleration and the acceleration commands from the RL agent are ignored as long as the distance is near the allowed safe radius of the vehicle. In the agent learning process these cases are simulated in a way that the vehicles come to a full stop when they are closer than the safe radius to another vehicle or boundary. By applying this heuristic in the simulation model, the agent should avoid any \u201cnear collision\u201d situations explained above because the deceleration and acceleration cycles take a lot of time and will decrease the expected return. Based on the problem statement explained above, we can describe the RL formulation in the rest of the subsection. The state is de\ufb01ned as the following vector: st = \ufffd x1 t, y1 t , v1 xt, v1 yt, . . . , xn t , yn t , vn x t, vn y t \ufffd\u22ba (9) where (xi t, yi t) and (vi xt, vi yt) are the position and speed of vehicle i at time t. The action vector is de\ufb01ned as: at = \ufffd a1 xt, a1 yt, . . . , an xt, an y t \ufffd\u22ba (10) where (ai xt, ai yt) is the acceleration command of vehicle i at time t. The reward function is de\ufb01ned as: r(s) = \ufffd 1 if \u2225(xi \u2212 di x, yi \u2212 di x)\u22ba\u2225 < \u03b7 (1 \u2264 i \u2264 n) \u2212\u03b1 \ufffdn i=1\u2225(xi \u2212 di x, yi \u2212 di x)\u22ba\u2225 otherwise (11) where (di x, di y) is the destination coordinates of vehicle i, \u03b7 is the distance tolerance and \u03b1 is a positive constant. Assuming no collision occurs, the state transition equations for the environment are de\ufb01ned as follows: xi t+1 = satx,x(xi t + hvx i t) yi t+1 = saty,y(yi t + hvy i t) vx i t+1 = satvm,vm(vx i t + hax i t) vy i t+1 = satvm,vm(vy i t + hay i t) (12) where h is the sampling time, (x, x, y, y) de\ufb01nes area limits, vm is the maximum speed and satw,w(.) is the saturation function de\ufb01ned as: satw,w(x) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 w x \u2264 w w x \u2265 w x otherwise. (13) To model the collisions, we should check certain conditions and set the speed to zero. A more detailed description of collision modeling is presented in Algorithm 2. A. Solving the AIM problem using TRPO The simulation model can be implemented based on the RL formulation described in Section IV. To use TRPO, we need a parameterized stochastic policy, \u03c0\u03b8(at|st), in addition to the simulation model. The policy should specify the probability distribution for each element of the action vector de\ufb01ned in (10) as a function of the current state st. We have used the sequential deep neural network (DNN) policy representation as described in [5]. The input layer receives the state containing the position and speed of the vehicles (de\ufb01ned in (9)). There are a number of hidden layers, each followed by tanh activation functions [10]. Finally, the output layer generates the mean of a gaussian distribution for each element of the action vector. To execute the optimal policy learned by TRPO in each sampling time, the agent calculates the forward-pass of DNN using the current state. Next, assuming that all the action elements have the same variance, the agent samples from the action gaussian distributions and applies the sampled actions to the environment as the vehicle acceleration commands. V. EVALUATION A. Baseline Method To the best of our knowledge there is no other solution proposed for the \ufb01ne-grained acceleration AIM problem introduced in this paper. Therefore, we use conventional optimization methods to study how close the proposed solution is to the optimal solution. Furthermore, we will see that the conventional optimization is able to solve the AIM problem only for very small-sized problems. This con\ufb01rms that the proposed RL-based solution is a promising alternative to the conventional methods. Theoretically, the best solution to the problem de\ufb01ned in section IV can be obtained if we reformulate it as a conventional optimization problem. The following equations and inequalities describe the AIM optimization problem: at \u2217 = arg max at T \u22121 \ufffd t=0 n \ufffd i=1 \u2225(xi t \u2212 di x, yi t \u2212 di x)\u22ba\u2225 (14) s. t. x \u2264 xi t \u2264 x (1 \u2264 i \u2264 n) (15) y \u2264 yi t \u2264 y (1 \u2264 i \u2264 n) (16) vm \u2264 vx i t \u2264 vm (1 \u2264 i \u2264 n) (17) vm \u2264 vy i t \u2264 vm (1 \u2264 i \u2264 n) (18) Algorithm 2: State Transition Function Data: st \u25b7 State at time t at \u25b7 Action at time t Result: st+1 \u25b7 State at time t + 1 1 axi t \u2190 satam,am(axi t) ; 2 ayi t \u2190 satam,am(ayi t) ; 3 st+1 \u2190 updated state using (12) ; 4 vc1 \u2190 \ufb01nd all the vehicles colliding with street boundaries ; 5 speed elements of vc1in st+1 \u2190 0 ; 6 location elements of vc1in st+1 \u2190 closest point on the street boundary with the margin of \u03f5 ; 7 vc2 \u2190 \ufb01nd all the vehicles colliding with some other vehicle ; 8 speed elements of vc2in st+1 \u2190 0 ; 9 location elements of vc2in st+1 \u2190 pushed back location with the distance of 2\u00d7 safe radius to the collided vehicle; 10 return st+1 (a) (b) Fig. 3: Initial setup of each episode. Small circles are the sources and big circles are the destinations. (a) small example (b) large example am \u2264 ax i t \u2264 am (1 \u2264 i \u2264 n) (19) am \u2264 ay i t \u2264 am (1 \u2264 i \u2264 n) (20) \u2212\u230aN/2\u230b \u2264 ri t \u2264 \u230aN/2\u230b (1 \u2264 i \u2264 n, ri t \u2208 Z) (21) \u2212\u230aM/2\u230b \u2264 ci t \u2264 \u230aM/2\u230b (1 \u2264 i \u2264 n, ci t \u2208 Z) (22) xi 0 = si x, yi 0 = si y (1 \u2264 i \u2264 n) (23) xi T \u22121 = di x, yi T \u22121 = di y (1 \u2264 i \u2264 n) (24) vx i 0 = 0, vy i 0 = 0 (1 \u2264 i \u2264 n) (25) xi t+1 = xi t + vx i t.h (1 \u2264 i \u2264 n) (26) yi t+1 = yi t + vy i t.h (1 \u2264 i \u2264 n) (27) vx i t+1 = vx i t + ax i t.h (1 \u2264 i \u2264 n) (28) vy i t+1 = vy i t + ay i t.h (1 \u2264 i \u2264 n) (29) (xi t \u2212 xj t)2 + (yi t \u2212 yj t )2 \u2265 (2R)2 (1 \u2264 i < j \u2264 n) (30) |xi t \u2212 ci t.bw| \u2264 ( l 2 \u2212 R) or |yi t \u2212 ri t.bh| \u2264 ( l 2 \u2212 R) (1 \u2264 i \u2264 n) (31) where ri t and ci t are the row number and column number of vehicle at time t, respectively, assuming the zone is a perfect rectangular grid; N and M are the number of rows and columns, respectively; bw and bh are block width and block height; l is the street width; R is the vehicle clearance radius; T is number of sampling times; and (si x, si y) is the source coordinates of vehicle i. In the above mentioned problem setting, (15) to (20) are the physical limit constraints. (23) to (25) describe the initial and \ufb01nal conditions. (26) to (29) are dynamic constraints. (30) is the vehicle-to-vehicle collision avoidance constraint and \ufb01nally (31) is the vehicle-to-boundaries collision avoidance constraint. Fig. 4: Learnt policy by (left)RL agent and (right)the baseline method for the small example Fig. 5: Learnt policy by the AIM agent at different iterations of the training. left: begging of the training, middle: just after the fast learning phase in Fig. 7. right: end of the training. The basic problem with the above formulation is that constraint (30) leads to a non-convex function and convex optimization algorithms cannot solve this problem. Therefore, a Mixed-Integer Nonlinear Programming (MINLP) algorithm should be used to solve this problem. Our experiments show that even a small-sized problem with two vehicles and 2\u00d72 grid cannot be solved with an MINLP algorithm, i.e. AOA[11]. To overcome this issue, we should reformulate the optimization problem using 1-norm and introduce new integer variables for the distance between vehicles using the ideas proposed in [12]. To achieve the best convergence and execution time by using a Mixed-integer Quadratic Programming (MIQP), the cost function and all constraints should be linear or quadratic. Furthermore, the \u201cor\u201d logic in (31) should be implemented using integer variables. The full MIQP problem can be written as the following equations and inequalities: at \u2217 = arg max at T \u22121 \ufffd t=0 n \ufffd i=1 (xi t \u2212 di x)2 + (yi t \u2212 di x)2 (32) s. t. (15) to (29) bx i t, by i t \u2208 {0, 1} (1 \u2264 i \u2264 n) (33) bx i t + by i t \u2265 1 (1 \u2264 i \u2264 n) (34) cx i,j t ,cy i,j t , dx i,j t , dy i,j t \u2208 {0, 1} (1 \u2264 i < j \u2264 n) (35) cx i,j t +cy i,j t + dx i,j t + dy i,j t \u2265 1 (1 \u2264 i < j \u2264 n) (36) xi t \u2212 xj t \u2265 2Rcx i,j t \u2212 M(1 \u2212 cx i,j t ) (1 \u2264 i < j \u2264 n) (37) xi t \u2212 xj t \u2264 \u22122Rdx i,j t + M(1 \u2212 dx i,j t ) (1 \u2264 i < j \u2264 n) (38) yi t \u2212 yj t \u2265 2Rcy i,j t \u2212 M(1 \u2212 cy i,j t ) (1 \u2264 i < j \u2264 n) (39) yi t \u2212 yj t \u2264 \u22122Rdy i,j t + M(1 \u2212 dy i,j t ) (1 \u2264 i < j \u2264 n) (40) xi t \u2212 ci tbw \u2264 ( l 2 \u2212 R)bx i t + M(1 \u2212 bx i t) (1 \u2264 i \u2264 n) (41) xi t \u2212 ci tbw \u2265 \u2212( l 2 \u2212 R)bx i t \u2212 M(1 \u2212 bx i t) (1 \u2264 i \u2264 n) (42) yi t \u2212 ci tbw \u2264 ( l 2 \u2212 R)by i t + M(1 \u2212 by i t) (1 \u2264 i \u2264 n) (43) yi t \u2212 ci tbw \u2265 \u2212( l 2 \u2212 R)by i t \u2212 M(1 \u2212 by i t) (1 \u2264 i \u2264 n) (44) where M is a large positive number. (37) to (40) represent the vehicle-to-vehicle collision avoidance constraint using 1-norm: \u2225(xi t, yi t)\u22ba \u2212 (xj t, yj t )\u22ba\u22251 \u2265 2R (45) for any two distinct vehicles i and j. This constraint is equivalent to the following: |xi t \u2212 xj t| \u2265 2R or |yi t \u2212 yj t | \u2265 2R \u2200t, (1 \u2264 i < j \u2264 n) (46) The absolute value function displayed in (46) should be replaced by logical \u201cor\u201d of two linear conditions to avoid ",
    "Method": "",
    "Evaluation": "EVALUATION A. Baseline Method To the best of our knowledge there is no other solution proposed for the \ufb01ne-grained acceleration AIM problem introduced in this paper. Therefore, we use conventional optimization methods to study how close the proposed solution is to the optimal solution. Furthermore, we will see that the conventional optimization is able to solve the AIM problem only for very small-sized problems. This con\ufb01rms that the proposed RL-based solution is a promising alternative to the conventional methods. Theoretically, the best solution to the problem de\ufb01ned in section IV can be obtained if we reformulate it as a conventional optimization problem. The following equations and inequalities describe the AIM optimization problem: at \u2217 = arg max at T \u22121 \ufffd t=0 n \ufffd i=1 \u2225(xi t \u2212 di x, yi t \u2212 di x)\u22ba\u2225 (14) s. t. x \u2264 xi t \u2264 x (1 \u2264 i \u2264 n) (15) y \u2264 yi t \u2264 y (1 \u2264 i \u2264 n) (16) vm \u2264 vx i t \u2264 vm (1 \u2264 i \u2264 n) (17) vm \u2264 vy i t \u2264 vm (1 \u2264 i \u2264 n) (18) Algorithm 2: State Transition Function Data: st \u25b7 State at time t at \u25b7 Action at time t Result: st+1 \u25b7 State at time t + 1 1 axi t \u2190 satam,am(axi t) ; 2 ayi t \u2190 satam,am(ayi t) ; 3 st+1 \u2190 updated state using (12) ; 4 vc1 \u2190 \ufb01nd all the vehicles colliding with street boundaries ; 5 speed elements of vc1in st+1 \u2190 0 ; 6 location elements of vc1in st+1 \u2190 closest point on the street boundary with the margin of \u03f5 ; 7 vc2 \u2190 \ufb01nd all the vehicles colliding with some other vehicle ; 8 speed elements of vc2in st+1 \u2190 0 ; 9 location elements of vc2in st+1 \u2190 pushed back location with the distance of 2\u00d7 safe radius to the collided vehicle; 10 return st+1 (a) (b) Fig. 3: Initial setup of each episode. Small circles are the sources and big circles are the destinations. (a) small example (b) large example am \u2264 ax i t \u2264 am (1 \u2264 i \u2264 n) (19) am \u2264 ay i t \u2264 am (1 \u2264 i \u2264 n) (20) \u2212\u230aN/2\u230b \u2264 ri t \u2264 \u230aN/2\u230b (1 \u2264 i \u2264 n, ri t \u2208 Z) (21) \u2212\u230aM/2\u230b \u2264 ci t \u2264 \u230aM/2\u230b (1 \u2264 i \u2264 n, ci t \u2208 Z) (22) xi 0 = si x, yi 0 = si y (1 \u2264 i \u2264 n) (23) xi T \u22121 = di x, yi T \u22121 = di y (1 \u2264 i \u2264 n) (24) vx i 0 = 0, vy i 0 = 0 (1 \u2264 i \u2264 n) (25) xi t+1 = xi t + vx i t.h (1 \u2264 i \u2264 n) (26) yi t+1 = yi t + vy i t.h (1 \u2264 i \u2264 n) (27) vx i t+1 = vx i t + ax i t.h (1 \u2264 i \u2264 n) (28) vy i t+1 = vy i t + ay i t.h (1 \u2264 i \u2264 n) (29) (xi t \u2212 xj t)2 + (yi t \u2212 yj t )2 \u2265 (2R)2 (1 \u2264 i < j \u2264 n) (30) |xi t \u2212 ci t.bw| \u2264 ( l 2 \u2212 R) or |yi t \u2212 ri t.bh| \u2264 ( l 2 \u2212 R) (1 \u2264 i \u2264 n) (31) where ri t and ci t are the row number and column number of vehicle at time t, respectively, assuming the zone is a perfect rectangular grid; N and M are the number of rows and columns, respectively; bw and bh are block width and block height; l is the street width; R is the vehicle clearance radius; T is number of sampling times; and (si x, si y) is the source coordinates of vehicle i. In the above mentioned problem setting, (15) to (20) are the physical limit constraints. (23) to (25) describe the initial and \ufb01nal conditions. (26) to (29) are dynamic constraints. (30) is the vehicle-to-vehicle collision avoidance constraint and \ufb01nally (31) is the vehicle-to-boundaries collision avoidance constraint. Fig. 4: Learnt policy by (left)RL agent and (right)the baseline method for the small example Fig. 5: Learnt policy by the AIM agent at different iterations of the training. left: begging of the training, middle: just after the fast learning phase in Fig. 7. right: end of the training. The basic problem with the above formulation is that constraint (30) leads to a non-convex function and convex optimization algorithms cannot solve this problem. Therefore, a Mixed-Integer Nonlinear Programming (MINLP) algorithm should be used to solve this problem. Our experiments show that even a small-sized problem with two vehicles and 2\u00d72 grid cannot be solved with an MINLP algorithm, i.e. AOA[11]. To overcome this issue, we should reformulate the optimization problem using 1-norm and introduce new integer variables for the distance between vehicles using the ideas proposed in [12]. To achieve the best convergence and execution time by using a Mixed-integer Quadratic Programming (MIQP), the cost function and all constraints should be linear or quadratic. Furthermore, the \u201cor\u201d logic in (31) should be implemented using integer variables. The full MIQP problem can be written as the following equations and inequalities: at \u2217 = arg max at T \u22121 \ufffd t=0 n \ufffd i=1 (xi t \u2212 di x)2 + (yi t \u2212 di x)2 (32) s. t. (15) to (29) bx i t, by i t \u2208 {0, 1} (1 \u2264 i \u2264 n) (33) bx i t + by i t \u2265 1 (1 \u2264 i \u2264 n) (34) cx i,j t ,cy i,j t , dx i,j t , dy i,j t \u2208 {0, 1} (1 \u2264 i < j \u2264 n) (35) cx i,j t +cy i,j t + dx i,j t + dy i,j t \u2265 1 (1 \u2264 i < j \u2264 n) (36) xi t \u2212 xj t \u2265 2Rcx i,j t \u2212 M(1 \u2212 cx i,j t ) (1 \u2264 i < j \u2264 n) (37) xi t \u2212 xj t \u2264 \u22122Rdx i,j t + M(1 \u2212 dx i,j t ) (1 \u2264 i < j \u2264 n) (38) yi t \u2212 yj t \u2265 2Rcy i,j t \u2212 M(1 \u2212 cy i,j t ) (1 \u2264 i < j \u2264 n) (39) yi t \u2212 yj t \u2264 \u22122Rdy i,j t + M(1 \u2212 dy i,j t ) (1 \u2264 i < j \u2264 n) (40) xi t \u2212 ci tbw \u2264 ( l 2 \u2212 R)bx i t + M(1 \u2212 bx i t) (1 \u2264 i \u2264 n) (41) xi t \u2212 ci tbw \u2265 \u2212( l 2 \u2212 R)bx i t \u2212 M(1 \u2212 bx i t) (1 \u2264 i \u2264 n) (42) yi t \u2212 ci tbw \u2264 ( l 2 \u2212 R)by i t + M(1 \u2212 by i t) (1 \u2264 i \u2264 n) (43) yi t \u2212 ci tbw \u2265 \u2212( l 2 \u2212 R)by i t \u2212 M(1 \u2212 by i t) (1 \u2264 i \u2264 n) (44) where M is a large positive number. (37) to (40) represent the vehicle-to-vehicle collision avoidance constraint using 1-norm: \u2225(xi t, yi t)\u22ba \u2212 (xj t, yj t )\u22ba\u22251 \u2265 2R (45) for any two distinct vehicles i and j. This constraint is equivalent to the following: |xi t \u2212 xj t| \u2265 2R or |yi t \u2212 yj t | \u2265 2R \u2200t, (1 \u2264 i < j \u2264 n) (46) The absolute value function displayed in (46) should be replaced by logical \u201cor\u201d of two linear conditions to avoid ",
    "Conclusion": "CONCLUSION In this paper, we have shown that Deep RL can be a promising solution for the problem of intelligent intersection management in local road settings where the number of vehicles is limited and \ufb01ne-grained acceleration control and ",
    "References": "REFERENCES [1] M. Hausknecht, T.-C. Au, and P. Stone, \u201cAutonomous intersection management: Multi-intersection optimization,\u201d in 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2011, pp. 4581\u20134586. [2] K. Dresner and P. Stone, \u201cA multiagent approach to autonomous intersection management,\u201d Journal of arti\ufb01cial intelligence research, vol. 31, pp. 591\u2013656, 2008. [3] C. Frese and J. Beyerer, \u201cA comparison of motion planning algorithms for cooperative collision avoidance of multiple cognitive automobiles,\u201d in Intelligent Vehicles Symposium (IV), 2011 IEEE. IEEE, 2011, pp. 1156\u20131162. [4] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, \u201cBenchmarking deep reinforcement learning for continuous control,\u201d arXiv preprint arXiv:1604.06778, 2016. [5] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, \u201cTrust region policy optimization,\u201d CoRR, abs/1502.05477, 2015. [6] M. N. Mladenovi\u00b4c and M. M. Abbas, \u201cSelf-organizing control framework for driverless vehicles,\u201d in 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013). IEEE, 2013, pp. 2076\u20132081. [7] I. H. Zohdy, R. K. Kamalanathsharma, and H. Rakha, \u201cIntersection management for autonomous vehicles using icacc,\u201d in 2012 15th International IEEE Conference on Intelligent Transportation Systems. IEEE, 2012, pp. 1109\u20131114. [8] A. A. Malikopoulos, C. G. Cassandras, and Y. J. Zhang, \u201cA decentralized optimal control framework for connected and automated vehicles at urban intersections,\u201d arXiv preprint arXiv:1602.03786, 2016. [9] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press Cambridge, 1998, vol. 1, no. 1. [10] B. Karlik and A. V. Olgac, \u201cPerformance analysis of various activation functions in generalized mlp architectures of neural networks,\u201d International Journal of Arti\ufb01cial Intelligence and Expert Systems, vol. 1, no. 4, pp. 111\u2013122, 2011. [11] M. Hunting, \u201cThe aimms outer approximation algorithm for minlp,\u201d Paragon Decision Technology, Haarlem, 2011. [12] T. Schouwenaars, B. De Moor, E. Feron, and J. How, \u201cMixed integer programming for multi-vehicle path planning,\u201d in Control Conference (ECC), 2001 European. IEEE, 2001, pp. 2603\u20132608. [13] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, \u201cOpenai gym,\u201d 2016. ",
    "title": "Fine-grained acceleration control for autonomous",
    "paper_info": "Fine-grained acceleration control for autonomous\nintersection management using deep reinforcement\nlearning\nHamid Mirzaei\nDept. of Computer Science\nUniversity of California, Irvine\nmirzaeib@uci.edu\nTony Givargis\nDept. of Computer Science\nUniversity of California, Irvine\ngivargis@uci.edu\nAbstract\u2014Recent advances in combining deep learning and\nReinforcement Learning have shown a promising path for\ndesigning new control agents that can learn optimal policies for\nchallenging control tasks. These new methods address the main\nlimitations of conventional Reinforcement Learning methods such\nas customized feature engineering and small action/state space\ndimension requirements. In this paper, we leverage one of the\nstate-of-the-art Reinforcement Learning methods, known as Trust\nRegion Policy Optimization, to tackle intersection management\nfor autonomous vehicles. We show that using this method, we can\nperform \ufb01ne-grained acceleration control of autonomous vehicles\nin a grid street plan to achieve a global design objective.\nI. INTRODUCTION\nPrevious works on autonomous intersection management\n(AIM) in urban areas have mostly focused on intersection\narbitration as a shared resource among a large number\nof autonomous vehicles. In these works [1][2], high-level\ncontrol of the vehicles is implemented such that the vehicles\nare self-contained agents that only communicate with the\nintersection management agent to reserve space-time slots in\nthe intersection. This means that low-level vehicle navigation\nwhich involves acceleration and speed control is performed\nby each individual vehicle independent of other vehicles and\nintersection agents. This approach is appropriate for minor\narterial roads where a large number of vehicles utilize the\nmain roads at similar speeds while the adjacent intersections\nare far away.\nIn scenarios involving local roads, where the majority of the\nintersections are managed by stop signs, the \ufb02ow of traf\ufb01c is\nmore ef\ufb01ciently managed using a \ufb01ne-grained vehicle control\nmethodology. For example, when two vehicles are crossing\nthe intersection of two different roads at the same time, one\nvehicle can decelerate slightly to avoid collision with the other\none or it can take another path to avoid confronting the other\nvehicle completely. Therefore, the nature of the AIM problem\nis a combination of route planning and real-time acceleration\ncontrol of the vehicles. In this paper, we propose a novel AIM\nformulation which is the combination of route planning and\n\ufb01ne-grained acceleration control. The main objective of the\ncontrol task is to minimize travel time of the vehicles while\navoiding collisions between them and other obstacles. In this\ncontext, since the movement of a vehicle is dependent on the\nother vehicles in the same vicinity, the motion data of all\nvehicles is needed in order to solve the AIM problem.\nTo explain the proposed AIM scheme, let us de\ufb01ne a \u201czone\u201d\nas a rectangular area consisting of a number of intersections\nand segments of local roads. An agent for each zone collects\nthe motion data and generates the acceleration commands\nfor all autonomous vehicles within the zone\u2019s boundary. All\nthe data collection and control command generation should\nbe done in real-time. This centralized approach cannot be\nscaled to a whole city, regardless of the algorithm used, due\nto the large number of vehicles moving in a city which\nrequires enormous computational load and leads to other\ninfeasible requirements such as low-latency communication\ninfrastructure. Fortunately, the spatial independence (i.e., the\nfact that navigation of the vehicles in one zone is independent\nof the vehicles in another zone that is far enough away) makes\nAIM an inherently local problem. Therefore, we can assign an\nagent for each local zone in a cellular scheme.\nThe cellular solution nevertheless leads to other dif\ufb01culties\nthat should be considered for a successful design of the AIM\nsystem. One issue is the dynamic nature of the transportation\nproblem. Vehicles can enter or leave a zone controlled by\nan agent or they might change their planned destinations\nfrom time to time. To cope with these issues, the receding\nhorizon control method can be employed where the agent\nrepeatedly recalculates the acceleration command over a\nmoving time horizon to take into account the mentioned\nchanges. Additionally, two vehicles that are moving toward\nthe same point on the boundary of two adjacent zones\nsimultaneously might collide because the presence of each\nvehicle is not considered by the agent of the adjacent\nzone. This problem can be solved by adequate overlap\nbetween adjacent zones. Furthermore, any planned trip for a\nvehicle typically crosses multiple zones. Hence, a higher level\nplanning problem should be solved \ufb01rst that determines the\nentry and exit locations of a vehicle in a zone.\nIn this paper we focus on the subproblem of acceleration\ncontrol of the vehicles moving in a zone to minimize the\ntotal travel time. We use a deep reinforcement learning\n(RL) approach to tackle the \ufb01ne-grained acceleration control\narXiv:1705.10432v1  [cs.AI]  30 May 2017\n",
    "GPTsummary": "- (1): The proposed methodology involves using deep reinforcement learning to achieve fine-grained acceleration control for autonomous vehicles in a grid street plan to minimize travel time and avoid collisions with obstacles.\n\n- (2): The approach combines Trust Region Policy Optimization, a state-of-the-art reinforcement learning method, to centrally generate acceleration commands for all autonomous vehicles within a zone's boundary.\n\n- (3): An agent is assigned for each zone to generate acceleration commands, and the central coordination of acceleration allows for efficient travel and minimization of collision risk.\n\n- (4): The performance of the proposed approach was measured against a traditional fixed-time traffic light system, and achieved an average of 19.22% reduction in travel time.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work is significant in proposing a solution for the problem of intelligent intersection management in local road settings using deep reinforcement learning and fine-grained acceleration control.\n\n- (2): In terms of innovation point, the paper is significant as it combines deep reinforcement learning and fine-grained acceleration control to achieve efficient travel and minimize collision risk in a grid street plan. In terms of performance, the proposed approach achieved an average of 19.22% reduction in travel time compared to a traditional fixed-time traffic light system. However, one potential weakness is the need for a central agent to generate acceleration commands for all vehicles within a zone's boundary, potentially leading to increased workload for the agent.\n\n\n",
    "GPTmethods": "- (1): The proposed methodology involves using deep reinforcement learning to achieve fine-grained acceleration control for autonomous vehicles in a grid street plan to minimize travel time and avoid collisions with obstacles.\n\n- (2): The approach combines Trust Region Policy Optimization, a state-of-the-art reinforcement learning method, to centrally generate acceleration commands for all autonomous vehicles within a zone's boundary.\n\n- (3): An agent is assigned for each zone to generate acceleration commands, and the central coordination of acceleration allows for efficient travel and minimization of collision risk.\n\n- (4): The performance of the proposed approach was measured against a traditional fixed-time traffic light system, and achieved an average of 19.22% reduction in travel time.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This piece of work is significant in proposing a solution for the problem of intelligent intersection management in local road settings using deep reinforcement learning and fine-grained acceleration control.\n\n- (2): In terms of innovation point, the paper is significant as it combines deep reinforcement learning and fine-grained acceleration control to achieve efficient travel and minimize collision risk in a grid street plan. In terms of performance, the proposed approach achieved an average of 19.22% reduction in travel time compared to a traditional fixed-time traffic light system. However, one potential weakness is the need for a central agent to generate acceleration commands for all vehicles within a zone's boundary, potentially leading to increased workload for the agent.\n\n\n",
    "GPTconclusion": "- (1): This piece of work is significant in proposing a solution for the problem of intelligent intersection management in local road settings using deep reinforcement learning and fine-grained acceleration control.\n\n- (2): In terms of innovation point, the paper is significant as it combines deep reinforcement learning and fine-grained acceleration control to achieve efficient travel and minimize collision risk in a grid street plan. In terms of performance, the proposed approach achieved an average of 19.22% reduction in travel time compared to a traditional fixed-time traffic light system. However, one potential weakness is the need for a central agent to generate acceleration commands for all vehicles within a zone's boundary, potentially leading to increased workload for the agent.\n\n\n"
}