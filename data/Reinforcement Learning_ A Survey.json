{
    "Abstract": "Abstract This pap er surv eys the \ufffdeld of reinforcemen t learning from a computer-science p ersp ectiv e. It is written to b e accessible to researc hers familia r with mac hine learning. Both the historical basis of the \ufffdeld and a broad selection of curren t w ork are summarized. Reinforcemen t learning is the problem faced b y an agen t that learns b eha vior through trial-and-error in teractions with a dynamic en vironmen t. The w ork describ ed here has a resem blance to w ork in psyc hology , but di\ufffders considerably in the details and in the use of the w ord \\reinforcemen t.\" The pap er discusses cen tral issues of reinforcemen t learning, including trading o\ufffd exploration and exploitation, establishing the foundations of the \ufffdeld via Mark o v decision theory , learning from dela y ed reinforcemen t, constructing empirical mo dels to accelerate learning, making use of generalization and hierarc h y , and coping with hidden state. It concludes with a surv ey of some implemen ted systems and an assessmen t of the practical utilit y of curren t metho ds for reinforcemen t learning. \u0001. In tro duction Reinforcemen t learning dates bac k to the early da ys of cyb ernetics and w ork in statistics, psyc hology , neuroscience, and computer science. In the last \ufffdv e to ten y ears, it has attracted rapidly increasing in terest in the mac hine learning and arti\ufffdcial in telligence comm unities. Its promise is b eguiling|a w a y of programming agen ts b y rew ard and punishmen t without needing to sp ecify how the task is to b e ac hiev ed. But there are formidable computational obstacles to ful\ufffdlling the promise. This pap er surv eys the historical basis of reinforcemen t learning and some of the curren t w ork from a computer science p ersp ectiv e. W e giv e a high-lev el o v erview of the \ufffdeld and a taste of some sp eci\ufffdc approac hes. It is, of course, imp ossible to men tion all of the imp ortan t w ork in the \ufffdeld; this should not b e tak en to b e an exhaustiv e accoun t. Reinforcemen t learning is the problem faced b y an agen t that m ust learn b eha vior through trial-and-error in teractions with a dynamic en vironmen t. The w ork describ ed here has a strong family resem blance to ep on ymous w ork in psyc hology , but di\ufffders considerably in the details and in the use of the w ord \\reinforcemen t.\" It is appropriately though t of as a class of problems, rather than as a set of tec hniques. There are t w o main strategies for solving reinforcemen t-learning problems. The \ufffdrst is to searc h in the space of b eha viors in order to \ufffdnd one that p erforms w ell in the en vironmen t. This approac h has b een tak en b y w ork in genetic algorithms and genetic programming, c \ufffd\u0001\t\t\u0006 AI Access F oundation and Morgan Kaufmann Publishers. All righ ts reserv ed. a T s i r B I R as w ell as some more no v el searc h tec hniques (Sc hmidh ub er, \u0001\t\t\u0006). The second is to use statistical tec hniques and dynamic programming metho ds to estimate the utilit y of taking actions in states of the w orld. This pap er is dev oted almost en tirely to the second set of tec hniques b ecause they tak e adv an tage of the sp ecial structure of reinforcemen t-learning problems that is not a v ailable in optimization problems in general. It is not y et clear whic h set of approac hes is b est in whic h circumstances. The rest of this section is dev oted to establishing notation and describing the basic reinforcemen t-learning mo del. Section \u0002 explains the trade-o\ufffd b et w een exploration and exploitation and presen ts some solutions to the most basic case of reinforcemen t-learning problems, in whic h w e w an t to maximize the immediate rew ard. Section \u0003 considers the more general problem in whic h rew ards can b e dela y ed in time from the actions that w ere crucial to gaining them. Section \u0004 considers some classic mo del-free algorithms for reinforcemen t learning from dela y ed rew ard: adaptiv e heuristic critic, T D (\ufffd) and Q-learning. Section \u0005 demonstrates a con tin uum of algorithms that are sensitiv e to the amoun t of computation an agen t can p erform b et w een actual steps of action in the en vironmen t. Generalization|the cornerstone of mainstream mac hine learning researc h|has the p oten tial of considerably aiding reinforcemen t learning, as describ ed in Section \u0006. Section \u0007 considers the problems that arise when the agen t do es not ha v e complete p erceptual access to the state of the en vironmen t. Section \b catalogs some of reinforcemen t learning's successful applications. Finally , Section \t concludes with some sp eculations ab out imp ortan t op en problems and the future of reinforcemen t learning. \u0001.\u0001 Reinforcemen t-Learning Mo del In the standard reinforcemen t-learning mo del, an agen t is connected to its en vironmen t via p erception and action, as depicted in Figure \u0001. On eac h step of in teraction the agen t receiv es as input, i, some indication of the curren t state, s, of the en vironmen t; the agen t then c ho oses an action, a, to generate as output. The action c hanges the state of the en vironmen t, and the v alue of this state transition is comm unicated to the agen t through a scalar r einfor c ement signal, r . The agen t's b eha vior, B , should c ho ose actions that tend to increase the long-run sum of v alues of the reinforcemen t signal. It can learn to do this o v er time b y systematic trial and error, guided b y a wide v ariet y of algorithms that are the sub ject of later sections of this pap er. \u0002\u0003\b with the follo wing example dialogue. En vironmen t: Y ou are in state \u0006\u0005. Y ou ha v e \u0004 p ossible actions. Agen t: I'll tak e action \u0002. En vironmen t: Y ou receiv ed a reinforcemen t of \u0007 units. Y ou are no w in state \u0001\u0005. Y ou ha v e \u0002 p ossible actions. Agen t: I'll tak e action \u0001. En vironmen t: Y ou receiv ed a reinforcemen t of -\u0004 units. Y ou are no w in state \u0006\u0005. Y ou ha v e \u0004 p ossible actions. Agen t: I'll tak e action \u0002. En vironmen t: Y ou receiv ed a reinforcemen t of \u0005 units. Y ou are no w in state \u0004\u0004. Y ou ha v e \u0005 p ossible actions. . . . . . . The agen t's job is to \ufffdnd a p olicy \ufffd , mapping states to actions, that maximizes some long-run measure of reinforcemen t. W e exp ect, in general, that the en vironmen t will b e non-deterministic; that is, that taking the same action in the same state on t w o di\ufffderen t o ccasions ma y result in di\ufffderen t next states and/or di\ufffderen t reinforcemen t v alues. This happ ens in our example ab o v e: from state \u0006\u0005, applying action \u0002 pro duces di\ufffdering reinforcemen ts and di\ufffdering states on t w o o ccasions. Ho w ev er, w e assume the en vironmen t is stationary; that is, that the pr ob abilities of making state transitions or receiving sp eci\ufffdc reinforcemen t signals do not c hange o v er time. \u0001 Reinforcemen t learning di\ufffders from the more widely studied problem of sup ervised learning in sev eral w a ys. The most imp ortan t di\ufffderence is that there is no presen tation of input/output pairs. Instead, after c ho osing an action the agen t is told the immediate rew ard and the subsequen t state, but is not told whic h action w ould ha v e b een in its b est long-term in terests. It is necessary for the agen t to gather useful exp erience ab out the p ossible system states, actions, transitions and rew ards activ ely to act optimally . Another di\ufffderence from sup ervised learning is that on-line p erformance is imp ortan t: the ev aluation of the system is often concurren t with learning. \u0001. This assumption ma y b e disapp oin ti ng; after all, op eration in non-stationary en vironmen ts is one of the motiv ations for buildin g learning systems. In fact, man y of the algorithms describ ed in later sections are e\ufffdectiv e in slo wly-v arying non-stationary en vironmen ts, but there is v ery little theoretical analysis in this area. \u0002\u0003\t \u0001.\u0002 Mo dels of Optimal Beha vior Before w e can start thinking ab out algorithms for learning to b eha v e optimally , w e ha v e to decide what our mo del of optimalit y will b e. In particular, w e ha v e to sp ecify ho w the agen t should tak e the future in to accoun t in the decisions it mak es ab out ho w to b eha v e no w. There are three mo dels that ha v e b een the sub ject of the ma jorit y of w ork in this area. The \ufffdnite-horizon mo del is the easiest to think ab out; at a giv en momen t in time, the agen t should optimize its exp ected rew ard for the next h steps: E ( h X t=0 r t ) ; it need not w orry ab out what will happ en after that. In this and subsequen t expressions, r t represen ts the scalar rew ard receiv ed t steps in to the future. This mo del can b e used in t w o w a ys. In the \ufffdrst, the agen t will ha v e a non-stationary p olicy; that is, one that c hanges o v er time. On its \ufffdrst step it will tak e what is termed a h-step optimal action. This is de\ufffdned to b e the b est action a v ailable giv en that it has h steps remaining in whic h to act and gain reinforcemen t. On the next step it will tak e a (h \ufffd \u0001)-step optimal action, and so on, un til it \ufffdnally tak es a \u0001-step optimal action and terminates. In the second, the agen t do es r e c e ding-horizon c ontr ol, in whic h it alw a ys tak es the h-step optimal action. The agen t alw a ys acts according to the same p olicy , but the v alue of h limits ho w far ahead it lo oks in c ho osing its actions. The \ufffdnite-horizon mo del is not alw a ys appropriate. In man y cases w e ma y not kno w the precise length of the agen t's life in adv ance. The in\ufffdnite-horizon discoun ted mo del tak es the long-run rew ard of the agen t in to accoun t, but rew ards that are receiv ed in the future are geometrically discoun ted according to discoun t factor \ufffd , (where 0 \ufffd \ufffd < \u0001): E ( \u0001 X t=0 \ufffd t r t ) : W e can in terpret \ufffd in sev eral w a ys. It can b e seen as an in terest rate, a probabilit y of living another step, or as a mathematical tric k to b ound the in\ufffdnite sum. The mo del is conceptually similar to receding-horizon con trol, but the discoun ted mo del is more mathematically tractable than the \ufffdnite-horizon mo del. This is a dominan t reason for the wide atten tion this mo del has receiv ed. \u0002\u00040 long-run a v erage p erformance. It is p ossible to generalize this mo del so that it tak es in to accoun t b oth the long run a v erage and the amoun t of initial rew ard than can b e gained. In the generalized, bias optimal mo del, a p olicy is preferred if it maximizes the long-run a v erage and ties are brok en b y the initial extra rew ard. Figure \u0002 con trasts these mo dels of optimalit y b y pro viding an en vironmen t in whic h c hanging the mo del of optimalit y c hanges the optimal p olicy . In this example, circles represen t the states of the en vironmen t and arro ws are state transitions. There is only a single action c hoice from ev ery state except the start state, whic h is in the upp er left and mark ed with an incoming arro w. All rew ards are zero except where mark ed. Under a \ufffdnite-horizon mo del with h = \u0005, the three actions yield rew ards of +\u0006:0, +0:0, and +0:0, so the \ufffdrst action should b e c hosen; under an in\ufffdnite-horizon discoun ted mo del with \ufffd = 0:\t, the three c hoices yield +\u0001\u0006:\u0002, +\u0005\t:0, and +\u0005\b:\u0005 so the second action should b e c hosen; and under the a v erage rew ard mo del, the third action should b e c hosen since it leads to an a v erage rew ard of +\u0001\u0001. If w e c hange h to \u0001000 and \ufffd to 0.\u0002, then the second action is optimal for the \ufffdnite-horizon mo del and the \ufffdrst for the in\ufffdnite-horizon discoun ted mo del; ho w ev er, the a v erage rew ard mo del will alw a ys prefer the b est long-term a v erage. Since the c hoice of optimalit y mo del and parameters matters so m uc h, it is imp ortan t to c ho ose it carefully in an y application. The \ufffdnite-horizon mo del is appropriate when the agen t's lifetime is kno wn; one imp ortan t asp ect of this mo del is that as the length of the remaining lifetime decreases, the agen t's p olicy ma y c hange. A system with a hard deadline w ould b e appropriately mo deled this w a y . The relativ e usefulness of in\ufffdnite-horizon discoun ted and bias-optimal mo dels is still under debate. Bias-optimalit y has the adv an tage of not requiring a discoun t parameter; ho w ev er, algorithms for \ufffdnding bias-optimal p olicies are not y et as w ell-understo o d as those for \ufffdnding optimal in\ufffdnite-horizon discoun ted p olicies. \u0001.\u0003 Measuring Learning P erformance The criteria giv en in the previous section can b e used to assess the p olicies learned b y a giv en algorithm. W e w ould also lik e to b e able to ev aluate the qualit y of learning itself. There are sev eral incompatible measures in use. \ufffd Ev en tual con v ergence to optimal. Man y algorithms come with a pro v able guaran tee of asymptotic con v ergence to optimal b eha vior (W atkins & Da y an, \u0001\t\t\u0002). This is reassuring, but useless in practical terms. An agen t that quic kly reac hes a plateau \u0002\u0004\u0001 Finite horizon, h=4 In\ufb01nite horizon, \u03b3=0.9 Average reward +2 +10 +11 at \t\t% of optimalit y ma y , in man y applications, b e preferable to an agen t that has a guaran tee of ev en tual optimalit y but a sluggish early learning rate. \ufffd Sp eed of con v ergence to optimalit y . Optimalit y is usually an asymptotic result, and so con v ergence sp eed is an ill-de\ufffdned measure. More practical is the sp e e d of c onver genc e to ne ar-optimality. This measure b egs the de\ufffdnition of ho w near to optimalit y is su\ufffdcien t. A related measure is level of p erformanc e after a given time, whic h similarly requires that someone de\ufffdne the giv en time. It should b e noted that here w e ha v e another di\ufffderence b et w een reinforcemen t learning and con v en tional sup ervised learning. In the latter, exp ected future predictiv e accuracy or statistical e\ufffdciency are the prime concerns. F or example, in the w ell-kno wn P A C framew ork (V alian t, \u0001\t\b\u0004), there is a learning p erio d during whic h mistak es do not coun t, then a p erformance p erio d during whic h they do. The framew ork pro vides b ounds on the necessary length of the learning p erio d in order to ha v e a probabilistic guaran tee on the subsequen t p erformance. That is usually an inappropriate view for an agen t with a long existence in a complex en vironmen t. In spite of the mismatc h b et w een em b edded reinforcemen t learning and the train/test p ersp ectiv e, Fiec h ter (\u0001\t\t\u0004) pro vides a P A C analysis for Q-learning (describ ed in Section \u0004.\u0002) that sheds some ligh t on the connection b et w een the t w o views. Measures related to sp eed of learning ha v e an additional w eakness. An algorithm that merely tries to ac hiev e optimalit y as fast as p ossible ma y incur unnecessarily large p enalties during the learning p erio d. A less aggressiv e strategy taking longer to ac hiev e optimalit y , but gaining greater total reinforcemen t during its learning migh t b e preferable. \ufffd Regret. A more appropriate measure, then, is the exp ected decrease in rew ard gained due to executing the learning algorithm instead of b eha ving optimally from the v ery b eginning. This measure is kno wn as r e gr et (Berry & F ristedt, \u0001\t\b\u0005). It p enalizes mistak es wherev er they o ccur during the run. Unfortunately , results concerning the regret of algorithms are quite hard to obtain. \u0002\u0004\u0002 mathematical analysis, whic h in turn lead to robust, practical, and widely deplo y ed adaptiv e con trol algorithms. \u0002. Exploitation v ersus Exploration: The Single-State Case One ma jor di\ufffderence b et w een reinforcemen t learning and sup ervised learning is that a reinforcemen t-learner m ust explicitly explore its en vironmen t. In order to highligh t the problems of exploration, w e treat a v ery simple case in this section. The fundamen tal issues and approac hes describ ed here will, in man y cases, transfer to the more complex instances of reinforcemen t learning discussed later in the pap er. The simplest p ossible reinforcemen t-learning problem is kno wn as the k -armed bandit problem, whic h has b een the sub ject of a great deal of study in the statistics and applied mathematics literature (Berry & F ristedt, \u0001\t\b\u0005). The agen t is in a ro om with a collection of k gam bling mac hines (eac h called a \\one-armed bandit\" in collo quial English). The agen t is p ermitted a \ufffdxed n um b er of pulls, h. An y arm ma y b e pulled on eac h turn. The mac hines do not require a dep osit to pla y; the only cost is in w asting a pull pla ying a sub optimal mac hine. When arm i is pulled, mac hine i pa ys o\ufffd \u0001 or 0, according to some underlying probabilit y parameter p i , where pa y o\ufffds are indep enden t ev en ts and the p i s are unkno wn. What should the agen t's strategy b e? This problem illustrates the fundamen tal tradeo\ufffd b et w een exploitation and exploration. The agen t migh t b eliev e that a particular arm has a fairly high pa y o\ufffd probabilit y; should it c ho ose that arm all the time, or should it c ho ose another one that it has less information ab out, but seems to b e w orse? Answ ers to these questions dep end on ho w long the agen t is exp ected to pla y the game; the longer the game lasts, the w orse the consequences of prematurely con v erging on a sub-optimal arm, and the more the agen t should explore. There is a wide v ariet y of solutions to this problem. W e will consider a represen tativ e selection of them, but for a deep er discussion and a n um b er of imp ortan t theoretical results, see the b o ok b y Berry and F ristedt (\u0001\t\b\u0005). W e use the term \\action\" to indicate the agen t's c hoice of arm to pull. This eases the transition in to dela y ed reinforcemen t mo dels in Section \u0003. It is v ery imp ortan t to note that bandit problems \ufffdt our de\ufffdnition of a reinforcemen t-learning en vironmen t with a single state with only self transitions. Section \u0002.\u0001 discusses three solutions to the basic one-state bandit problem that ha v e formal correctness results. Although they can b e extended to problems with real-v alued rew ards, they do not apply directly to the general m ulti-state dela y ed-reinforcemen t case. \u0002\u0004\u0003 to solv e for an optimal strategy (Berry & F ristedt, \u0001\t\b\u0005). This requires an assumed prior join t distribution for the parameters fp i g, the most natural of whic h is that eac h p i is indep enden tly uniformly distributed b et w een 0 and \u0001. W e compute a mapping from b elief states (summaries of the agen t's exp eriences during this run) to actions. Here, a b elief state can b e represen ted as a tabulation of action c hoices and pa y o\ufffds: fn \u0001 ; w \u0001 ; n \u0002 ; w \u0002 ; : : : ; n k ; w k g denotes a state of pla y in whic h eac h arm i has b een pulled n i times with w i pa y o\ufffds. W e write V \ufffd (n \u0001 ; w \u0001 ; : : : ; n k ; w k ) as the exp ected pa y o\ufffd remaining, giv en that a total of h pulls are a v ailable, and w e use the remaining pulls optimally . If P i n i = h, then there are no remaining pulls, and V \ufffd (n \u0001 ; w \u0001 ; : : : ; n k ; w k ) = 0. This is the basis of a recursiv e de\ufffdnition. If w e kno w the V \ufffd v alue for all b elief states with t pulls remaining, w e can compute the V \ufffd v alue of an y b elief state with t + \u0001 pulls remaining: V \ufffd (n \u0001 ; w \u0001 ; : : : ; n k ; w k ) = max i E \" F uture pa y o\ufffd if agen t tak es action i, then acts optimally for remaining pulls # = max i   \ufffd i V \ufffd (n \u0001 ; w i ; : : : ; n i + \u0001; w i + \u0001; : : : ; n k ; w k )+ (\u0001 \ufffd \ufffd i )V \ufffd (n \u0001 ; w i ; : : : ; n i + \u0001; w i ; : : : ; n k ; w k ) ! where \ufffd i is the p osterior sub jectiv e probabilit y of action i pa ying o\ufffd giv en n i , w i and our prior probabilit y . F or the uniform priors, whic h result in a b eta distribution, \ufffd i = (w i + \u0001)=(n i + \u0002). The exp ense of \ufffdlling in the table of V \ufffd v alues in this w a y for all attainable b elief states is linear in the n um b er of b elief states times actions, and th us exp onen tial in the horizon. \u0002.\u0001.\u0002 Gittins Alloca tion Indices Gittins giv es an \\allo cation index\" metho d for \ufffdnding the optimal c hoice of action at eac h step in k -armed bandit problems (Gittins, \u0001\t\b\t). The tec hnique only applies under the discoun ted exp ected rew ard criterion. F or eac h action, consider the n um b er of times it has b een c hosen, n, v ersus the n um b er of times it has paid o\ufffd, w . F or certain discoun t factors, there are published tables of \\index v alues,\" I (n; w ) for eac h pair of n and w . Lo ok up the index v alue for eac h action i, I (n i ; w i ). It represen ts a comparativ e measure of the com bined v alue of the exp ected pa y o\ufffd of action i (giv en its history of pa y o\ufffds) and the v alue of the information that w e w ould get b y c ho osing it. Gittins has sho wn that c ho osing the action with the largest index v alue guaran tees the optimal balance b et w een exploration and exploitation. \u0002\u0004\u0004 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N+1 a = 0 a = 1 r = 0 r = 1 1 2 3 N-1 N 2N 2N-1 N+3 N+2 N+1 a = 0 a = 1 ro w sho ws transitions after a rew ard of 0. In states in the left half of the \ufffdgure, action 0 is tak en; in those on the righ t, action \u0001 is tak en. Because of the guaran tee of optimal exploration and the simplicit y of the tec hnique (giv en the table of index v alues), this approac h holds a great deal of promise for use in more complex applications. This metho d pro v ed useful in an application to rob otic manipulation with immediate rew ard (Salganico\ufffd & Ungar, \u0001\t\t\u0005). Unfortunately , no one has y et b een able to \ufffdnd an analog of index v alues for dela y ed reinforcemen t problems. \u0002.\u0001.\u0003 Learning A utoma t a A branc h of the theory of adaptiv e con trol is dev oted to le arning automata, surv ey ed b y Narendra and Thathac har (\u0001\t\b\t), whic h w ere originally describ ed explicitly as \ufffdnite state automata. The Tsetlin automaton sho wn in Figure \u0003 pro vides an example that solv es a \u0002-armed bandit arbitrarily near optimally as N approac hes in\ufffdnit y . It is incon v enien t to describ e algorithms as \ufffdnite-state automata, so a mo v e w as made to describ e the in ternal state of the agen t as a probabilit y distribution according to whic h actions w ould b e c hosen. The probabilities of taking di\ufffderen t actions w ould b e adjusted according to their previous successes and failures. An example, whic h stands among a set of algorithms indep enden tly dev elop ed in the mathematical psyc hology literature (Hilgard & Bo w er, \u0001\t\u0007\u0005), is the line ar r ewar d-inaction algorithm. Let p i b e the agen t's probabilit y of taking action i. \ufffd When action a i succeeds, p i := p i + \ufffd(\u0001 \ufffd p i ) p j := p j \ufffd \ufffdp j for j \u0006= i \ufffd When action a i fails, p j remains unc hanged (for all j ). This algorithm con v erges with probabilit y \u0001 to a v ector con taining a single \u0001 and the rest 0's (c ho osing a particular action with probabilit y \u0001). Unfortunately , it do es not alw a ys con v erge to the correct action; but the probabilit y that it con v erges to the wrong one can b e made arbitrarily small b y making \ufffd small (Narendra & Thathac har, \u0001\t\u0007\u0004). There is no literature on the regret of this algorithm. \u0002\u0004\u0005 will alw a ys b e pic k ed, lea ving the true optimal action starv ed of data and its sup eriorit y nev er disco v ered. An agen t m ust explore to ameliorate this outcome. A useful heuristic is optimism in the fac e of unc ertainty in whic h actions are selected greedily , but strongly optimistic prior b eliefs are put on their pa y o\ufffds so that strong negativ e evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unluc ky action, but the risk of this can b e made arbitrarily small. T ec hniques lik e this ha v e b een used in sev eral reinforcemen t learning algorithms including the in terv al exploration metho d (Kaelbling, \u0001\t\t\u0003b) (describ ed shortly), the explor ation b onus in Dyna (Sutton, \u0001\t\t0), curiosity-driven explor ation (Sc hmidh ub er, \u0001\t\t\u0001a), and the exploration mec hanism in prioritized sw eeping (Mo ore & A tk eson, \u0001\t\t\u0003). \u0002.\u0002.\u0002 Randomized Stra tegies Another simple exploration strategy is to tak e the action with the b est estimated exp ected rew ard b y default, but with probabilit y p, c ho ose an action at random. Some v ersions of this strategy start with a large v alue of p to encourage initial exploration, whic h is slo wly decreased. An ob jection to the simple strategy is that when it exp erimen ts with a non-greedy action it is no more lik ely to try a promising alternativ e than a clearly hop eless alternativ e. A sligh tly more sophisticated strategy is Boltzmann explor ation. In this case, the exp ected rew ard for taking action a, E R(a) is used to c ho ose an action probabilisticall y according to the distribution P (a) = e E R(a)=T P a 0 \u0002A e E R(a 0 )=T : The temp er atur e parameter T can b e decreased o v er time to decrease exploration. This metho d w orks w ell if the b est action is w ell separated from the others, but su\ufffders somewhat when the v alues of the actions are close. It ma y also con v erge unnecessarily slo wly unless the temp erature sc hedule is man ually tuned with great care. \u0002.\u0002.\u0003 Inter v al-based Techniques Exploration is often more e\ufffdcien t when it is based on second-order information ab out the certain t y or v ariance of the estimated v alues of actions. Kaelbling's interval estimation algorithm (\u0001\t\t\u0003b) stores statistics for eac h action a i : w i is the n um b er of successes and n i the n um b er of trials. An action is c hosen b y computing the upp er b ound of a \u000100 \ufffd (\u0001 \ufffd \ufffd)% \u0002\u0004\u0006 \u0002.\u0003 More General Problems When there are m ultiple states, but reinforcemen t is still immediate, then an y of the ab o v e solutions can b e replicated, once for eac h state. Ho w ev er, when generalization is required, these solutions m ust b e in tegrated with generalization metho ds (see section \u0006); this is straigh tforw ard for the simple ad-ho c metho ds, but it is not understo o d ho w to main tain theoretical guaran tees. Man y of these tec hniques fo cus on con v erging to some regime in whic h exploratory actions are tak en rarely or nev er; this is appropriate when the en vironmen t is stationary . Ho w ev er, when the en vironmen t is non-stationary , exploration m ust con tin ue to tak e place, in order to notice c hanges in the w orld. Again, the more ad-ho c tec hniques can b e mo di\ufffded to deal with this in a plausible manner (k eep temp erature parameters from going to 0; deca y the statistics in in terv al estimation), but none of the theoretically guaran teed metho ds can b e applied. \u0003. Dela y ed Rew ard In the general case of the reinforcemen t learning problem, the agen t's actions determine not only its immediate rew ard, but also (at least probabilistically) the next state of the en vironmen t. Suc h en vironmen ts can b e though t of as net w orks of bandit problems, but the agen t m ust tak e in to accoun t the next state as w ell as the immediate rew ard when it decides whic h action to tak e. The mo del of long-run optimalit y the agen t is using determines exactly ho w it should tak e the v alue of the future in to accoun t. The agen t will ha v e to b e able to learn from dela y ed reinforcemen t: it ma y tak e a long sequence of actions, receiving insigni\ufffdcan t reinforcemen t, then \ufffdnally arriv e at a state with high reinforcemen t. The agen t m ust b e able to learn whic h of its actions are desirable based on rew ard that can tak e place arbitrarily far in the future. \u0003.\u0001 Mark o v Decision Pro cesses Problems with dela y ed reinforcemen t are w ell mo deled as Markov de cision pr o c esses (MDPs). An MDP consists of \ufffd a set of states S , \ufffd a set of actions A, \u0002\u0004\u0007 Although general MDPs ma y ha v e in\ufffdnite (ev en uncoun table) state and action spaces, w e will only discuss metho ds for solving \ufffdnite-state and \ufffdnite-action problems. In section \u0006, w e discuss metho ds for solving problems with con tin uous input and output spaces. \u0003.\u0002 Finding a P olicy Giv en a Mo del Before w e consider algorithms for learning to b eha v e in MDP en vironmen ts, w e will explore tec hniques for determining the optimal p olicy giv en a correct mo del. These dynamic programming tec hniques will serv e as the foundation and inspiration for the learning algorithms to follo w. W e restrict our atten tion mainly to \ufffdnding optimal p olicies for the in\ufffdnite-horizon discoun ted mo del, but most of these algorithms ha v e analogs for the \ufffdnitehorizon and a v erage-case mo dels as w ell. W e rely on the result that, for the in\ufffdnite-horizon discoun ted mo del, there exists an optimal deterministic stationary p olicy (Bellman, \u0001\t\u0005\u0007). W e will sp eak of the optimal value of a state|it is the exp ected in\ufffdnite discoun ted sum of rew ard that the agen t will gain if it starts in that state and executes the optimal p olicy . Using \ufffd as a complete decision p olicy , it is written V \ufffd (s) = max \ufffd E   \u0001 X t=0 \ufffd t r t ! : This optimal v alue function is unique and can b e de\ufffdned as the solution to the sim ultaneous equations V \ufffd (s) = max a 0 @ R(s; a) + \ufffd X s 0 \u0002S T (s; a; s 0 )V \ufffd (s 0 ) \u0001 A ; \bs \u0002 S ; (\u0001) whic h assert that the v alue of a state s is the exp ected instan taneous rew ard plus the exp ected discoun ted v alue of the next state, using the b est a v ailable action. Giv en the optimal v alue function, w e can sp ecify the optimal p olicy as \ufffd \ufffd (s) = arg max a 0 @ R(s; a) + \ufffd X s 0 \u0002S T (s; a; s 0 )V \ufffd (s 0 ) \u0001 A : \u0003.\u0002.\u0001 V alue Itera tion One w a y , then, to \ufffdnd an optimal p olicy is to \ufffdnd the optimal v alue function. It can b e determined b y a simple iterativ e algorithm called value iter ation that can b e sho wn to con v erge to the correct V \ufffd v alues (Bellman, \u0001\t\u0005\u0007; Bertsek as, \u0001\t\b\u0007). \u0002\u0004\b b ounds the p erformance of the curren t greedy p olicy as a function of the Bel lman r esidual of the curren t v alue function (Williams & Baird, \u0001\t\t\u0003b). It sa ys that if the maxim um di\ufffderence b et w een t w o successiv e v alue functions is less than \ufffd, then the v alue of the greedy p olicy , (the p olicy obtained b y c ho osing, in ev ery state, the action that maximizes the estimated discoun ted rew ard, using the curren t estimate of the v alue function) di\ufffders from the v alue function of the optimal p olicy b y no more than \u0002\ufffd\ufffd =(\u0001 \ufffd \ufffd ) at an y state. This pro vides an e\ufffdectiv e stopping criterion for the algorithm. Puterman (\u0001\t\t\u0004) discusses another stopping criterion, based on the sp an semi-norm, whic h ma y result in earlier termination. Another imp ortan t result is that the greedy p olicy is guaran teed to b e optimal in some \ufffdnite n um b er of steps ev en though the v alue function ma y not ha v e con v erged (Bertsek as, \u0001\t\b\u0007). And in practice, the greedy p olicy is often optimal long b efore the v alue function has con v erged. V alue iteration is v ery \ufffdexible. The assignmen ts to V need not b e done in strict order as sho wn ab o v e, but instead can o ccur async hronously in parallel pro vided that the v alue of ev ery state gets up dated in\ufffdnitely often on an in\ufffdnite run. These issues are treated extensiv ely b y Bertsek as (\u0001\t\b\t), who also pro v es con v ergence results. Up dates based on Equation \u0001 are kno wn as ful l b ackups since they mak e use of information from all p ossible successor states. It can b e sho wn that up dates of the form Q(s; a) := Q(s; a) + \ufffd(r + \ufffd max a 0 Q(s 0 ; a 0 ) \ufffd Q(s; a)) can also b e used as long as eac h pairing of a and s is up dated in\ufffdnitely often, s 0 is sampled from the distribution T (s; a; s 0 ), r is sampled with mean R(s; a) and b ounded v ariance, and the learning rate \ufffd is decreased slo wly . This t yp e of sample b ackup (Singh, \u0001\t\t\u0003) is critical to the op eration of the mo del-free metho ds discussed in the next section. The computational complexit y of the v alue-iteration algorithm with full bac kups, p er iteration, is quadratic in the n um b er of states and linear in the n um b er of actions. Commonly , the transition probabilities T (s; a; s 0 ) are sparse. If there are on a v erage a constan t n um b er of next states with non-zero probabilit y then the cost p er iteration is linear in the n um b er of states and linear in the n um b er of actions. The n um b er of iterations required to reac h the optimal v alue function is p olynomial in the n um b er of states and the magnitude of the largest rew ard if the discoun t factor is held constan t. Ho w ev er, in the w orst case the n um b er of iterations gro ws p olynomially in \u0001=(\u0001 \ufffd \ufffd ), so the con v ergence rate slo ws considerably as the discoun t factor approac hes \u0001 (Littman, Dean, & Kaelbling, \u0001\t\t\u0005b). \u0002\u0004\t improve the policy at each state: \ufffd 0 (s) := arg max a (R(s; a) + \ufffd P s 0 \u0002S T (s; a; s 0 )V \ufffd (s 0 )) until \ufffd = \ufffd 0 The v alue function of a p olicy is just the exp ected in\ufffdnite discoun ted rew ard that will b e gained, at eac h state, b y executing that p olicy . It can b e determined b y solving a set of linear equations. Once w e kno w the v alue of eac h state under the curren t p olicy , w e consider whether the v alue could b e impro v ed b y c hanging the \ufffdrst action tak en. If it can, w e c hange the p olicy to tak e the new action whenev er it is in that situation. This step is guaran teed to strictly impro v e the p erformance of the p olicy . When no impro v emen ts are p ossible, then the p olicy is guaran teed to b e optimal. Since there are at most jAj jS j distinct p olicies, and the sequence of p olicies impro v es at eac h step, this algorithm terminates in at most an exp onen tial n um b er of iterations (Puterman, \u0001\t\t\u0004). Ho w ev er, it is an imp ortan t op en question ho w man y iterations p olicy iteration tak es in the w orst case. It is kno wn that the running time is pseudop olynomial and that for an y \ufffdxed discoun t factor, there is a p olynomial b ound in the total size of the MDP (Littman et al., \u0001\t\t\u0005b). \u0003.\u0002.\u0003 Enhancement to V alue Itera tion and Policy Itera tion In practice, v alue iteration is m uc h faster p er iteration, but p olicy iteration tak es few er iterations. Argumen ts ha v e b een put forth to the e\ufffdect that eac h approac h is b etter for large problems. Puterman's mo di\ufffde d p olicy iter ation algorithm (Puterman & Shin, \u0001\t\u0007\b) pro vides a metho d for trading iteration time for iteration impro v emen t in a smo other w a y . The basic idea is that the exp ensiv e part of p olicy iteration is solving for the exact v alue of V \ufffd . Instead of \ufffdnding an exact v alue for V \ufffd , w e can p erform a few steps of a mo di\ufffded v alue-iteration step where the p olicy is held \ufffdxed o v er successiv e iterations. This can b e sho wn to pro duce an appro ximation to V \ufffd that con v erges linearly in \ufffd . In practice, this can result in substan tial sp eedups. Sev eral standard n umerical-analysis tec hniques that sp eed the con v ergence of dynamic programming can b e used to accelerate v alue and p olicy iteration. Multigrid metho ds can b e used to quic kly seed a go o d initial appro ximation to a high resolution v alue function b y initially p erforming v alue iteration at a coarser resolution (R \ufffd ude, \u0001\t\t\u0003). State aggr egation w orks b y collapsing groups of states to a single meta-state solving the abstracted problem (Bertsek as & Casta ~ non, \u0001\t\b\t). \u0002\u00050 practictioners (Rust, \u0001\t\t\u0006). Linear programming (Sc hrijv er, \u0001\t\b\u0006) is an extremely general problem, and MDPs can b e solv ed b y general-purp ose linear-programming pac k ages (Derman, \u0001\t\u00070; D'Ep enoux, \u0001\t\u0006\u0003; Ho\ufffdman & Karp, \u0001\t\u0006\u0006). An adv an tage of this approac h is that commercial-qualit y linear-programming pac k ages are a v ailable, although the time and space requiremen ts can still b e quite high. F rom a theoretic p ersp ectiv e, linear programming is the only kno wn algorithm that can solv e MDPs in p olynomial time, although the theoretically e\ufffdcien t algorithms ha v e not b een sho wn to b e e\ufffdcien t in practice. \u0004. Learning an Optimal P olicy: Mo del-free Metho ds In the previous section w e review ed metho ds for obtaining an optimal p olicy for an MDP assuming that w e already had a mo del. The mo del consists of kno wledge of the state transition probabilit y function T (s; a; s 0 ) and the reinforcemen t function R(s; a). Reinforcemen t learning is primarily concerned with ho w to obtain the optimal p olicy when suc h a mo del is not kno wn in adv ance. The agen t m ust in teract with its en vironmen t directly to obtain information whic h, b y means of an appropriate algorithm, can b e pro cessed to pro duce an optimal p olicy . A t this p oin t, there are t w o w a ys to pro ceed. \ufffd Mo del-free: Learn a con troller without learning a mo del. \ufffd Mo del-based: Learn a mo del, and use it to deriv e a con troller. Whic h approac h is b etter? This is a matter of some debate in the reinforcemen t-learning comm unit y . A n um b er of algorithms ha v e b een prop osed on b oth sides. This question also app ears in other \ufffdelds, suc h as adaptiv e con trol, where the dic hotom y is b et w een dir e ct and indir e ct adaptiv e con trol. This section examines mo del-free learning, and Section \u0005 examines mo del-based metho ds. The biggest problem facing a reinforcemen t-learning agen t is temp or al cr e dit assignment. Ho w do w e kno w whether the action just tak en is a go o d one, when it migh t ha v e farreac hing e\ufffdects? One strategy is to w ait un til the \\end\" and rew ard the actions tak en if the result w as go o d and punish them if the result w as bad. In ongoing tasks, it is di\ufffdcult to kno w what the \\end\" is, and this migh t require a great deal of memory . Instead, w e will use insigh ts from v alue iteration to adjust the estimated v alue of a state based on \u0002\u0005\u0001 AHC RL v s r a temp oral-di\ufffderence learning strategies for the discoun ted in\ufffdnite-horizon mo del. \u0004.\u0001 Adaptiv e Heuristic Critic and TD (\ufffd) The adaptive heuristic critic algorithm is an adaptiv e v ersion of p olicy iteration (Barto, Sutton, & Anderson, \u0001\t\b\u0003) in whic h the v alue-function computation is no longer implemen ted b y solving a set of linear equations, but is instead computed b y an algorithm called T D (0). A blo c k diagram for this approac h is giv en in Figure \u0004. It consists of t w o comp onen ts: a critic (lab eled AHC), and a reinforcemen t-learning comp onen t (lab eled RL). The reinforcemen t-learning comp onen t can b e an instance of an y of the k -armed bandit algorithms, mo di\ufffded to deal with m ultiple states and non-stationary rew ards. But instead of acting to maximize instan taneous rew ard, it will b e acting to maximize the heuristic v alue, v , that is computed b y the critic. The critic uses the real external reinforcemen t signal to learn to map states to their exp ected discoun ted v alues giv en that the p olicy b eing executed is the one curren tly instan tiated in the RL comp onen t. W e can see the analogy with mo di\ufffded p olicy iteration if w e imagine these comp onen ts w orking in alternation. The p olicy \ufffd implemen ted b y RL is \ufffdxed and the critic learns the v alue function V \ufffd for that p olicy . No w w e \ufffdx the critic and let the RL comp onen t learn a new p olicy \ufffd 0 that maximizes the new v alue function, and so on. In most implemen tations, ho w ev er, b oth comp onen ts op erate sim ultaneously . Only the alternating implemen tation can b e guaran teed to con v erge to the optimal p olicy , under appropriate conditions. Williams and Baird explored the con v ergence prop erties of a class of AHC-related algorithms they call \\incremen tal v arian ts of p olicy iteration\" (Williams & Baird, \u0001\t\t\u0003a). It remains to explain ho w the critic can learn the v alue of a p olicy . W e de\ufffdne hs; a; r ; s 0 i to b e an exp erienc e tuple summarizing a single transition in the en vironmen t. Here s is the agen t's state b efore the transition, a is its c hoice of action, r the instan taneous rew ard it receiv es, and s 0 its resulting state. The v alue of a p olicy is learned using Sutton's T D (0) algorithm (Sutton, \u0001\t\b\b) whic h uses the up date rule V (s) := V (s) + \ufffd(r + \ufffd V (s 0 ) \ufffd V (s)) : Whenev er a state s is visited, its estimated v alue is up dated to b e closer to r + \ufffd V (s 0 ), since r is the instan taneous rew ard receiv ed and V (s 0 ) is the estimated v alue of the actually o ccurring next state. This is analogous to the sample-bac kup rule from v alue iteration|the only di\ufffderence is that the sample is dra wn from the real w orld rather than b y sim ulating a kno wn mo del. The k ey idea is that r + \ufffd V (s 0 ) is a sample of the v alue of V (s), and it is \u0002\u0005\u0002 immediately previous state, s. One v ersion of the eligibil it y trace is de\ufffdned to b e e(s) = t X k =\u0001 (\ufffd\ufffd ) t\ufffdk \ufffd s;s k , where \ufffd s;s k = ( \u0001 if s = s k 0 otherwise . The eligibili t y of a state s is the degree to whic h it has b een visited in the recen t past; when a reinforcemen t is receiv ed, it is used to up date all the states that ha v e b een recen tly visited, according to their eligibili t y . When \ufffd = 0 this is equiv alen t to T D (0). When \ufffd = \u0001, it is roughly equiv alen t to up dating all the states according to the n um b er of times they w ere visited b y the end of a run. Note that w e can up date the eligibili t y online as follo ws: e(s) := ( \ufffd \ufffde(s) + \u0001 if s = curren t state \ufffd \ufffde(s) otherwise . It is computationally more exp ensiv e to execute the general T D (\ufffd), though it often con v erges considerably faster for large \ufffd (Da y an, \u0001\t\t\u0002; Da y an & Sejno wski, \u0001\t\t\u0004). There has b een some recen t w ork on making the up dates more e\ufffdcien t (Cic hosz & Mula wk a, \u0001\t\t\u0005) and on c hanging the de\ufffdnition to mak e T D (\ufffd) more consisten t with the certain t y-equiv alen t metho d (Singh & Sutton, \u0001\t\t\u0006), whic h is discussed in Section \u0005.\u0001. \u0004.\u0002 Q-learning The w ork of the t w o comp onen ts of AHC can b e accomplished in a uni\ufffded manner b y W atkins' Q-learning algorithm (W atkins, \u0001\t\b\t; W atkins & Da y an, \u0001\t\t\u0002). Q-learning is t ypically easier to implemen t. In order to understand Q-learning, w e ha v e to dev elop some additional notation. Let Q \ufffd (s; a) b e the exp ected discoun ted reinforcemen t of taking action a in state s, then con tin uing b y c ho osing actions optimally . Note that V \ufffd (s) is the v alue of s assuming the b est action is tak en initially , and so V \ufffd (s) = max a Q \ufffd (s; a). Q \ufffd (s; a) can hence b e written recursiv ely as Q \ufffd (s; a) = R(s; a) + \ufffd X s 0 \u0002S T (s; a; s 0 ) max a 0 Q \ufffd (s 0 ; a 0 ) : Note also that, since V \ufffd (s) = max a Q \ufffd (s; a), w e ha v e \ufffd \ufffd (s) = arg max a Q \ufffd (s; a) as an optimal p olicy . Because the Q function mak es the action explicit, w e can estimate the Q v alues online using a metho d essen tially the same as T D (0), but also use them to de\ufffdne the p olicy , \u0002\u0005\u0003 When the Q v alues are nearly con v erged to their optimal v alues, it is appropriate for the agen t to act greedily , taking, in eac h situation, the action with the highest Q v alue. During learning, ho w ev er, there is a di\ufffdcult exploitation v ersus exploration trade-o\ufffd to b e made. There are no go o d, formally justi\ufffded approac hes to this problem in the general case; standard practice is to adopt one of the ad ho c metho ds discussed in section \u0002.\u0002. AHC arc hitectures seem to b e more di\ufffdcult to w ork with than Q-learning on a practical lev el. It can b e hard to get the relativ e learning rates righ t in AHC so that the t w o comp onen ts con v erge together. In addition, Q-learning is explor ation insensitive: that is, that the Q v alues will con v erge to the optimal v alues, indep enden t of ho w the agen t b eha v es while the data is b eing collected (as long as all state-action pairs are tried often enough). This means that, although the exploration-exploitation issue m ust b e addressed in Q-learning, the details of the exploration strategy will not a\ufffdect the con v ergence of the learning algorithm. F or these reasons, Q-learning is the most p opular and seems to b e the most e\ufffdectiv e mo del-free algorithm for learning from dela y ed reinforcemen t. It do es not, ho w ev er, address an y of the issues in v olv ed in generalizing o v er large state and/or action spaces. In addition, it ma y con v erge quite slo wly to a go o d p olicy . \u0004.\u0003 Mo del-free Learning With Av erage Rew ard As describ ed, Q-learning can b e applied to discoun ted in\ufffdnite-horizon MDPs. It can also b e applied to undiscoun ted problems as long as the optimal p olicy is guaran teed to reac h a rew ard-free absorbing state and the state is p erio dicall y reset. Sc h w artz (\u0001\t\t\u0003) examined the problem of adapting Q-learning to an a v erage-rew ard framew ork. Although his R-learning algorithm seems to exhibit con v ergence problems for some MDPs, sev eral researc hers ha v e found the a v erage-rew ard criterion closer to the true problem they wish to solv e than a discoun ted criterion and therefore prefer R-learning to Q-learning (Mahadev an, \u0001\t\t\u0004). With that in mind, researc hers ha v e studied the problem of learning optimal a v eragerew ard p olicies. Mahadev an (\u0001\t\t\u0006) surv ey ed mo del-based a v erage-rew ard algorithms from a reinforcemen t-learning p ersp ectiv e and found sev eral di\ufffdculties with existing algorithms. In particular, he sho w ed that existing reinforcemen t-learning algorithms for a v erage rew ard (and some dynamic programming algorithms) do not alw a ys pro duce bias-optimal p olicies. Jaakk ola, Jordan and Singh (\u0001\t\t\u0005) describ ed an a v erage-rew ard learning algorithm with guaran teed con v ergence prop erties. It uses a Mon te-Carlo comp onen t to estimate the exp ected future rew ard for eac h state as the agen t mo v es through the en vironmen t. In \u0002\u0005\u0004 In this section w e still b egin b y assuming that w e don't kno w the mo dels in adv ance, but w e examine algorithms that do op erate b y learning these mo dels. These algorithms are esp ecially imp ortan t in applications in whic h computation is considered to b e c heap and real-w orld exp erience costly . \u0005.\u0001 Certain t y Equiv alen t Metho ds W e b egin with the most conceptually straigh tforw ard metho d: \ufffdrst, learn the T and R functions b y exploring the en vironmen t and k eeping statistics ab out the results of eac h action; next, compute an optimal p olicy using one of the metho ds of Section \u0003. This metho d is kno wn as c ertainty e quivlanc e (Kumar & V araiy a, \u0001\t\b\u0006). There are some serious ob jections to this metho d: \ufffd It mak es an arbitrary division b et w een the learning phase and the acting phase. \ufffd Ho w should it gather data ab out the en vironmen t initially? Random exploration migh t b e dangerous, and in some en vironmen ts is an immensely ine\ufffdcien t metho d of gathering data, requiring exp onen tially more data (Whitehead, \u0001\t\t\u0001) than a system that in terlea v es exp erience gathering with p olicy-buil din g more tigh tly (Ko enig & Simmons, \u0001\t\t\u0003). See Figure \u0005 for an example. \ufffd The p ossibilit y of c hanges in the en vironmen t is also problematic. Breaking up an agen t's life in to a pure learning and a pure acting phase has a considerable risk that the optimal con troller based on early life b ecomes, without detection, a sub optimal con troller if the en vironmen t c hanges. A v ariation on this idea is c ertainty e quivalenc e, in whic h the mo del is learned con tin ually through the agen t's lifetime and, at eac h step, the curren t mo del is used to compute an optimal p olicy and v alue function. This metho d mak es v ery e\ufffdectiv e use of a v ailable data, but still ignores the question of exploration and is extremely computationally demanding, ev en for fairly small state spaces. F ortunately , there are a n um b er of other mo del-based algorithms that are more practical. \u0005.\u0002 Dyna Sutton's Dyna arc hitecture (\u0001\t\t0, \u0001\t\t\u0001) exploits a middle ground, yielding strategies that are b oth more e\ufffdectiv e than mo del-free learning and more computationally e\ufffdcien t than \u0002\u0005\u0005 . . . . . . .  Goal 1 2 3 n the certain t y-equiv alence approac h. It sim ultaneously uses exp erience to build a mo del ( ^ T and ^ R), uses exp erience to adjust the p olicy , and uses the mo del to adjust the p olicy . Dyna op erates in a lo op of in teraction with the en vironmen t. Giv en an exp erience tuple hs; a; s 0 ; r i, it b eha v es as follo ws: \ufffd Up date the mo del, incremen ting statistics for the transition from s to s 0 on action a and for receiving rew ard r for taking action a in state s. The up dated mo dels are ^ T and ^ R. \ufffd Up date the p olicy at state s based on the newly up dated mo del using the rule Q(s; a) := ^ R(s; a) + \ufffd X s 0 ^ T (s; a; s 0 ) max a 0 Q(s 0 ; a 0 ) ; whic h is a v ersion of the v alue-iteration up date for Q v alues. \ufffd P erform k additional up dates: c ho ose k state-action pairs at random and up date them according to the same rule as b efore: Q(s k ; a k ):= ^ R(s k ; a k ) + \ufffd X s 0 ^ T (s k ; a k ; s 0 ) max a 0 Q(s 0 ; a 0 ) : \ufffd Cho ose an action a 0 to p erform in state s 0 , based on the Q v alues but p erhaps mo di\ufffded b y an exploration strategy . The Dyna algorithm requires ab out k times the computation of Q-learning p er instance, but this is t ypically v astly less than for the naiv e mo del-based metho d. A reasonable v alue of k can b e determined based on the relativ e sp eeds of computation and of taking action. Figure \u0006 sho ws a grid w orld in whic h in eac h cell the agen t has four actions (N, S, E, W) and transitions are made deterministically to an adjacen t cell, unless there is a blo c k, in whic h case no mo v emen t o ccurs. As w e will see in T able \u0001, Dyna requires an order of magnitude few er steps of exp erience than do es Q-learning to arriv e at an optimal p olicy . Dyna requires ab out six times more computational e\ufffdort, ho w ev er. \u0002\u0005\u0006 Figure \u0006: A \u0003\u0002\u0007\u0007-state grid w orld. This w as form ulated as a shortest-path reinforcemen tlearning problem, whic h yields the same result as if a rew ard of \u0001 is giv en at the goal, a rew ard of zero elsewhere and a discoun t factor is used. Steps b efore Bac kups b efore con v ergence con v ergence Q-learning \u0005\u0003\u0001,000 \u0005\u0003\u0001,000 Dyna \u0006\u0002,000 \u0003,0\u0005\u0005,000 prioritized sw eeping \u0002\b,000 \u0001,0\u00010,000 T able \u0001: The p erformance of three algorithms describ ed in the text. All metho ds used the exploration heuristic of \\optimism in the face of uncertain t y\": an y state not previously visited w as assumed b y default to b e a goal state. Q-learning used its optimal learning rate parameter for a deterministic maze: \ufffd = \u0001. Dyna and prioritized sw eeping w ere p ermitted to tak e k = \u000200 bac kups p er transition. F or prioritized sw eeping, the priorit y queue often emptied b efore all bac kups w ere used. \u0002\u0005\u0007 (as in Q-learning). T o mak e appropriate c hoices, w e m ust store additional information in the mo del. Eac h state remem b ers its pr e de c essors: the states that ha v e a non-zero transition probabilit y to it under some action. In addition, eac h state has a priority, initially set to zero. Instead of up dating k random state-action pairs, prioritized sw eeping up dates k states with the highest priorit y . F or eac h high-priorit y state s, it w orks as follo ws: \ufffd Remem b er the curren t v alue of the state: V old = V (s). \ufffd Up date the state's v alue V (s) := max a   ^ R(s; a) + \ufffd X s 0 ^ T (s; a; s 0 )V (s 0 ) ! : \ufffd Set the state's priorit y bac k to 0. \ufffd Compute the v alue c hange \ufffd = jV old \ufffd V (s)j. \ufffd Use \ufffd to mo dify the priorities of the predecessors of s. If w e ha v e up dated the V v alue for state s 0 and it has c hanged b y amoun t \ufffd, then the immediate predecessors of s 0 are informed of this ev en t. An y state s for whic h there exists an action a suc h that ^ T (s; a; s 0 ) \u0006= 0 has its priorit y promoted to \ufffd \ufffd ^ T (s; a; s 0 ), unless its priorit y already exceeded that v alue. The global b eha vior of this algorithm is that when a real-w orld transition is \\surprising\" (the agen t happ ens up on a goal state, for instance), then lots of computation is directed to propagate this new information bac k to relev an t predecessor states. When the realw orld transition is \\b oring\" (the actual result is v ery similar to the predicted result), then computation con tin ues in the most deserving part of the space. Running prioritized sw eeping on the problem in Figure \u0006, w e see a large impro v emen t o v er Dyna. The optimal p olicy is reac hed in ab out half the n um b er of steps of exp erience and one-third the computation as Dyna required (and therefore ab out \u00020 times few er steps and t wice the computational e\ufffdort of Q-learning). \u0002\u0005\b \u0001\t\t\u0004) exploits a similar in tuition. It starts b y making an appro ximate v ersion of the MDP whic h is m uc h smaller than the original one. The appro ximate MDP con tains a set of states, called the envelop e, that includes the agen t's curren t state and the goal state, if there is one. States that are not in the en v elop e are summarized b y a single \\out\" state. The planning pro cess is an alternation b et w een \ufffdnding an optimal p olicy on the appro ximate MDP and adding useful states to the en v elop e. Action ma y tak e place in parallel with planning, in whic h case irrelev an t states are also pruned out of the en v elop e. \u0006. Generalization All of the previous discussion has tacitly assumed that it is p ossible to en umerate the state and action spaces and store tables of v alues o v er them. Except in v ery small en vironmen ts, this means impractical memory requiremen ts. It also mak es ine\ufffdcien t use of exp erience. In a large, smo oth state space w e generally exp ect similar states to ha v e similar v alues and similar optimal actions. Surely , therefore, there should b e some more compact represen tation than a table. Most problems will ha v e con tin uous or large discrete state spaces; some will ha v e large or con tin uous action spaces. The problem of learning in large spaces is addressed through gener alization te chniques, whic h allo w compact storage of learned information and transfer of kno wledge b et w een \\similar\" states and actions. The large literature of generalization tec hniques from inductiv e concept learning can b e applied to reinforcemen t learning. Ho w ev er, tec hniques often need to b e tailored to sp eci\ufffdc details of the problem. In the follo wing sections, w e explore the application of standard function-appro ximation tec hniques, adaptiv e resolution mo dels, and hierarc hical metho ds to the problem of reinforcemen t learning. The reinforcemen t-learning arc hitectures and algorithms discussed ab o v e ha v e included the storage of a v ariet y of mappings, including S ! A (p olicies), S ! < (v alue functions), S \ufffd A ! < (Q functions and rew ards), S \ufffd A ! S (deterministic transitions), and S \ufffd A \ufffd S ! [0; \u0001] (transition probabilities). Some of these mappings, suc h as transitions and immediate rew ards, can b e learned using straigh tforw ard sup ervised learning, and can b e handled using an y of the wide v ariet y of function-appro ximation tec hniques for sup ervised learning that supp ort noisy training examples. P opular tec hniques include v arious neuralnet w ork metho ds (Rumelhart & McClelland, \u0001\t\b\u0006), fuzzy logic (Berenji, \u0001\t\t\u0001; Lee, \u0001\t\t\u0001). CMA C (Albus, \u0001\t\b\u0001), and lo cal memory-based metho ds (Mo ore, A tk eson, & Sc haal, \u0001\t\t\u0005), suc h as generalizations of nearest neigh b or metho ds. Other mappings, esp ecially the p olicy \u0002\u0005\t approac hes to generating actions or ev aluations as a function of a description of the agen t's curren t state. The \ufffdrst group of tec hniques co v ered here is sp ecialized to the case when rew ard is not dela y ed; the second group is more generally applicable. \u0006.\u0001.\u0001 Immedia te Rew ard When the agen t's actions do not in\ufffduence state transitions, the resulting problem b ecomes one of c ho osing actions to maximize immediate rew ard as a function of the agen t's curren t state. These problems b ear a resem blance to the bandit problems discussed in Section \u0002 except that the agen t should condition its action selection on the curren t state. F or this reason, this class of problems has b een describ ed as asso ciative reinforcemen t learning. The algorithms in this section address the problem of learning from immediate b o olean reinforcemen t where the state is v ector v alued and the action is a b o olean v ector. Suc h algorithms can and ha v e b een used in the con text of a dela y ed reinforcemen t, for instance, as the RL comp onen t in the AHC arc hitecture describ ed in Section \u0004.\u0001. They can also b e generalized to real-v alued rew ard through r ewar d c omp arison metho ds (Sutton, \u0001\t\b\u0004). CRBP The complemen tary reinforcemen t bac kpropagation algorithm (Ac kley & Littman, \u0001\t\t0) (crbp) consists of a feed-forw ard net w ork mapping an enco ding of the state to an enco ding of the action. The action is determined probabilistically from the activ ation of the output units: if output unit i has activ ation y i , then bit i of the action v ector has v alue \u0001 with probabilit y y i , and 0 otherwise. An y neural-net w ork sup ervised training pro cedure can b e used to adapt the net w ork as follo ws. If the result of generating action a is r = \u0001, then the net w ork is trained with input-output pair hs; ai. If the result is r = 0, then the net w ork is trained with input-output pair hs; \ufffd a i, where \ufffd a = (\u0001 \ufffd a \u0001 ; : : : ; \u0001 \ufffd a n ). The idea b ehind this training rule is that whenev er an action fails to generate rew ard, crbp will try to generate an action that is di\ufffderen t from the curren t c hoice. Although it seems lik e the algorithm migh t oscillate b et w een an action and its complemen t, that do es not happ en. One step of training a net w ork will only c hange the action sligh tly and since the output probabilities will tend to mo v e to w ard 0.\u0005, this mak es action selection more random and increases searc h. The hop e is that the random distribution will generate an action that w orks b etter, and then that action will b e reinforced. AR C The asso ciativ e reinforcemen t comparison (ar c) algorithm (Sutton, \u0001\t\b\u0004) is an instance of the ahc arc hitecture for the case of b o olean actions, consisting of t w o feed\u0002\u00060 enco des whic h action w as tak en. The actions are enco ded as 0 and \u0001, so a \ufffd \u0001=\u0002 alw a ys has the same magnitude; if the rew ard and the action ha v e the same sign, then action \u0001 will b e made more lik ely , otherwise action 0 will b e. As describ ed, the net w ork will tend to seek actions that giv en p ositiv e rew ard. T o extend this approac h to maximize rew ard, w e can compare the rew ard to some baseline, b. This c hanges the adjustmen t to e = (r \ufffd b)(a \ufffd \u0001=\u0002) ; where b is the output of the second net w ork. The second net w ork is trained in a standard sup ervised mo de to estimate r as a function of the input state s. V ariations of this approac h ha v e b een used in a v ariet y of applications (Anderson, \u0001\t\b\u0006; Barto et al., \u0001\t\b\u0003; Lin, \u0001\t\t\u0003b; Sutton, \u0001\t\b\u0004). REINF OR CE Algorithms Williams (\u0001\t\b\u0007, \u0001\t\t\u0002) studied the problem of c ho osing actions to maximize immedate rew ard. He iden ti\ufffded a broad class of up date rules that p erform gradien t descen t on the exp ected rew ard and sho w ed ho w to in tegrate these rules with bac kpropagation. This class, called reinf or ce algorithms, includes linear rew ard-inaction (Section \u0002.\u0001.\u0003) as a sp ecial case. The generic reinf or ce up date for a parameter w ij can b e written \ufffdw ij = \ufffd ij (r \ufffd b ij ) @ @ w ij ln (g j ) where \ufffd ij is a non-negativ e factor, r the curren t reinforcemen t, b ij a reinforcemen t baseline, and g i is the probabilit y densit y function used to randomly generate actions based on unit activ ations. Both \ufffd ij and b ij can tak e on di\ufffderen t v alues for eac h w ij , ho w ev er, when \ufffd ij is constan t throughout the system, the exp ected up date is exactly in the direction of the exp ected rew ard gradien t. Otherwise, the up date is in the same half space as the gradien t but not necessarily in the direction of steep est increase. Williams p oin ts out that the c hoice of baseline, b ij , can ha v e a profound e\ufffdect on the con v ergence sp eed of the algorithm. Logic-Based Metho ds Another strategy for generalization in reinforcemen t learning is to reduce the learning problem to an asso ciativ e problem of learning b o olean functions. A b o olean function has a v ector of b o olean inputs and a single b o olean output. T aking inspiration from mainstream mac hine learning w ork, Kaelbling dev elop ed t w o algorithms for learning b o olean functions from reinforcemen t: one uses the bias of k -DNF to driv e \u0002\u0006\u0001 \u0006.\u0001.\u0002 Dela yed Rew ard Another metho d to allo w reinforcemen t-learning tec hniques to b e applied in large state spaces is mo deled on v alue iteration and Q-learning. Here, a function appro ximator is used to represen t the v alue function b y mapping a state description to a v alue. Man y reseac hers ha v e exp erimen ted with this approac h: Bo y an and Mo ore (\u0001\t\t\u0005) used lo cal memory-based metho ds in conjunction with v alue iteration; Lin (\u0001\t\t\u0001) used bac kpropagation net w orks for Q-learning; W atkins (\u0001\t\b\t) used CMA C for Q-learning; T esauro (\u0001\t\t\u0002, \u0001\t\t\u0005) used bac kpropagation for learning the v alue function in bac kgammon (describ ed in Section \b.\u0001); Zhang and Dietteric h (\u0001\t\t\u0005) used bac kpropagation and T D (\ufffd) to learn go o d strategies for job-shop sc heduling. Although there ha v e b een some p ositiv e examples, in general there are unfortunate interactions b et w een function appro ximation and the learning rules. In discrete en vironmen ts there is a guaran tee that an y op eration that up dates the v alue function (according to the Bellman equations) can only reduce the error b et w een the curren t v alue function and the optimal v alue function. This guaran tee no longer holds when generalization is used. These issues are discussed b y Bo y an and Mo ore (\u0001\t\t\u0005), who giv e some simple examples of v alue function errors gro wing arbitrarily large when generalization is used with v alue iteration. Their solution to this, applicable only to certain classes of problems, discourages suc h div ergence b y only p ermitting up dates whose estimated v alues can b e sho wn to b e near-optimal via a battery of Mon te-Carlo exp erimen ts. Thrun and Sc h w artz (\u0001\t\t\u0003) theorize that function appro ximation of v alue functions is also dangerous b ecause the errors in v alue functions due to generalization can b ecome comp ounded b y the \\max\" op erator in the de\ufffdnition of the v alue function. Sev eral recen t results (Gordon, \u0001\t\t\u0005; Tsitsiklis & V an Ro y , \u0001\t\t\u0006) sho w ho w the appropriate c hoice of function appro ximator can guaran tee con v ergence, though not necessarily to the optimal v alues. Baird's r esidual gr adient tec hnique (Baird, \u0001\t\t\u0005) pro vides guaran teed con v ergence to lo cally optimal solutions. P erhaps the glo ominess of these coun ter-examples is misplaced. Bo y an and Mo ore (\u0001\t\t\u0005) rep ort that their coun ter-examples c an b e made to w ork with problem-sp eci\ufffdc hand-tuning despite the unreliabilit y of un tuned algorithms that pro v ably con v erge in discrete domains. Sutton (\u0001\t\t\u0006) sho ws ho w mo di\ufffded v ersions of Bo y an and Mo ore's examples can con v erge successfully . An op en question is whether general principles, ideally supp orted b y theory , can help us understand when v alue function appro ximation will succeed. In Sutton's com\u0002\u0006\u0002 whereas Sutton's metho d sampled along empirical tra jectories. There are in tuitiv e reasons to b eliev e that the fourth factor is particularly imp ortan t, but more careful researc h is needed. Adaptiv e Resolution Mo dels In man y cases, what w e w ould lik e to do is partition the en vironmen t in to regions of states that can b e considered the same for the purp oses of learning and generating actions. Without detailed prior kno wledge of the en vironmen t, it is v ery di\ufffdcult to kno w what gran ularit y or placemen t of partitions is appropriate. This problem is o v ercome in metho ds that use adaptiv e resolution; during the course of learning, a partition is constructed that is appropriate to the en vironmen t. Decision T rees In en vironmen ts that are c haracterized b y a set of b o olean or discretev alued v ariables, it is p ossible to learn compact decision trees for represen ting Q v alues. The G-le arning algorithm (Chapman & Kaelbling, \u0001\t\t\u0001), w orks as follo ws. It starts b y assuming that no partitioning is necessary and tries to learn Q v alues for the en tire en vironmen t as if it w ere one state. In parallel with this pro cess, it gathers statistics based on individual input bits; it asks the question whether there is some bit b in the state description suc h that the Q v alues for states in whic h b = \u0001 are signi\ufffdcan tly di\ufffderen t from Q v alues for states in whic h b = 0. If suc h a bit is found, it is used to split the decision tree. Then, the pro cess is rep eated in eac h of the lea v es. This metho d w as able to learn v ery small represen tations of the Q function in the presence of an o v erwhelming n um b er of irrelev an t, noisy state attributes. It outp erformed Q-learning with bac kpropagation in a simple videogame en vironmen t and w as used b y McCallum (\u0001\t\t\u0005) (in conjunction with other tec hniques for dealing with partial observ abilit y) to learn b eha viors in a complex driving-sim ulator. It cannot, ho w ev er, acquire partitions in whic h attributes are only signi\ufffdcan t in com bination (suc h as those needed to solv e parit y problems). V ariable Resolution Dynamic Programming The VRDP algorithm (Mo ore, \u0001\t\t\u0001) enables con v en tional dynamic programming to b e p erformed in real-v alued m ultiv ariate state-spaces where straigh tforw ard discretization w ould fall prey to the curse of dimensionalit y . A k d-tree (similar to a decision tree) is used to partition state space in to coarse regions. The coarse regions are re\ufffdned in to detailed regions, but only in parts of the state space whic h are predicted to b e imp ortan t. This notion of imp ortance is obtained b y running \\men tal tra jectories\" through state space. This algorithm pro v ed e\ufffdectiv e on a n um b er of problems for whic h full high-resolution arra ys w ould ha v e b een impractical. It has the disadv an tage of requiring a guess at an initially v alid tra jectory through state-space. \u0002\u0006\u0003 G Start Goal (a) G (b) G (c) Figure \u0007: (a) A t w o-dimensional maze problem. The p oin t rob ot m ust \ufffdnd a path from start to goal without crossing an y of the barrier lines. (b) The path tak en b y P artiGame during the en tire \ufffdrst trial. It b egins with in tense exploration to \ufffdnd a route out of the almost en tirely enclosed start region. Ha ving ev en tually reac hed a su\ufffdcien tly high resolution, it disco v ers the gap and pro ceeds greedily to w ards the goal, only to b e temp orarily blo c k ed b y the goal's barrier region. (c) The second trial. P artiGame Algorithm Mo ore's P artiGame algorithm (Mo ore, \u0001\t\t\u0004) is another solution to the problem of learning to ac hiev e goal con\ufffdgurations in deterministic high-dimensional con tin uous spaces b y learning an adaptiv e-resolution mo del. It also divides the en vironmen t in to cells; but in eac h cell, the actions a v ailable consist of aiming at the neigh b oring cells (this aiming is accomplished b y a lo cal con troller, whic h m ust b e pro vided as part of the problem statemen t). The graph of cell transitions is solv ed for shortest paths in an online incremen tal manner, but a minimax criterion is used to detect when a group of cells is to o coarse to prev en t mo v emen t b et w een obstacles or to a v oid limit cycles. The o\ufffdending cells are split to higher resolution. Ev en tually , the en vironmen t is divided up just enough to c ho ose appropriate actions for ac hieving the goal, but no unnecessary distinctions are made. An imp ortan t feature is that, as w ell as reducing memory and computational requiremen ts, it also structures exploration of state space in a m ulti-resolution manner. Giv en a failure, the agen t will initially try something v ery di\ufffderen t to rectify the failure, and only resort to small lo cal c hanges when all the qualitativ ely di\ufffderen t strategies ha v e b een exhausted. Figure \u0007a sho ws a t w o-dimensional con tin uous maze. Figure \u0007b sho ws the p erformance of a rob ot using the P artiGame algorithm during the v ery \ufffdrst trial. Figure \u0007c sho ws the second trial, started from a sligh tly di\ufffderen t p osition. This is a v ery fast algorithm, learning p olicies in spaces of up to nine dimensions in less than a min ute. The restriction of the curren t implemen tation to deterministic en vironmen ts limits its applicabili t y , ho w ev er. McCallum (\u0001\t\t\u0005) suggests some related tree-structured metho ds. \u0002\u0006\u0004 action space is con tin uous, neither approac h is p ossible. An alternativ e strategy is to use a single net w ork with b oth the state and action as input and Q v alue as the output. T raining suc h a net w ork is not conceptually di\ufffdcult, but using the net w ork to \ufffdnd the optimal action can b e a c hallenge. One metho d is to do a lo cal gradien t-ascen t searc h on the action in order to \ufffdnd one with high v alue (Baird & Klopf, \u0001\t\t\u0003). Gullapalli (\u0001\t\t0, \u0001\t\t\u0002) has dev elop ed a \\neural\" reinforcemen t-learning unit for use in con tin uous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and v ariance based on previous exp erience. When the c hosen actions are not p erforming w ell, the v ariance is high, resulting in exploration of the range of c hoices. When an action p erforms w ell, the mean is mo v ed in that direction and the v ariance decreased, resulting in a tendency to generate more action v alues near the successful one. This metho d w as successfully emplo y ed to learn to con trol a rob ot arm with man y con tin uous degrees of freedom. \u0006.\u0003 Hierarc hical Metho ds Another strategy for dealing with large state spaces is to treat them as a hierarc h y of learning problems. In man y cases, hierarc hical solutions in tro duce sligh t sub-optimalit y in p erformance, but p oten tially gain a go o d deal of e\ufffdciency in execution time, learning time, and space. Hierarc hical learners are commonly structured as gate d b ehaviors, as sho wn in Figure \b. There is a collection of b ehaviors that map en vironmen t states in to lo w-lev el actions and a gating function that decides, based on the state of the en vironmen t, whic h b eha vior's actions should b e switc hed through and actually executed. Maes and Bro oks (\u0001\t\t0) used a v ersion of this arc hitecture in whic h the individual b eha viors w ere \ufffdxed a priori and the gating function w as learned from reinforcemen t. Mahadev an and Connell (\u0001\t\t\u0001b) used the dual approac h: they \ufffdxed the gating function, and supplied reinforcemen t functions for the individual b eha viors, whic h w ere learned. Lin (\u0001\t\t\u0003a) and Dorigo and Colom b etti (\u0001\t\t\u0005, \u0001\t\t\u0004) b oth used this approac h, \ufffdrst training the b eha viors and then training the gating function. Man y of the other hierarc hical learning metho ds can b e cast in this framew ork. \u0006.\u0003.\u0001 Feud al Q-learning F eudal Q-learning (Da y an & Hin ton, \u0001\t\t\u0003; W atkins, \u0001\t\b\t) in v olv es a hierarc h y of learning mo dules. In the simplest case, there is a high-lev el master and a lo w-lev el sla v e. The master receiv es reinforcemen t from the external en vironmen t. Its actions consist of commands that \u0002\u0006\u0005 s b1 b2 b3 g a Figure \b: A structure of gated b eha viors. it can giv e to the lo w-lev el learner. When the master generates a particular command to the sla v e, it m ust rew ard the sla v e for taking actions that satisfy the command, ev en if they do not result in external reinforcemen t. The master, then, learns a mapping from states to commands. The sla v e learns a mapping from commands and states to external actions. The set of \\commands\" and their asso ciated reinforcemen t functions are established in adv ance of the learning. This is really an instance of the general \\gated b eha viors\" approac h, in whic h the sla v e can execute an y of the b eha viors dep ending on its command. The reinforcemen t functions for the individual b eha viors (commands) are giv en, but learning tak es place sim ultaneously at b oth the high and lo w lev els. \u0006.\u0003.\u0002 Compositional Q-learning Singh's comp ositional Q-learning (\u0001\t\t\u0002b, \u0001\t\t\u0002a) (C-QL) consists of a hierarc h y based on the temp oral sequencing of subgoals. The elemental tasks are b eha viors that ac hiev e some recognizable condition. The high-lev el goal of the system is to ac hiev e some set of conditions in sequen tial order. The ac hiev emen t of the conditions pro vides reinforcemen t for the elemen tal tasks, whic h are trained \ufffdrst to ac hiev e individual subgoals. Then, the gating function learns to switc h the elemen tal tasks in order to ac hiev e the appropriate high-lev el sequen tial goal. This metho d w as used b y Tham and Prager (\u0001\t\t\u0004) to learn to con trol a sim ulated m ulti-link rob ot arm. \u0006.\u0003.\u0003 Hierar chical Dist ance to Go al Esp ecially if w e consider reinforcemen t learning mo dules to b e part of larger agen t arc hitectures, it is imp ortan t to consider problems in whic h goals are dynamically input to the learner. Kaelbling's HDG algorithm (\u0001\t\t\u0003a) uses a hierarc hical approac h to solving problems when goals of ac hiev emen t (the agen t should get to a particular state as quic kly as p ossible) are giv en to an agen t dynamically . The HDG algorithm w orks b y analogy with na vigation in a harb or. The en vironmen t is partitioned (a priori, but more recen t w ork (Ashar, \u0001\t\t\u0004) addresses the case of learning the partition) in to a set of regions whose cen ters are kno wn as \\landmarks.\" If the agen t is \u0002\u0006\u0006 2/5 1/5 2/5 printer of\ufb01ce +100 hall hall Figure \t: An example of a partially observ able en vironmen t. curren tly in the same region as the goal, then it uses lo w-lev el actions to mo v e to the goal. If not, then high-lev el information is used to determine the next landmark on the shortest path from the agen t's closest landmark to the goal's closest landmark. Then, the agen t uses lo w-lev el information to aim to w ard that next landmark. If errors in action cause deviations in the path, there is no problem; the b est aiming p oin t is recomputed on ev ery step. \u0007. P artially Observ able En vironmen ts In man y real-w orld en vironmen ts, it will not b e p ossible for the agen t to ha v e p erfect and complete p erception of the state of the en vironmen t. Unfortunately , complete observ abilit y is necessary for learning metho ds based on MDPs. In this section, w e consider the case in whic h the agen t mak es observations of the state of the en vironmen t, but these observ ations ma y b e noisy and pro vide incomplete information. In the case of a rob ot, for instance, it migh t observ e whether it is in a corridor, an op en ro om, a T-junction, etc., and those observ ations migh t b e error-prone. This problem is also referred to as the problem of \\incomplete p erception,\" \\p erceptual aliasing,\" or \\hidden state.\" In this section, w e will consider extensions to the basic MDP framew ork for solving partially observ able problems. The resulting formal mo del is called a p artial ly observable Markov de cision pr o c ess or POMDP . \u0007.\u0001 State-F ree Deterministic P olicies The most naiv e strategy for dealing with partial observ abilit y is to ignore it. That is, to treat the observ ations as if they w ere the states of the en vironmen t and try to learn to b eha v e. Figure \t sho ws a simple en vironmen t in whic h the agen t is attempting to get to the prin ter from an o\ufffdce. If it mo v es from the o\ufffdce, there is a go o d c hance that the agen t will end up in one of t w o places that lo ok lik e \\hall\", but that require di\ufffderen t actions for getting to the prin ter. If w e consider these states to b e the same, then the agen t cannot p ossibly b eha v e optimally . But ho w w ell can it do? The resulting problem is not Mark o vian, and Q-learning cannot b e guaran teed to conv erge. Small breac hes of the Mark o v requiremen t are w ell handled b y Q-learning, but it is p ossible to construct simple en vironmen ts that cause Q-learning to oscillate (Chrisman & \u0002\u0006\u0007 \u0007.\u0002 State-F ree Sto c hastic P olicies Some impro v emen t can b e gained b y considering sto c hastic p olicies; these are mappings from observ ations to probabilit y distributions o v er actions. If there is randomness in the agen t's actions, it will not get stuc k in the hall forev er. Jaakk ola, Singh, and Jordan (\u0001\t\t\u0005) ha v e dev elop ed an algorithm for \ufffdnding lo cally-optimal sto c hastic p olicies, but \ufffdnding a globally optimal p olicy is still NP hard. In our example, it turns out that the optimal sto c hastic p olicy is for the agen t, when in a state that lo oks lik e a hall, to go east with probabilit y \u0002 \ufffd p \u0002 \ufffd 0:\u0006 and w est with probabilit y p \u0002 \ufffd \u0001 \ufffd 0:\u0004. This p olicy can b e found b y solving a simple (in this case) quadratic program. The fact that suc h a simple example can pro duce irrational n um b ers giv es some indication that it is a di\ufffdcult problem to solv e exactly . \u0007.\u0003 P olicies with In ternal State The only w a y to b eha v e truly e\ufffdectiv ely in a wide-range of en vironmen ts is to use memory of previous actions and observ ations to disam biguate the curren t state. There are a v ariet y of approac hes to learning p olicies with in ternal state. Recurren t Q-learning One in tuitiv ely simple approac h is to use a recurren t neural netw ork to learn Q v alues. The net w ork can b e trained using bac kpropagation through time (or some other suitable tec hnique) and learns to retain \\history features\" to predict v alue. This approac h has b een used b y a n um b er of researc hers (Meeden, McGra w, & Blank, \u0001\t\t\u0003; Lin & Mitc hell, \u0001\t\t\u0002; Sc hmidh ub er, \u0001\t\t\u0001b). It seems to w ork e\ufffdectiv ely on simple problems, but can su\ufffder from con v ergence to lo cal optima on more complex problems. Classi\ufffder Systems Classi\ufffder systems (Holland, \u0001\t\u0007\u0005; Goldb erg, \u0001\t\b\t) w ere explicitly dev elop ed to solv e problems with dela y ed rew ard, including those requiring short-term memory . The in ternal mec hanism t ypically used to pass rew ard bac k through c hains of decisions, called the bucket brigade algorithm, b ears a close resem blance to Q-learning. In spite of some early successes, the original design do es not app ear to handle partially observ ed en vironmen ts robustly . Recen tly , this approac h has b een reexamined using insigh ts from the reinforcemen tlearning literature, with some success. Dorigo did a comparativ e study of Q-learning and classi\ufffder systems (Dorigo & Bersini, \u0001\t\t\u0004). Cli\ufffd and Ross (\u0001\t\t\u0004) start with Wilson's zeroth\u0002\u0006\b i b a SE \u03c0 approac h is unlik ely to scale to more complex en vironmen ts. Dorigo and Colom b etti applied classi\ufffder systems to a mo derately complex problem of learning rob ot b eha vior from immediate reinforcemen t (Dorigo, \u0001\t\t\u0005; Dorigo & Colom b etti, \u0001\t\t\u0004). Finite-history-windo w Approac h One w a y to restore the Mark o v prop ert y is to allo w decisions to b e based on the history of recen t observ ations and p erhaps actions. Lin and Mitc hell (\u0001\t\t\u0002) used a \ufffdxed-width \ufffdnite history windo w to learn a p ole balancing task. McCallum (\u0001\t\t\u0005) describ es the \\utile su\ufffdx memory\" whic h learns a v ariable-width windo w that serv es sim ultaneously as a mo del of the en vironmen t and a \ufffdnite-memory p olicy . This system has had excellen t results in a v ery complex driving-sim ulation domain (McCallum, \u0001\t\t\u0005). Ring (\u0001\t\t\u0004) has a neural-net w ork approac h that uses a v ariable history windo w, adding history when necessary to disam biguate situations. POMDP Approac h Another strategy consists of using hidden Mark o v mo del (HMM) tec hniques to learn a mo del of the en vironmen t, including the hidden state, then to use that mo del to construct a p erfe ct memory con troller (Cassandra, Kaelbling, & Littman, \u0001\t\t\u0004; Lo v ejo y , \u0001\t\t\u0001; Monahan, \u0001\t\b\u0002). Chrisman (\u0001\t\t\u0002) sho w ed ho w the forw ard-bac kw ard algorithm for learning HMMs could b e adapted to learning POMDPs. He, and later McCallum (\u0001\t\t\u0003), also ga v e heuristic statesplitting rules to attempt to learn the smallest p ossible mo del for a giv en en vironmen t. The resulting mo del can then b e used to in tegrate information from the agen t's observ ations in order to mak e decisions. Figure \u00010 illustrates the basic structure for a p erfect-memory con troller. The comp onen t on the left is the state estimator, whic h computes the agen t's b elief state, b as a function of the old b elief state, the last action a, and the curren t observ ation i. In this con text, a b elief state is a probabilit y distribution o v er states of the en vironmen t, indicating the lik eliho o d, giv en the agen t's past exp erience, that the en vironmen t is actually in eac h of those states. The state estimator can b e constructed straigh tforw ardly using the estimated w orld mo del and Ba y es' rule. No w w e are left with the problem of \ufffdnding a p olicy mapping b elief states in to action. This problem can b e form ulated as an MDP, but it is di\ufffdcult to solv e using the tec hniques describ ed earlier, b ecause the input space is con tin uous. Chrisman's approac h (\u0001\t\t\u0002) do es not tak e in to accoun t future uncertain t y , but yields a p olicy after a small amoun t of computation. A standard approac h from the op erations-researc h literature is to solv e for the \u0002\u0006\t ranged from rob otics, to industrial man ufacturing, to com binatorial searc h problems suc h as computer game pla ying. Practical applications pro vide a test of the e\ufffdcacy and usefulness of learning algorithms. They are also an inspiration for deciding whic h comp onen ts of the reinforcemen t learning framew ork are of practical imp ortance. F or example, a researc her with a real rob otic task can pro vide a data p oin t to questions suc h as: \ufffd Ho w imp ortan t is optimal exploration? Can w e break the learning p erio d in to exploration phases and exploitation phases? \ufffd What is the most useful mo del of long-term rew ard: Finite horizon? Discoun ted? In\ufffdnite horizon? \ufffd Ho w m uc h computation is a v ailable b et w een agen t decisions and ho w should it b e used? \ufffd What prior kno wledge can w e build in to the system, and whic h algorithms are capable of using that kno wledge? Let us examine a set of practical applications of reinforcemen t learning, while b earing these questions in mind. \b.\u0001 Game Pla ying Game pla ying has dominated the Arti\ufffdcial In telligence w orld as a problem domain ev er since the \ufffdeld w as b orn. Tw o-pla y er games do not \ufffdt in to the established reinforcemen t-learning framew ork since the optimalit y criterion for games is not one of maximizing rew ard in the face of a \ufffdxed en vironmen t, but one of maximizing rew ard against an optimal adv ersary (minimax). Nonetheless, reinforcemen t-learning algorithms can b e adapted to w ork for a v ery general class of games (Littman, \u0001\t\t\u0004a) and man y researc hers ha v e used reinforcemen t learning in these en vironmen ts. One application, sp ectacularly far ahead of its time, w as Sam uel's c hec k ers pla ying system (Sam uel, \u0001\t\u0005\t). This learned a v alue function represen ted b y a linear function appro ximator, and emplo y ed a training sc heme similar to the up dates used in v alue iteration, temp oral di\ufffderences and Q-learning. More recen tly , T esauro (\u0001\t\t\u0002, \u0001\t\t\u0004, \u0001\t\t\u0005) applied the temp oral di\ufffderence algorithm to bac kgammon. Bac kgammon has appro ximately \u00010 \u00020 states, making table-based reinforcemen t learning imp ossible. Instead, T esauro used a bac kpropagation-based three-la y er \u0002\u00070 T able \u0002: TD-Gammon's p erformance in games against the top h uman professional pla y ers. A bac kgammon tournamen t in v olv es pla ying a series of games for p oin ts un til one pla y er reac hes a set target. TD-Gammon w on none of these tournamen ts but came su\ufffdcien tly close that it is no w considered one of the b est few pla y ers in the w orld. neural net w ork as a function appro ximator for the v alue function Bo ar d Position ! Pr ob ability of victory for curr ent player: Tw o v ersions of the learning algorithm w ere used. The \ufffdrst, whic h w e will call Basic TDGammon, used v ery little prede\ufffdned kno wledge of the game, and the represen tation of a b oard p osition w as virtually a ra w enco ding, su\ufffdcien tly p o w erful only to p ermit the neural net w ork to distinguish b et w een conceptually di\ufffderen t p ositions. The second, TD-Gammon, w as pro vided with the same ra w state information supplemen ted b y a n um b er of handcrafted features of bac kgammon b oard p ositions. Pro viding hand-crafted features in this manner is a go o d example of ho w inductiv e biases from h uman kno wledge of the task can b e supplied to a learning algorithm. The training of b oth learning algorithms required sev eral mon ths of computer time, and w as ac hiev ed b y constan t self-pla y . No exploration strategy w as used|the system alw a ys greedily c hose the mo v e with the largest exp ected probabilit y of victory . This naiv e exploration strategy pro v ed en tirely adequate for this en vironmen t, whic h is p erhaps surprising giv en the considerable w ork in the reinforcemen t-learning literature whic h has pro duced n umerous coun ter-examples to sho w that greedy exploration can lead to p o or learning p erformance. Bac kgammon, ho w ev er, has t w o imp ortan t prop erties. Firstly , whatev er p olicy is follo w ed, ev ery game is guaran teed to end in \ufffdnite time, meaning that useful rew ard information is obtained fairly frequen tly . Secondly , the state transitions are su\ufffdcien tly sto c hastic that indep enden t of the p olicy , all states will o ccasionally b e visited|a wrong initial v alue function has little danger of starving us from visiting a critical part of state space from whic h imp ortan t information could b e obtained. The results (T able \u0002) of TD-Gammon are impressiv e. It has comp eted at the v ery top lev el of in ternational h uman pla y . Basic TD-Gammon pla y ed resp ectably , but not at a professional standard. \u0002\u0007\u0001   \u03c43 \u03c41 \u03c42 \u03b1 \u03b8 x, y p \u03c41 \u03c42 Figure \u0001\u0001: Sc haal and A tk eson's devil-stic king rob ot. The tap ered stic k is hit alternately b y eac h of the t w o hand stic ks. The task is to k eep the devil stic k from falling for as man y hits as p ossible. The rob ot has three motors indicated b y torque v ectors \ufffd \u0001 ; \ufffd \u0002 ; \ufffd \u0003 . Although exp erimen ts with other games ha v e in some cases pro duced in teresting learning b eha vior, no success close to that of TD-Gammon has b een rep eated. Other games that ha v e b een studied include Go (Sc hraudolph, Da y an, & Sejno wski, \u0001\t\t\u0004) and Chess (Thrun, \u0001\t\t\u0005). It is still an op en question as to if and ho w the success of TD-Gammon can b e rep eated in other domains. \b.\u0002 Rob otics and Con trol In recen t y ears there ha v e b een man y rob otics and con trol applications that ha v e used reinforcemen t learning. Here w e will concen trate on the follo wing four examples, although man y other in teresting ongoing rob otics in v estigations are underw a y . \u0001. Sc haal and A tk eson (\u0001\t\t\u0004) constructed a t w o-armed rob ot, sho wn in Figure \u0001\u0001, that learns to juggle a device kno wn as a devil-stic k. This is a complex non-linear con trol task in v olving a six-dimensional state space and less than \u000200 msecs p er con trol decision. After ab out \u00040 initial attempts the rob ot learns to k eep juggling for h undreds of hits. A t ypical h uman learning the task requires an order of magnitude more practice to ac hiev e pro\ufffdciency at mere tens of hits. The juggling rob ot learned a w orld mo del from exp erience, whic h w as generalized to un visited states b y a function appro ximation sc heme kno wn as lo cally w eigh ted regression (Clev eland & Delvin, \u0001\t\b\b; Mo ore & A tk eson, \u0001\t\t\u0002). Bet w een eac h trial, a form of dynamic programming sp eci\ufffdc to linear con trol p olicies and lo cally linear transitions w as used to impro v e the p olicy . The form of dynamic programming is kno wn as linear-quadratic-regulator design (Sage & White, \u0001\t\u0007\u0007). \u0002\u0007\u0002 ical reinforcemen t learning, an un think ably high dimensional state space, con taining man y dozens of degrees of freedom. F our mobile rob ots tra v eled within an enclosure collecting small disks and transp orting them to a destination region. There w ere three enhancemen ts to the basic Q-learning algorithm. Firstly , pre-programmed signals called pr o gr ess estimators w ere used to break the monolithic task in to subtasks. This w as ac hiev ed in a robust manner in whic h the rob ots w ere not forced to use the estimators, but had the freedom to pro\ufffdt from the inductiv e bias they pro vided. Secondly , con trol w as decen tralized. Eac h rob ot learned its o wn p olicy indep enden tly without explicit comm unication with the others. Thirdly , state space w as brutally quan tized in to a small n um b er of discrete states according to v alues of a small n umb er of pre-programmed b o olean features of the underlying sensors. The p erformance of the Q-learned p olicies w ere almost as go o d as a simple hand-crafted con troller for the job. \u0004. Q-learning has b een used in an elev ator dispatc hing task (Crites & Barto, \u0001\t\t\u0006). The problem, whic h has b een implemen ted in sim ulation only at this stage, in v olv ed four elev ators servicing ten \ufffdo ors. The ob jectiv e w as to minimize the a v erage squared w ait time for passengers, discoun ted in to future time. The problem can b e p osed as a discrete Mark o v system, but there are \u00010 \u0002\u0002 states ev en in the most simpli\ufffded v ersion of the problem. Crites and Barto used neural net w orks for function appro ximation and pro vided an excellen t comparison study of their Q-learning approac h against the most p opular and the most sophisticated elev ator dispatc hing algorithms. The squared w ait time of their con troller w as appro ximately \u0007% less than the b est alternativ e algorithm (\\Empt y the System\" heuristic with a receding horizon con troller) and less than half the squared w ait time of the con troller most frequen tly used in real elev ator systems. \u0005. The \ufffdnal example concerns an application of reinforcemen t learning b y one of the authors of this surv ey to a pac k aging task from a fo o d pro cessing industry . The problem in v olv es \ufffdlling con tainers with v ariable n um b ers of non-iden tical pro ducts. The pro duct c haracteristics also v ary with time, but can b e sensed. Dep ending on the task, v arious constrain ts are placed on the con tainer-\ufffdlling pro cedure. Here are three examples: \ufffd The mean w eigh t of all con tainers pro duced b y a shift m ust not b e b elo w the man ufacturer's declared w eigh t W . \u0002\u0007\u0003 so far. The system w as discretized in to \u000200,000 discrete states and lo cal w eigh ted regression w as used to learn and generalize a transition mo del. Prioritized sw eeping w as used to main tain an optimal v alue function as eac h new piece of transition information w as obtained. In sim ulated exp erimen ts the sa vings w ere considerable, t ypically with w astage reduced b y a factor of ten. Since then the system has b een deplo y ed successfully in sev eral factories within the United States. Some in teresting asp ects of practical reinforcemen t learning come to ligh t from these examples. The most striking is that in all cases, to mak e a real system w ork it pro v ed necessary to supplemen t the fundamen tal algorithm with extra pre-programmed kno wledge. Supplying extra kno wledge comes at a price: more h uman e\ufffdort and insigh t is required and the system is subsequen tly less autonomous. But it is also clear that for tasks suc h as these, a kno wledge-free approac h w ould not ha v e ac hiev ed w orth while p erformance within the \ufffdnite lifetime of the rob ots. What forms did this pre-programmed kno wledge tak e? It included an assumption of linearit y for the juggling rob ot's p olicy , a man ual breaking up of the task in to subtasks for the t w o mobile-rob ot examples, while the b o x-pusher also used a clustering tec hnique for the Q v alues whic h assumed lo cally consisten t Q v alues. The four disk-collecting rob ots additionally used a man ually discretized state space. The pac k aging example had far few er dimensions and so required corresp ondingly w eak er assumptions, but there, to o, the assumption of lo cal piecewise con tin uit y in the transition mo del enabled massiv e reductions in the amoun t of learning data required. The exploration strategies are in teresting to o. The juggler used careful statistical analysis to judge where to pro\ufffdtably exp erimen t. Ho w ev er, b oth mobile rob ot applications w ere able to learn w ell with greedy exploration|alw a ys exploiting without delib erate exploration. The pac k aging task used optimism in the face of uncertain t y . None of these strategies mirrors theoretically optimal (but computationally in tractable) exploration, and y et all pro v ed adequate. Finally , it is also w orth considering the computational regimes of these exp erimen ts. They w ere all v ery di\ufffderen t, whic h indicates that the di\ufffdering computational demands of v arious reinforcemen t learning algorithms do indeed ha v e an arra y of di\ufffdering applications. The juggler needed to mak e v ery fast decisions with lo w latency b et w een eac h hit, but had long p erio ds (\u00030 seconds and more) b et w een eac h trial to consolidate the exp eriences collected on the previous trial and to p erform the more aggressiv e computation necessary to pro duce a new reactiv e con troller on the next trial. The b o x-pushing rob ot w as mean t to \u0002\u0007\u0004 reinforcemen t learning. The insigh ts and task constrain ts that they pro duce will ha v e an imp ortan t e\ufffdect on shaping the kind of algorithms that are dev elop ed in future. \t. Conclusions There are a v ariet y of reinforcemen t-learning tec hniques that w ork e\ufffdectiv ely on a v ariet y of small problems. But v ery few of these tec hniques scale w ell to larger problems. This is not b ecause researc hers ha v e done a bad job of in v en ting learning tec hniques, but b ecause it is v ery di\ufffdcult to solv e arbitrary problems in the general case. In order to solv e highly complex problems, w e m ust giv e up tabula r asa learning tec hniques and b egin to incorp orate bias that will giv e lev erage to the learning pro cess. The necessary bias can come in a v ariet y of forms, including the follo wing: shaping: The tec hnique of shaping is used in training animals (Hilgard & Bo w er, \u0001\t\u0007\u0005); a teac her presen ts v ery simple problems to solv e \ufffdrst, then gradually exp oses the learner to more complex problems. Shaping has b een used in sup ervised-learning systems, and can b e used to train hierarc hical reinforcemen t-learning systems from the b ottom up (Lin, \u0001\t\t\u0001), and to alleviate problems of dela y ed reinforcemen t b y decreasing the dela y un til the problem is w ell understo o d (Dorigo & Colom b etti, \u0001\t\t\u0004; Dorigo, \u0001\t\t\u0005). lo cal reinforcemen t signals: Whenev er p ossible, agen ts should b e giv en reinforcemen t signals that are lo cal. In applications in whic h it is p ossible to compute a gradien t, rew arding the agen t for taking steps up the gradien t, rather than just for ac hieving the \ufffdnal goal, can sp eed learning signi\ufffdcan tly (Mataric, \u0001\t\t\u0004). imitation: An agen t can learn b y \\w atc hing\" another agen t p erform the task (Lin, \u0001\t\t\u0001). F or real rob ots, this requires p erceptual abilities that are not y et a v ailable. But another strategy is to ha v e a h uman supply appropriate motor commands to a rob ot through a jo ystic k or steering wheel (P omerleau, \u0001\t\t\u0003). problem decomp osition: Decomp osing a h uge learning problem in to a collection of smaller ones, and pro viding useful reinforcemen t signals for the subproblems is a v ery p o w erful tec hnique for biasing learning. Most in teresting examples of rob otic reinforcemen t learning emplo y this tec hnique to some exten t (Connell & Mahadev an, \u0001\t\t\u0003). re\ufffdexes: One thing that k eeps agen ts that kno w nothing from learning an ything is that they ha v e a hard time ev en \ufffdnding the in teresting parts of the space; they w ander \u0002\u0007\u0005 ",
    "References": "References Ac kley , D. H., & Littman, M. L. (\u0001\t\t0). Generalization and scaling in reinforcemen t learning. In T ouretzky , D. S. (Ed.), A dvanc es in Neur al Information Pr o c essing Systems \u0002, pp. \u0005\u00050{\u0005\u0005\u0007 San Mateo, CA. Morgan Kaufmann. Albus, J. S. (\u0001\t\u0007\u0005). A new approac h to manipulator con trol: Cereb ellar mo del articulation con troller (cmac). Journal of Dynamic Systems, Me asur ement and Contr ol, \t\u0007, \u0002\u00020{ \u0002\u0002\u0007. Albus, J. S. (\u0001\t\b\u0001). Br ains, Behavior, and R ob otics. BYTE Bo oks, Subsidiary of McGra wHill, P eterb orough, New Hampshire. Anderson, C. W. (\u0001\t\b\u0006). L e arning and Pr oblem Solving with Multilayer Conne ctionist Systems. Ph.D. thesis, Univ ersit y of Massac h usetts, Amherst, MA. Ashar, R. R. (\u0001\t\t\u0004). Hierarc hical learning in sto c hastic domains. Master's thesis, Bro wn Univ ersit y , Pro vidence, Rho de Island. Baird, L. (\u0001\t\t\u0005). Residual algorithms: Reinforcemen t learning with function appro ximation. In Prieditis, A., & Russell, S. (Eds.), Pr o c e e dings of the Twelfth International Confer enc e on Machine L e arning, pp. \u00030{\u0003\u0007 San F rancisco, CA. Morgan Kaufmann. Baird, L. C., & Klopf, A. H. (\u0001\t\t\u0003). Reinforcemen t learning with high-dimensional, contin uous actions. T ec h. rep. WL-TR-\t\u0003-\u0001\u0001\u0004\u0007, W righ t-P atterson Air F orce Base Ohio: Wrigh t Lab oratory . \u0002\u0007\u0006 Berry , D. A., & F ristedt, B. (\u0001\t\b\u0005). Bandit Pr oblems: Se quential A l lo c ation of Exp eriments. Chapman and Hall, London, UK. Bertsek as, D. P . (\u0001\t\b\u0007). Dynamic Pr o gr amming: Deterministic and Sto chastic Mo dels. Pren tice-Hall, Englew o o d Cli\ufffds, NJ. Bertsek as, D. P . (\u0001\t\t\u0005). Dynamic Pr o gr amming and Optimal Contr ol. A thena Scien ti\ufffdc, Belmon t, Massac h usetts. V olumes \u0001 and \u0002. Bertsek as, D. P ., & Casta ~ non, D. A. (\u0001\t\b\t). Adaptiv e aggregation for in\ufffdnite horizon dynamic programming. IEEE T r ansactions on A utomatic Contr ol, \u0003\u0004 (\u0006), \u0005\b\t{\u0005\t\b. Bertsek as, D. P ., & Tsitsiklis, J. N. (\u0001\t\b\t). Par al lel and Distribute d Computation: Numeric al Metho ds. Pren tice-Hall, Englew o o d Cli\ufffds, NJ. Bo x, G. E. P ., & Drap er, N. R. (\u0001\t\b\u0007). Empiric al Mo del-Building and R esp onse Surfac es. Wiley . Bo y an, J. A., & Mo ore, A. W. (\u0001\t\t\u0005). Generalization in reinforcemen t learning: Safely appro ximating the v alue function. In T esauro, G., T ouretzky , D. S., & Leen, T. K. (Eds.), A dvanc es in Neur al Information Pr o c essing Systems \u0007 Cam bridge, MA. The MIT Press. Burghes, D., & Graham, A. (\u0001\t\b0). Intr o duction to Contr ol The ory including Optimal Contr ol. Ellis Horw o o d. Cassandra, A. R., Kaelbling, L. P ., & Littman, M. L. (\u0001\t\t\u0004). Acting optimally in partially observ able sto c hastic domains. In Pr o c e e dings of the Twelfth National Confer enc e on A rti\ufffdcial Intel ligenc e Seattle, W A. Chapman, D., & Kaelbling, L. P . (\u0001\t\t\u0001). Input generalization in dela y ed reinforcemen t learning: An algorithm and p erformance comparisons. In Pr o c e e dings of the International Joint Confer enc e on A rti\ufffdcial Intel ligenc e Sydney , Australia. Chrisman, L. (\u0001\t\t\u0002). Reinforcemen t learning with p erceptual aliasing: The p erceptual distinctions approac h. In Pr o c e e dings of the T enth National Confer enc e on A rti\ufffdcial Intel ligenc e, pp. \u0001\b\u0003{\u0001\b\b San Jose, CA. AAAI Press. \u0002\u0007\u0007 Cli\ufffd, D., & Ross, S. (\u0001\t\t\u0004). Adding temp orary memory to ZCS. A daptive Behavior, \u0003 (\u0002), \u00010\u0001{\u0001\u00050. Condon, A. (\u0001\t\t\u0002). The complexit y of sto c hastic games. Information and Computation, \t\u0006 (\u0002), \u00020\u0003{\u0002\u0002\u0004. Connell, J., & Mahadev an, S. (\u0001\t\t\u0003). Rapid task learning for real rob ots. In R ob ot L e arning. Klu w er Academic Publishers. Crites, R. H., & Barto, A. G. (\u0001\t\t\u0006). Impro ving elev ator p erformance using reinforcemen t learning. In T ouretzky , D., Mozer, M., & Hasselmo, M. (Eds.), Neur al Information Pr o c essing Systems \b. Da y an, P . (\u0001\t\t\u0002). The con v ergence of TD(\ufffd) for general \ufffd. Machine L e arning, \b (\u0003), \u0003\u0004\u0001{ \u0003\u0006\u0002. Da y an, P ., & Hin ton, G. E. (\u0001\t\t\u0003). F eudal reinforcemen t learning. In Hanson, S. J., Co w an, J. D., & Giles, C. L. (Eds.), A dvanc es in Neur al Information Pr o c essing Systems \u0005 San Mateo, CA. Morgan Kaufmann. Da y an, P ., & Sejno wski, T. J. (\u0001\t\t\u0004). TD(\ufffd) con v erges with probabilit y \u0001. Machine L e arning, \u0001\u0004 (\u0003). Dean, T., Kaelbling, L. P ., Kirman, J., & Nic holson, A. (\u0001\t\t\u0003). Planning with deadlines in sto c hastic domains. In Pr o c e e dings of the Eleventh National Confer enc e on A rti\ufffdcial Intel ligenc e W ashington, DC. D'Ep enoux, F. (\u0001\t\u0006\u0003). A probabilistic pro duction and in v en tory problem. Management Scienc e, \u00010, \t\b{\u00010\b. Derman, C. (\u0001\t\u00070). Finite State Markovian De cision Pr o c esses. Academic Press, New Y ork. Dorigo, M., & Bersini, H. (\u0001\t\t\u0004). A comparison of q-learning and classi\ufffder systems. In F r om A nimals to A nimats: Pr o c e e dings of the Thir d International Confer enc e on the Simulation of A daptive Behavior Brigh ton, UK. Dorigo, M., & Colom b etti, M. (\u0001\t\t\u0004). Rob ot shaping: Dev eloping autonomous agen ts through learning. A rti\ufffdcial Intel ligenc e, \u0007\u0001 (\u0002), \u0003\u0002\u0001{\u0003\u00070. \u0002\u0007\b ",
    "Preliminary": "Preliminary results. In Pr o c e e dings of the T enth International Confer enc e on Machine L e arning Amherst, MA. Morgan Kaufmann. Kaelbling, L. P . (\u0001\t\t\u0003b). L e arning in Emb e dde d Systems. The MIT Press, Cam bridge, MA. Kaelbling, L. P . (\u0001\t\t\u0004a). Asso ciativ e reinforcemen t learning: A generate and test algorithm. Machine L e arning, \u0001\u0005 (\u0003). \u0002\u0007\t Lee, C. C. (\u0001\t\t\u0001). A self learning rule-based con troller emplo ying appro ximate reasoning and neural net concepts. International Journal of Intel ligent Systems, \u0006 (\u0001), \u0007\u0001{\t\u0003. Lin, L.-J. (\u0001\t\t\u0001). Programming rob ots using reinforcemen t learning and teac hing. In Pr o c e e dings of the Ninth National Confer enc e on A rti\ufffdcial Intel ligenc e. Lin, L.-J. (\u0001\t\t\u0003a). Hierac hical learning of rob ot skills b y reinforcemen t. In Pr o c e e dings of the International Confer enc e on Neur al Networks. Lin, L.-J. (\u0001\t\t\u0003b). R einfor c ement L e arning for R ob ots Using Neur al Networks. Ph.D. thesis, Carnegie Mellon Univ ersit y , Pittsburgh, P A. Lin, L.-J., & Mitc hell, T. M. (\u0001\t\t\u0002). Memory approac hes to reinforcemen t learning in nonMark o vian domains. T ec h. rep. CMU-CS-\t\u0002-\u0001\u0003\b, Carnegie Mellon Univ ersit y , Sc ho ol of Computer Science. Littman, M. L. (\u0001\t\t\u0004a). Mark o v games as a framew ork for m ulti-agen t reinforcemen t learning. In Pr o c e e dings of the Eleventh International Confer enc e on Machine L e arning, pp. \u0001\u0005\u0007{\u0001\u0006\u0003 San F rancisco, CA. Morgan Kaufmann. Littman, M. L. (\u0001\t\t\u0004b). Memoryless p olicies: Theoretical limitations and practical results. In Cli\ufffd, D., Husbands, P ., Mey er, J.-A., & Wilson, S. W. (Eds.), F r om A nimals to A nimats \u0003: Pr o c e e dings of the Thir d International Confer enc e on Simulation of A daptive Behavior Cam bridge, MA. The MIT Press. Littman, M. L., Cassandra, A., & Kaelbling, L. P . (\u0001\t\t\u0005a). Learning p olicies for partially observ able en vironmen ts: Scaling up. In Prieditis, A., & Russell, S. (Eds.), Pr o c e e dings of the Twelfth International Confer enc e on Machine L e arning, pp. \u0003\u0006\u0002{\u0003\u00070 San F rancisco, CA. Morgan Kaufmann. Littman, M. L., Dean, T. L., & Kaelbling, L. P . (\u0001\t\t\u0005b). On the complexit y of solving Mark o v decision problems. In Pr o c e e dings of the Eleventh A nnual Confer enc e on Unc ertainty in A rti\ufffdcial Intel ligenc e (UAI{\t\u0005) Mon treal, Qu \ufffd eb ec, Canada. Lo v ejo y , W. S. (\u0001\t\t\u0001). A surv ey of algorithmic metho ds for partially observ able Mark o v decision pro cesses. A nnals of Op er ations R ese ar ch, \u0002\b, \u0004\u0007{\u0006\u0006. \u0002\b0 Mahadev an, S., & Connell, J. (\u0001\t\t\u0001a). Automatic programming of b eha vior-based rob ots using reinforcemen t learning. In Pr o c e e dings of the Ninth National Confer enc e on A rti\ufffdcial Intel ligenc e Anaheim, CA. Mahadev an, S., & Connell, J. (\u0001\t\t\u0001b). Scaling reinforcemen t learning to rob otics b y exploiting the subsumption arc hitecture. In Pr o c e e dings of the Eighth International Workshop on Machine L e arning, pp. \u0003\u0002\b{\u0003\u0003\u0002. Mataric, M. J. (\u0001\t\t\u0004). Rew ard functions for accelerated learning. In Cohen, W. W., & Hirsh, H. (Eds.), Pr o c e e dings of the Eleventh International Confer enc e on Machine L e arning. Morgan Kaufmann. McCallum, A. K. (\u0001\t\t\u0005). R einfor c ement L e arning with Sele ctive Per c eption and Hidden State. Ph.D. thesis, Departmen t of Computer Science, Univ ersit y of Ro c hester. McCallum, R. A. (\u0001\t\t\u0003). Ov ercoming incomplete p erception with utile distinction memory . In Pr o c e e dings of the T enth International Confer enc e on Machine L e arning, pp. \u0001\t0{ \u0001\t\u0006 Amherst, Massac h usetts. Morgan Kaufmann. McCallum, R. A. (\u0001\t\t\u0005). Instance-based utile distinctions for reinforcemen t learning with hidden state. In Pr o c e e dings of the Twelfth International Confer enc e Machine L e arning, pp. \u0003\b\u0007{\u0003\t\u0005 San F rancisco, CA. Morgan Kaufmann. Meeden, L., McGra w, G., & Blank, D. (\u0001\t\t\u0003). Emergen t con trol and planning in an autonomous v ehicle. In T ouretsky , D. (Ed.), Pr o c e e dings of the Fifte enth A nnual Me eting of the Co gnitive Scienc e So ciety, pp. \u0007\u0003\u0005{\u0007\u00040. La w erence Erlbaum Asso ciates, Hillsdale, NJ. Millan, J. d. R. (\u0001\t\t\u0006). Rapid, safe, and incremen tal learning of na vigation strategies. IEEE T r ansactions on Systems, Man, and Cyb ernetics, \u0002\u0006 (\u0003). Monahan, G. E. (\u0001\t\b\u0002). A surv ey of partially observ able Mark o v decision pro cesses: Theory , mo dels, and algorithms. Management Scienc e, \u0002\b, \u0001{\u0001\u0006. Mo ore, A. W. (\u0001\t\t\u0001). V ariable resolution dynamic programming: E\ufffdcien tly learning action maps in m ultiv ariate real-v alued spaces. In Pr o c. Eighth International Machine L e arning Workshop. \u0002\b\u0001 Mo ore, A. W., A tk eson, C. G., & Sc haal, S. (\u0001\t\t\u0005). Memory-based learning for con trol. T ec h. rep. CMU-RI-TR-\t\u0005-\u0001\b, CMU Rob otics Institute. Narendra, K., & Thathac har, M. A. L. (\u0001\t\b\t). L e arning A utomata: An Intr o duction. Pren tice-Hall, Englew o o d Cli\ufffds, NJ. Narendra, K. S., & Thathac har, M. A. L. (\u0001\t\u0007\u0004). Learning automata|a surv ey . IEEE T r ansactions on Systems, Man, and Cyb ernetics, \u0004 (\u0004), \u0003\u0002\u0003{\u0003\u0003\u0004. P eng, J., & Williams, R. J. (\u0001\t\t\u0003). E\ufffdcien t learning and planning within the Dyna framew ork. A daptive Behavior, \u0001 (\u0004), \u0004\u0003\u0007{\u0004\u0005\u0004. P eng, J., & Williams, R. J. (\u0001\t\t\u0004). Incremen tal m ulti-step Q-learning. In Pr o c e e dings of the Eleventh International Confer enc e on Machine L e arning, pp. \u0002\u0002\u0006{\u0002\u0003\u0002 San F rancisco, CA. Morgan Kaufmann. P omerleau, D. A. (\u0001\t\t\u0003). Neur al network p er c eption for mobile r ob ot guidanc e. Klu w er Academic Publishing. Puterman, M. L. (\u0001\t\t\u0004). Markov De cision Pr o c esses|Discr ete Sto chastic Dynamic Pr ogr amming. John Wiley & Sons, Inc., New Y ork, NY. Puterman, M. L., & Shin, M. C. (\u0001\t\u0007\b). Mo di\ufffded p olicy iteration algorithms for discoun ted Mark o v decision pro cesses. Management Scienc e, \u0002\u0004, \u0001\u0001\u0002\u0007{\u0001\u0001\u0003\u0007. Ring, M. B. (\u0001\t\t\u0004). Continual L e arning in R einfor c ement Envir onments. Ph.D. thesis, Univ ersit y of T exas at Austin, Austin, T exas. R \ufffd ude, U. (\u0001\t\t\u0003). Mathematic al and c omputational te chniques for multilevel adaptive metho ds. So ciet y for Industrial and Applied Mathematics, Philadelphi a, P ennsylv ania. Rumelhart, D. E., & McClelland, J. L. (Eds.). (\u0001\t\b\u0006). Par al lel Distribute d Pr o c essing: Explor ations in the micr ostructur es of c o gnition. V olume \u0001: Foundations. The MIT Press, Cam bridge, MA. Rummery , G. A., & Niranjan, M. (\u0001\t\t\u0004). On-line Q-learning using connectionist systems. T ec h. rep. CUED/F-INFENG/TR\u0001\u0006\u0006, Cam bridge Univ ersit y. \u0002\b\u0002 Journal of R ese ar ch and Development, \u0003, \u0002\u0001\u0001{\u0002\u0002\t. Reprin ted in E. A. F eigen baum and J. F eldman, editors, Computers and Thought, McGra w-Hill, New Y ork \u0001\t\u0006\u0003. Sc haal, S., & A tk eson, C. (\u0001\t\t\u0004). Rob ot juggling: An implemen tation of memory-based learning. Contr ol Systems Magazine, \u0001\u0004. Sc hmidh ub er, J. (\u0001\t\t\u0006). A general metho d for m ulti-agen t learning and incremen tal selfimpro v emen t in unrestricted en vironmen ts. In Y ao, X. (Ed.), Evolutionary Computation: The ory and Applic ations. Scien ti\ufffdc Publ. Co., Singap ore. Sc hmidh ub er, J. H. (\u0001\t\t\u0001a). Curious mo del-buildi ng con trol systems. In Pr o c. International Joint Confer enc e on Neur al Networks, Singap or e, V ol. \u0002, pp. \u0001\u0004\u0005\b{\u0001\u0004\u0006\u0003. IEEE. Sc hmidh ub er, J. H. (\u0001\t\t\u0001b). Reinforcemen t learning in Mark o vian and non-Mark o vian en vironmen ts. In Lippman, D. S., Mo o dy , J. E., & T ouretzky , D. S. (Eds.), A dvanc es in Neur al Information Pr o c essing Systems \u0003, pp. \u000500{\u00050\u0006 San Mateo, CA. Morgan Kaufmann. Sc hraudolph, N. N., Da y an, P ., & Sejno wski, T. J. (\u0001\t\t\u0004). T emp oral di\ufffderence learning of p osition ev aluation in the game of Go. In Co w an, J. D., T esauro, G., & Alsp ector, J. (Eds.), A dvanc es in Neur al Information Pr o c essing Systems \u0006, pp. \b\u0001\u0007{\b\u0002\u0004 San Mateo, CA. Morgan Kaufmann. Sc hrijv er, A. (\u0001\t\b\u0006). The ory of Line ar and Inte ger Pr o gr amming. Wiley-In terscience, New Y ork, NY. Sc h w artz, A. (\u0001\t\t\u0003). A reinforcemen t learning metho d for maximizing undiscoun ted rew ards. In Pr o c e e dings of the T enth International Confer enc e on Machine L e arning, pp. \u0002\t\b{\u00030\u0005 Amherst, Massac h usetts. Morgan Kaufmann. Singh, S. P ., Barto, A. G., Grup en, R., & Connolly , C. (\u0001\t\t\u0004). Robust reinforcemen t learning in motion planning. In Co w an, J. D., T esauro, G., & Alsp ector, J. (Eds.), A dvanc es in Neur al Information Pr o c essing Systems \u0006, pp. \u0006\u0005\u0005{\u0006\u0006\u0002 San Mateo, CA. Morgan Kaufmann. Singh, S. P ., & Sutton, R. S. (\u0001\t\t\u0006). Reinforcemen t learning with replacing eligibili t y traces. Machine L e arning, \u0002\u0002 (\u0001). \u0002\b\u0003 Stengel, R. F. (\u0001\t\b\u0006). Sto chastic Optimal Contr ol. John Wiley and Sons. Sutton, R. S. (\u0001\t\t\u0006). Generalization in Reinforcemen t Learning: Successful Examples Using Sparse Coarse Co ding. In T ouretzky , D., Mozer, M., & Hasselmo, M. (Eds.), Neur al Information Pr o c essing Systems \b. Sutton, R. S. (\u0001\t\b\u0004). T emp or al Cr e dit Assignment in R einfor c ement L e arning. Ph.D. thesis, Univ ersit y of Massac h usetts, Amherst, MA. Sutton, R. S. (\u0001\t\b\b). Learning to predict b y the metho d of temp oral di\ufffderences. Machine L e arning, \u0003 (\u0001), \t{\u0004\u0004. Sutton, R. S. (\u0001\t\t0). In tegrated arc hitectures for learning, planning, and reacting based on appro ximating dynamic programming. In Pr o c e e dings of the Seventh International Confer enc e on Machine L e arning Austin, TX. Morgan Kaufmann. Sutton, R. S. (\u0001\t\t\u0001). Planning b y incremen tal dynamic programming. In Pr o c e e dings of the Eighth International Workshop on Machine L e arning, pp. \u0003\u0005\u0003{\u0003\u0005\u0007. Morgan Kaufmann. T esauro, G. (\u0001\t\t\u0002). Practical issues in temp oral di\ufffderence learning. Machine L e arning, \b, \u0002\u0005\u0007{\u0002\u0007\u0007. T esauro, G. (\u0001\t\t\u0004). TD-Gammon, a self-teac hing bac kgammon program, ac hiev es masterlev el pla y . Neur al Computation, \u0006 (\u0002), \u0002\u0001\u0005{\u0002\u0001\t. T esauro, G. (\u0001\t\t\u0005). T emp oral di\ufffderence learning and TD-Gammon. Communic ations of the A CM, \u0003\b (\u0003), \u0005\b{\u0006\u0007. Tham, C.-K., & Prager, R. W. (\u0001\t\t\u0004). A mo dular q-learning arc hitecture for manipulator task decomp osition. In Pr o c e e dings of the Eleventh International Confer enc e on Machine L e arning San F rancisco, CA. Morgan Kaufmann. Thrun, S. (\u0001\t\t\u0005). Learning to pla y the game of c hess. In T esauro, G., T ouretzky , D. S., & Leen, T. K. (Eds.), A dvanc es in Neur al Information Pr o c essing Systems \u0007 Cam bridge, MA. The MIT Press. \u0002\b\u0004 Tsitsiklis, J. N., & V an Ro y , B. (\u0001\t\t\u0006). F eature-based metho ds for large scale dynamic programming. Machine L e arning, \u0002\u0002 (\u0001). V alian t, L. G. (\u0001\t\b\u0004). A theory of the learnable. Communic ations of the A CM, \u0002\u0007 (\u0001\u0001), \u0001\u0001\u0003\u0004{\u0001\u0001\u0004\u0002. W atkins, C. J. C. H. (\u0001\t\b\t). L e arning fr om Delaye d R ewar ds. Ph.D. thesis, King's College, Cam bridge, UK. W atkins, C. J. C. H., & Da y an, P . (\u0001\t\t\u0002). Q-learning. Machine L e arning, \b (\u0003), \u0002\u0007\t{\u0002\t\u0002. Whitehead, S. D. (\u0001\t\t\u0001). Complexit y and co op eration in Q-learning. In Pr o c e e dings of the Eighth International Workshop on Machine L e arning Ev anston, IL. Morgan Kaufmann. Williams, R. J. (\u0001\t\b\u0007). A class of gradien t-estimating algorithms for reinforcemen t learning in neural net w orks. In Pr o c e e dings of the IEEE First International Confer enc e on Neur al Networks San Diego, CA. Williams, R. J. (\u0001\t\t\u0002). Simple statistical gradien t-follo wing algorithms for connectionist reinforcemen t learning. Machine L e arning, \b (\u0003), \u0002\u0002\t{\u0002\u0005\u0006. Williams, R. J., & Baird, I I I, L. C. (\u0001\t\t\u0003a). Analysis of some incremen tal v arian ts of p olicy iteration: First steps to w ard understanding actor-critic learning systems. T ec h. rep. NU-CCS-\t\u0003-\u0001\u0001, Northeastern Univ ersit y , College of Computer Science, Boston, MA. Williams, R. J., & Baird, I I I, L. C. (\u0001\t\t\u0003b). Tigh t p erformance b ounds on greedy p olicies based on imp erfect v alue functions. T ec h. rep. NU-CCS-\t\u0003-\u0001\u0004, Northeastern Univ ersit y , College of Computer Science, Boston, MA. Wilson, S. (\u0001\t\t\u0005). Classi\ufffder \ufffdtness based on accuracy . Evolutionary Computation, \u0003 (\u0002), \u0001\u0004\u0007{\u0001\u0007\u0003. Zhang, W., & Dietteric h, T. G. (\u0001\t\t\u0005). A reinforcemen t learning approac h to job-shop sc heduling. In Pr o c e e dings of the International Joint Confer enc e on A rti\ufffdcial Intellienc e. \u0002\b\u0005 ",
    "title": "",
    "paper_info": " \n\u03c43\n\u03c41\n\u03c42\n\u03b1\n\u03b8\nx, y\np\n\u03c41\n\u03c42\nFigure\n\u0001\u0001:\nSc\nhaal\nand\nA\ntk\neson's\ndevil-stic\nking\nrob\not.\nThe\ntap\nered\nstic\nk\nis\nhit\nalternately\nb\ny\neac\nh\nof\nthe\nt\nw\no\nhand\nstic\nks.\nThe\ntask\nis\nto\nk\neep\nthe\ndevil\nstic\nk\nfrom\nfalling\nfor\nas\nman\ny\nhits\nas\np\nossible.\nThe\nrob\not\nhas\nthree\nmotors\nindicated\nb\ny\ntorque\nv\nectors\n\ufffd\n\u0001\n;\n\ufffd\n\u0002\n;\n\ufffd\n\u0003\n.\nAlthough\nexp\nerimen\nts\nwith\nother\ngames\nha\nv\ne\nin\nsome\ncases\npro\nduced\nin\nteresting\nlearning\nb\neha\nvior,\nno\nsuccess\nclose\nto\nthat\nof\nTD-Gammon\nhas\nb\neen\nrep\neated.\nOther\ngames\nthat\nha\nv\ne\nb\neen\nstudied\ninclude\nGo\n(Sc\nhraudolph,\nDa\ny\nan,\n&\nSejno\nwski,\n\u0001\t\t\u0004)\nand\nChess\n(Thrun,\n\u0001\t\t\u0005).\nIt\nis\nstill\nan\nop\nen\nquestion\nas\nto\nif\nand\nho\nw\nthe\nsuccess\nof\nTD-Gammon\ncan\nb\ne\nrep\neated\nin\nother\ndomains.\n\b.\u0002\nRob\notics\nand\nCon\ntrol\nIn\nrecen\nt\ny\nears\nthere\nha\nv\ne\nb\neen\nman\ny\nrob\notics\nand\ncon\ntrol\napplications\nthat\nha\nv\ne\nused\nreinforcemen\nt\nlearning.\nHere\nw\ne\nwill\nconcen\ntrate\non\nthe\nfollo\nwing\nfour\nexamples,\nalthough\nman\ny\nother\nin\nteresting\nongoing\nrob\notics\nin\nv\nestigations\nare\nunderw\na\ny\n.\n\u0001.\nSc\nhaal\nand\nA\ntk\neson\n(\u0001\t\t\u0004)\nconstructed\na\nt\nw\no-armed\nrob\not,\nsho\nwn\nin\nFigure\n\u0001\u0001,\nthat\nlearns\nto\njuggle\na\ndevice\nkno\nwn\nas\na\ndevil-stic\nk.\nThis\nis\na\ncomplex\nnon-linear\ncon\ntrol\ntask\nin\nv\nolving\na\nsix-dimensional\nstate\nspace\nand\nless\nthan\n\u000200\nmsecs\np\ner\ncon\ntrol\ndeci-\nsion.\nAfter\nab\nout\n\u00040\ninitial\nattempts\nthe\nrob\not\nlearns\nto\nk\neep\njuggling\nfor\nh\nundreds\nof\nhits.\nA\nt\nypical\nh\numan\nlearning\nthe\ntask\nrequires\nan\norder\nof\nmagnitude\nmore\npractice\nto\nac\nhiev\ne\npro\ufffdciency\nat\nmere\ntens\nof\nhits.\nThe\njuggling\nrob\not\nlearned\na\nw\norld\nmo\ndel\nfrom\nexp\nerience,\nwhic\nh\nw\nas\ngeneralized\nto\nun\nvisited\nstates\nb\ny\na\nfunction\nappro\nximation\nsc\nheme\nkno\nwn\nas\nlo\ncally\nw\neigh\nted\nregression\n(Clev\neland\n&\nDelvin,\n\u0001\t\b\b;\nMo\nore\n&\nA\ntk\neson,\n\u0001\t\t\u0002).\nBet\nw\neen\neac\nh\ntrial,\na\nform\nof\ndynamic\nprogramming\nsp\neci\ufffdc\nto\nlinear\ncon\ntrol\np\nolicies\nand\nlo\ncally\nlinear\ntransitions\nw\nas\nused\nto\nimpro\nv\ne\nthe\np\nolicy\n.\nThe\nform\nof\ndynamic\nprogramming\nis\nkno\nwn\nas\nlinear-quadratic-regulator\ndesign\n(Sage\n&\nWhite,\n\u0001\t\u0007\u0007).\n\u0002\u0007\u0002\n",
    "GPTsummary": "\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to demonstrate the potential for a general-purpose algorithm that can solve a range of reinforcement learning tasks. \n\n- (2): Innovation Point: The use of deep neural networks to approximate the action-value function and experience replay to decorrelate updates is a significant innovation. Performance: The methods in this paper achieve state-of-the-art performance on a range of Atari games and outperform previous methods using linear function approximation and sample-based updates. Workload: The workload of implementing and training the deep Q-network algorithm on raw pixel inputs may be considered high compared to some other reinforcement learning methods. However, this is offset by the algorithm's ability to generalize across different Atari games.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to demonstrate the potential for a general-purpose algorithm that can solve a range of reinforcement learning tasks. \n\n- (2): Innovation Point: The use of deep neural networks to approximate the action-value function and experience replay to decorrelate updates is a significant innovation. Performance: The methods in this paper achieve state-of-the-art performance on a range of Atari games and outperform previous methods using linear function approximation and sample-based updates. Workload: The workload of implementing and training the deep Q-network algorithm on raw pixel inputs may be considered high compared to some other reinforcement learning methods. However, this is offset by the algorithm's ability to generalize across different Atari games.\n\n\n"
}