{
    "Abstract": "Abstract There are t w o distinct approac hes to solving reinforcemen t learning problems, namely , searc hing in v alue function space and searc hing in p olicy space. T emp oral di\ufffderence metho ds and ev olutionary algorithms are w ell-kno wn examples of these approac hes. Kaelbling, Littman and Mo ore recen tly pro vided an informativ e surv ey of temp oral di\ufffderence metho ds. This article fo cuses on the application of ev olutionary algorithms to the reinforcemen t learning problem, emphasizing alternativ e p olicy represen tations, credit assignmen t metho ds, and problem-sp eci\ufffdc genetic op erators. Strengths and w eaknesses of the ev olutionary approac h to reinforcemen t learning are presen ted, along with a surv ey of represen tativ e applications. \u0001. In tro duction Kaelbling, Littman, and Mo ore (\u0001\t\t\u0006) and more recen tly Sutton and Barto (\u0001\t\t\b) provide informativ e surv eys of the \ufffdeld of reinforcemen t learning (RL). They c haracterize t w o classes of metho ds for reinforcemen t learning: metho ds that searc h the space of v alue functions and metho ds that searc h the space of p olicies. The former class is exempli\ufffded b y the temp oral di\ufffderence (TD) metho d and the latter b y the ev olutionary algorithm (EA) approac h. Kaelbling et al. fo cus en tirely on the \ufffdrst set of metho ds and they pro vide an excellen t accoun t of the state of the art in TD learning. This article is in tended to round out the picture b y addressing ev olutionary metho ds for solving the reinforcemen t learning problem. As Kaelbling et al. clearly illustrate, reinforcemen t learning presen ts a c hallenging arra y of di\ufffdculties in the pro cess of scaling up to realistic tasks, including problems asso ciated with v ery large state spaces, partially observ able states, rarely o ccurring states, and nonstationary en vironmen ts. A t this p oin t, whic h approac h is b est remains an op en question, so it is sensible to pursue parallel lines of researc h on alternativ e metho ds. While it is b ey ond the scop e of this article to address whether it is b etter in general to searc h v alue function space or p olicy space, w e do hop e to highligh t some of the strengths of the ev olutionary approac h to the reinforcemen t learning problem. The reader is advised not to view this c \ufffd\u0001\t\t\t AI Access F oundation and Morgan Kaufmann Publishers. All righ ts reserv ed. of the EA approac h. Section \u00010 brie\ufffdy surv eys some successful applications of EA systems on c hallenging RL tasks. The \ufffdnal section summarizes our presen tation and p oin ts out directions for further researc h. \u0002. Reinforcemen t Learning All reinforcemen t learning metho ds share the same goal: to solv e se quential de cision tasks through trial and error in teractions with the en vironmen t (Barto, Sutton, & W atkins, \u0001\t\t0; Grefenstette, Ramsey , & Sc h ultz, \u0001\t\t0). In a sequen tial decision task, an agen t in teracts with a dynamic system b y selecting actions that a\ufffdect state transitions to optimize some rew ard function. More formally , at an y giv en time step t, an agen t p erceiv es its state s t and selects an action a t . The system resp onds b y giving the agen t some (p ossibly zero) n umerical r ewar d r (s t ) and c hanging in to state s t+\u0001 = \ufffd (s t ; a t ). The state transition ma y b e determined solely b y the curren t state and the agen t's action or ma y also in v olv e sto c hastic pro cesses. The agen t's goal is to learn a p olicy, \ufffd : S ! A, whic h maps states to actions. The optimal p olicy, \ufffd \ufffd , can b e de\ufffdned in man y w a ys, but is t ypically de\ufffdned as the p olicy that pro duces the greatest cum ulativ e rew ard o v er all states s: \ufffd \ufffd = argmax \ufffd V \ufffd (s); (\bs) (\u0001) where V \ufffd (s) is the cum ulativ e rew ard receiv ed from state s using p olicy \ufffd . There are also man y w a ys to compute V \ufffd (s). One approac h uses a discoun t rate \ufffd to discoun t rew ards o v er time. The sum is then computed o v er an in\ufffdnite horizon: V \ufffd (s t ) = \u0001 X i=0 \ufffd i r t+i (\u0002) where r t is the rew ard receiv ed at time step t. Alternativ ely , V \ufffd (s) could b e computed b y summing the rew ards o v er a \ufffdnite horizon h: V \ufffd (s t ) = h X i=0 r t+i (\u0003) The agen t's state descriptions are usually iden ti\ufffded with the v alues returned b y its sensors, whic h pro vide a description of b oth the agen t's curren t state and the state of the \u0002\u0004\u0002 mo del do es not exist. Consequen tly , an agen t in the RL paradigm m ust activ ely explore its en vironmen t in order to observ e the e\ufffdects of its actions. Unlik e planning, RL agen ts cannot normally undo state transitions. Of course, in some cases it ma y b e p ossible to build up an action mo del through exp erience (Sutton, \u0001\t\t0), enabling more planning as exp erience accum ulates. Ho w ev er, RL researc h fo cuses on the b eha vior of an agen t when it has insu\ufffdcien t kno wledge to p erform planning. Agen ts can also b e trained through sup ervised learning. In sup ervised learning, the agen t is presen ted with examples of state-action pairs, along with an indication that the action w as either correct or incorrect. The goal in sup ervised learning is to induce a general p olicy from the training examples. Th us, sup ervised learning requires an or acle that can supply correctly lab eled examples. In con trast, RL do es not require prior kno wledge of correct and incorrect decisions. RL can b e applied to situations in whic h rew ards are sparse; for example, rew ards ma y b e asso ciated only with certain states. In suc h cases, it ma y b e imp ossible to asso ciate a lab el of \\correct\" or \\incorrect\" on particular decisions without reference to the agen t's subsequen t decisions, making sup ervised learning infeasible. In summary , RL pro vides a \ufffdexible approac h to the design of in telligen t agen ts in situations for whic h b oth planning and sup ervised learning are impractical. RL can b e applied to problems for whic h signi\ufffdcan t domain kno wledge is either una v ailable or costly to obtain. F or example, a common RL task is rob ot con trol. Designers of autonomous rob ots often lac k su\ufffdcien t kno wledge of the in tended op erational en vironmen t to use either the planning or the sup ervised learning regime to design a con trol p olicy for the rob ot. In this case, the goal of RL w ould b e to enable the rob ot to generate e\ufffdectiv e decision p olicies as it explores its en vironmen t. Figure \u0001 sho ws a simple sequen tial decision task that will b e used as an example later in this pap er. The task of the agen t in this grid w orld is to mo v e from state to state b y selecting among t w o actions: righ t (R) or do wn (D ). The sensor of the agen t returns the iden tit y of the curren t state. The agen t alw a ys starts in state a\u0001 and receiv es the rew ard indicated up on visiting eac h state. The task con tin ues un til the agen t mo v es o\ufffd the grid w orld (e.g., b y taking action D from state a\u0005). The goal is to learn a p olicy that returns the highest cum ulativ e rew ards. F or example, a p olicy whic h results in the sequences of actions R; D ; R; D ; D; R ; R ; D starting from from state a\u0001 giv es the optimal score of \u0001\u0007. \u0002\u0004\u0003 -2 3 1 1 2 4 1 2 1 1 3 2 0 2 -1 1 2 0 1 1 -5 1 4 1 1 a b c d e 1 2 3 4 5 Figure \u0001: A simple grid-w orld sequen tial decision task. The agen t starts in state a\u0001 and receiv es the ro w and column of the curren t b o x as sensory input. The agen t mo v es from one b o x to another b y selecting b et w een t w o mo v es (righ t or do wn), and the agen t's score is increased b y the pa y o\ufffd indicated in eac h b o x. The goal is to \ufffdnd a p olicy that maximizes the cum ulativ e score. \u0002.\u0001 P olicy Space vs. V alue-F unction Space Giv en the reinforcemen t learning problem as describ ed in the previous section, w e no w address the main topic: ho w to \ufffdnd an optimal p olicy , \ufffd \ufffd . W e consider t w o main approac hes, one in v olv es searc h in p olicy sp ac e and the other in v olv es searc h in value function sp ac e. P olicy-space searc h metho ds main tain explicit represen tations of p olicies and mo dify them through a v ariet y of searc h op erators. Man y searc h metho ds ha v e b een considered, including dynamic programming, v alue iteration, sim ulated annealing, and ev olutionary algorithms. This pap er fo cuses on ev olutionary algorithms that ha v e b een sp ecialized for the reinforcemen t learning task. In con trast, v alue function metho ds do not main tain an explicit represen tation of a p olicy . Instead, they attempt learn the v alue function V \ufffd \ufffd , whic h returns the exp ected cum ulativ e rew ard for the optimal p olicy from an y state. The fo cus of researc h on v alue function approac hes to RL is to design algorithms that learn these v alue functions through exp erience. The most common approac h to learning v alue functions is the temp oral di\ufffderence (TD) metho d, whic h is describ ed in the next section. \u0003. T emp oral Di\ufffderence Algorithms for Reinforceme n t Learning As stated in the In tro duction, a comprehensiv e comparison of v alue function searc h and direct p olicy-space searc h is b ey ond the scop e of this pap er. Nev ertheless, it will b e useful to p oin t out k ey conceptual di\ufffderences b et w een t ypical v alue function metho ds and t ypical ev olutionary algorithms for searc hing p olicy space. The most common approac h for learning a v alue function V for RL problems is the temp oral di\ufffderence (TD) metho d (Sutton, \u0001\t\b\b). \u0002\u0004\u0004 the last prediction V (s n ) con taining the only non-zero rew ard from the en vironmen t. Ov er man y iterations of this sequence, the up date rule will adjust the v alues of eac h state so that they agree with their successors and ev en tually with the rew ard receiv ed in V (s n ). In other w ords, the single rew ard is propagated bac kw ards through the c hain of v alue predictions. The net result is an accurate v alue function that can b e used to predict the exp ected rew ard from an y state of the system. As men tioned earlier, the goal of TD metho ds is to learn the v alue function for the optimal p olicy , V \ufffd \ufffd . Giv en V \ufffd \ufffd , the optimal action, \ufffd (s), can b e computed using the follo wing equation: \ufffd (s) = argmax a V \ufffd \ufffd (\ufffd (s; a)) (\u0005) Of course, w e ha v e already stated that in RL the state transition function \ufffd (s; a) is unkno wn to the agen t. Without this kno wledge, w e ha v e no w a y of ev aluating (\u0005). An alternativ e v alue function that can b e used to compute \ufffd \ufffd (s) is called a Q-function, Q(s; a) (W atkins, \u0001\t\b\t; W atkins & Da y an, \u0001\t\t\u0002). The Q-function is a v alue function that represen ts the exp ected v alue of taking action a in state s and acting optimally thereafter: Q(s; a) = r (s) + V \ufffd (\ufffd (s; a)) (\u0006) where r (s) represen ts an y immediate rew ard receiv ed in state s. Giv en the Q-function, actions from the optimal p olicy can b e directly computed using the follo wing equation: \ufffd \ufffd (s) = argmax a Q(s; a) (\u0007) T able \u0001 sho ws the Q-function for the grid w orld problem of Figure \u0001. This table-based represen tation of the Q-function asso ciates cum ulativ e future pa y o\ufffds for eac h state-action pair in the system. (The letter-n um b er pairs at the top represen t the state giv en b y the ro w and column in Figure \u0001, and R and D represen t the actions right and down, resp ectiv ely .) The TD metho d adjusts the Q-v alues after eac h decision. When selecting the next action, the agen t considers the e\ufffdect of that action b y examining the exp ected v alue of the state transition caused b y the action. The Q-function is learned through the follo wing TD up date equation: Q(s t ; a t ) = Q(s t ; a t ) + \ufffd(max a t+\u0001 Q(s t+\u0001 ; a t+\u0001 ) \ufffd Q(s t ; a t ) + r (s t )) (\b) \u0002\u0004\u0005 the estimates will asymptotically con v erge to the correct v alues. A reinforcemen t learning system can th us use the Q v alues to select the optimal action in an y state. Because Qlearning is the most widely kno wn implemen tation of temp oral di\ufffderence learning, w e will use it in our qualitativ e comparisons with ev olutionary approac hes in later sections. \u0004. Ev olutionary Algorithms for Reinforcemen t Learning (EARL) The p olicy-space approac h to RL searc hes for p olicies that optimize an appropriate ob jectiv e function. While man y searc h algorithms migh t b e used, this surv ey fo cuses on ev olutionary algorithms. W e b egin with a brief o v erview of a simple EA for RL, follo w ed b y a detailed discussion of features that c haracterize the general class of EAs for RL. \u0004.\u0001 Design Considerations for Ev olutionary Algorithms Ev olutionary algorithms (EAs) are global searc h tec hniques deriv ed from Darwin's theory of ev olution b y natural selection. An EA iterativ ely up dates a p opulation of p oten tial solutions, whic h are often enco ded in structures called chr omosomes. During eac h iteration, called a gener ation, the EA ev aluates solutions and generates o\ufffdspring based on the \ufffdtness of eac h solution in the task en vironmen t. Substructures, or genes, of the solutions are then mo di\ufffded through genetic op erators suc h as m utation and recom bination. The idea is that structures that are asso ciated with go o d solutions can b e m utated or com bined to form ev en b etter solutions in subsequen t generations. The canonical ev olutionary algorithm is sho wn in Figure \u0002. There ha v e b een a wide v ariet y of EAs dev elop ed, including genetic algorithms (Holland, \u0001\t\u0007\u0005; Goldb erg, \u0001\t\b\t), ev olutionary programming (F ogel, Ow ens, & W alsh, \u0001\t\u0006\u0006), genetic programming (Koza, \u0001\t\t\u0002), and ev olutionary strategies (Rec hen b erg, \u0001\t\u0006\u0004). EAs are general purp ose searc h metho ds and ha v e b een applied in a v ariet y of domains including n umerical function optimization, com binatorial optimization, adaptiv e con trol, adaptiv e testing, and mac hine learning. One reason for the widespread success of EAs is that there are relativ ely few requiremen ts for their application, namely , \u0001. An appropriate mapping b et w een the searc h space and the space of c hromosomes, and \u0002. An appropriate \ufffdtness function. \u0002\u0004\u0006 alter structures in P(t); evaluate structures in P(t); end end. Figure \u0002: Pseudo-co de Ev olutionary Algorithm. F or example, in the case of parameter optimization, it is common to represen t the list of parameters as either a v ector of real n um b ers or a bit string that enco des the parameters. With either of these represen tations, the \\standard\" genetic op erators of m utation and cut-and-splice crosso v er can b e applied in a straigh tforw ard manner to pro duce the genetic v ariations required (see Figure \u0003). The user m ust still decide on a (rather large) n um b er of con trol parameters for the EA, including p opulation size, m utation rates, recom bination rates, paren t selection rules, but there is an extensiv e literature of studies whic h suggest that EAs are relativ ely robust o v er a wide range of con trol parameter settings (Grefenstette, \u0001\t\b\u0006; Sc ha\ufffder, Caruana, Eshelman, & Das, \u0001\t\b\t). Th us, for man y problems, EAs can b e applied in a relativ ely straigh tforw ard manner. Ho w ev er, for man y other applications, EAs need to b e sp ecialized for the problem domain (Grefenstette, \u0001\t\b\u0007). The most critical design c hoice facing the user is the represen tation, that is, the mapping b et w een the searc h space of kno wledge structures (or, the phenotyp e space) and the space of c hromosomes (the genotyp e space). Man y studies ha v e sho wn that the e\ufffdectiv eness of EAs is sensitiv e to the c hoice of represen tations. It is not su\ufffdcien t, for example, to c ho ose an arbitrary mapping from the searc h space in to the space of c hromosomes, apply the standard genetic op erators and hop e for the b est. What mak es a go o d mapping is a sub ject for con tin uing researc h, but the general consensus is that candidate solutions that share imp ortan t phenot ypic similarities m ust also exhibit similar forms of \\building blo c ks\" when represen ted as c hromosomes (Holland, \u0001\t\u0007\u0005). It follo ws that the user of an EA m ust carefully consider the most natural w a y to represen t the elemen ts of the searc h space as c hromosomes. Moreo v er, it is often necessary to design appropriate m utation and recom bination op erators that are sp eci\ufffdc to the c hosen represen tation. The end result of this design pro cess is that the represen tation and genetic op erators selected for the EA comprise a form of searc h bias similar to biases in other mac hine learning meth\u0002\u0004\u0007 Parent 1: A B C D E F G Parent 2: a b c d e f g Offspring 1: A B C d e f g Offspring 2: a b c D E F G ated b y crossing o v er the selected paren ts. The op eration sho wn is called one-p oint cr ossover. The \ufffdrst o\ufffdspring inherits the initial segmen t of one paren t and the \ufffdnal segmen t of the other paren t. The second o\ufffdspring inherits the same pattern of genes from the opp osite paren ts. The crosso v er p oin t is p osition \u0003, c hosen at random. The second o\ufffdspring has also incurred a m utation in the shaded gene. o ds. Giv en the prop er bias, the EA can quic kly iden tify useful \\building blo c ks\" within the p opulation, and con v erge on the most promising areas of the searc h space. \u0001 In the case of RL, the user needs to mak e t w o ma jor design decisions. First, ho w will the space of p olicies b e represen ted b y c hromosomes in the EA? Second, ho w will the \ufffdtness of p opulation elemen ts b e assessed? The answ ers to these questions dep end on ho w the user c ho oses to bias the EA. The next section presen ts a simple EARL that adopts the most straigh tforw ard set of design decisions. This example is mean t only to pro vide a baseline for comparison with more elab orate designs. \u0004.\u0002 A Simple EARL As the remainder of this pap er sho ws, there are man y w a ys to use EAs to searc h the space of RL p olicies. This section pro vides a concrete example of a simple EARL, whic h w e call Earl \u0001 . The pseudo-co de is sho wn in Figure \u0004. This system pro vides the EA coun terpart to the simple table-based TD system describ ed in Section \u0003. The most straigh tforw ard w a y to represen t a p olicy in an EA is to use a single c hromosome p er p olicy with a single gene asso ciated with eac h observ ed state. In Earl \u0001 , eac h gene's v alue (or al lele in biological terminology) represen ts the action v alue asso ciated with the corresp onding state, as sho wn in Figure \u0005. T able \u0002 sho ws part of an Earl \u0001 p opulation of p olicies for the sample grid w orld problem. The n um b er of p olicies in a p opulation is usually on the order of \u000100 to \u0001000. The \ufffdtness of eac h p olicy in the p opulation m ust re\ufffdect the exp ected accum ulated \ufffdtness for an agen t that uses the giv en p olicy . There are no \ufffdxed constrain ts on ho w the \ufffdtness of an individual p olicy is ev aluated. If the w orld is deterministic, lik e the sample grid-w orld, \u0001. Other w a ys to exploit problem sp eci\ufffdc kno wledge in EAs include the use of heuristics to initiali ze the p opulation and the h ybridizatio n with problem sp eci\ufffdc searc h algorithms. See (Grefenstette, \u0001\t\b\u0007) for further discussions of these metho ds. \u0002\u0004\b up date p olicies in P(t); evaluate p olicies in P(t); end end. Figure \u0004: Pseudo-co de for Ev olutionary Algorithm Reinforcemen t Learning system. Policy i: a1 a1 a3 ... aN s1 s1 s3 sN Figure \u0005: T able-based p olicy represen tation. Eac h observ ed state has a gene whic h indicates the preferred action for that state. With this represen tation, standard genetic op erators suc h as m utation and crosso v er can b e applied. the \ufffdtness of a p olicy can b e ev aluated during a single trial that starts with the agen t in the initial state and terminates when the agen t reac hes a terminal state (e.g., falls o\ufffd the grid in the grid-w orld). In non-deterministic w orlds, the \ufffdtness of a p olicy is usually a v eraged o v er a sample of trials. Other options include measuring the total pa y o\ufffd ac hiev ed b y the agen t after a \ufffdxed n um b er of steps, or measuring the n um b er of steps required to ac hiev e a \ufffdxed lev el of pa y o\ufffd. Once the \ufffdtness of all p olicies in the p opulation has b een determined, a new p opulation is generated according to the steps in the usual EA (Figure \u0002). First, paren ts are selected for repro duction. A t ypical selection metho d is to probabilisticall y select individuals based on relativ e \ufffdtness: Pr(p i ) = F itness(p i ) P n j =\u0001 F itness(p j ) (\t) where p i represen ts individual i and n is the total n um b er of individual s. Using this selection rule, the exp ected n um b er of o\ufffdspring for a giv en p olicy is prop ortional to that p olicy's \ufffdtness. F or example, a p olicy with a v erage \ufffdtness migh t ha v e a single o\ufffdspring, whereas \u0002\u0004\t a p olicy with t wice the a v erage \ufffdtness w ould ha v e t w o o\ufffdspring. \u0002 O\ufffdspring are formed b y cloning the selected paren ts. Then new p olicies are generated b y applying the standard genetic op erators of crosso v er and m utation to the clones, as sho wn in Figure \u0003. The pro cess of generating new p opulations of strategies can con tin ue inde\ufffdnitely or can b e terminated after a \ufffdxed n um b er of generations or once an acceptable lev el of p erformance is ac hiev ed. F or simple RL problems suc h as the grid-w orld, Earl \u0001 ma y pro vide an adequate approac h. In later sections, w e will p oin t out some w a ys in whic h ev en Earl \u0001 exhibits strengths that are complemen tary to TD metho ds for RL. Ho w ev er, as in the case of TD metho ds, EARL metho ds ha v e b een extended to handle the man y c hallenges inheren t in more realistic RL problems. The follo wing sections surv ey some of these extensions, organized around three sp eci\ufffdc biases that distinguish EAs for Reinforcemen t Learning (EARL) from more generic EAs: p olicy represen tations, \ufffdtness/credit-assignmen t mo dels, and RLsp eci\ufffdc genetic op erators. \u0005. P olicy Represen tations in EARL P erhaps the most critical feature that distinguishes classes of EAs from one another is the represen tation used. F or example, EAs for function optimization use a simple string or v ector represen tation, whereas EAs for com binatorial optimization use distinctiv e represen tations for p erm utations, trees or other graph structures. Lik ewise, EAs for RL use a distinctiv e set of represen tations for p olicies. While the range of p oten tial p olicy represen tations is unlimited, the represen tations used in most EARL systems to date can b e largely categorized along t w o discrete dimensions. First, p olicies ma y b e represen ted either b y condition-action rules or b y neural net w orks. Second, p olicies ma y b e represen ted b y a single c hromosome or the represen tation ma y b e distributed through one or more p opulations. \u0005.\u0001 Single-Chromosom e Represen tation of P olicies \u0005.\u0001.\u0001 R ule-based Policies F or most RL problems of practical in terest, the n um b er of observ able states is v ery large, and the simple table-based represen tation in Earl \u0001 is impractical. F or large scale state \u0002. Man y other paren t selection rules ha v e b een explored (Grefenstette, \u0001\t\t\u0007a, \u0001\t\t\u0007b). \u0002\u00050 Policy i: ci1 \u2192 ai1 ci2 \u2192 ai2 ci3 \u2192 ai3 ... cik \u2192 aik Policy i: w1 w2 w3 ... wk => ... w1 w j wk\u22121 wk Figure \u0007: A simple parameter represen tation of w eigh ts for a neural net w ork. The \ufffdtness of the p olicy is the pa y o\ufffd when the agen t uses the corresp onding neural net as its decision p olicy . spaces, it is more reasonable to represen t a p olicy as a set of condition-action rules in whic h the condition expresses a predicate that matc hes a set of states, as sho wn in Figure \u0006. Early examples of this represen tation include the systems LS-\u0001 (Smith, \u0001\t\b\u0003) and LS-\u0002 (Sc ha\ufffder & Grefenstette, \u0001\t\b\u0005), follo w ed later b y Samuel (Grefenstette et al., \u0001\t\t0). \u0005.\u0001.\u0002 Neural Net Represent a tion of Policies As in TD-based RL systems, EARL systems often emplo y neural net represen tations as function appro ximators. In the simplest case (see Figure \u0007), a neural net w ork for the agen t's decision p olicy is represen ted as a sequence of real-v alued connection w eigh ts. A straigh tforw ard EA for parameter optimization can b e used to optimize the w eigh ts of the neural net w ork (Belew, McInerney , & Sc hraudolph, \u0001\t\t\u0001; Whitley , Dominic, Das, & Anderson, \u0001\t\t\u0003; Y amauc hi & Beer, \u0001\t\t\u0003). This represen tation th us requires the least mo di\ufffdcation of the standard EA. W e no w turn to distributed represen tations of p olicies in EARL systems. \u0005.\u0002 Distributed Represen tation of P olicies In the previous section w e outlined EARL approac hes that treat the agen t's decision p olicy as a single genetic structure that ev olv es o v er time. This section addresses EARL approac hes that decomp ose a decision p olicy in to smaller comp onen ts. Suc h approac hes ha v e t w o p oten tial adv an tages. First, they allo w ev olution to w ork at a more detailed lev el of the task, e.g., on sp eci\ufffdc subtasks. Presumably , ev olving a solution to a restricted subtask should b e \u0002\u0005\u0001 Message List Sensors Classifiers Evolutionary Algorithm Decision Rewards Figure \b: Holland's Learning Classi\ufffder System. easier than ev olving a monolithic p olicy for a complex task. Second, decomp osition p ermits the user to exploit bac kground kno wledge. The user migh t base the decomp osition in to subtasks on a prior analysis of the o v erall p erformance task; for example, it migh t b e kno wn that certain subtasks are m utually exclusiv e and can therefore b e learned indep enden tly . The user migh t also decomp ose a complex task in to subtasks suc h that certain comp onen ts can b e explicitly programmed while other comp onen ts are learned. In terms of kno wledge represen tation in EARL, the alternativ e to the single c hromosome represen tation is to distribute the p olicy o v er sev eral p opulation elemen ts. By assigning a \ufffdtness to these individual elemen ts of the p olicy , ev olutionary selection pressure can b e brough t to b ear on more detailed asp ects of the learning task. That is, \ufffdtness is no w a function of individual subp olicies or individual rules or ev en individual neurons. This general approac h is analogous to the classic TD metho ds that tak e this approac h to the extreme of learning statistics concerning eac h state-action pair. As in the case of single-c hromosome represen tations, w e can partition distributed EARL represen tations in to rule-based and neural-net-based classes. \u0005.\u0002.\u0001 Distributed R ule-based Policies The most w ell-kno wn example of a distributed rule-based approac h to EARL is the Learning Classi\ufffder Systems (LCS) mo del (Holland & Reitman, \u0001\t\u0007\b; Holland, \u0001\t\b\u0007; Wilson, \u0001\t\t\u0004). An LCS uses an ev olutionary algorithm to ev olv e if-then rules called classi\ufffders that map sensory input to an appropriate action. Figure \b outlines Holland's LCS framew ork (Holland, \u0001\t\b\u0006). When sensory input is receiv ed, it is p osted on the message list. If the left hand side of a classi\ufffder matc hes a message on the message list, its righ t hand side is p osted on the message list. These new messages ma y subsequen tly trigger other classi\ufffders to p ost messages or in v ok e a decision from the LCS, as in the traditional forw ard-c haining mo del of rule-based systems. In an LCS, eac h c hromosome represen ts a single decision rule and the en tire p opulation represen ts the agen t's p olicy . In general, classi\ufffders map a set of observ ed states to a set of messages, whic h ma y b e in terpreted as either in ternal state c hanges or actions. F or example, \u0002\u0005\u0002 Environment LCS LCS LCS Figure \t: A t w o-lev el hierarc hical Alecsys system. Eac h LCS learns a sp eci\ufffdc b eha vior. The in teractions among the rule sets are pre-programmed. if the learning agen t for the grid w orld in Figure \u0001 has t w o sensors, one for the column and one for the ro w, then the p opulation in an LCS migh t app ear as sho wn in T able \u0003. The \ufffdrst classi\ufffder matc hes an y state in the column a and recommends action R. Eac h classi\ufffder has a statistic called str ength that estimates the utilit y of the rule. The strength statistics are used in b oth con\ufffdict resolution (when more than one action is recommended) and as \ufffdtness for the genetic algorithm. Genetic op erators are applied to highly \ufffdt classi\ufffders to generate new rules. Generally , the p opulation size (i.e., the n um b er of rules in the p olicy) is k ept constan t. Th us classi\ufffders comp ete for space in the p olicy . Another w a y that EARL systems distribute the represen tation of p olicies is to partition the p olicy in to separate mo dules, with eac h mo dule up dated b y its o wn EA. Dorigo and Colom b etti (\u0001\t\t\b) describ e an arc hitecture called Alecsys in whic h a complex reinforcemen t learning task is decomp osed in to subtasks, eac h of whic h is learned via a separate LCS, as sho wn in Figure \t. They pro vide a metho d called b ehavior analysis and tr aining (BA T) to manage the incremen tal training of agen ts using the distributed LCS arc hitecture. The single-c hromosome represen tation can also b e extended b y partitioning the p olicy across m ultiple co-ev olving p opulations. F or example, in the co op erativ e co-ev olution mo del (P otter, \u0001\t\t\u0007), the agen t's p olicy is formed b y com bining c hromosomes from several indep enden tl y ev olving p opulations. Eac h c hromosome represen ts a set of rules, as in Figure \u0006, but these rules address only a subset of the p erformance task. F or example, separate p opulations migh t ev olv e p olicies for di\ufffderen t comp onen ts of a complex task, or \u0002\u0005\u0003 Population EA i Evolutionary Algorithm Merge Domain Model collaboration fitness individual to be evaluated EA 1 EA 2 EA n representative representative representative representative Figure \u00010: Co op erativ e co ev olutionary arc hitecture from the p ersp ectiv e of the i th EA instance. Eac h EA con tributes a represen tativ e, whic h is merged with the others' represen tativ es to form a c ol lab or ation, or p olicy for the agen t. The \ufffdtness of eac h represen tativ e re\ufffdects the a v erage \ufffdtness of its collab orations. migh t address m utually exclusiv e sets of observ ed states. The \ufffdtness of eac h c hromosome is computed based on the o v erall \ufffdtness of the agen ts that emplo y that c hromosome as part of its com bined c hromosomes. The com bined c hromosomes represen t the decision p olicy and are called a c ol lab or ation (Figure \u00010). \u0005.\u0002.\u0002 Distributed Netw ork-based Policies Distributed EARL systems using neural net represen tations ha v e also b een designed. In (P otter & De Jong, \u0001\t\t\u0005), separate p opulations of neurons ev olv e, with the ev aluation of eac h neuron based on the \ufffdtness of a collab oration of neurons selected from eac h p opulation. In SANE (Moriart y & Miikkulainen, \u0001\t\t\u0006a, \u0001\t\t\b), t w o separate p opulations are main tained and ev olv ed: a p opulation of neurons and a p opulation of net w ork blueprin ts. The motiv ation for SANE comes from our a priori kno wledge that individual neurons are fundamen tal building blo c ks in neural net w orks. SANE explicitly decomp oses the neural net w ork searc h problem in to sev eral parallel searc hes for e\ufffdectiv e single neurons. The neuron-lev el ev olution pro vides ev aluation and recom bination of the neural net w ork building blo c ks, while the p opulation of blueprin ts searc h for e\ufffdectiv e com binations of these building blo c ks. Figure \u0001\u0001 giv es an o v erview of the in teraction of the t w o p opulations. Eac h individual in the blueprin t p opulation consists of a set of p oin ters to individuals in the neuron p opulation. During eac h generation, neural net w orks are constructed b y com bining the hidden neurons sp eci\ufffded in eac h blueprin t. Eac h blueprin t receiv es a \ufffdtness according to ho w w ell the corresp onding net w ork p erforms in the task. Eac h neuron receiv es a \ufffdtness according to ho w w ell the top net w orks in whic h it participates p erform in the task. An aggressiv e genetic selection and recom bination strategy is used to quic kly build and propagate highly \ufffdt structures in b oth the neuron and blueprin t p opulations. \u0002\u0005\u0004 l l l w w w l l l w w w l l l w w w l l l w w w l l l w w w Network Blueprint Population l l l w w w l l l w w w l l l w w w Neuron Population Figure \u0001\u0001: An o v erview of the t w o p opulations in SANE. Eac h mem b er of the neuron p opulation sp eci\ufffdes a series of connections (connection lab els and w eigh ts) to b e made within a neural net w ork. Eac h mem b er of the net w ork blueprin t p opulation sp eci\ufffdes a series of p oin ters to sp eci\ufffdc neurons whic h are used to build a neural net w ork. \u0006. Fitness and Credit Assignmen t in EARL Ev olutionary algorithms are all driv en b y the concept of natural selection: p opulation elemen ts that ha v e higher \ufffdtness lea v e more o\ufffdspring to later generations, th us in\ufffduencing the direction of searc h in fa v or of high p erformance regions of the searc h space. The concept of \ufffdtness is cen tral to an y EA. In this section, w e discuss features of the \ufffdtness mo del that are common across most EARL systems. W e sp eci\ufffdcally fo cus on w a ys in whic h the \ufffdtness function re\ufffdects the distinctiv e structure of the RL problem. \u0006.\u0001 The Agen t Mo del The \ufffdrst common features of all EARL \ufffdtness mo dels is that \ufffdtness is computed with resp ect to an RL agen t. That is, ho w ev er the p olicy is represen ted in the EA, it m ust b e con v erted to a decision p olicy for an agen t op erating in a RL en vironmen t. The agen t is assumed to observ e a description of the curren t state, select its next action b y consulting its curren t p olicy , and collect whatev er rew ard is pro vided b y the en vironmen t. In EARL systems, as in TD systems, the agen t is generally assumed to p erform v ery little additional computation when selecting its next action. While neither approac h limits the agen t to strict stim ulus-resp onse b eha vior, it is usually assumed that the agen t do es not p erform extensiv e planning or other reasoning b efore acting. This assumption re\ufffdects the fact that RL tasks in v olv e some sort of con trol activit y in whic h the agen t m ust resp ond to a dynamic en vironmen t within a limited time frame. \u0002\u0005\u0005 extended sequence of decisions, rather than an y individual decision. F or example, a rob ot ma y receiv e a rew ard after a mo v emen t that places it in a \\goal\" p osition within a ro om. The rob ot's rew ard, ho w ev er, dep ends on man y of its previous mo v emen ts leading it to that p oin t. A di\ufffdcult cr e dit assignment problem therefore exists in ho w to app ortion the rew ards of a sequence of decisions to individual decisions. In general, EA and TD metho ds address the credit assignmen t problem in v ery differen t w a ys. In TD approac hes, credit from the rew ard signal is explicitly propagated to eac h decision made b y the agen t. Ov er man y iterations, pa y o\ufffds are distributed across a sequence of decisions so that an appropriately discoun ted rew ard v alue is asso ciated with eac h individual state and decision pair. In simple EARL systems suc h as Earl \u0001 , rew ards are asso ciated only with sequences of decisions and are not distributed to the individual decisions. Credit assignmen t for an individual decision is made implicitly , since p olicies that prescrib e p o or individual decisions will ha v e few er o\ufffdspring in future generations. By selecting against p o or p olicies, ev olution automatically selects against p o or individual decisions. That is, building blo c ks consisting of particular state-action pairs that are highly correlated with go o d p olicies are propagated through the p opulation, replacing state-action pairs asso ciated with p o orer p olicies. Figure \u0001\u0002 illustrates the di\ufffderences in credit assignmen t b et w een TD and Earl \u0001 in the grid w orld of Figure \u0001. The Q-learning TD metho d explicitly assigns credit or blame to eac h individual state-action pair b y passing bac k the immediate rew ard and the estimated pa y o\ufffd from the new state. Th us, an error term b ecomes asso ciated with eac h action p erformed b y the agen t. The EA approac h do es not explicitly propagate credit to eac h action but rather asso ciates an o v erall \ufffdtness with the en tire p olicy . Credit is assigned implicitly , based on the \ufffdtness ev aluations of en tire sequences of decisions. Consequen tly , the EA will tend to select against p olicies that generate the \ufffdrst and third sequences b ecause they ac hiev e lo w er \ufffdtness scores. The EA th us implicitly selects against action D in state b\u0002, for example, whic h is presen t in the bad sequences but not presen t in the go o d sequences. \u0006.\u0003 Subp olicy Credit Assignme n t Besides the implicit credit assignmen t p erformed on building blo c ks, EARL systems ha v e also addressed the credit assignmen t problem more directly . As sho wn in Section \u0004, the individuals in an EARL system migh t represen t either en tire p olicies or comp onen ts of a p olicy (e.g., comp onen t rule-sets, individual decision rules, or individual neurons). F or distributed-represen tation EARLs, \ufffdtness is explicitly assigned to individual comp onen ts. \u0002\u0005\u0006 b1,D b2,R c2,D 1+Max(Q(b2,a)) -5+Max(Q(c2,a)) 4+Max(Q(c3,a)) c3 2+Max(Q(b1,a)) a1,R a2,R b2,D b3,D 1+Max(Q(b2,a)) -5+Max(Q(b3,a)) 4+Max(Q(c3,a)) c3 2+Max(Q(a2,a)) a1,D a2,D b2,R c2,D 1+Max(Q(b2,a)) -5+Max(Q(c2,a)) 4+Max(Q(d2,a)) d2 2+Max(Q(a2,a)) a1,D b1,D b2,D b3,D 1+Max(Q(b2,a)) -5+Max(Q(b3,a)) 4+Max(Q(c3,a)) c3 2+Max(Q(b1,a)) a1,R Fitness 2 9 1 8 b1,D b2,R c2,D c3 a1,R a2,R b2,D b3,D c3 a1,D a2,D b2,R c2,D d2 a1,D b1,D b2,D b3,D c3 a1,R TD Explicit Credit Assignment EA Implicit Credit Assignment Figure \u0001\u0002: Explicit vs. implicit credit assignmen t. The Q-learning TD metho d assigns credit to eac h state-action pair based on the immediate rew ard and the predicted future rew ards. The EA metho d assigns credit implicitl y b y asso ciating \ufffdtness v alues with en tire sequences of decisions. In cases in whic h a p olicy is represen ted b y explicit comp onen ts, di\ufffderen t \ufffdtness functions can b e asso ciated with di\ufffderen t ev olving p opulations, allo wing the implemen ter to \\shap e\" the o v erall p olicy b y ev olving subp olicies for sp eci\ufffdc subtasks (Dorigo & Colom b etti, \u0001\t\t\b; P otter, De Jong, & Grefenstette, \u0001\t\t\u0005). The most am bitious goal is to allo w the system to manage the n um b er of co-ev olving sp ecies as w ell as the form of in teractions (P otter, \u0001\t\t\u0007). This exciting researc h is still at an early stage. F or example, in the LCS mo del, eac h classi\ufffder (decision rule) has a str ength whic h is up dated using a TD-lik e metho d called the bucket brigade algorithm (Holland, \u0001\t\b\u0006). In the buc k et brigade algorithm, the strength of a classi\ufffder is used to bid against other classi\ufffders for the righ t to p ost messages. Bids are subtracted from winning classi\ufffders and passed bac k to the classi\ufffders that p osted the enabling message on the previous step. Classi\ufffder strengths are th us reinforced if the classi\ufffder p osts a message that triggers another classi\ufffder. The classi\ufffder that in v ok es a decision from the LCS receiv es a strength reinforcemen t directly from the en vironmen t. The buc k et brigade bid passing mec hanism clearly b ears a strong relation to the metho d of temp oral di\ufffderences (Sutton, \u0001\t\b\b). The buc k et brigade up dates a giv en classi\ufffder's strength based on the strength of the classi\ufffders that \ufffdre as a direct result of its activ ation. The TD metho ds di\ufffder sligh tly in this resp ect b ecause they assign credit based strictly on temp oral succession and do not tak e in to accoun t causal relations of steps. It remains unclear whic h is more appropriate for distributing credit. Ev en for single c hromosome represen tations, TD-lik e metho ds ha v e b een adopted in some EARL systems. In Samuel, eac h gene (decision rule) also main tains a quan tit y called str ength that is used to resolv e con\ufffdict when more than one rule matc hes the agen t's curren t sensor readings. When pa y o\ufffd is obtained (thereb y terminating the trial), the strengths of \u0002\u0005\u0007 to co v er the new set of sensor readings. A similar rule-creation op erator w as included in early v ersions of Samuel (Grefenstette et al., \u0001\t\t0). Later v ersions of Samuel included a n um b er of m utation op erators whic h created altered rules based on an agen t's early exp eriences. F or example, Samuel's Sp e cialization m utation op erator is triggered when a lo w-strength, general rule \ufffdres during an episo de that results in high pa y o\ufffd. In suc h a case, the rule's conditions are reduced in generalit y to more closely matc h the agen t's sensor readings. F or example, if the agen t has a sensor readings (r ang e = \u00040; bear ing = \u000100) and the original rule is: IF r ang e = [\u0002\u0005; \u0005\u0005] AND bear ing = [0; \u0001\b0] THEN SET tur n = \u0002\u0004 (str eng th 0.\u0001) then the new rule w ould b e: IF r ang e = [\u0003\u0005; \u0004\u0005] AND bear ing = [\u00050; \u0001\u00040] THEN SET tur n = \u0002\u0004 (str eng th 0.\b) Since the episo de triggering the op erator resulted in high pa y o\ufffd, one migh t susp ect that the original rule w as o v er-generalized, and that the new, more sp eci\ufffdc v ersion migh t lead to b etter results. (The strength of the new rule is initialized to the pa y o\ufffd receiv ed during the triggering episo de.) This is considered a Lamarc kian op erator b ecause the agen t's exp erience is causing a genetic c hange whic h is passed on to later o\ufffdspring. \u0003 Samuel also uses an RL-sp eci\ufffdc crosso v er op erator to recom bine p olicies. In particular, crosso v er in Samuel attempts to cluster decision rules b efore assigning them to o\ufffdspring. F or example, supp ose that the traces of the most previous ev aluations of the paren t strategies are as follo ws (R i;j denotes the j th decision rule in p olicy i): T race for paren t #\u0001: Episo de: . . . \b. R \u0001;\u0003 ! R \u0001;\u0001 ! R \u0001;\u0007 ! R \u0001;\u0005 High P a y o\ufffd \t. R \u0001;\u0002 ! R \u0001;\b ! R \u0001;\u0004 Lo w P a y o\ufffd \u0003. Jean Baptiste Lamarc k dev elop ed an ev olutionary theory that stressed the inheritance of acquired c haracteristics, in particular acquired c haracteristics that are w ell adapted to the surrounding en vironmen t. Of course, Lamarc k's theory w as sup erseded b y Darwin's emphasis on t w o-stage adaptation: undirected v ariation follo w ed b y selection. Researc h has generally failed to substan tiate an y Lamarc kian mec hanisms in biological systems (Gould, \u0001\t\b0). \u0002\u0005\b fR \u0001;\b ; : : : ; R \u0001;\u0003 ; R \u0001;\u0001 ; R \u0001;\u0007 ; R \u0001;\u0005 ; : : : ; R \u0002;\u0006 ; R \u0002;\u0002 ; R \u0002;\u0004 ; : : : ; R \u0002;\u0007 g The motiv ation here is that rules that \ufffdre in sequence to ac hiev e a high pa y o\ufffd should b e treated as a group during recom bination, in order to increase the lik eliho o d that the o\ufffdspring p olicy will inherit some of the b etter b eha vior patterns of its paren ts. Rules that do not \ufffdre in successful episo des (e.g., R \u0001;\b ) are randomly assigned to one of the t w o o\ufffdspring. This form of crosso v er is not only Lamarc kian (since it is triggered b y the exp eriences of the agen t), but is directly related to the structure of the RL problem, since it groups comp onen ts of p olicies according to the temp oral asso ciation among the decision rules. \b. Strengths of EARL The EA approac h represen ts an in teresting alternativ e for solving RL problems, o\ufffdering sev eral p oten tial adv an tages for scaling up to realistic applications. In particular, EARL systems ha v e b een dev elop ed that address di\ufffdcult c hallenges in RL problems, including: \ufffd Large state spaces; \ufffd Incomplete state information; and \ufffd Non-stationary en vironmen ts. This section fo cuses on w a ys that EARL address these c hallenges. \b.\u0001 Scaling Up to Large State Spaces Man y early pap ers in the RL literature analyze the e\ufffdciency of alternativ e learning metho ds on to y problems similar to the grid w orld sho wn in Figure \u0001. While suc h studies are useful as academic exercises, the n um b er of observ ed states in realistic applications of RL is lik ely to preclude an y approac h that requires the explicit storage and manipulation of statistics asso ciated with eac h observ able state-action pair. There are t w o w a ys that EARL p olicy represen tations help address the problem of large state spaces: gener alization and sele ctivity. \b.\u0001.\u0001 Policy Generaliza tion Most EARL p olicy represen tations sp ecify the p olicy at a lev el of abstraction higher than an explicit mapping from observ ed states to actions. In the case of rule-based represen tations, the rule language allo ws conditions to matc h sets of states, th us greatly reducing the storage \u0002\u0005\t are main tained. required to sp ecify a p olicy . It should b e noted, ho w ev er, that the generalit y of the rules within a p olicy ma y v ary considerably , from the lev el of rules that sp ecify an action for a single observ ed state all the w a y to completely general rules that recommend an action regardless of the curren t state. Lik ewise, in neural net represen tations, the mapping function is stored implicitly in the w eigh ts on the connections of the neural net. In either case, a generalized p olicy represen tation facilitates the searc h for go o d p olicies b y grouping together states for whic h the same action is required. \b.\u0001.\u0002 Policy Selectivity Most EARL systems ha v e sele ctive represen tations of p olicies. That is, the EA learns mappings from observ ed states to recommended actions, usually eliminating explicit information concerning less desirable actions. Kno wledge ab out bad decisions is not explicitly preserv ed, since p olicies that mak e suc h decisions are selected against b y the ev olutionary algorithm and are ev en tually eliminated from the p opulation. The adv an tage of selectiv e represen tations is that atten tion is fo cused on pro\ufffdtable actions only , reducing space requiremen ts for p olicies. Consider our example of the simple EARL op erating on the grid w orld. As the p opulation ev olv es, p olicies normally con v erge to the b est actions from a sp eci\ufffdc state, b ecause of the selectiv e pressure to ac hiev e high \ufffdtness lev els. F or example, the p opulation sho wn in T able \u0002 has con v erged alleles (actions) in states a\u0003; a\u0005; b\u0002; b\u0005 ; d \u0003; e\u0001; and e\u0002. Eac h of these con v erged state-action pairs is highly correlated with \ufffdtness. F or example, all p olicies ha v e con v erged to action R in state b\u0002. T aking action R in state b\u0002 ac hiev es a m uc h higher exp ected return than action D (\u0001\u0005 vs. \b from T able \u0001). P olicies that select action D from state b\u0002 ac hiev e lo w er \ufffdtness scores and are selected against. F or this simple EARL, a snapshot of the p opulation (T able \u0002) pro vides an implicit estimate of a corresp onding TD v alue function (T able \u0004), but the distribution is biased to w ard the more pro\ufffdtable state-actions pairs. \u0002\u00060 Green Blue Red Blue 1.0 L R L R L R L R .75 3.0 1.0 .5 - 4.0 Figure \u0001\u0003: An en vironmen t with incomplete state information. The circles represen t the states of the w orld and the colors represen t the agen t's sensory input. The agen t is equally lik ely to start in the r ed state or the g r een state \b.\u0002 Dealing with Incomplete State Information Clearly , the most fa v orable condition for reinforcemen t learning o ccurs when the agen t can observ e the true state of the dynamic system with whic h it in teracts. When complete state information is a v ailable, TD metho ds mak e e\ufffdcien t use of a v ailable feedbac k b y asso ciating rew ard directly with individual decisions. In real w orld situations, ho w ev er, the agen t's sensors are more lik ely to pro vide only a partial view that ma y fail to disam biguate man y states. Consequen tly , the agen t will often b e unable to completely distinguish its curren t state. This problem has b een termed p er c eptual aliasing or the hidden state problem. In the case of limited sensory information, it ma y b e more useful to asso ciate rew ards with larger blo c ks of decisions. Consider the situation in Figure \u0001\u0003, in whic h the agen t m ust act without complete state information. Circles represen t the sp eci\ufffdc states of the w orld, and the colors represen t the sensor information the agen t receiv es within the state. Square no des represen t goal states with the corresp onding rew ard sho wn inside. In eac h state, the agen t has a c hoice of t w o actions (L or R). W e further assume that the state transitions are deterministic and that the agen t is equally lik ely to start in either the state with the red or green sensor readings. In this example, there are t w o di\ufffderen t states that return a sensor reading of bl ue, and the agen t is unable to distinguish b et w een them. Moreo v er, the actions for eac h bl ue state return v ery di\ufffderen t rew ards. A Q function applied to this problem treats the sensor reading of bl ue as one observ able state, and the rew ards for eac h action are a v eraged o v er b oth bl ue states. Th us, Q(bl ue; L) and Q(bl ue; R) will con v erge to -0.\u0005 and \u0001, resp ectiv ely . Since the rew ard from Q(bl ue; R) is higher than the alternativ es from observ able states r ed and g r een, the agen t's p olicy under Q-learning will c ho ose to en ter observ able state bl ue eac h time. The \ufffdnal decision p olicy under Q-learning is sho wn in T able \u0005. This table also sho ws the optimal p olicy with resp ect to the agen t's limited view of its w orld. In other \u0002\u0006\u0001 states. By asso ciating v alues with individual observ able states, the simple TD metho ds are vulnerable to hidden state problems. In this example, the am biguous state information misleads the TD metho d, and it mistak enly com bines the rew ards from t w o di\ufffderen t states of the system. By confounding information from m ultiple states, TD cannot recognize that adv an tages migh t b e asso ciated with sp eci\ufffdc actions from sp eci\ufffdc states, for example, that action L from the top bl ue state ac hiev es a v ery high rew ard. In con trast, since EA metho ds asso ciate credit with en tire p olicies, they rely more on the net results of decision sequences than on sensor information, that ma y , after all, b e am biguous. In this example, the ev olutionary algorithm exploits the disparit y in rew ards from the di\ufffderen t bl ue states and ev olv es p olicies that en ter the go o d bl ue state and a v oid the bad one. The agen t itself remains unable to distinguish the t w o bl ue states, but the ev olutionary algorithm implicitly distinguishes among am biguous states b y rew arding p olicies that a v oid the bad states. F or example, an EA metho d can b e exp ected to ev olv e an optimal p olicy in the curren t example giv en the existing, am biguous state information. P olicies that c ho ose the action sequence R,L when starting in the r ed state will ac hiev e the highest lev els of \ufffdtness, and will therefore b e selected for repro duction b y the EA. If agen ts using these p olicies are placed in the g r een state and select action L, they receiv e the lo w est \ufffdtness score, since their subsequen t action, L from the bl ue sensors, returns a negativ e rew ard. Th us, man y of the p olicies that ac hiev e high \ufffdtness when started in the r ed state will b e selected against if they c ho ose L from the g r een state. Ov er the course of man y generations, the p olicies m ust c ho ose action R from the g r een state to maximize their \ufffdtness and ensure their surviv al. W e con\ufffdrmed these h yp otheses in empirical tests. A Q-learner using single-step up dates and a table-based represen tation con v erged to the v alues in T able \u0005 in ev ery run. An ev olutionary algorithm \u0004 consisten tly con v erged \b0% of its p opulation on the optimal p olicy . Figure \u0001\u0004 sho ws the a v erage p ercen tage of the optimal p olicy in the p opulation as a function of time, a v eraged o v er \u000100 indep enden t runs. Th us ev en simple EA metho ds suc h as Earl \u0001 app ear to b e more robust in the presence of hidden states than simple TD metho ds. Ho w ev er, more re\ufffdned sensor information could still b e helpful. In the previous example, although the EA p olicies ac hiev e a b etter a v erage rew ard than the TD p olicy , the ev olv ed p olicy remains unable to pro cure b oth the \u0003.0 \u0004. W e used a binary tournamen t selection, a \u00050 p olicy p opulation , 0.\b crosso v er probabili t y , and 0.0\u0001 m utation rate. \u0002\u0006\u0002 0 20 40 60 80 100 0 10 20 30 40 50 60 70 80 90 100 Percentage Optimal Generation Figure \u0001\u0004: The optimal p olicy distribution in the hidden state problem for an ev olutionary algorithm. The graph plots the p ercen tage of optimal p olicies in the p opulation, a v eraged o v er \u000100 runs. and \u0001.0 rew ards from the t w o bl ue states. These rew ards could b e realized, ho w ev er, if the agen t could separate the t w o bl ue states. Th us, an y metho d that generates additional features to disam biguate states presen ts an imp ortan t asset to EA metho ds. Kaelbling et al. (\u0001\t\t\u0006) describ e sev eral promising solutions to the hidden state problem, in whic h additional features suc h as the agen t's previous decisions and observ ations are automatically generated and included in the agen t's sensory information (Chrisman, \u0001\t\t\u0002; Lin & Mitc hell, \u0001\t\t\u0002; McCallum, \u0001\t\t\u0005; Ring, \u0001\t\t\u0004). These metho ds ha v e b een e\ufffdectiv e at disam biguating states for TD metho ds in initial studies, but further researc h is required to determine the exten t to whic h similar metho ds can resolv e signi\ufffdcan t hidden state information in realistic applications. It w ould b e useful to dev elop w a ys to use suc h metho ds to augmen t the sensory data a v ailable in EA metho ds as w ell. \b.\u0003 Non-Stationary En vironmen ts If the agen t's en vironmen t c hanges o v er time, the RL problem b ecomes ev en more di\ufffdcult, since the optimal p olicy b ecomes a mo ving target. The classic trade-o\ufffd b et w een exploration and exploitation b ecomes ev en more pronounced. T ec hniques for encouraging exploration in TD-based RL include adding an explor ation b onus to the estimated v alue of state-action pairs that re\ufffdects ho w long it has b een since the agen t has tried that action (Sutton, \u0001\t\t0), and building a statistical mo del of the agen t's uncertain t y (Da y an & Sejno wski, \u0001\t\t\u0006). Simple mo di\ufffdcations of standard ev olutionary algorithms o\ufffder an abilit y to trac k nonstationary en vironmen ts, and th us pro vide a promising approac h to RL for these di\ufffdcult cases. The fact that ev olutionary searc h is based on comp etition within a p opulation of p olicies suggest some immediate b ene\ufffdts for trac king non-stationary en vironmen ts. T o the exten t that the p opulation main tains a div erse set of p olicies, c hanges in the en vironmen t will bias \u0002\u0006\u0003 \ufffdtness landscap es. Main taining this source of div ersit y p ermits the EA to resp ond rapidly to large, sudden c hanges in the \ufffdtness landscap e. By k eeping the randomized p ortion of the p opulation to less than ab out \u00030% of the p opulation, the impact on searc h e\ufffdciency in stationary en vironmen ts is minimized. This is a general approac h that can easily b e applied in EARL systems. Other useful algorithms that ha v e b een dev elop ed to ensure div ersit y in ev olving p opultions include \ufffdtness sharing (Goldb erg & Ric hardson, \u0001\t\b\u0007), cro wding (De Jong, \u0001\t\u0007\u0005), and lo cal mating (Collins & Je\ufffderson, \u0001\t\t\u0001). In Goldb erg's \ufffdtness sharing mo del, for example, similar individuals are forced to share a large p ortion of a single \ufffdtness v alue from the shared solution p oin t. Sharing decreases the \ufffdtness of similar individuals and causes ev olution to select against individual s in o v erp opulated nic hes. EARL metho ds that emplo y distributed p olicy represen tations ac hiev e div ersit y automatically and are w ell-suited for adaptation in dynamic en vironmen ts. In a distributed represen tation, eac h individual represen ts only a partial solution. Complete solutions are built b y com bining individuals. Because no individual can solv e the task on its o wn, the ev olutionary algorithm will searc h for sev eral complemen tary individuals that together can solv e the task. Ev olutionary pressures are therefore presen t to prev en t con v ergence of the p opulation. Moriart y and Miikkulainen (\u0001\t\t\b) sho w ed ho w the inheren t div ersit y and sp ecialization in SANE allo w it to adapt m uc h more quic kly to c hanges in the en vironmen t than standard, con v ergen t ev olutionary algorithms. Finally , if the learning system can detect c hanges in the en vironmen t, ev en more direct resp onse is p ossible. In the anytime le arning mo del (Grefenstette & Ramsey , \u0001\t\t\u0002), an EARL system main tains a case-base of p olicies, indexed b y the v alues of the en vironmen tal detectors corresp onding to the en vironmen t in whic h a giv en p olicy w as ev olv ed. When an en vironmen tal c hange is detected, the p opulation of p olicies is partially reinitialized, using previously learned p olicies selected on the basis of similarit y b et w een the previously encoun tered en vironmen t and the curren t en vironmen t. As a result, if the en vironmen t c hanges are cyclic, then the p opulation can b e immediately seeded with those p olicies in e\ufffdect during the last o ccurrence of the curren t en vironmen t. By ha ving a p opulation of p olicies, this approac h is protected against some kinds of errors in detecting en vironmen tal c hanges. F or example, ev en if a spurious en vironmen tal c hange is mistak enly detected, learning is not unduly a\ufffdected, since only a part of the curren t p opulation of p olicies is replaced b y previously learned p olicies. Zhou (\u0001\t\t0) explored a similar approac h based on LCS. \u0002\u0006\u0004 W e can distinguish t w o broad approac hes to reinforcemen t learning |online le arning and o\ufffdine le arning. In online learning, an agen t learns directly from its exp eriences in its op erational en vironmen t. F or example, a rob ot migh t learn to na vigate in a w arehouse b y actually mo ving ab out its ph ysical en vironmen t. There are t w o problems with using EARL in this situation. First, it is lik ely to require a large n um b er of exp eriences in order to ev aluate a large p opulation of p olicies. Dep ending on ho w quic kly the agen t p erforms tasks that result in some en vironmen tal feedbac k, it ma y tak e an unacceptable amoun t of time to run h undreds of generations of an EA that ev aluates h undreds or thousands of p olicies. Second, it ma y b e dangerous or exp ensiv e to p ermit an agen t to p erform some actions in its actual op erational en vironmen t that migh t cause harm to itself or its en vironmen t. Y et it is v ery lik ely that at least some p olicies that the EA generates will b e v ery bad p olicies. Both of these ob jections apply to TD metho ds as w ell. F or example, the theoretical results that pro v e the optimalit y of Q-learning require that ev ery state b e visited in\ufffdnitely often, whic h is ob viously imp ossible in practice. Lik ewise, TD metho ds ma y explore some v ery undesirable states b efore an acceptable v alue-function is found. F or b oth TD and EARL, practical considerations p oin t to w ard the use of o\ufffdine learning, in whic h the RL system p erforms its exploration on sim ulation mo dels of the en vironmen t. Sim ulation mo dels pro vide a n um b er of adv an tages for EARL, including the abilit y to p erform parallel ev aluations of all the p olicies in a p opulation sim ultaneously (Grefenstette, \u0001\t\t\u0005). \t.\u0002 Rare States The memory or record of observ ed states and rew ards di\ufffders greatly b et w een EA and TD metho ds. T emp oral di\ufffderence metho ds normally main tain statistics concerning ev ery stateaction pair. As states are revisited, the new reinforcemen t is com bined with the previous v alue. New information th us supplemen ts previous information, and the information conten t of the agen t's reinforcemen t mo del increases during exploration. In this manner, TD metho ds sustain kno wledge of b oth go o d and bad state-action pairs. As p oin ted out previously , EA metho ds normally main tain information only ab out go o d p olicies or p olicy comp onen ts. Kno wledge of bad decisions is not explicitly preserv ed, since p olicies that mak e suc h decisions are selected against b y the ev olutionary algorithm and are ev en tually eliminated from the p opulation. F or example, refer once again to T able \u0004, whic h sho ws the implicit statistics of the p opulation from T able \u0002. Note the question \u0002\u0006\u0005 uses a function appro ximator suc h as a neural net w ork as its v alue function, then it to o can su\ufffder from memory loss concerning rare states, since man y up dates from frequen tly o ccurring states can dominate the few up dates from the rare states. \t.\u0003 Pro ofs of Optimalit y One of the attractiv e features of TD metho ds is that the Q-learning algorithm has a pro of of optimalit y (W atkins & Da y an, \u0001\t\t\u0002). Ho w ev er, the practical imp ortance of this result is limited, since the assumptions underlying the pro of (e.g., no hidden states, all state visited in\ufffdnitely often) are not satis\ufffded in realistic applications. The curren t theory of ev olutionary algorithms pro vide a similar lev el of optimalit y pro ofs for restricted classes of searc h spaces (V ose & W righ t, \u0001\t\t\u0005). Ho w ev er, no general theoretical to ols are a v ailable that can b e applied to realistic RL problems. In an y case, ultimate con v ergence to an optimal p olicy ma y b e less imp ortan t in practice than e\ufffdcien tly \ufffdnding a reasonable appro ximation. A more pragmatic approac h ma y b e to ask ho w e\ufffdcien t alternativ e RL algorithms are, in terms of the n um b er of reinforcemen ts receiv ed b efore dev eloping a p olicy that is within some tolerance lev el of an optimal p olicy . In the mo del of pr ob ably appr oximately c orr e ct (P A C) learning (V alian t, \u0001\t\b\u0004), the p erformance of a learner is measured b y ho w man y learning exp eriences (e.g., samples in sup ervised learning) are required b efore con v erging to a correct h yp othesis within sp eci\ufffded error b ounds. Although dev elop ed initially for sup ervised learning, the P A C approac h has b een extended recen tly to b oth TD metho ds (Fiec h ter, \u0001\t\t\u0004) and to general EA metho ds (Ros, \u0001\t\t\u0007). These analytic metho ds are still in an early stage of dev elopmen t, but further researc h along these lines ma y one da y pro vide useful to ols for understanding the theoretical and practical adv an tages of alternativ e approac hes to RL. Un til that time, exp erimen tal studies will pro vide v aluable evidence for the utilit y of an approac h. \u00010. Examples of EARL Metho ds Finally , w e tak e a lo ok at a few signi\ufffdcan t examples of the EARL approac h and results on RL problems. Rather than attempt an exhaustiv e surv ey , w e ha v e selected four EARL systems that are represen tativ e of the div erse p olicies represen tations outlined in Section \u0005. Samuel represen ts the class of single-c hromosome rule-based EARL systems. Alecsys is an example of a distributed rule-based EARL metho d. Genitor is a single c hromosome neural-net system, and Sane is a distributed neural net system. This brief surv ey should \u0002\u0006\u0006 maps the state of the w orld to actions to b e p erformed. An example rule migh t b e: IF r ang e = [\u0003\u0005; \u0004\u0005] AND bear ing = [0; \u0004\u0005] THEN SET tur n = \u0001\u0006 (str eng th 0.\b) The use of a high-lev el language for rules o\ufffders sev eral adv an tages o v er lo w-lev el binary pattern languages t ypically adopted in genetic learning systems. First, it mak es it easier to incorp orate existing kno wledge, whether acquired from exp erts or b y sym b olic learning programs. Second, it is easier to transfer the kno wledge learned to h uman op erators. Samuel also includes mec hanisms to allo w co ev olution of m ultiple b eha viors sim ultaneously . In addition to the usual genetic op erators of crosso v er and m utation, Samuel uses more traditional mac hine learning tec hniques in the form of Lamarc kian op erators. Samuel k eeps a record of recen t exp eriences and will allo w op erators suc h as generalization, sp ecialization, co v ering, and deletion to mak e informed c hanges to the individual genes (rules) based on these exp eriences. Samuel has b een used successfully in man y reinforcemen t learning applications. Here w e will brie\ufffdy describ e three examples of learning complex b eha viors for real rob ots. In these applications of Samuel, learning is p erformed under sim ulation, re\ufffdecting the fact that during the initial phases of learning, con trolling a real system can b e exp ensiv e or dangerous. Learned b eha viors are then tested on the on-line system. In (Sc h ultz & Grefenstette, \u0001\t\t\u0002; Sc h ultz, \u0001\t\t\u0004; Sc h ultz & Grefenstette, \u0001\t\t\u0006), Samuel is used to learn collision a v oidance and lo cal na vigation b eha viors for a Nomad \u000200 mobile rob ot. The sensors a v ailable to the learning task w ere \ufffdv e sonars, \ufffdv e infrared sensors, and the range and b earing to the goal, and the curren t sp eed of the v ehicle. Samuel learned a mapping from those sensors to the con trollable actions { a turning rate and a translation rate for the wheels. Samuel to ok a h uman-written rule set that could reac h the goal within a limited time without hitting an obstacle only \u00070 p ercen t of the time, and after \u00050 generations w as able to obtain a \t\u0003.\u0005 p ercen t success rate. In (Sc h ultz & Grefenstette, \u0001\t\t\u0006), the rob ot learned to herd a second rob ot to a \\pasture\". In this task, the learning system used the range and b earing to the second rob ot, the heading of the second rob ot, and the range and b earing to the goal, as its input sensors. The system learned a mapping from these sensors to a turning rate and steering rate. In these exp erimen ts, success w as measured as the p ercen tage of times that the rob ot could maneuv er the second rob ot to the goal within a limited amoun t of time. The second rob ot implemen ted a random w alk, plus a b eha vior that made it a v oid an y nearb y obstacles. The \ufffdrst rob ot learned to exploit this to ac hiev e its goal of mo ving the second rob ot to the goal. \u0002\u0006\u0007 and the en vironmen t. The learning mo dule con tin uously tests new strategies for the agen t against the sim ulation mo del, using a genetic algorithm to ev olv e impro v ed strategies, and up dates the kno wledge base used b y the execution mo dule with the b est a v ailable results. Whenev er the sim ulation mo del is mo di\ufffded due to some observ ed c hange in the agen t or the en vironmen t, the genetic algorithm is restarted on the mo di\ufffded mo del. The learning system op erates inde\ufffdnitely , and the execution system uses the results of learning as they b ecome a v ailable. The w ork with Samuel sho ws that the EA metho d is particularly w ell-suited for an ytime learning. Previously learned strategies can b e treated as cases, indexed b y the set of conditions under whic h they w ere learned. When a new situation is encoun tered, a nearest neigh b or algorithm is used to \ufffdnd the most similar previously learned cases. These nearest neigh b ors are used to re-initialize the genetic p opulation of p olicies for the new case. Grefenstette (\u0001\t\t\u0006) rep orts on exp erimen ts in whic h a mobile rob ot learns to trac k another rob ot, and dynamically adapts its p olicies using an ytime learning as its encoun ters a series of partial system failures. This approac h blurs the line b et w een online and o\ufffdine learning, since the online system is b eing up dated whenev er the o\ufffdine learning system dev elops an impro v ed p olicy . In fact, the o\ufffdine learning system can ev en b e executed on-b oard the op erating mobile rob ot. \u00010.\u0002 Alecsys As describ ed previously , Alecsys (Dorigo & Colom b etti, \u0001\t\t\b) is a distributed rule-based EA that supp orts an approac h to the design of autonomous systems called b ehavior al engine ering. In this approac h, the tasks to b e p erformed b y a complex autonomous systems are decomp osed in to individual b eha viors, eac h of whic h is learned via a learning classi\ufffder systems mo dule, as sho wn in Figure \t. The decomp osition is p erformed b y the h uman designer, so the \ufffdtness function asso ciated with eac h LCS can b e carefully designed to re\ufffdect the role of the asso ciated comp onen t b eha vior within the o v erall autonomous system. F urthermore, the in teractions among the mo dules is also preprogrammed. F or example, the designer ma y decide that the rob ot should learn to approac h a goal except when a threatening predator is near, in whic h case the rob ot should ev ade the predator. The o v erall arc hitecture of the set of b eha viors can then b e set suc h that the ev asion b eha vior has higher priorit y than the goal-seeking b eha vior, but the individual LCS mo dules can ev olv e decision rules for optimally p erforming the subtasks. Alecsys has b een used to dev elop b eha vioral rules for a n um b er of b eha viors for autonomous rob ots, including complex b eha vior groups suc h as Chase/Feed/Escape \u0002\u0006\b Genitor relies solely on its ev olutionary algorithm to adjust the w eigh ts in neural net w orks. In solving RL problems, eac h mem b er of the p opulation in Genitor represen ts a neural net w ork as a sequence of connection w eigh ts. The w eigh ts are concatenated in a realv alued c hromosome along with a gene that represen ts a crosso v er probabilit y . The crosso v er gene determines whether the net w ork is to b e m utated (randomly p erturb ed) or whether a crosso v er op eration (recom bination with another net w ork) is to b e p erformed. The crosso v er gene is mo di\ufffded and passed to the o\ufffdspring based on the o\ufffdspring's p erformance compared to the paren t. If the o\ufffdspring outp erforms the paren t, the crosso v er probabilit y is decreased. Otherwise, it is increased. Whitley et al. refer to this tec hnique as adaptive mutation, whic h tends to increase the m utation rate as p opulations con v erge. Essen tially , this metho d promotes div ersit y within the p opulation to encourage con tin ual exploration of the solution space. Genitor also uses a so-called \\steady-state\" genetic algorithm in whic h new paren ts are selected and genetic op erators are applied after eac h individual is ev aluated. This approac h con trasts with \\generational\" GAs in whic h the en tire p opulation is ev aluated and replaced during eac h generation. In a steady-state GA, eac h p olicy is ev aluated just once and retains this same \ufffdtness v alue inde\ufffdnitely . Since p olicies with lo w er \ufffdtness are more lik ely to b e replaced, it is p ossible that a \ufffdtness based on a noisy ev aluation function ma y ha v e an undesirable in\ufffduence on the direction of the searc h. In the case of the p ole-balancing RL application, the \ufffdtness v alue dep ends on the length of time that the p olicy can main tain a go o d balance, giv en a randomly c hosen initial state. The \ufffdtness is therefore a random v ariable that dep ends on the initial state. The authors b eliev e that noise in the \ufffdtness function had little negativ e impact on learning go o d p olicies, p erhaps b ecause it w as more di\ufffdcult for p o or net w orks to obtain a go o d \ufffdtness than for go o d net w orks (of whic h there w ere man y copies in the p opulation) to surviv e an o ccasional bad \ufffdtness ev aluation. This is an in teresting general issue in EARL that needs further analysis. Genitor adopts some sp eci\ufffdc mo di\ufffdcation for its RL applications. First, the representation uses a real-v alued c hromosome rather than a bit-string represen tation for the w eigh ts. Consequen tly , Genitor alw a ys recom bines p olicies b et w een w eigh t de\ufffdnitions, th us reducing p oten tially random disruption of neural net w ork w eigh ts that migh t result if crosso v er op erations o ccurred in the middle of a w eigh t de\ufffdnition. The second mo di\ufffdcation is a v ery high m utation rate whic h helps to main tain div ersit y and promote rapid exploration of the p olicy space. Finally , Genitor uses un usually small p opulations in order to discourage di\ufffderen t, comp eting neural net w ork \\sp ecies\" from forming within the p opulation. Whit\u0002\u0006\t to b e that the EA tends to ignore those cases where the p ole cannot b e balanced, and concen trate on successful cases. This serv es as another example of the adv an tages asso ciated with searc h in p olicy space, based on o v erall p olicy p erformance, rather than pa ying to o m uc h atten tion to the v alue asso ciated with individual states. \u00010.\u0004 Sane The Sane (Sym biotic, Adaptiv e Neuro-Ev olution) system w as designed as a e\ufffdcien t metho d for building arti\ufffdcial neural net w orks in RL domains where it is not p ossible to generate training data for normal sup ervised learning (Moriart y & Miikkulainen, \u0001\t\t\u0006a, \u0001\t\t\b). The Sane system uses an ev olutionary algorithm to form the hidden la y er connections and w eigh ts in a neural net w ork. The neural net w ork forms a direct mapping from sensors to actions and pro vides e\ufffdectiv e generalization o v er the state space. Sane's only metho d of credit assignmen t is through the EA, whic h allo ws it to apply to man y problems where reinforcemen t is sparse and co v ers a sequence of decisions. As describ ed previously , Sane uses a distributed represen tation for p olicies. Sane o\ufffders t w o imp ortan t adv an tages for reinforcemen t learning that are normally not presen t in other implemen tations of neuro-ev olution. First, it main tains div erse p opulations. Unlik e the canonical function optimization EA that con v erge the p opulation on a single solution, Sane forms solutions in an unc onver ge d p opulation. Because sev eral di\ufffderen t t yp es of neurons are necessary to build an e\ufffdectiv e neural net w ork, there is inheren t ev olutionary pressure to dev elop neurons that p erform di\ufffderen t functions and th us main tain sev eral differen t t yp es of individual s within the p opulation. Div ersit y allo ws recom bination op erators suc h as crosso v er to con tin ue to generate new neural structures ev en in prolonged ev olution. This feature helps ensure that the solution space will b e explored e\ufffdcien tly throughout the learning pro cess. Sane is therefore more resilien t to sub optimal con v ergence and more adaptiv e to c hanges in the domain. The second feature of Sane is that it explicitly decomp oses the searc h for complete solutions in to a searc h for partial solutions. Instead of searc hing for complete neural net w orks all at once, solutions to smaller problems (go o d neurons) are ev olv ed, whic h can b e combined to form an e\ufffdectiv e full solution (a neural net w ork). In other w ords, Sane e\ufffdectiv ely p erforms a problem reduction searc h on the space of neural net w orks. Sane has b een sho wn e\ufffdectiv e in sev eral di\ufffderen t large scale problems. In one problem, Sane ev olv ed neural net w orks to direct or fo cus a minimax game-tree searc h (Moriart y \u0002\u00070 training examples is extremely di\ufffdcult. A reinforcemen t learning approac h, ho w ev er, do es not require examples of correct b eha vior and can learn the in termediate mo v emen ts from general reinforcemen ts. Sane w as implemen ted to form neuro-con trol net w orks capable of maneuv ering the OSCAR-\u0006 rob ot arm among obstacles to reac h random target lo cations. Giv en b oth camera-based visual and infrared sensory input, the neural net w orks learned to e\ufffdectiv ely com bine b oth target reac hing and obstacle a v oidance strategies. F or further related examples of ev olutionary metho ds for learning neural-net con trol systems for rob otics, the reader should see (Cli\ufffd, Harv ey , & Husbands, \u0001\t\t\u0003; Husbands, Harv ey , & Cli\ufffd, \u0001\t\t\u0005; Y amauc hi & Beer, \u0001\t\t\u0003). \u0001\u0001. Summary This article b egan b y suggesting t w o distinct approac hes to solving reinforcemen t learning problems; one can searc h in v alue function space or one can searc h in p olicy space. TD and EARL are examples of these t w o complemen tary approac hes. Both approac hes assume limited kno wledge of the underlying system and learn b y exp erimen ting with di\ufffderen t p olicies and using reinforcemen t to alter those p olicies. Neither approac h requires a precise mathematical mo del of the domain, and b oth ma y learn through direct in teractions with the op erational en vironmen t. Unlik e TD metho ds, EARL metho ds generally base \ufffdtness on the o v erall p erformance of a p olicy . In this sense, EA metho ds pa y less atten tion to individual decisions than TD metho ds do. While at \ufffdrst glance, this approac h app ears to mak e less e\ufffdcien t use of information, it ma y in fact pro vide a robust path to w ard learning go o d p olicies, esp ecially in situations where the sensors are inadequate to observ e the true state of the w orld. It is not useful to view the path to w ard practical RL systems as a c hoice b et w een EA and TD metho ds. W e ha v e tried to highligh t some of the strengths of the ev olutionary approac h, but w e ha v e also sho wn that EARL and TD, while complemen tary approac hes, are b y no means m utually exclusiv e. W e ha v e cited examples of successful EARL systems suc h as Samuel and Alecsys that explicitly incorp orate TD elemen ts in to their m ultilev el credit assignmen t metho ds. It is lik ely that man y practical applications will dep end on these kinds of m ulti-strategy approac hes to mac hine learning. W e ha v e also listed a n um b er of areas that need further w ork, particularly on the theoretical side. In RL, it w ould b e highly desirable to ha v e a b etter to ols for predicting the amoun t of exp erience needed b y a learning agen t b efore reac hing a sp eci\ufffded lev el of p er\u0002\u0007\u0001 IEEE Contr ol Systems Magazine, \t, \u0003\u0001{\u0003\u0007. Barto, A. G., Sutton, R. S., & W atkins, C. J. C. H. (\u0001\t\t0). Learning and sequen tial decision making. In Gabriel, M., & Mo ore, J. W. (Eds.), L e arning and Computational Neur oscienc e. MIT Press, Cam bridge, MA. Belew, R. K., McInerney , J., & Sc hraudolph, N. N. (\u0001\t\t\u0001). Ev olving net w orks: Using the genetic algorithm with connectionist learning. In F armer, J. D., Langton, C., Rasm ussen, S., & T a ylor, C. (Eds.), A rti\ufffdcial Life II Reading, MA. Addison-W esley . Chrisman, L. (\u0001\t\t\u0002). Reinforcemen t learning with p erceptual aliasing: The p erceptual distinctions approac h. In Pr o c e e dings of the T enth National Confer enc e on A rti\ufffdcial Intel ligenc e, pp. \u0001\b\u0003{\u0001\b\b San Jose, CA. Cli\ufffd, D., Harv ey , I., & Husbands, P . (\u0001\t\t\u0003). Explorations in ev olutionary rob otics. A daptive Behavior, \u0002, \u0007\u0003{\u0001\u00010. Cobb, H. G., & Grefenstette, J. J. (\u0001\t\t\u0003). Genetic algorithms for trac king c hanging en vironmen ts. In Pr o c. Fifth International Confer enc e on Genetic A lgorithms, pp. \u0005\u0002\u0003{\u0005\u00030. Collins, R. J., & Je\ufffderson, D. R. (\u0001\t\t\u0001). Selection in massiv ely parallel genetic algorithms. In Pr o c e e dings of the F ourth International Confer enc e on Genetic A lgorithms, pp. \u0002\u0004\t{\u0002\u0005\u0006 San Mateo, CA. Morgan Kaufmann. Da y an, P ., & Sejno wski, T. J. (\u0001\t\t\u0006). Exploration b on uses and dual con trol. Machine L e arning, \u0002\u0005 (\u0001), \u0005{\u0002\u0002. De Jong, K. A. (\u0001\t\u0007\u0005). A n A nalysis of the Behavior of a Class of Genetic A daptive Systems. Ph.D. thesis, The Univ ersit y of Mic higan, Ann Arb or, MI. Dorigo, M., & Colom b etti, M. (\u0001\t\t\b). R ob ot Shaping: A n Exp eriment in Behavior al Engine ering. MIT Press, Cam bridge, MA. Fiec h ter, C.-N. (\u0001\t\t\u0004). E\ufffdcien t reinforcemen t learning. In Pr o c e e dings of the Seventh A nnual A CM Confer enc e on Computational L e arning The ory, pp. \b\b{\t\u0007. Asso ciation for Computing Mac hinery . F ogel, L. J., Ow ens, A. J., & W alsh, M. J. (\u0001\t\u0006\u0006). A rti\ufffdcial Intel ligenc e thr ough Simulate d Evolution. Wiley Publishing, New Y ork. \u0002\u0007\u0002 In Da vis, L. (Ed.), Genetic A lgorithms and Simulate d A nne aling, pp. \u0004\u0002{\u00060 San Mateo, CA. Morgan Kaufmann. Grefenstette, J. J. (\u0001\t\b\b). Credit assignmen t in rule disco v ery system based on genetic algorithms. Machine L e arning, \u0003 (\u0002/\u0003), \u0002\u0002\u0005{\u0002\u0004\u0005. Grefenstette, J. J. (\u0001\t\t\u0002). Genetic algorithms for c hanging en vironmen ts. In M\ufffd anner, R., & Manderic k, B. (Eds.), Par al lel Pr oblem Solving fr om Natur e, \u0002, pp. \u0001\u0003\u0007{\u0001\u0004\u0004. Grefenstette, J. J. (\u0001\t\t\u0005). Rob ot learning with parallel genetic algorithms on net w ork ed computers. In Pr o c e e dings of the \u0001\t\t\u0005 Summer Computer Simulation Confer enc e (SCSC '\t\u0005), pp. \u0003\u0005\u0002{\u0002\u0005\u0007. Grefenstette, J. J. (\u0001\t\t\u0006). Genetic learning for adaptation in autonomous rob ots. In R ob otics and Manufacturing: R e c ent T r ends in R ese ar ch and Applic ations, V olume \u0006, pp. \u0002\u0006\u0005{ \u0002\u00070. ASME Press, New Y ork. Grefenstette, J. J. (\u0001\t\t\u0007a). Prop ortional selection and sampling algorithms. In Handb o ok of Evolutionary Computation, c hap. C\u0002.\u0002. IOP Publishing and Oxford Univ ersit y Press. Grefenstette, J. J. (\u0001\t\t\u0007b). Rank-based selection. In Handb o ok of Evolutionary Computation, c hap. C\u0002.\u0004. IOP Publishing and Oxford Univ ersit y Press. Grefenstette, J. J., & Ramsey , C. L. (\u0001\t\t\u0002). An approac h to an ytime learning. In Pr o c. Ninth International Confer enc e on Machine L e arning, pp. \u0001\b\t{\u0001\t\u0005 San Mateo, CA. Morgan Kaufmann. Grefenstette, J. J., Ramsey , C. L., & Sc h ultz, A. C. (\u0001\t\t0). Learning sequen tial decision rules using sim ulation mo dels and comp etition. Machine L e arning, \u0005, \u0003\u0005\u0005{\u0003\b\u0001. Holland, J. H. (\u0001\t\u0007\u0005). A daptation in Natur al and A rti\ufffdcial Systems: An Intr o ductory A nalysis with Applic ations to Biolo gy, Contr ol and A rti\ufffdcial Intel ligenc e. Univ ersit y of Mic higan Press, Ann Arb or, MI. Holland, J. H. (\u0001\t\b\u0006). Escaping brittleness: The p ossibilities of general-purp ose learning algorithms applied to parallel rule-based systems. In Machine L e arning: A n A rti\ufffdcial Intel ligenc e Appr o ach, V ol. \u0002. Morgan Kaufmann, Los Altos, CA. \u0002\u0007\u0003 Koza, J. R. (\u0001\t\t\u0002). Genetic Pr o gr amming: On the Pr o gr amming of Computers by Me ans of Natur al Sele ction. MIT Press, Cam bridge, MA. Lee, K.-F., & Maha jan, S. (\u0001\t\t0). The dev elopmen t of a w orld class Othello program. A rti\ufffdcial Intel ligenc e, \u0004\u0003, \u0002\u0001{\u0003\u0006. Lin, L.-J., & Mitc hell, T. M. (\u0001\t\t\u0002). Memory approac hes to reinforcemen t learning in nonMark o vian domains. T ec h. rep. CMU-CS-\t\u0002-\u0001\u0003\b, Carnegie Mellon Univ ersit y , Sc ho ol of Computer Science. McCallum, A. K. (\u0001\t\t\u0005). R einfor c ement L e arning with Sele ctive Per c eption and Hidden State. Ph.D. thesis, The Univ ersit y of Ro c hester. Moriart y , D. E., & Miikkulainen, R. (\u0001\t\t\u0004). Ev olving neural net w orks to fo cus minimax searc h. In Pr o c e e dings of the Twelfth National Confer enc e on A rti\ufffdcial Intel ligenc e (AAAI-\t\u0004), pp. \u0001\u0003\u0007\u0001{\u0001\u0003\u0007\u0007 Seattle, W A. MIT Press. Moriart y , D. E., & Miikkulainen, R. (\u0001\t\t\u0006a). E\ufffdcien t reinforcemen t learning through sym biotic ev olution. Machine L e arning, \u0002\u0002, \u0001\u0001{\u0003\u0002. Moriart y , D. E., & Miikkulainen, R. (\u0001\t\t\u0006b). Ev olving obstacle a v oidance b eha vior in a rob ot arm. In F r om A nimals to A nimats: Pr o c e e dings of the F ourth International Confer enc e on Simulation of A daptive Behavior (SAB-\t\u0006), pp. \u0004\u0006\b{\u0004\u0007\u0005 Cap e Co d, MA. Moriart y , D. E., & Miikkulainen, R. (\u0001\t\t\b). F orming neural net w orks through e\ufffdcien t and adaptiv e co-ev olution. Evolutionary Computation, \u0005 (\u0004), \u0003\u0007\u0003{\u0003\t\t. P otter, M. A. (\u0001\t\t\u0007). The Design and A nalysis of a Computational Mo del of Co op er ative Co evolution. Ph.D. thesis, George Mason Univ ersit y . P otter, M. A., & De Jong, K. A. (\u0001\t\t\u0005). Ev olving neural net w orks with collab orativ e sp ecies. In Pr o c e e dings of the \u0001\t\t\u0005 Summer Computer Simulation Confer enc e Otta w a, Canada. P otter, M. A., De Jong, K. A., & Grefenstette, J. (\u0001\t\t\u0005). A co ev olutionary approac h to learning sequen tial decision rules. In Eshelman, L. (Ed.), Pr o c e e dings of the Sixth International Confer enc e on Genetic A lgorithms Pittsburgh, P A. \u0002\u0007\u0004 pp. \u0005\u0001{\u00060. Morgan Kaufmann. Sc ha\ufffder, J. D., & Grefenstette, J. J. (\u0001\t\b\u0005). Multi-ob jectiv e learning via genetic algorithms. In Pr o c e e dings of the Ninth International Joint Confer enc e on A rti\ufffdcial Intel ligenc e, pp. \u0005\t\u0003{\u0005\t\u0005. Morgan Kaufmann. Sc h ultz, A. C. (\u0001\t\t\u0004). Learning rob ot b eha viors using genetic algorithms. In Intel ligent A utomation and Soft Computing: T r ends in R ese ar ch, Development, and Applic ations, pp. \u00060\u0007{\u0006\u0001\u0002. TSI Press, Albuquerque. Sc h ultz, A. C., & Grefenstette, J. J. (\u0001\t\t\u0002). Using a genetic algorithm to learn b eha viors for autonomous v ehicles. In Pr o c e e dings of the A iAA Guidanc e, Navigation, and Contr ol Confer enc e Hilton Head, SC. Sc h ultz, A. C., & Grefenstette, J. J. (\u0001\t\t\u0006). Rob o-shepherd: Learning complex rob otic b eha viors. In R ob otics and Manufacturing: R e c ent T r ends in R ese ar ch and Applic ations, V olume \u0006, pp. \u0007\u0006\u0003{\u0007\u0006\b. ASME Press, New Y ork. Smith, S. F. (\u0001\t\b\u0003). Flexible learning of problem solving heuristics through adaptiv e searc h. In Pr o c e e dings of the Eighth International Joint Confer enc e on A rti\ufffdcial Intel ligenc e, pp. \u0004\u0002\u0002{\u0004\u0002\u0005. Morgan Kaufmann. Sutton, R. (\u0001\t\t0). In tegrated arc hitectures for learning, planning, and reacting based on appro ximate dynamic programming. In Machine L e arning: Pr o c e e dings of the Seventh International Confer enc e, pp. \u0002\u0001\u0006{\u0002\u0002\u0004. Sutton, R. S. (\u0001\t\b\b). Learning to predict b y the metho ds of temp oral di\ufffderences. Machine L e arning, \u0003, \t{\u0004\u0004. Sutton, R. S., & Barto, A. (\u0001\t\t\b). R einfor c ement L e arning: A n Intr o duction. MIT Press, Cam bridge, MA. V alian t, L. G. (\u0001\t\b\u0004). A theory of the learnable. Communic ations of the A CM, \u0002\u0007, \u0001\u0001\u0003\u0004{ \u0001\u0001\u0004\u0002. V ose, M. D., & W righ t, A. H. (\u0001\t\t\u0005). Simple genetic algorithms with linear \ufffdtness. Evolutionary Computation, \u0002, \u0003\u0004\u0007{\u0003\u0006\b. \u0002\u0007\u0005 Whitley , D., Dominic, S., Das, R., & Anderson, C. W. (\u0001\t\t\u0003). Genetic reinforcemen t learning for neuro con trol problems. Machine L e arning, \u0001\u0003, \u0002\u0005\t{\u0002\b\u0004. Wilson, S. W. (\u0001\t\t\u0004). ZCS: A zeroth lev el classi\ufffder system. Evolutionary Computation, \u0002 (\u0001), \u0001{\u0001\b. Y amauc hi, B. M., & Beer, R. D. (\u0001\t\t\u0003). Sequen tial b eha vior and learning in ev olv ed dynamical neural net w orks. A daptive Behavior, \u0002, \u0002\u0001\t{\u0002\u0004\u0006. Zhou, H. (\u0001\t\t0). CSM: A computational mo del of cum ulativ e learning. Machine L e arning, \u0005 (\u0004), \u0003\b\u0003{\u00040\u0006. \u0002\u0007\u0006 ",
    "title": "Policy",
    "paper_info": "Policy i:\nci1 \u2192 ai1\nci2 \u2192 ai2\nci3 \u2192 ai3\n...\ncik \u2192 aik\nPolicy i:\nw1\nw2\nw3\n...\nwk\n=>\n...\nw1\nw j\nwk\u22121\nwk\nFigure\n\u0007:\nA\nsimple\nparameter\nrepresen\ntation\nof\nw\neigh\nts\nfor\na\nneural\nnet\nw\nork.\nThe\n\ufffdtness\nof\nthe\np\nolicy\nis\nthe\npa\ny\no\ufffd\nwhen\nthe\nagen\nt\nuses\nthe\ncorresp\nonding\nneural\nnet\nas\nits\ndecision\np\nolicy\n.\nspaces,\nit\nis\nmore\nreasonable\nto\nrepresen\nt\na\np\nolicy\nas\na\nset\nof\ncondition-action\nrules\nin\nwhic\nh\nthe\ncondition\nexpresses\na\npredicate\nthat\nmatc\nhes\na\nset\nof\nstates,\nas\nsho\nwn\nin\nFigure\n\u0006.\nEarly\nexamples\nof\nthis\nrepresen\ntation\ninclude\nthe\nsystems\nLS-\u0001\n(Smith,\n\u0001\t\b\u0003)\nand\nLS-\u0002\n(Sc\nha\ufffder\n&\nGrefenstette,\n\u0001\t\b\u0005),\nfollo\nw\ned\nlater\nb\ny\nSamuel\n(Grefenstette\net\nal.,\n\u0001\t\t0).\n\u0005.\u0001.\u0002\nNeural\nNet\nRepresent\na\ntion\nof\nPolicies\nAs\nin\nTD-based\nRL\nsystems,\nEARL\nsystems\noften\nemplo\ny\nneural\nnet\nrepresen\ntations\nas\nfunction\nappro\nximators.\nIn\nthe\nsimplest\ncase\n(see\nFigure\n\u0007),\na\nneural\nnet\nw\nork\nfor\nthe\nagen\nt's\ndecision\np\nolicy\nis\nrepresen\nted\nas\na\nsequence\nof\nreal-v\nalued\nconnection\nw\neigh\nts.\nA\nstraigh\ntforw\nard\nEA\nfor\nparameter\noptimization\ncan\nb\ne\nused\nto\noptimize\nthe\nw\neigh\nts\nof\nthe\nneural\nnet\nw\nork\n(Belew,\nMcInerney\n,\n&\nSc\nhraudolph,\n\u0001\t\t\u0001;\nWhitley\n,\nDominic,\nDas,\n&\nAnderson,\n\u0001\t\t\u0003;\nY\namauc\nhi\n&\nBeer,\n\u0001\t\t\u0003).\nThis\nrepresen\ntation\nth\nus\nrequires\nthe\nleast\nmo\ndi\ufffdcation\nof\nthe\nstandard\nEA.\nW\ne\nno\nw\nturn\nto\ndistributed\nrepresen\ntations\nof\np\nolicies\nin\nEARL\nsystems.\n\u0005.\u0002\nDistributed\nRepresen\ntation\nof\nP\nolicies\nIn\nthe\nprevious\nsection\nw\ne\noutlined\nEARL\napproac\nhes\nthat\ntreat\nthe\nagen\nt's\ndecision\np\nolicy\nas\na\nsingle\ngenetic\nstructure\nthat\nev\nolv\nes\no\nv\ner\ntime.\nThis\nsection\naddresses\nEARL\napproac\nhes\nthat\ndecomp\nose\na\ndecision\np\nolicy\nin\nto\nsmaller\ncomp\nonen\nts.\nSuc\nh\napproac\nhes\nha\nv\ne\nt\nw\no\np\noten\ntial\nadv\nan\ntages.\nFirst,\nthey\nallo\nw\nev\nolution\nto\nw\nork\nat\na\nmore\ndetailed\nlev\nel\nof\nthe\ntask,\ne.g.,\non\nsp\neci\ufffdc\nsubtasks.\nPresumably\n,\nev\nolving\na\nsolution\nto\na\nrestricted\nsubtask\nshould\nb\ne\n\u0002\u0005\u0001\n",
    "GPTsummary": "- (1): The research background of this article is in reinforcement learning, which involves searching for optimal strategies to control an agent in uncertain environments.\n \n- (2): There are two main approaches to reinforcement learning: searching in the space of value functions and searching in the space of policies. Existing methods like temporal difference methods have limitations in scaling to realistic tasks with large state spaces, partially observable states, rarely occurring states, and non-stationary environments. The evolutionary algorithm approach to reinforcement learning is well motivated due to its ability to handle complex spaces and its parallelizable nature.\n \n- (3): The research methodology proposed in this paper is the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators.\n  \n- (4): The methods in this paper are evaluated on a variety of reinforcement learning tasks, and the performance achieved supports their goals. The strengths and weaknesses of the evolutionary approach to reinforcement learning are also presented, along with a survey of representative applications.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This work has significant value in the field of reinforcement learning by proposing a new methodology using evolutionary algorithms to solve increasingly complex environments.\n\n- (2): Innovation point: The use of evolutionary algorithms to solve reinforcement learning problems is innovative and has the potential to handle large state spaces, partial observability, rare states, and non-stationary environments. \n\nPerformance: The methods proposed in this paper were evaluated on different tasks, and the achieved performance supports its goals. However, more thorough comparison with other methods is needed.\n\nWorkload: The implementation and evaluation of the proposed methodology requires a significant amount of computational resources, especially for larger tasks.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This work has significant value in the field of reinforcement learning by proposing a new methodology using evolutionary algorithms to solve increasingly complex environments.\n\n- (2): Innovation point: The use of evolutionary algorithms to solve reinforcement learning problems is innovative and has the potential to handle large state spaces, partial observability, rare states, and non-stationary environments. \n\nPerformance: The methods proposed in this paper were evaluated on different tasks, and the achieved performance supports its goals. However, more thorough comparison with other methods is needed.\n\nWorkload: The implementation and evaluation of the proposed methodology requires a significant amount of computational resources, especially for larger tasks.\n\n\n"
}