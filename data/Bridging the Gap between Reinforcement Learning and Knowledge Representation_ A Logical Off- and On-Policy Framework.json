{
    "Abstract": "Abstract. Knowledge Representation is important issue in  reinforcement learning. In this paper, we bridge the gap between  reinforcement learning and knowledge representation, by providing  a rich knowledge representation framework, based on normal logic  programs with answer set semantics, that is capable of solving  model-free reinforcement learning problems for more complex domains and exploits the domain-specific knowledge. We prove the  correctness of our approach. We show that the complexity of  finding an offline and online policy for a model-free reinforcement  learning problem in our approach is NP-complete. Moreover, we  show that any model-free reinforcement learning problem in MDP  environment can be encoded as a SAT problem. The importance of  that is model-free reinforcement learning problems can be now  solved as SAT problems.    1     Introduction    Reinforcement learning is the problem of learning to act by trial and  error interaction in dynamic environments. Under the assumption that a  complete model of the environment is known, a reinforcement learning  problem is modeled as a Markov Decision Process (MDP), in which an  optimal policy can be learned. Operation research methods, in  particular dynamic programming by value iteration, have been  extensively used to learn the optimal policy for a reinforcement learning problem in MDP environment. However, an agent may not know  the model of the environment. In addition, an agent may not be able to  consider all possibilities and use its knowledge to plan ahead, because  of the agent\u2019s limited computational abilities to consider all states  systematically [4]. Therefore, Q-learning [4] and SARSA [17] are  proposed as model-free reinforcement learning algorithms that learn  optimal policies without the need for the agent to know the model of  the environment.    Q-learning and SARSA are incremental dynamic programming  algorithms, that learns optimal policy from actual experience from  interaction with the environment, where to guarantee convergence the  following assumptions must hold; the action-value function is  represented as a look-up table; the environment is a deterministic MDP;  for each starting state and action, there are an infinite number of  episodes; and the learning rate is decreased appropriately over time.  However, these assumptions imply that all actions are tried in every  possible state and every state must be visited infinitely many times,  which leads to a slow convergence, although, it is sufficient for the  agent to try all possible actions in every possible state only    1 Department of Computer Science, Gulf University for Science and Technology, Kuwait, email: saad.e@gust.edu.kw     once to learn about the reinforcements resulting from executing  actions in states. In addition, in some situations it is not possible for  the agent to visit a state more than once. Consider a deer that eats in  an area where a cheetah appears and the deer flees and survived. If  the deer revisits this area again it will be eaten and does not learn  anymore. This is unavoidable in Q-learning and SARSA because of  the iterative dynamic programming approach they adopt and their  convergence assumptions. Moreover, dynamic programming methods use primitive representation of states and actions and do not exploit domain-specific knowledge of the problem domain, in  addition they solve MDP with relatively small domain sizes [16].  However, using richer knowledge representation frameworks for  MDP allow to efficiently find optimal policies in more complex  and larger domains.    A logical framework to model-based reinforcement learning has  been proposed in [19] that overcomes the representational  limitations of dynamic programming methods and capable of  representing domain specific knowledge. The framework in [19] is  based on the integration of model-based reinforcement learning in  MDP environment with normal hybrid probabilistic logic programs  with probabilistic answer set semantics [23] that allows  representing and reasoning about a variety of fundamental  probabilistic reasoning problems including probabilistic planning  [18], contingent probabilistic planning [21], the most probable  explanation in belief networks, and the most likely trajectory [20].    In this paper we integrate model-free reinforcement learning with  normal logic programs with answer set semantics and SAT, providing a  logical framework to model-free reinforcement learning using Qlearning and SARSA update rules to learn the optimal off- and onpolicy respectively. This framework is considered a model-free  extension to the model-based reinforcement learning framework of  [19]. The importance of the proposed framework is twofold. First, the  proposed framework overcomes the representational limitations of  dynamic programming methods to model-free reinforcement learning  and capable of representing domain-specific knowledge, and hence  bridges the gap between reinforcement learning and knowledge  representation. Second, it eliminates the requirement of visiting every  state infinitely many times which is required for the convergence of the  Q-learning and SARSA.    This integration is achieved by encoding the representation of a  model-free reinforcement learning problem in a new high level action  language we develop in this paper called,  , into normal logic  program with answer set semantics, where all actions are tried in every  state only once. We show the correctness of the translation. We prove  that the complexity of finding an off- and on-policy in our ap-    proach is NP-complete. In addition, we show that any model-free  reinforcement learning problem in MDP environment can be  encoded as SAT problem.    2     Preliminaries    As in the underlying assumptions of the original Q-learning and  SARSA, the subsequent results in the rest of this paper assume that  the considered MDPs are deterministic. Normal logic programs [7]  and Q-learning [4] and SARSA [17] are reviewed in this section.    2.1     Normal Logic Programs    Let   be a first-order language with finitely many predicate  symbols, function symbols, constants, and infinitely many  variables. The Herbrand base of   is denoted by  . A Herbrand  interpretation is a subset of the Herbrand base  . A normal logic  program is a finite set of rules of the form    Where   are atoms and   is the  negation-as-failure. A normal logic program is ground if no  variables appear in any of its rules. Let   be a ground normal logic  program and  be a Herbrand interpretation, then, we say that the  above rule is satisfied by  iff  , whenever    and  ,  or  for  some  ,      A Herbrand model of   is a Herbrand interpretation that satisfies  every rule in  . A Herbrand interpretation   of a normal logic program   is said to be an answer set of   if  is the minimal Herbrand  model (with respect to the set inclusion) of the reduct, denoted by  , where           2.2     Q-learning and SARSA    Q-learning learns the optimal Q-function, , from the agent\u2019s experience (set of episodes) by repeatedly estimating the optimal Qvalue for every state-action pair  . The Q-value,  ,  given a policy (a mapping from states to actions), is defined as the  expected sum of discounted rewards resulting from executing the  action  in a state  and then following the policy thereafter. Given  , an optimal policy,  , can be determined by identifying the  optimal action in every state, where  is optimal in a state , i.e.,    and    is  executable in . An episode is an exploration of the environment  which is a sequence of state-action-reward-state of the form  , where for each   means that an agent executed  action   in state   and rests in state   where it received reward  .   denotes an initial state and   is a terminal (goal) state. Given  that the agent sufficiently explored the environment, the optimal Qvalues are repeatedly estimated by the following algorithm:     initialize   arbitrary  Repeat forever for each episode    Select the initial state  of an episode    Repeat  Choose an action   for the current state   Execute the action   in     Observe the subsequent state      Receive an immediate reward      Set      Until   is the end of an episode    where   is the learning rate,   is the discount factor, and   is the reward received in   from executing   in  . Q-learning is an offline algorithm that learns the optimal Qfunction while executing another policy. Under the same convergence assumptions as in Q-learning, SARSA [17] has been developed as an online model-free reinforcement learning algorithm, that  learns optimal Q-function while exploring the environment. Similar  to Q-learning, SARSA is an iterative dynamic programming algorithm whose update rule is given by:      In addition, SARSA converges slowly to Q\u2217, since it requires every state to be visited infinitely many times with all actions are  tried. Although, it is sufficient for an agent to try all possible  actions in every possible state only once to learn about the  reinforcements resulting from executing every possible action in  every possible state. This assumption could not be eliminated in Qlearning and SARSA, since both are iterative dynamic  programming algorithms. However, under the assumption that the  environment is finite-horizon Markov decision process with finite  length episodes, estimating the optimal Q-function,   for Qlearning, can be simply computed recursively as:        Similarly, the estimate of the optimal Q-function for SARSA can  be described as:      Equations (1) and (2) show that it is sufficient to consider the  rewards collected from the set of all episodes,  , only once to  calculate estimate of the optimal Q-function,  , which eliminates  the need to visit every possible state infinitely many times.    Unlike Q-learning, our estimate of  , can be computed online as  well as offline. It can be computed online by accumulating estimate of   during the exploration of the environment. On the other hand, it can  be computed offline by first exploring the environment and    collecting the set of all possible episodes, then computing estimate  of  .  3     Action Language      This section develops the syntax and semantics of the action language,  , that allows the representation of model-free reinforcement learning problems, which extends the action language   [8].    3.1     Language syntax    A fluent is a predicate, which may contain variables, that describes  a property of the environment. Let   be a set of fluents and   be a  set of action names that can contain variables. A fluent literal is  either a fluent  , the negation of . Conjunctive fluent  formula is a conjunction of fluent literals of the form  ,  where   are fluent literals. Sometimes we abuse the  notation and refer to a conjunctive fluent formula as a set of fluent  literals ( ). An action theory,   is a tuple of  the form  , where   is a proposition of the form (3),   is a set of propositions from (4-6), and   is a discount  factor as follows:        where  is a fluent literal,    are conjunctive fluent  formulas,   is an action, and  is a real number in .    Proposition (3) represents the set of possible initial states.  Proposition (4) states that an action   is executable in any state in  which  holds, where each variable that appears in  also appears in .  Indirect effect of action is described by proposition (5), which says that   holds in every state in which  also holds. A proposition of the form    (6) represents the conditional effects of an action  along with the  rewards received in a state resulting from executing . All variables  that appear in  also appear in  and . Proposition (6) says that   causes  to hold with reward  is received in a successor state to a  state in which   is executed and   holds. An action theory is  ground if it does not contain any variables.    Example 1 Consider an elevator of n-story building domain  adapted from [5] that is represented by an action theory,   is described by (7) (  is a  particular value in  ) and    is represented by (8)-(14).         The actions in the elevator domain are   for move up to floor  ,   for move down to floor  , and   for closing the  elevator door. The predicates    are    fluents represent respectively that the elevator current floor is  ,  light of floor   is on, and elevator door is opened. The target is to  get all floors serviced and   is true for all  .    3.2     Semantics    We say a set of ground literals  is consistent if it does not contain  a pair of complementary literals. If a literal  , then we say   holds in , and  does not hold in  if  . A set of literals   holds in  if  is contained in , otherwise,  does not hold in .  We say that a set of literals  satisfies an indirect effect of action of  the form (5), if  belongs to  whenever  is contained in  or  is  not contained in . Let   be an action theory in   and  be a set  of literals. Then   is the smallest set of literals that contains   and satisfies all indirect effects of actions propositions in  . A state   is a complete and consistent set of literals that satisfies all the  indirect effects of actions propositions in  .    Definition 1 Let   be a ground action theory in  ,   be a state,   be a proposition in  . Then,   is the state resulting from executing  in , given  that  is executable in , where   is defined as:      where the reward received in  .    An episode in   is an expression of the form  ,  where  for  each  Definition 2 Let   be a ground action theory and    be  the  set  of  all  episodes  in  .  Then,  for  , the optimal Q-function,  , for Q-learning and SARSA are respectively estimated by        Considering SARSA, the optimal Q-function can be computed incrementally as follows. For any episode in  , the optimal Q-value  for the initial state-action pair is estimated by         that  is  calculated  online  during  the  exploration of the  environment. Then, for any state-action pair,  , in the  episode,  , is calculated from   by        However, for Q-learning,   can be computed incrementally as  well by first computing   incrementally using (15), then (16) is  used as an update rule only once, where for          Notice that, unlike [4], by using (15) and (16), Q-learning can be  computed online during the exploration of the environment as well  as offline.    4 Off- and On-Policy Model-free Reinforcement  Learning Using Answer Set Programming     We provide a translation from any action theory  , a  representation of a model-free reinforcement learning problem into  a normal logic program with answer set semantics  , where the  rules in   encode (1) the set of possible initial states  , (2) the  transition function  , (3) the set of propositions in  , (4) and the  discount factor . The answer sets of   correspond to episodes in  , with associated estimated optimal Q-values. This translation  follows some related translations described in [24, 18, 19].    We assume the environment is a finite-horizon Markov decision  process, where the length of each episode is known and finite. We use  the predicates;    to represent a literal   holds at time  moment  ;   for action   executes at time   ;   for reward received at time   after executing  is ;   says the estimate of the optimal Q-value of the initial stateaction pair, in a given episode,   steps from the initial state is  ; and   for the discount factor. We use lower case letters to  represent constants and upper case letters to represent variables.    Let   be the normal logic program translation of   that contains a set of rules described as follows. To  simplify the presentation, given  is a predicate and   be a set of literals, we use   to denote       For each action   includes the set of facts         Literals describe states of the world are encoded by        where   is a set of facts that describe the properties of the  world. To specify that   are contrary literals the  following rules are added to  .       The reward  received at time   after executing  at time    given that  is executable is encoded by         Estimate of the optimal Q-value of an initial state-action pair, in a  given episode,   steps away from the initial state, is equal to  the estimate of the optimal Q-value of the same initial state-action  pair, in the same episode,   steps away from the initial state added  to the discounted reward (by  ) received at time  , where                      The following rule says that  holds at the time moment   if  it holds at the time moment   and its contrary does not hold at the  time moment  .           A literal   and its negation   cannot hold at the same time is  encoded in   by         Rules that generate actions occurrences once at a time are  encoded by          The set of initial states,  , is encoded as  follows. Let   be the set of initial states, where for  . Moreover, let  ,  Intuitively, for any literal    belongs to , then     contains only . For each literal   includes        which says  holds at time 0. Literals in  belong to every initial  state. For each   includes    which says that  (similarly  ) holds at time 0, if   (similarly )  does not hold at the time 0.     Each proposition of the form (4) is encoded in   as         Let    be a goal expression, then  is encoded  in   as        Estimates of the optimal Q-value of initial state-action pair,  , is represented in  ,  where    represents the estimate of   at the end  of episode of length  . These Q-values,  , can be  computed online during the exploration of the environment as well  as offline after the exploration of the environment. Moreover, the  action generation rules (31) and (32) in our translation, choose  actions greedily at random. However, other action selection  strategies can be encoded instead.    Example 2 The normal logic program encoding,  , of the  elevator domain described in Example 1 is given as follows, where   consists of the following rules, along with the rules (18), (19),  (20), (21), (29), (30), (31), (32):      Each   is encoded as        , which says that if  occurs at time   holds at the same time moment, then  holds at time      for   The atoms    describe properties of the world that for  are encoded  as          The initial state is encoded as follows, where  , for   and for some          The executability conditions of actions, for  , are  encoded as         Effects, rewards, and the Q-value of the initial state-action pair  resulting after executing the actions   and  ,  for , are given by           where   is a fact,  , and      Effects of the   action is given by      The reward received after executing   is given by        Q-value of the initial state-action pair is given by the following  rule, where  is a fact.        The goal is encoded by the following rule for some          5     Correctness    This section shows the correctness of our translation. We prove that  the answer sets of the normal logic program translation of an action theory,   in  , correspond to episodes in  ,  associated with estimates of the optimal Q-values. Moreover, we  show that the complexity of finding a policy for   in our approach  is NP-complete. Let the domain of   be  . Let   be a  transition function associated with   is an initial state, and   be a set of actions in  . An episode in   is stateaction-reward-state  sequence  of  the  form  ,  such  that  ,    are  states,    is  an  action,  , and  .    Theorem 1 Let   be an action theory representing a model-free  reinforcement learning problem in  . Then,   is an episode in   iff       Theorem 1 says that an action theory,  , in  , can be translated  into a normal logic program,  , such that an answer set of   is  equivalent to an episode in  .    Theorem 2 Let   be an action theory in   be an answer set of   be the set of all episodes in  . Let   be a set such  that   iff       Theorem 2 asserts that, given an action theory   and by  considering Q-learning update rule, the expected sum of discounted  rewards resulting after executing an action   in a state   and  following the optimal policy thereafter,  , is equal to the  maximum over , appearing in   which is satisfied by  every answer set   for which     is  also satisfied. However, by considering the update rule of SARSA,   is equal to  in   that is satisfied by some  answer set of   for which  is also satisfied. For any   and  in  SARSA,   is calculated from   by (15), where   But, for Q-learning,  for any   and ,   is calculated from   by (15),  then (16) is used as an update rule only once.   In addition, we show that any model-free reinforcement learning  problem in MDP environment can be encoded as SAT problem.  Hence, state-of-the-art SAT solvers can be used to solve model-free  reinforcement learning problems. Any normal logic program, ,  can be translated into a SAT problem,  , where the models of    are equivalent to the answer sets of   [13]. Hence, the normal logic  program encoding of a model-free reinforcement learning problem   can be translated into an equivalent SAT problem, where the  models of S correspond to episodes in  .    Theorem 3 Let   be an action theory in   and   be the normal  logic program encoding of  . Then, the models of the SAT  encoding of   are equivalent to valid episodes in  .    For SAT encoding, the optimal Q-function is computed in a similar  way as in the normal logic program encoding of model-free  reinforcement learning problems. The transformation step from  nor-mal logic program encoding of a model-free reinforcement  learning problem into SAT can be avoided, by encoding a modelfree reinforcement learning problem directly into SAT [22]. The  following corollary shows any model-free reinforcement learning  problem can be encoded directly as SAT problem.  Corollary 1 Let   be an action theory in  . Then,   can be  directly encoded as a SAT formula   where the models of   are  equivalent to valid episodes in  .  Normal logic programs with answer set semantics find optimal policies  for model-free reinforcement learning problems in finite horizon MDP  environments using the flat representation of the problem domains.   The flat representation of reinforcement learning problem domains  is the explicit enumeration of world states [14]. Hence, Theorem 5  follows directly from Theorem 4 [14].    Theorem 4 The stationary policy existence problem for finitehorizon MDP in the flat representation is NP-complete.    Theorem 5 The policy existence problem for a model-free  reinforcement learning problem in MDP environment using normal  logic pro-grams with answer set semantics and SAT is NP-complete.    6     Conclusions and Related Work    We described a high level action language called   that allows  the representation of model-free reinforcement learning problems  in MDP environments. In addition, we introduced online and  offline logical framework to model-free reinforcement learning by  relating model-free reinforcement learning in MDP environment to  normal logic programs with answer set semantics and SAT.    The translation from an action theory in   into a normal logic  program builds on similar translations described in [24, 18, 19]. The  literature is rich with action languages that are capable of represent-ing  and reasoning about MDPs and actions with probabilistic effects, which  include [1, 2, 6, 9, 12]. The main difference between these languages  and   is that   allows the factored characterization of MDP for  model-free reinforcement learning.    Many approaches for solving MDP to find the optimal policy for  both reinforcement learning and probabilistic planning have been  presented. These approaches can be classified into two main categories of approaches; dynamic programming approaches and the  search-based approaches (a detailed survey on these approaches can  be found in [10, 2]). However, dynamic programming approaches  use primitive domain knowledge representation. On the other hand,  the search-based approaches mainly rely on search heuristics which  have limited knowledge representation capabilities to represent and  use domain-specific knowledge.    A logic based approach for solving MDP, for probabilistic plan-ning,  has been presented in [15]. The approach of [15] converts MDP  specification of a probabilistic planning problem into a stochastic  satisfiability problem and solving the stochastic satisfiability problem  instead. First-order logic representation of MDP for model-based  reinforcement learning has been described in [11] based on first-order  logic programs without nonmonotonic negations. Similar to the firstorder representation of MDP in [11],   allows objects and relations.  However, unlike  , [11] finds policies in the abstract level. A more  expressive first-order representation of MDP than [11] has been  presented in [3] that is a probabilistic extension to Reiter\u2019s situation  calculus. Although more expressive, it is more complex than [11].  Unlike the logical model-based reinforcement learning frame-work of  [19] that uses normal hybrid probabilistic logic programs to encode  model-based reinforcement learning problems, normal logic program  with answer set semantics is used to encode model-free reinforcement  learning problems.    REFERENCES    [1] C. Baral, N. Tran, and L. C. Tuan, \u2018Reasoning about actions in a probabilistic setting\u2019, in AAAI, (2002).     [2] C. Boutilier, T. Dean, and S. Hanks, \u2018Decision-theoretic planning:  structural assumptions and computational leverage\u2019, Journal of AI Research, 11(1), (1999).   [3] C. Boutilier, R. Reiter, and B. Price, \u2018Symbolic dynamic  programming for first-order mdps\u2019, in 17th IJCAI, (2001).     [4] Christopher C. Watkins, Learning from delayed rewards, Ph.D.  dissertation, University of Cambridge, 1989.     [5] R. Crites and A. Barto, \u2018Improving elevator performance using reinforcement learning\u2019, in Advances in Neural Information Processing,  (1996).     [6] T. Eiter and T. Lukasiewicz, \u2018probabilistic reasoning about actions in  nonmonotonic causal theories\u2019, in 19th Conference on UAI, (2003).     [7] M. Gelfond and V. Lifschitz, \u2018The stable model semantics for logic  programming\u2019, in ICSLP. MIT Pres, (1988).     [8] M. Gelfond and V. Lifschitz, \u2018Action languages\u2019, Electronic Transactions on AI, 3(16), 193\u2013210, (1998).   [9] L. Iocchi, T. Lukasiewicz, D. Nardi, and R. Rosati, \u2018Reasoning about  actions with sensing under qualitative and probabilistic uncertainty\u2019, in   16th European Conference on Artificial Intelligence , (2004).     [10] L. Kaelbling, M. Littman, and A. Moore, \u2018Reinforcement learning: A  survey\u2019, JAIR, 4, 237\u2013285, (1996).   [11] K. Kersting and L. De Raedt, \u2018Logical markov decision programs and  the convergence of logical td(\u03bb)\u2019, in 14th International Conference on  Inductive Logic Programming, (2004).     [12] N. Kushmeric, S. Hanks, and D. Weld, \u2018An algorithm for probabilistic  planning\u2019, Artificial Intelligenc , 76(1-2), 239\u2013286, (1995).   [13] F. Lin and Y. Zhao, \u2018Assat: Computing answer sets of a logic program  by sat solvers\u2019, Artificial Intelligence , 157(1-2), 115\u2013137, (2004).   [14] M. Littman, J. Goldsmith, and M. Mundhenk, \u2018The computational  com-plexity of probabilistic planning\u2019, Journal of Artificial  Intelligence Re-search, 9, 1\u201336, (1998).     [15] S. Majercik and M. Littman, \u2018Maxplan: A new approach to  probabilistic planning\u2019, in Fourth International Conference on  Artificial Intelligence Planning, pp. 86\u201393, (1998).     [16] S. Majercik and M. Littman, \u2018Contingent planning under uncertainty  via stochastic satisfiability\u2019, Artificial Intelligence , 147(1-2), 119\u2013 162, (2003).     [17] G. Rummery and M. Niranjan, \u2018Online q-learning using connectionist systems\u2019, Technical report, CUED/F-INFENG/TR166, Cambridge  University, (1994).     [18] E. Saad, \u2018Probabilistic planning in hybrid probabilistic logic  programs\u2019, in 1st International Conference on Scalable Uncertainty  Management, (2007).     [19] E. Saad, \u2018A logical framework to reinforcement learning using hybrid  probabilistic logic programs\u2019, in Second International Conference on  Scalable Uncertainty Management, (2008).     [20] E. Saad, \u2018On the relationship between hybrid probabilistic logic programs and stochastic satisfiability\u2019, in Second International  Conference on Scalable Uncertainty Management, (2008).     [21] E. Saad, \u2018Probabilistic planning with imperfect sensing actions using  hybrid probabilistic logic programs\u2019, in Third International  Conference on Scalable Uncertainty Management, (2009).     [22] E. Saad, \u2018Probabilistic reasoning by sat solvers\u2019, in Tenth ECSQARU,  (2009).     [23] E. Saad and E. Pontelli, \u2018A new approach to hybrid probabilistic logic  programs\u2019, Annals of Mathematics and Artificial Intelligence , 48(34), 187\u2013243, (2006).     [24] T. Son, C. Baral, T. Nam, and S. McIlraith, \u2018Domain-dependent  knowl-edge in answer set planning\u2019, ACM Transactions on  Computational Logic, 7(4), 613\u2013657, (2006).   ",
    "title": "Framework",
    "paper_info": " \nThe flat representation of reinforcement learning problem domains \nis the explicit enumeration of world states [14]. Hence, Theorem 5 \nfollows directly from Theorem 4 [14]. \n \nTheorem 4 The stationary policy existence problem for finite-\nhorizon MDP in the flat representation is NP-complete. \n \nTheorem 5 The policy existence problem for a model-free \nreinforcement learning problem in MDP environment using normal \nlogic pro-grams with answer set semantics and SAT is NP-complete. \n \n6     Conclusions and Related Work \n \nWe described a high level action language called \n that allows \nthe representation of model-free reinforcement learning problems \nin MDP environments. In addition, we introduced online and \noffline logical framework to model-free reinforcement learning by \nrelating model-free reinforcement learning in MDP environment to \nnormal logic programs with answer set semantics and SAT. \n \nThe translation from an action theory in \n into a normal logic \nprogram builds on similar translations described in [24, 18, 19]. The \nliterature is rich with action languages that are capable of represent-ing \nand reasoning about MDPs and actions with probabilistic effects, which \ninclude [1, 2, 6, 9, 12]. The main difference between these languages \nand \n is that \n allows the factored characterization of MDP for \nmodel-free reinforcement learning. \n \nMany approaches for solving MDP to find the optimal policy for \nboth reinforcement learning and probabilistic planning have been \npresented. These approaches can be classified into two main cat-\negories of approaches; dynamic programming approaches and the \nsearch-based approaches (a detailed survey on these approaches can \nbe found in [10, 2]). However, dynamic programming approaches \nuse primitive domain knowledge representation. On the other hand, \nthe search-based approaches mainly rely on search heuristics which \nhave limited knowledge representation capabilities to represent and \nuse domain-specific knowledge. \n \nA logic based approach for solving MDP, for probabilistic plan-ning, \nhas been presented in [15]. The approach of [15] converts MDP \nspecification of a probabilistic planning problem into a stochastic \nsatisfiability problem and solving the stochastic satisfiability problem \ninstead. First-order logic representation of MDP for model-based \nreinforcement learning has been described in [11] based on first-order \nlogic programs without nonmonotonic negations. Similar to the first-\norder representation of MDP in [11], \n allows objects and relations. \nHowever, unlike \n, [11] finds policies in the abstract level. A more \nexpressive first-order representation of MDP than [11] has been \npresented in [3] that is a probabilistic extension to Reiter\u2019s situation \ncalculus. Although more expressive, it is more complex than [11]. \nUnlike the logical model-based reinforcement learning frame-work of \n[19] that uses normal hybrid probabilistic logic programs to encode \nmodel-based reinforcement learning problems, normal logic program \nwith answer set semantics is used to encode model-free reinforcement \nlearning problems. \n \nREFERENCES \n \n[1] C. Baral, N. Tran, and L. C. Tuan, \u2018Reasoning about actions in a prob-\nabilistic setting\u2019, in AAAI, (2002).  \n \n[2] C. Boutilier, T. Dean, and S. Hanks, \u2018Decision-theoretic planning: \nstructural assumptions and computational leverage\u2019, Journal of AI Re-\nsearch, 11(1), (1999).  \n[3] C. Boutilier, R. Reiter, and B. Price, \u2018Symbolic dynamic \nprogramming for first-order mdps\u2019, in 17th IJCAI, (2001).  \n \n[4] Christopher C. Watkins, Learning from delayed rewards, Ph.D. \ndissertation, University of Cambridge, 1989.  \n \n[5] R. Crites and A. Barto, \u2018Improving elevator performance using rein-\nforcement learning\u2019, in Advances in Neural Information Processing, \n(1996).  \n \n[6] T. Eiter and T. Lukasiewicz, \u2018probabilistic reasoning about actions in \nnonmonotonic causal theories\u2019, in 19th Conference on UAI, (2003).  \n \n[7] M. Gelfond and V. Lifschitz, \u2018The stable model semantics for logic \nprogramming\u2019, in ICSLP. MIT Pres, (1988).  \n \n[8] M. Gelfond and V. Lifschitz, \u2018Action languages\u2019, Electronic Transac-\ntions on AI, 3(16), 193\u2013210, (1998).  \n[9] L. Iocchi, T. Lukasiewicz, D. Nardi, and R. Rosati, \u2018Reasoning about \nactions with sensing under qualitative and probabilistic uncertainty\u2019, in  \n16th European Conference on Artificial Intelligence , (2004).  \n \n[10] L. Kaelbling, M. Littman, and A. Moore, \u2018Reinforcement learning: A \nsurvey\u2019, JAIR, 4, 237\u2013285, (1996).  \n[11] K. Kersting and L. De Raedt, \u2018Logical markov decision programs and \nthe convergence of logical td(\u03bb)\u2019, in 14th International Conference on \nInductive Logic Programming, (2004).  \n \n[12] N. Kushmeric, S. Hanks, and D. Weld, \u2018An algorithm for probabilistic \nplanning\u2019, Artificial Intelligenc , 76(1-2), 239\u2013286, (1995).  \n[13] F. Lin and Y. Zhao, \u2018Assat: Computing answer sets of a logic program \nby sat solvers\u2019, Artificial Intelligence , 157(1-2), 115\u2013137, (2004).  \n[14] M. Littman, J. Goldsmith, and M. Mundhenk, \u2018The computational \ncom-plexity of probabilistic planning\u2019, Journal of Artificial \nIntelligence Re-search, 9, 1\u201336, (1998).  \n \n[15] S. Majercik and M. Littman, \u2018Maxplan: A new approach to \nprobabilistic planning\u2019, in Fourth International Conference on \nArtificial Intelligence Planning, pp. 86\u201393, (1998).  \n \n[16] S. Majercik and M. Littman, \u2018Contingent planning under uncertainty \nvia stochastic satisfiability\u2019, Artificial Intelligence , 147(1-2), 119\u2013\n162, (2003).  \n \n[17] G. Rummery and M. Niranjan, \u2018Online q-learning using connection-\nist systems\u2019, Technical report, CUED/F-INFENG/TR166, Cambridge \nUniversity, (1994).  \n \n[18] E. Saad, \u2018Probabilistic planning in hybrid probabilistic logic \nprograms\u2019, in 1st International Conference on Scalable Uncertainty \nManagement, (2007).  \n \n[19] E. Saad, \u2018A logical framework to reinforcement learning using hybrid \nprobabilistic logic programs\u2019, in Second International Conference on \nScalable Uncertainty Management, (2008).  \n \n[20] E. Saad, \u2018On the relationship between hybrid probabilistic logic pro-\ngrams and stochastic satisfiability\u2019, in Second International \nConference on Scalable Uncertainty Management, (2008).  \n \n[21] E. Saad, \u2018Probabilistic planning with imperfect sensing actions using \nhybrid probabilistic logic programs\u2019, in Third International \nConference on Scalable Uncertainty Management, (2009).  \n \n[22] E. Saad, \u2018Probabilistic reasoning by sat solvers\u2019, in Tenth ECSQARU, \n(2009).  \n \n[23] E. Saad and E. Pontelli, \u2018A new approach to hybrid probabilistic logic \nprograms\u2019, Annals of Mathematics and Artificial Intelligence , 48(3-\n4), 187\u2013243, (2006).  \n \n[24] T. Son, C. Baral, T. Nam, and S. McIlraith, \u2018Domain-dependent \nknowl-edge in answer set planning\u2019, ACM Transactions on \nComputational Logic, 7(4), 613\u2013657, (2006).  \n",
    "GPTsummary": "  - (1): The research background of this article is the issue of knowledge representation in reinforcement learning. \n\n  - (2): Previous methods for reinforcement learning include dynamic programming by value iteration as well as model-free reinforcement learning algorithms. However, these methods have limitations, such as the need for a complete model of the environment, slow convergence, and lack of exploitation of domain-specific knowledge. The approach proposed in this paper is well motivated and aims to bridge the gap between reinforcement learning and knowledge representation. \n\n  - (3): The research methodology proposed in this paper is a rich knowledge representation framework based on normal logic programs with answer set semantics. The framework can solve model-free reinforcement learning problems in more complex domains and exploits domain-specific knowledge. The approach is proven to be correct and can be encoded as a SAT problem. \n\n  - (4): The methods in this paper achieve the task of providing a logical framework for model-free reinforcement learning using Q-learning and SARSA update rules to learn the optimal off- and on-policy, respectively. The importance of the proposed framework is twofold: it enables the representation of domain-specific knowledge and eliminates the requirement of visiting every state infinitely many times, which is required for convergence in some previous methods.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this piece of work is to propose a new framework for reinforcement learning that bridges the gap between reinforcement learning and knowledge representation. This framework can be used to solve model-free reinforcement learning problems in more complex domains and exploit domain-specific knowledge.\n\n- (2): Innovation point: The proposed framework is based on normal logic programs with answer set semantics, which solves the problem of knowledge representation in reinforcement learning. Performance: The proposed framework can learn optimal off- and on-policy using Q-learning and SARSA update rules, respectively. It can also exploit domain-specific knowledge and eliminate the requirement of visiting every state infinitely many times, which is required for convergence in some previous methods. Workload: The proposed framework requires encoding as a SAT problem, which may increase computational complexity.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this piece of work is to propose a new framework for reinforcement learning that bridges the gap between reinforcement learning and knowledge representation. This framework can be used to solve model-free reinforcement learning problems in more complex domains and exploit domain-specific knowledge.\n\n- (2): Innovation point: The proposed framework is based on normal logic programs with answer set semantics, which solves the problem of knowledge representation in reinforcement learning. Performance: The proposed framework can learn optimal off- and on-policy using Q-learning and SARSA update rules, respectively. It can also exploit domain-specific knowledge and eliminate the requirement of visiting every state infinitely many times, which is required for convergence in some previous methods. Workload: The proposed framework requires encoding as a SAT problem, which may increase computational complexity.\n\n\n"
}