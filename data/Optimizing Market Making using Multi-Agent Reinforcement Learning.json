{
    "Abstract": "Abstract\u2014 In this paper, reinforcement learning is applied to the problem of optimizing market making. A multi-agent reinforcement learning framework is used to optimally place limit orders that lead to successful trades. The framework consists of two agents. The macro-agent optimizes on making the decision to buy, sell, or hold an asset. The micro-agent optimizes on placing limit orders within the limit order book. For the context of this paper, the proposed framework is applied and studied on the Bitcoin cryptocurrency market. The goal of this paper is to show that reinforcement learning is a viable strategy that can be applied to complex problems (with complex environments) such as market making. I. INTRODUCTION Algorithmic trading, and in particular high-frequency algorithmic trading (HFT), has gained immense popularity in the recent decade. With advances in hardware and software, algorithmic trading has rapidly become the norm. The increasing popularity of machine learning has slowly made its way to \ufb01nancial markets [1], where it is primarily used to predict price movements of assets. However, there are a number of challenges that these classic machine learning techniques entail: 1) Prediction time: In machine learning, model complexity can have an impact on prediction time. Many times, in the supervised learning setting, neural networks are used to make predictions. Due to the computational complexity that comes with these models, as the model complexity increases, the decision time also increases [2]. In the HFT setting, by the time the model makes a prediction, it may already be too late to take the predicted action. The problem then becomes, how can these added latency costs be incorporated into our prediction? 2) Prediction accuracy: Financial markets are secondorder chaotic systems, i.e. they are highly unpredictable since markets can respond to predictions. The general rule of thumb in \ufb01nance is that the historical performance of an asset does not predict the future performance of the asset, i.e. forecasting and predicting the market from historical performance alone is virtually impossible. In fact, markets are sometimes compared to random walk processes [10]. Therefore, approaches that solely rely on the historical performance of an asset are likely to have low prediction accuracy. 3) Policy optimization: Suppose that our model predicted a 55% chance of increase and a 45% chance of decrease for an asset. How can our model\u2019s prediction be converted into an action? An optimized policy and certain decision thresholds are needed to turn the prediction into an action. However, coming up with such a policy is usually done by hand. The development and optimization of these policies are commonly done by mix of humans and computers. Therefore, if the market suddenly shifted, these policies would likely not be able to adapt. II. RELATED WORK The concept of reinforcement learning is relatively new in \ufb01nance. There has been little research done into whether or not reinforcement learning is a viable approach for market making. The optimization problem of market making is a complex problem [11], and reinforcement learning is not a common approach used to solve it. Multi-agent approaches to stock trading have been taken previously [9]. However, they do not account for placing limit orders. The general concept of the micro-agent (see section 3) has been shown by Nevmyvaka et al. [3], and further extended by Juchli [4]. These papers present the problem of optimally buying (or selling) an asset over a \ufb01xed time horizon. This is similar to the idea of the micro-agent. The work for the micro-agent is substantially based, and builds upon the ideas presented in these two papers. III. PROBLEM FORMULATION In this paper, a less common approach of reinforcement learning is utilized to create an optimal market maker. More speci\ufb01cally, a multi-agent approach (with two agents) is used: 1) Macro-agent: The macro-agent is given minute tick data (data at a macro-level) and makes the decision to buy, sell, or hold the asset. 2) Micro-agent: The micro-agent is given order book data (data at a micro-level) and makes the decision of where to place the order within the limit order book. Fig. 1. Time-steps where agents take actions. The Macro-agent takes actions at every bold black line, while the Micro-agent takes actions between the dashed lines. At the start of every minute, the macro-agent makes the decision to buy, sell, or hold the asset. A running count of how many buys it chooses to make is kept. The total number of buys before the sell is fed into the micro-agent, 1 arXiv:1812.10252v1  [q-fin.TR]  26 Dec 2018 i.e. all collected assets are exhausted on each decision to sell. Additionally, on each decision to sell, the count is reset. Within the 60 second time horizon that follows, the microagent attempts to exhaust its held assets by only placing limit orders. The agent may place multiple limit orders, however, the it is limited to a single order placement every 10 seconds, as depicted in Fig. 1. A. Data Collection Historical order book and minute tick data for markets is not readily available. The \ufb01rst step is to collect this data. The Bitcoin cryptocurrency market is used in this study for the reason that its data is readily available. All trade, bid, and ask data is collected by subscribing to Bittrex\u2019s WebSocket in the date range of November 2nd, 2018 to November 17th, 2018. With data provided by the WebSocket, a historical order book is constructed, i.e. a sequence of historical order book states over the time period. A minute tick dataset is then created using the trade data collected. In total, 41830629 trade, bid, and ask data points, and 10945 minute tick data points (Fig. 2) were collected for this study. However, the entire dataset was not used. Instead, only the most recent few days were chosen. Fig. 2. Bitcoin Minute tick data collected over 2018-11-02 to 2018-11-17 IV. BACKGROUND: BRIEF OVERVIEW OF REINFORCEMENT LEARNING Reinforcement learning (RL) is a learning technique within sequential decision making where an agent learns to take actions optimally in an environment. Unlike supervised learning, in RL, the agent learns to take actions that maximize the reward it receives from the environment. At each time-step, the agent observes its state and takes an action based on the observation. The environment provides feedback on how well the action performed in the form of a reward. Many times these rewards can be delayed. For example, the decision to hold an asset may not yield an instant reward. The process of an agent interacting with the environment is formalized as a Markov Decision Process (MDP). MDPs are used to describe the environment in the context of an RL problem. The importance behind MDPs in this problem is the Markov property that MDPs assume: the effects of some action taken in some state depends only on that state and not on prior states encountered. Market prices are Markovian in nature, i.e. the probability distribution of the price of an asset depends only on the current price and not on the prior history. Hence, it makes sense to formulate this problem as an MDP problem with the goal being to solve this MDP by \ufb01nding an optimal policy. For each agent, the tuple (S, A, P, r, \u03b3) describes the problem: \u2022 S is the set of states. \u2022 A is the set of actions. \u2022 P : S \u00d7 A \u00d7 S \u2192 R is the transition probability distribution which determines how the environment changes based on the state and actions taken by the agent. \u2022 r : S \u2192 R is the reward function. \u2022 \u03b3 \u2208 (0, 1) is the discount factor which determines the importance of future rewards. Let Rt denote the sum of future rewards, i.e. Rt = rt+1 + rt+2 + \u00b7 \u00b7 \u00b7 + rT Then, at time-step t, the future discounted reward is then given by R\u2032 t = Rt+1 + \u03b3 \u00b7 Rt+2 + \u03b32 \u00b7 Rt+3 + . . . + \u03b3T \u00b7 RT = T \ufffd i=t \u03b3i \u00b7 Rt+i+1 (4.1) where T is the last time-step, potentially in\ufb01nity. States in the context of market making are much more complex than typical RL problems. In fact, the states encountered are most often not the complete states since there exist many other traders in the environment. Thus, the market making problem is a Partially Observable Markov Decision Process (POMDP). The state the agent observes, S\u2032, is some derivation of the true state, S, i.e. S\u2032 \u223c O(S). Nevertheless, given a proper simulation of the environment, the agent should be able to optimize well against the unknowns in the environment. As previously mentioned, an RL agent continuously goes through a cycle of states by interacting with the environment. For this problem, the interaction between the agents and the environment is depicted in Fig. 3. Fig. 3. Multi-agent Reinforcement Learning Framework Both agents encounter states and output actions. The action determined by the macro-agent is fed into the microagent, which collectively turns into a single action: placing 2 an order in the limit order book. The environment takes this action and outputs a reward and a next state. As noted in the problem statement, discrete time-steps are chosen (rather than continuous time-steps) for the reason that continuous time-steps would not be possible in the real world since the WebSocket data itself arrives at discrete time-steps. A. Q-Learning The speci\ufb01c reinforcement learning algorithm used for both agents is deep Q-learning. Recall that the goal is to \ufb01nd an optimal policy \u03c0 : S \u2192 A, i.e. a function mapping between states and actions such that it maximizes the expected reward. Q-learning is a model-free value-based approach, which uses the action-value function Q(s, a), where s \u2208 S and a \u2208 A, to estimate the expected reward given some stateaction pair. In other words, under policy \u03c0, the true value of being in state s and performing action a is given by Q\u03c0(s, a) = E\u03c0 [Rt | St = s, At = a, \u03c0] (4.2) Typically the Q-values are updated using the bellman equation, which are derived by expanding (4.2): Q\u03c0(s, a) = E\u03c0 [Rt | St = s, At = a, \u03c0] = E\u03c0 \ufffd Rt+1 + \u03b3R\u2032 t+1 | St = s, At = a \ufffd = \ufffd s\u2032 \ufffd r P(s\u2032, r | s, a) \ufffd r + \u03b3E\u03c0 \ufffd R\u2032 t+1 | St+1 = s\u2032\ufffd\ufffd = \ufffd s\u2032,r P(s\u2032, r | s, a) [r + \u03b3Q\u03c0(s\u2032, \u03c0t(s\u2032))] Based on this expansion, the update rule is given by: Qt+1(st, at) = \ufffd s\u2032,r P(s\u2032, r | s, a) [r + \u03b3 \u00b7 Q\u03c0 t (s\u2032, \u03c0t(s\u2032))] Although this update rule would lead to an optimal policy, it is unfeasible for this problem since P(s\u2032, r | s, a) is unknown, i.e. the data collected does not provide a complete picture of the market environment. Instead we use the following update rule: Qt+1(st, at) = (1 \u2212 \u03b1) \u00b7 Qt(st, at) + \u03b1 \ufffd rt+1 + \u03b3 max a Qt(st+1, a) \ufffd where \u03b1 is a learning rate. In this problem, a neural network is used to approximate the Q-value function where the input is the state (instead of state-action pairs), and the output are the Q-values for each of the actions. Thus, the Q-value function is parameterized by weights \u03b8, i.e. we assume that Q(s, a; \u03b8) \u2248 Q\u2217(s, a) B. Exploration-Exploitation Trade-off In reinforcement learning, there is a trade-off between exploration and exploitation: how often do we want our agent to continue taking actions with the policy it has determined versus how often do we want our agent to explore taking new actions. To get a good balance between both worlds, a decaying \u03f5-greedy approach is used. In this approach, the agent takes random actions with probability \u03f5 and takes the policy action with probability 1 \u2212 \u03f5, while \u03f5 decays over time. This strategy encourages the agent to explore more at the beginning, and exploit more at the end, i.e. stick to the policy it has computed. C. Experience Replay Another concept that is used is experience replay. In online learning, we receive data sequentially. Due to the nature of markets, there is always a concern for market shifts. If the market suddenly shifted, the agent may forget its past experiences as it aims to get a better advantage in the new states. By storing the agent\u2019s experiences in a memory buffer, and having the agent relive randomly sampled batches of experiences often, the high temporal correlation in the data is reduced. Additionally, training on these randomly sampled batches gives the feel of training on iid data. Therefore, as in supervised learning problems, this would yield a better convergence for the function approximator to the optimal policy. Furthermore, this allows to update the parameters of the function approximator with a stochastic gradient descent approach. V. DETAILS OF THE MACRO-AGENT The Macro-agent is responsible for making a discrete decision to buy, sell, or hold an asset by looking at the data from a macro level. The data used is minute tick data. Although the data is collected over a span of multiple days, the entire data-set is not used for the reason that the agent may under-explore the more recent and relevant states, and over-explore other states. Since most of the data had little volatility, while the more recent data was much more volatile, the agent would not have been able to perform well in volatile states due to exploring the less volatile states more often. Therefore, the recent minute tick data spanning the course of the last few days is used: November 15th, 2018 to November 17th, 2018. Fig. 4 shows the portion of the data that was chosen. Fig. 4. Bitcoin Minute tick data (2018-11-15 00:00 to 2018-11-17 17:06) To train the agent, the dataset is split into training and test sets. The training set consists of the time-steps prior to November 16th, 2018. 3 A. State The state space at a certain time-step t consists of historical price data in the range t \u2212 h to t, where h denotes how far back in history the agent looks. Furthermore, featurized data of various momentum and reversion indicators are also incorporated into the state space. These indicator features are computed in the following ways: Market Indicator Features One commonly used feature in market trading is the z-score indicator, which determines how far (in standard deviations \u03c3) a data point x is from the mean \u00b5: zx = x \u2212 \u00b5 \u03c3 (5.1) In a way, the z-score indicator reveals anomalies in data. It has also been proven to be a useful indicator in determining future trends [5]. Equation (5.1) can be applied by using an expanding window approach with a window size of n time-steps. Let pt denote the closing price at time-step t, let SMA(pt\u2212n,t) denote simple moving average (SMA) of closing prices over n time-steps prior to t, and let STDDEV(pt\u2212n,t) denote the standard deviation of closing prices over n time-steps prior to t. Thus, the z-score at timestep t for a window size of n is zn t = pt \u2212 SMA(pt\u2212n,t) STDDEV(pt\u2212n,t) The z-score indicator can be extended to volume as well. Thus, the following are the features that are extracted: \u2022 Price Level: To determine price levels, the z-scores are calculated for the prices. This essentially expresses how far each of the prices in the time period are from the average price in the time period. \u2022 Price Change: To determine price changes, the current price is compared to the average of a window of prices prior to it, i.e. calculated for a window size n at timestep t PCn t = pt SMA(pt\u2212n,t) \u2212 1 and take the z-score of the result. \u2022 Volume Level: To determine volume levels, the z-scores are calculated for the traded volumes. As with price levels, this expresses how far the volume at each time step is from the volume in the time period. \u2022 Volume Change: Similar to price change, volume change for a window size n at time-step t is calculated: PCn t = vt SMA(vt\u2212n,t) \u2212 1 and the z-score of the result is taken. \u2022 Volatility: To determine volatility, exponential moving averages (EMA) are used to determine the rate of price change over a span of n days. The reasoning behind using EMA instead of SMA is that EMA gives more weight to the recent time-steps, whereas in SMA all time-steps are weighted equally. Let pt be the current price, and let n be the window size. The EMA at timestep t is given by EMAn t = 2pt n + 1 + \ufffdt j=t\u2212n pj n \ufffd 100 \u2212 2 n + 1 \ufffd Thus, the volatility over m days is given by Volatilitym t = EMAt,n \u2212 EMAt\u2212m,n EMAt\u2212m,n Along with these market indicator features and the price data, there is an additional state parameter which plays an essential role in determining rewards. Each time the agent chooses to buy, the price at which the agent buys the asset (the current open price) is stored into a current assets list. This list of prices is stored as part of the state as well. Each time the agent chooses to sell, all of the bought assets are exhausted (sold). Thus, the historical prices, market indicator features, and current assets list form the state. B. Action There are three actions that the agent can choose: 1) Buy: If the decision to buy is chosen, then the agent is choosing to buy 1 bitcoin at the current opening price. As mentioned previously, when the agent chooses to buy, the price gets appended to the current assets list. Note that the decision to buy 1 bitcoin is purely a design choice. The proposed framework can be applied to any quantity of bitcoins. 2) Sell: If the decision to sell is chosen, then the agent is choosing to sell all of its accrued assets at the current opening price. 3) Hold: If the decision to hold is chosen, then the agent does not do anything. C. Reward Upon taking an action, the agent receives a reward from the environment. Algorithm 1 depicts how rewards are handled. Algorithm 1: Macro-agent Reward Function 1 if action = hold then 2 reward \u2190 0 3 else if action = buy then 4 Append current open price to current assets list 5 reward \u2190 0 6 else if action = sell then 7 if there are no assets to sell then 8 reward \u2190 \u22121 9 else 10 reward \u2190 sell off all assets from current assets list and determine pro\ufb01t based on current open price 11 end 12 Clip Rewards Note that the rewards are clipped to {\u22121, 0, 1} based on negative, zero, or positive rewards, respectively. This is done 4 in order to have the agent deal with different types of market environments, e.g. high/low volatility, as well as reduce the impact of anomalous market shifts. Clipping rewards has proven to be a useful method when dealing with different scales of rewards when in various Atari games [6]. D. Deep Q-Network Architecture & Training To parameterize the value function, a deep Q-network is used. A simple multilayer perceptron with two hidden layers (with ReLu activation functions) is chosen. Furthermore, the Adam optimizer is used for training. The Q-network takes in as inputs the price data, market indicator data, and the current assets list, and outputs 3 Q-values which are then used to determine the optimal action. In Fig. 5 a depiction of the training framework for the macro-agent is provided. Fig. 5. Macro-agent Training Framework The details of how training happens within this framework are outlined in Algorithm 2. Note that for each epoch the agent is trained on the entire training set. E. Results It is interesting to see how the macro-agent would perform on its own. Although the closing and opening prices, and the prices used to calculate pro\ufb01ts for the macro-agent are usually not indicative of the true prices where trades may happen, they can hypothetically be assumed so to evaluate the performance of the agent. For these results, the agent was trained on the training set and tested on the test set, as de\ufb01ned previously, for 500 epochs. To understand the potential of the macro-agent, its performance is compared to two common investment strategies: \u2022 Buy and Hold investing: Buy and Hold investing is a naive long-term investment strategy where an investor buys an asset at the start of a time period and holds the asset in the hopes that it will accrue value over time. From the looks of Fig. 4 it is not expected that this strategy will yield any positive pro\ufb01t as the general trend in the test set is downwards. In this strategy, 10 Bitcoins are chosen to be bought and held over the time period. In order to simulate the Buy and Hold investing strategy, at each time-step the pro\ufb01t is plotted assuming that the investor decided to sell at each time-step, i.e. if Algorithm 2: Macro-agent Deep Q-Learning Training 1 Initialize replay memory M 2 Initialize \u03f5, and discount factor \u03b3 3 Initialize Q network with random weights 4 for epoch = 1 to E do 5 st \u2190 Reset environment (environment outputs initial state) 6 while not done do 7 at \u2190 Select random action with probability \u03f5 or choose action maxa Q(s, a; \u03b8) 8 st+1, rt, done \u2190 Act based on a (environment outputs new state, reward, and done) 9 if replay memory M is full then 10 Remove the \ufb01rst element from M 11 Append (st, at, rt, st+1, done) to replay memory M 12 b \u2190 Sample mini-batch from replay memory M at random 13 q \u2190 Initialize empty array of size |b| 14 foreach (si, ai, ri, si+1) \u2208 b do 15 qi = \ufffd ri + \u03b3 \u00b7 maxa Q(si+1, ai; \u03b8) not done ri done 16 end 17 Apply gradient descent with loss Es,a,r,s\u2032 \ufffd (qi \u2212 Q(si, ai; \u03b8))2\ufffd 18 Decay \u03f5 19 end 20 end p0 is the price the asset was initially bought at, at each time-step the difference pt \u2212 p0 is plotted. \u2022 Momentum investing: Momentum investing is where an investor chooses to buy or sell an asset at a certain time-step given the performance of the asset in the last n steps. To implement this, a simple moving average of the prices with a window size of n = 20 is computed, i.e. 20 minutes prior. At each time-step, if the current open price is less than the average price in the minutes prior, 1 Bitcoin is bought. If the current open price is greater than the average price in the minutes prior, all Bitcoins bought thus far are sold. In the case they are equal, all assets are held. These investment strategies serve as benchmarks and are compared with the macro-agent. The macro-agent is similar to momentum investing with the only difference being that the policy of when to buy, sell, or hold is different. Similar to momentum investing, the macro-agent buys one bitcoin on a decision to buy and exhausts all assets on a decision to sell. Fig. 6 depicts the performance comparisons between the different strategies. Fig. 6 compares the strategies in terms of pro\ufb01t. The accumulated pro\ufb01t is plotted for the momentum investing strategy and the macro-agent. The macro-agent\u2019s accumulated pro\ufb01t 5 ",
    "Introduction": "",
    "Related Work": "RELATED WORK The concept of reinforcement learning is relatively new in \ufb01nance. There has been little research done into whether or not reinforcement learning is a viable approach for market making. The optimization problem of market making is a complex problem [11], and reinforcement learning is not a common approach used to solve it. Multi-agent approaches to stock trading have been taken previously [9]. However, they do not account for placing limit orders. The general concept of the micro-agent (see section 3) has been shown by Nevmyvaka et al. [3], and further extended by Juchli [4]. These papers present the problem of optimally buying (or selling) an asset over a \ufb01xed time horizon. This is similar to the idea of the micro-agent. The work for the micro-agent is substantially based, and builds upon the ideas presented in these two papers. III. ",
    "Problem Formulation": "PROBLEM FORMULATION In this paper, a less common approach of reinforcement learning is utilized to create an optimal market maker. More speci\ufb01cally, a multi-agent approach (with two agents) is used: 1) Macro-agent: The macro-agent is given minute tick data (data at a macro-level) and makes the decision to buy, sell, or hold the asset. 2) Micro-agent: The micro-agent is given order book data (data at a micro-level) and makes the decision of where to place the order within the limit order book. Fig. 1. Time-steps where agents take actions. The Macro-agent takes actions at every bold black line, while the Micro-agent takes actions between the dashed lines. At the start of every minute, the macro-agent makes the decision to buy, sell, or hold the asset. A running count of how many buys it chooses to make is kept. The total number of buys before the sell is fed into the micro-agent, 1 arXiv:1812.10252v1  [q-fin.TR]  26 Dec 2018 i.e. all collected assets are exhausted on each decision to sell. Additionally, on each decision to sell, the count is reset. Within the 60 second time horizon that follows, the microagent attempts to exhaust its held assets by only placing limit orders. The agent may place multiple limit orders, however, the it is limited to a single order placement every 10 seconds, as depicted in Fig. 1. A. Data Collection Historical order book and minute tick data for markets is not readily available. The \ufb01rst step is to collect this data. The Bitcoin cryptocurrency market is used in this study for the reason that its data is readily available. All trade, bid, and ask data is collected by subscribing to Bittrex\u2019s WebSocket in the date range of November 2nd, 2018 to November 17th, 2018. With data provided by the WebSocket, a historical order book is constructed, i.e. a sequence of historical order book states over the time period. A minute tick dataset is then created using the trade data collected. In total, 41830629 trade, bid, and ask data points, and 10945 minute tick data points (Fig. 2) were collected for this study. However, the entire dataset was not used. Instead, only the most recent few days were chosen. Fig. 2. Bitcoin Minute tick data collected over 2018-11-02 to 2018-11-17 IV. BACKGROUND: BRIEF OVERVIEW OF REINFORCEMENT LEARNING Reinforcement learning (RL) is a learning technique within sequential decision making where an agent learns to take actions optimally in an environment. Unlike supervised learning, in RL, the agent learns to take actions that maximize the reward it receives from the environment. At each time-step, the agent observes its state and takes an action based on the observation. The environment provides feedback on how well the action performed in the form of a reward. Many times these rewards can be delayed. For example, the decision to hold an asset may not yield an instant reward. The process of an agent interacting with the environment is formalized as a Markov Decision Process (MDP). MDPs are used to describe the environment in the context of an RL problem. The importance behind MDPs in this problem is the Markov property that MDPs assume: the effects of some action taken in some state depends only on that state and not on prior states encountered. Market prices are Markovian in nature, i.e. the probability distribution of the price of an asset depends only on the current price and not on the prior history. Hence, it makes sense to formulate this problem as an MDP problem with the goal being to solve this MDP by \ufb01nding an optimal policy. For each agent, the tuple (S, A, P, r, \u03b3) describes the problem: \u2022 S is the set of states. \u2022 A is the set of actions. \u2022 P : S \u00d7 A \u00d7 S \u2192 R is the transition probability distribution which determines how the environment changes based on the state and actions taken by the agent. \u2022 r : S \u2192 R is the reward function. \u2022 \u03b3 \u2208 (0, 1) is the discount factor which determines the importance of future rewards. Let Rt denote the sum of future rewards, i.e. Rt = rt+1 + rt+2 + \u00b7 \u00b7 \u00b7 + rT Then, at time-step t, the future discounted reward is then given by R\u2032 t = Rt+1 + \u03b3 \u00b7 Rt+2 + \u03b32 \u00b7 Rt+3 + . . . + \u03b3T \u00b7 RT = T \ufffd i=t \u03b3i \u00b7 Rt+i+1 (4.1) where T is the last time-step, potentially in\ufb01nity. States in the context of market making are much more complex than typical RL problems. In fact, the states encountered are most often not the complete states since there exist many other traders in the environment. Thus, the market making problem is a Partially Observable Markov Decision Process (POMDP). The state the agent observes, S\u2032, is some derivation of the true state, S, i.e. S\u2032 \u223c O(S). Nevertheless, given a proper simulation of the environment, the agent should be able to optimize well against the unknowns in the environment. As previously mentioned, an RL agent continuously goes through a cycle of states by interacting with the environment. For this problem, the interaction between the agents and the environment is depicted in Fig. 3. Fig. 3. Multi-agent Reinforcement Learning Framework Both agents encounter states and output actions. The action determined by the macro-agent is fed into the microagent, which collectively turns into a single action: placing 2 an order in the limit order book. The environment takes this action and outputs a reward and a next state. As noted in the problem statement, discrete time-steps are chosen (rather than continuous time-steps) for the reason that continuous time-steps would not be possible in the real world since the WebSocket data itself arrives at discrete time-steps. A. Q-Learning The speci\ufb01c reinforcement learning algorithm used for both agents is deep Q-learning. Recall that the goal is to \ufb01nd an optimal policy \u03c0 : S \u2192 A, i.e. a function mapping between states and actions such that it maximizes the expected reward. Q-learning is a model-free value-based approach, which uses the action-value function Q(s, a), where s \u2208 S and a \u2208 A, to estimate the expected reward given some stateaction pair. In other words, under policy \u03c0, the true value of being in state s and performing action a is given by Q\u03c0(s, a) = E\u03c0 [Rt | St = s, At = a, \u03c0] (4.2) Typically the Q-values are updated using the bellman equation, which are derived by expanding (4.2): Q\u03c0(s, a) = E\u03c0 [Rt | St = s, At = a, \u03c0] = E\u03c0 \ufffd Rt+1 + \u03b3R\u2032 t+1 | St = s, At = a \ufffd = \ufffd s\u2032 \ufffd r P(s\u2032, r | s, a) \ufffd r + \u03b3E\u03c0 \ufffd R\u2032 t+1 | St+1 = s\u2032\ufffd\ufffd = \ufffd s\u2032,r P(s\u2032, r | s, a) [r + \u03b3Q\u03c0(s\u2032, \u03c0t(s\u2032))] Based on this expansion, the update rule is given by: Qt+1(st, at) = \ufffd s\u2032,r P(s\u2032, r | s, a) [r + \u03b3 \u00b7 Q\u03c0 t (s\u2032, \u03c0t(s\u2032))] Although this update rule would lead to an optimal policy, it is unfeasible for this problem since P(s\u2032, r | s, a) is unknown, i.e. the data collected does not provide a complete picture of the market environment. Instead we use the following update rule: Qt+1(st, at) = (1 \u2212 \u03b1) \u00b7 Qt(st, at) + \u03b1 \ufffd rt+1 + \u03b3 max a Qt(st+1, a) \ufffd where \u03b1 is a learning rate. In this problem, a neural network is used to approximate the Q-value function where the input is the state (instead of state-action pairs), and the output are the Q-values for each of the actions. Thus, the Q-value function is parameterized by weights \u03b8, i.e. we assume that Q(s, a; \u03b8) \u2248 Q\u2217(s, a) B. Exploration-Exploitation Trade-off In reinforcement learning, there is a trade-off between exploration and exploitation: how often do we want our agent to continue taking actions with the policy it has determined versus how often do we want our agent to explore taking new actions. To get a good balance between both worlds, a decaying \u03f5-greedy approach is used. In this approach, the agent takes random actions with probability \u03f5 and takes the policy action with probability 1 \u2212 \u03f5, while \u03f5 decays over time. This strategy encourages the agent to explore more at the beginning, and exploit more at the end, i.e. stick to the policy it has computed. C. Experience Replay Another concept that is used is experience replay. In online learning, we receive data sequentially. Due to the nature of markets, there is always a concern for market shifts. If the market suddenly shifted, the agent may forget its past experiences as it aims to get a better advantage in the new states. By storing the agent\u2019s experiences in a memory buffer, and having the agent relive randomly sampled batches of experiences often, the high temporal correlation in the data is reduced. Additionally, training on these randomly sampled batches gives the feel of training on iid data. Therefore, as in supervised learning problems, this would yield a better convergence for the function approximator to the optimal policy. Furthermore, this allows to update the parameters of the function approximator with a stochastic gradient descent approach. V. DETAILS OF THE MACRO-AGENT The Macro-agent is responsible for making a discrete decision to buy, sell, or hold an asset by looking at the data from a macro level. The data used is minute tick data. Although the data is collected over a span of multiple days, the entire data-set is not used for the reason that the agent may under-explore the more recent and relevant states, and over-explore other states. Since most of the data had little volatility, while the more recent data was much more volatile, the agent would not have been able to perform well in volatile states due to exploring the less volatile states more often. Therefore, the recent minute tick data spanning the course of the last few days is used: November 15th, 2018 to November 17th, 2018. Fig. 4 shows the portion of the data that was chosen. Fig. 4. Bitcoin Minute tick data (2018-11-15 00:00 to 2018-11-17 17:06) To train the agent, the dataset is split into training and test sets. The training set consists of the time-steps prior to November 16th, 2018. 3 A. State The state space at a certain time-step t consists of historical price data in the range t \u2212 h to t, where h denotes how far back in history the agent looks. Furthermore, featurized data of various momentum and reversion indicators are also incorporated into the state space. These indicator features are computed in the following ways: Market Indicator Features One commonly used feature in market trading is the z-score indicator, which determines how far (in standard deviations \u03c3) a data point x is from the mean \u00b5: zx = x \u2212 \u00b5 \u03c3 (5.1) In a way, the z-score indicator reveals anomalies in data. It has also been proven to be a useful indicator in determining future trends [5]. Equation (5.1) can be applied by using an expanding window approach with a window size of n time-steps. Let pt denote the closing price at time-step t, let SMA(pt\u2212n,t) denote simple moving average (SMA) of closing prices over n time-steps prior to t, and let STDDEV(pt\u2212n,t) denote the standard deviation of closing prices over n time-steps prior to t. Thus, the z-score at timestep t for a window size of n is zn t = pt \u2212 SMA(pt\u2212n,t) STDDEV(pt\u2212n,t) The z-score indicator can be extended to volume as well. Thus, the following are the features that are extracted: \u2022 Price Level: To determine price levels, the z-scores are calculated for the prices. This essentially expresses how far each of the prices in the time period are from the average price in the time period. \u2022 Price Change: To determine price changes, the current price is compared to the average of a window of prices prior to it, i.e. calculated for a window size n at timestep t PCn t = pt SMA(pt\u2212n,t) \u2212 1 and take the z-score of the result. \u2022 Volume Level: To determine volume levels, the z-scores are calculated for the traded volumes. As with price levels, this expresses how far the volume at each time step is from the volume in the time period. \u2022 Volume Change: Similar to price change, volume change for a window size n at time-step t is calculated: PCn t = vt SMA(vt\u2212n,t) \u2212 1 and the z-score of the result is taken. \u2022 Volatility: To determine volatility, exponential moving averages (EMA) are used to determine the rate of price change over a span of n days. The reasoning behind using EMA instead of SMA is that EMA gives more weight to the recent time-steps, whereas in SMA all time-steps are weighted equally. Let pt be the current price, and let n be the window size. The EMA at timestep t is given by EMAn t = 2pt n + 1 + \ufffdt j=t\u2212n pj n \ufffd 100 \u2212 2 n + 1 \ufffd Thus, the volatility over m days is given by Volatilitym t = EMAt,n \u2212 EMAt\u2212m,n EMAt\u2212m,n Along with these market indicator features and the price data, there is an additional state parameter which plays an essential role in determining rewards. Each time the agent chooses to buy, the price at which the agent buys the asset (the current open price) is stored into a current assets list. This list of prices is stored as part of the state as well. Each time the agent chooses to sell, all of the bought assets are exhausted (sold). Thus, the historical prices, market indicator features, and current assets list form the state. B. Action There are three actions that the agent can choose: 1) Buy: If the decision to buy is chosen, then the agent is choosing to buy 1 bitcoin at the current opening price. As mentioned previously, when the agent chooses to buy, the price gets appended to the current assets list. Note that the decision to buy 1 bitcoin is purely a design choice. The proposed framework can be applied to any quantity of bitcoins. 2) Sell: If the decision to sell is chosen, then the agent is choosing to sell all of its accrued assets at the current opening price. 3) Hold: If the decision to hold is chosen, then the agent does not do anything. C. Reward Upon taking an action, the agent receives a reward from the environment. Algorithm 1 depicts how rewards are handled. Algorithm 1: Macro-agent Reward Function 1 if action = hold then 2 reward \u2190 0 3 else if action = buy then 4 Append current open price to current assets list 5 reward \u2190 0 6 else if action = sell then 7 if there are no assets to sell then 8 reward \u2190 \u22121 9 else 10 reward \u2190 sell off all assets from current assets list and determine pro\ufb01t based on current open price 11 end 12 Clip Rewards Note that the rewards are clipped to {\u22121, 0, 1} based on negative, zero, or positive rewards, respectively. This is done 4 in order to have the agent deal with different types of market environments, e.g. high/low volatility, as well as reduce the impact of anomalous market shifts. Clipping rewards has proven to be a useful method when dealing with different scales of rewards when in various Atari games [6]. D. Deep Q-Network Architecture & Training To parameterize the value function, a deep Q-network is used. A simple multilayer perceptron with two hidden layers (with ReLu activation functions) is chosen. Furthermore, the Adam optimizer is used for training. The Q-network takes in as inputs the price data, market indicator data, and the current assets list, and outputs 3 Q-values which are then used to determine the optimal action. In Fig. 5 a depiction of the training framework for the macro-agent is provided. Fig. 5. Macro-agent Training Framework The details of how training happens within this framework are outlined in Algorithm 2. Note that for each epoch the agent is trained on the entire training set. E. Results It is interesting to see how the macro-agent would perform on its own. Although the closing and opening prices, and the prices used to calculate pro\ufb01ts for the macro-agent are usually not indicative of the true prices where trades may happen, they can hypothetically be assumed so to evaluate the performance of the agent. For these results, the agent was trained on the training set and tested on the test set, as de\ufb01ned previously, for 500 epochs. To understand the potential of the macro-agent, its performance is compared to two common investment strategies: \u2022 Buy and Hold investing: Buy and Hold investing is a naive long-term investment strategy where an investor buys an asset at the start of a time period and holds the asset in the hopes that it will accrue value over time. From the looks of Fig. 4 it is not expected that this strategy will yield any positive pro\ufb01t as the general trend in the test set is downwards. In this strategy, 10 Bitcoins are chosen to be bought and held over the time period. In order to simulate the Buy and Hold investing strategy, at each time-step the pro\ufb01t is plotted assuming that the investor decided to sell at each time-step, i.e. if Algorithm 2: Macro-agent Deep Q-Learning Training 1 Initialize replay memory M 2 Initialize \u03f5, and discount factor \u03b3 3 Initialize Q network with random weights 4 for epoch = 1 to E do 5 st \u2190 Reset environment (environment outputs initial state) 6 while not done do 7 at \u2190 Select random action with probability \u03f5 or choose action maxa Q(s, a; \u03b8) 8 st+1, rt, done \u2190 Act based on a (environment outputs new state, reward, and done) 9 if replay memory M is full then 10 Remove the \ufb01rst element from M 11 Append (st, at, rt, st+1, done) to replay memory M 12 b \u2190 Sample mini-batch from replay memory M at random 13 q \u2190 Initialize empty array of size |b| 14 foreach (si, ai, ri, si+1) \u2208 b do 15 qi = \ufffd ri + \u03b3 \u00b7 maxa Q(si+1, ai; \u03b8) not done ri done 16 end 17 Apply gradient descent with loss Es,a,r,s\u2032 \ufffd (qi \u2212 Q(si, ai; \u03b8))2\ufffd 18 Decay \u03f5 19 end 20 end p0 is the price the asset was initially bought at, at each time-step the difference pt \u2212 p0 is plotted. \u2022 Momentum investing: Momentum investing is where an investor chooses to buy or sell an asset at a certain time-step given the performance of the asset in the last n steps. To implement this, a simple moving average of the prices with a window size of n = 20 is computed, i.e. 20 minutes prior. At each time-step, if the current open price is less than the average price in the minutes prior, 1 Bitcoin is bought. If the current open price is greater than the average price in the minutes prior, all Bitcoins bought thus far are sold. In the case they are equal, all assets are held. These investment strategies serve as benchmarks and are compared with the macro-agent. The macro-agent is similar to momentum investing with the only difference being that the policy of when to buy, sell, or hold is different. Similar to momentum investing, the macro-agent buys one bitcoin on a decision to buy and exhausts all assets on a decision to sell. Fig. 6 depicts the performance comparisons between the different strategies. Fig. 6 compares the strategies in terms of pro\ufb01t. The accumulated pro\ufb01t is plotted for the momentum investing strategy and the macro-agent. The macro-agent\u2019s accumulated pro\ufb01t 5 Fig. 6. Performances of Various Investment Strategies Realized-PNL Graph growth is not very volatile, indicating that the macro-agent strategy is stable as well. VI. DETAILS OF THE MIRCO-AGENT In the previous section, the results indicated that the macro-agent performed well compared to two known investment strategies. However, one issue with the macro-agent is that it assumes that the trade occurs at the opening prices. In real market environments a trade happening at the opening price is most often not the case. Therefore, there lies this uncertainty in the framework thus far of what price the order should be set at. This motivates the purpose for the microagent. Many of the ideas used here are derived from [3] and [4]. A. Introduction In \ufb01nancial markets, there is an order book, a data structure that holds all bid and ask price/volume values. Fig. 7 depicts a snapshot of an order book state. Fig. 7. Example of an Order Book State The order book is separated into two parts: bids and asks. The bid section contains all the prices and quantities buyers are willing to buy at. The ask section contains all the prices and quantities sellers are willing to sell at. There are two main ways that a trader can place an order in the order book: 1) Market Order: When a trader places a market order, they are placing an order at the market price, i.e. the best price of the opposing side of the order book. Most often, this order is immediately \ufb01lled. However, depending on the quantity the order was placed for, the order might not be executed at the same prices, as it depends on how much quantity is available at market price. Another disadvantage is that market orders usually come with additional trading fees. Depending on the state, this may even yield negative pro\ufb01t. 2) Limit Order: When a trader places a limit order, they are essentially demanding the order price (or a better price). A limit order guarantees the trader the order price, however, it is possible for the order to never be \ufb01lled. When a trader places a limit order, they stand behind all the traders that have placed an order at that price. An advantage to placing limit orders is that since limit orders provide liquidity to other market participants, exchanges incentivize limit orders by removing additional trading fees. Once a trader places an order, a match engine attempts to match the trader\u2019s order to orders on the opposing order book side. Only when there is a matching order does a trade happen. The agent should be able to optimally place orders in the order book. As noted previously, the agent has two order options: market orders or limit orders. However, there is a trade-off between placing market orders and placing limit orders. Therefore, the goal of the agent is to optimize on this trade-off: is there an action better than simply placing a market order at the beginning of the time horizon? This is a challenging optimization problem since there are many unknown variables here: \u2022 Other Market Participants: This agent is not the only one trading on the market. There are many other market participants that the agent interacts with. \u2022 Adversarialness: Among these market participants, there are also algorithmic traders, some of which might be playing adversarially, e.g. Bitcoin whales. The agent must also optimize over these unknowns as well. In order to do that, the match engine needs to accurately simulate the market. Since this problem is posed as an RL problem, such a simulator is essential, since the agent requires reward feedback. An open-source matching engine [7] is used to help simulate the exchange match engine. However, since the data is limited to the time periods it was collected for, the match engine simulator is only indicative to that time period. Therefore, there is still a disadvantage to using an arti\ufb01cial match engine. B. Environment In reinforcement learning problems, the environment is essential since it provides the agent with subsequent states, 6 as well as reward feedback. The environment for this problem is vastly different than the macro-agent\u2019s environment. The environment takes in as input the order side, i.e. buy or sell from the macro-agent. The two major components of the environment are the matching engine and the order book. The order book plays a key role in the environment. Bid, ask, and trade data are collected from the WebSocket. Additionally, at the start of data collection, a snapshot of the current order book state for the \ufb01rst 20 levels on each side are taken. Based on this snapshot, and the bid and ask data alone, the entire historical order book is created. The data is again split into a training and test set similar to the macro-agent. During training, the environment starts at a random time-step within the training set (at the start of a minute). The environment lasts until the end of the minute. Within the minute, the agent chooses to place an order which the matching engine attempts to match. If for some reason the order has not been matched until the last time-step, the environment forces a market order for all remaining assets. The environment provides reward feedback only when a trade has occurred, or when the time horizon has been exhausted. This single interaction is de\ufb01ned as one epoch. C. State As part of the state, the concept of private and market variables is used: \u2022 Private Variables: Private variables contain two features: quantity remaining, and time remaining. \u2022 Market Variables: Market variables contain order book states and trade data from the past 30 time-steps. An order book state consists of the bid and ask prices and quantities up to 20 levels. Trade data consists of the price and quantity, and order side at which the last trade occurred. D. Action The action the agent decides is the price at which to place a limit order. More speci\ufb01cally, the agent chooses an integer action a \u2208 [\u221250, 50], such that the price, pt, at time-step t is pt = pmt + 0.10a where pmt is the market price. Thus, there are a total of 101 possible discrete actions the agent can choose from. E. Reward Recall that the goal of the micro-agent is to optimally place limit orders. Therefore, the agent is rewarded accordingly. The reward function is the difference between what the agent was able to sell at and the market price before the order was placed. The Volume Weighted Average Price (VWAP) is used to determine the aggregated price the agent was able to trade at. Thus, the reward function is Rt = pmt \u2212 1 \ufffd i vi \ufffd p\u2208P vp \u00b7 p where pmt is the market price, \ufffd i vi is the total volume of assets, and P is all the prices the agent ordered at. F. Deep Q-Network Architecture & Training For this agent, a similar training framework to the macroagent is used. However, a different network architecture is used. Dueling Deep Q-Network For many states, the actions a on the edge of the order book may have little value to the states. Therefore, for some states, it may be unnecessary to estimate the action values for all actions. Dueling Deep Q-Networks (DDQNs) provide a solution to this. First applied to Atari games, DDQNs not only help to speed up training, but also yield more reliable Q-values [8]. Fig. 8. Dueling Deep Q-Network Framework Recall that a Q-value can be decomposed into two parts: V (s), the value of being in a particular state s, and A(s, a), the advantage of taking action a in state s, i.e. the Q-value is decomposed as Q(s, a) = V (s) + A(s, a) DDQNs decouple the prediction into two network streams (Fig. 8): value stream, which estimates V (s), and advantage stream, which estimates A(s, a). This decoupling allows the agent to learn which states might not be valuable enough to explore every action. After this decoupling, there is an aggregation layer which aggregates the two streams by the following: Q(s, a; \u03b8, \u03b1, \u03b2) = \u02c6V (s; \u03b8, \u03b2) + \ufffd \u02c6A(s, a; \u03b8, \u03b1) \u2212 1 |A| \ufffd a\u2032 \u02c6A(s, a\u2032; \u03b8, \u03b1) \ufffd where \u03b8 are the weights for the MLP, \u03b1 are the weights for the advantage stream, and \u03b2 are the weight for the value stream. The concept of dueling deep Q-networks is applied to the micro-agent. Besides the dueling concept, the rest of the training framework remains the same (see Fig. 3). The modi\ufb01cations to Algorithm 2 are detailed in Algorithm 3. 7 Algorithm 3: Micro-agent Deep Q-Learning Training 1 Initialize replay memory M 2 Initialize \u03f5, discount factor \u03b3 3 Initialize Q network with random weights 4 for epoch = 1 to E do 5 st \u2190 Reset environment (environment outputs random state and gets inputs from macro-agent) 6 while t \u2a7d 60 seconds do 7 at \u2190 Select random action with probability \u03f5 or choose action maxa Q(s, a; \u03b8) 8 st+1, rt, done \u2190 Act based on a (environment outputs new state, reward, and done, based on match engine output) 9 if replay memory M is full then 10 Remove the \ufb01rst element from M 11 Append (st, at, rt, st+1, done) to replay memory M 12 b \u2190 Sample mini-batch from replay memory M at random 13 q \u2190 Initialize empty array of size |b| 14 foreach (si, ai, ri, si+1) \u2208 b do 15 qi = \ufffd ri + \u03b3 \u00b7 maxa Q(si+1, ai; \u03b8) not done ri done 16 end 17 Apply gradient descent with loss Es,a,r,s\u2032 \ufffd (qi \u2212 Q(si, ai; \u03b8))2\ufffd 18 Decay \u03f5 19 end 20 end G. Results In this section, the performance of the micro-agent strategy is evaluated. The basic goal behind this agent is to optimally place limit orders to buy or sell an asset within the allotted time horizon. Here, the agent is tested through two scenarios, each evaluating its performance on ideal and un-ideal states. 1) Market Trending Downwards: In this scenario a timestep is chosen within the test set where the market is generally trending downwards. Here, the ideal case is to buy an asset, since the price is generally decreasing. Fig. 9 depicts the snapshot chosen to test the agent against a downward trend. More speci\ufb01cally, the agent is tested on the 07:36 to 07:37 time horizon. Buying an Asset When buying an asset on an upward trend, the agent decided to take an action of \u221211. This meant setting a limit order to buy at $1.10 lower than market price. This resulted in immediate execution. This was the right choice to make, since the market was trending downward. Selling an Asset When selling an asset on a downward trend, the agent decided to take an action of 29. This amounted to a limit Fig. 9. Order Book Visualization for Downward Trend order price of $2.90 greater than the market price. Although this may not seem like the optimal decision, given that the agent was able to recognize the price drop, it decided to immediately sell off its asset. At such a price it is almost guaranteed for a trade to occur, as was the case here. The agent was able to sell off 1 bitcoin in the \ufb01rst step (before the market dropped). 2) Market Trending Upwards: In this scenario a time-step within the test set is chosen where the market is generally trending upwards. Here, selling an asset is the ideal case, since the price is generally increasing. Fig. 10. Order Book Visualization for Upward Trend Fig. 10 depicts the snapshot chosen to test the agent on an upward trend. More speci\ufb01cally, agent was tested on the 10:09 to 10:10 time horizon. Buying an Asset When buying an asset on an upward trend, the agent decided to take an action of 18. This meant setting a limit order to buy at $1.80 higher than market price. This resulted 8 ",
    "Results": "Results The end-to-end pipeline is evaluated using the test set de\ufb01ned in Section 5. Similar to the evaluation of the macroagent, the multi-agent strategy is compared to the Buy and Hold investing and Momentum investing strategies. In Fig. 12 the performance comparisons between the Buy and Hold investment, Momentum investment, Macro-agent, and Multi-agent (Macro-agent and Micro-agent) strategies are given. It is interesting to see that the multi-agent approach under-performs in comparison to the macro-agent. Previously, in the evaluation of the micro-agent, it was noticed that often times the agent would decide that the right decision was to place an order towards the opposing side of the order book, i.e. a price slightly worse than market price. Although this resulted in immediate execution, this resulted in a drop in cumulative pro\ufb01t. In terms of total orders placed by the micro-agent in the multi-agent setting, 91% of all orders placed were limit orders. Thus, the micro-agent was able to optimize placing limit orders. Note that in the other strategies hypothetical 9 ",
    "Conclusion": "CONCLUSION A multi-agent reinforcement learning framework was provided to solve the problem of optimizing market making. The results showed that the policy these agents were able to learn led to a stable trading strategy which resulted in a low-volatile linear growth in pro\ufb01t. Applying reinforcement learning to such a problem also shows that it has the ability to perform well in complex environments, and is a viable tool that can be used for market making. ",
    "References": "REFERENCES [1] Matthew F Dixon. \u201cA High Frequency Trade Execution Model for Supervised Learning.\u201d arXiv preprint arXiv:1710.03870 (2017). [2] Prakhar Ganesh, Puneet Rakheja. \u201cDeep Reinforcement Learning in High Frequency Trading.\u201d arXiv preprint arXiv:1809.01506 (2018). [3] Yuriy Nevmyvaka, Yi Feng, Michael Kearns \u201cReinforcement Learning for Optimized Trade Execution.\u201d https://www.cis.upenn.edu/ mkearns/papers/rlexec.pdf (2006). [4] Marc Juchli \u201cLimit order placement optimization with Deep Reinforcement Learning\u201d https://repository.tudelft.nl/islandora/object/uuid:e2e99579-541b4b5a-8cbb-36ea17a4a93a?collection=education (2018). [5] \u201cUse Z-Scores To Maximize Your Portfolio\u2019s Potential\u201d https://seekingalpha.com/article/3224666-use-z-scores-to-maximizeyour-portfolios-potential (2015). [6] Hado van Hasselt, et al. \u201cLearning values across many orders of magnitude\u201d arXiv preprint arXiv:1602.07714 (2016). [7] Gavin Chan. \u201cLight Matching Engine\u201d https://github.com/gavincyi/LightMatchingEngine (2017). [8] Ziyu Wang, et al. \u201cDueling Network Architectures for Deep Reinforcement Learning\u201d arXiv preprint arXiv:1511.06581 (2016). [9] Jae Won Lee, Jangmin O. \u201cA Multi-agent Q-learning Framework for Optimizing Stock Trading Systems\u201d ftp://ftp.cse.buffalo.edu/users/azhang/disc/springer/0558/papers/2453/ 24530153.pdf (2002). [10] Investopedia \u201cRandom Walk Theory\u201d https://www.investopedia.com/terms/r/randomwalktheory.asp (2018). [11] Olivier Gueant. \u201cOptimal market making\u201d arXiv preprint arXiv:1605.01862 (2017). 10 ",
    "title": "Optimizing Market Making using Multi-Agent Reinforcement Learning",
    "paper_info": "Optimizing Market Making using Multi-Agent Reinforcement Learning\nYagna Patel\nyagna.patel@berkeley.edu\nAbstract\u2014 In this paper, reinforcement learning is applied\nto the problem of optimizing market making. A multi-agent\nreinforcement learning framework is used to optimally place\nlimit orders that lead to successful trades. The framework\nconsists of two agents. The macro-agent optimizes on making\nthe decision to buy, sell, or hold an asset. The micro-agent\noptimizes on placing limit orders within the limit order book.\nFor the context of this paper, the proposed framework is applied\nand studied on the Bitcoin cryptocurrency market. The goal of\nthis paper is to show that reinforcement learning is a viable\nstrategy that can be applied to complex problems (with complex\nenvironments) such as market making.\nI. INTRODUCTION\nAlgorithmic trading, and in particular high-frequency al-\ngorithmic trading (HFT), has gained immense popularity in\nthe recent decade. With advances in hardware and software,\nalgorithmic trading has rapidly become the norm. The in-\ncreasing popularity of machine learning has slowly made\nits way to \ufb01nancial markets [1], where it is primarily used\nto predict price movements of assets. However, there are\na number of challenges that these classic machine learning\ntechniques entail:\n1) Prediction time: In machine learning, model complex-\nity can have an impact on prediction time. Many times,\nin the supervised learning setting, neural networks are\nused to make predictions. Due to the computational\ncomplexity that comes with these models, as the model\ncomplexity increases, the decision time also increases\n[2]. In the HFT setting, by the time the model makes\na prediction, it may already be too late to take the\npredicted action. The problem then becomes, how can\nthese added latency costs be incorporated into our\nprediction?\n2) Prediction accuracy:\nFinancial markets are second-\norder chaotic systems, i.e. they are highly unpre-\ndictable since markets can respond to predictions. The\ngeneral rule of thumb in \ufb01nance is that the historical\nperformance of an asset does not predict the future\nperformance of the asset, i.e. forecasting and predict-\ning the market from historical performance alone is\nvirtually impossible. In fact, markets are sometimes\ncompared to random walk processes [10]. Therefore,\napproaches that solely rely on the historical perfor-\nmance of an asset are likely to have low prediction\naccuracy.\n3) Policy optimization: Suppose that our model predicted\na 55% chance of increase and a 45% chance of\ndecrease for an asset. How can our model\u2019s prediction\nbe converted into an action? An optimized policy and\ncertain decision thresholds are needed to turn the pre-\ndiction into an action. However, coming up with such\na policy is usually done by hand. The development\nand optimization of these policies are commonly done\nby mix of humans and computers. Therefore, if the\nmarket suddenly shifted, these policies would likely\nnot be able to adapt.\nII. RELATED WORK\nThe concept of reinforcement learning is relatively new in\n\ufb01nance. There has been little research done into whether or\nnot reinforcement learning is a viable approach for market\nmaking. The optimization problem of market making is a\ncomplex problem [11], and reinforcement learning is not a\ncommon approach used to solve it. Multi-agent approaches\nto stock trading have been taken previously [9]. However,\nthey do not account for placing limit orders. The general\nconcept of the micro-agent (see section 3) has been shown\nby Nevmyvaka et al. [3], and further extended by Juchli [4].\nThese papers present the problem of optimally buying (or\nselling) an asset over a \ufb01xed time horizon. This is similar to\nthe idea of the micro-agent. The work for the micro-agent is\nsubstantially based, and builds upon the ideas presented in\nthese two papers.\nIII. PROBLEM FORMULATION\nIn this paper, a less common approach of reinforcement\nlearning is utilized to create an optimal market maker. More\nspeci\ufb01cally, a multi-agent approach (with two agents) is used:\n1) Macro-agent: The macro-agent is given minute tick\ndata (data at a macro-level) and makes the decision to\nbuy, sell, or hold the asset.\n2) Micro-agent: The micro-agent is given order book data\n(data at a micro-level) and makes the decision of where\nto place the order within the limit order book.\nFig. 1.\nTime-steps where agents take actions. The Macro-agent takes\nactions at every bold black line, while the Micro-agent takes actions between\nthe dashed lines.\nAt the start of every minute, the macro-agent makes the\ndecision to buy, sell, or hold the asset. A running count\nof how many buys it chooses to make is kept. The total\nnumber of buys before the sell is fed into the micro-agent,\n1\narXiv:1812.10252v1  [q-fin.TR]  26 Dec 2018\n",
    "GPTsummary": "                    - (1): The research background of this article is algorithmic trading, in particular, high-frequency algorithmic trading (HFT), which has become popular in recent years. However, the challenges that traditional machine learning techniques face in HFT (prediction time, prediction accuracy and policy optimization) have yet to be solved.\n \n                    - (2): Past methods used in finance, particularly in market making, are based on statistical arbitrage and heuristic optimization approaches. Statistical arbitrage is based on searching for some statistical relationship between securities that can be used to determine when a security is overpriced or underpriced. Whereas heuristic optimization approaches are employed to determine a price for a security that balances supply and demand. The problem with these methods is that they are not fast enough for the constant updating of financial data needed in HFT. The approach in this paper is motivated by the potential of reinforcement learning (RL) to learn from and adapt to a dynamic environment very quickly.\n \n                    - (3): The proposed approach in this paper is to use a multi-agent RL framework to optimize for successful trades as a market maker. The framework consists of a macro-agent that optimizes buying, selling or holding an asset based on minute tick data and a micro-agent that optimizes the placement of limit orders within the limit order book based on order book data. The proposed approach is applied to the Bitcoin cryptocurrency market due to its complex and dynamic environment.\n  \n                    - (4): The performance of the proposed approach is evaluated through backtesting and simulation. The results suggest that using the multi-agent RL framework can achieve better results than traditional statistical arbitrage and heuristic optimization approaches. The proposed approach showcases the potential of RL as a viable strategy for complex problems such as market making in HFT.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work lies in the application of a multi-agent reinforcement learning framework to optimize market making, which has the potential to outperform traditional statistical arbitrage and heuristic optimization approaches in high-frequency trading (HFT).\n\n- (2): In terms of innovation, the proposed approach provides a novel solution to the challenges faced in HFT using RL. The performance of the approach was evaluated through backtesting and simulation, showcasing its potential for success. However, the lack of real-world testing and comparison with other state-of-the-art approaches is a weakness. In terms of workload, the proposed framework requires significant computational resources, which may prove challenging for some implementations.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in the application of a multi-agent reinforcement learning framework to optimize market making, which has the potential to outperform traditional statistical arbitrage and heuristic optimization approaches in high-frequency trading (HFT).\n\n- (2): In terms of innovation, the proposed approach provides a novel solution to the challenges faced in HFT using RL. The performance of the approach was evaluated through backtesting and simulation, showcasing its potential for success. However, the lack of real-world testing and comparison with other state-of-the-art approaches is a weakness. In terms of workload, the proposed framework requires significant computational resources, which may prove challenging for some implementations.\n\n\n"
}