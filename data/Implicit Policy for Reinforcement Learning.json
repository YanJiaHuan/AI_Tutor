{
    "Abstract": "Abstract We introduce Implicit Policy, a general class of expressive policies that can \ufb02exibly represent complex action distributions in reinforcement learning, with ef\ufb01cient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality. 1 ",
    "Introduction": "Introduction Reinforcement Learning (RL) combined with deep neural networks have led to a wide range of successful applications, including the game of Go, robotics control and video game playing [32, 30, 24]. During the training of deep RL agent, the injection of noise into the learning procedure can usually prevent the agent from premature convergence to bad locally optimal solutions, for example, by entropy regularization [30, 23] or by explicitly optimizing a maximum entropy objective [13, 25]. Though entropy regularization is much simpler to implement in practice, it greedily optimizes the policy entropy at each time step, without accounting for future effects. On the other hand, maximum entropy objective considers the entropy of the distribution over entire trajectories, and is more conducive to theoretical analysis [2]. Recently, [13, 14] also shows that optimizing the maximum entropy objective can lead to desirable properties such as robustness and multi-modal policy. Can we preserve the simplicity of entropy regularization while attaining desirable properties under maximum entropy framework? To achieve this, a necessary condition is an expressive representation of policy. Though various \ufb02exible probabilistic models have been proposed in generative modeling [10, 37], such models are under-explored in policy based RL. To address such issues, we propose \ufb02exible policy classes and ef\ufb01cient algorithms to compute entropy regularized policy gradients. In Section 3, we introduce Implicit Policy, a generic policy representation from which we derive two expressive policy classes, Normalizing Flows Policy (NFP) and more generally, Non-invertible Blackbox Policy (NBP). NFP provides a novel architecture that embeds state information into Normalizing Flows; NBP assumes little about policy architecture, yet we propose algorithms to ef\ufb01ciently compute entropy regularized policy gradients when the policy density is not accessible. In Section 4, we show that entropy regularization optimizes a lower bound of maximum entropy objective. In Section 5, we show that when combined with entropy regularization, expressive policies achieve competitive performance on benchmarks and leads to robust and multi-modal policies. 2 Preliminaries 2.1 ",
    "Background": "Background We consider the standard RL formalism consisting of an agent interacting with the environment. At time step t \u2265 0, the agent is in state st \u2208 S, takes action at \u2208 A, receives instant reward rt \u2208 R and transitions to next state st+1 \u223c p(st+1|st, at). Let \u03c0 : S \ufffd\u2192 A be a policy. The objective of \u2217 This research was supported by an Amazon Research Award (2017) and AWS cloud credits. arXiv:1806.06798v2  [cs.LG]  3 Feb 2019 ",
    "Related Work": "Related Work A large number of prior works have implemented policy gradient algorithms with entropy regularization [30, 31, 23, 26], which boost exploration by greedily maximizing policy entropy at each time step. In contrast to such greedy procedure, maximum entropy objective considers entropy over the entire policy trajectories [13, 25, 29]. Though entropy regularization is simpler to implement in practice, [12, 13] argues in favor of maximum entropy objective by showing that trained policies can be robust to noise, which is desirable for real life robotics tasks; and multi-modal, a potentially desired property for exploration and \ufb01ne-tuning for downstream tasks. However, their training procedure is fairly complex, which consists of training a soft Q function by \ufb01xed point iteration and a neural sampler by Stein variational gradient [21]. We argue that properties as robustness and multi-modality are attainable through simple entropy regularized policy gradient algorithms combined with expressive policy representations. Prior works have studied the property of maximum entropy objective [25, 39], entropy regularization [26] and their connections with variants of operators [2]. It is commonly believed that entropy regularization greedily maximizes local policy entropy and does not account for how a policy update impacts future states. In Section 4, we show that entropy regularized policy gradient update maximizes a lower bound of maximum entropy objective, given constraints on the differences between consecutive policy iterates. This partially justi\ufb01es why simple entropy regularization combined with expressive policy classes can achieve competitive empirical performance in practice. There is a number of prior works that discuss different policy architectures. The most common policy for continuous control is unimodal Gaussian [30, 31, 23]. [14] discusses mixtures of Gaussian, which 2 can represent multi-modal policies but it is necessary to specify the number of modes in advance. [13] also represents a policy using implicit model, but the policy is trained to sample from the soft Q function instead of being trained directly. Recently, we \ufb01nd [11] also uses Normalizing Flows to represent policies, but their focus is learning an hierarchy and involves layers of pre-training. Contrary to early works, we propose to represent \ufb02exible policies using implicit models/Normalizing Flows and ef\ufb01cient algorithms to train the policy end-to-end. Implicit models have been extensively studied in probabilistic inference and generative modeling [10, 17, 19, 37]. Implicit models de\ufb01ne distributions by transforming source noise via a forward pass of neural networks, which in general sacri\ufb01ce tractable probability density for more expressive representation. Normalizing Flows are a special case of implicit models [27, 5, 6], where transformations from source noise to output are invertible and allow for maximum likelihood inference. Borrowing inspirations from prior works, we introduce implicit models into policy representation and empirically show that such rich policy class entails multi-modal behavior during training. In [37], GAN [10] is used as an optimal density estimator for likelihood free inference. In our work, we apply similar idea to compute entropy regularization when policy density is not available. 3 Implicit Policy for Reinforcement Learning We assume the action space A to be a compact subset of Rm. Any suf\ufb01ciently smooth stochastic policy can be represented as a blackbox f\u03b8(\u00b7) with parameter \u03b8 that incorporates state information s and independent source noise \u03f5 sampled from a simple distribution \u03c10(\u00b7). In state s, the action a is sampled by a forward pass in the blackbox. a = f\u03b8(s, \u03f5), \u03f5 \u223c \u03c10(\u00b7). (2) For example, Gaussian policy is reduced to a = \u03c3\u03b8(s) \u00b7 \u03f5 + \u00b5\u03b8(s) where \u03c10 is standard Gaussian [30]. In general, the distribution of at is implicitly de\ufb01ned: for any set A of A, P(a \u2208 A|s) = \ufffd \u03f5:f\u03b8(s,\u03f5)=a \u03c10(\u03f5)d\u03f5. Let \u03c0\u03b8(\u00b7|s) be the density of this distribution2. We call such policy Implicit Policy as similar ideas have been previous explored in implicit generative modeling literature [10, 19, 37]. In the following, we derive two expressive stochastic policy classes following this blackbox formulation, and propose algorithms to ef\ufb01ciently compute entropy regularized policy gradients. 3.1 Normalizing Flows Policy (NFP) We \ufb01rst construct a stochastic policy with Normalizing Flows. Normalizing Flows [27, 6] have been applied in variational inference and probabilistic modeling to represent complex distributions. In general, consider transforming a source noise \u03f5 \u223c \u03c10(\u00b7) by a series of invertible nonlinear function g\u03b8i(\u00b7), 1 \u2264 i \u2264 K each with parameter \u03b8i, to output a target sample x, x = g\u03b8K \u25e6 g\u03b8K\u22121 \u25e6 ... \u25e6 g\u03b82 \u25e6 g\u03b81(\u03f5). (3) Let \u03a3i be the Jacobian matrix of g\u03b8(\u00b7), then the density of x is computed by chain rule, log p(x) = log p(\u03f5) + K \ufffd i=1 log det(\u03a3i). (4) For a general invertible transformation g\u03b8i(\u00b7), computing det(\u03a3i) is expensive. We follow the architecture of [5] to ensure that det(\u03a3i) is computed in linear time. To combine state information, we embed state s by another neural network L\u03b8s(\u00b7) with parameter \u03b8s and output a state vector L\u03b8s(s) with the same dimension as \u03f5. We can then insert the state vector between any two layers of (3) to make the distribution conditional on state s. In our implementation, we insert the state vector after the \ufb01rst transformation (we detail our architecture design in Appendix C). a = g\u03b8K \u25e6 g\u03b8K\u22121 \u25e6 ... \u25e6 g\u03b82 \u25e6 (L\u03b8s(s) + g\u03b81(\u03f5)). (5) Though the additive form of L\u03b8s(s) and g\u03b81(\u03f5) may in theory limit the capacity of the model, in practice we \ufb01nd the resulting policy still very expressive. For simplicity, we denote the above 2In future notations, when the context is clear, we use \u03c0\u03b8(\u00b7|s) to denote both the density of the policy as well as the policy itself: for example, a \u223c \u03c0\u03b8(\u00b7|s) means sampling a from the policy; log \u03c0\u03b8(a|s) means the log density of policy at a in state s. 3 transformation (5) as a = f\u03b8(s, \u03f5) with parameter \u03b8 = {\u03b8s, \u03b8i, 1 \u2264 i \u2264 K}. It is obvious that \u03f5 \u2194 a = f\u03b8(s, \u03f5) is still invertible between a and \u03f5, which is critical for computing log \u03c0\u03b8(a|s) according to (4). Such representations build complex policy distributions with explicit probability density \u03c0\u03b8(\u00b7|s), and hence entail training using score function gradient estimators. Since there is no analytic form for entropy, we use samples to estimate entropy by re-parameterization, H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = Ea\u223c\u03c0\u03b8(\u00b7|s) \ufffd \u2212 log \u03c0\u03b8(a|s) \ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2212 log \u03c0\u03b8(f\u03b8(s, \u03f5)|s) \ufffd . The gradient of entropy can be easily computed by a pathwise gradient and easily implemented using back-propagation \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2212 \u2207\u03b8 log \u03c0\u03b8(f\u03b8(s, \u03f5)|s) \ufffd . On-policy algorithm for NFP. Any on-policy policy optimization algorithms can be easily combined with NFP. Since NFP has explicit access to policy density, it allows for training using score function gradient estimators with ef\ufb01cient entropy regularization. 3.2 Non-invertible Blackbox Policy (NBP) The forward pass in (2) transforms the simple noise distribution \u03f5 \u223c \u03c10(\u00b7) to complex action distribution at \u223c \u03c0\u03b8(\u00b7|st) through the blackbox f\u03b8(\u00b7). However, the mapping \u03f5 \ufffd\u2192 at is in general non-invertible and we do not have access to the density \u03c0\u03b8(\u00b7|st). We derive a pathwise gradient for such cases and leave all the proof in Appendix A. Theorem 3.1 (Stochastic Pathwise Gradient). Given an implicit stochastic policy at = f\u03b8(st, \u03f5), \u03f5 \u223c \u03c10(\u00b7). Let \u03c0\u03b8 be the implicitly de\ufb01ned policy. Then the pathwise policy gradient for the stochastic policy is \u2207\u03b8J(\u03c0\u03b8) = E\u03c0\u03b8 \ufffd E\u03f5\u223c\u03c10(\u00b7)[\u2207\u03b8f\u03b8(s, \u03f5)\u2207aQ\u03c0\u03b8(s, a)|a=f\u03b8(s,\u03f5)] \ufffd . (6) To compute the gradient of policy entropy for such general implicit policy, we propose to train an additional classi\ufb01er c\u03c8 : S \u00d7 A \ufffd\u2192 R with parameter \u03c8 along with policy \u03c0\u03b8. The classi\ufb01er c\u03c8 is trained to minimize the following objective given a policy \u03c0\u03b8 min \u03c8 Ea\u223c\u03c0\u03b8(\u00b7|s) \ufffd \u2212 log \u03c3(c\u03c8(a, s)) \ufffd + Ea\u223cU(A) \ufffd \u2212 log(1 \u2212 \u03c3(c\u03c8(a, s))) \ufffd , (7) where U(A) is a uniform distribution over A and \u03c3(\u00b7) is the sigmoid function. We have lemma A.1 in Appendix A.2 to guarantee that the optimal solution \u03c8\u2217 of (7) provides an estimate of policy density, c\u03c8\u2217(s, a) = log \u03c0\u03b8(a|s) |A|\u22121 . As a result, we could evaluate the entropy by simple re-parametrization H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2212log \u03c0(f\u03b8(s, \u03f5)|s) \ufffd \u2248 E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2212c\u03c8(f\u03b8(s, \u03f5), s) \ufffd . Further, we can compute gradients of the policy entropy through the density estimate as shown by the following theorem. Theorem 3.2 (Unbiased Entropy Gradient). Let \u03c8\u2217 be the optimal solution from (7), where the policy \u03c0\u03b8(\u00b7|s) is given by implicit policy a = f\u03b8(s, \u03f5), \u03f5 \u223c \u03c10(\u00b7). The gradient of entropy \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd can be computed as \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = \u2212E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8c\u03c8\u2217(f(\u03b8, \u03f5), s) \ufffd (8) It is worth noting that to compute \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd , simply plugging in c\u03c8\u2217(a, s) to replace log \u03c0\u03b8(a|s) in the entropy de\ufb01nition does not work in general, since the optimal solution \u03c8\u2217 of (7) implicitly depends on \u03b8. However, fortunately in this case the additional term vanishes. The above theorem guarantees that we could apply entropy regularization even when the policy density is not accessible. Off-policy algorithm for NBP. We develop an off-policy algorithm for NBP. The agent contains an implicit f\u03b8(s, \u03f5) with parameter \u03b8, a critic Q\u03c6(s, a) with parameter \u03c6 and a classi\ufb01er c\u03c8(s, a) with parameter \u03c8. At each time step t, we sample action at = f\u03b8(st, \u03f5), \u03f5 \u223c \u03c10(\u00b7) and save experience tuple {st, at, rt, st+1} to a replay buffer B. During training, we sample a mini-batch of tuples from B, update critic Q\u03c6(s, a) using TD learning, update policy f\u03b8(s, \u03f5) using pathwise gradient (6) and update classi\ufb01er c\u03c8(s, a) by gradient descent on (7). We also maintain target networks f\u03b8\u2212(s, \u03f5), Q\u03c6\u2212(s, a) with parameter \u03b8\u2212, \u03c6\u2212 to stabilize learning [24, 32]. The pseudocode is listed in Appendix D. 4 ",
    "Experiments": "Experiments Our experiments aim to answer the following questions: (1) Will expressive policy be hard to train, does implicit policy provide competitive performance on benchmark tasks? (2) Are implicit policies robust to noises on locomotion tasks? (3) Does implicit policy + entropy regularization entail multi-modal policies as displayed under maximum entropy framework [13]? To answer (1), we evaluate both NFP and NBP agent on benchmark continuous control tasks in MuJoCo [36] and compare with baselines. To answer (2), we compare NFP with unimodal Gaussian policy on locomotion tasks with additive observational noises. To answer (3), we illustrate the multi-modal capacity of both policy representations on specially designed tasks illustrated below, and compare with baselines. In all experiments, for NFP, we implement with standard PPO for on-policy update to approximately enforce the KL constraint (10) as in [31]; for NBP, we implement the off-policy algorithm developed in Section 3. In Appendix C and F, we detail hyper-parameter settings in the experiments and provide a small ablation study. 5 5.1 Locomotion Tasks Benchmark tasks. One potential disadvantage of expressive policies compared to simple policies (like unimodal Gaussian) is that they pose a more serious statistical challenge due to a larger number of parameters. To see if implicit policy suffers from such problems, we evaluate NFP and NBP on MuJoCo benchmark tasks. For each task, we train for a prescribed number of time steps, then report the results averaged over 5 random seeds. We compare the results with baseline algorithms, such as DDPG [32], SQL [13], TRPO [30] and PPO [31], where baseline TPRO and PPO use unimodal Gaussian policies. As can be seen from Table 1, both NFP and NBP achieve competitive performances on benchmark tasks: they outperform DDPG, SQL and TRPO on most tasks. However, baseline PPO tends to come on top on most tasks. Interestingly on HalfCheetah, baseline PPO gets stuck on a locally optimal gait, which NFP improves upon by a large margin. Tasks Timesteps DDPG SQL TRPO PPO NFP NBP Hopper 2.00 \u00b7 106 \u2248 1100 \u2248 1500 \u2248 1250 \u2248 2130 \u2248 1640 \u2248 1880 HalfCheetah 1.00 \u00b7 107 \u2248 6500 \u2248 8000 \u2248 1800 \u2248 1590 \u2248 4000 \u2248 6560 Walker2d 5.00 \u00b7 106 \u2248 1600 \u2248 2100 \u2248 800 \u2248 3800 \u2248 3000 \u2248 2450 Ant 1.00 \u00b7 107 \u2248 200 \u2248 2000 \u2248 0 \u2248 4440 \u2248 2500 \u2248 2070 Table 1: A comparison of implicit policy optimization with baseline algorithms on MuJoCo benchmark tasks. For each task, we show the average rewards achieved after training the agent for a \ufb01xed number of time steps. The results for NFP and NBP are averaged over 5 random seeds. The results for DDPG, SQL and TRPO are approximated based on the \ufb01gures in [14], PPO is from OpenAI baseline implementation [4]. We highlight the top two algorithms for each task in bold font. Both TRPO and PPO use unimodal Gaussian policies. Robustness to Noisy Observations. We add independent Gaussian noise N(0, 0.12) to each component of the observations to make the original tasks partially observable. Since PPO with unimodal Gaussian achieves leading performance on noise-free locomotion tasks across on-policy baselines (A2C [23], TRPO [30]) as shown in [31] and Appendix E.1, we compare NFP only with PPO with unimodal Gaussian on such noisy locomotion tasks. In Figure 1, we show the learning curves of both agents, where on many tasks NFP learns signi\ufb01cantly faster than unimodal Gaussian. Why complex policies may add to robustness? We propose that since these control tasks are known to be solved by multiple separate modes of policy [22], observational noises potentially blur these modes and make it harder for a unimodal Gaussian policy to learn any single mode (e.g. unimodal Gaussian puts probability mass between two neighboring modes [18]). On the contrary, NFP can still navigate a more complex reward landscape thanks to a potentially multi-modal policy distribution and learn effectively. We leave a more detailed study of robustness, multi-modality and complex reward landscape as interesting future work. (a) Hopper (b) Walker (c) Reacher (d) Swimmer (e) HalfCheetah (f) Ant (g) MountainCar (h) Pendulum Figure 1: Noisy Observations: learning curves on noisy locomotion tasks. For each task, the observation is added a Gaussian noise N(0, 0.12) component-wise. Each curve is averaged over 4 random seeds. Red is NFP and blue is unimodal Gaussian, both implemented with PPO. NFP beats Gaussian on most tasks. 6 5.2 Multi-modal policy Gaussian Bandits. Though factorized unimodal policies suf\ufb01ce for most benchmark tasks, below we motivate the importance of a \ufb02exible policy by a simple example: Gaussian bandits. Consider a two dimensional bandit A = [\u22121, 1]2. The reward of action a is \u2212aT \u03a3\u2212a for a positive de\ufb01nite matrix \u03a3. The optimal policy for maximum entropy objective is \u03c0\u2217(a) \u221d exp(\u2212 aT \u03a3\u2212a \u03b2 ), i.e. a Gaussian policy with covariance matrix \u03a3. We compare NFP with PPO with factorized Gaussian. As illustrated in Figure 2(a), NFP can approximate the optimal Gaussian policy pretty closely while the factorized Gaussian cannot capture the high correlation between the two action components. Navigating 2D Multi-goal. We motivate the strength of implicit policy to represent multi-modal policy by Multi-goal environment [13]. The agent has 2D coordinates as states S \u2282 R2 and 2D forces as actions A \u2282 R2. A ball is randomly initialized near the origin and the goal is to push the ball to reach one of the four goal positions plotted as red dots in Figure 2(b). While a unimodal policy can only deterministically commit the agent to one of the four goals, a multi-modal policy obtained by NBP can stochastically commit the agent to multiple goals. On the right of Figure 2(b) we also show sampled actions and contours of Q value functions at various states: NBP learns a very \ufb02exible policy with different number of modes in different states. Learning a Bimodal Reacher. For a more realistic example, consider learning a bimodal policy for reaching one of two targets (Figure 3(a)). The agent has the physical coordinates of the reaching arms as states S \u2282 R9 and applies torques to the joints as actions A \u2282 R2. The objective is to move the reacher head to be close to one of the targets. As illustrated by trajectories in Figure 2(c), while a unimodal Gaussian policy can only deterministically reach one target (red curves), a NFP agent can capture both modes by stochastically reaching one of the two targets (blue curves). (a) Gaussian Bandit (b) 2D Multi-goal (c) Bimodal Reacher Figure 2: (a): Illustration of Gaussian bandits. The x and y axes are actions. Green dots are actions from the optimal policy, a Gaussian distribution with covariance structure illustrated by the contours. Red dots and blue dots are actions sampled from a learned factorized Gaussian and NFP. NFP captures the covariance of the optimal policy while factorized Gaussian cannot. (b): Illustration of 2D multi-goal environment. Left: trajectories generated by trained NBP agent (solid blue curves). The x and y axes are coordinates of the agent (state). The agent is initialized randomly near the origin. The goals are red dots, and instant rewards are proportional to the agent\u2019s minimum distance to one of the four goals. Right: predicted Q value contours by the critic (light blue: low value, light green: high value and actions sampled from the policy (blue dots) at three selected states. The NFP policy has different number of modes at different states. (c): Trajectories of the reacher head by NFP (blue curves) and unimodal Gaussian policies (red curves) for the bimodal reacher. Yellow dots are locations of the two targets, and the green dot is the starting location of the reacher. Fine-tuning for downstream tasks. A recent paradigm for RL is to pre-train an agent to perform a conceptually high-level task, which may accelerate \ufb01ne-tuning the agent to perform more speci\ufb01c tasks [13]. We consider pre-training a quadrupedal robot (Figure 3(b)) to run fast, then \ufb01ne-tune the robot to run fast in a particular direction [13] as illustrated in Figure 3(c), where we set walls to limit the directions in which to run. Wide and Narrow Hallways tasks differ by the distance of the opposing walls. If an algorithm does not inject enough diversity during pre-training, it will commit the agent to prematurely run in a particular direction, which is bad for \ufb01ne-tuning. We compare the pre-training capacity of DDPG [20], SQL [13] and NBP. As shown in Figure 3(d), after pre-training, NBP agent manages to run in multiple directions, while DDPG agent runs in a single direction due to a deterministic policy (Appendix E.2). In Table 2, we compare the cumulative rewards of agents after \ufb01ne-tuning on downstream tasks with different pre-training as initializations. In both tasks, we \ufb01nd NBP to outperform DDPG, SQL and random initialization (no pre-training) by statistically signi\ufb01cant margins, 7 ",
    "Conclusion": "Conclusion We have proposed Implicit Policy, a rich class of policy that can represent complex action distributions. We have derived ef\ufb01cient algorithms to compute entropy regularized policy gradients for generic implicit policies. Importantly, we have also showed that entropy regularization maximizes a lower bound of maximum entropy objective, which implies that in practice entropy regularization + rich policy class can lead to desired properties of maximum entropy RL. We have empirically showed that implicit policy achieves competitive performance on benchmark tasks, is more robust to observational noise, and can \ufb02exibly represent multi-modal distributions. 8 ",
    "References": "References [1] Abbeel, P. and Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-\ufb01rst international conference on Machine learning, page 1. ACM. [2] Asadi, K. and Littman, M. L. (2017). An alternative softmax operator for reinforcement learning. In International Conference on Machine Learning, pages 243\u2013252. [3] Degris, T., White, M., and Sutton, R. S. (2012). Off-policy actor-critic. arXiv preprint arXiv:1205.4839. [4] Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., and Wu, Y. (2017). Openai baselines. https://github.com/openai/baselines. [5] Dinh, L., Krueger, D., and Bengio, Y. (2014). Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516. [6] Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation using real nvp. arXiv preprint arXiv:1605.08803. [7] Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u20131338. [8] Finn, C., Christiano, P., Abbeel, P., and Levine, S. (2016). A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852. [9] Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295. [10] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680. [11] Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a). Latent space policies for hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808. [12] Haarnoja, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S. (2018b). Composable deep reinforcement learning for robotic manipulation. arXiv preprint arXiv:1803.06773. [13] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165. [14] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018c). Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290. [15] Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In ICML, volume 2, pages 267\u2013274. [16] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. [17] Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. [18] Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909. [19] Li, Y. and Turner, R. E. (2017). Gradient estimators for implicit models. arXiv preprint arXiv:1705.07107. [20] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. 9 [21] Liu, Q. and Wang, D. (2016). Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pages 2378\u20132386. [22] Mania, H., Guy, A., and Recht, B. (2018). Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055. [23] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937. [24] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602. [25] Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pages 2772\u20132782. [26] O\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016). Pgq: Combining policy gradient and q-learning. arXiv preprint arXiv:1611.01626. [27] Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing \ufb02ows. arXiv preprint arXiv:1505.05770. [28] Ross, S., Gordon, G., and Bagnell, D. (2011). A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on arti\ufb01cial intelligence and statistics, pages 627\u2013635. [29] Schulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440. [30] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897. [31] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017b). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. [32] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489. [33] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic policy gradient algorithms. In ICML. [34] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from over\ufb01tting. The Journal of Machine Learning Research, 15(1):1929\u20131958. [35] Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063. [36] Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE. [37] Tran, D., Ranganath, R., and Blei, D. M. (2017). Hierarchical implicit models and likelihoodfree variational inference. arXiv preprint arXiv:1702.08896. [38] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5\u201332. Springer. [39] Ziebart, B. D. (2010). Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University. 10 A Proof of Theorems A.1 Stochastic Pathwise Gradient Theorem 3.1 (Stochastic Pathwise Gradient). Given an implicit stochastic policy at = f\u03b8(st, \u03f5), \u03f5 \u223c \u03c10(\u00b7). Let \u03c0\u03b8 be the implicitly de\ufb01ned policy. Then the pathwise policy gradient for the stochastic policy is \u2207\u03b8J(\u03c0\u03b8) = E\u03c0\u03b8 \ufffd E\u03f5\u223c\u03c10(\u00b7)[\u2207\u03b8f\u03b8(s, \u03f5)\u2207aQ\u03c0\u03b8(s, a)|a=f\u03b8(s,\u03f5)] \ufffd . (6) Proof. We follow closely the derivation of deterministic policy gradient [33]. We assume that all conditions are satis\ufb01ed to exchange expectations and gradients when necessary. Let \u03c0 = \u03c0\u03b8 denote the implicit policy at = f\u03b8(st, \u03f5), \u03f5 \u223c \u03c10(\u00b7). Let V \u03c0, Q\u03c0 be the value function and action value function under such stochastic policy. We introduce p(s \u2192 s\u2032, k, \u03c0) as the probability of transitioning from s to s\u2032 in k steps under policy \u03c0. Overloading the notation a bit, p(s \u2192 s\u2032, 1, a) is the probability of s \u2192 s\u2032 in one step by taking action a (i.e., p(s \u2192 s\u2032, 1, a) = p(s\u2032|s, a)). We have \u2207\u03b8V \u03c0(s) = \u2207\u03b8Ea\u223c\u03c0(\u00b7|s) \ufffd Q\u03c0(s, a) \ufffd = \u2207\u03b8E\u03f5\u223c\u03c10(\u00b7) \ufffd Q\u03c0(s, f\u03b8(s, \u03f5)) \ufffd = \u2207\u03b8E\u03f5\u223c\u03c10(\u00b7) \ufffd r(s, f\u03b8(s, \u03f5)) + \ufffd S \u03b3p(s\u2032|s, f\u03b8(s, \u03f5))V \u03c0(s\u2032)ds\u2032\ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8r(s, f\u03b8(s, \u03f5)) + \u2207\u03b8 \ufffd S \u03b3p(s\u2032|s, f\u03b8(s, \u03f5))V \u03c0(s\u2032)ds\u2032\ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8r(s, f\u03b8(s, \u03f5)) + \ufffd S \u03b3V \u03c0(s\u2032)\u2207\u03b8p(s\u2032|s, f\u03b8(s, \u03f5))ds\u2032 + \ufffd S \u03b3p(s\u2032|s, f\u03b8(s, \u03f5))\u2207\u03b8V \u03c0(s\u2032)ds\u2032\ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s, \u03f5)\u2207a[r(s, a) + \u03b3 \ufffd S \u03b3p(s\u2032|s, a)V \u03c0(s\u2032)ds\u2032)]|a=f\u03b8(s,\u03f5) + \ufffd S \u03b3p(s\u2032|s, f\u03b8(s, \u03f5))\u2207\u03b8V \u03c0(s\u2032)ds\u2032\ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s, \u03f5)\u2207aQ\u03c0(s, a)|a=f\u03b8(s,\u03f5) \ufffd + E\u03f5\u223c\u03c10(\u00b7) \ufffd \ufffd S \u03b3p(s \u2192 s\u2032, 1, f\u03b8(s, \u03f5))\u2207\u03b8V \u03c0(s\u2032)ds\u2032\ufffd . In the above derivation, we have used the Fubini theorem to interchange integral (expectation) and gradients. We can iterate the above derivation and have the following \u2207\u03b8V \u03c0(s) = \u2207\u03b8Ea\u223c\u03c0(\u00b7|s) \ufffd Q\u03c0(s, a) \ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s, \u03f5)\u2207aQ\u03c0(s, a)|a=f\u03b8(s,\u03f5) \ufffd + E\u03f5\u223c\u03c10(\u00b7) \ufffd \u03b3p(s \u2192 s\u2032, 1, f\u03b8(s, \u03f5))\u2207\u03b8V \u03c0(s\u2032)ds\u2032\ufffd = E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s, \u03f5)\u2207aQ\u03c0(s, a)|a=f\u03b8(s,\u03f5) \ufffd + E\u03f5\u223c\u03c10(\u00b7) \ufffd \ufffd S \u03b3p(s \u2192 s\u2032, 1, f\u03b8(s, \u03f5))E\u03f5\u2032\u223c\u03c10(\u00b7)[\u2207\u03b8f\u03b8(s\u2032, \u03f5\u2032)\u2207aQ\u03c0(s\u2032, a\u2032)|a\u2032=f\u03b8(s\u2032,\u03f5\u2032)]ds\u2032\ufffd + E\u03f5\u223c\u03c10(\u00b7) \ufffd \ufffd S \u03b3p(s \u2192 s\u2032, 1, f\u03b8(s, \u03f5\u2032))E\u03f5\u2032\u223c\u03c10(\u00b7) \ufffd \ufffd S \u03b3p(s\u2032 \u2192 s\u2032\u2032, 1, f\u03b8(s\u2032, \u03f5\u2032))\u2207\u03b8V \u03c0(s\u2032\u2032)ds\u2032\u2032\ufffd ds\u2032\ufffd = ... = \ufffd S \u221e \ufffd t=0 \u03b3tp(s \u2192 s\u2032, t, \u03c0)E\u03f5\u2032\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s\u2032, \u03f5\u2032)\u2207aQ\u03c0(s\u2032, a\u2032)|a\u2032=f\u03b8(s,\u03f5\u2032) \ufffd ds\u2032. 11 With the above, we derive the pathwise policy gradient as follows \u2207\u03b8J(\u03c0\u03b8) = \u2207\u03b8 \ufffd S p1(s)V \u03c0(s)ds = \ufffd S p1(s)\u2207\u03b8V \u03c0(s)ds = \ufffd S \ufffd S \u221e \ufffd t=0 \u03b3tp1(s)p(s \u2192 s\u2032, t, \u03c0)dsE\u03f5\u2032\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s\u2032, \u03f5\u2032)\u2207aQ\u03c0(s\u2032, a\u2032)|a\u2032=f\u03b8(s,\u03f5\u2032) \ufffd ds\u2032 = \ufffd S \u03c1\u03c0(s\u2032)E\u03f5\u2032\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8f\u03b8(s\u2032, \u03f5\u2032)\u2207aQ\u03c0(s\u2032, a\u2032)|a\u2032=f\u03b8(s,\u03f5\u2032) \ufffd ds\u2032, where \u03c1\u03c0(s\u2032) = \ufffd S \ufffd\u221e t=0 \u03b3tp1(s)p(s \u2192 s\u2032, t, \u03c0)ds is the discounted state visitation probability under policy \u03c0. Writing the whole integral as an expectation over states, the policy gradient is \u2207\u03b8J(\u03c0\u03b8) = Es\u223c\u03c1\u03c0(s) \ufffd E\u03f5\u223c\u03c10(\u00b7)[\u2207\u03b8f\u03b8(s, \u03f5)\u2207aQ\u03c0(s, a)|a=f\u03b8(s,\u03f5)] \ufffd . which is equivalent to in (6) in theorem 3.1. We can recover the result for deterministic policy gradient by using a degenerate functional form f\u03b8(s, \u03f5) = f\u03b8(s), i.e. with a deterministic function to compute actions. A.2 Unbiased Entropy Gradient Lemma A.1 (Optimal Classi\ufb01er as Density Estimator). Assume c\u03c8 is expressive enough to represent any classi\ufb01er (for example c\u03c8 is a deep neural net). Assume A to be bounded and let U(A) be uniform distribution over A. Let \u03c8\u2217 be the optimizer to the optimization problem in (7). Then c\u03c8\u2217(s, a) = log \u03c0\u03b8(a|s) |A|\u22121 and |A| is the volume of A. Proof. Observe that (7) is a binary classi\ufb01cation problem with data from a \u223c \u03c0\u03b8(\u00b7|s) against a \u223c U(A). The optimal classi\ufb01er of the problem produces the density ratio of these two distributions. See for example [10] for a detailed proof. Theorem 3.2 (Unbiased Entropy Gradient). Let \u03c8\u2217 be the optimal solution from (7), where the policy \u03c0\u03b8(\u00b7|s) is given by implicit policy a = f\u03b8(s, \u03f5), \u03f5 \u223c \u03c10(\u00b7). The gradient of entropy \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd can be computed as \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = \u2212E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207\u03b8c\u03c8\u2217(f(\u03b8, \u03f5), s) \ufffd (8) Proof. Let \u03c0\u03b8(\u00b7|s) be the density of implicit policy a = f\u03b8(\u03f5, s), \u03f5 \u223c \u03c10(\u00b7). The entropy is computed as follows H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = \u2212Ea\u223c\u03c0\u03b8(\u00b7|s) \ufffd log \u03c0\u03b8(a|s) \ufffd . Computing its gradient \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd = \u2212\u2207\u03b8E\u03f5\u223c\u03c10(\u00b7) \ufffd log \u03c0\u03b8(f\u03b8(s, \u03f5)|s) \ufffd = \u2212Ea\u223c\u03c0\u03b8(\u00b7|s) \ufffd \u2207\u03b8 log \u03c0\u03b8(a|s) \ufffd \u2212 E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207a log \u03c0\u03b8(a|s)|a=f\u03b8(s,\u03f5)\u2207\u03b8f\u03b8(s, \u03f5) \ufffd = \u2212E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207a log \u03c0\u03b8(a|s)|a=f\u03b8(s,\u03f5)\u2207\u03b8f\u03b8(s, \u03f5) \ufffd = \u2212E\u03f5\u223c\u03c10(\u00b7) \ufffd \u2207ac\u03c8\u2217(f(\u03b8, \u03f5), s)\u2207\u03b8f\u03b8(s, \u03f5) \ufffd (11) In the second line we highlight the fact that the expectation depends on parameter \u03b8 both implicitly through the density \u03c0\u03b8 and through the sample f\u03b8(s, \u03f5). After decomposing the gradient using chain rule, we \ufb01nd that the \ufb01rst term vanishes, leaving the result shown in the theorem. 12 A.3 Lower Bound We recall that given a policy \u03c0, the standard RL objective is J(\u03c0) = E\u03c0 \ufffd \ufffd\u221e t=0 \u03b3trt \ufffd . In maximum entropy formulation, the maximum entropy objective is JMaxEnt(\u03c0) = E\u03c0 \ufffd \u221e \ufffd t=0 \u03b3t(rt + \u03b2H[\u03c0(\u00b7|st)]) \ufffd , (12) where \u03b2 > 0 is a regularization constant and H(\u03c0(\u00b7|st)) is the entropy of policy \u03c0 at st. We construct a surrogate objective based on another policy \u02dc\u03c0 as follows Jsurr(\u03c0, \u02dc\u03c0) = E\u03c0 \ufffd \u221e \ufffd t=0 \u03b3trt \ufffd + \u03b2E\u02dc\u03c0 \ufffd \u221e \ufffd t=0 \u03b3tH[\u03c0(\u00b7|st)] \ufffd . (13) The following proof highly mimics the proof in [30]. We have the following de\ufb01nition for coupling two policies De\ufb01nition A.1 (\u03b1\u2212coupled). Two policies \u03c0, \u02dc\u03c0 are \u03b1\u2212coupled if P(\u03c0(\u00b7|s) \u0338= \u02dc\u03c0(\u00b7|s)) \u2264 \u03b1 for any s \u2208 S. Lemma A.2. Given \u03c0, \u02dc\u03c0 are \u03b1\u2212coupled, then |Est\u223c\u03c0 \ufffd H[\u03c0(\u00b7|st)] \ufffd \u2212 Est\u223c\u02dc\u03c0 \ufffd H[\u02dc\u03c0(\u00b7|st)] \ufffd | \u2264 2(1 \u2212 (1 \u2212 \u03b1)t) max s |H[\u03c0(\u00b7|s)]|. Proof. Let nt denote the number of times that ai \u0338= \u02dcai for i < t, i.e. the number of times that \u03c0, \u02dc\u03c0 disagree before time t. We can decompose the expectations as follows Est\u223c\u03c0 \ufffd H[\u03c0(\u00b7|st)] \ufffd = Est\u223c\u03c0|nt=0 \ufffd H[\u03c0(\u00b7|st)] \ufffd P(nt = 0) + Est\u223c\u03c0|nt>0 \ufffd H[\u03c0(\u00b7|st)] \ufffd P(nt > 0), Est\u223c\u02dc\u03c0 \ufffd H[\u03c0(\u00b7|st)] \ufffd = Est\u223c\u02dc\u03c0|nt=0 \ufffd H[\u03c0(\u00b7|st)] \ufffd P(nt = 0) + Est\u223c\u02dc\u03c0|nt>0 \ufffd H[\u03c0(\u00b7|st)] \ufffd P(nt > 0). Note that nt = 0 implies ai = \u02dcai for all i < t hence Est\u223c\u02dc\u03c0|nt=0 \ufffd H[\u03c0(\u00b7|st)] \ufffd = Est\u223c\u03c0|nt=0 \ufffd H[\u03c0(\u00b7|st)] \ufffd . The de\ufb01nition of \u03b1\u2212coupling implies P(nt = 0) \u2265 (1 \u2212 \u03b1)t, and so P(nt > 0) \u2264 1 \u2212 (1 \u2212 \u03b1)t. Now we note that |Est\u223c\u03c0|nt>0 \ufffd H[\u03c0(\u00b7|st)] \ufffd P(nt > 0) \u2212 Est\u223c\u02dc\u03c0|nt>0 \ufffd H[\u03c0(\u00b7|st)] \ufffd P(nt > 0)| \u2264 2(1 \u2212 (1 \u2212 \u03b1)t) max s |H[\u03c0(\u00b7|s)]|. Combining previous observations, we have proved the lemma. Note that if we take \u03c0 = \u03c0\u03b8, \u02dc\u03c0 = \u03c0\u03b8old, then the surrogate objective Jsurr(\u03c0\u03b8) in (9) is equivalent to Jsurr(\u03c0, \u02dc\u03c0) de\ufb01ned in (13). With lemma A.2, we prove the following theorem. Theorem 4.1 (Lower Bound). If KL[\u03c0\u03b8||\u03c0\u03b8old] \u2264 \u03b1, then JMaxEnt(\u03c0) \u2265 Jsurr(\u03c0) \u2212 \u03b2\u03b3\u221a\u03b1\u03f5 (1 \u2212 \u03b3)2 , where \u03f5 = max s |H[\u03c0(\u00b7|s)]|. (10) Proof. We \ufb01rst show the result for general policies \u03c0 and \u02dc\u03c0 with KL \ufffd \u03c0||\u02dc\u03c0 \ufffd \u2264 \u03b1. As a result, [30] shows that \u03c0, \u02dc\u03c0 are \u221a\u03b1\u2212coupled. Recall the maximum entropy objective JMaxEnt(\u03c0) de\ufb01ned in (12) and surrogate objective in (13), take the difference of two objectives |JMaxEnt(\u03c0) \u2212 Jsurr(\u03c0, \u02dc\u03c0)| = \u03b2| \u221e \ufffd t=0 \u03b3tEst\u223c\u03c0 \ufffd H[\u03c0(\u00b7|st)] \ufffd \u2212 Est\u223c\u02dc\u03c0 \ufffd H[\u02dc\u03c0(\u00b7|st)] \ufffd | \u2264 \u03b2 \u221e \ufffd t=0 \u03b3t|Est\u223c\u03c0 \ufffd H[\u03c0(\u00b7|st)] \ufffd \u2212 Est\u223c\u02dc\u03c0 \ufffd H[\u02dc\u03c0(\u00b7|st)] \ufffd | \u2264 \u03b2 \u221e \ufffd t=0 \u03b3t(1 \u2212 (1 \u2212 \u221a\u03b1)t) max s |H[\u03c0(\u00b7|s)]| = ( 1 1 \u2212 \u03b3 \u2212 1 1 \u2212 \u03b3(1 \u2212 \u221a\u03b1))\u03b2\u221a\u03b1 max s |H[\u03c0(\u00b7|s)]| = \u03b2\u03b3\u221a\u03b1 (1 \u2212 \u03b3)2 max s |H[\u03c0(\u00b7|s)]| 13 Now observe that by taking \u03c0 = \u03c0\u03b8, \u02dc\u03c0 = \u03c0\u03b8old, the above inequality implies the theorem. In practice, \u03b1\u2212coupling enforced by KL divergence is often relaxed [30, 31]. The theorem implies that, by constraining the KL divergence between consecutive policy iterates, the surrogate objective of entropy regularization maximizes a lower bound of maximum entropy objective. B Operator view of Entropy Regularization and Maximum Entropy RL Recall in standard RL formulation, the agent is in state s, takes action a, receives reward r and transitions to s\u2032. Let the discount factor \u03b3 < 1. Assume that the reward r is deterministic and the transitions s\u2032 \u223c p(\u00b7|s, a) are deterministic, i.e. s\u2032 = f(s, a), it is straightforward to extend the following to general stochastic transitions. For a given policy \u03c0, de\ufb01ne linear Bellman operator as T \u03c0Q(s, a) = r + \u03b3Ea\u2032\u223c\u03c0(\u00b7|s\u2032) \ufffd Q(s\u2032, a\u2032) \ufffd . Any policy \u03c0 satis\ufb01es the linear Bellman equation T \u03c0Q\u03c0 = Q\u03c0. De\ufb01ne Bellman optimality operator (we will call it Bellman operator) as T \u2217Q(s, a) = r + \u03b3 max a\u2032 Q(s\u2032, a\u2032). Now we de\ufb01ne Mellowmax operator [2, 13] with parameter \u03b2 > 0 as follows, TsQ(s, a) = r + \u03b3\u03b2 log \ufffd a\u2032\u2208A exp(Q(s\u2032, a\u2032) \u03b2 )da\u2032. It can be shown that both T \u2217 and T are contractive operator when \u03b3 < 1. Let Q\u2217 be the unique \ufb01xed point of T \u2217Q = Q, then Q\u2217 is the action value function of the optimal policy \u03c0\u2217 = arg max\u03c0 J(\u03c0). Let Q\u2217 s be the unique \ufb01xed point of TsQ = Q, then Q\u2217 s is the soft action value function of \u03c0\u2217 s = arg max\u03c0 JMaxEnt(\u03c0). In addition, the optimal policy \u03c0\u2217(\u00b7|s) = arg maxa Q\u2217(s, a) and \u03c0\u2217 s(a|s) \u221d exp( Q\u2217 s(s,a) \u03b2 ). De\ufb01ne Boltzmann operator with parameter \u03b2 as follows TBQ(s, a) = r + \u03b3Ea\u2032\u223cpB(\u00b7|s\u2032) \ufffd Q(s\u2032, a\u2032) \ufffd , where pB(a\u2032|s\u2032) \u221d exp( Q(s\u2032,a\u2032) \u03b2 ) is the Boltzmann distribution de\ufb01ned by Q(s\u2032, a\u2032). [26] shows that the stationary points of entropy regularization procedure are policies of the form \u03c0(a|s) \u221d exp( Q\u03c0(s,a) \u03b2 ). We illustrate the connection between such stationary points and \ufb01xed points of Boltzmann operator as follows. Theorem B.1 (Fixed points of Boltzmann Operators). Any \ufb01xed point Q(s, a) of Boltzmann operator TBQ = Q, de\ufb01nes a stationary point for entropy regularized policy gradient algorithm by \u03c0(a|s) \u221d exp( Q(s,a) \u03b2 ); reversely, any stationary point of entropy regularized policy gradient algorithm \u03c0, has its action value function Q\u03c0(s, a) as a \ufb01xed point to Boltzmann operator. Proof. Take any \ufb01xed point Q of Boltzmann operator, TBQ = Q, de\ufb01ne a policy \u03c0(a|s) \u221d exp( Q(s,a) \u03b2 ). From the de\ufb01nition of Boltzmann operator, we can easily check that \u03c0\u2019s entropy regularized policy gradient is exactly zero, hence it is a stationary point for entropy regularized gradient algorithm. Take any policy \u03c0 such that its entropy regularized gradient is zero, from [26] we know for such policy \u03c0(a|s) \u221d exp(Q\u03c0(s, a)). The linear Bellman equation for such a policy T \u03c0Q\u03c0 = Q\u03c0 translates directly into the Boltzmann equation TBQ\u03c0 = Q\u03c0. Hence Q\u03c0 is indeed a \ufb01xed point of Boltzmann operator. The above theorem allows us to associate the policies trained by entropy regularized policy gradient with the Boltzmann operator. Unfortunately, [2] shows that unlike MellowMax operator, Boltzmann operator does not have unique \ufb01xed point and is not a contractive operator in general, though this does not necessarily prevent policy gradient algorithms from converging. We make a \ufb01nal observation that 14 shows that Boltzmann operator TB interpolates Bellman operator T \u2217 and MellowMax operator Ts: for any Q and \ufb01xed \u03b2 > 0 (see theorem B.2), T \u2217Q \u2265 TBQ \u2265 TsQ. (14) If we view all operators as picking out the largest value among Q(s\u2032, a\u2032), a\u2032 \u2208 A in next state s\u2032, then T \u2217 is the greediest and Ts is the most conservative, as it incorporates trajectory entropy as part of the objective. TB is between these two operators, since it looks ahead for only one step. The \ufb01rst inequality in (14) is trivial, now we show the second inequality. Theorem B.2 (Boltzmann Operator is greedier than Mellowmax Operator). For any Q \u2208 Rn, we have TBQ \u2265 TsQ, and the equality is tight if and only if Qi = Qj for \u2200i, j. Proof. Recall the de\ufb01nition of both operators, we essentially need to show the following inequality Ea\u2032\u223cpB(\u00b7|s\u2032)[Q(s\u2032, a\u2032)] \u2265 \u03b2 log \ufffd a\u2032\u2208A exp(Q(s\u2032, a\u2032) \u03b2 ). Without loss of generality, assume there are n actions in total and let xi = exp( Q(s\u2032,ai) \u03b2 ) for the ith action. The above inequality reduces to \ufffdn i=1 xi log xi \ufffdn i=1 xi \u2265 log 1 n n \ufffd i=1 xi. We did not \ufb01nd any reference for the above inequality so we provide a proof below. Notice that xi > 0, \u2200i. Introduce the objective J(x) = \ufffdn i=1 xi log xi \ufffdn i=1 xi \u2212 log 1 n \ufffdn i=1 xi. Compute the gradient of J(x), \u2202J(x) \u2202xj = \ufffdn i=1(log xj \u2212 log xi)xi (\ufffdn i=1 xi)2 , \u2200j. The stationary point at which \u2202J(x) \u2202x = 0 is of the form xi = xj, \u2200i, j. At such point, let xi = x0, \u2200i for some generic x0. Then we compute the Hessian of J(x) at such stationary point \u22022J(x) \u2202x2 j |xi=x0,\u2200i = n \u2212 1 n2x2 0 , \u2200j. \u22022J(x) \u2202xk\u2202xj |xi=x0,\u2200i = 1 n2x2 0 , \u2200j \u0338= k. Let H(x0) be the Hessian at this stationary point. Let t \u2208 Rn be any vector and we can show tT H(x0)t = \ufffd i\u0338=j 1 n2x2 0 (ti \u2212 tj)2 \u2265 0, which implies that H(x0) is positive semi-de\ufb01nite. It is then implied that at such x0 we will achieve local minimum. Let xi = x0, \u2200i we \ufb01nd J(x) = 0, which implies that J(x) \u2265 0, \u2200x. Hence the proof is concluded. C Implicit Policy Architecture C.1 Normalizing Flows Policy Architecture We design the neural network architectures following the idea of [5, 6]. Recall that Normalizing Flows [27] consist of layers of transformations as follows , x = g\u03b8K \u25e6 g\u03b8K\u22121 \u25e6 ... \u25e6 g\u03b82 \u25e6 g\u03b81(\u03f5), where each g\u03b8i(\u00b7) is an invertible transformation. We focus on how to design each atomic transformation g\u03b8i(\u00b7). We overload the notations and let x, y be the input/output of a generic layer g\u03b8(\u00b7), y = g\u03b8(x). 15 We design a generic transformation g\u03b8(\u00b7) as follows. Let xI be the components of x corresponding to subset indices I \u2282 {1, 2...m}. Then we propose as in [6], y1:d = x1:d yd+1:m = xd+1:m \u2299 exp(s(x1:d)) + t(x1:d), (15) where t(\u00b7), s(\u00b7) are two arbitrary functions t, s : Rd \ufffd\u2192 Rm\u2212d. It can be shown that such transformation entails a simple Jacobian matrix | \u2202y \u2202xT | = exp(\ufffdm\u2212d j=1 [s(x1:d)]j) where [s(x1:d)]j refers to the jth component of s(x1:d) for 1 \u2264 j \u2264 m \u2212 d. For each layer, we can permute the input x before apply the simple transformation (15) so as to couple different components across layers. Such coupling entails a complex transformation when we stack multiple layers of (15). To de\ufb01ne a policy, we need to incorporate state information. We propose to preprocess the state s \u2208 Rn by a neural network L\u03b8s(\u00b7) with parameter \u03b8s, to get a state vector L\u03b8s(s) \u2208 Rm. Then combine the state vector into (15) as follows, z1:d = x1:d zd+1:m = xd+1:m \u2299 exp(s(x1:d)) + t(x1:d) y = z + L\u03b8s(s). (16) It is obvious that x \u2194 y is still bijective regardless of the form of L\u03b8s(\u00b7) and the Jacobian matrix is easy to compute accordingly. In our experiments, we implement s, t both as 4-layers neural networks with k = 3 or k = 6 units per hidden layer. We stack K = 4 transformations: we implement (16) to inject state information only after the \ufb01rst transformation, and the rest is conventional coupling as in (15). L\u03b8s(s) is implemented as a feedforward neural network with 2 hidden layers each with 64 hidden units. Value function critic is implemented as a feedforward neural network with 2 hidden layers each with 64 hidden units with recti\ufb01ed-linear between hidden layers. C.2 Non-Invertible Blackbox Policy Architecture Any implicit model architecture as in [10, 37] can represent a Non-invertible Blackbox Policy (NBP). On MuJoCo control tasks, consider a task with state space S \u2282 Rn and action space A \u2282 Rm. Consider a feedforward neural network with n input units and m output units. The intermediate layers have parameters \u03b8 and the output is a deterministic mapping from the input a = f\u03b8(s). We choose an architecture similar to NoisyNet [9]: introduce a distribution over \u03b8. In our case, we choose factorized Gaussian \u03b8 = \u00b5\u03b8 + \u03c3\u03b8 \u00b7 \u03f5. The implicit policy is generated as a = f\u03b8(s), \u03b8 = \u00b5\u03b8 + \u03c3\u03b8 \u00b7 \u03f5, \u03f5 \u223c N(0, 1), which induces an implicit distribution over output a. In practice, we \ufb01nd randomizing parameters \u03b8 to generate implicit policy works well and is easy to implement, we leave other approaches for future research. In all experiments, we implement the network f\u03b8(\u00b7) as a feedforward neural network with 2 hidden layers each with 64 hidden units. Between layers we use recti\ufb01ed-linear for non-linear activation, layer normalization to standardize inputs, and dropout before the last output. Both value function critic and classi\ufb01er critic are implemented as feedforward neural networks with 2 hidden layers each with 64 hidden units with recti\ufb01ed-linear between hidden layers. Note that \u00b5\u03b8, \u03c3\u03b8 are the actual parameters of the model: we initialize \u00b5\u03b8 using standard initialization method and initialize \u03c3\u03b8 = log(exp(\u03c1\u03b8) + 1) with \u03c1\u03b8 \u2208 [\u22129.0, \u22121.0]. For simplicity, we set all \u03c1\u03b8 to be the same and let \u03c1 = \u03c1\u03b8. We show below that dropout is an ef\ufb01cient technique to represent multi-modal policy. Dropout for multi-modal distributions. Dropout [34] is an ef\ufb01cient technique to regularize neural networks in supervised learning. However, in reinforcement learning where over\ufb01tting is not a big issue, the application of dropout seems limited. Under the framework of implicit policy, we want to highlight that dropout serves as a natural method to parameterize multi-modal distributions. Consider a feed-forward neural network with output y \u2208 Rm. Assume that the last layer is a fully-connected network with h inputs. Let x \u2208 Rn be an input to the original neural network and \u03c6(x) \u2208 Rh be the input to the last layer (we get \u03c6(x) by computing forward pass of x through the network until the last layer), where \u03c6(x) can be interpreted as a representation learned by previous layers. 16 Let W \u2208 Rm\u00d7h, b \u2208 Rm be the weight matrix and bias vector of the last layer, then the output is computed as (we ignore the non-linear activation at the output) yi = h \ufffd j=1 \u03c6j(x)Wij + bi, \u2200 1 \u2264 i \u2264 m. (17) If dropout is applied to the last layer, let z be the Bernoulli mask i.e. zi \u223c Bernoulli(p), 1 \u2264 i \u2264 h where p is the probability for dropping an input to the layer. Then yi = h \ufffd j=1 (\u03c6j(x) \u00b7 zj)Wij + bi, \u2200 1 \u2264 i \u2264 m (18) Given an i, if each \u03c6j(x)Wij has a different value, their stochastic sum \ufffdh j=1 \u03c6j(x) \u00b7 zj)Wij in (18) can take up to about 2h values. Despite some redundancy in these 2h values, in general yi in (18) has a multi-modal distribution supported on multiple values. We have hence moved from a unimodal distribution (17) to a multi-modal distribution (18) by adding a simple dropout. D Algorithm Pseudocode Below we present the pseudocode for an off-policy algorithm to train NBP. On the other hand, for NFP we can apply any on-policy optimization algorithms [31, 30] and we omit the pseudocode here. Algorithm 1 Non-invertible Blackbox Policy (NBF) Off-policy update 1: INPUT: target parameter update period \u03c4; learning rate \u03b1\u03b8, \u03b1\u03c6, \u03b1\u03c8; entropy regularization constant \u03b2. 2: INITIALIZE: parameters \u03b8, \u03c6, \u03c8 and target network parameters \u03b8\u2212, \u03c6\u2212; replay buffer B \u2190 {}; step counter counter \u2190 0. 3: for e = 1, 2, 3...E do 4: while episode not terminated do 5: // Control 6: counter \u2190 counter + 1. 7: In state s, sample noise \u03f5 \u223c \u03c10(\u00b7), compute action a = f\u03b8(s, \u03f5), transition to s\u2032 and receive instant reward r. 8: Save experience tuple {s, a, r, s\u2032} to buffer B. 9: Sample N tuples D = {sj, aj, rj, s\u2032 j} from B. 10: // Update Critic 11: Compute TD error as in [24, 32] as follows, where a\u2032 j = f\u03b8\u2212(s\u2032 j, \u03f5j), \u03f5j \u223c \u03c10(\u00b7). J\u03c6 = 1 N N \ufffd j=1 (Q\u03c6(sj, aj) \u2212 rj \u2212 \u03b3Q\u03c6\u2212(s\u2032 j, a\u2032 j))2. 12: Update \u03c6 \u2190 \u03c6 \u2212 \u03b1\u03c6\u2207\u03c6J\u03c6. 13: // Update classi\ufb01er 14: Sample N actions uniformly from action space a(u) j \u223c U(A). Compute classi\ufb01cation objective C\u03c8 (7) using data {sj, aj}N j=1 against {sj, a(u) j }N j=1. 15: Update classi\ufb01er \u03c8 \u2190 \u03c8 \u2212 \u03b1\u03c8\u2207\u03c8C\u03c8. 16: // Update policy with entropy regularization. 17: Compute pathwise gradient \u2207\u03b8J(\u03c0\u03b8) (6) with Q\u03c0(s, a) replaced by critic Q\u03c6(s, a) and states replaced by sampled states sj. 18: Compute entropy gradient \u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd using (8) on sampled data. 19: Update \u03b8 \u2190 \u03b8 + \u03b1\u03b8(\u2207\u03b8J(\u03c0\u03b8) + \u03b2\u2207\u03b8H \ufffd \u03c0\u03b8(\u00b7|s) \ufffd ) 20: if counter mod \u03c4 = 0 then 21: Update target parameter \u03c6\u2212 \u2190 \u03c6, \u03b8\u2212 \u2190 \u03b8. 22: end if 23: end while 24: end for 17 ",
    "Results": "Results E.1 Locomotion tasks As has been shown in previous works [31], PPO is almost the most competitive on-policy optimization baseline on locomotion control tasks. We provide a table of comparison among on-policy baselines below. On each task we train for a speci\ufb01ed number of time steps and report the average results over 5 random seeds. Though NFP remains a competitive algorithm, PPO with unimodal Gaussian generally achieves better performance. Tasks Timesteps PPO A2C CEM TRPO NFP Hopper 106 \u2248 2300 \u2248 900 \u2248 500 \u2248 2000 \u2248 1880 HalfCheetah 106 \u2248 1900 \u2248 1000 \u2248 500 \u2248 0 \u2248 2200 Walker2d 106 \u2248 3500 \u2248 900 \u2248 800 \u2248 1000 \u2248 1980 InvertedDoublePendulum 106 \u2248 8000 \u2248 6500 \u2248 0 \u2248 0 \u2248 8000 Table 1: A comparison of NFP with (on-policy) baseline algorithms on MuJoCo benchmark tasks. For each task, we show the average rewards achieved after training the agent for a \ufb01xed number of time steps. The results for NFP are averaged over 5 random seeds. The results for A2C, CEM [7] and TRPO are approximated based on the \ufb01gures in [31], PPO is from OpenAI baseline implementation [4]. We highlight the top two algorithms for each task in bold font. PPO, A2C and TRPO all use unimodal Gaussians. PPO is the most competitive. E.2 Multi-modal policy: Fine-tuning for downstream tasks In Figure 4, we compare trajectories generated by agents pre-trained by DDPG and NBP on the running task. Since DDPG uses a deterministic policy, starting from a \ufb01xed position, the agent can only run in a single direction. On the other hand, NBP agent manages to run in multiple directions. This comparison partially illustrates that a NBP agent can learn the concept of general running, instead of specialized running \u2013 running in a particular direction. (a) Trajectories by DDPG agent (b) Trajectories by NBP agent Figure 4: (a)(b): Trajectories generated by DDPG pre-trained agents and NBP pre-trained agent on Ant-Running task under different random seeds. Starting from the initial position, DDPG agent can only produce a single deterministic trajectory due to the deterministic policy; NBP agent produces trajectories that are more diverse, illustrating that NBP agent learns to run in multiple directions. E.3 Multi-modal policy: Combining multiple modes by Imitation Learning. Didactic Example. We motivate combining multiple modes of imitation learning with a simple example: imitating an expert with two modes of behavior. Consider a simple MDP on an axis with state space S = [\u221210, 10], action space A = [\u22121, 1]. The agent chooses which direction to move and transitions according to the equation st+1 = st + at. We design an expert that commits itself randomly to one of the two endpoints of the state space s = \u221210 or s = 10 by a bimodal stochastic policy. We generate 10000 trajectories from the expert and use them as training data for direct behavior cloning. We train a NBP agent using GAN training [10]: given the expert trajectories, train a NBP as a generator that produces similar trajectories and train a separate classi\ufb01er to distinguish true expert/generated trajectories. Unlike maximum likelihood, GAN training tends to capture modes of the expert trajectories. If we train a unimodal Gaussian policy using GAN training, the agent may commit to a single mode; below we show that trained NBP policy captures both modes. 18 (a) Actions by Expert (b) Actions by NBP agent (c) Trajectories by Expert (d) Trajectories by NBP agent Figure 5: Imitating a bimodal expert: (a)(b) compare the actions produced by the expert and the trained NBP agent at different states s. The expert policy has a bimodal policy across different states and becomes increasingly unimodal when s \u2248 \u00b110; the trained policy captures such bimodal behavior. (c)(d) compare the trajectories of expert/trained agent. The vertical axis indicates the states s and horizontal axis is the time steps in an episode, each trajectory is terminated at s = \u00b110. Trajectories of both expert and trained policy are very similar. Stochastic Swimmer. The goal is to train a Swimmer robot that moves either forward or backward. It is not easy to specify a reward function that directly translates into such bimodal behavior and it is not easy to train a bimodal agent under such complex dynamics even if the reward is available. Instead, we train two Swimmers using RL objectives corresponding to two deterministic modes: swimming forward and swimming backward. Combining the trajectories generated by these two modes provides a policy that stochastically commits to either swimming forward or backward. We train a NFP agent (with maximum likelihood behavior cloning [8]) and NBP agent (with GAN training [10]) to imitate expert policy. The trajectories generated by trained policies show that the trained policies have fused these two modes of movement. F Hyper-parameters and Ablation Study Hyper-parameters. Refer to Appendix C for a detailed description of architectures of NFP and NBP and hyper-parameters used in the experiments. For NFP, critical hyper-parameters are entropy coef\ufb01cient \u03b2, number of transformation layers K and number of hidden units per layer k for transformation function s, t. For NBP, critical hyper-parameters are entropy coef\ufb01cient \u03b2 and the initialized variance parameter for factorized Gaussian \u03c1. In all conventional locomotion tasks, we set \u03b2 = 0.0; for multi-modal policy tasks, we set \u03b2 \u2208 {0.1, 0.01, 0.001}. We use Adam [16] for optimization with learning rate \u03b1 \u2208 {3 \u00b7 10\u22125, 3 \u00b7 10\u22124}. Ablation Study. For NBP, the default baseline is \u03b2 = 0.0, \u03c1 = \u22124.0. We \ufb01x other hyper-parameters and change only one set of hyper-parameter to observe its effect on the performance. Intuitively, large \u03c1 encourages and widespread distribution over parameter \u03b8 and consequently and a more uniform initial distribution over actions. From Figure 7 we see that the performance is not monotonic in \u03c1, \u03b2. We \ufb01nd the model is relatively sensitive to hyper-parameter \u03c1 and a general good choice is \u03c1 \u2208 {\u22124.0, \u22125.0, \u22126.0}. For NFP, the default baseline is \u03b2 = 0.0, K = 4, k = 3. We \ufb01x other hyper-parameters and change only one set of hyper-parameter to observe its effect on the performance. In general, we \ufb01nd the model\u2019s performance is fairly robust to hyper-parameters (see Figure 8): large K, k will increase the complexity of the policy but does not necessarily bene\ufb01t performance on benchmark tasks; strictly positive entropy coef\ufb01cient \u03b2 > 0.0 does not make much difference on benchmark tasks, though for learning multi-modal policies, adding positive entropy regularization is more likely to lead to multi-modality. 19 (a) Swimmer (b) Trajectories by Expert (c) Trajectories by NBP agent (d) Trajectories by NFP agent Figure 6: Combining multiple modes by Imitation Learning: stochastic Swimmer. (a) Illustration of Swimmer; (b) Expert trajectories produced by two Swimmers moving in two opposite directions (forward and backward). Vertical axis is the x coordinate of the Swimmer, horizontal axis is the time steps; (c) Trajectories produced by NBP agent trained using GAN under different seeds; (d) Trajectories produced by NFP agent trained using maximum likelihood under different seeds. Implicit policy agents have incorporated two modes of behavior into a single policy, yet a unimodal Gaussian policy can at most commit to one mode. 20 (a) Reacher: entropy (b) Reacher: initial Gaussian variance parameter (c) HalfCheetah: entropy (d) HalfCheetah: initial Gaussisn variance parameter Figure 7: Ablation study: NBP (a) Hopper: entropy (b) Hopper: # of layers (c) Hopper: # of units (d) Reacher: entropy (e) Reacher: # of layers (f) Reacher: # of units (g) HalfCheetah: entropy (h) HalfCheetah: # of layers (i) HalfCheetah: # of units Figure 8: Ablation study: NFP 21 ",
    "title": "Implicit Policy for Reinforcement Learning",
    "paper_info": "Implicit Policy for Reinforcement Learning\nYunhao Tang\nColumbia University\nyt2541@columbia.edu\nShipra Agrawal \u2217\nColumbia University\nsa3305@columbia.edu\nAbstract\nWe introduce Implicit Policy, a general class of expressive policies that can \ufb02exibly\nrepresent complex action distributions in reinforcement learning, with ef\ufb01cient\nalgorithms to compute entropy regularized policy gradients. We empirically show\nthat, despite its simplicity in implementation, entropy regularization combined\nwith a rich policy class can attain desirable properties displayed under maximum\nentropy reinforcement learning framework, such as robustness and multi-modality.\n1\nIntroduction\nReinforcement Learning (RL) combined with deep neural networks have led to a wide range of\nsuccessful applications, including the game of Go, robotics control and video game playing [32, 30,\n24]. During the training of deep RL agent, the injection of noise into the learning procedure can\nusually prevent the agent from premature convergence to bad locally optimal solutions, for example,\nby entropy regularization [30, 23] or by explicitly optimizing a maximum entropy objective [13, 25].\nThough entropy regularization is much simpler to implement in practice, it greedily optimizes the\npolicy entropy at each time step, without accounting for future effects. On the other hand, maximum\nentropy objective considers the entropy of the distribution over entire trajectories, and is more\nconducive to theoretical analysis [2]. Recently, [13, 14] also shows that optimizing the maximum\nentropy objective can lead to desirable properties such as robustness and multi-modal policy.\nCan we preserve the simplicity of entropy regularization while attaining desirable properties under\nmaximum entropy framework? To achieve this, a necessary condition is an expressive representation\nof policy. Though various \ufb02exible probabilistic models have been proposed in generative modeling\n[10, 37], such models are under-explored in policy based RL. To address such issues, we propose\n\ufb02exible policy classes and ef\ufb01cient algorithms to compute entropy regularized policy gradients.\nIn Section 3, we introduce Implicit Policy, a generic policy representation from which we derive\ntwo expressive policy classes, Normalizing Flows Policy (NFP) and more generally, Non-invertible\nBlackbox Policy (NBP). NFP provides a novel architecture that embeds state information into\nNormalizing Flows; NBP assumes little about policy architecture, yet we propose algorithms to\nef\ufb01ciently compute entropy regularized policy gradients when the policy density is not accessible.\nIn Section 4, we show that entropy regularization optimizes a lower bound of maximum entropy\nobjective. In Section 5, we show that when combined with entropy regularization, expressive policies\nachieve competitive performance on benchmarks and leads to robust and multi-modal policies.\n2\nPreliminaries\n2.1\nBackground\nWe consider the standard RL formalism consisting of an agent interacting with the environment. At\ntime step t \u2265 0, the agent is in state st \u2208 S, takes action at \u2208 A, receives instant reward rt \u2208 R\nand transitions to next state st+1 \u223c p(st+1|st, at). Let \u03c0 : S \ufffd\u2192 A be a policy. The objective of\n\u2217 This research was supported by an Amazon Research Award (2017) and AWS cloud credits.\narXiv:1806.06798v2  [cs.LG]  3 Feb 2019\n",
    "GPTsummary": "- (1): This paper aims to address the issue of expressive policy representation in policy-based reinforcement learning.\n \n- (2): Previous methods, such as entropy regularization and maximum entropy objective, have limitations with respect to robustness and multi-modality. The proposed approach combines entropy regularization with an expressive policy class to obtain desirable properties under the maximum entropy reinforcement learning framework.\n \n- (3): The proposed methodology introduces Implicit Policy, a generic policy representation that leads to two expressive policy classes, Normalizing Flows Policy and Non-invertible Blackbox Policy. These policy classes can flexibly represent complicated action distributions in RL with efficient algorithms to compute entropy regularized policy gradients.\n\n- (4): The proposed approach is evaluated on various benchmarks and demonstrates competitive performance in terms of robustness and multi-modality of policies. Therefore, the performance achieved supports the goals of the paper.\n\n\n\n\n\n8. Conclusion: \n\n- (1): The significance of this work lies in proposing Implicit Policy, a generic policy representation, which can represent complex action distributions in policy-based reinforcement learning. It also provides efficient algorithms to compute entropy regularized policy gradients for such policies. \n\n- (2): Innovation point: The paper proposes a novel approach to address the issue of expressive policy representation in reinforcement learning using Implicit Policy. It introduces two expressive policy classes, Normalizing Flows Policy and Non-invertible Blackbox Policy, to flexibly represent complicated action distributions with efficient entropy regularization techniques.\n\nPerformance: The proposed method is evaluated on various benchmarks and shows competitive performance in terms of robustness and multi-modality of policies.\n\nWorkload: The proposed approach requires efficient computation of entropy regularized policy gradients, which may require some additional computational resources. However, the authors have provided efficient algorithms to address this issue. \n\nOverall, the paper presents a significant contribution to the field of reinforcement learning with its innovative approach, competitive performance, and reasonable workload.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): The significance of this work lies in proposing Implicit Policy, a generic policy representation, which can represent complex action distributions in policy-based reinforcement learning. It also provides efficient algorithms to compute entropy regularized policy gradients for such policies. \n\n- (2): Innovation point: The paper proposes a novel approach to address the issue of expressive policy representation in reinforcement learning using Implicit Policy. It introduces two expressive policy classes, Normalizing Flows Policy and Non-invertible Blackbox Policy, to flexibly represent complicated action distributions with efficient entropy regularization techniques.\n\nPerformance: The proposed method is evaluated on various benchmarks and shows competitive performance in terms of robustness and multi-modality of policies.\n\nWorkload: The proposed approach requires efficient computation of entropy regularized policy gradients, which may require some additional computational resources. However, the authors have provided efficient algorithms to address this issue. \n\nOverall, the paper presents a significant contribution to the field of reinforcement learning with its innovative approach, competitive performance, and reasonable workload.\n\n\n"
}