{
    "Abstract": "Abstract We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without signi\ufb01cantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods. 1 ",
    "Introduction": "",
    "Methods": "Methods Alexander Brown and Marek Petrik University of New Hampshire, 105 Main St, Durham, NH 03824 USA August 9, 2018 Abstract We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without signi\ufb01cantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods. 1 Introduction Reinforcement learning continues to break bounds on what we even thought possible, recently with AlphaGo\u2019s triumph over leading Go player Lee Sedol and with the further successes of AlphaGoZero, which surpassed AlphaGo learning only from self-play [14]. While the performance of such systems is impressive and very useful, sometimes it is desirable to understand and interpret the actions of a reinforcement learning system, and machine learning systems in general. These circumstances are more common in high-pressure applications, such as healthcare, targeted advertising, or \ufb01nance [6]. For example, researchers at the University of Pittsburgh Medical Center trained a variety of machine learning models including neural networks and decision trees to predict whether pneumonia patients might develop severe complications. The neural networks performed the best on their testing data, but upon examination of the rules of the decision trees, the researchers found that the trees recommended sending pneumonia patients who had asthma directly home, despite the fact that asthma makes patients with pneumonia much more likely to suffer complications. Through further investigation they discovered the rule represented a trend in their data: the hospital had a policy to automatically send pneumonia patients with asthma to intensive care, and because this policy was so effective, those patients almost never developed complications. Without the interpretability from the decision trees, it might have been much harder to determine the source of the strange recommendations coming from the neural networks [3]. Interpretability varies between machine learning methods and is dif\ufb01cult to quantify, but decision trees are generally accepted as a good way to create an interpretable model [6]. A decision tree represents a sequence of decisions resulting in a prediction. Small decision trees are easy to show graphically, which helps people interpret and digest them. Although small decision trees are not particularly powerful learners, several trees can be grouped together in an ensemble using techniques such as boosting and bagging in order to create a single more expressive model. No de\ufb01nitive measure of interpretability has been devised, but for decision trees one could count the number of nodes to compare interpretability between trees. With this measure one could also compare ensembles of trees, and as long as the trees are kept small, an ensemble of a few trees could be as interpretable as a single slightly deeper tree. Thus a reasonable ensemble could still be considered interpretable. A more detailed description of decision trees is found in [9]. Of ensemble-building algorithms, gradient boosting is of particular interest to this work. Gradient boosting iteratively builds an ensemble of learners, such as decision trees, in such a way that each new learner attempts to correct the mistakes of its predecessors, optimizing some differentiable loss function. We propose two new ways to compute interpretable solutions to reinforcement learning problems. Firstly, we show, in some benchmark environments, that policy data from a successful reinforcement learning agent (i.e. a neural network) can be used to train an ensemble of decision trees via gradient boosting, and that such an ensemble can match 1 arXiv:1809.06995v1  [cs.LG]  19 Sep 2018 ",
    "Related Work": "Related Work Our work makes cursory use of neural networks and a reinforcement learning algorithm called SARSA, detailed descriptions of which can be found in [9] and [17] respectively. A lot of reinforcement learning has historically been done with neural networks, but there has been work in using other tools from supervised learning in reinforcement learning algorithms. The work of Sridharan and Tesauro is among the \ufb01rst to successfully combine regression trees with Q-learning [16]. The authors of [7] propose two new ensemble algorithms for use in tree-base batch mode reinforcement learning. The general integration of classi\ufb01ers in reinforcement learning algorithms is demonstrated in [10], with the goal of leveraging advances in support vector machines to make reinforcement learning easier to apply \u201cout-of-the-box.\u201d An extensive analysis of classi\ufb01cationbased reinforcement learning with policy iteration is given by [11], which explores a variant of classi\ufb01cation in policy iteration that weights misclassi\ufb01cations by regret, or the difference between the value of the greedy action and that of the action chosen. Our policy gradient boosting is a form of a policy gradient algorithm, which is a class of reinforcement learning techniques which attempt to learn a policy directly, rather than learning any kind of value function, by performing gradient ascent on a performance measure of the current policy. The general form of this kind of algorithm is described in [20], which refers to the class of algorithms as REINFORCE algorithms. These algorithms can be weak by themselves, and can oscillate or even diverge when searching for an optimal policy, so they are often used in combination with a value function or function approximation, forming an \u201dactor-critic\u201d pair [18]. Our algorithm uses a value function approximation, but purely for the purpose of reducing variance, as described in [17]. An overview of interpretable machine learning can be found in [19]. Petrik and Luss show that computing an optimal interpretable policy can be viewed as a MDP and that this task is NP hard [13]. 3 Problem Setting We consider reinforcement learning tasks, which typically consist of an agent interacting with some environment in a series of actions, observations, and rewards, formalized as a Markov Decision Process (MDP). At each time step the agent chooses an action a from a set of legal actions A, which is passed to the environment, which returns a new state s \u2208 S and reward r to the agent, where S is the set of all states. The agent chooses actions according to some policy \u03c0, which is a (potentially stochastic) mapping from S to A. We conduct our experiments in two benchmark environments: cart pole and mountain car. The cart pole problem [2] has been a staple reinforcement learning benchmark for many years. The idea is simple: a cart which can move in one dimension carries an upright pole. The cart can be pushed either left or right, and the goal is to balance the pole for as long as possible. It is a good benchmark because it is easy to visualize, and has a relatively simple solution. The benchmark\u2019s dif\ufb01culty can also easily be scaled upwards inde\ufb01nitely, by stacking additional poles on the initial one [12]. We deal with a version of the problem which terminates when the pole falls too far to be recovered, or when 200 timesteps have elapsed. The mountain car environment, as used in [4] to test control with continuous states, involves a car whose task it is to drive up a hill from in a valley. The car cannot make it up the hill simply by driving forward from the middle of the valley, but rather must reverse up the other side of the valley \ufb01rst to gain momentum. A reward of -1 per time step is given until the car reaches the goal state, at which point the reward is 0 and the episode terminates. The episode also terminates when 200 timesteps have elapsed, if the car has not managed to traverse the hill to the goal. 2 ",
    "Experimental Results": "Experimental ",
    "Results": "Results Supervised learning with reinforcement data was quite successful on both benchmark environments. In the cart-pole environment, the initial policy data was obtained from a neural network, the training of which is shown in Figure 1. As can be seen in the plot, the neural network\u2019s performance was less than perfect, which, although it indicates that the neural network could have been tuned to better suit the environment, serves to show something important about the results. The ensemble performed perfectly in the environment (see Figure 2), lasting the full 200 timesteps each episode, which is noticeably better than the data on which is was trained. We believe this to be a result of over\ufb01tting on the part of the neural network, and it is very promising that the ensemble was resistant enough to over\ufb01tting to outperform the neural network. 3 Algorithm 1 REINFORCE with Trees Input: approximate state-value function v ENSEMBLE \u2190 empty set of trees while training ensemble do L \u2190 empty list for all episodes in batch do reset environment while timestep i in episode do read state si from environment p \u2190 empty list for all action a do pa \u2190 1 |A| where |A| is the number of actions for all trees t in ENSEMBLE do compute weight vector ws according to t p \u2190 p + ws p \u2190 softmax(p) randomly select action ai according to action probability vector p reward ri \u2190 take ai in environment L \u2190 L append (si, ai, reward ri, i) for all s, a, r, i in L do compute discounted future reward Gi compute value of s: v(s) replace current r in L by Gi \u2212 v(s) (see [17]) update v train tree t to predict a from s in L ENSEMBLE \u2190 ENSEMBLE append t 0 20 40 60 80 100 120 140 160 180 200 0 200 400 600 800 1000 1200 1400 Reward Episode Cart-Pole Neural Network Performance Figure 1: Neural Network training in the cart-pole environment 0 20 40 60 80 100 120 140 160 180 200 0 200 400 600 800 1000 Reward Episode Cartpole Trees Performance Figure 2: Ensemble trained from the neural network policy in the cart-pole environment 4 -200 -180 -160 -140 -120 -100 -80 -60 -40 -20 0 0 5000 10000 15000 20000 Reward Episode Mountain Car SARSA Performance Figure 3: SARSA training in the mountain car environment -200 -180 -160 -140 -120 -100 -80 -60 -40 -20 0 0 200 400 600 800 1000 Reward Episode Mountain Car Trees Performance Figure 4: Ensemble trained from the SARSA policy in the mountain car environment 0 20 40 60 80 100 120 140 160 180 200 0 2000 4000 6000 8000 10000 Reward Episode Cart-Pole Gradient Boosting Performance Figure 5: Policy gradient boosting in the cart-pole environment 0 20 40 60 80 100 120 140 160 180 200 0 2000 4000 6000 8000 10000 Reward Episode Cart-Pole Ensemble Recycling Performance Figure 6: Policy gradient boosting with ensemble recycling in the cart-pole environment In the mountain car environment the initial policy data was obtained from a trained SARSA agent [17]. The ensemble did not outperform SARSA, as in the case of cart-pole, but it did match the episodic performance, which was the goal. This is also promising but not truly conclusive, as both environments are quite simple. Policy gradient boosting was successful in the sense that the ensembles did learn how to better behave in the environment. As can be seen in Figure 5, they learn at a decaying rate approaching the maximum reward per episode of 200. The rate at which they learned, however, was much lower than that of the neural network. The neural network began receiving the maximum reward around episode 400, whereas the ensembles only got close after 8,000 - 10,000 episodes. Ensemble recycling was less successful, but does appear to learn in Figure 6. Once ensembles begin to be recycled after episode 4,000 the performance starts to dip, and never really recovers. At this stage it does not seem to be a viable alternative to the non-recycling version. 7 Discussion and Further Research Supervised learning on policy data was very successful, to the extent it was tested. The next step for further research is clear: attempt it in more complicated domains. When considering these domains, in particular those with high dimensional action spaces, the question of whether it makes sense to try to learn how to behave with an ensemble of small trees. There has been work to suggest that some such problems may have simple solutions [15], but it may be challenging for trees to learn those solutions from policy data alone. 5 ",
    "References": "References [1] David Abel et al. \u201cExploratory Gradient Boosting for Reinforcement Learning in Complex Domains\u201d. In: arXiv:1603.04119 (2016). [2] A. G. Barto, R. S. Sutton, and C. W. Anderson. \u201cNeuronlike adaptive elements that can solve dif\ufb01cult learning control problems\u201d. In: IEEE transactions on systems, man, and cybernetics SMC-13 (Sept. 1983), pp. 834\u2013846. [3] Aaron Bornstein. \u201cIs Arti\ufb01cial Intelligence Permanently Inscrutable?\u201d In: Nautilus (2016). [4] Justin A. Boyan and Andrew W. Moore. \u201cGeneralization in Reinforcement Learning: Safely Approximating the Value Function\u201d. In: Advances in Neural Information Processing (1995). [5] Leo Breiman et al. Classi\ufb01cation and Regression Trees. 1984. [6] Amit Dhurandhar, Sechan Oh, and Marek Petrik. \u201cBuilding an Interpretable Recommender via Loss-Preserving Transformation\u201d. In: 2016 ICML Workshop on Human Interpretability in Machine Learning (2016). [7] Damien Ernst, Pierre Geurts, and Louis Wehenkel. \u201cTree-Based Batch Mode Reinforcement Learning\u201d. In: Journal of Machine Learning Research 6 (2005), pp. 503\u2013556. [8] Jerome H. Friedman. \u201cGreedy Function Approximation: A Gradient Boosting Machine\u201d. In: The Annals of Statistics 29.5 (2001), pp. 1189\u20131232. [9] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer, 2009. [10] Michail G. Lagoudakis and Ronald Parr. \u201cReinforcement Learning as Classi\ufb01cation: Leveraging Modern Classi\ufb01ers\u201d. In: Proceedings of ICML (2003). [11] Alessandro Lazaric, Mohammad Ghavamzadeh, and Remi Munos. \u201cAnalysis of Classifcation-based Policy Iteration Algorithms\u201d. In: Journal of Machine Learning Research 16 (2016). [12] Donald Michie and R.A. Chambers. \u201cBoxes: An Experiment in Adaptive Control\u201d. In: Machine Intelligence 2 (1968). [13] Marek Petrik and Ronny Luss. \u201cInterpretable Policies for Dynamic Product Recommendations\u201d. In: Proceedings of the Thirty-Second Conference on Uncertainty in Arti\ufb01cial Intelligence. UAI\u201916. Jersey City, New Jersey, USA: AUAI Press, 2016, pp. 607\u2013616. ISBN: 978-0-9966431-1-5. 6 [14] David Silver et al. \u201cMastering the game of Go without human knowledge\u201d. In: Nature 550 (Oct. 2017). [15] Ozgur Simsek, Simon Algorta, and Amit Kothiyal. \u201cWhy Most Decisions Are Easy in Tetris\u2014And Perhaps in Other Sequential Decision Problems, As Well\u201d. In: Proceedings of The 33rd International Conference on Machine Learning. Ed. by Maria Florina Balcan and Kilian Q. Weinberger. Vol. 48. Proceedings of Machine Learning Research. New York, New York, USA: PMLR, 20\u201322 Jun 2016, pp. 1757\u20131765. [16] Manu Sridharan and Gerald Tesauro. \u201cMulti-agent Q-learning and regression trees for automated pricing decisions\u201d. In: Seventeenth International Conference on Machine Learning (2000), pp. 927\u2013934. [17] Richard S. Sutton and Andrew G. Barto. \u201cReinforcement Learning: an Introduction\u201d. In: 2nd ed. MIT Press, 2016. Chap. 13, pp. 265\u2013279. [18] Csaba Szepesvari. Algorithms for Reinforcement Learning. Morgan and Claypool Publishers, 2009. [19] Alfredo Velido, Jose D. Martin-Guerro, and Paulo J.G. Lisboa. \u201cMaking machine learning models interpretable\u201d. In: Proceedings of the European Symposium on Arti\ufb01cial Neural Networks, Computational Intelligence and Machine Learning (2012). [20] Ronald J. Williams. \u201cSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning\u201d. In: Machine Learning 8 (1992), pp. 229\u2013256. 7 ",
    "title": "Interpretable Reinforcement Learning with Ensemble Methods",
    "paper_info": "Interpretable Reinforcement Learning with Ensemble Methods\nAlexander Brown and Marek Petrik\nUniversity of New Hampshire, 105 Main St, Durham, NH 03824 USA\nAugust 9, 2018\nAbstract\nWe propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforce-\nment learning problems. Boosting combines several regression trees to improve their accuracy without signi\ufb01cantly\nreducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on inter-\npretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental\nresults show that boosted regression trees compute solutions that are both interpretable and match the quality of lead-\ning reinforcement learning methods.\n1\nIntroduction\nReinforcement learning continues to break bounds on what we even thought possible, recently with AlphaGo\u2019s tri-\numph over leading Go player Lee Sedol and with the further successes of AlphaGoZero, which surpassed AlphaGo\nlearning only from self-play [14]. While the performance of such systems is impressive and very useful, sometimes\nit is desirable to understand and interpret the actions of a reinforcement learning system, and machine learning sys-\ntems in general. These circumstances are more common in high-pressure applications, such as healthcare, targeted\nadvertising, or \ufb01nance [6].\nFor example, researchers at the University of Pittsburgh Medical Center trained a variety of machine learning\nmodels including neural networks and decision trees to predict whether pneumonia patients might develop severe\ncomplications. The neural networks performed the best on their testing data, but upon examination of the rules of the\ndecision trees, the researchers found that the trees recommended sending pneumonia patients who had asthma directly\nhome, despite the fact that asthma makes patients with pneumonia much more likely to suffer complications. Through\nfurther investigation they discovered the rule represented a trend in their data: the hospital had a policy to automatically\nsend pneumonia patients with asthma to intensive care, and because this policy was so effective, those patients almost\nnever developed complications. Without the interpretability from the decision trees, it might have been much harder\nto determine the source of the strange recommendations coming from the neural networks [3].\nInterpretability varies between machine learning methods and is dif\ufb01cult to quantify, but decision trees are gener-\nally accepted as a good way to create an interpretable model [6]. A decision tree represents a sequence of decisions\nresulting in a prediction. Small decision trees are easy to show graphically, which helps people interpret and digest\nthem. Although small decision trees are not particularly powerful learners, several trees can be grouped together in\nan ensemble using techniques such as boosting and bagging in order to create a single more expressive model. No\nde\ufb01nitive measure of interpretability has been devised, but for decision trees one could count the number of nodes to\ncompare interpretability between trees. With this measure one could also compare ensembles of trees, and as long as\nthe trees are kept small, an ensemble of a few trees could be as interpretable as a single slightly deeper tree. Thus a\nreasonable ensemble could still be considered interpretable. A more detailed description of decision trees is found in\n[9].\nOf ensemble-building algorithms, gradient boosting is of particular interest to this work. Gradient boosting itera-\ntively builds an ensemble of learners, such as decision trees, in such a way that each new learner attempts to correct\nthe mistakes of its predecessors, optimizing some differentiable loss function.\nWe propose two new ways to compute interpretable solutions to reinforcement learning problems. Firstly, we\nshow, in some benchmark environments, that policy data from a successful reinforcement learning agent (i.e. a neural\nnetwork) can be used to train an ensemble of decision trees via gradient boosting, and that such an ensemble can match\n1\narXiv:1809.06995v1  [cs.LG]  19 Sep 2018\n",
    "GPTsummary": "- (1): The researchers aim to develop a method for computing human-interpretable solutions in reinforcement learning problems.\n\n- (2): Prior to this paper, there was little progress in interpretable reinforcement learning. Previous work focused independently on reinforcement learning and on interpretable machine learning. Decision trees were accepted as a good way to create an interpretable model, but they were not powerful learners. Boosting could improve their accuracy without significantly reducing their inherent interpretability. The researchers proposed using a boosted regression tree to compute interpretable solutions.\n\n- (3): The researchers proposed two ways to compute interpretable solutions to reinforcement learning problems. Firstly, policy data from a successful reinforcement agent can be used to train a boosted regression tree ensemble via gradient boosting. Secondly, a reward shaping algorithm is used to bias the agent towards behaving preferably even during exploration. This algorithm is based on a linear combination of features derived from the decision trees.\n\n- (4): The proposed methods were tested on various benchmark environments. In most cases, the proposed methods achieved performance that matched the quality of leading reinforcement learning methods. The interpretability of the learned policies is supported by the fact that the researchers could explain the policies in human language.\n\n\n7. Methods:\n\n- (1): The researchers proposed to use boosted regression trees to compute interpretable solutions to reinforcement learning problems.\n\n- (2): Boosting was used to combine several regression trees and improve their accuracy without significantly reducing their inherent interpretability.\n\n- (3): Prior work had focused independently on reinforcement learning and on interpretable machine learning, but this paper proposed a method for computing human-interpretable solutions in reinforcement learning problems.\n\n- (4): Two ways to compute interpretable solutions were proposed. Firstly, policy data from a successful reinforcement agent was used to train a boosted regression tree ensemble via gradient boosting. Secondly, a reward shaping algorithm was used to bias the agent towards preferable behavior during exploration, based on a linear combination of features derived from the decision trees.\n\n- (5): The proposed methods were tested on various benchmark environments, achieving performance that matched the quality of leading reinforcement learning methods while maintaining interpretability.\n\n- (6): Interpretable machine learning is important in high-pressure applications, such as healthcare, targeted advertising, and finance, where understanding and interpreting the actions of a reinforcement learning system is desirable. Decision trees are generally accepted as a good way to create an interpretable model, and gradient boosting was shown to be of particular interest in ensemble-building algorithms.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in its contribution to the development of interpretable reinforcement learning methods. Interpretable machine learning is becoming increasingly important, especially in high-pressure applications, such as healthcare, targeted advertising, and finance, where understanding and interpreting the actions of a reinforcement learning system is desirable. \n\n- (2): Innovation point: The article proposes using boosted regression trees to compute interpretable solutions in reinforcement learning problems, which is a novel approach. Performance: The proposed methods achieved performance that matched the quality of leading reinforcement learning methods while maintaining interpretability. Workload: The proposed methods do require some overhead for computing the decision trees, but this is reasonable given the need for interpretable solutions.\n\n\n",
    "GPTmethods": "- (1): The researchers proposed to use boosted regression trees to compute interpretable solutions to reinforcement learning problems.\n\n- (2): Boosting was used to combine several regression trees and improve their accuracy without significantly reducing their inherent interpretability.\n\n- (3): Prior work had focused independently on reinforcement learning and on interpretable machine learning, but this paper proposed a method for computing human-interpretable solutions in reinforcement learning problems.\n\n- (4): Two ways to compute interpretable solutions were proposed. Firstly, policy data from a successful reinforcement agent was used to train a boosted regression tree ensemble via gradient boosting. Secondly, a reward shaping algorithm was used to bias the agent towards preferable behavior during exploration, based on a linear combination of features derived from the decision trees.\n\n- (5): The proposed methods were tested on various benchmark environments, achieving performance that matched the quality of leading reinforcement learning methods while maintaining interpretability.\n\n- (6): Interpretable machine learning is important in high-pressure applications, such as healthcare, targeted advertising, and finance, where understanding and interpreting the actions of a reinforcement learning system is desirable. Decision trees are generally accepted as a good way to create an interpretable model, and gradient boosting was shown to be of particular interest in ensemble-building algorithms.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The significance of this piece of work lies in its contribution to the development of interpretable reinforcement learning methods. Interpretable machine learning is becoming increasingly important, especially in high-pressure applications, such as healthcare, targeted advertising, and finance, where understanding and interpreting the actions of a reinforcement learning system is desirable. \n\n- (2): Innovation point: The article proposes using boosted regression trees to compute interpretable solutions in reinforcement learning problems, which is a novel approach. Performance: The proposed methods achieved performance that matched the quality of leading reinforcement learning methods while maintaining interpretability. Workload: The proposed methods do require some overhead for computing the decision trees, but this is reasonable given the need for interpretable solutions.\n\n\n",
    "GPTconclusion": "- (1): The significance of this piece of work lies in its contribution to the development of interpretable reinforcement learning methods. Interpretable machine learning is becoming increasingly important, especially in high-pressure applications, such as healthcare, targeted advertising, and finance, where understanding and interpreting the actions of a reinforcement learning system is desirable. \n\n- (2): Innovation point: The article proposes using boosted regression trees to compute interpretable solutions in reinforcement learning problems, which is a novel approach. Performance: The proposed methods achieved performance that matched the quality of leading reinforcement learning methods while maintaining interpretability. Workload: The proposed methods do require some overhead for computing the decision trees, but this is reasonable given the need for interpretable solutions.\n\n\n"
}