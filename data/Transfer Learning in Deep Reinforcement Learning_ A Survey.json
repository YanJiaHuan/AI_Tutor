{
    "Abstract": "",
    "Introduction": "INTRODUCTION R Einforcement Learning (RL) is an effective framework to solve sequential decision-making tasks, where a learning agent interacts with the environment to improve its performance through trial and error [1]. Originated from cybernetics and thriving in computer science, RL has been widely applied to tackle challenging tasks which were previously intractable. Traditional RL algorithms were mostly designed for tabular cases, which provide principled solutions to simple tasks but face dif\ufb01culties when handling highly complex domains, e.g. tasks with 3D environments. With the recent advances in computing capability and deep learning research, the combination of RL agents and deep neural networks is developed to address challenging tasks. The combination of deep learning with RL is hence referred to as Deep Reinforcement Learning [2], which learns powerful function approximators using deep neural networks to address complicated domains. RL powered with deep learning has achieved notable success in applications such as robotics control [3, 4] and game playing [5]. It also has a promising prospects in domains such as health informatics [6], electricity networks [7], intelligent transportation systems[8, 9], to name just a few. Besides its remarkable advancement, RL still faces intriguing dif\ufb01culties induced by the exploration-exploitation dilemma [1]. Speci\ufb01cally, for practical RL problems, the environment dynamics are usually unknown, and the agent cannot exploit knowledge about the environment to improve its performance until enough interaction experiences are collected via exploration. Due to the partial observability, sparse feedbacks, and the high complexity of state and action spaces, acquiring suf\ufb01cient interaction samples can be prohibitive, which may even incur safety concerns for \u2022 Zhuangdi Zhu, Anil K. Jain, and Jiayu Zhou are with the Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, 48824. E-mail: {zhuzhuan, jain, jiayuz}@msu.edu \u2022 Kaixiang Lin is with the Amazon Alexa AI. E-mail: lkxcarson@gmail.com domains such as automatic-driving and health informatics, where the consequences of wrong decisions can be too high to take. The abovementioned challenges have motivated various efforts to improve the current RL procedure. As a result, transfer learning, or equivalently referred as knowledge transfer, which is a technique to utilize external expertise from other domains to bene\ufb01t the learning process of the target task, becomes a crucial topic in RL. While transfer learning techniques have been extensively studied in the supervised learning domain [10], it is still an emerging topic for RL. Transfer learning can be more complicated under the RL framework, in that the knowledge needs to transfer in the context of a Markov Decision Process. Moreover, due to the delicate components of the Markov decision process, expert knowledge may take different forms, which need to transfer in different ways. Noticing that previous efforts on summarizing transfer learning for the RL domain have not covered the most recent advancement [11, 12], in this survey, we make a comprehensive investigation of Transfer Learning in Deep Reinforcement Learning. Especially, we propose a systematic framework to categorize the state-of-the-art transfer learning techniques into different sub-topics, review their theories and applications, and analyze their inter-connections. The rest of this survey is organized as follows: In section 2, we introduce the preliminaries of RL and its key algorithms, including those recently designed based on deep neural networks. Next, we clarify the de\ufb01nition of transfer learning in the context of RL and discuss its relevant research topics (Section 2.4). In Section 3, we provide a framework to categorize transfer learning approaches from multiple perspectives, analyze their fundamental differences, and summarize their evaluation metrics (Section 3.3). In Section 4, we elaborate on different transfer learning approaches in the context of deep RL, organized by the format of transferred knowledge, such as reward shaping (Section 4.1), learning from demonstrations (Section 4.2), or learning from arXiv:2009.07888v5  [cs.LG]  16 May 2022 2 teacher policies (Section 4.3). We also investigate transfer learning approaches by the way that knowledge transfer occurs, such as inter-task mapping (Section 4.4), or learning transferrable representations (Section 4.5), etc. We discuss the recent applications of transfer learning in the context of deep RL in Section 5 and provide some future perspectives and open questions in Section 6. 2 DEEP REINFORCEMENT LEARNING AND TRANSFER LEARNING In this section, we provide a brief overview of the recent development in RL and the de\ufb01nitions of some key terminologies. Next, we provide categorizations to organize different transfer learning approaches, then point out some of the other topics in the context of RL, which are relevant to transfer learning but will not be elaborated on in this survey. 2.1 Reinforcement Learning De\ufb01nitions A typical RL problem can be considered as training an agent to interact with an environment that follows a Markov Decision Process (MPD) [13]. For each interaction with the MDP, the agent starts with an initial state and performs an action accordingly, which yields a reward to guide the agent actions. Once the action is taken, the MDP transits to the next state by following the underlying transition dynamics of the MDP. The agent accumulates the time-discounted rewards along with its interactions with the MDP. A subsequence of interactions is referred to as an episode. For MDPs with in\ufb01nite horizons, one can assume that there are absorbing states, such that any action taken upon an absorbing state will only lead to itself and yield zero rewards. All abovementioned components in an MDP can be represented using a tuple, i.e. M = (\u00b50, S, A, T , \u03b3, R, S0), in which: \u2022 \u00b50 is the set of initial states. \u2022 S is the state space. \u2022 A is the action space. \u2022 T : S \u00d7 A \u00d7 S \u2192 R is the transition probability distribution, where T (s\u2032|s, a) speci\ufb01es the probability of the state transitioning to s\u2032 upon taking action a from state s. \u2022 R : S \u00d7 A \u00d7 S \u2192 R is the reward distribution, where R(s, a, s\u2032) is the reward that an agent can get by taking action a from state s with the next state being s\u2032. \u2022 \u03b3 is a discounted factor, with \u03b3 \u2208 (0, 1]. \u2022 S0 is the set of absorbing states. A RL agent behaves in M by following its policy \u03c0, which is a mapping from states to actions: \u03c0 : S \u2192 A . For a stochastic policy \u03c0, \u03c0(a|s) denotes the probability of taking action a from state s. Given an MDP M and a policy \u03c0, one can derive a value function V \u03c0 M(s), which is de\ufb01ned over the state space: V \u03c0 M(s) = E \ufffdr0 + \u03b3r1 + \u03b32r2 + . . . ; \u03c0, s \ufffd , where ri = R(si, ai, si+1) is the reward that an agent receives by taking action ai in the i-th state si, and the next state transits to si+1. The expectation E is taken over s0 \u223c \u00b50, ai \u223c \u03c0(\u00b7|si), si+1 \u223c T (\u00b7|si, ai). The value function estimates the quality of being in state s, by evaluating the expected rewards that an agent can get from s, given that the agent follows policy \u03c0 in the environment M afterward. Similar to the value function, each policy also carries a Q-function, which is de\ufb01ned over the state-action space to estimate the quality of taking action a from state s: Q\u03c0 M(s, a) = Es\u2032\u223cT (\u00b7|s,a) [R(s, a, s\u2032) + \u03b3V \u03c0 M(s\u2032)] . The objective for a RL agent is to learn an optimal policy \u03c0\u2217 M to maximize the expectation of accumulated rewards, so that: \u2200s \u2208 S, \u03c0\u2217 M(s) = arg max a\u2208A Q\u2217 M(s, a), where Q\u2217 M(s, a) = sup \u03c0 Q\u03c0 M(s, a). 2.2 Reinforcement Learning Algorithms In this section, we review the key RL algorithms developed over the recent years, which provide cornerstones for the transfer learning approaches discussed in this survey. Prediction and Control: an RL problem can be disassembled into two subtasks: prediction and control [1]. In the prediction phase, the quality of the current policy is being evaluated. In the control phase or the policy improvement phase, the learning policy is adjusted based on evaluation results from the prediction step. Policies can be improved by iteratively conducting these two steps. The above procedure is therefore called policy iteration. Policy iterations can be model-free, which means that the target policy is optimized without requiring knowledge of the MDP transition dynamics. Traditional model-free RL includes Monte-Carlo methods, which uses samples of episodes to estimate the value of each state based on complete episodes starting from that state. Monte-Carlo methods can be on-policy if the samples are collected by following the target policy, or off-policy if the episodic samples are collected by following a behavior policy that is different from the target policy. Temporal Difference (TD) Learning is an alternative to Monte-Carlo for solving the prediction problem. The key idea behind TD-learning is to learn the state quality function by bootstrapping. It can also be extended to solve the control problem so that both value function and policy can get improved simultaneously. Examples of on-policy TD-learning algorithms include SARSA [14], Expected SARSA [15], ActorCritic [16], and its deep neural network extension called A3C [17]. The off-policy TD-learning approaches include SAC [18] for continuous state-action spaces, and Q-learning [19] for discrete state-action spaces, along with its variants built on deep-neural networks, such as DQN [20], Double-DQN [20], Rainbow [21], etc. TD-learning approaches, such as Q-learning, focus more on estimating the state-action value functions. Policy Gradient, on the other hand, is a mechanism that emphasizes on direct optimization of a parametrizable policy. Traditional policy-gradient approaches include REINFORCE [22]. Recent years have witnessed the joint presence of TD-learning and policy-gradient approaches. Representative algorithms along this line include Trust region policy optimization (TRPO) [23], Proximal Policy optimization (PPO) [24], Deterministic policy gradient (DPG) [25] and its extensions such as DDPG [26] and Twin Delayed DDPG [27]. 2.3 Transfer Learning in the Context of Reinforcement Learning Remark 1. Without losing clarify, for the rest of this survey, we refer to MDPs, domains, and tasks equivalently. 3 We now provide a concrete de\ufb01nition of transfer learning from the RL perspective: Remark 2. [Transfer Learning in the Context of RL] Given a set of source domains Ms = {Ms|Ms \u2208 Ms} and a target domain Mt, Transfer Learning aims to learn an optimal policy \u03c0\u2217 for the target domain, by leveraging exterior information Is from Ms as well as interior information It from Mt, where: \u03c0\u2217 = arg max \u03c0 Es\u223c\u00b5t 0,a\u223c\u03c0[Q\u03c0 M(s, a)], where \u03c0 = \u03c6(Is \u223c Ms, It \u223c Mt) : St \u2192 At is a function mapping from the states to actions for the target domain Mt, which is learned based on information from both It and Is. In the above de\ufb01nition, we use \u03c6(I) to denote the learned policy based on information I. Especially, in the context of deep RL, the policy \u03c0 is learned using deep neural networks. For the simplistic case, knowledge can transfer between two agents within the same domain, which results in |Ms| = 1, and Ms = Mt. One can consider regular RL without transfer learning as a special case of the above de\ufb01nition, by treating Is = \u2205, so that a policy \u03c0 is learned purely on the feedback provided by the target domain, i.e. \u03c0 = \u03c6(It). 2.4 Related Topics In addition to transfer learning, other efforts have been made to bene\ufb01t RL by leveraging different forms of supervision. In this section, we brie\ufb02y discuss other techniques that are relevant to transfer learning by analyzing the differences and connections between transfer leraning and these relevant techniques, which we hope can further clarify the scope of this survey. Imitation Learning aims to train a policy to mimic the behavior of an expert policy. It is considered as an alternative to RL to solve sequential decision-making problems when the environment feedbacks are unavailable [28\u201330]. Closely related to transfer learning, imitation learning can be actually adapted into a transfer learning approach called Learning from Demonstrations (LfD), which will be elaborated in Section 4.2. What distinguishes LfD and the classic Imitation Learning approaches is that LfD still interacts with the domain to access reward signals, in the hope of improving the target policy assisted by a few expert demonstrations, rather than recovering the reward functions or the expert behavior. LfD can be more effective than imitation learning when the expert demonstrations are actually sub-optimal [31, 32]. Lifelong Learning, or Continual Learning, refers to the ability to learn multiple tasks that are temporally or spatially related. The key to acquiring Lifelong Learning is a tradeoff between obtaining new information over time and retaining the previously learned knowledge across new tasks. Lifelong Learning is a technique that is applicable to both supervised learning [33] and RL [34, 35]. Lifelong Learning can be a more challenging task compared to transfer learning, mainly because that it requires an agent to transfer knowledge across a sequence of dynamically-changing tasks which cannot be foreseen, rather than performing knowledge transfer among a \ufb01xed group of tasks. Moreover, the ability of automatic task detection can also be a requirement for Lifelong Learning [36], whereas for transfer learning, the agent is usually noti\ufb01ed of the emergence of a new task. Hierarchical RL has been proposed to resolve real-world tasks that are hierarchical. Different from traditional RL, in a hierarchical RL setting, the action space is grouped into different granularities to form higher-level macro actions. Accordingly, the learning task is also decomposed into hierarchically dependent subgoals. Most well-known hierarchical RL frameworks include Feudal learning [37], Options framework[38], Hierarchical Abstract Machines [39], and MAXQ [40]. Given the higher-level abstraction on tasks, actions, and state spaces, hierarchical RL can facilitate knowledge transfer across similar domains. Multi-Agent RL has strong connections with Game Theory [41]. Different from a single-agent setting, multi-agent RL considers an MDP with multiple agents acting simultaneously in the environment. It aims to solve problems that were dif\ufb01cult or infeasible to be addressed by a single RL agent [42]. The interactive mode for multiple agents can either be independent, cooperative, competitive, or even a hybrid setting [43]. Approaches of knowledge transfer for Multi-agent RL fall into two classes: inter-agent transfer and intra-agent transfer. We refer users to [44] for a more comprehensive survey under this problem setting. Different from their perspective, this survey emphasizes the general transfer learning approaches for a single agent scenario, although approaches mentioned in this survey may also be applicable to multi-agent MPDs. 3 ANALYZING TRANSFER LEARNING FROM MULTIPLE PERSPECTIVES In this section, we provide multiple perspectives to analyze transfer learning approaches in the context of RL. We provide an illustrative example to discuss the potential differences in the source and target domain and the different forms of transferrable knowledge. Then we summarize the metrics for evaluating transfer learning approaches. 3.1 Categorization of Transfer Learning Approaches We point out that transfer learning approaches can be categorized by answering the following key questions: 1) What knowledge is transferred: Knowledge from the source domain can take different forms of supervision, such as expert experiences [45], the action probability distribution of an expert policy [46], or even a potential function that estimates the quality of state and action pairs in the source or target MDP [47]. The divergence in the representations and granularities of knowledge fundamentally decides the way that transfer learning is performed. The quality of the transferred knowledge, e.g. whether it comes from an oracle policy [48] or is provided by a suboptimal teacher [32], also affects the way transfer learning methods are designed. 2) What RL frameworks are compatible with the transfer learning approach: We can rephrase this question into other forms, e.g., is the transfer learning approach policyagnostic, or does it only apply to certain types of RL backbones, such as the Temporal Difference (TD) methods? Answers to this question are closely related to the format of the transferred knowledge. For example, transferring knowledge in the form of expert demonstrations are usually policy-agnostic (see Section 4.2), while policy 4 distillation, as will be discussed in Section 4.3, may not be suitable for RL algorithms such as DQN, which does not explicitly learn a policy function. 3) What is the difference between the source and the target domain: Some transfer learning approaches are suitable for the scenario where the source domain Ms and the target domain Mt are equivalent, whereas others are designed to transfer knowledge between different domains. For example, in video gaming tasks where observations are RGB pixels, Ms and Mt may share the same action space (A) but differs in their observation spaces (S). For other problem settings, such as the goalconditioned RL [49], the two domains may differ only by the reward distribution: Rs \u0338= Rt. Such domain difference induces dif\ufb01culties in transfer learning and affects how much knowledge can transfer. 4) What information is available in the target domain: While the cost of accessing knowledge from source domains is usually affordable, it can be prohibitive for the learning agent to access the target domain, or the learning agent can only have a very limited number of environment interactions due to a high sampling cost. Examples for this scenario include learning an autodriving agent after training it in simulated platforms [50], or training a navigation robot using simulated image inputs before adapting it to real environments [51]. The accessibility of information in the target domain can affect the way that transfer learning approaches are designed. 5) How sample-ef\ufb01cient the transfer learning approach is: This question is related to the previous one regarding the accessibility of a target domain. Compared with learning from scratch, transfer learning enables the learning agent with better initial performance, which usually needs few interactions with the target domain to converge to a good policy, guided by the transferred knowledge. Based on the number of interactions needed to enable transfer learning, we can categorize approaches into the following classes: (i) Zero-shot transfer, which learns an agent that is directly applicable to the target domain without requiring any training interactions with it; (ii) Few-shot transfer, which only requires a few samples (interactions) from the target domain; (iii) Sample-ef\ufb01cient transfer, where an agent can bene\ufb01t by transfer learning to be more sample ef\ufb01cient compared to normal RL. 6) What are the goals of transfer learning: We can answer this question by analyzing two aspects of a transfer learning approach: (i) the evaluation metrics and (i) the objective function. Evaluation metrics can vary from the asymptotic performance to the training iterations used to reach a certain performance threshold, which implies the different emphasis of the transfer learning approach. On the other hand, transfer learning approaches may optimize towards various objective functions augmented with different regularizations, which is usually hinged on the format of the transferred knowledge. For example, maximizing the policy entropy can be combined with the maximum-return learning objective in order to encourage explorations when the transferred knowledge is imperfect demonstrations [52]. 3.2 Case Analysis of Transfer Learning In this section, we use HalfCheetah1 as a working example to illustrate how transfer learning can be performed between the source and the target domain. HalfCheetah is a standard RL benchmark for solving physical locomotion tasks, whose objective is to train a two-leg agent to run as fast as possible without losing control of itself. 3.2.1 Potential Domain Differences: During transfer learning, the differences between the source and target domain may reside in any component of an MDP. For instance, domains for learning a HalfCheetah agent can be different in the following aspects: \u2022 S (State-space): domains can be made different by extending or constraining the available positions for the HalfCheetah agent to move. \u2022 A (Action-space) can be adjusted by changing the range of available torques for the thigh, shin, or foot of the agent. \u2022 R (Reward function): a domain can be simpli\ufb01ed by using only the distance moved forward as rewards or be perplexed by using the scale of accelerated velocity in each direction as extra penalty costs. \u2022 T (Transition dynamics): two domains can differ by following different physical rules, leading to different transition probabilities given the same state-action pairs. \u2022 \u00b50 (Initial states): the source and target domains may have different initial states, specifying where and with what posture the agent can start moving. \u2022 \u03c4 (Trajectories): the source and target domains may allow a different number of steps for the agent to move before a task is done. 3.2.2 Transferrable Knowledge: We list the following transferrable knowledge, assuming that the source and target domains are variants of the HalfCheetah benchmark, although other forms of knowledge transfer may also be feasible: \u2022 Demonstrated trajectories: the target agent can learn from the behavior of a pre-trained expert, e.g. a sequence of running demonstrations. \u2022 Model dynamics: the learning agent may access an approximation model of the physical dynamics, which is learned from the source domain but also applicable in the target domain. The agent can therefore perform dynamic programing based on the physical rules, running as fast as possible while avoiding losing its control due to the accelerated velocity. \u2022 Teacher policies: an expert policy may be consulted by the learning agent, which outputs the probability of taking different actions upon a given state example. \u2022 Teacher value functions: besides teacher policy, the learning agent may also refer to the value function derived by a teacher policy, which implies the quality of state-actions from the teacher\u2019s point of view. 3.3 Evaluation metrics In this section, we enumerate the following representative metrics for evaluating transfer learning approaches, some of which have also been summarized in prior work [11, 53]: 1. https://gym.openai.com/envs/HalfCheetah-v2/ ",
    "Approaches": "APPROACHES In this section, we elaborate on various transfer learning approaches and organize them into different sub-topics, mostly by answering the question of \u201cwhat knowledge is transferred\u201d. For each type of transfer learning approach, we investigate them by following the other criteria mentioned in Section 3. We start with the reward shaping approach (Section 4.1), which is generally applicable to different RL algorithms while requiring minimal changes to the underline RL framework, and overlaps with the other transfer learning approaches discussed in this chapter. We also provide an overview of different transfer learning approaches discussed in this survey in Figure 1. 4.1 Reward Shaping Reward Shaping (RS) is a technique that leverages the exterior knowledge to reconstruct the reward distribution of the target domain to guide the agent\u2019s policy learning. More speci\ufb01cally, in addition to the environment reward signals, RS learns a reward-shaping function F : S \u00d7 S \u00d7 A \u2192 R to render auxiliary rewards, provided that the additional rewards contain external knowledge to guide the agent for better action selections. Intuitively, an RS strategy will assign higher rewards to more bene\ufb01cial state-actions, which can navigate the agent to desired trajectories. As a result, the agent will learn its policy using the newly shaped rewards R\u2032: R\u2032 = R + F, which means that RS has altered the target domain with a different reward function: M = (S, A, T , \u03b3, R)) \u2192 M\u2032 = (S, A, T , \u03b3, R\u2032). Along the line of RS, Potential based Reward Shaping (PBRS) is one of the most classical approaches. [47] proposed PBRS to form a shaping function F as the difference between two potential functions (\u03a6(\u00b7)): F(s, a, s\u2032) = \u03b3\u03a6(s\u2032) \u2212 \u03a6(s), (1) where the potential function \u03a6(\u00b7) comes from the knowledge of expertise and evaluates the quality of a given state. It has been proved that, without further restrictions on the underlying MDP or the shaping function F, PBRS is suf\ufb01cient and necessary to preserve the policy invariance. Moreover, the optimal Q-function in the original and transformed MDP are related by the potential function: Q\u2217 M\u2032(s, a) = Q\u2217 M(s, a) \u2212 \u03a6(s), (2) which draws a connection between potential based rewardshaping and advantage-based learning approaches [55]. The idea of PBRS was extended to [56], which formulated the potential as a function over both the state and the action spaces. This approach is called Potential Based stateaction Advice (PBA). The potential function \u03a6(s, a) therefore evaluates how bene\ufb01cial an action a is to take from state s: F(s, a, s\u2032, a\u2032) = \u03b3\u03a6(s\u2032, a\u2032) \u2212 \u03a6(s, a). (3) One limitation of PBA is that it requires on-policy learning, which can be sample-inef\ufb01cient, as in Equation (3), a\u2032 is the 6 Fig. 1: An overview of different transfer learning approaches, organized by the format of transferred knowledge. action to take upon state s is transitioning to s\u2032 by following the learning policy. Traditional RS approaches assumed a static potential function, until [57] proposed a Dynamic Potential Based (DPB) approach which makes the potential a function of both states and time: F(s, t, s\u2032, t\u2032) = \u03b3\u03a6(s\u2032, t\u2032) \u2212 \u03a6(s, t).They proved that this dynamic approach can still maintain policy invariance: Q\u2217 M\u2032(s, a) = Q\u2217 M(s, a) \u2212 \u03a6(s, t),where t is the current tilmestep. [58] later introduced a way to incorporate any prior knowledge into a dynamic potential function structure, which is called Dynamic Value Function Advice (DPBA). The underline rationale of DPBA is that, given any extra reward function R+ from prior knowledge, in order to add this extra reward to the original reward function, the potential function should satisfy: \u03b3\u03a6(s\u2032, a\u2032) \u2212 \u03a6(s, a) = F(s, a) = R+(s, a). If \u03a6 is not static but learned as an extra state-action Value function overtime, then the Bellman equation for \u03a6 is : \u03a6\u03c0(s, a) = r\u03a6(s, a) + \u03b3\u03a6(s\u2032, a\u2032). The shaping rewards F(s, a) is therefore the negation of r\u03a6(s, a) : F(s, a) = \u03b3\u03a6(s\u2032, a\u2032) \u2212 \u03a6(s, a) = \u2212r\u03a6(s, a). This leads to the approach of using the negation of R+ as the immediate reward to train an extra state-action Value function \u03a6 and the policy simultaneously, with r\u03a6(s, a) = \u2212R+(s, a). \u03a6 will be updated by a residual term \u03b4(\u03a6): \u03a6(s, a) \u2190 \u03a6(s, a) + \u03b2\u03b4(\u03a6), where \u03b4(\u03a6) = \u2212R+(s, a) + \u03b3\u03a6(s\u2032, a\u2032) \u2212 \u03a6(s, a), and \u03b2 is the learning rate. Accordingly, the dynamic potential function F becomes: Ft(s, a) = \u03b3\u03a6t+1(s\u2032, a\u2032) \u2212 \u03a6t(s, a). The advantage of DPBA is that it provides a framework to allow arbitrary knowledge to be shaped as auxiliary rewards. Efforts along this line mainly focus on designing different shaping functions F(s, a), while little work has addressed the question of what knowledge can be used to derive this potential function. One work by [59] proposed to use RS to transfer an expert policy from the source domain (Ms) to the target domain (Mt). This approach assumed the existence of two mapping functions, MS and MA, which can transform the state and action from the source to the target domain. Then the augmented reward is just \u03c0s((MS(s), MA(a))), which is the probability that the mapped state and action will be taken by the expert policy in the source domain. Another work used demonstrated state-action samples from an expert policy to shape rewards [60]. Learning the augmented reward involves a discriminator, which is trained to distinguish samples generated by an expert policy from samples generated by the target policy. The loss of the discriminator is applied to shape rewards to incentivize the learning agent to mimic the expert behavior. This work is a combination of two transfer learning approaches: RS and Learning from Demonstrations, the latter of which will be elaborated in Section 4.2. Besides the single-agent and model-free RL scheme, there have been efforts to apply RS to multi-agent RL [61] and model-based RL [62]. Especially, [61] extended the idea of RS to multi-agent systems, showing that the Nash Equilibria of the underlying stochastic game is unchanged under a potential-based reward shaping structure. [62] applied RS to model-based RL, where the potential function is learned based on the free space assumption, an approach to model transition dynamics in the environment. RS approaches discussed so far are built upon a consensus that the source information for shaping the reward comes externally, which coincides with the notion of knowledge transfer. Some work of RS also considers the scenario where the augmented reward comes intrinsically. Belief Reward Shaping was proposed by [63], which utilizes a Bayesian reward shaping framework to generate the potential value that decays with experience, where the potential value comes from the critic itself. Another work is proposed by [63], which aims at better exploiting the environment-provided reward signal by learning potential functions using ideas from graph representation learning. In addition, [64] proposed to learn the potential functions by leveraging graph convolutional networks to perform message passing from rewarding states. The message then can be used as potential functions for reward shaping. The above RS approaches are summarized in Table 1. 7 Most RS approaches follow the potential based RS principle that has been developed systematically: from the classical PBRS which is built on a static potential shaping function of states, to PBA which generates the potential as a function of both states and actions, and DPB which learns a dynamic potential function of states and time, to the most recent DPBA, which involves a dynamic potential function of states and actions to be learned as an extra state-action Value function in parallel with the environment Value function. As an effective transfer learning paradigm, RS has been widely applied to \ufb01elds including robot training [65], spoken dialogue systems [66], and question answering [67]. It provides a feasible framework for transferring knowledge as the augmented reward and is generally applicable to various RL algorithms. How to integrate RS with other transfer learning approaches to build the potential function, such as Learning from demonstrations (Section 4.2) and Policy Transfer (Section 4.3) will be an intriguing question for future research. 4.2 Learning from Demonstrations In this section, we review transfer learning techniques in which the transferred knowledge takes the form of external demonstrations. The demonstrations may come from different sources with different qualities: it can be provided by a human expert, a previously learned expert policy, or even a suboptimal policy. For the following discussion, we use DE to denote a set of demonstrations, and each element in DE is a tuple of transition: i.e. (s, a, s\u2032, r) \u2208 DE. Efforts along this line mostly address a speci\ufb01c transfer learning scenario, i.e. the source and the target MDPs are the same: Ms = Mt, although there has been work that learns from demonstrations generated in a different domain [68, 69]. In general, learning from demonstrations (LfD) is a technique to assist RL by utilizing provided demonstrations for more ef\ufb01cient exploration. Knowledge conveyed in demonstrations encourages agents to explore states which can bene\ufb01t their policy learning. Depending on when the demonstrations are used for knowledge transfer, approaches can be organized into of\ufb02ine methods and online methods. For of\ufb02ine approaches, demonstrations are either used for pre-training RL components, or used to assist of\ufb02ine RL [70, 71]. When leveraging demonstrations for pre-training, RL components such as the value function V (s) [72], the policy \u03c0 [73], or even the model of transition dynamics [74], can be initialized by learning from these demonstrations. Speci\ufb01cally, [71] illustrated that using demonstrations for representation learning can remarkably improve various downstream tasks, including both of\ufb02ine and online RL. For the online approach, demonstrations are directly used in the online learning stage to guide agent actions for ef\ufb01cient explorations [75]. Most work discussed in this section follows the online transfer paradigm or combines of\ufb02ine pre-training with online RL [76]. Depending on what RL frameworks are compatible, work along this line can be categorized into different branches: some adopts the policy-iteration framework [45, 77, 78], others follow a Q-learning framework [75, 79], while more recent work follows the policy-gradient framework [32, 60, 76, 80]. Demonstrations have been leveraged in the policy iterations framework by [81]. Later, [77] introduced the Direct Policy Iteration with Demonstrations (DPID) algorithm. This approach samples complete demonstrated rollouts DE from an expert policy \u03c0E, in combination with the self-generated rollouts D\u03c0 gathered from the learning agent. D\u03c0 \u222a DE are used to learn a Monte-Carlo estimation of the Q-value: \u02c6Q, from which a learning policy can be derived greedily: \u03c0(s) = arg max a\u2208A \u02c6Q(s, a). This policy \u03c0 is further regularized by a loss function L(s, \u03c0E) to minimize its discrepancy from the expert policy decision: L(\u03c0, \u03c0E) = 1 NE \ufffdNE i=1 1{\u03c0E(si) \u0338= \u03c0(si)}, where NE is the number of expert demonstration samples. Another work along this line includes the Approximate Policy Iteration with Demonstration (APID) algorithm, which was proposed by [45] and extended by [78]. Different from DPID where both DE and D\u03c0 are used for value estimation, the APID algorithm applies only D\u03c0 to approximate on the Q function. The expert demonstrations DE are used to learn the value function, which, given any state si, renders expert actions \u03c0E(si) with higher Q-value margins compared with other actions that are not shown in DE: Q(si, \u03c0E(si)) \u2212 max a\u2208A\\\u03c0E(si)Q(si, a) \u2265 1 \u2212 \u03bei. The term \u03bei is used to account for the case of imperfect demonstrations. [78] further extended the work of APID with a different evaluation loss: L\u03c0 = E(s,a)\u223cD\u03c0\u2225T \u2217Q(s, a) \u2212 Q(s, a)\u2225, where T \u2217Q(s, a) = R(s, a) + \u03b3Es\u2032\u223cp(.|s,a)[max a\u2032 Q(s\u2032, a\u2032)]. Their work theoretically converges to the optimal Q-function compared with APID, as L\u03c0 is minimizing the optimal Bellman residual instead of the empirical norm. In addition to policy iteration, the following two approaches integrate demonstration data into the TD-learning framework, such as Q-learning. Speci\ufb01cally, [75] proposed the Deep Q-learning from Demonstration (DQfD) algorithm, which maintains two separate replay buffers to store demonstrated data and self-generated data, respectively, so that expert demonstrations can always be sampled with a certain probability. Their work leverages the re\ufb01ned priority replay mechanism [82] where the probability of sampling a transition i is based on its priority pi with a temperature parameter \u03b1: P(i) = p\u03b1 i \ufffd k p\u03b1 k . Another work under the Qlearning framework was proposed by [79]. Their approach, dubbed as LfDS, draws a close connection to the reward shaping technique in Section 4.1. It builds the potential function based on a set of expert demonstrations, and the potential value of a given state-action pair is measured by the highest similarity between the given pair and the expert experiences. This augmented reward assigns more credits to state-actions that are more similar to expert demonstrations, which can eventually encourage the agent for expert-like behavior. Besides Q-learning, recent work has integrated LfD into the policy gradient framework [28, 32, 60, 76, 80]. A representative work along this line is Generative Adversarial Imitation Learning (GAIL), proposed by [28]. GAIL introduced the notion of occupancy measure d\u03c0, which is the stationary state-action distributions derived from a policy \u03c0. Based on this notion, a new reward function is designed such that maximizing the accumulated new rewards encourages minimizing the distribution divergence between the occupancy measure of the current policy \u03c0 and the expert policy 8 Methods MDP difference Format of shaping reward Knowledge source PBRS Ms = Mt F = \u03b3\u03a6(s\u2032) \u2212 \u03a6(s) \u0017 PBA Ms = Mt F = \u03b3\u03a6(s\u2032, a\u2032) \u2212 \u03a6(s, a) \u0017 DPB Ms = Mt F = \u03b3\u03a6(s\u2032, t\u2032) \u2212 \u03a6(s, t) \u0017 DPBA Ms = Mt Ft = \u03b3\u03a6t+1(s\u2032, a\u2032) \u2212 \u03a6t(s, a) , \u03a6 learned as an extra Q function \u0017 [59] Ss \u0338= St, As \u0338= At Ft = \u03b3\u03a6t+1(s\u2032, a\u2032) \u2212 \u03a6t(s, a) \u03c0s [60] Ms = Mt Ft = \u03b3\u03a6t+1(s\u2032, a\u2032) \u2212 \u03a6t(s, a) DE TABLE 1: A comparison of reward shaping approaches. \u0017 denotes that the information is not revealed in the paper. \u03c0E. Speci\ufb01cally, the new reward is learned by adversarial training [48]: a discriminator D is trained to distinguish interactions sampled from the current policy \u03c0 and the expert policy \u03c0E: JD = max D:S\u00d7A\u2192(0,1) Ed\u03c0 log[1 \u2212 D(s, a)] + EdE log[D(s, a)] Since \u03c0E is unknown, its state-action distribution dE is estimated based on the given expert demonstrations DE. It has been proved that, for a optimized discriminator, its output satis\ufb01es D(s, a) = d\u03c0 d\u03c0+dE . The output of the discriminator is used as new rewards to encourage distribution matching, with r\u2032(s, a) = \u2212 log(1\u2212D(s, a)). The RL process is naturally altered to perform distribution matching by optimizing the following minimax objective: max \u03c0 min D J(\u03c0, D) : = Ed\u03c0 log[1 \u2212 D(s, a)] + EdE log[D(s, a)]. Although GAIL is more related to imitation learning than LfD, its philosophy of using expert demonstrations for distribution matching has inspired other LfD algorithms. For example, [80] extended GAIL with an algorithm called Policy Optimization from Demonstrations (POfD), which combines the discriminator reward with the environment reward, so that the the agent is trained to maximize the accumulated environment rewards (RL objective) as well as performing distribution matching (imitation learning objective): max \u03b8 = Ed\u03c0[r(s, a)] \u2212 \u03bbDJS[d\u03c0||dE]. (4) Both GAIL and POfD are under an on-policy RL framework. To further improve the sample ef\ufb01ciency of transfer learning, some off-policy algorithms have been proposed, such as DDPGfD [60] which is built upon the DDPG framework. DDPGfD shares a similar idea as DQfD in that they both use a second replay buffer for storing demonstrated data, and each demonstrated sample holds a sampling priority pi. For a demonstrated sample, its priority pi is augmented with a constant bias \u03f5D > 0 for encouraging more frequent sampling of expert demonstrations: pi = \u03b42 i + \u03bb\u2225\u2207aQ(si, ai|\u03b8Q)\u22252 + \u03f5 + \u03f5D, where \u03b4i is the TD-residual for transition, \u2225\u2207aQ(si, ai|\u03b8Q)\u22252 is the loss applied to the actor, and \u03f5 is a small positive constant to ensure all transitions are sampled with some probability. Another work also adopted the DDPG framework to learn from demonstrations [76]. Their approach differs from DDPGfD in that its objective function is augmented with a Behavior Cloning Loss to encourage imitating on provided demonstrations: LBC = \ufffd|DE| i=1 ||\u03c0(si|\u03b8\u03c0) \u2212 ai||2. To further address the issue of suboptimal demonstrations, in [76] the form of Behavior Cloning Loss is altered based on the critic output, so that only demonstration actions with higher Q values will lead to the loss penalty: LBC = |DE| \ufffd i=1 \u2225\u03c0(si|\u03b8\u03c0) \u2212 ai\u22252 1[Q(si, ai) > Q(si, \u03c0(si))]. There are several challenges faced by LfD, one of which is the imperfect demonstrations. Previous approaches usually presume near-oracle demonstrations. However, demonstrations can also be biased estimations of the environment or even from a sub-optimal policy [32]. Current solutions to imperfect demonstrations include altering the objective function. For example, [45] leveraged the hinge-loss function to allow occasional violations of the property that Q(si, \u03c0E(si)) \u2212 max a\u2208A\\\u03c0E(si)Q(si, a) \u2265 1. Some other work uses regularizations on the objective to alleviate over\ufb01tting on biased data [75, 82]. A different strategy to confront the sub-optimality is to leverage those sub-optimal demonstrations only to boost the initial learning stage. Speci\ufb01cally, in the same spirit of GAIL, [32] proposed Self-Adaptive Imitation Learning (SAIL), which learns from sub-optimal demonstrations using generative adversarial training while gradually selecting self-generated trajectories with high qualities to replace less superior demonstrations. Another challenge faced by LfD is covariate drift ([83]): demonstrations may be provided in limited numbers, which results in the learning agent lacking guidance on states that are unseen in the demonstration dataset. This challenge is aggravated in MDPs with sparse reward feedbacks, as the learning agent cannot obtain much supervision information from the environment either. Current efforts to address this challenge include encouraging explorations by using an entropy-regularized objective [52], decaying the effects of demonstration guidance by softening its regularization on policy learning over time [31], and introducing disagreement regularizations by training an ensemble of policies based on the given demonstrations, where the variance among policies serves as a cost (negative reward) function [84]. We summarize the above-discussed approaches in Table 2. In general, demonstration data can help in both of\ufb02ine pretraining for better initialization and online RL for ef\ufb01cient exploration. During the RL phase, demonstration data can be used together with self-generated data to encourage expertlike behaviors (DDPGfD, DQFD), to shape value functions (APID), or to guide the policy update in the form of an auxiliary objective function (PID,GAIL, POfD). The current RL framework used for LfD includes policy iteration, Qlearning, and policy gradient. Developing more general LfD approaches that are agnostic to RL frameworks and can learn from sub-optimal or limited demonstrations would be the next focus for this research domain. ",
    "Methods": "Methods Optimality guarantee Format of transferred demonstrations Reinforcement learning framework DQfD \u0017 Cached transitions in the replay buffer DQN LfDS \u0017 Reward shaping function DQN GAIL \u0013 Reward shaping function: \u2212\u03bb log(1 \u2212 D(s, a)) TRPO POfD \u0013 Reward shaping function: r(s, a) \u2212 \u03bb log(1 \u2212 D(s, a)) TRPO,PPO DDPGfD \u0013 Increasing sampling priority DDPG [76] \u0013 Increasing sampling priority and behavior cloning loss DDPG DPID \u0013 Indicator binary-loss : L(si) = 1{\u03c0E(si) \u0338= \u03c0(si) API APID \u0017 Hinge loss on the marginal-loss: \ufffd L(Q, \u03c0, \u03c0E) \ufffd + API APID extend \u0013 Marginal-loss: L(Q, \u03c0, \u03c0E) API SAIL \u0017 Reward shaping function: r(s, a) \u2212 \u03bb log(1 \u2212 D(s, a)) DDPG TABLE 2: A comparison of learning from demonstration approaches. 4.3 Policy Transfer In this section, we review a transfer learning approach dubbed as policy transfer, where the external knowledge takes the form of pretrained policies from one or multiple source domains. Work discussed in this section is built upon a many-to-one problem setting, which we describe as below: Policy Transfer. A set of teacher policies \u03c0E1, \u03c0E2, . . . , \u03c0EK are trained on a set of source domains M1, M2, . . . , MK, respectively. A student policy \u03c0 is learned for a target domain by leveraging knowledge from {\u03c0Ei}K i=1. For the one-to-one scenario, which contains only one teacher policy, one can consider it as a special case of the above problem setting with K = 1. Next, we categorize recent work of policy transfer into two techniques: policy distillation and policy reuse. 4.3.1 Transfer Learning via Policy Distillation The term knowledge distillation was proposed by [85] as an approach of knowledge ensemble from multiple teacher models into a single student model. This technique is later extended from the \ufb01eld of supervised learning to RL. Since the student model is usually shallower than the teacher model and can perform across multiple teacher tasks, policy distillation is also considered as an effective approach of model compression [86]. The idea of knowledge distillation has been recently applied to the \ufb01eld of RL to enable policy distillation. Conventional policy distillation approaches transfer the teacher policy in a supervised learning paradigm [87, 88]. Speci\ufb01cally, a student policy is learned by minimizing the divergence of action distributions between the teacher policy \u03c0E and student policy \u03c0\u03b8, which is denoted as H\u00d7(\u03c0E(\u03c4t)|\u03c0\u03b8(\u03c4t)): min \u03b8 E\u03c4\u223c\u03c0E \uf8ee \uf8f0 |\u03c4| \ufffd t=1 \u2207\u03b8H\u00d7(\u03c0E(\u03c4t)|\u03c0\u03b8(\u03c4t)) \uf8f9 \uf8fb . The above expectation is taken over trajectories sampled from the teacher policy \u03c0E, which therefore makes this approach teacher distillation. A representative example of work along this line is [87], in which N teacher policies are learned for N source tasks separately, and each teacher yields a dataset DE = {si, qi}N i=0 consisting of observations (states) s and vectors of the corresponding Q-values q, such that qi = [Q(si, a1), Q(si, a2), ...|aj \u2208 A]. Teacher policies are further distilled to a single student agent \u03c0\u03b8 by minimizing the KL-Divergence between each teacher policy \u03c0Ei(a|s) and the student policy \u03c0\u03b8, approximated using the dataset DE: min\u03b8 DKL(\u03c0E|\u03c0\u03b8) \u2248 \ufffd|DE| i=1 softmax \ufffd qE i \u03c4 \ufffd ln \ufffd softmax(qE i ) softmax(q\u03b8 i ) \ufffd . Another policy distillation approach is student distillation [46, 89], which is similar to teacher distillation, except that during the optimization step, the expectation is taken over trajectories sampled from the student policy instead of the teacher policy: min\u03b8 E\u03c4\u223c\u03c0\u03b8 \ufffd\ufffd|\u03c4| t=1 \u2207\u03b8H\u00d7(\u03c0E(\u03c4t)|\u03c0\u03b8(\u03c4t)) \ufffd . [46] provides a nice summarization of the related work on both kinds of distillation approaches. While it is feasible to combine both distillation approaches [83], we observe that more recent work focuses on student distillation, which empirically shows better exploration ability compared to teacher distillation, especially when the teacher policy is deterministic. Taking an alternative perspective, there are two approaches of distilling the knowledge from teacher policies to a student: (1) minimizing the cross-entropy between the teacher and student policy distributions over actions [89, 90]; and (2) maximizing the probability that the teacher policy will visit trajectories generated by the student, i.e. max\u03b8 P(\u03c4 \u223c \u03c0E|\u03c4 \u223c \u03c0\u03b8) [91, 92]. One example of approach (1) is the Actor-mimic algorithm [89]. This algorithm distills the knowledge of expert agents into the student by minimizing the cross entropy between the student policy \u03c0\u03b8 and each teacher policy \u03c0Ei over actions: Li(\u03b8) = \ufffd a\u2208AEi \u03c0Ei(a|s) log\u03c0\u03b8(a|s), where each teacher agent is learned using a DQN framework, whose policy is therefore derived from the Boltzmann distributions over the Q-function output: \u03c0Ei(a|s) = e \u03c4\u22121QEi (s,a) \ufffd a\u2032\u2208AEi e \u03c4\u22121QEi (s,a\u2032) . An instantiation of approach (2) is the Distral algorithm [91]. which learns a centroid policy \u03c0\u03b8 that is derived from K teacher policies. The knowledge in each teacher \u03c0Ei is distilled to the centroid and get transferred to student policies, while both the transition dynamics Ti and reward distributions Ri for source domain Mi are heterogeneous. Speci\ufb01cally, the distilled policy (student) is learned to perform in different domains by maximizing a multi-task learning objective max\u03b8 \ufffdK i=1 J(\u03c0\u03b8, \u03c0Ei), where J(\u03c0\u03b8, \u03c0Ei) = \ufffd t E(st,at)\u223c\u03c0\u03b8 \ufffd \ufffd t\u22650 \u03b3t(ri(at, st)+ \u03b1 \u03b2 log \u03c0\u03b8(at|st) \u2212 1 \u03b2 log(\u03c0Ei(at|st))) \ufffd , in which both log \u03c0\u03b8(at|st) and \u03c0\u03b8 are used as augmented 10 rewards. Therefore, the above approach also draws a close connection to Reward Shaping (Section 4.1). In effect, the log \u03c0\u03b8(at|st) term guides the learning policy \u03c0\u03b8 to yield actions that are more likely to be generated by the teacher policy, whereas the entropy term \u2212 log(\u03c0Ei(at|st) encourages exploration. A similar approach was proposed by [90] which only uses the cross-entropy between teacher and student policy \u03bbH(\u03c0E(at|st)||\u03c0\u03b8(at|st)) to reshape rewards. Moreover, they adopted a dynamically fading coef\ufb01cient to alleviate the effect of the augmented reward so that the student policy becomes independent of the teachers after certain optimization iterations. 4.3.2 Transfer Learning via Policy Reuse Another policy transfer approach is policy reuse, which directly reuses policies from source tasks to build the target policy. The notion of policy reuse was proposed by [93], which directly learns the target policy as a weighted combination of different source-domain policies, while the probability for each source domain policy to be used is related to its expected performance gain in the target domain: P(\u03c0Ei) = exp (tWi) \ufffdK j=0 exp (tWj), where t is a dynamic temperature parameter that increases over time. Under a Q-learning framework, the Q-function of the target policy is learned in an iterative scheme: during every learning episode, Wi is evaluated for each expert policy \u03c0Ei, and W0 is obtained for the learning policy, from which a reuse probability P is derived. Next, a behavior policy is sampled from this probability P. After each training episode, both Wi and the temperature t for calculating the reuse probability is updated accordingly. One limitation of this approach is that the Wi, i.e. the expected return of each expert policy on the target task, needs to be evaluated frequently. This work was implemented in a tabular case, leaving the scalability issue unresolved. More recent work by [94] extended the policy improvement theorem [95] from one to multiple policies, which is named as Generalized Policy Improvement. We refer its main theorem as follows: Theorem. [Generalized Policy Improvement (GPI)] Let {\u03c0i}n i=1 be n policies and let { \u02c6Q\u03c0i}n i=1 be their approximated action-value functions, s.t: \ufffd\ufffd\ufffdQ\u03c0i(s, a) \u2212 \u02c6Q\u03c0i(s, a) \ufffd\ufffd\ufffd \u2264 \u03f5 \u2200s \u2208 S, a \u2208 A, and i \u2208 [n]. De\ufb01ne \u03c0(s) = arg max a max i \u02c6Q\u03c0i(s, a), then: Q\u03c0(s, a) \u2265 max i Q\u03c0i(s, a) \u2212 2 1\u2212\u03b3 \u03f5, \u2200 s \u2208 S, a \u2208 A. Based on this theorem, a policy improvement approach can be naturally derived by greedily choosing the action which renders the highest Q-value among all policies for a given state. Another work along this line is [94], in which an expert policy \u03c0Ei is also trained on a different source domain Mi with reward function Ri, so that Q\u03c0 M0(s, a) \u0338= Q\u03c0 Mi(s, a). To ef\ufb01ciently evaluate the Qfunctions of different source policies in the target MDP, a disentangled representation \u03c8(s, a) over the states and actions is learned using neural networks and is generalized across multiple tasks. Next, a task (reward) mapper wi is learned, based on which the Q-function can be derived: Q\u03c0 i (s, a) = \u03c8(s, a)T wi. [94] proved that the loss of GPI is bounded by the difference between the source and the target tasks. In addition to policy-reuse, their approach involves learning a shared representation \u03c8(s, a), which is also a form of transferred knowledge and will be elaborated more in Section 4.5.2. We summarize the abovementioned policy transfer approaches in Table 3. In general, policy transfer can be realized by knowledge distillation, which can be either optimized from the student\u2019s perspecive (student distillation), or from the teacher\u2019s perspective (teacher distillation) Alternatively, teacher policies can also be directly reused to update the target policy. All approaches discussed so far presumed one or multiple expert policies, which are always at the disposal of the learning agent. Questions such as How to leverage imperfect policies for knowledge transfer, and How to refer to teacher policies within a budget, are still open to be resolved by future research along this line. 4.4 Inter-Task Mapping In this section, we review transfer learning approaches that utilize mapping functions between the source and the target domains to assist knowledge transfer. Research in this domain can be analyzed from two perspectives: (1) which domain does the mapping function apply to, and (2) how is the mapped representation utilized. Most work discussed in this section shares a common assumption as below: Assumption. [Existence of Domain Mapping] One-to-one mappings exist between the source domain Ms and the target domain Mt. Earlier work along this line requires a given mapping function [53, 96]. One examples is [53] which assumes that each target state (action) has a unique correspondence in the source domain, and two mapping functions XS, XA are provided over the state space and the action space, respectively, so that XS(St) \u2192 Ss, XA(At) \u2192 As. Based on XS and XA, a mapping function over the Q-values M(Qs) \u2192 Qt can be derived accordingly. Another work is done by [96] which transfers advice as the knowledge between two domains. In their settings, the advice comes from a human expert who provides the mapping function over the Q-values in the source domain and transfers it to the learning policy for the target domain. This advice encourages the learning agent to prefer certain good actions over others, which equivalently provides a relative ranking of actions in the new task. More later research tackles the inter-task mapping problem by automatically learning a mapping function [97\u201399]. Most work learns a mapping function over the state space or a subset of the state space. In their work, state representations are usually divided into agent-speci\ufb01c and task-speci\ufb01c representations, denoted as sagent and senv, respectively. In [97] and [98], the mapping function is learned on the agent-speci\ufb01c sub state, and the mapped representation is applied to reshape the immediate reward. For [97], the invariant feature space mapped from sagent can be applied across agents who have distinct action space but share some morphological similarity. Speci\ufb01cally, they assume that both agents have been trained on the same proxy task, based on which the mapping function is learned. The mapping function is learned using an encoder-decoder neural network structure [100] in order to reserve as much information about the source domain as possible. While transferring knowledge 11 Citation Transfer approach MDP difference Reinforcement learning framework Metrics [87] Distillation S, A DQN ap [88] Distillation S, A DQN ap, ps [89] Distillation S, A Soft Q-learning ap, ar, ps [91] Distillation S, A A3C ap, pe, tt [93] Reuse R Tabular Q-learning ap [94] Reuse R DQN ap, ar TABLE 3: A comparison of policy transfer approaches. from the source agent to the target agent on a new task, the environment reward is augmented with a shaped reward term to encourage the target agent to imitate the source agent on the embedded feature space: r\u2032(s, \u00b7) = \u03b1 \ufffd\ufffdf(ss agent; \u03b8f) \u2212 g(st agent; \u03b8g) \ufffd\ufffd , where f(ss agent) is the agent-speci\ufb01c state in the source domain, and g(st agent) is for the target domain. [99] applied the Unsupervised Manifold Alignment (UMA) approach [101] to automatically learn the state mapping between tasks. In their approach, trajectories are collected from both the source and the target domain to learn a mapping between states. While applying policy gradient learning, trajectories from Mt are \ufb01rst mapped back to the source: \u03c4t \u2192 \u03c4s, then an expert policy in the source domain is applied to each initial state of those trajectories to generate near-optimal trajectories \u223c\u03c4s, which are further mapped to the target domain: \u223c\u03c4s \u2192 \u223c\u03c4t. The deviation between \u223c\u03c4t and \u03c4t are used as a loss to be minimized in order to improve the target policy. Similar ideas of using UMA to assist transfer by inter-task mapping can also be found in [102] and [103]. In addition to approaches that utilizes mapping over states or actions, [104] proposed to learn an inter-task mapping over the transition dynamics space: S \u00d7 A \u00d7 S. Their work assumes that the source and target domains are different in terms of the transition space dimensionality. Transitions from both the source domain \u27e8ss, as, s\u2032s\u27e9 and the target domain \u27e8st, at, s\u2032t\u27e9 are mapped to a latent space Z. Given the latent feature representations, a similarity measure can be applied to \ufb01nd a correspondence between the source and target task triplets. Triplet pairs with the highest similarity in this feature space Z are used to learn a mapping function X : \u27e8st, at, s\u2032t\u27e9 = X(\u27e8ss, as, s\u2032s\u27e9). After the transition mapping, states sampled from the expert policy in the source domain can be leveraged to render bene\ufb01cial states in the target domain, which assists the target agent learning with a better initialization performance. A similar idea of mapping transition dynamics can be found in [105], which, however, requires a stronger assumption on the similarity of the transition probability and the state representations between the source and the target domains. As summarized in Table 4, for transfer learning approaches that utilize an inter-task mapping, the mapped knowledge can be (a subset of) the state space [97, 98], the Q-function [53], or (representations of) the state-action-sate transitions [104]. In addition to being directly applicable in the target domain [104], the mapped representation can also be used as an augmented shaping reward [97, 98] or a loss objective [99] in order to guide the agent learning in the target domain. 4.5 Representation Transfer In this section, we review approaches the transfer knowledge are feature representations, such as representations learned for the value function or Q-function. Approaches discussed in this section are developed based on the powerful approximation ability of deep neural networks and are built upon the following consensual assumption: Assumption. [Existence of Task-Invariance Subspace] The state space (S), action space (A), or even reward space (R) can be disentangled into orthogonal subspaces, some of which are task-invariant and are shared by both the source and target domains, such that knowledge can be transferred between domains on the universal sub-space. We organize recent work along this line into two subtopics: i) approaches that directly reuse representations from the source domain (Section 4.5.1), and ii) approaches that learn to disentangle the source domain representations into independent sub-feature representations, some of which are on the universal feature space shared by both the source and the target domains (Section 4.5.2). 4.5.1 Reusing Representations A representative work of reusing representations is [106], which proposed the progressive neural network structure to enable knowledge transfer across multiple RL tasks in a progressive way. A progressive network is composed of multiple columns, where each column is a policy network for training one speci\ufb01c task. It starts with one single column for training the \ufb01rst task, and then the number of columns increases with the number of new tasks. While training on a new task, neuron weights on the previous columns are frozen, and representations from those frozen tasks are applied to the new column via a collateral connection to assist in learning the new task. This process can be mathematically generalized as follows: h(k) i = f \ufffd W (k) i h(k) i\u22121 + \ufffd j<k U (k:j) i h(j) i\u22121 \ufffd , where h(k) i is the i-th hidden layer for task (column) k, W (k) i is the associated weight matrix, and U (k:j) i are the lateral connections from layer i \u2212 1 of previous tasks to the current layer of task k. Although progressive network is an effective multi-task approach, it comes with a cost of giant network structure, as the network grows proportionally with the number of incoming tasks. A later framework called PathNet is proposed by [107] which alleviates this issue by using a network with a \ufb01xed size. PathNet contains pathways, which are subsets of neurons whose weights contain the knowledge of previous tasks and are frozen during training on new tasks. The population of pathway is evolved using a tournament selection genetic algorithm [108]. 12 Citation Algorithm MDP difference Mapping function Usage of mapping [53] SARSA St \u0338= St, As \u0338= At M(Qs) \u2192 Qt Q value reuse [96] Q-learning As \u0338= At, Rs \u0338= Rt M(Qs) \u2192 advice Relative Q ranking [97] Generally Applicable Ss \u0338= St M(st) \u2192 r\u2032 Reward shaping [98] SARSA(\u03bb) Ss \u0338= St Rs \u0338= Rt M(st) \u2192 r\u2032 Reward shaping [99] Fitted Value Iteration Ss \u0338= St M(ss) \u2192 st Penalty loss on state deviation from expert policy [105] Fitted Q Iteration Ss \u00d7 As \u0338= St \u00d7 At M \ufffd (ss, as, s\u2032 s) \u2192 (st, at, s\u2032 t) \ufffd Reduce random exploration [104] No constraint Ss \u00d7 As \u0338= St \u00d7 At M \ufffd (ss, as, s\u2032 s) \u2192 (st, at, s\u2032 t) \ufffd Reduce random exploration TABLE 4: A comparison of inter-task mapping approaches. Another approach of reusing representations for transfer learning is modular networks [109\u2013111]. For example, [109] proposed to decompose the policy network into a taskspeci\ufb01c module and agent-speci\ufb01c module. Speci\ufb01cally, let \u03c0 be a policy performed by any agent (robot) r over the task Mk as a function \u03c6 over states s, it can be decomposed into two sub-modules gk and fr, i.e.: \u03c0(s) := \u03c6(senv, sagent) = fr(gk(senv), sagent), where fr is the agent-speci\ufb01c module and gk is the taskspeci\ufb01c module. Their core idea is that the task-speci\ufb01c module can be applied to different agents performing the same task, which serves as a transferred knowledge. Accordingly, the agent-speci\ufb01c module can be applied to different tasks for the same agent. A model-based approach along this line is [111], which learns a model to map the state observation s to a latentrepresentation z. The transition probability is modeled on the latent space instead of the original state space, i.e. \u02c6zt+1 = f\u03b8(zt, at), where \u03b8 is the parameter of the transition model, zt is the latent-representation of the state observation, and at is the action accompanying that state. Next, a reward module learns the value function as well as the policy from the latent space z using an actor-critic framework. One potential bene\ufb01t of this latent representation is that knowledge can be transferred across tasks that have different rewards but share the same transition dynamics, in which case the dynamics module can be directly applied to the target domain. 4.5.2 Disentangling Representations Methods discussed in this section mostly focus on learning a disentangled representation. Speci\ufb01cally, we elaborate on transfer learning approaches that are derived from two techniques: Successor Representation (SR) and Universal Value Function Approximating (UVFA). Successor Representations (SR) is an approach to decouple the state features of a domain from its reward distributions. It enables knowledge transfer across multiple domains: M = {M1, M2, . . . , MK}, so long as the only difference among them is the reward distributions: Ri \u0338= Rj. SR was originally derived from neuroscience, until [112] proposed to leverage it as a generalization mechanism for state representations in the RL domain. Different from the v-value or Q-value that describes states as dependent on the reward distribution of the MDP, SR features a state based on the occupancy measure of its successor states. Speci\ufb01cally, SR decomposes the value function of any policy into two independent components, \u03c8 and R: V \u03c0(s) = \ufffd s\u2032 \u03c8(s, s\u2032)w(s\u2032), where w(s\u2032) is a reward mapping function that maps states to scalar rewards, and \u03c8 is the SR which describes any state s as the occupancy measure of the future occurred states when following \u03c0: \u03c8(s, s\u2032) = E\u03c0 \ufffd\ufffd\u221e i=t \u03b3i\u2212t1[Si = s\u2032]|St = s \ufffd , with 1[S = s\u2032] = 1 as an indicator function. The successor nature of SR makes it learnable using any TD-learning algorithms. Especially, [112] proved the feasibility of learning such representation in a tabular case, in which the state transitions can be described using a matrix. SR was later extended by [94] from three perspectives: (i) the feature domain of SR is extended from states to state-action pairs; (ii) deep neural networks are used as function approximators to represent the SR \u03c8\u03c0(s, a) and the reward mapper w; (iii) Generalized policy improvement (GPI) algorithm is introduced to accelerate policy transfer for multi-tasks facilitated by the SR framework (See Section 4.3.2 for more details about GPI). These extensions, however, are built upon a stronger assumption about the MDP: Assumption. [Linearity of Reward Distributions] The reward functions of all tasks can be computed as a linear combination of a \ufb01xed set of features: r(s, a, s\u2032) = \u03c6(s, a, s\u2032)\u22a4w, where \u03c6(s, a, s\u2032) \u2208 Rd denotes the latent representation of the state transition, and w \u2208 Rd is the task-speci\ufb01c reward mapper. Based on this assumption, SR can be decoupled from the rewards when evaluating the Q-function of any policy \u03c0 in a task Mi with a reward function Ri: Q\u03c0 i (s, a) = E\u03c0[\u03c6\u22a4 t+1wi + \u03b3\u03c6\u22a4 t+2wi + . . . |St = s, At = a] = \u03c8\u03c0(s, a)\u22a4wi. (5) The advantage of SR is that, when the knowledge of \u03c8\u03c0(s, a) in Ms is observed, one can quickly get the performance evaluation of the same policy in Mt by replacing ws with wt: Q\u03c0 Mt = \u03c8\u03c0(s, a)wt. Similar ideas of learning SR as a TD-algorithm on a latent representation \u03c6(s, a, s\u2032) can also be found in [113, 114]. Speci\ufb01cally, the work of [113] was developed based on a weaker assumption about the reward function: Instead of requiring linearly-decoupled rewards, the latent space \u03c6(s, a, s\u2032) is learned in an encoder-decoder structure to ensure that the information loss is minimized when mapping states to the latent space. This structure, therefore, comes with an extra cost of learning a decoder fd to reconstruct the state: fd(\u03c6(st)) \u2248 st. An intriguing question faced by the SR approach is: Is there a way that evades the linearity assumption about reward functions and still enables learning the SR without extra modular cost? An extended work of SR [54] answered this question af\ufb01rmatively, which proved that the reward functions does not necessarily have to follow the linear structure, yet at the ",
    "Discussion": "Discussion We provide a summary of the discussed work in this section in Table 5. In general, representation transfer approaches can facilitate transfer learning in many ways, with certain shared assumptions about some task-invariant property. Most of them assume that tasks are different only in terms of their reward distributions while sharing the same states (or actions or transitions) probabilities. Other stronger assumptions include (i) decoupled dynamics, rewards [94], or policies [117] from the Q-function representations, and (ii) the feasibility of de\ufb01ning tasks in terms of states [117]. Based on those assumptions, approaches such as TD-algorithms [54] or matrix-factorization [49] become applicable to learn such disentangled representations. To further exploit the effectiveness of disentangled structure, we think that generalization approaches, which allow changing dynamics or state distributions, are important future work that is worth more attention in this domain. As an intriguing research topic, there are unresolved questions along the line of representation transfer. One is how to handle drastic changes of reward functions between domains. As discussed in [118], good policies in one MDP may perform poorly in another due to the fact that bene\ufb01cial states or actions in Ms may become detrimental in Mt with totally different reward functions. Learning a set of basis functions [54] to represent unseen tasks (reward functions), or decoupling policies from Q-function representation [117] may serve as a good start to address this issue, as they propose a generalized latent space, from which different tasks (reward functions) can be interpreted. However, the limitation of this work is that it is not clear how many and what kind of sub-tasks need to be learned to make the latent space generalizable enough to interpret unseen tasks. Another question is how to generalize the representation framework to allow transfer learning across domains with different dynamics (or state-action spaces). A learned SR might not be transferrable to an MDP with different transition dynamics, as the distribution of occupancy measure for successor states may no longer hold. Potential solutions may include model-based approaches that approximate the dynamics directly or training a latent representation space for states using multiple tasks with different dynamics for better generalization [119]. Alternatively, transfer learning mechanisms from the supervised learning domain, such as meta-learning, which enables the ability of fast adaptation to new tasks [120], or importance sampling [121], which can 14 compensate for the prior distribution changes [10], might also shed light on this question. 5 APPLICATIONS In this section, we summarize recent applications of RL based transfer learning techniques. Robotics learning is a practical domain for RL applications. [122] provided a comprehensive summary of RL in robotics learning. A classical approach along this direction is robotics learning from demonstrations, where expert demonstrations from humans or other robots are leveraged as the knowledge source. [123] provided a comprehensive survey along this direction. Later there emerged a scheme of collaborative robotic training [124], where knowledge from different robots is transferred by sharing their policies and episodic demonstrations with each other. One representative work is [125], which performs policy transfer across multiple robot agents under the DQN framework. Recent work shifted their focus to fast and robust adaptation to unseen tasks. A typical approach is to design and select multiple source domains for robust training, in order to learn a generalized policy from those source tasks, which can be quickly adapted to target domains. Examples include the EPOPT algorithm [126], which is a combination of policy transfer via source domain ensemble and learning from limited demonstrations. Another application can be found in [127], in which robust agent policies are trained using a large number of synthetic demonstrations from a simulator to handle dynamic environments. Another solution to fast adaptation is to learn latent representations from observations in the source domain that are generally applicable to the target domain, e.g. training robots using simulated 2D image inputs and applying the robot in real 3D environments. Work along this line includes [128], which learns the latent representation using 3D CAD models, and [129, 130] which are derived based on the Generative-Adversarial Network. Another example is DARLA [131], which is a zero-shot transfer approach to learn disentangled representations that are robust against domain shifts. Game Playing is one representative testbed for transfer learning and RL algorithms, whose complexity has evolved over the recent decades, from classical testbeds such as gridworld games to more complex game settings such as onlinestrategy games or video games with pixel GRB inputs. A representative application in game playing is AlphaGo, which is an algorithm for learning the online chessboard games using both transfer learning and RL techniques [73]. AlphaGo is \ufb01rst pre-trained of\ufb02ine using expert demonstrations and then learns to optimize its policy using Monte-Carlo Tree Search. Its successor, AlphaGo Master [132], even beat the world\u2019s \ufb01rst ranked human player. In addition to online chessboard games, transfer learning approaches have also performed well in video game playing using RL backbones. State-of-the-art video game platforms include MineCraft, Atari, and Starcraft. Especially, [133] designed new RL tasks under the MineCraft platform for a better comparison of different RL algorithms. We refer readers to [134] for a survey of AI for real-time strategy (RTS) games on the Starcraft platform, with a dataset available from [135]. Moreover, [136] provided a comprehensive survey on DL applications in video game playing, which also covers transfer learning and RL strategies from certain perspectives. A large portion of transfer learning approaches reviewed in this survey have been applied to the Atari [137] and other game above-mentioned platforms. Especially, OpenAI trained an Dota2 agent that can surpass human experts [138]. Natural Language Processing (NLP) has evolved rapidly along with the advancement of DL and RL. Applications of RL on NLP range widely, from Question Answering (QA) [139], Dialogue systems [140], Machine Translation [141], to an integration of NLP and Computer Vision tasks, such as Visual Question Answering (VQA) [142], Image Caption [143], etc. Many NLP applications have implicitly applied transfer learning approaches, including learning from demonstrations, policy transfer, or reward shaping, to better tailor these RL techniques as NLP solutions, which were previously dominated by the supervise-learning counterparts [144]. Examples in this \ufb01led include applying expert demonstration to build RL solutions for Spoken Dialogue Systems [145], VQA [142]; or building shaped rewards for Sequence Generation [146], Spoken Dialogue Systems [66],QA [67, 147], and Image Caption [143], or transferring policies for Structured Prediction [148] and VQA [149], etc. Health Informatics is another domain that has bene\ufb01ted from the advancement of RL. RL has been applied to solve various healthcare tasks, including dynamic treatment regimes [150, 151], automatic medical diagnosis [152, 153], health resource scheduling [154, 155], and drug discovery and development, [156, 157], etc. An overview of recent achievements along this line is provided by [158]. Despite the emergence of RL to address healthcare problems, only a limited number of them have utilized transfer learning approaches, although we do observe some applications that leverage prior knowledge to improve the RL procedure. Speci\ufb01cally, [159] utilized Qlearning for drug delivery individualization. They integrated the prior knowledge of the dose-response characteristics into their Q-learning framework to avoid random exploration. Some work considered reusing representations for speeding up learning [160, 161]. For example, [160] proposed to highlight both the individual variability and the common policy model structure for individual HIV treatment. [161] applied a DQN framework for prescribing effective HIV treatments, in which they learned a latent representation to estimate the uncertainty when transferring a pertained policy to the unseen domains. [162] considered the possibility of applying human-involved interactive RL training for health informatics. In general, transfer learning combined with RL is a promising integration for health informatics to improve the learning effectiveness and sample ef\ufb01ciency, especially given the dif\ufb01culty of accessing large amounts of clinical data. Others: RL has also been utilized in many other real-life applications. Applications in the Transportation Systems have adopted RL to address traf\ufb01c congestion issues with better traf\ufb01c signal scheduling and transportation resource allocation [8, 9, 163, 164]. We refer readers to [165] for a review along this line. Deep RL are also effective solutions to problems in Finance, including portfolio management [166, 167], asset allocation [168], and trading optimization [169]. Another application is the Electricity Systems, especially the intelligent electricity networks, which can bene\ufb01t from RL techniques ",
    "References": "REFERENCES [1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, \u201cA brief survey of deep reinforcement learning,\u201d arXiv preprint arXiv:1708.05866, 2017. [3] S. Levine, C. Finn, T. Darrell, and P. Abbeel, \u201cEnd-toend training of deep visuomotor policies,\u201d The Journal of Machine Learning Research, 2016. [4] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, \u201cLearning hand-eye coordination for robotic grasping with deep learning and large-scale data collection,\u201d The International Journal of Robotics Research, 2018. [5] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, \u201cThe arcade learning environment: An evaluation platform for general agents,\u201d Journal of Arti\ufb01cial Intelligence Research, 2013. [6] M. R. Kosorok and E. E. Moodie, Adaptive TreatmentStrategies in Practice: Planning Trials and Analyzing Data for Personalized Medicine. SIAM, 2015. [7] M. Glavic, R. Fonteneau, and D. Ernst, \u201cReinforcement learning for electric power system decision and control: Past considerations and perspectives,\u201d IFACPapersOnLine, 2017. [8] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, \u201cMultiagent reinforcement learning for integrated network of adaptive traf\ufb01c signal controllers (marlin-atsc): methodology and large-scale application on downtown toronto,\u201d IEEE Transactions on Intelligent Transportation Systems, 2013. [9] H. Wei, G. Zheng, H. Yao, and Z. Li, \u201cIntellilight: A reinforcement learning approach for intelligent traf\ufb01c 16 light control,\u201d ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2018. [10] S. J. Pan and Q. Yang, \u201cA survey on transfer learning,\u201d IEEE Transactions on knowledge and data engineering, 2009. [11] M. E. Taylor and P. Stone, \u201cTransfer learning for reinforcement learning domains: A survey,\u201d Journal of Machine Learning Research, 2009. [12] A. Lazaric, \u201cTransfer in reinforcement learning: a framework and a survey.\u201d Springer, 2012. [13] R. Bellman, \u201cA markovian decision process,\u201d Journal of mathematics and mechanics, 1957. [14] G. A. Rummery and M. Niranjan, On-line Q-learning using connectionist systems. University of Cambridge, Department of Engineering Cambridge, England, 1994. [15] H. Van Seijen, H. Van Hasselt, S. Whiteson, and M. Wiering, \u201cA theoretical and empirical analysis of expected sarsa,\u201d IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, 2009. [16] V. Konda and J. Tsitsiklis, \u201cActor-critic algorithms,\u201d Advances in neural information processing systems, 2000. [17] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, \u201cAsynchronous methods for deep reinforcement learning,\u201d ICML, 2016. [18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \u201cSoft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,\u201d International Conference on Machine Learning, 2018. [19] C. J. Watkins and P. Dayan, \u201cQ-learning,\u201d Machine learning, 1992. [20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., \u201cHuman-level control through deep reinforcement learning,\u201d Nature, 2015. [21] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, \u201cRainbow: Combining improvements in deep reinforcement learning,\u201d AAAI, 2018. [22] R. J. Williams, \u201cSimple statistical gradient-following algorithms for connectionist reinforcement learning,\u201d Machine learning, 1992. [23] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, \u201cTrust region policy optimization,\u201d ICML, 2015. [24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv preprint arXiv:1707.06347, 2017. [25] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, \u201cDeterministic policy gradient algorithms,\u201d 2014. [26] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971, 2015. [27] S. Fujimoto, H. Van Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic methods,\u201d arXiv preprint arXiv:1802.09477, 2018. [28] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d NeurIPS, 2016. [29] Z. Zhu, K. Lin, B. Dai, and J. Zhou, \u201cOff-policy imitation learning from observations,\u201d NeurIPS, 2020. [30] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson, \u201cDiscriminator-actor-critic: Addressing sample inef\ufb01ciency and reward bias in adversarial imitation learning,\u201d arXiv preprint arXiv:1809.02925, 2018. [31] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang, B. Fang, and H. Liu, \u201cReinforcement learning from imperfect demonstrations under soft expert guidance.\u201d AAAI, 2020. [32] Z. Zhu, K. Lin, B. Dai, and J. Zhou, \u201cLearning sparse rewarded tasks from sub-optimal demonstrations,\u201d arXiv preprint arXiv:2004.00530, 2020. [33] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, \u201cContinual lifelong learning with neural networks: A review,\u201d Neural Networks, 2019. [34] R. S. Sutton, A. Koop, and D. Silver, \u201cOn the role of tracking in stationary environments,\u201d Proceedings of the 24th international conference on Machine learning, 2007. [35] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and P. Abbeel, \u201cContinuous adaptation via meta-learning in nonstationary and competitive environments,\u201d ICLR, 2018. [36] C. H. Lampert, H. Nickisch, and S. Harmeling, \u201cLearning to detect unseen object classes by between-class attribute transfer,\u201d IEEE Conference on Computer Vision and Pattern Recognition, 2009. [37] P. Dayan and G. E. Hinton, \u201cFeudal reinforcement learning,\u201d NeurIPS, 1993. [38] R. S. Sutton, D. Precup, and S. Singh, \u201cBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,\u201d Arti\ufb01cial intelligence, 1999. [39] R. Parr and S. J. Russell, \u201cReinforcement learning with hierarchies of machines,\u201d NeurIPS, 1998. [40] T. G. Dietterich, \u201cHierarchical reinforcement learning with the maxq value function decomposition,\u201d Journal of arti\ufb01cial intelligence research, 2000. [41] R. B. Myerson, Game theory. Harvard university press, 2013. [42] L. Bu, R. Babu, B. De Schutter et al., \u201cA comprehensive survey of multiagent reinforcement learning,\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 2008. [43] M. Tan, \u201cMulti-agent reinforcement learning: Independent vs. cooperative agents,\u201d 1993. [44] F. L. Da Silva and A. H. R. Costa, \u201cA survey on transfer learning for multiagent reinforcement learning systems,\u201d Journal of Arti\ufb01cial Intelligence Research, 2019. [45] B. Kim, A.-m. Farahmand, J. Pineau, and D. Precup, \u201cLearning from limited demonstrations,\u201d NeurIPS, 2013. [46] W. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, and M. Jaderberg, \u201cDistilling policy distillation,\u201d The 22nd International Conference on Arti\ufb01cial Intelligence and Statistics, 2019. [47] A. Y. Ng, D. Harada, and S. Russell, \u201cPolicy invariance under reward transformations: Theory and application to reward shaping,\u201d ICML, 1999. [48] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, 17 \u201cGenerative adversarial nets,\u201d NeurIPS, pp. 2672\u20132680, 2014. [49] T. Schaul, D. Horgan, K. Gregor, and D. Silver, \u201cUniversal value function approximators,\u201d ICML, 2015. [50] C. Finn and S. Levine, \u201cMeta-learning: from few-shot learning to rapid reinforcement learning,\u201d ICML, 2019. [51] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke, \u201cSim-to-real: Learning agile locomotion for quadruped robots,\u201d arXiv preprint arXiv:1804.10332, 2018. [52] Y. Gao, J. Lin, F. Yu, S. Levine, T. Darrell et al., \u201cReinforcement learning from imperfect demonstrations,\u201d arXiv preprint arXiv:1802.05313, 2018. [53] M. E. Taylor, P. Stone, and Y. Liu, \u201cTransfer learning via inter-task mappings for temporal difference learning,\u201d Journal of Machine Learning Research, 2007. [54] A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. \u02c7Z\u00b4\u0131dek, and R. Munos, \u201cTransfer in deep reinforcement learning using successor features and generalised policy improvement,\u201d arXiv preprint arXiv:1901.10964, 2019. [55] R. J. Williams and L. C. Baird, \u201cTight performance bounds on greedy policies based on imperfect value functions,\u201d Tech. Rep., 1993. [56] E. Wiewiora, G. W. Cottrell, and C. Elkan, \u201cPrincipled methods for advising reinforcement learning agents,\u201d ICML, 2003. [57] S. M. Devlin and D. Kudenko, \u201cDynamic potentialbased reward shaping,\u201d ICAAMAS, 2012. [58] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Now\u00b4e, \u201cExpressing arbitrary reward functions as potentialbased advice,\u201d AAAI, 2015. [59] T. Brys, A. Harutyunyan, M. E. Taylor, and A. Now\u00b4e, \u201cPolicy transfer using reward shaping,\u201d ICAAMS, 2015. [60] M. Ve\u02c7cer\u00b4\u0131k, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Roth\u00a8orl, T. Lampe, and M. Riedmiller, \u201cLeveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards,\u201d arXiv preprint arXiv:1707.08817, 2017. [61] S. Devlin, L. Yliniemi, D. Kudenko, and K. Tumer, \u201cPotential-based difference rewards for multiagent reinforcement learning,\u201d ICAAMS, 2014. [62] M. Grzes and D. Kudenko, \u201cLearning shaping rewards in model-based reinforcement learning,\u201d Proc. AAMAS Workshop on Adaptive Learning Agents, 2009. [63] O. Marom and B. Rosman, \u201cBelief reward shaping in reinforcement learning,\u201d AAAI, 2018. [64] M. Klissarov and D. Precup, \u201cReward propagation using graph convolutional networks,\u201d NeurIPS, 2020. [65] A. C. Tenorio-Gonzalez, E. F. Morales, and L. Villase\u02dcnor-Pineda, \u201cDynamic reward shaping: Training a robot by voice,\u201d Advances in Arti\ufb01cial Intelligence \u2013 IBERAMIA, 2010. [66] P.-H. Su, D. Vandyke, M. Gasic, N. Mrksic, T.-H. Wen, and S. Young, \u201cReward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems,\u201d arXiv preprint arXiv:1508.03391, 2015. [67] X. V. Lin, R. Socher, and C. Xiong, \u201cMulti-hop knowledge graph reasoning with reward shaping,\u201d arXiv preprint arXiv:1808.10568, 2018. [68] F. Liu, Z. Ling, T. Mu, and H. Su, \u201cState alignment-based imitation learning,\u201d arXiv preprint arXiv:1911.10947, 2019. [69] K. Kim, Y. Gu, J. Song, S. Zhao, and S. Ermon, \u201cDomain adaptive imitation learning,\u201d ICML, 2020. [70] Y. Ma, Y.-X. Wang, and B. Narayanaswamy, \u201cImitationregularized of\ufb02ine learning,\u201d International Conference on Arti\ufb01cial Intelligence and Statistics, 2019. [71] M. Yang and O. Nachum, \u201cRepresentation matters: Of\ufb02ine pretraining for sequential decision making,\u201d arXiv preprint arXiv:2102.05815, 2021. [72] X. Zhang and H. Ma, \u201cPretraining deep actor-critic reinforcement learning algorithms with expert demonstrations,\u201d arXiv preprint arXiv:1801.10459, 2018. [73] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, 2016. [74] S. Schaal, \u201cLearning from demonstration,\u201d NeurIPS, 1997. [75] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband et al., \u201cDeep q-learning from demonstrations,\u201d AAAI, 2018. [76] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, \u201cOvercoming exploration in reinforcement learning with demonstrations,\u201d IEEE International Conference on Robotics and Automation (ICRA), 2018. [77] J. Chemali and A. Lazaric, \u201cDirect policy iteration with demonstrations,\u201d International Joint Conference on Arti\ufb01cial Intelligence, 2015. [78] B. Piot, M. Geist, and O. Pietquin, \u201cBoosted bellman residual minimization handling expert demonstrations,\u201d Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2014. [79] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Now\u00b4e, \u201cReinforcement learning from demonstration through shaping,\u201d International Joint Conference on Arti\ufb01cial Intelligence, 2015. [80] B. Kang, Z. Jie, and J. Feng, \u201cPolicy optimization with demonstrations,\u201d ICML, 2018. [81] D. P. Bertsekas, \u201cApproximate policy iteration: A survey and some new methods,\u201d Journal of Control Theory and Applications, 2011. [82] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, \u201cPrioritized experience replay,\u201d ICLR, 2016. [83] S. Ross, G. Gordon, and D. Bagnell, \u201cA reduction of imitation learning and structured prediction to noregret online learning,\u201d AISTATS, 2011. [84] K. Brantley, W. Sun, and M. Henaff, \u201cDisagreementregularized imitation learning,\u201d ICLR, 2019. [85] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d Deep Learning and Representation Learning Workshop, NeurIPS, 2014. [86] A. Polino, R. Pascanu, and D. Alistarh, \u201cModel compression via distillation and quantization,\u201d arXiv preprint arXiv:1802.05668, 2018. [87] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, \u201cPolicy distillation,\u201d 18 arXiv preprint arXiv:1511.06295, 2015. [88] H. Yin and S. J. Pan, \u201cKnowledge transfer for deep reinforcement learning with hierarchical experience replay,\u201d AAAI, 2017. [89] E. Parisotto, J. L. Ba, and R. Salakhutdinov, \u201cActormimic: Deep multitask and transfer reinforcement learning,\u201d arXiv preprint arXiv:1511.06342, 2015. [90] S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan et al., \u201cKickstarting deep reinforcement learning,\u201d arXiv preprint arXiv:1803.03835, 2018. [91] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu, \u201cDistral: Robust multitask reinforcement learning,\u201d NeurIPS, 2017. [92] J. Schulman, X. Chen, and P. Abbeel, \u201cEquivalence between policy gradients and soft q-learning,\u201d arXiv preprint arXiv:1704.06440, 2017. [93] F. Fern\u00b4andez and M. Veloso, \u201cProbabilistic policy reuse in a reinforcement learning agent,\u201d Proceedings of the \ufb01fth international joint conference on Autonomous agents and multiagent systems, 2006. [94] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver, \u201cSuccessor features for transfer in reinforcement learning,\u201d NuerIPS, 2017. [95] R. Bellman, \u201cDynamic programming,\u201d Science, 1966. [96] L. Torrey, T. Walker, J. Shavlik, and R. Maclin, \u201cUsing advice to transfer knowledge acquired in one reinforcement learning task to another,\u201d European Conference on Machine Learning, 2005. [97] A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine, \u201cLearning invariant feature spaces to transfer skills with reinforcement learning,\u201d ICLR, 2017. [98] G. Konidaris and A. Barto, \u201cAutonomous shaping: Knowledge transfer in reinforcement learning,\u201d ICML, 2006. [99] H. B. Ammar and M. E. Taylor, \u201cReinforcement learning transfer via common subspaces,\u201d Proceedings of the 11th International Conference on Adaptive and Learning Agents, 2012. [100] V. Badrinarayanan, A. Kendall, and R. Cipolla, \u201cSegnet: A deep convolutional encoder-decoder architecture for image segmentation,\u201d IEEE transactions on pattern analysis and machine intelligence, 2017. [101] C. Wang and S. Mahadevan, \u201cManifold alignment without correspondence,\u201d International Joint Conference on Arti\ufb01cial Intelligence, 2009. [102] B. Bocsi, L. Csat\u00b4o, and J. Peters, \u201cAlignment-based transfer learning for robot models,\u201d The 2013 International Joint Conference on Neural Networks (IJCNN), 2013. [103] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E. Taylor, \u201cUnsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment,\u201d AAAI, 2015. [104] H. B. Ammar, K. Tuyls, M. E. Taylor, K. Driessens, and G. Weiss, \u201cReinforcement learning transfer via sparse coding,\u201d ICAAMS, 2012. [105] A. Lazaric, M. Restelli, and A. Bonarini, \u201cTransfer of samples in batch reinforcement learning,\u201d ICML, 2008. [106] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, \u201cProgressive neural networks,\u201d arXiv preprint arXiv:1606.04671, 2016. [107] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra, \u201cPathnet: Evolution channels gradient descent in super neural networks,\u201d arXiv preprint arXiv:1701.08734, 2017. [108] I. Harvey, \u201cThe microbial genetic algorithm,\u201d European Conference on Arti\ufb01cial Life, 2009. [109] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, \u201cLearning modular neural network policies for multitask and multi-robot transfer,\u201d 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017. [110] J. Andreas, D. Klein, and S. Levine, \u201cModular multitask reinforcement learning with policy sketches,\u201d ICML, 2017. [111] A. Zhang, H. Satija, and J. Pineau, \u201cDecoupling dynamics and reward for transfer learning,\u201d arXiv preprint arXiv:1804.10689, 2018. [112] P. Dayan, \u201cImproving generalization for temporal difference learning: The successor representation,\u201d Neural Computation, 1993. [113] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman, \u201cDeep successor reinforcement learning,\u201d arXiv preprint arXiv:1606.02396, 2016. [114] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard, \u201cDeep reinforcement learning with successor features for navigation across similar environments,\u201d IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017. [115] N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern, \u201cTransfer in variable-reward hierarchical reinforcement learning,\u201d Machine Learning, 2008. [116] R. H. Keshavan, A. Montanari, and S. Oh, \u201cMatrix completion from a few entries,\u201d IEEE transactions on information theory, 2010. [117] D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos, H. van Hasselt, D. Silver, and T. Schaul, \u201cUniversal successor features approximators,\u201d arXiv preprint arXiv:1812.07626, 2018. [118] L. Lehnert, S. Tellex, and M. L. Littman, \u201cAdvantages and limitations of using successor features for transfer in reinforcement learning,\u201d arXiv preprint arXiv:1708.00102, 2017. [119] J. C. Petangoda, S. Pascual-Diaz, V. Adam, P. Vrancx, and J. Grau-Moya, \u201cDisentangled skill embeddings for reinforcement learning,\u201d arXiv preprint arXiv:1906.09223, 2019. [120] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d ICML, 2017. [121] B. Zadrozny, \u201cLearning and evaluating classi\ufb01ers under sample selection bias,\u201d ICML, 2004. [122] J. Kober, J. A. Bagnell, and J. Peters, \u201cReinforcement learning in robotics: A survey,\u201d The International Journal of Robotics Research, 2013. [123] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, \u201cA survey of robot learning from demonstration,\u201d Robotics and autonomous systems, 2009. [124] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, \u201cA survey of research on cloud robotics and automation,\u201d 19 IEEE Transactions on automation science and engineering, 2015. [125] S. Gu, E. Holly, T. Lillicrap, and S. Levine, \u201cDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates,\u201d IEEE international conference on robotics and automation (ICRA), 2017. [126] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, \u201cEpopt: Learning robust neural network policies using model ensembles,\u201d arXiv preprint arXiv:1610.01283, 2016. [127] W. Yu, J. Tan, C. K. Liu, and G. Turk, \u201cPreparing for the unknown: Learning a universal policy with online system identi\ufb01cation,\u201d arXiv preprint arXiv:1702.02453, 2017. [128] F. Sadeghi and S. Levine, \u201cCad2rl: Real single-image \ufb02ight without a single real image,\u201d arXiv preprint arXiv:1611.04201, 2016. [129] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige et al., \u201cUsing simulation and domain adaptation to improve ef\ufb01ciency of deep robotic grasping,\u201d IEEE International Conference on Robotics and Automation (ICRA), 2018. [130] H. Bharadhwaj, Z. Wang, Y. Bengio, and L. Paull, \u201cA data-ef\ufb01cient framework for training and sim-to-real transfer of navigation policies,\u201d International Conference on Robotics and Automation (ICRA), 2019. [131] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner, \u201cDarla: Improving zero-shot transfer in reinforcement learning,\u201d ICML, 2017. [132] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., \u201cMastering the game of go without human knowledge,\u201d Nature, 2017. [133] J. Oh, V. Chockalingam, S. Singh, and H. Lee, \u201cControl of memory, active perception, and action in minecraft,\u201d arXiv preprint arXiv:1605.09128, 2016. [134] S. Ontan\u00b4on, G. Synnaeve, A. Uriarte, F. Richoux, D. Churchill, and M. Preuss, \u201cA survey of real-time strategy game ai research and competition in starcraft,\u201d IEEE Transactions on Computational Intelligence and AI in games, 2013. [135] Z. Lin, J. Gehring, V. Khalidov, and G. Synnaeve, \u201cStardata: A starcraft ai research dataset,\u201d Thirteenth Arti\ufb01cial Intelligence and Interactive Digital Entertainment Conference, 2017. [136] N. Justesen, P. Bontrager, J. Togelius, and S. Risi, \u201cDeep learning for video game playing,\u201d IEEE Transactions on Games, 2019. [137] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, \u201cPlaying atari with deep reinforcement learning,\u201d arXiv preprint arXiv:1312.5602, 2013. [138] OpenAI. (2019) Dotal2 blog. [Online]. Available: https://openai.com/blog/openai-\ufb01ve/ [139] H. Chen, X. Liu, D. Yin, and J. Tang, \u201cA survey on dialogue systems: Recent advances and new frontiers,\u201d Acm Sigkdd Explorations Newsletter, 2017. [140] S. P. Singh, M. J. Kearns, D. J. Litman, and M. A. Walker, \u201cReinforcement learning for spoken dialogue systems,\u201d NeurIPS, 2000. [141] B. Zoph and Q. V. Le, \u201cNeural architecture search with reinforcement learning,\u201d arXiv preprint arXiv:1611.01578, 2016. [142] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko, \u201cLearning to reason: End-to-end module networks for visual question answering,\u201d IEEE International Conference on Computer Vision, 2017. [143] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, \u201cDeep reinforcement learning-based image captioning with embedding reward,\u201d IEEE Conference on Computer Vision and Pattern Recognition, 2017. [144] T. Young, D. Hazarika, S. Poria, and E. Cambria, \u201cRecent trends in deep learning based natural language processing,\u201d IEEE Computational intelligenCe magazine, 2018. [145] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, \u201cLearning to compose neural networks for question answering,\u201d arXiv preprint arXiv:1601.01705, 2016. [146] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio, \u201cAn actorcritic algorithm for sequence prediction,\u201d arXiv preprint arXiv:1607.07086, 2016. [147] F. Godin, A. Kumar, and A. Mittal, \u201cLearning when not to answer: a ternary reward structure for reinforcement learning based question answering,\u201d Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. [148] K.-W. Chang, A. Krishnamurthy, A. Agarwal, J. Langford, and H. Daum\u00b4e III, \u201cLearning to search better than your teacher,\u201d 2015. [149] J. Lu, A. Kannan, J. Yang, D. Parikh, and D. Batra, \u201cBest of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model,\u201d NeurIPS, 2017. [150] E. B. Laber, D. J. Lizotte, M. Qian, W. E. Pelham, and S. A. Murphy, \u201cDynamic treatment regimes: Technical challenges and applications,\u201d Electronic journal of statistics, 2014. [151] M. Tenenbaum, A. Fern, L. Getoor, M. Littman, V. Manasinghka, S. Natarajan, D. Page, J. Shrager, Y. Singer, and P. Tadepalli, \u201cPersonalizing cancer therapy via machine learning,\u201d Workshops of NeurIPS, 2010. [152] A. Alansary, O. Oktay, Y. Li, L. Le Folgoc, B. Hou, G. Vaillant, K. Kamnitsas, A. Vlontzos, B. Glocker, B. Kainz et al., \u201cEvaluating reinforcement learning agents for anatomical landmark detection,\u201d 2019. [153] K. Ma, J. Wang, V. Singh, B. Tamersoy, Y.-J. Chang, A. Wimmer, and T. Chen, \u201cMultimodal image registration with deep context reinforcement learning,\u201d International Conference on Medical Image Computing and Computer-Assisted Intervention, 2017. [154] Z. Huang, W. M. van der Aalst, X. Lu, and H. Duan, \u201cReinforcement learning based resource allocation in business process management,\u201d Data & Knowledge Engineering, 2011. [155] T. S. M. T. Gomes, \u201cReinforcement learning for primary care e appointment scheduling,\u201d 2017. [156] A. Serrano, B. Imbern\u00b4on, H. P\u00b4erez-S\u00b4anchez, J. M. Cecilia, A. Bueno-Crespo, and J. L. Abell\u00b4an, \u201cAccelerating 20 drugs discovery with deep reinforcement learning: An early approach,\u201d International Conference on Parallel Processing Companion, 2018. [157] M. Popova, O. Isayev, and A. Tropsha, \u201cDeep reinforcement learning for de novo drug design,\u201d Science advances, 2018. [158] C. Yu, J. Liu, and S. Nemati, \u201cReinforcement learning in healthcare: A survey,\u201d arXiv preprint arXiv:1908.08796, 2019. [159] A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada, and M. E. Brier, \u201cIncorporating prior knowledge into q-learning for drug delivery individualization,\u201d Fourth International Conference on Machine Learning and Applications, 2005. [160] V. N. Marivate, J. Chemali, E. Brunskill, and M. Littman, \u201cQuantifying uncertainty in batch personalized sequential decision making,\u201d Workshops at the Twenty-Eighth AAAI, 2014. [161] T. W. Killian, S. Daulton, G. Konidaris, and F. DoshiVelez, \u201cRobust and ef\ufb01cient transfer learning with hidden parameter markov decision processes,\u201d NeurIPS, 2017. [162] A. Holzinger, \u201cInteractive machine learning for health informatics: when do we need the human-in-the-loop?\u201d Brain Informatics, 2016. [163] L. Li, Y. Lv, and F.-Y. Wang, \u201cTraf\ufb01c signal timing via deep reinforcement learning,\u201d IEEE/CAA Journal of Automatica Sinica, 2016. [164] K. Lin, R. Zhao, Z. Xu, and J. Zhou, \u201cEf\ufb01cient largescale \ufb02eet management via multi-agent deep reinforcement learning,\u201d ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018. [165] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling, and P. Komisarczuk, \u201cA survey on reinforcement learning models and algorithms for traf\ufb01c signal control,\u201d ACM Computing Surveys (CSUR), 2017. [166] J. Moody, L. Wu, Y. Liao, and M. Saffell, \u201cPerformance functions and reinforcement learning for trading systems and portfolios,\u201d Journal of Forecasting, 1998. [167] Z. Jiang and J. Liang, \u201cCryptocurrency portfolio management with deep reinforcement learning,\u201d IEEE Intelligent Systems Conference (IntelliSys), 2017. [168] R. Neuneier, \u201cEnhancing q-learning for optimal asset allocation,\u201d NeurIPS, 1998. [169] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, \u201cDeep direct reinforcement learning for \ufb01nancial signal representation and trading,\u201d IEEE transactions on neural networks and learning systems, 2016. [170] G. Dalal, E. Gilboa, and S. Mannor, \u201cHierarchical decision making in electricity grid management,\u201d International Conference on Machine Learning, 2016. [171] F. Ruelens, B. J. Claessens, S. Vandael, B. De Schutter, R. Babu\u02c7ska, and R. Belmans, \u201cResidential demand response of thermostatically controlled loads using batch reinforcement learning,\u201d IEEE Transactions on Smart Grid, 2016. [172] Z. Wen, D. O\u2019Neill, and H. Maei, \u201cOptimal demand response using device-based reinforcement learning,\u201d IEEE Transactions on Smart Grid, 2015. [173] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal, \u201cExplaining explanations: An overview of interpretability of machine learning,\u201d IEEE 5th International Conference on data science and advanced analytics (DSAA), 2018. [174] Q.-s. Zhang and S.-C. Zhu, \u201cVisual interpretability for deep learning: a survey,\u201d Frontiers of Information Technology & Electronic Engineering, 2018. [175] Y. Li, J. Song, and S. Ermon, \u201cInfogail: Interpretable imitation learning from visual demonstrations,\u201d NeurIPS, 2017. [176] R. Ramakrishnan and J. Shah, \u201cTowards interpretable explanations for transfer learning in sequential tasks,\u201d AAAI Spring Symposium Series, 2016. Zhuangdi Zhu received her bachelor\u2019s degree in Computer Science from the College of Elite Education at Nanjing University of Science and Technology, in 2015. She is currently a Ph.D. candidate at the Computer Science department of Michigan State University, MI, USA. Her current research interests reside in the general area of Machine Learning and its practical applications. Kaixiang Lin is an applied scientist at Amazon web services. He obtained his Ph.D. from Michigan State University. He has broad research interests across multiple \ufb01elds, including reinforcement learning, human-robot interactions, and natural language processing. His research has been published on multiple top-tiered machine learning and data mining conferences such as ICLR, KDD, NeurIPS, etc. He serves as a reviewer for top machine learning conferences regularly. Anil K. Jain is a University distinguished professor in the Department of Computer Science and Engineering at Michigan State University. His research interests include pattern recognition and biometric authentication. He served as the editorin-chief of the IEEE Transactions on Pattern Analysis and Machine Intelligence and was a member of the United States Defense Science Board. He has received Fulbright, Guggenheim, Alexander von Humboldt, and IAPR King Sun Fu awards. He is a member of the National Academy of Engineering and a foreign fellow of the Indian National Academy of Engineering and the Chinese Academy of Sciences. Jiayu Zhou is currently an associate professor in the Department of Computer Science and Engineering at Michigan State University. He received his Ph.D. degree in computer science from Arizona State University in 2014. He has broad research interests in the \ufb01elds of largescale machine learning and data mining as well as biomedical informatics. He has served as a technical program committee member for premier conferences such as NIPS, ICML, and SIGKDD. His papers have received the Best Student Paper Award at the 2014 IEEE International Conference on Data Mining (ICDM), the Best Student Paper Award at the 2016 International Symposium on Biomedical Imaging (ISBI) and the Best Paper Award at IEEE Big Data 2016. ",
    "title": "Transfer Learning in Deep Reinforcement",
    "paper_info": "1\nTransfer Learning in Deep Reinforcement\nLearning: A Survey\nZhuangdi Zhu, Kaixiang Lin, Anil K. Jain, and Jiayu Zhou\nAbstract\u2014Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed\nremarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of\nreinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges\nfaced by reinforcement learning, by transferring knowledge from external expertise to facilitate the ef\ufb01ciency and effectiveness of the\nlearning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep\nreinforcement learning. Speci\ufb01cally, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under\nwhich we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw\nconnections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential\nchallenges that await future research progress.\nIndex Terms\u2014Transfer Learning, Reinforcement Learning, Deep Learning, Survey.\n!\n1\nINTRODUCTION\nR\nEinforcement Learning (RL) is an effective framework\nto solve sequential decision-making tasks, where a\nlearning agent interacts with the environment to improve\nits performance through trial and error [1]. Originated\nfrom cybernetics and thriving in computer science, RL\nhas been widely applied to tackle challenging tasks which\nwere previously intractable. Traditional RL algorithms were\nmostly designed for tabular cases, which provide principled\nsolutions to simple tasks but face dif\ufb01culties when handling\nhighly complex domains, e.g. tasks with 3D environments.\nWith the recent advances in computing capability and deep\nlearning research, the combination of RL agents and deep\nneural networks is developed to address challenging tasks.\nThe combination of deep learning with RL is hence referred to\nas Deep Reinforcement Learning [2], which learns powerful\nfunction approximators using deep neural networks to ad-\ndress complicated domains. RL powered with deep learning\nhas achieved notable success in applications such as robotics\ncontrol [3, 4] and game playing [5]. It also has a promising\nprospects in domains such as health informatics [6], electric-\nity networks [7], intelligent transportation systems[8, 9], to\nname just a few.\nBesides its remarkable advancement, RL still faces in-\ntriguing dif\ufb01culties induced by the exploration-exploitation\ndilemma [1]. Speci\ufb01cally, for practical RL problems, the\nenvironment dynamics are usually unknown, and the agent\ncannot exploit knowledge about the environment to improve\nits performance until enough interaction experiences are\ncollected via exploration. Due to the partial observability,\nsparse feedbacks, and the high complexity of state and\naction spaces, acquiring suf\ufb01cient interaction samples can\nbe prohibitive, which may even incur safety concerns for\n\u2022\nZhuangdi Zhu, Anil K. Jain, and Jiayu Zhou are with the Department\nof Computer Science and Engineering, Michigan State University, East\nLansing, MI, 48824. E-mail: {zhuzhuan, jain, jiayuz}@msu.edu\n\u2022\nKaixiang Lin is with the Amazon Alexa AI. E-mail: lkxcarson@gmail.com\ndomains such as automatic-driving and health informatics,\nwhere the consequences of wrong decisions can be too high\nto take. The abovementioned challenges have motivated\nvarious efforts to improve the current RL procedure. As a re-\nsult, transfer learning, or equivalently referred as knowledge\ntransfer, which is a technique to utilize external expertise\nfrom other domains to bene\ufb01t the learning process of the\ntarget task, becomes a crucial topic in RL.\nWhile transfer learning techniques have been exten-\nsively studied in the supervised learning domain [10], it is\nstill an emerging topic for RL. Transfer learning can be\nmore complicated under the RL framework, in that the\nknowledge needs to transfer in the context of a Markov\nDecision Process. Moreover, due to the delicate components\nof the Markov decision process, expert knowledge may\ntake different forms, which need to transfer in different\nways. Noticing that previous efforts on summarizing transfer\nlearning for the RL domain have not covered the most\nrecent advancement [11, 12], in this survey, we make a\ncomprehensive investigation of Transfer Learning in Deep\nReinforcement Learning. Especially, we propose a systematic\nframework to categorize the state-of-the-art transfer learning\ntechniques into different sub-topics, review their theories\nand applications, and analyze their inter-connections.\nThe rest of this survey is organized as follows: In section 2,\nwe introduce the preliminaries of RL and its key algorithms,\nincluding those recently designed based on deep neural\nnetworks. Next, we clarify the de\ufb01nition of transfer learning\nin the context of RL and discuss its relevant research\ntopics (Section 2.4). In Section 3, we provide a framework\nto categorize transfer learning approaches from multiple\nperspectives, analyze their fundamental differences, and\nsummarize their evaluation metrics (Section 3.3). In Section\n4, we elaborate on different transfer learning approaches\nin the context of deep RL, organized by the format of\ntransferred knowledge, such as reward shaping (Section 4.1),\nlearning from demonstrations (Section 4.2), or learning from\narXiv:2009.07888v5  [cs.LG]  16 May 2022\n",
    "GPTsummary": "- (1): The study aims to explore transfer learning approaches in deep reinforcement learning and their potential applications, challenges, and connections with other relevant topics.\n \n- (2): The traditional RL algorithms were mostly designed for tabular cases, which provide principled solutions to simple tasks but face difficulties handling highly complex domains. Transfer learning, which utilizes external expertise from other domains to facilitate the efficiency and effectiveness of the learning process, becomes a crucial topic in RL. Previous efforts on summarizing transfer learning for the RL domain have not covered recent advancements, and the knowledge needs to transfer in the context of a Markov Decision Process. The approach is well motivated since the challenges have motivated various efforts to improve the current RL procedure.\n\n- (3): The study systematically investigates the recent progress of transfer learning approaches in the context of deep reinforcement learning. The authors provide a framework to categorize the state-of-the-art transfer learning approaches, analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. They also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.\n\n- (4): The authors elaborate on different transfer learning approaches in the context of deep RL, such as reward shaping, learning from demonstrations, or learning from teacher policies. They also investigate transfer learning approaches by the way that knowledge transfer occurs, such as inter-task mapping, or learning transferrable representations, etc. The study discusses recent applications of transfer learning in the context of deep RL and provides some future perspectives and open questions. Theoretically, the methodology proposed in this paper can facilitate knowledge transfer in deep RL, but the performance depends on the specific task and application.\n7. Methods:\n\n- (1): The study aims to systematically investigate and categorize transfer learning approaches in the context of deep reinforcement learning, as well as analyzing their goals, methodologies, compatible reinforcement learning backbones, and practical applications. The authors provide an overview of different approaches, such as reward shaping, learning from demonstrations, or learning from teacher policies, and investigate transfer learning approaches by the way that knowledge transfer occurs, such as inter-task mapping, or learning transferrable representations.\n\n- (2): The authors draw connections between transfer learning and other relevant topics in reinforcement learning and explore potential challenges that require future research progress. They also discuss recent applications of transfer learning in the context of deep reinforcement learning and provide some future perspectives and open questions.\n\n- (3): The study provides a feasible framework for transferring knowledge as the augmented reward and is generally applicable to various RL algorithms. How to integrate reward shaping with other transfer learning approaches, such as learning from demonstrations and policy transfer, will be an intriguing question for future research. Additionally, the methodology proposed in this paper can facilitate knowledge transfer in deep RL, but the performance depends on the specific task and application.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This article provides a valuable survey on transfer learning in deep reinforcement learning, exploring the potential applications, challenges, and connections with other relevant topics. The study aims to facilitate knowledge transfer in deep reinforcement learning and can inspire future research progress in this field.\n\n- (2): Innovation point: The paper systematically investigates recent progress in transfer learning approaches in the context of deep reinforcement learning, providing a framework to categorize the state-of-the-art transfer learning approaches and analyzing their goals, methodologies, compatible reinforcement learning backbones, and practical applications. Performance: The approach proposed in this paper can facilitate knowledge transfer in deep reinforcement learning, and the authors discuss recent applications of transfer learning and provide some future perspectives and open questions. Workload: The study is comprehensive but may require prior knowledge in reinforcement learning, deep learning, and transfer learning.\n\n\n",
    "GPTmethods": "- (1): The study aims to systematically investigate and categorize transfer learning approaches in the context of deep reinforcement learning, as well as analyzing their goals, methodologies, compatible reinforcement learning backbones, and practical applications. The authors provide an overview of different approaches, such as reward shaping, learning from demonstrations, or learning from teacher policies, and investigate transfer learning approaches by the way that knowledge transfer occurs, such as inter-task mapping, or learning transferrable representations.\n\n- (2): The authors draw connections between transfer learning and other relevant topics in reinforcement learning and explore potential challenges that require future research progress. They also discuss recent applications of transfer learning in the context of deep reinforcement learning and provide some future perspectives and open questions.\n\n- (3): The study provides a feasible framework for transferring knowledge as the augmented reward and is generally applicable to various RL algorithms. How to integrate reward shaping with other transfer learning approaches, such as learning from demonstrations and policy transfer, will be an intriguing question for future research. Additionally, the methodology proposed in this paper can facilitate knowledge transfer in deep RL, but the performance depends on the specific task and application.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This article provides a valuable survey on transfer learning in deep reinforcement learning, exploring the potential applications, challenges, and connections with other relevant topics. The study aims to facilitate knowledge transfer in deep reinforcement learning and can inspire future research progress in this field.\n\n- (2): Innovation point: The paper systematically investigates recent progress in transfer learning approaches in the context of deep reinforcement learning, providing a framework to categorize the state-of-the-art transfer learning approaches and analyzing their goals, methodologies, compatible reinforcement learning backbones, and practical applications. Performance: The approach proposed in this paper can facilitate knowledge transfer in deep reinforcement learning, and the authors discuss recent applications of transfer learning and provide some future perspectives and open questions. Workload: The study is comprehensive but may require prior knowledge in reinforcement learning, deep learning, and transfer learning.\n\n\n",
    "GPTconclusion": "- (1): This article provides a valuable survey on transfer learning in deep reinforcement learning, exploring the potential applications, challenges, and connections with other relevant topics. The study aims to facilitate knowledge transfer in deep reinforcement learning and can inspire future research progress in this field.\n\n- (2): Innovation point: The paper systematically investigates recent progress in transfer learning approaches in the context of deep reinforcement learning, providing a framework to categorize the state-of-the-art transfer learning approaches and analyzing their goals, methodologies, compatible reinforcement learning backbones, and practical applications. Performance: The approach proposed in this paper can facilitate knowledge transfer in deep reinforcement learning, and the authors discuss recent applications of transfer learning and provide some future perspectives and open questions. Workload: The study is comprehensive but may require prior knowledge in reinforcement learning, deep learning, and transfer learning.\n\n\n"
}