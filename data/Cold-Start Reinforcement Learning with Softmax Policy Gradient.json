{
    "Abstract": "Abstract Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the ef\ufb01ciency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks. 1 ",
    "Introduction": "Introduction Reinforcement learning is the study of optimal sequential decision-making in an environment [16]. Its recent developments underpin a large variety of applications related to robotics [11, 5] and games [20]. Policy search in reinforcement learning refers to the search for optimal parameters for a given policy parameterization [5]. Policy search based on policy-gradient [26, 21] has been recently applied to structured output prediction for sequence generations. These methods alleviate two common problems that approaches based on training with the Maximum-likelihood Estimation (MLE) objective exhibit, namely the exposure-bias problem [24, 19] and the wrong-objective problem [19, 15] (more on this in Section 2). As a result of addressing these problems, policy-gradient methods achieve improved performance compared to MLE training in various tasks, including machine translation [19, 7], text summarization [19], and image captioning [19, 15]. Policy-gradient methods for sequence generation work as follows: \ufb01rst the model proposes a sequence, and the ground-truth target is used to compute a reward for the proposed sequence with respect to the reward of choice (using metrics known to correlate well with human-rated correctness, such as ROUGE [13] for summarization, BLEU [18] for machine translation, CIDEr [23] or SPICE [1] for image captioning, etc.). The reward is used as a weight for the log-likelihood of the proposed sequence, and learning is done by optimizing the weighted average of the log-likelihood of the proposed sequences. The policy-gradient approach works around the dif\ufb01culty of differentiating the reward function (the majority of which are non-differentiable) by using it as a weight. However, since sequences proposed by the model are also used as the target of the model, they are very noisy and their initial quality is extremely poor. The dif\ufb01culty of aligning the model output distribution with the reward distribution over the large search space of possible sequences makes training slow and inef\ufb01cient\u2217. As a result, overhead procedures such as warm-start training with the MLE objective and sophisticated methods for sample variance reduction are required to train with policy gradient. \u2217Search space size is O(V T ), where V is the number of word types in the vocabulary (typically between 104 and 106) and T is the the sequence length (typically between 10 and 50), hence between 1040 and 10300. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1709.09346v2  [cs.LG]  13 Oct 2017 The fundamental reason for the inef\ufb01ciency of policy-gradient\u2013based reinforcement learning is the large discrepancy between the model-output distribution and the reward distribution, especially in the early stages of training. If, instead of generating the target based solely on the model-output distribution, we generate it based on a proposal distribution that incorporates both the model-output distribution and the reward distribution, learning would be ef\ufb01cient, and neither warm-start training nor sample variance reduction would be needed. The outstanding problem is \ufb01nding a value function that induces such a proposal distribution. In this paper, we describe precisely such a value function, which in turn gives us a Softmax Policy Gradient (SPG) method. The softmax terminology comes from the equation that de\ufb01nes this value function, see Section 3. The gradient of the softmax value function is equal to the average of the gradient of the log-likelihood of the targets whose proposal distribution combines both model output distribution and reward distribution. Although this distribution is infeasible to sample exactly, we show that one can draw samples approximately, based on an ef\ufb01cient forward-pass sampling scheme. To balance the importance between the model output distribution and the reward distribution, we use a bang-bang [8] mixture model to combine the two distributions. Such a scheme removes the need of \ufb01ne-tuning the weights across different datasets and throughout the learning epochs. In addition to using a main metric as the task reward (ROUGE, CIDEr, etc.), we show that one can also incorporate additional, task-speci\ufb01c metrics to enforce various properties on the output sequences (Section 4). We numerically evaluate our method on two sequence generation benchmarks, a headline-generation task and an image-caption\u2013generation task (Section 5). In both cases, the SPG method signi\ufb01cantly improves the accuracy, compared to maximum-likelihood and other competing methods. Finally, it is worth noting that although the training and inference of the SPG method in the paper is mainly based on sequence learning, the idea can be extended to other reinforcement learning applications. 2 Limitations of Existing Sequence Learning Regimes One of the standard approaches to sequence-learning training is Maximum-likelihood Estimation (MLE). Given a set of inputs X = \ufffd xi\ufffd and target sequences Y = \ufffd yi\ufffd , the MLE loss function is: LMLE(\u03b8) = \ufffd i Li MLE(\u03b8), where Li MLE(\u03b8) = \u2212 log p\u03b8(yi|xi). (1) Here xi and yi = \ufffd yi 1, . . . , yi T \ufffd denote the input and the target sequence of the i-th example, respectively. For instance, in the image captioning task, xi is the image of the i-th example, and yi is the groundtruth caption of the i-th example. Although widely used in many different applications, MLE estimation for sequence learning suffers from the exposure-bias problem [24, 19]. Exposure-bias refers to training procedures that produce brittle models that have only been exposed to their training data distribution but not to their own predictions. At training-time, log p\u03b8(yi|xi) = \ufffd t log p\u03b8(yi t|xi, yi 1...t\u22121), i.e. the loss of the t-th word is conditional on the true previous-target tokens yi 1...t\u22121. However, since yi 1...t\u22121 are unavailable during inference, replacing them with tokens zi 1...t\u22121 generated by p\u03b8(zi 1...t\u22121|xi) yields a signi\ufb01cant discrepancy between how the model is used at training time versus inference time. The exposure-bias problem has recently received attention in neural-network settings with the \u201cdata as demonstrator\u201d [24] and \u201cscheduled sampling\u201d [3] approaches. Although improving model performance in practice, such proposals have been shown to be statistically inconsistent [10], and still need to perform MLE-based warm-start training. A more general approach to MLE is the Reward Augmented Maximum Likelihood (RAML) method [17]. RAML makes the correct observation that, under MLE, all alternative outputs are equally penalized through normalization, regardless of their relationship to the ground-truth target. Instead, RAML corrects for this shortcoming using an objective of the form: Li RAML(\u03b8) = \u2212 \ufffd zi rR(zi|yi) log p\u03b8(zi|xi). (2) where rR(zi|yi) = exp(R(zi|yi)/\u03c4) \ufffd zi exp(R(zi|yi)/\u03c4). This formulation uses R(zi|yi) to denote the value of a similarity metric R between zi and yi (the reward), with yi = argmaxzi R(zi|yi); \u03c4 is a temperature hyper-parameter to control the peakiness of this reward distribution. Since the sum over all zi for 2 the reward distribution rR(zi|yi) in Eq. (2) is infeasible to compute, a standard approach is to draw J samples zij from the reward distribution, and approximate the expectation by Monte Carlo integration: Li RAML(\u03b8) \u2243 \u2212 1 J J \ufffd j=1 log p\u03b8(zij|xi). (3) Although a clear improvement over Eq. (1), the sampling for zij in Eq. (3) is solely based on rR(zi|yi) and completely ignores the model probability. At the same time, this technique does not address the exposure bias problem at all. A different approach, based on reinforcement learning methods, achieves sequence learning following a policy-gradient method [21]. Its appeal is that it not only solves the exposure-bias problem, but also directly alleviates the wrong-objective problem [19, 15] of MLE approaches. Wrong-objective refers to the critique that MLE-trained models tend to have suboptimal performance because such models are trained on a convenient objective (i.e., maximum likelihood) rather than a desirable objective (e.g., a metric known to correlate well with human-rated correctness). The policy-gradient method uses a value function VP G, which is equivalent to a loss LP G de\ufb01ned as: Li P G(\u03b8) = \u2212V i P G(\u03b8), V i P G(\u03b8) = Ep\u03b8(zi|xi)[R(zi|yi)]. (4) The gradient for Eq. (4) is: \u2202 \u2202\u03b8Li P G(\u03b8) = \u2212 \ufffd zi p\u03b8(zi|xi)R(zi|yi) \u2202 \u2202\u03b8 log p\u03b8(zi|xi). (5) Similar to (3), one can draw J samples zij from p\u03b8(zi|xi) to approximate the expectation by MonteCarlo integration: \u2202 \u2202\u03b8Li P G(\u03b8) \u2243 \u2212 1 J J \ufffd j=1 R(zij|yi) \u2202 \u2202\u03b8 log p\u03b8(zij|xi). (6) However, the large discrepancy between the model prediction distribution p\u03b8(zi|xi) and the reward R(zi|yi)\u2019s values, which is especially acute during the early training stages, makes the Monte-Carlo integration extremely inef\ufb01cient. As a result, this method also requires a warm-start phase in which the model distribution achieves some local maximum with respect to a reward-metric\u2013free objective (e.g., MLE), followed by a model re\ufb01nement phase in which reward-metric\u2013based PG updates are used to re\ufb01ne the model [19, 7, 15]. Although this combination achieves better results in practice compared to pure likelihood-based approaches, it is unsatisfactory from a theoretical and modeling perspective, as well as inef\ufb01cient from a speed-to-convergence perspective. Both these issues are addressed by the value function we describe next. 3 Softmax Policy Gradient (SPG) Method In order to smoothly incorporate both the model distribution p\u03b8(zi|xi) and the reward metric R(zi|yi), we replace the value function from Eq. 4 with a Softmax value function for Policy Gradient (SPG), VSP G, equivalent to a loss LSP G de\ufb01ned as: Li SP G(\u03b8) = \u2212V i SP G(\u03b8), V i SP G(\u03b8) = log \ufffd Ep\u03b8(zi|xi)[exp(R(zi|yi))] \ufffd . (7) Because the value function for example i is equal to Softmaxzi(log p\u03b8(zi|xi) + R(zi|yi)), where Softmaxzi(\u00b7) = log \ufffd zi exp(\u00b7), we call it the softmax value function. Note that the softmax value function from Eq. (7) is the dual of the entropy-regularized policy search (REPS) objective [5, 16] L(q) = Eq[R] + KL(q|p\u03b8). However, our learning and sampling procedures are signi\ufb01cantly different from REPS, as shown in what follows. The gradient for Eq. (7) is: \u2202 \u2202\u03b8Li SP G(\u03b8) = \u2212 1 \ufffd zi p\u03b8(zi|xi) exp(R(zi|yi)) \ufffd\ufffd zi p\u03b8(zi|xi) exp(R(zi|yi)) \u2202 \u2202\u03b8 log p\u03b8(zi|xi) \ufffd = \u2212 \ufffd zi q\u03b8(zi|xi, yi) \u2202 \u2202\u03b8 log p\u03b8(zi|xi) (8) where q\u03b8(zi|xi, yi) = 1 \ufffd zi p\u03b8(zi|xi) exp(R(zi|yi))p\u03b8(zi|xi) exp(R(zi|yi)). 3 MLE target RAML targets PG targets SPG targets \ufffd\ufffd \ufffd\ufffd rR Figure 1: Comparing the target samples for MLE, RAML (the rR distribution), PG (the p\u03b8 distribution), and SPG (the q\u03b8 distribution). There are several advantages associated with the gradient from Eq. (8). First, q\u03b8(zi|xi, yi) takes into account both p\u03b8(zi|xi) and R(zi|yi). As a result, Monte Carlo integration over q\u03b8-samples approximates Eq. (8) better, and has smaller variance compared to Eq. (5). This allows our model to start learning from scratch without the warm-start and variance-reduction crutches needed by previously-proposed PG approaches. Second, as Figure 1 shows, the samples for the SPG method (pentagons) lie between the ground-truth target distribution (triangle and circles) and the model distribution (squares). These targets are both easier to learn by p\u03b8 compared to ground-truth\u2013only targets like the ones for MLE (triangle) and RAML (circles), and also carry more information about the ground-truth target compared to model-only samples (PG squares). This formulation allows us to directly address the exposure-bias problem, by allowing the model distribution to learn at training time how to deal with events conditioned on model-generated tokens, similar with what happens at inference time (more on this in Section 3.2). At the same time, the updates used for learning rely heavily on the in\ufb02uence of the reward metric R(zi|yi), therefore directly addressing the wrong-objective problem. Together, these properties allow the model to achieve improved accuracy. Third, although q\u03b8 is infeasible for exact sampling, since both p\u03b8(zi|xi) and exp(R(zi|yi)) are factorizable across zi t (where zi t denotes the t-th word of the i-th output sequence), we can apply ef\ufb01cient approximate inference for the SPG method as shown in the next section. 3.1 Inference In order to estimate the gradient from Eq. (8) with Monte-Carlo integration, one needs to be able to draw samples from q\u03b8(zi|xi, yi). To tackle this problem, we \ufb01rst decompose R(zi|yi) along the t-axis: R(zi|yi) = T \ufffd t=1 R(zi 1:t|yi) \u2212 R(zi 1:t\u22121|yi) \ufffd \ufffd\ufffd \ufffd \u225c\u2206ri t(zi t|yi,zi 1:t\u22121) , where R(zi 1:t|yi) \u2212 R(zi 1:t\u22121|yi) characterizes the reward increment for zi t. Using the reward increment notation, we can rewrite: q\u03b8(zi|xi, yi) = 1 Z\u03b8(xi, yi) T \ufffd t=1 exp(log p\u03b8(zi t|zi 1:t\u22121, xi) + \u2206ri t(zi t|yi, zi 1:t\u22121)) where Z\u03b8(xi, yi) is the partition function equal to the sum over all con\ufb01gurations of zi. Since the number of such con\ufb01gurations grows exponentially with respect to the sequence-length T, directly drawing from q\u03b8(zi|xi, yi) is infeasible. To make the inference ef\ufb01cient, we replace q\u03b8(zi|xi, yi) with the following approximate distribution: \u02dcq\u03b8(zi|xi, yi) = T \ufffd t=1 \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121), where \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121) = 1 \u02dcZ\u03b8(xi, yi, zi 1:t\u22121) exp(log p\u03b8(zi t|zi 1:t\u22121, xi) + \u2206ri t(zi t|yi, zi 1:t\u22121)). By replacing q\u03b8 in Eq. (8) with \u02dcq\u03b8, we obtain: \u2202 \u2202\u03b8Li SP G(\u03b8) = \u2212 \ufffd zi q\u03b8(zi|xi, yi) \u2202 \u2202\u03b8 log p\u03b8(zi|xi) \u2243 \u2212 \ufffd zi \u02dcq\u03b8(zi|xi, yi) \u2202 \u2202\u03b8 log p\u03b8(zi|xi) \u225c \u2202 \u2202\u03b8 \u02dcLi SP G(\u03b8) (9) 4 Compared to Z\u03b8(xi, yi), \u02dcZ\u03b8(xi, yi, zi 1:t\u22121) sums over the con\ufb01gurations of one zi t only. Therefore, the cost of drawing one zi from \u02dcq\u03b8(zi|xi, yi) grows only linearly with respect to T. Furthermore, for common reward metrics such as ROUGE and CIDEr, the computation of \u2206ri t(zi t|yi, zi 1:t\u22121) can be done in O(T) instead of O(V ) (where V is the size of the state space for zi t, i.e., vocabulary size). That is because the maximum number of unique words in yi is T, and any words not in yi have the same reward increment. When we limit ourselves to J = 1 sample for each example in Eq. (9), the approximate SPG inference time of each example is similar to the inference time for the gradient of the MLE objective. Combined with the empirical \ufb01ndings in Section 5 (Figure 3) where the steps for convergence are comparable, we conclude that the time for convergence for the SPG method is similar to the MLE based method. 3.2 Bang-bang Rewarded SPG Method One additional dif\ufb01culty for the SPG method is that the model\u2019s log-probability values log p\u03b8(zi t|zi 1:t\u22121, xi) and the reward-increment values R(zi 1:t|yi) \u2212 R(zi 1:t\u22121|yi) are not on the same scale. In order to balance the impact of these two factors, we need to weigh them appropriately. Formally, we achieve this by adding a weight wi t to the reward increments: \u2206ri t(zi t|yi, zi 1:t\u22121, wi t) \u225c wi t\u00b7\u2206ri t(zi t|yi, zi 1:t\u22121) so that the total reward R(zi|yi, wi) = \ufffdT t=1 \u2206ri t(zi t|yi, zi 1:t\u22121, wi t). The approximate proposal distribution becomes \u02dcq\u03b8(zi|xi, yi, wi) = \ufffdT t=1 \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121, wi t), where \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121, wi t) \u221d exp(log p\u03b8(zi t|zi 1:t\u22121, xi) + \u2206ri t(zi t|yi, zi 1:t\u22121, wi t)). The challenge in this case is to choose an appropriate weight wi t, because log p\u03b8(zi t|zi 1:t\u22121, xi) varies heavily for different i, t, as well as across different iterations and tasks. In order to minimize the efforts for \ufb01ne-tuning the reward weights, we propose a bang-bang rewarded softmax value function, equivalent to a loss LBBSP G de\ufb01ned as: Li BBSP G(\u03b8) = \u2212 \ufffd wi p(wi) log \ufffd Ep\u03b8(zi|xi)[exp(R(zi|yi, wi))] \ufffd , (10) and \u2202 \u2202\u03b8 \u02dcLi BBSP G(\u03b8) = \u2212 \ufffd wi p(wi) \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi|xi) \ufffd \ufffd\ufffd \ufffd \u225c\u2212 \u2202 \u2202\u03b8 \u02dcLi SP G(\u03b8|wi) , (11) where p(wi) = \ufffd t p(wi t) and p(wi t = 0) = pdrop = 1 \u2212 p(wi t = W). Here W is a suf\ufb01ciently large number (e.g., 10,000), pdrop is a hyper-parameter in [0, 1]. The name bang-bang is borrowed from control theory [8], and refers to a system which switches abruptly between two extreme states (namely W and 0). t 1 2 3 4 5 6 7 yt a man is sitting in the park W W W 0 W ... ... wt zt a man is in the ... ... argmax \ufffdr5(z5|y, z1:4) = \u2018the\u2019 \u2260 y5 = \u2018in\u2019 Figure 2: An example of sequence generation with the bang-bang reward weights. z4 = \u201din\u201d is sampled from the model distribution since w4 = 0. Although w5 = W, z5 = \u201dthe\u201d \u0338= y5 because z4 = \u201din\u201d. When wi t = W, the term \u2206ri t(zi t|yi, zi 1:t\u22121, wi t) overwhelms log p\u03b8(zi t|zi 1:t\u22121, xi), so the sampling of zi t is decided by the reward increment of zi t. It is important to emphasize that in general the groundtruth label yi t \u0338= argmaxzi t \u2206ri t(zi t|yi, zi 1:t\u22121), because zi 1:t\u22121 may not be the same as yi 1:t\u22121 (see an example in Figure 2). The only special case is when pdrop = 0, which forces wi t to always equal W, and implies zi t is always equal\u2020 to yi t (and therefore the SPG method reduces to the MLE method). On the other hand, when wi t = 0, by de\ufb01nition \u2206ri t(zi t|yi, zi 1:t\u22121, wi t) = 0. In this case, the sampling of zi t is based only on the model prediction distribution p\u03b8(zi t|zi 1:t\u22121, xi), the same situation we have at inference time. Furthermore, we have the following lemma (with the proof provided in the Supplementary Material): \u2020This follows from recursively applying R\u2019s property that yi t = argmaxzi t \u2206ri t(zi t|yi, zi 1:t\u22121 = yi 1:t\u22121). 5 Lemma 1 When wi t = 0, \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = 0. As a result, \u2202 \u2202\u03b8 \u02dcLi SP G(\u03b8|wi) is very different from traditional PG-method gradients, in that only the zi t with wi t \u0338= 0 are included. To see that, using the fact that log p\u03b8(zi|xi) = \ufffdT t=1 log p\u03b8(zi t|xi, zi 1:t\u22121), \u2202 \u2202\u03b8 \u02dcLi SP G(\u03b8|wi) = \u2212 \ufffd t \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121), (12) Using the result of Lemma 1, Eq. (12) is equal to: \u2202 \u2202\u03b8 \u02dcLi SP G(\u03b8|wi) = \u2212 \ufffd {t:wi t\u0338=0} \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = \u2212 \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \ufffd {t:wi t\u0338=0} \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) (13) Using Monte-Carlo integration, we approximate Eq. (11) by \ufb01rst drawing wij from p(wi) and then iteratively drawing zij t from \u02dcq\u03b8(zi t|xi, zi 1:t\u22121, yi, wij t ) for t = 1, . . . , T. For larger values of pdrop, the wij sample contains more wij t = 0 and the resulting zij contains proportionally more samples from the model prediction distribution (with a direct effect on alleviating the exposure-bias problem). After zij is obtained, only the log-likelihood of zij t when wij t \u0338= 0 are included in the loss: \u2202 \u2202\u03b8 \u02dcLi BBSP G(\u03b8) \u2243 \u2212 1 J J \ufffd j=1 \ufffd \ufffd t:w ij t \u0338=0 \ufffd \u2202 \u2202\u03b8 log p\u03b8(zij t |xi, zij 1:t\u22121). (14) The details about the gradient evaluation for the bang-bang rewarded softmax value function are described in Algorithm 1 of the Supplementary Material. 4 Additional Reward Functions Besides the main reward function R(zi|yi), additional reward functions can be used to enforce desirable properties for the output sequences. For instance, in summarization, we occasionally \ufb01nd that the decoded output sequence contains repeated words, e.g. \"US R&B singer Marie Marie Marie Marie ...\". In this framework, this can be directly \ufb01xed by using an additional auxiliary reward function that simply rewards negatively two consecutive tokens in the generated sequence: DUPi t = \ufffd\u22121 if zi t = zi t\u22121, 0 otherwise. In conjunction with the bang-bang weight scheme, the introduction of such a reward function has the immediate effect of severely penalizing such \u201cstuttering\u201d in the model output; the decoded sequence after applying the DUP negative reward becomes: \"US R&B singer Marie Christina has ...\". Additionally, we can use the same approach to correct for certain biases in the forward sampling approximation. For example, the following function negatively rewards the end-of-sentence symbol when the length of the output sequence is less than that of the ground-truth target sequence |yi|: EOSi t = \ufffd\u22121 if zi t = </S> and t < |yi|, 0 otherwise. A more detailed discussion about such reward functions is available in the Supplementary Material. During training, we linearly combine the main reward function with the auxiliary functions: \u2206ri t(zi t|yi, zi 1:t\u22121, wi t) = wi t \u00b7 \ufffd R(zi 1:t|yi) \u2212 R(zi 1:t\u22121|yi) + DUPi t + EOSi t \ufffd , with W = 10, 000. During testing, since the ground-truth target yi is unavailable, this becomes: \u2206ri t(zi t|yi, zi 1:t\u22121, W) = W \u00b7 DUPi t. 6 5 Experiments We numerically evaluate the proposed softmax policy gradient (SPG) method on two sequence generation benchmarks: a document-summarization task for headline generation, and an automatic image-captioning task. We compare the results of the SPG method against the standard maximum likelihood estimation (MLE) method, as well as the reward augmented maximum likelihood (RAML) method [17]. Our experiments indicate that the SPG method outperforms signi\ufb01cantly the other approaches on both the summarization and image-captioning tasks. We implemented all the algorithms using TensorFlow 1.0 [6]. For the RAML method, we used \u03c4 = 0.85 which was the best performer in [17]. For the SPG algorithm, all the results were obtained using a variant of ROUGE [13] as the main reward metric R, and J = 1 (sample one target for each example, see Eq. (14)). We report the impact of the pdrop for values in {0.2, 0.4, 0.6, 0.8}. In addition to using the main reward-metric for sampling targets, we also used it to weight the loss for target zij, as we found that it improved the performance of the SPG algorithm. We also applied a naive version of the policy gradient (PG) algorithm (without any variance reduction) by setting pdrop = 0.0, W \u2192 0, but failed to train any meaningful model with cold-start. When starting from a pre-trained MLE checkpoint, we found that it was unable to improve the original MLE result. This result con\ufb01rms that variance-reduction is a requirement for the PG method to work, whereas our SPG method is free of such requirements. 5.1 Summarization Task: Headline Generation Headline generation is a standard text generation task, taking as input a document and generating a concise summary/headline for it. In our experiments, the supervised data comes from the English Gigaword [9], and consists of news-articles paired with their headlines. We use a training set of about 6 million article-headline pairs, in addition to two randomly-extracted validation and evaluation sets of 10K examples each. In addition to the Gigaword evaluation set, we also report results on the standard DUC-2004 test set. The DUC-2004 consists of 500 news articles paired with four different human-generated groundtruth summaries, capped at 75 bytes.\u2021 The expected output is a summary of roughly 14 words, created based on the input article. Method Gigaword-10K DUC-2004 MLE 35.2 \u00b1 0.3 22.6 \u00b1 0.6 RAML 36.4 \u00b1 0.2 23.1 \u00b1 0.6 SPG 0.2 36.6 \u00b1 0.2 23.5 \u00b1 0.6 SPG 0.4 37.8 \u00b1 0.2 24.3 \u00b1 0.5 SPG 0.6 37.4 \u00b1 0.2 24.1 \u00b1 0.5 SPG 0.8 37.3 \u00b1 0.2 24.6 \u00b1 0.5 Table 1: The F1 ROUGE-L scores (with standard errors) for headline generation. We use the sequence-to-sequence recurrent neural network with attention model [2]. For encoding, we use a three-layer, 512-dimensional bidirectional RNN architecture, with a Gated Recurrent Unit (GRU) as the unit-cell [4]; for decoding, we use a similar three-layer, 512-dimensional GRU-based architecture. Both the encoder and decoder networks use a shared vocabulary and embedding matrix for encoding/decoding the word sequences, with a vocabulary consisting of 220K word types and a 512-dimensional embedding. We truncate the encoding sequences to a maximum of 30 tokens, and the decoding sequences to a maximum of 15 tokens. The model is optimized using ADAGRAD with a mini-batch size of 200, a learning rate of 0.01, and gradient clipping with norm equal to 4. We use 40 workers for computing the updates, and 10 parameter servers for model storing and (asynchronous and distributed) updating. We run the training procedure for 10M steps and pick the checkpoint with the best ROUGE-2 score on the Gigaword validation set. We report ROUGE-L scores on the Gigaword evaluation set, as well as the DUC-2004 set, in Table 1. The scores are computed using the standard pyrouge package\u00a7, with standard errors computed using bootstrap resampling [12]. As the numerical values indicate, the maximum performance is achieved when pdrop is in mid-range, with 37.8 F1 ROUGE-L at pdrop = 0.4 on the large Gigaword evaluation set (a larger range for pdrop between 0.4 and 0.8 gives comparable scores on the smaller DUC-2004 set). These numbers are signi\ufb01cantly better compared to RAML (36.4 on Gigaword-10K), which in turn is signi\ufb01cantly better compared to MLE (35.2). \u2021This dataset is available by request at http://duc.nist.gov/data.html. \u00a7Available at pypi.python.org/pypi/pyrouge/0.1.3 7 5.2 Automatic Image-Caption Generation Validation-4K C40 Method CIDEr ROUGE-L CIDEr MLE 0.968 37.7 \u00b1 0.1 0.94 RAML 0.997 38.0 \u00b1 0.1 0.97 SPG 0.2 1.001 38.0 \u00b1 0.1 0.98 SPG 0.4 1.013 38.1 \u00b1 0.1 1.00 SPG 0.6 1.033 38.2 \u00b1 0.1 1.01 SPG 0.8 1.009 37.7 \u00b1 0.1 1.00 Table 2: The CIDEr (with the coco-caption package) and ROUGE-L (with the pyrouge package) scores for image captioning on MSCOCO. For the image-captioning task, we use the standard MSCOCO dataset [14]. The MSCOCO dataset contains 82K training images and 40K validation images, each with at least 5 groundtruth captions. The results are reported using the numerical values for the C40 testset reported by the MSCOCO online evaluation server\u00b6. Following standard practice, we combine the training and validation datasets for training our model, and hold out a subset of 4K images as our validation set. Our model architecture is simple, following the approach taken by the Show-and-Tell approach [25]. We use a one 512-dimensional RNN architecture with an LSTM unit-cell, with a dropout rate equal of 0.3 applied to both input and output of the LSTM layer. We use the same vocabulary size of 8,854 word-types as in [25], with 512-dimensional word-embeddings. We truncate the decoding sequences to a maximum of 15 tokens. The input image is embedded by \ufb01rst passing it through a pretrained Inception-V3 network [22], and then projected to a 512-dimensional vector. The model is optimized using ADAGRAD with a mini-batch size of 25, a learning rate of 0.01, and gradient clipping with norm equal to 4. We run the training procedure for 4M steps and pick the checkpoint of the best CIDEr score [23] on our held-out 4K validation set. 0 500000 1000000 1500000 2000000 2500000 Steps 0.90 0.92 0.94 0.96 0.98 1.00 1.02 1.04 CIDER Score MLE RAML SPG 0.6 Figure 3: Number of training steps vs. CIDEr scores (on Validation-4K) for various learning regimes. We report both CIDEr and ROUGE-L scores on our 4K Validation set, as well as CIDEr scores on the of\ufb01cial C40 testset as reported by the MSCOCO online evaluation server, in Table 2. The CIDEr scores are reported using the coco-caption evaluation toolkit\u2225, while ROUGE-L scores are reported using the standard pyrouge package (note that these ROUGE-L scores are generally lower than those reported by the coco-caption toolkit, as it reports an average score over multiple reference, while the latter reports the maximum). The evaluation results indicate that the SPG method is superior to both the MLE and RAML methods. The maximum score is obtained with pdrop = 0.6, with a CIDEr score of 1.01 on the C40 testset. In contrast, on the same testset, the RAML method has a CIDEr score of 0.97, and the MLE method a score of 0.94. In Figure 3, we show that the number of steps for SPG to converge is similar to the one for MLE/RAML. With the per-step inference cost of those methods being similar (see Section 3.1), the overall convergence time for the SPG method is similar to the MLE and RAML methods. 6 Conclusion The reinforcement learning method presented in this paper, based on a softmax value function, is an ef\ufb01cient policy-gradient approach that eliminates the need for warm-start training and sample variance reduction during policy updates. We show that this approach allows us to tackle sequence generation tasks by training models that avoid two long-standing issues: the exposure-bias problem and the wrong-objective problem. Experimental results con\ufb01rm that the proposed method achieves superior performance on two different structured output prediction problems, one for text-to-text (automatic summarization) and one for image-to-text (automatic image captioning). We plan to explore and exploit the properties of this method for other reinforcement learning problems as well as the impact of various, more-advanced reward functions on the performance of the learned models. \u00b6Available at http://mscoco.org/dataset/#captions-eval. \u2225Available at https://github.com/tylin/coco-caption. 8 Acknowledgments We greatly appreciate Sebastian Goodman for his contributions to the experiment code. We would also like to acknowledge Ning Ye and Zhenhai Zhu for their help with the image captioning model calibration as well as the anonymous reviewers for their valuable comments. References [1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: semantic propositional image caption evaluation. In ECCV, 2016. [2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR, 2015. [3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28, pages 1171\u20131179. 2015. [4] K. Cho, B. van Merrienboer, C. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of EMNLP, pages 1724\u20131734, 2014. [5] Marc P. Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. Foundations and Trends R\u20dd in Robotics, 2(1\u20132):1\u2013142, 2013. ISSN 1935-8253. [6] M. Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. [7] Y. Wu et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. [8] L. C. Evans. An introduction to mathematical optimal control theory. Preprint, version 0.2. [9] David Graff and Christopher Cieri. English Gigaword Fifth Edition LDC2003T05. In Linguistic Data Consortium, Philadelphia, 2003. [10] Ferenc Huszar. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101, 2015. [11] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013. [12] Philipp Koehn. Statistical signi\ufb01cance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388\u2014-395, 2004. [13] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of ACL, 2004. [14] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. [15] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image description metrics using policy gradient methods. In International Conference on Computer Vision (ICCV), 2017. [16] Gergely Neu, Anders Jonsson, and Vicen\u00e7 G\u00f3mez. A uni\ufb01ed view of entropy-regularized markov decision processes. CoRR, abs/1705.07798, 2017. [17] M. Norouzi, S. Bengio, Z. Chen, N. Jaitly, M. Schuster, Y. Wu, and D. Schuurmans. Reward augmented maximum likelihood for neural structured prediction. In Advances in Neural Information Processing Systems 29, pages 1723\u20131731, 2016. [18] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL, 2002. 9 [19] Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. CoRR, abs/1511.06732, 2015. [20] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. [21] RS Sutton, D McAllester, S Singh, and Y Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1999. [22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. volume abs/1512.00567, 2015. [23] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. [24] Arun Venkatraman, Martial Hebert, and J. Andrew Bagnell. Improving multi-step prediction of learned time series models. In Proceedings of the Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence, pages 3024\u20133030. AAAI Press, 2015. [25] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. [26] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, 1992. 10 ",
    "Method": "",
    "Experiments": "Experiments We numerically evaluate the proposed softmax policy gradient (SPG) method on two sequence generation benchmarks: a document-summarization task for headline generation, and an automatic image-captioning task. We compare the results of the SPG method against the standard maximum likelihood estimation (MLE) method, as well as the reward augmented maximum likelihood (RAML) method [17]. Our experiments indicate that the SPG method outperforms signi\ufb01cantly the other approaches on both the summarization and image-captioning tasks. We implemented all the algorithms using TensorFlow 1.0 [6]. For the RAML method, we used \u03c4 = 0.85 which was the best performer in [17]. For the SPG algorithm, all the results were obtained using a variant of ROUGE [13] as the main reward metric R, and J = 1 (sample one target for each example, see Eq. (14)). We report the impact of the pdrop for values in {0.2, 0.4, 0.6, 0.8}. In addition to using the main reward-metric for sampling targets, we also used it to weight the loss for target zij, as we found that it improved the performance of the SPG algorithm. We also applied a naive version of the policy gradient (PG) algorithm (without any variance reduction) by setting pdrop = 0.0, W \u2192 0, but failed to train any meaningful model with cold-start. When starting from a pre-trained MLE checkpoint, we found that it was unable to improve the original MLE result. This result con\ufb01rms that variance-reduction is a requirement for the PG method to work, whereas our SPG method is free of such requirements. 5.1 Summarization Task: Headline Generation Headline generation is a standard text generation task, taking as input a document and generating a concise summary/headline for it. In our experiments, the supervised data comes from the English Gigaword [9], and consists of news-articles paired with their headlines. We use a training set of about 6 million article-headline pairs, in addition to two randomly-extracted validation and evaluation sets of 10K examples each. In addition to the Gigaword evaluation set, we also report results on the standard DUC-2004 test set. The DUC-2004 consists of 500 news articles paired with four different human-generated groundtruth summaries, capped at 75 bytes.\u2021 The expected output is a summary of roughly 14 words, created based on the input article. Method Gigaword-10K DUC-2004 MLE 35.2 \u00b1 0.3 22.6 \u00b1 0.6 RAML 36.4 \u00b1 0.2 23.1 \u00b1 0.6 SPG 0.2 36.6 \u00b1 0.2 23.5 \u00b1 0.6 SPG 0.4 37.8 \u00b1 0.2 24.3 \u00b1 0.5 SPG 0.6 37.4 \u00b1 0.2 24.1 \u00b1 0.5 SPG 0.8 37.3 \u00b1 0.2 24.6 \u00b1 0.5 Table 1: The F1 ROUGE-L scores (with standard errors) for headline generation. We use the sequence-to-sequence recurrent neural network with attention model [2]. For encoding, we use a three-layer, 512-dimensional bidirectional RNN architecture, with a Gated Recurrent Unit (GRU) as the unit-cell [4]; for decoding, we use a similar three-layer, 512-dimensional GRU-based architecture. Both the encoder and decoder networks use a shared vocabulary and embedding matrix for encoding/decoding the word sequences, with a vocabulary consisting of 220K word types and a 512-dimensional embedding. We truncate the encoding sequences to a maximum of 30 tokens, and the decoding sequences to a maximum of 15 tokens. The model is optimized using ADAGRAD with a mini-batch size of 200, a learning rate of 0.01, and gradient clipping with norm equal to 4. We use 40 workers for computing the updates, and 10 parameter servers for model storing and (asynchronous and distributed) updating. We run the training procedure for 10M steps and pick the checkpoint with the best ROUGE-2 score on the Gigaword validation set. We report ROUGE-L scores on the Gigaword evaluation set, as well as the DUC-2004 set, in Table 1. The scores are computed using the standard pyrouge package\u00a7, with standard errors computed using bootstrap resampling [12]. As the numerical values indicate, the maximum performance is achieved when pdrop is in mid-range, with 37.8 F1 ROUGE-L at pdrop = 0.4 on the large Gigaword evaluation set (a larger range for pdrop between 0.4 and 0.8 gives comparable scores on the smaller DUC-2004 set). These numbers are signi\ufb01cantly better compared to RAML (36.4 on Gigaword-10K), which in turn is signi\ufb01cantly better compared to MLE (35.2). \u2021This dataset is available by request at http://duc.nist.gov/data.html. \u00a7Available at pypi.python.org/pypi/pyrouge/0.1.3 7 ",
    "Conclusion": "Conclusion The reinforcement learning method presented in this paper, based on a softmax value function, is an ef\ufb01cient policy-gradient approach that eliminates the need for warm-start training and sample variance reduction during policy updates. We show that this approach allows us to tackle sequence generation tasks by training models that avoid two long-standing issues: the exposure-bias problem and the wrong-objective problem. Experimental results con\ufb01rm that the proposed method achieves superior performance on two different structured output prediction problems, one for text-to-text (automatic summarization) and one for image-to-text (automatic image captioning). We plan to explore and exploit the properties of this method for other reinforcement learning problems as well as the impact of various, more-advanced reward functions on the performance of the learned models. \u00b6Available at http://mscoco.org/dataset/#captions-eval. \u2225Available at https://github.com/tylin/coco-caption. 8 ",
    "References": "References [1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: semantic propositional image caption evaluation. In ECCV, 2016. [2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR, 2015. [3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28, pages 1171\u20131179. 2015. [4] K. Cho, B. van Merrienboer, C. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of EMNLP, pages 1724\u20131734, 2014. [5] Marc P. Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. Foundations and Trends R\u20dd in Robotics, 2(1\u20132):1\u2013142, 2013. ISSN 1935-8253. [6] M. Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. [7] Y. Wu et al. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. [8] L. C. Evans. An introduction to mathematical optimal control theory. Preprint, version 0.2. [9] David Graff and Christopher Cieri. English Gigaword Fifth Edition LDC2003T05. In Linguistic Data Consortium, Philadelphia, 2003. [10] Ferenc Huszar. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101, 2015. [11] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013. [12] Philipp Koehn. Statistical signi\ufb01cance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388\u2014-395, 2004. [13] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of ACL, 2004. [14] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. [15] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image description metrics using policy gradient methods. In International Conference on Computer Vision (ICCV), 2017. [16] Gergely Neu, Anders Jonsson, and Vicen\u00e7 G\u00f3mez. A uni\ufb01ed view of entropy-regularized markov decision processes. CoRR, abs/1705.07798, 2017. [17] M. Norouzi, S. Bengio, Z. Chen, N. Jaitly, M. Schuster, Y. Wu, and D. Schuurmans. Reward augmented maximum likelihood for neural structured prediction. In Advances in Neural Information Processing Systems 29, pages 1723\u20131731, 2016. [18] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL, 2002. 9 [19] Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. CoRR, abs/1511.06732, 2015. [20] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. [21] RS Sutton, D McAllester, S Singh, and Y Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1999. [22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. volume abs/1512.00567, 2015. [23] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. [24] Arun Venkatraman, Martial Hebert, and J. Andrew Bagnell. Improving multi-step prediction of learned time series models. In Proceedings of the Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence, pages 3024\u20133030. AAAI Press, 2015. [25] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. [26] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, 1992. 10 Supplementary Material: Cold-Start Reinforcement Learning with Softmax Policy Gradient 7 Bang-bang Rewarded SPG: Lemma 1 We provide here the proof for Lemma 1, as part of the derivation for the gradient computation method for the Bang-bang rewarded SPG method. Lemma 1 When wi t = 0, \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = 0. Proof First of all, \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121, wi t) \u221d exp \ufffd log p\u03b8(zi t|zi 1:t\u22121, xi) + \u2206ri t \ufffd , (15) where \u2206ri t = wi t \u00b7 (R(zi 1:t|yi) \u2212 R(zi 1:t\u22121|yi)). When wi t = 0, \u2206ri t = 0, therefore, \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121, wi t) \u221d exp \ufffd log p\u03b8(zi t|zi 1:t\u22121, xi) \ufffd = p\u03b8(zi t|zi 1:t\u22121, xi). Therefore, the gradient component at time t of example i is: \ufffd zi \u02dcq\u03b8(zi|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = \ufffd zi 1:t \u02dcq\u03b8(zi 1:t|xi, yi, wi) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = \ufffd zi 1:t\u22121 \u02dcq\u03b8(zi 1:t\u22121|xi, yi, wi) \ufffd zi t \u02dcq\u03b8(zi t|xi, yi, zi 1:t\u22121, wi t) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = \ufffd zi 1:t\u22121 \u02dcq\u03b8(zi 1:t\u22121|xi, yi, wi) \ufffd zi t p\u03b8(zi t|xi, zi 1:t\u22121) \u2202 \u2202\u03b8 log p\u03b8(zi t|xi, zi 1:t\u22121) = \ufffd zi 1:t\u22121 \u02dcq\u03b8(zi 1:t\u22121|xi, yi, wi) \ufffd zi t \u2202 \u2202\u03b8p\u03b8(zi t|xi, zi 1:t\u22121) = \ufffd zi 1:t\u22121 \u02dcq\u03b8(zi 1:t\u22121|xi, yi, wi) \u2202 \u2202\u03b8 \ufffd zi t p\u03b8(zi t|xi, zi 1:t\u22121) \ufffd \ufffd\ufffd \ufffd = \u2202 \u2202\u03b8 1=0 = 0. 8 Algorithm 1: Gradient for the Bang-bang Rewarded Softmax Value Function The gradient computation for the Bang-bang Rewarded Softmax Value Function is formulated in Algorithm 1. The reward functions used by the algorithm above are the ones discussed in Section 4 of the main paper. We extend that discussion in the section below. 9 Reward Functions for the SPG Method 9.1 Main Reward Function In our experiments, the main reward metric is an average over ROUGE-1, ROUGE-2, and ROUGE-3 F1 scores. We choose ROUGE-n [13] based on its good performance as an evaluation metric for 11 Algorithm 1: GRADIENT FOR THE BANG-BANG REWARDED SOFTMAX VALUE FUNCTION Input: Data point (xi, yi), hyperparameter pdrop, W, J, model parameter \u03b8. Result: Gradient of data point (xi, yi): \u2202 \u2202\u03b8 \u02dcLi BBSP G(\u03b8). \u2202 \u2202\u03b8 \u02dcLi BBSP G(\u03b8) = 0 for j \u2208 1, . . . , J do zij \u2190 \u2205 for t \u2208 1, . . . , T do Sample \u00b5ij t \u223c U[0, 1] if \u00b5ij t > pdrop then \u2206rij t = W \ufffd R(zij 1:t|yi) \u2212 R(zij 1:t\u22121|yi) + DUPij t + EOSij t \ufffd Sample zij t \u223c exp \ufffd log p\u03b8(zij t |zij 1:t\u22121, xi) + \u2206rij t \ufffd /Z \u2202 \u2202\u03b8 \u02dcLi BBSP G(\u03b8) = \u2202 \u2202\u03b8 \u02dcLi BBSP G(\u03b8) \u2212 \u2202 \u2202\u03b8 log p\u03b8(zij t |xi, zij 1:t\u22121) else Sample zij t \u223c p\u03b8(zij t |zij 1:t\u22121, xi) end zij \u2190 zij \u222a \ufffd zij t \ufffd . end end both summarization and image-captioning, as well as because it is more computationally ef\ufb01cient compared to other scores such as CIDEr [23] or SPICE [1]. The reason we average up to n = 3 (instead of just 2) is illustrated in the following target example: a man is standing on a street </S> (16) In the above sentence, the word \u2019a\u2019 appears twice. When using a ROUGE average up to n = 2 as the reward metric, for zt\u22121 = \u2019a\u2019, both words \u2019man\u2019 and \u2019street\u2019 have identical reward increments. Therefore, this reward metric cannot distinguish between them. More generally, if the metric used does not account for n-grams longer than 2, it is suboptimal for decisions following common words (like \u2019the\u2019, \u2019of\u2019, or \u2019a\u2019). 9.2 ROUGE-L as a Reward Function The ROUGE-L metric [13] also cannot be applied as the main reward metric by itself. Using Example (16) above, when z1:t\u22121 = \u2019a\u2019, all the remaining target words have identical reward increments under ROUGE-L, because the length of the longest-common-subsequences is the same for all (i.e., 2). Furthermore, if z1:t\u22121 = \u2019a street\u2019, all words (inside or outside the target) except \u2019</S>\u2019 have a 0 reward increment value because it would not improve the length of the longest-commonsubsequence. Although not attempted in this paper, one may combine the ROUGE-L metric with other metrics, such as the one in Section 9.1 above. A similar proposal, albeit in a more traditional PG setting, has been made in [15], taking advantage of the additional signal provided by various metrics. 9.3 EOS Reward Function In the main paper, we introduce an EOS reward function which negatively rewards the end-of-sentence symbol when the length of the output sequence is less than the length of the ground-truth target sequence |yi|: EOSi t = \ufffd\u22121 if zi t = </S> and t < |yi|, 0 otherwise. We illustrate the reason for this reward function using Example (16) again. If z1:t\u22121 = \u2019a street\u2019, then the word with the most reward increment is \u2019</S>\u2019. However, target sequence z = \u2019a street </S>\u2019 is too short and misses a lot information, since there are \ufb01ve remaining words in the ground-truth target 12 that have not been exploited. The EOS function encourages the generation of longer sequences, by correcting the bias introduced by the greediness of the forward-pass sampling step. 9.4 Before/After Examples when using the DUP Reward Function The DUP function penalizes consecutive tokens in the generated sequence, which helps alleviating \"stuttering\" in the model output. The use of the DUP function helps improving the ROUGE-L score for about 0.1 points on the Gigaword dataset. Although without a signi\ufb01cant boost on the ROUGE-L score, we notice clear differences before and after applying the DUP function, as the examples in Table 3 help illustrating. Before After Reference bosnian pm\u2019s resignation provokes bosnian pm\u2019s resignation prime minister\u2019s resignation throws political political political crisis provokes political turmoil bosnia into crisis with yugoslavia sandelin sandelin sandelin wins sandelin wins spanish open sandelin wins spanish open eds: adds spanish open quotes from sandelin and spence credit markets subdued amid stress credit markets subdued amid dif\ufb01cult credit markets show stress crisis stress fears strained banking system spanish \u2019belle rafael rafael azcona spanish \u2019belle rafael azcona spanish \u2019belle epoque\u2019 scriptwriter dies at 81 dies at 81 rafael azcona dies aged 81 nigerian productivity award nigerian productivity award productivity award can be revoked, licence licence licence can be withdrawn says nigerian of\ufb01cial sports column : the big big big sports column : the big league in the big 12, basketball does the big big big big ap photo <UNK> is a big place muscle \ufb02exing Table 3: Examples of the impact of the DUP function on model output. 13 ",
    "title": "Cold-Start Reinforcement Learning with",
    "paper_info": "Cold-Start Reinforcement Learning with\nSoftmax Policy Gradient\nNan Ding\nGoogle Inc.\nVenice, CA 90291\ndingnan@google.com\nRadu Soricut\nGoogle Inc.\nVenice, CA 90291\nrsoricut@google.com\nAbstract\nPolicy-gradient approaches to reinforcement learning have two common and un-\ndesirable overhead procedures, namely warm-start training and sample variance\nreduction. In this paper, we describe a reinforcement learning method based on a\nsoftmax value function that requires neither of these procedures. Our method com-\nbines the advantages of policy-gradient methods with the ef\ufb01ciency and simplicity\nof maximum-likelihood approaches. We apply this new cold-start reinforcement\nlearning method in training sequence generation models for structured output\nprediction problems. Empirical evidence validates this method on automatic sum-\nmarization and image captioning tasks.\n1\nIntroduction\nReinforcement learning is the study of optimal sequential decision-making in an environment [16]. Its\nrecent developments underpin a large variety of applications related to robotics [11, 5] and games [20].\nPolicy search in reinforcement learning refers to the search for optimal parameters for a given policy\nparameterization [5]. Policy search based on policy-gradient [26, 21] has been recently applied to\nstructured output prediction for sequence generations. These methods alleviate two common problems\nthat approaches based on training with the Maximum-likelihood Estimation (MLE) objective exhibit,\nnamely the exposure-bias problem [24, 19] and the wrong-objective problem [19, 15] (more on this\nin Section 2). As a result of addressing these problems, policy-gradient methods achieve improved\nperformance compared to MLE training in various tasks, including machine translation [19, 7], text\nsummarization [19], and image captioning [19, 15].\nPolicy-gradient methods for sequence generation work as follows: \ufb01rst the model proposes a sequence,\nand the ground-truth target is used to compute a reward for the proposed sequence with respect to\nthe reward of choice (using metrics known to correlate well with human-rated correctness, such\nas ROUGE [13] for summarization, BLEU [18] for machine translation, CIDEr [23] or SPICE [1]\nfor image captioning, etc.). The reward is used as a weight for the log-likelihood of the proposed\nsequence, and learning is done by optimizing the weighted average of the log-likelihood of the\nproposed sequences. The policy-gradient approach works around the dif\ufb01culty of differentiating the\nreward function (the majority of which are non-differentiable) by using it as a weight. However, since\nsequences proposed by the model are also used as the target of the model, they are very noisy and\ntheir initial quality is extremely poor. The dif\ufb01culty of aligning the model output distribution with\nthe reward distribution over the large search space of possible sequences makes training slow and\ninef\ufb01cient\u2217. As a result, overhead procedures such as warm-start training with the MLE objective\nand sophisticated methods for sample variance reduction are required to train with policy gradient.\n\u2217Search space size is O(V T ), where V is the number of word types in the vocabulary (typically between 104\nand 106) and T is the the sequence length (typically between 10 and 50), hence between 1040 and 10300.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1709.09346v2  [cs.LG]  13 Oct 2017\n",
    "GPTsummary": "- (1): The authors propose a cold-start reinforcement learning method that aims to address the inefficiencies of policy-gradient-based reinforcement learning. They introduce a softmax value function that combines the model-output distribution with the reward distribution to induce a proposal distribution.\n\n- (2): The proposed method is based on a Softmax Policy Gradient (SPG) approach. The method incorporates both the model-output and reward distributions in a bang-bang mixture model to generate a proposal distribution.\n\n- (3): The method utilizes a forward-pass sampling scheme to draw approximate samples, which are then combined with additional task-specific metrics to enforce various properties on the output sequences.\n\n- (4): The proposed method is evaluated on two sequence generation benchmarks: headline-generation and image-caption\u2013generation tasks. The results show significant improvements in accuracy compared to maximum-likelihood and other competing methods.\n\n- (5): The proposed method is shown to efficiently train sequence generation models for structured output prediction problems without requiring warm-start training or sophisticated methods for sample variance reduction.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This work proposes a cold-start reinforcement learning method based on a softmax value function to efficiently train sequence generation models for structured output prediction problems without requiring warm-start training or sophisticated methods for sample variance reduction. The proposed method significantly improves accuracy compared to maximum-likelihood and other competing methods.\n\n- (2): Innovation point: The proposed method introduces a softmax value function that combines the model-output distribution with the reward distribution to induce a proposal distribution. The use of a bang-bang mixture model and additional task-specific metrics also contributes to the innovation of the method. \n\nPerformance: The proposed method achieves superior performance on both headline-generation and image-caption\u2013generation tasks, which confirms the effectiveness of the approach in tackling sequence generation tasks. \n\nWorkload: Although the proposed method improves the efficiency of policy-gradient-based reinforcement learning by eliminating the need for warm-start training and sample variance reduction, it still requires some complexity in incorporating the task-specific metrics and designing the value function. The workload for implementing the method may be moderate.\n\n\n",
    "GPTmethods": "- (1): The authors propose a cold-start reinforcement learning method that aims to address the inefficiencies of policy-gradient-based reinforcement learning. They introduce a softmax value function that combines the model-output distribution with the reward distribution to induce a proposal distribution.\n\n- (2): The proposed method is based on a Softmax Policy Gradient (SPG) approach. The method incorporates both the model-output and reward distributions in a bang-bang mixture model to generate a proposal distribution.\n\n- (3): The method utilizes a forward-pass sampling scheme to draw approximate samples, which are then combined with additional task-specific metrics to enforce various properties on the output sequences.\n\n- (4): The proposed method is evaluated on two sequence generation benchmarks: headline-generation and image-caption\u2013generation tasks. The results show significant improvements in accuracy compared to maximum-likelihood and other competing methods.\n\n- (5): The proposed method is shown to efficiently train sequence generation models for structured output prediction problems without requiring warm-start training or sophisticated methods for sample variance reduction.\n\n\n\n\n\n8. Conclusion: \n\n- (1): This work proposes a cold-start reinforcement learning method based on a softmax value function to efficiently train sequence generation models for structured output prediction problems without requiring warm-start training or sophisticated methods for sample variance reduction. The proposed method significantly improves accuracy compared to maximum-likelihood and other competing methods.\n\n- (2): Innovation point: The proposed method introduces a softmax value function that combines the model-output distribution with the reward distribution to induce a proposal distribution. The use of a bang-bang mixture model and additional task-specific metrics also contributes to the innovation of the method. \n\nPerformance: The proposed method achieves superior performance on both headline-generation and image-caption\u2013generation tasks, which confirms the effectiveness of the approach in tackling sequence generation tasks. \n\nWorkload: Although the proposed method improves the efficiency of policy-gradient-based reinforcement learning by eliminating the need for warm-start training and sample variance reduction, it still requires some complexity in incorporating the task-specific metrics and designing the value function. The workload for implementing the method may be moderate.\n\n\n",
    "GPTconclusion": "- (1): This work proposes a cold-start reinforcement learning method based on a softmax value function to efficiently train sequence generation models for structured output prediction problems without requiring warm-start training or sophisticated methods for sample variance reduction. The proposed method significantly improves accuracy compared to maximum-likelihood and other competing methods.\n\n- (2): Innovation point: The proposed method introduces a softmax value function that combines the model-output distribution with the reward distribution to induce a proposal distribution. The use of a bang-bang mixture model and additional task-specific metrics also contributes to the innovation of the method. \n\nPerformance: The proposed method achieves superior performance on both headline-generation and image-caption\u2013generation tasks, which confirms the effectiveness of the approach in tackling sequence generation tasks. \n\nWorkload: Although the proposed method improves the efficiency of policy-gradient-based reinforcement learning by eliminating the need for warm-start training and sample variance reduction, it still requires some complexity in incorporating the task-specific metrics and designing the value function. The workload for implementing the method may be moderate.\n\n\n"
}