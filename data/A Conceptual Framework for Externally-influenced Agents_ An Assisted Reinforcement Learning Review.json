{
    "Abstract": "",
    "Introduction": "Introduction Reinforcement learning (RL) [1] is a learning approach in which an agent uses sequential decisions to interact with its environment trying to \ufb01nd a (near-) optimal policy to perform an intended task. RL agents have the ability to improve while operating, to learn without supervision, and to adapt to changing circumstances [2]. By exploring, a standard agent learns solely from the signals it receives from the environment. The RL approach has shown success in domains such as robotics [3, 4, 5, 6], game-playing [7, 8], inventory management [9], and cloud computing [10, 11, 12], among others. This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in the Journal of Ambient Intelligence and Humanized Computing (JAIHC), and is available online at https://doi.org/10.1007/s12652-021-03489-y. arXiv:2007.01544v2  [cs.AI]  20 Sep 2021 Like many machine learning techniques, RL faces the problem of high-dimensionality spaces. As environments become larger, the agent\u2019s learning time increases and \ufb01nding the optimal solution becomes impractical [13]. Early research on this topic [2, 14] argued that for RL to successfully scale into real-world scenarios, then the use of information external to the environment would be needed. Di\ufb00erent RL strategies using this approach have emerged in order to speed up the learning process. They use external information to assist either the process of generalising the environment representation [15], the agent\u2019s decisionmaking process [16], or in providing more focused exploration [17]. In this article, we refer to external information as any kind of information provided to the agent originating from outside of the agent\u2019s representation of the environment. This may include demonstrations [18, 19, 20], advice and critiques [21, 16], initial bias based on previously gathered data [22], or highly-detailed domain-speci\ufb01c shaping functions [23]. Additionally, in this work, we use independently the concepts of RL approach, method, and technique to refer to the underlying learning algorithm. These concepts have been previously used mostly equally by the RL research community. In this regard, we de\ufb01ne Assisted reinforcement learning (ARL) as a range of techniques that use external information, either before, during, or after training, to improve the performance of the learner agent, as well as to scale RL to larger and more complex scenarios. While a relevant characteristic of RL is its ability to endow agents with new skills from the ground up, ARL also makes use of existing information and/or previously learned behaviour. Some methods for improving the agent\u2019s performance using external information include: directly altering weights for actions and states (biasing) [24]; altering the state or action space [25]; critiquing past or advising on future decision-making [26]; dynamically altering reward functions [21]; directly modifying the policy [16]; guiding exploration and action selection [17]; and, creating information repositories/models to supplement the environmental information [15]. Figure 1 captures all of these methods in a basic view of the ARL conceptual framework used in this work. The Environment Agent rt+1 st+1 action at state st , reward rt External information source advice Reinforcement Learning Assisted Reinforcement Learning modi\ufb01cation Figure 1: Assisted reinforcement learning simpli\ufb01ed framework. In autonomous reinforcement learning, an agent performs an action at from a state st and the environment produces an answer leading the agent to a new state st+1 and receiving a reward rt+1. Assisted reinforcement learning adds an external information source, referred to as a trainer, teacher, advisor or assistant, that observes the environment and the agent in order to generate advice. The trainer may advise the learner agent or sometimes directly modify the environment. Moreover, the agent may also actively ask advice to the external information source. classic RL approach is shown within the \ufb01gure where an agent performs an action on the environment reaching a new state and obtaining a reward. In ARL, the response of the environment is also shared with the external information source from where advice is given to the agent or changes sometimes made directly to the environment [27]. To date, many methods using external information have been proposed aiming to speed up the learning process for an autonomous agent [28, 29, 30, 31]. Usually, they have been organized according to the technique employed, e.g., heuristic, interactive, or transfer learning, among others. Nevertheless, there 2 is an important lack of understanding of how these techniques are related and what characteristics they share. Therefore, in this review, we present a conceptual framework and a taxonomy to be used to describe the practice of using external information. A standardised ARL taxonomy will foster collaboration between di\ufb00erent RL communities, improve comparability, allow a precise description of new approaches, and assist in identifying and addressing key questions for further research. 2 A Conceptual Framework for Assisted Reinforcement Learning In this section, we give more details about the ARL approach including some introductory examples of works in which external information sources have been used. Moreover, we de\ufb01ne a conceptual framework identifying the di\ufb00erent parts that comprise the underlying process used in ARL techniques. Based on this conceptual framework, in the following section, we de\ufb01ne a more detailed taxonomy for ARL approaches. 2.1 Assisted Reinforcement Learning The main strength of RL is its ability for endowing an agent with new skills given no initial knowledge about the environment. With an appropriate reward function and enough interaction with its environment, an RL agent can learn (near-) optimal behaviour [1]. The agent\u2019s behaviour at every step is de\ufb01ned by its policy. The reward function promotes desirable behaviour and sometimes penalises undesirable behaviour. In the traditional view of RL, the reward function, and the rewards it produces, are internal to the environment [2]. Traditional RL, in which the environment is the sole provider of information to the agent, has been demonstrated to perform well in many di\ufb00erent domains, especially when facing small and bounded problems [1]. However, RL has some di\ufb03culties when scaling up to large, unbounded environments, particularly regarding the time needed for the agent to learn the optimal policy [32, 33]. In RL, one approach to tackling this issue is to use external information to supplement the information that the environment provides [34, 35]. Information is considered external if it originates from outside of the agent\u2019s interactions with the environment. In this regard, internal information is determined solely through interactions and observations with the environment. For example, in the case of a human the internal information would be anything the person can observe from the environment using their senses [36]. The external information would be any information provided by peers, advisors, the internet, books, maps, and tutelage. In RL, anything external to the agent is usually considered part of the environment. In this regard, if an agent is learning in an environment, a person can be considered as part of it, therefore, the agent could model that person or communicate with them [37]. Although it is possible that external sources of information could be just treated as part of the environment, this is handicapping the agent in an unnecessary way. There are external sources of information that might not necessarily be treated as part of the environment because they are socially advantaged. For instance, if an external source is providing action advice using directions as \u2018left\u2019 and \u2018right\u2019, the agent does not have to learn the meaning of these words from the ground up, or learn how to react to these instructions. Instead, we assume the agent knows that advice is coming, what it means, and how to use it. For example, if a person eats some berries and later becomes sick, the person may determine that those berries are poisonous. In this case, this would be internal information obtained by interaction with the environment. If instead, a peer had previously advised the person that eating those berries will make them sick, that would be external information provided by an extrinsic source. In this work, we refer to methods using externallyin\ufb02uenced agent learning as as assisted reinforcement learning. The ARL framework is de\ufb01ned to include any type of RL that uses external information to supplement agent learning and the decision-making process. Some common practices include the direct alteration of the agent\u2019s understanding of the environment [15], focusing exploration e\ufb00orts through critique and advice [26], or assisting the agent in 3 the decision-making process [17]. For instance, existing ARL techniques include interactive reinforcement learning [38, 39], learning from demonstration [40, 41], and transfer learning [22, 42], among others. The previously mentioned RL approaches are just examples of ARL methods that use external information to supplement the agent\u2019s decision-making process and learning. Additional details of these and other approaches and how they use an external information source to assist the agent (in terms of our ARL framework) are addressed in Section 4. The external information source is most commonly a human or another arti\ufb01cial agent. Regardless of the source, the use of external information has often been shown to improve an agent\u2019s ability and learning speed. In the next section, we present a more detailed conceptual framework for ARL which is the base for the taxonomy we propose subsequently. 2.2 Conceptual Framework The proposed ARL framework is built to improve the classi\ufb01cation, the comparability, and the discussion on di\ufb00erent externally-in\ufb02uenced RL methods. To achieve this aim, the framework has been designed using insights and observations drawn from many different ARL approaches. The result is a framework that can describe existing methods while also being \ufb02exible enough to include future research. The framework details are shown in Figure 2. The proposed ARL framework comprises four processing components shown using red boxes in the diagram, i.e., information source, advice interpretation, external model, and the assisted agent itself. The external information source may not have perfect observability and also may not know details about the RL agent (algorithms, weights, hyperparameters, etc.), or make assumptions, e.g., value-based learners [43]. The processing components are responsible for providing, transforming, and storing information. We do include the agent as part of the processing components since it is part of the RL process as well. However, an agent using ARL generally behaves as a traditional RL agent, i.e., it interacts with the environment by exploring/exploiting actions. Inside the agent, there are three di\ufb00erent stages: reward update, internal processing, and action selection. Each of those stages may be altered by the external model using reward/state modi\ufb01cations, internal modi\ufb01cations, or action modi\ufb01cations respectively. Moreover, the ARL framework also comprises three communication links that connect the four processing components and are labelled: temporality, advice structure, and agent modi\ufb01cation. These links are shown between the processing components and represent the communication lines in Figure 2 that connect the processing components together. The communication links convey information or denote constraints on the data such as where or when to provide information. The ARL framework describes the transmission, modi\ufb01cation, and modality of sourced information. In this regard, we consider the ARL framework as a whole unit, comprising traditional autonomous RL plus the components and links for assistance. Thus, the taxonomy is a part of the framework and oriented to describe the assisted learning section. Although the framework has been developed on how ARL is usually built, not all ARL approaches use all the proposed components and links. Below, we brie\ufb02y describe each of the components and links of the framework. They are subsequently used in the next section to describe in detail the proposed taxonomy. \u2022 Information source: is the origin of the assistance being provided to the agent. The source may be a human, a repository, or another agent. There may be multiple information sources providing assistance to an agent. \u2022 Temporality: determines both the time at which information is provided to the agent, and the frequency with which it is provided. Information may be provided, before, during, or after agent training, and occur multiple times through the learning process. Therefore, it is also responsible for how the information source communicates temporal issues to the advice interpreter. \u2022 Advice interpretation: denotes the process of transforming incoming information into a format better suited for the agent. This may involve extracting key frames from video, converting audio 4 Environment rt+1 Internal  processing st+1 action at state st , reward rt reward/state  modi\ufb01cation Reinforcement  Learning environment  modi\ufb01cation Action  selection Reward  update Information source Advice  interpretation External model Assisted agent internal  modi\ufb01cation action  modi\ufb01cation temporality advice  structure agent  modi\ufb01cation Assisted Reinforcement Learning Figure 2: Detailed view of the assisted reinforcement learning framework. The diagram includes four processing components shown as dashed red boxes. Inside the assisted agent, one can observe three di\ufb00erent points where it can receive possible modi\ufb01cations from the external model. Additionally, three communication links are shown with underlined text. This framework is subsequently used to further discuss the proposed ARL taxonomy. samples to rewards, or mapping information to states. \u2022 Advice structure: represents the structure of the advice after translation in a form suitable for the external model. Some approaches may not have an explicit external model, therefore, this structure might instead be directly used to modify the agent. \u2022 External model: is responsible for retaining and relaying the information between the source and the agent. The model may retain the received information in the learning model, using it for 5 Information Source Advice Interpretation External Model Assisted Agent Temporality Advice structure Agent modification Figure 3: Relation between the processing components and the communication links as a UML sequence diagram. later decisions, or it may discard the received information as soon as it has been used. \u2022 Agent modi\ufb01cation: denotes the approach that the agent uses to bene\ufb01t from the incoming information. The most common modi\ufb01cation approaches may use information to alter the environmental reward signal or modify the agent\u2019s behaviour or the decision-making process directly. \u2022 Assisted Agent: is the RL agent receiving the external information or advice while learning a new task. The agent needs to work out how to incorporate the provided information with its own learning. If a di\ufb00erent action is suggested by the trainer then the agent may decide if it should follow to that advice or not. Figure 3 shows in a UML sequence diagram the interaction between the processing components and communication links according to Figure 2. 3 Assisted Reinforcement Learning Taxonomy In this section, we describe the processing components and communication links included in the proposed framework within an ARL taxonomy1 and give more 1In this context, we refer the taxonomy as a classi\ufb01cation of the di\ufb00erent elements of the ARL framework, i.e., processing components and communication links, and not as a way to classify each ARL method. details of each of them. Figure 4 shows all the elements of the proposed ARL taxonomy including examples for each processing component and communication link. In the taxonomy, we include the agent as a component being the one that receives the advice. Each of the seven elements, i.e., processing components and communication links, is described in detail in the following subsections. In our work, the concept of taxonomy is used to classify the di\ufb00erent elements within a class of problems, i.e., ARL problems. In this regard, our proposal is represented by a general ontology where the class is ARL, the properties are the processing components and the communication links, and the relations between the properties are as shown in Figure 4. 3.1 Information Source The external information source is the main factor that sets ARL apart from traditional RL approaches. It is responsible for introducing new information about the task to the agent, supplementing or replacing the information the agent receives from the environment. The source is external to the agent and the environment, providing information that either the agent may not have had access to, or would have eventually learned itself. The information source may be able to observe the environment, the agent, or the agent\u2019s decision-making process. The objective of the information source is to assist the agent in achieving its goal faster. There may be multiple information sources communicating with an agent. This may be humans, agents, other digital sources, or any combination of the three [44]. The use of multiple sources o\ufb00ers a wider range of available information to the agent. However, more complex modi\ufb01cation methods may be required to manage the information and handle con\ufb02icting advice [45]. There are many examples of external information sources in current ARL literature, the most common of which are humans and additional reward functions [46, 47, 35]. For instance, RLfD and IntRL use human guidance to provide the agent with a generalised view of the solution [48, 49]. Moreover, the use of additional reward functions is one of the earliest 6 Information source Advice  interpretation External model Assisted Reinforcement Learning Taxonomy Temporality Advice  structure Assisted agent Agent  modi\ufb01cation Human Agent Number of advisors Other Speech-to-text Feature identi\ufb01cation Key-frame identi\ufb01cation Binary / scalar Vector Rule / Tree Immediate Retained Combined Planned Interactive Reward / policy shaping Internal modi\ufb01cation Action biasing State shaping Normal agent Curiosity-driven agent Multi-policy agent Figure 4: The assisted reinforcement learning taxonomy. This \ufb01gure shows the four processing components as dashed red boxes and the communication links as green parallelograms using underlined text. Examples for each component and method are included at the right. examples of ARL. In such cases, the designer of the agent encodes some further information about the environment or goal as an additional reward, supplementing the original reward given by the environment. An example of the use of additional reward functions can be found in Randl\u00f8v and Alstr\u00f8m\u2019s bicycle experiment [23], in which, they teach an agent to ride a bicycle towards a goal point. Without additional assistance, the RL agent would only receive a reward upon reaching the termination state. Randl\u00f8v and Alstr\u00f8m encoded some of their knowledge as a shaping reward signal external to the environment, providing the agent with additional rewards if it is cycling towards the goal point. In this scenario, the system designers acted as an external information source, providing extra information to the RL agent. The use of this external information results in the agent learning the solution faster than using the traditional RL approach. Some other information sources include behaviours from past experiences or other agents, repositories of labelled data or examples, or distribution tables for initialising/biasing agent behaviour [39]. Video, audio, and text sources may be used as well [50]. However, these sources may require substantial amounts of interpretation and preprocessing to be of use. The accuracy, availability, or consistency of the information source can a\ufb00ect the maximum utility of the information [51, 52]. Identifying in advance inaccurate information given to the agent can signi\ufb01cantly improve performance [32, 53]. While the information source may perform the validation and the veri\ufb01cation of the given advice, the primary duty remains simply to act as a supplementary source of information. In this regard, both validation and veri\ufb01cation of information are functions better suited for the external model or the assisted agent. 3.2 Temporality The temporal component, or temporality, refers to the time at which information is communicated by the information source. The information may be provided in full to the agent at a set time (either before, during, or after training). This is referred to as planned assistance [54, 55]. Alternatively, the information may be provided at any time during the agent\u2019s operation, referred to as interactive assistance [56, 57]. Planned assistance, on the one hand, is common in ARL methods. Some examples are prede\ufb01ned additional shaping functions, agent policy initialisation based on either prior experience or a known distribution, and the creation of subgoals that lead the way to a \ufb01nal solution [54]. These methods let the experiment designer endow the agent with initial information about the environment or the goal to be 7 achieved. By providing this initial knowledge, the designer can reduce the agent\u2019s need for exploration. The bicycle experiment discussed in the previous section is an example of planned assistance. As mentioned, the agent is learning to control a bicycle and must learn to steer it towards a goal [23]. Before the experiment, the designers give the agent additional information in the form of a reward signal that correlates to the direction of the goal state. This planned assistance approach helps the agent to narrow the search space by giving it extra information about the environment. This small yet bene\ufb01cial initial information results in a signi\ufb01cant improvement in the agent\u2019s learning speed. Another example of planned assistance is found in heuristic RL. Heuristic RL is a method of applying advice to agent decision-making. One example is an experiment which implements heuristic RL in the RoboCup soccer domain [58], a domain known for its large state space and continuous state range. In this environment, one team attempts to score a goal, while the other team tries to block the \ufb01rst team from scoring, such as in half-\ufb01eld o\ufb00ence [59, 60]. In this experiment using heuristic RL, the defending team is given initial advice before training. This advice consists of two rules: if the agent is not near the ball then move closer, and if the agent is near the ball then do something with it. The experiment results show that a team that uses planned assistance performs better than a team that is given no initial knowledge [58]. Interactive assistance, on the other hand, refers to information provided by the source repeatedly throughout the agent\u2019s learning. Information sources that assist interactively often can observe the agent\u2019s current state, or the environment the agent is operating in. In current literature, humans are more commonly used as information sources for interactive assistance [61, 62]. The human can observe how the agent is performing and its current state in the environment, and provides guidance or critiques of the agent\u2019s behaviour [63]. For example, Sophie\u2019s Kitchen [26] presents an IntRL based agent, called Sophie, which attempts to bake a cake by interacting with the items and ingredients found in a kitchen. In this experiment, the agent will receive a reward if it successfully bakes the cake. At any point during the agent\u2019s training, an observing human can provide the agent with an additional reward to supplement the reward signal given by the environment. If the agent performs an undesirable action, such as forgetting to add eggs to the cake, the human can punish the agent by providing an immediate negative reward. The human can also reward the agent for performing desirable actions, such as adding ingredients in the correct order. In this experiment, the human advisor is acting as an interactive information source. Although the agent could learn the task without any assistance, the addition of the human advisor and interactive feedback allows the agent to learn the desired behaviour faster in comparison to autonomous RL [26]. The bene\ufb01t of using interactive advice rather than planned advice is that the information source can react to the current state of the agent. Additionally, an interactive information source does not need to encode all possibly useful advice up front. Instead, it can choose to provide relevant information only when required. This approach does have a signi\ufb01cant cost; the information source needs to be constantly observing the agent and determining what information is relevant. For instance, an approach using inverse RL through demonstrations may also consider providing failed examples to show the agent what not to do [64]. 3.3 Advice Interpretation The advice interpretation stage of the taxonomy denotes what transformations need to occur on the incoming information. The source provides information for the agent to use that may need to be translated into a format that the agent can understand. The information source may provide their assistance in many di\ufb00erent forms. Some examples include audio [65], video [50], text [66], distributions and probabilities [35], or prior learned behaviour from a di\ufb00erent task or agent [30]. This information needs to be adapted for use by the agent for the current task. The product of the advice interpretation stage depends on the structure that the agent or external model requires. A \ufb01eld where the interpretation of incoming advice 8 is crucial is Transfer Learning (TL). The goal of TL is to use behaviour learned in a prior task to improve performance in a new, previously unseen task [67]. A critical step in TL is the mapping of states and observations between the old and new domains. The information source provides information to the agent that does not fully align with its current task. Therefore, it is crucial that the information provided can be correctly interpreted, so as to be useful to the current domain. More commonly, this interpretation stage in TL is performed by hand. However, there has also been e\ufb00ort attempting to automate this stage [68, 69]. Another example of the use of the advice interpretation stage is with the sourcing of feedback for RL agents. In the Sophie\u2019s Kitchen experiment [26], discussed in the previous section, the agent can be given positive or negative feedback by a human regarding its choice of actions. In this experiment, the human creates either a green (positive) or a red (negative) bar to represent the desired feedback to be given to the agent. This bar is used to interpret the reward signal to give to the agent, with the colour of the bar designating whether the reward is positive or negative, and the size of the bar designating the magnitude of the reward. This type of feedback can also be extended to audio, where recording phrases such as \u2018Good\u2019 or \u2018Well Done\u2019 are interpreted as positive rewards and \u2018Bad\u2019 or \u2018Try Again\u2019 are interpreted as negative rewards [70]. These methods can also be combined into a multimodel architecture to provide advice to an RL robotic agent using audiovisual sensory inputs, such as work by Cruz et al. [50]. In this experiment, a simulated robot learns how to clean a table using a multi-modal associative function to integrate auditory and visual cues into a single piece of advice which is used by the RL algorithm. In this scenario, the external information source is a human trainer and the RL algorithm represents the integrated advice as a state-action pair. 3.4 Advice Structure The advice structure component refers to the form that the agent or external model requires incoming information to take. The information that the agent uses can be represented in a number of ways. Some examples of advice structures include: Boolean values denoting positive or negative feedback; rules determining action selection; matrices for mapping prior experiences to new states; case-based reasoning structures for the agent to consult with; or, hierarchical decision trees to represent options for the agent to take [62, 71]. The simplest form of structure is binary, in which the information takes only one from two options, such as \u2018Good\u2019 or \u2018Bad\u2019. An example of the use of a binary structure is the TAMER-RL agent [72]. TAMER-RL is an IntRL agent that uses binary feedback from an observing human. At any time step, the human can agree or disagree with the agent about its last action. In this case, the feedback is a binary structure indicating agree or disagree. A more complex advice structure is used in casebased RL agents [73]. A case in this context represents a generalised area of the state space and provides information about which actions to take in that state. The use of a case-based structure allows the agent to gain more information from the information source compared to a binary structure, at a cost of more complex sourcing and interpretation approaches. One of the more common advice structures is a simple state-action pair. A state-action pair consists of a single state and an associated piece of advice. The associated advice may be an additional scalar reward or a recommended action. Using a state-action pair, sourced information is interpreted to provide advice for a given state. In the cleaning-table robot task [50], discussed in the previous section, the external trainer using multi-modal advice provides an action to be performed in speci\ufb01c states. Once the advice is processed using the multi-modal integration function, the proposed action is given to the RL agent to be executed as a state-action pair considering the agent\u2019s current state. This state-action structure has also been used for other methods including TAMERRL [72], Sophie\u2019s Kitchen [26], and policy-shaping approaches [16]. A novel rule-based interactive advice structure is introduced in [74]. Interactive RL methods rely on constant human supervision and evaluation, requiring a substantial commitment from the advice-giver. This constraint restricts the user to providing advice relevant to the current state and no other, even when 9 such advice may be applicable to multiple states. Allowing users to provide information in the form of rules, rather than per-state action recommendations, increases the information per interaction, and does not limit the information to the current state. Rules can be interactively created during the agent\u2019s operation and be generalised over the state space while remaining \ufb02exible enough to handle potentially inaccurate or irrelevant information. The learner agent uses the rules as persistent advice allowing the retention and reuse of the information in the future. Rule-based advice signi\ufb01cantly reduces human guidance requirements while improving agent performance. 3.5 External Model The external model is responsible for retaining and relaying information between the information source and the agent. The external model receives interpreted information from the information source and may either retain the information for use by the agent when required or pass it to the agent immediately. A retained model is an external model that stores all information provided by the information source [17]. A retained model may be used if the cost of acquiring information is greater than the cost of storing it, if the information provided is general or applies to multiple states, or if the information is gathered incrementally. In instances where information is gathered incrementally, using a retained model allows the agent to build up a knowledge base over time. The agent may consult with the model at any time to determine if a reward signal is to be altered, or if there is any extra information that may assist with decision-making. An immediate model passes the information directly to the agent [75]. In this case, the information received is only relevant to the current time step, or the cost of reacquiring the information from the source is less than that of retaining the information. Approaches can also combine this by incorporating both a retained model as well as passing some information through directly, such as [32]. In this work, an RL agent uses a combination of interactive feedback and contextual a\ufb00ordances [76] to speed up the learning process of a robot performing a domestic task. On the one hand, contextual a\ufb00ordances are learned at the beginning of autonomous RL and are readily available from there on to avoid the so-called failed-states, which are states from where the robot is not able to \ufb01nish the task successfully anymore. On the other hand, interactive feedback is provided by an external advisor and used to suggest actions to perform when the robot is learning the task. This advice is given to the robot to be used in the current state and it is discarded immediately after. The external model may have di\ufb00erent functions depending on its implementation. For instance, heuristic RL hosts a model that stores rules and advice that generalise over sections of the state space [77]. In TL, the external model may hold information regarding past experiences and policies from problems similar to the current domain [22, 78], or in inverse RL, the external model is a substitute for the reward function [79]. 3.6 Agent Modi\ufb01cation The modi\ufb01cation stage of the framework denotes how the information that the external model contains is used to assist the agent in achieving its goal. It is responsible for supplementing the agent\u2019s reward, altering the agent\u2019s policy, or helping with the decisionmaking process. A popular method for injecting external information into agent learning is shaping [80]. Shaping is a common method for altering agent performance by modifying parameters in the learning process. Erez and Smart [25] propose a list of techniques in which shaping can be applied to RL agents. These include altering the reward, the agent\u2019s policy, agent learning parameters, and environmental dynamics [27]. Altering the reward the agent receives is a straightforward method for in\ufb02uencing an agent\u2019s learning [81]. It is known as reward-shaping, in which the external information is used to bias the agent\u2019s learning [46]. Special care must be taken to ensure that any modi\ufb01cation of the reward signal remains zero-sum to avoid the agent exploiting the shaped reward in ways that do not align with the desired goal. This can be achieved by ensuring that additional rewards are potential-based, meaning that they are derived from the di\ufb00erence in the values of a potential function at 10 the current and successor states [82]. However, recent work by [83] shows a \ufb02aw in the previous method when transforming non-potential-based reward-shaping into potential-based. Alternatively, the authors introduce a policy invariant explicit shaping algorithm allowing for arbitrary advice, con\ufb01rming that it ensures convergence to the optimal policy when the advice is misleading and also accelerates learning when the advice is useful [83]. Shaping techniques have also been used to alter state-action pairs [84], for dynamic situations [82, 85], and for multi-agent systems [86]. Policy-shaping is the modi\ufb01cation of the agent\u2019s behaviour [16]. This modi\ufb01cation can be done either by in\ufb02uencing how the agent makes decisions or by directly altering the agent\u2019s learned behaviour. A simple method of policy-shaping involves forcing it to take certain actions if advice from the information source has recommended them [87, 88]. Human-in-the-loop techniques may be bene\ufb01cial to address complex RL problems with the help of domain experts, e.g., in health informatics [89]. This allows the external information source to guide the agent and take direct control over exploration/exploitation. Alternatively, the information source can choose to alter the agent\u2019s behaviour directly by changing Q-values or installing rules that override the actions for chosen states [90]. This method of modi\ufb01cation can improve agent performance rapidly, as it can give the agent partial solutions. Internal modi\ufb01cation is a method of altering the parameters of the agent that are essential to its learning. Parameters such as the learning rate (\u03b1), discount factor (\u03b3), and exploration percentage (\u03f5), are all internal to the RL agent and may be altered to a\ufb00ect its performance [91]. For example, if an advisor observes that an agent is repeating actions and not exploring enough then the exploration percentage or learning rate may be temporarily increased. Internal modi\ufb01cation is a simple method to implement. However, it can be di\ufb03cult at times to know which parameters to adjust, and to what degree they are to be adjusted. Environmental modi\ufb01cation is an indirect method for in\ufb02uencing an RL agent. Altering the environment is not always achievable and may be a technique better suited for digital or simulated environments. Some examples of modifying the environment include altering or reducing the state space and observable information [92, 93], reducing the action space [94], modifying the agent\u2019s starting state [95], or altering the dynamics of the environment to make the task easier to solve [96] Below, we further describe these environmental modi\ufb01cations. Reducing the state space can speed up the agent\u2019s learning as there is less of the environment to search. While the agent cannot fully solve the task with an incomplete environment representation, it allows the agent to learn the basic behaviour. The level of detail in the state representation can then be increased, allowing the agent to re\ufb01ne its policy towards the correct behaviour [92, 93]. Reducing the action space is similar to the previous. The agent\u2019s available actions are limited, and the agent attempts to learn the best behaviour it can with the actions it has available. Once a suitable behaviour has been achieved, new actions can be provided, and the agent can begin to learn more complex solutions [94]. Modifying the agent\u2019s starting space alters where in the environment the agent begins learning. Using this approach, the agent can begin training close to the goal. As the agent learns how to navigate to the goal, the starting state is incrementally moved further away. This allows the agent to build upon its past knowledge of the environment [95]. Altering the dynamics of the environment involves changing how the environment operates to make the task easier for the agent to learn [27]. By altering attributes of the environment such as reducing gravity, lowering maximum driving speed, or reducing noise, the agent may learn the desired behaviour faster or more safely. After the agent learns a satisfactory behaviour, the environment dynamics can be changed to more typical levels [97]. 3.7 Assisted Agent The \ufb01nal component of the proposed ARL taxonomy is the RL agent. A key aspect of the taxonomy is that the agent, in the absence of any external information, should operate the same as any RL agent would. Given no external information, the agent should continue to explore and interact autonomously with its environment and attempt to achieve its goal. In the next section, we present an in-depth look at 11 some ARL techniques and describe them in terms of the taxonomy that has been presented in this section. 4 Illustrative Approaches with Components and Links from the Taxonomy This section presents an in-depth analysis of some popular and well-known ARL approaches. Each illustrative approach is described as an instance of the proposed taxonomy shown in Section 3, in some cases using a speci\ufb01c approach and in other cases a set of them. Therefore, for each presented ARL approach, we show how each processing component and each communication link particularly adapts to the ARL taxonomy using current literature in the respective \ufb01eld for concrete examples. 4.1 Heuristic Reinforcement Learning Heuristic RL uses pieces of information that generalise over an area of the state space. The information is used to assist the agent in decision-making and reduce the searchable state space [98, 99]. An example of a heuristic is a rule. A rule can cover multiple states, making its use e\ufb03cient at delivering advice to an agent. In Section 3.2, we have introduced a heuristic RL experiment applied to the RoboCup soccer domain [58]. In the RoboCup soccer domain, one team actively tries to score a goal, while the other team tries to block it. As mentioned, the defending team is given initial advice before training, consisting of two prede\ufb01ned rules. The following is an analysis of this heuristic RL example applied as the ARL taxonomy. \u2022 Information source: The information source for the RoboCup experiment is a person. In this case, the person has previously experimented with the robot soccer domain and can advise the agent with some rules that will speed up learning. \u2022 Temporality: The advice for the agent is given before training begins. Once training has begun the person does not interact with the agent again. This is an example of planned assistance, where Human-domain  expert Convert rule to  machine language Retained rule-set Heuristic Reinforcement Learning Planned Machine  rule Normal agent Policy  shaping Information source Temporality Advice interpretation Advice structure External model Agent modi\ufb01cation Assisted agent Figure 5: Heuristic RL components according the proposed ARL taxonomy. The particular processing components and communication links illustrate a technique used in the RoboCup soccer domain [58]. information is given to the agent at a \ufb01xed time, and the information is known by the information source in advance. \u2022 Advice interpretation: The information needs to be understandable by the agent. In the robot soccer domain, the person gives two rules; (i) if not near the ball then move towards the ball, and (ii) if near the ball do something with the ball. These rules are understandable by the human but need to be translated into machine code so that agent can use them. This is usually a task easily performed by a knowledgeable human operator. 12 The result is conditional-like rules as: (i) IF NOT close to ball() THEN target and move(), and (ii) IF close to ball() THEN kick ball(). \u2022 Advice structure: The structure of the advice after being interpreted is a new rule. The rule needs to be compatible with the agent, including the ability to substitute variables and evaluate expressions. \u2022 External model: The external model used by the heuristic RL agent is a rule set. The external model retains all rules given to it. The model may also retain statistics about the rule relating to con\ufb01dence, number of uses, and state space covered. \u2022 Agent modi\ufb01cation: Heuristic RL uses the rule set to assist the agent in its decision-making. If a rule applies to the current state, then the action that the rule recommends is taken by the agent. This is a form of policy-shaping as the agent\u2019s decision-making is directly manipulated by the external information. \u2022 Assisted Agent: The RL agent operates as usual. When it is time to decide on an action to take it consults the external model. The external model tests all the rules it has and checks to see if any applies to the current state, otherwise, the agent\u2019s default decision-making mechanism is used. Figure 5 shows how the heuristic RL approach \ufb01ts into the proposed ARL taxonomy taking into consideration the previous de\ufb01nitions of processing components and communication links from the RoboCup soccer domain. 4.2 Interactive Reinforcement Learning IntRL is another application of ARL. Most commonly, the information source is an observing human or a substitute for a human, such as an oracle, a simulated user, or another agent [100]. The human provides assessment and advice to the agent, reinforcing the agent\u2019s past actions and guiding future decisions. The human can assess past actions in two ways, by stating that the agent\u2019s chosen action is somehow correct or incorrect, or by telling the agent what the correct action to take is in that instance. Alternatively, the human can advise the agent on what actions to take in the future [101]. The human can recommend actions to take or to avoid, or provide more information about the current state to assist the agent in its decisionmaking [33]. IntRL applications include having a human to provide additional reward information [102, 103], and having a human or agent provide action advice [104, 105]. All of these methods work in real-time and similarly, di\ufb00ering mainly in the agent modi\ufb01cation stage. The following is an analysis of these IntRL approaches applied as the ARL taxonomy. \u2022 Information source: The information source is a human or simulated user. A simulated user is a program, analogous to a human, that acts how a human would in a given situation. The human can observe the agent\u2019s current and past states, past actions taken, and what action the agent recommends it takes [106]. \u2022 Temporality: IntRL agents operate interactively. The advisor can provide information to the agent before, during, or after learning, and repeatedly throughout the learning process. This allows the advisor to react to current information and supply the agent with relevant advice. \u2022 Advice interpretation: The advisor provides either an assessment of past actions taken, recommendations about actions to take, or a reward signal. Computer simulated agents can receive this information as key presses. However, physical agents may receive this information through audio or video inputs [50]. In the case of audio inputs, these may be simple commands such as \u2019Correct\u2019 or \u2019Go Right\u2019, which can be translated to a form the agent can understand [65]. Supporting input modalities such as natural language makes systems based on IntRL more accessible to users who are not themselves familiar with RL. 13 \u2022 Advice structure: A common structure of advice the agent requires is simply a state-action pair. Using this structure the human can assign advice to a state for the agent to use, such as: In this state, do this [107]. \u2022 External model: Either retained or immediate models are commonly used [17, 108]. A retained model tracks what advice/feedback has been received for each state [17]. The agent can use this model to determine the human\u2019s accuracy, consistency, and discount for each piece of advice received. The model acts as a lookup table for the agent, if advice exists for the current state, then the agent can use it. Alternative methods may not retain information given by the human and only use it for the current state [108]. \u2022 Agent modi\ufb01cation: The most common methods of using the advice to modify the agents learning process are reward- and policy-shaping [101]. Reward-shaping uses assessment/critique gathered from the advisor to alter the reward given to the agent. If the advisor disagrees with a past action, then the reward received for that state-action pair is decreased. If the advisor recommends an action to take in the future, then policy-shaping can be used to override the agent\u2019s usual action selection mechanism. One method of implementing policy-shaping for interactive advice is probabilistic policy reuse [17]. \u2022 Assisted Agent: Most of the time, the RL agent operates as any other RL agent would, i.e., it performs actions in the environment by exploiting/ exploring. The agent should continue to do so even if no advice from the trainer is given. Although a trainer could proactively provide advice to the learner, sometimes the student could decide to request such advice, and the trainer may or may not respond to that request. For instance, heuristics have been used to decide if the trainer should provide advice and/or if the learner should ask for it [105]. In contrast, recent work estimates the learner\u2019s uncertainty in its current state, asking for advice in case the level of uncertainty is above a prede\ufb01ned threshold [109]. Human / simulated  user Convert modal cue  to signal Retained /  immediate Interactive Reinforcement Learning Interactive State-action  pair Curiosity-driven  agent Policy / reward  shaping Information source Temporality Advice interpretation Advice structure External model Agent modi\ufb01cation Assisted agent Figure 6: Interactive RL as the proposed ARL taxonomy. In this approach, interactive advice is given by the user and more commonly used as policy and reward shaping. Figure 6 shows how the IntRL approach is adapted to the proposed ARL taxonomy taking into account the previous de\ufb01nitions of processing components and communication links. 4.3 Reinforcement Learning from Demonstration RLfD is a term coined by Schaal [110]. It refers to the setting where both a reward signal and demonstrations are available to learn from, combining the best of the \ufb01elds of RL and Learning from Demonstration 14 (LfD). Since RL presents an objective evaluation of behaviour, optimal behaviour can be achieved. Such an objective evaluation of behaviour is not present in LfD [111], where only expert demonstrations are available to be mimicked and generalised. The student can thus not surpass its master. Nevertheless, LfD is typically much more sample e\ufb03cient than RL. Therefore, the aim is to combine the fast LfD method with objective behaviour evaluation and theoretical guarantees from RL. Two di\ufb00erent approaches have been proposed to use demonstrations in an RL setting. The \ufb01rst is the generation of an initial value-function for temporaldi\ufb00erence learning by using the demonstrations as passive learning experiences for the RL agent [112]. The second approach derives an initial policy from the demonstrations and uses that to kickstart the RL agent [113, 114]. In this regard, Taylor et al. propose the Human-Agent Transfer (HAT) algorithm [115], which consists of three steps: (i) demonstration: the agent performs the task teleoperated and records all state-action transitions, (ii) policy summarising: in order to bootstrap autonomous learning, policy rules are derived from the recorded state-action transitions, and (iii) independent learning: autonomous reinforcement learning using the policy summary to bias the learning. Below we use the HAT algorithm to describe how RLfD \ufb01ts into the ARL taxonomy. \u2022 Information source: An expert of the task (human or otherwise) can provide sample behaviour by demonstrating its execution of the task. Preferably these demonstrations are e\ufb03cient and successful executions of the task. \u2022 Temporality: It uses planned assistance. Demonstrations are recorded and given to the learning agent before it starts training. \u2022 Advice interpretation: The received demonstrations must be \ufb01rst transformed into the agent\u2019s perspective by encoding them as sequences of state-action pairs. These are then processed using a classi\ufb01er, which serves as the LfD component, creating an approximation of the demonstrator\u2019s policy using rules. Domain expert Convert demonstration  to agent\u2019s perspective Retained rule  system Reinforcement Learning from Demonstration Planned Rule system Curiosity-driven  agent Action  biasing Information source Temporality Advice interpretation Advice structure External model Agent modi\ufb01cation Assisted agent Figure 7: RL from demonstration as the proposed ARL taxonomy. In this case, the processing components and communication links are de\ufb01ned from the HAT algorithm [115], which combines RL and LfD. \u2022 Advice structure: The information is encoded as a classi\ufb01er that maps states to the actions which the demonstrator is hypothesised to execute in those states. \u2022 External model: The generated rules are stored in the external model and not modi\ufb01ed anymore. The external model can be queried with a state and responds with the hypothesised demonstrator action in that state. \u2022 Agent modi\ufb01cation: The action proposed by the demonstrator can be integrated into the agent 15 through three action biasing methods: (i) attributing a value bonus to the Q-value for that state-action pair, (ii) extending the agent\u2019s action set with an action that executes the hypothesised demonstrator action, and (iii) probabilistically choosing to execute the action suggested by the model. \u2022 Assisted agent: During its decision-making (when and how depends on the implemented modi\ufb01cation method) the agent has the option to consult the external model to obtain the action that the demonstrator is assumed to take. This kind of agent is sometimes referred to as curiositydriven agent [116]. Otherwise, the agent acts as a usual RL agent. Figure 7 shows how the RLfD approach is adapted to the proposed ARL taxonomy taking into account the previous de\ufb01nitions of processing components and communication links for the HAT algorithm. 4.4 Transfer Learning The idea of transferring information between tasks (or between agents), rather than learning every task from the ground up seems to be obvious in retrospect. While transfer between di\ufb00erent tasks has long been studied in humans, it has only gained popularity in RL settings in the last decade [22]. We consider three distinct settings where TL can be useful. First, an agent may have learned how to perform a task and a new agent must learn to perform that same task or a variation on the task under di\ufb00erent circumstances. Let us consider two agents with di\ufb00erent state features, i.e., di\ufb00erent sensors, or di\ufb00erent action spaces (or di\ufb00erent actuators). In this case, an inter-task mapping [117, 118] can be hand speci\ufb01ed or learned from data [119, 120] to relate the new target agent to the existing source agent. One of the simplest ways to reuse such knowledge is to embed it into the target task agent, e.g., directly reuse the Q-values that the source agent had learned [118]. Second, let us now consider that the world may be non-stationary. In TL settings, it is common to assume that the agent is noti\ufb01ed when the world (or Agent with di\ufb00erent  capabilities Q-values, rules,  or models Retained source model Transfer Learning Planned Value, rule,  or model Normal agent Action  biasing Information source Temporality Advice interpretation Advice structure External model Agent modi\ufb01cation Assisted agent Figure 8: Transfer learning as the proposed ARL taxonomy. In this case, an agent with di\ufb00erent capabilities (or the same agent) provides the model of a source task which is transferred to a target task. task in that world) changes. However, a TL agent sometimes does not need to detect changes [121] or worry about the slow world changes over time [122]. As in the previous setting, the agent may want to modify the information, e.g., by using an inter-task mapping, to relate the two tasks. In addition, the agent may decide not to use its prior knowledge at all, e.g., to avoid negative transfer because the tasks are too dissimilar [118]. Third, TL could be a critical step within a curriculum learning approach [123, 124]. For example, previous work has shown that learning a sequence 16 of tasks that gradually increase in di\ufb03culty can be faster than directly training on the \ufb01nal (di\ufb03cult) task [119, 125]. In addition to curricula that are created by machine learning experts, curricula constructed by naive human participants have also been considered [126]. Others have considered as a complementary problem a learning agent autonomously creating a curriculum [127, 128]. In all cases, the di\ufb03culty is sca\ufb00olding correctly so that the agent can learn quickly on a sequence of tasks. These approaches are distinct from multi-task learning [17], where the agent wants to learn over a distribution of tasks, and lifelong learning [129, 130], where learning a new task should also improve performance on previous tasks. The following is an analysis of TL methods in terms of the ARL taxonomy. \u2022 Information source: The information comes from an agent with di\ufb00erent capabilities or the same agent that has trained on a di\ufb00erent task. \u2022 Temporality: Transfer typically occurs when a task changes or when an agent \ufb01rst faces a novel task. In both cases, it is planned assistance, i.e., the source agent transfers knowledge to the target agent before the target agent begins learning. If the inter-task mapping is initially unknown, some time may be spent trying to learn an inter-task mapping or estimate task similarity to previous tasks. However, the more time spent before the transfer, the less impact transfer can have. \u2022 Advice interpretation: There are many types of information that can be transferred, including Q-values, rules, a model, etc. [118]. TL methods assume the target agent has access to the source agent\u2019s \u2018brain\u2019, an assumption that may not always be true, e.g., if the designer of the source agent has not provided an API or if the source agent is a human. \u2022 Advice structure: The structure of the transferred knowledge is as varied as the types of information that can be provided. This variety of information includes Q-values, rules, or a model, among others. \u2022 External model: The source model is normally retained. Because the source task knowledge is not necessarily su\ufb03cient for optimal performance in the target task, it is important for the target agent to be able to learn to outperform the transferred information. \u2022 Agent modi\ufb01cation: The target task agent uses the transferred information to bias its learning. The transferred knowledge is not typically modi\ufb01ed. Instead, the target task agent builds on top of the knowledge, learning when to ignore it and instead follow the knowledge it has learned from the environment. \u2022 Assisted Agent: The agent is a typical RL agent that can take advantage of one or more types of prior knowledge. Figure 8 shows how the TL approach can be represented within the proposed ARL taxonomy taking into account the previous de\ufb01nitions of processing components and communication links. 4.5 Multiple Information Sources While the majority of work in ARL is based on a single source of advice, several researchers have considered scenarios where multiple sources of advice may exist [131, 132, 133, 134]. Although the use of multiple information sources is not an ARL approach by itself and could comprise sources utilising any of the previously mentioned approaches, we include it here to highlight how this multiple sources can be framed within the proposed taxonomy. The introduction of multiple advisors may have bene\ufb01ts for ARL agents, particularly in scenarios where each individual advisor has knowledge which is limited in some way [135], e.g., individual advisors may have expertise covering di\ufb00erent sub-areas of the problem domain. However, it also introduces additional problems for the agent, such as handling inconsistencies or direct con\ufb02icts between the guidance provided by di\ufb00erent advisors, or learning to judge the reliability of each advisor, possibly in a state-sensitive manner [104]. In the extreme case, an agent may even need to be able to identify and ignore the advice provided by 17 deliberately malicious advisors [136]. The following is an analysis of approaches using multiple information sources with respect to the proposed ARL taxonomy. \u2022 Information source: Prior research has identi\ufb01ed several scenarios in which an agent may have access to multiple sources of external information. Argall et al. [137] argue that when robots are applied to tasks within society in general, it is very likely that multiple users will interact with and guide the behaviour of a robot. In the context of TL, multiple sources of information may be derived either from experience on varying MDPs [138], or on alternative mappings from a single prior MDP to the current environment [139]. In multi-agent systems, each agent may serve as a potential source of information for every other agent [140, 141]. \u2022 Temporality: Assistance may be planned or interactive. For instance, Argall et al. [137] have considered two di\ufb00erent sources of information, in the form of teacher demonstrations and teacher feedback on trajectories generated by the learner. The former may be provided in advance of learning consisting of complete state-action trajectories, i.e., planned assistance, while the latter occurs on an interactive basis during learning, and structurally consists of a subset of the learner\u2019s actions being \ufb02agged as correct by the teacher, i.e., interactive assistance. \u2022 Advice interpretation: The majority of work so far on ARL from multiple information sources has assumed that these sources are homogeneous in terms of the timing and nature of the information provided. However, this need not be the case, and for heterogeneous information sources, some aspects of the advice may di\ufb00er in terms of interpretation and structure. In this regard, the advice needs to be integrated considering either all possible sources (equally or non-equally contributing), some sources (with the information provided partially or fully considered), or only from one source at a time [135]. \u2022 Advice structure: Each information source may use a di\ufb00erent structure of advice. Therefore, Multi-users or  multi-agent system Multi-source  integration Separated or  combined model Multiple Information Sources Planned or  interactive Integrated  advice Multi-policy  agent Weighted  combination Information source Temporality Advice interpretation Advice structure External model Agent modi\ufb01cation Assisted agent Figure 9: Multiple information sources as the proposed ARL taxonomy. In this case, there could be multiple humans or multiple agents. One important aspect is to integrate the di\ufb00erent pieces of advice. The agent may also learn multiple policies as in multiobjective RL. individually all the aforementioned structures in previous sections are possible to be used, e.g., machine rule, state-action pair, rule system, value, or model. The \ufb01nal structure into a single piece of advice may be done by integrating the multiple information sources, for instance using a multi-modal integration function [50] or using graph structures (e.g., graph neural networks) using causal links between features for multi-modal causability [142]. 18 \u2022 External model: An ARL agent must choose whether (i) to maintain a separate model for each information source, (ii) to combine the information from all sources into a single model, or (iii) a combination of both. An example of the latter approach is the inverse RL system presented in [143], which learns a model of each information source in the form of a feature-weighting function and then forms a combined feature-weighting via averaging. As noted by Karlsson [143], singlemodel approaches may encounter di\ufb03culties if dealing with information sources which are fundamentally incompatible with each other. An additional bene\ufb01t of maintaining independent models is that these can also be augmented by additional data on characteristics of each information source, such as the reliability or consistency of its advice [137, 139]. \u2022 Agent modi\ufb01cation: Any of the modi\ufb01cation approaches discussed in the earlier sections of this paper may also be applied in the context of multiple information sources. For example, agent modi\ufb01cation methods from LfD [137], TL [139, 138], reward-shaping [144, 145] as well as inverse RL [143, 146]. The main additional consideration is how these methods may be a\ufb00ected by the presence of multiple external models. The main methods examined so far use a combination of the models, either weighted or unweighted [137, 143] or select a single best model to use [139]. \u2022 Assisted Agent: In most circumstances, the operation of the agent itself is largely una\ufb00ected by the presence of more than one information source. However, Tanwani and Billard [146] consider the task of performing inverse RL from multiple demonstrations provided by multiple experts, operating according to di\ufb00erent strategies or preferences. To address the potential incompatibilities between these strategies, the agent attempts to learn a set of multiple policies, so as to be able to satisfy any policy expert strategy, including those not provided to the agent. This approach is closely related to multi-policy algorithms developed for multiobjective RL [147]. Figure 9 shows how an approach using multiple information sources is adapted to the proposed ARL taxonomy taking into account the previous de\ufb01nitions of processing components and communication links. Moreover, Table 1 summarises how each of the ARL approaches and examples reviewed in this section is adapted to the proposed taxonomy. 5 Future Directions and Open Challenges In this section, we discuss open issues and propose further possibilities for future work in the \ufb01eld of ARL. These open questions have been identi\ufb01ed from the current literature in the \ufb01eld. Many of these issues are shared with autonomous RL but it still remains open how they could be addressed within the ARL framework. 5.1 Incorrect Assistance A common assumption that ARL methods make is that all external information that the agent receives is accurate [148]. Accurate information is correct advice that assists the agent in completing its goal. However, the assumption that information will always be of use to the agent is wrong, especially when the information source is an observing human, as in RL from imperfect demonstrations [149, 150]. Humans may deliver advice late, and therefore the agent may relate it to a wrong state. The advice may be of shortterm use to the agent but prevent it from achieving optimal performance. Moreover, the human trainer may even be malicious and actively attempting to sabotage the agent\u2019s performance. Incorrect information can be introduced by other sources as well. Some examples for non-human incorrect advice include behaviour transferred from another domain that does not align correctly, rules that generalise over multiple states which may cover exception states, and noisy or missing information from audiovisual sources [50]. Information given to agents may be correct initially, but over time no longer be the optimal solution [122]. Other advice may be mostly accurate or correct for 19 ",
    "Approach": "Approach Information source Temporality Advice interpretation Advice structure External model Agent modi\ufb01cation Assisted agent Heuristic reinforcement learning Humandomain expert Planned Convert rule to machine language Machine rule Retained rule-set Policy shaping Normal agent Interactive reinforcement learning Human / simulated user Interactive Convert modal cue to signal Stateaction pair Immediate Policy / reward shaping Curiositydriven agent Reinforcement learning from demonstration Domain expert Planned Convert demonstration to agent\u2019s perspective Rule system Retained rule system Action biasing Curiositydriven agent Transfer learning Agent with di\ufb00erent capabilities Planned Q-values, rules, or models Value, rule, or model Retained source model Action biasing Normal agent Multiple information sources Multi-users or multi-agent system Planned or interactive Multisource integration Integrated advice Separated or combined model Weighted or unweighted combination Multipolicy agent most states, however, there can exist states of exception to the advice. These exception states can be the critical di\ufb00erence between an ordinary solution and the optimal solution. There is a need for research on how to identify and mitigate incorrect information in these scenarios, especially considering that even a very small amount of incorrect advice may be really detrimental for the learning process [53]. 5.2 Multiple Information Sources As reviewed in the previous section, the use of multiple information sources may naturally arise on some application scenarios, and can increase the agent\u2019s knowledge of the environment, and increase con\ufb01dence in decision-making if the di\ufb00erent sources agree on an action. However, the use of multiple sources raises additional questions: \u2022 What if the di\ufb00erent sources disagree on the best action to take? \u2022 How can the agent identify the best information source to listen to? \u2022 How can the agent manage con\ufb02icting information? \u2022 How can the agent measure trust in the di\ufb00erent information sources? 20 Additionally, the use of multiple sources may be extended to crowdsourcing [45]. In this context, crowdsourcing refers to the enlistment and use of a large number of people, either paid or unpaid and can range in size from tens to tens of thousands. Typically, crowdsourcing is performed via the internet. This can raise challenges of malicious users, anonymity, and large uncertainty in the value and reliability of the information. 5.3 Explainability Explainability refers to translating the agent\u2019s information into a form the human can understand [151, 152]. The reasons why an agent develops certain behaviours can sometimes be di\ufb03cult to understand for non-expert end-users. Systems to measure the quality of explanations generated by AI-based systems have been previously introduced in order to build e\ufb00ective and e\ufb03cient human-AI interaction [153]. When combining the RL method with policy modi\ufb01cation methods such as rules, expert assistance, external models, and policy-shaping, understanding why an agent chooses to take an action becomes even more di\ufb03cult. Developing methods for understanding agent learning and its decision-making is important as it allows the human to remain informed of the agent\u2019s motivations and decisions, and keep track of the accountability of the actions taken [154]. This can be bene\ufb01cial for arti\ufb01cial intelligence ethics, and humancomputer teaching, among other \ufb01elds. 5.4 Two-Way Communication Two-way communication refers to the ability for the information source and the agent to converse with each other, perhaps multiple times before making a decision [155]. Two-way communication can allow the information source, presumably human, and the agent to ask questions to each other, request more information, and to clarify decision-making and its reasoning. Although the proposed framework includes two-way communication, as shown in Figure 1, most current ARL methods do not have two-way communication to the extent that non-expert human advisors can interact with the agent freely. For two-way communication to apply to non-expert human advisors issues of explainability (as shown in the previous section), timing, and agent initiation need to be addressed. Timing refers to the time it takes to communicate back and forth. Agents sometimes have a \ufb01xed time limit, during which they need to learn, communicate, and decide on the next action. Methods for reducing the time it takes to interact with the human and reducing the number of interactions needed with the human are two areas open for research. Agent initiation refers to the ability for the agent to initiate communication with the human source itself. The agent may choose to do this so to request clari\ufb01cation on information, or request assistance for decision-making. A challenge for agent initiation is to determine when and how often the agent should request assistance. The requests for assistance should be frequent enough to make use of the information source while not becoming a nuisance to the human, or detracting from learning time, and should consider the cost of the request, e.g., in paid crowdsourcing. 5.5 Other Challenges There are also other challenges to be considered for future possibilities of ARL systems. Although many of the issues described in this section are also shared with autonomous RL [156], we focus the discussion on how particularly externally-in\ufb02uenced agents may be a\ufb00ected in the context of the ARL framework. While we describe the essential implications on ARL systems for each of the following areas, we note that further and deeper discussion may be addressed for each of them. \u2022 Real-time policy inference: Many RL systems need to be deployed in real-world scenarios and, therefore, policy inference must happen in real-time [157]. Using ARL frameworks may lead to additional issues since the external information source should observe and react to the RL agent\u2019s state as fast as possible, otherwise the assistance may become unnecessary or incorrect for the new reached state. \u2022 Assistance delay: There are RL systems where determining the state or receiving the reward sig21 nal may take even weeks, such as a recommender system where the reward is based on user interaction [158]. In these contexts, the external information source may also lead to unknown delays in the system actuators, sensors, or rewards, making the assistance atemporal, either delayed or ahead, or even in some cases being con\ufb02icting or redundant considering the RL agent\u2019s autonomous operation. \u2022 Continuous states and actions: When an RL agent works in high-dimensional continuous state and action spaces [35, 107] there could be issues for learning even in traditional RL [159]. In an ARL framework, additional problems may be present as the agent uses external information which may be not accurate enough given the high dimensionality. In the presence of highdimensional states and actions, even small di\ufb00erences in the received assistance may substantially slow the learning process since these di\ufb00erences may represent in essence a very di\ufb00erent state or action. \u2022 Safety constraints: In RL environments, there are safety constraints that should never or at least rarely be violated [160]. Special care is needed when receiving information from an external source since there could be situations that the advisor may repeatedly direct the agent to unsafe states and, in turn, lead to an increase in the time needed for learning. \u2022 Partially observable environments: In practice, many RL problems are partially observable [161]. For instance, partial observabilities may occur in non-stationary environments [35] or in presence of stochastic transitions [162]. If the external information source does not have observations to clearly infer the current state in the environment may lead to giving incorrect assistance to the learner agent. \u2022 Multi-objective reward: In many cases, RL agents need to balance multiple and con\ufb02icting subgoals, therefore, they may use multidimensional reward functions [163]. In this regard, an external information source may give priority to a particular subgoal over the others, unbalancing the global reward function. There could be also issues when multiple information sources are used covering or favouring di\ufb00erent subgoals. Moreover, when using a multi-objective reward in TL, there could only be some subgoals from the source task which are relevant in the target task, therefore, the RL agent should also coordinate and \ufb01lter relevant information. \u2022 Multi-agent systems: There could be multiple agents learning a task and multiple external information sources. In this case, if an information source provides advice it could be generalised to all of them or it could be pointed speci\ufb01cally to an agent. Moreover, advice useful for one agent may be detrimental to another, depending on the state, the agent\u2019s current knowledge, or its particular reward function. Using multiple information sources, if an agent consults an external source, it may be necessary to discriminate which one is the best for the particular state. Additionally, the teacher-student approach usually integrated into ARL requires the teacher to be an expert in the learning domain. In this regard, multiple learning agents may also advise each other while learning in a common environment [140]. 6 Conclusions In this article, we have reviewed ARL methods and presented an ARL framework, comprising all RL techniques that use external information. ARL methods use external information to supplement the information the agent receives from the environment to improve performance and decision-making. To describe the di\ufb00erent ARL methods, we propose a taxonomy to classify the di\ufb00erent functions of an externally-in\ufb02uenced RL agent. Through the analysis of the current literature, we have found seven key features that make up an ARL technique. They are divided into four processing components and three communication links. A de\ufb01nition and examples of each of these seven features have been presented. The contribution of this paper is twofold: the review 22 ",
    "References": "References [1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [2] L. P. Kaelbling, M. L. Littman, and A. W. Moore, \u201cReinforcement learning: A survey,\u201d Journal of arti\ufb01cial intelligence research, pp. 237\u2013285, 1996. [3] H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, E. Osawa, and H. Matsubara, \u201cRoboCup: A challenge problem for AI,\u201d AI magazine, vol. 18, no. 1, p. 73, 1997. [4] J. Kober, J. A. Bagnell, and J. Peters, \u201cReinforcement learning in robotics: A survey,\u201d The International Journal of Robotics Research, vol. 32, no. 11, pp. 1238\u20131274, 2013. [5] F. Cruz, P. W\u00a8uppen, A. Fazrie, C. Weber, and S. Wermter, \u201cAction selection methods in a robotic reinforcement learning scenario,\u201d in 2018 IEEE Latin American Conference on Computational Intelligence (LA-CCI), pp. 1\u20136, IEEE, 2018. [6] R. Contreras, A. Ayala, and F. Cruz, \u201cUnmanned aerial vehicle control through domainbased automatic speech recognition,\u201d Computers, vol. 9, no. 3, p. 75, 2020. [7] G. Tesauro, \u201cTD-Gammon, a self-teaching backgammon program, achieves master-level play,\u201d Neural computation, vol. 6, no. 2, pp. 215\u2013 219, 1994. [8] P. Barros, A. Tanevska, F. Cruz, and A. Sciutti, \u201cMoody learners-explaining competitive behaviour of reinforcement learning agents,\u201d in 2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob), pp. 1\u20138, IEEE, 2020. [9] I. Giannoccaro and P. Pontrandolfo, \u201cInventory management in supply chains: a reinforcement learning approach,\u201d International Journal of Production Economics, vol. 78, no. 2, pp. 153\u2013 161, 2002. [10] A. Shakarami, M. Ghobaei-Arani, M. Masdari, and M. Hosseinzadeh, \u201cA survey on the computation o\ufb04oading approaches in mobile edge/cloud computing environment: a stochastic-based perspective,\u201d Journal of Grid Computing, pp. 1\u201333, 2020. [11] A. Shahidinejad and M. Ghobaei-Arani, \u201cJoint computation o\ufb04oading and resource provisioning for edge-cloud computing environment: A machine learning-based approach,\u201d Software: Practice and Experience, vol. 50, no. 12, pp. 2212\u20132230, 2020. [12] M. Ghobaei-Arani, A. A. Rahmanian, M. Shamsi, and A. Rasouli-Kenari, \u201cA learningbased approach for virtual machine placement in cloud data centers,\u201d International Journal of Communication Systems, vol. 31, no. 8, p. e3537, 2018. 23 [13] A. R. Cassandra and L. P. Kaelbling, \u201cLearning policies for partially observable environments: Scaling up,\u201d in Proceedings of the International Conference on Machine Learning ICML, p. 362, Morgan Kaufmann, 2016. [14] L. J. Lin, \u201cProgramming robots using reinforcement learning and teaching.,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, pp. 781\u2013786, 1991. [15] B. Price and C. Boutilier, \u201cAccelerating reinforcement learning through implicit imitation,\u201d Journal of Arti\ufb01cial Intelligence Research, vol. 19, pp. 569\u2013629, 2003. [16] S. Gri\ufb03th, K. Subramanian, J. Scholz, C. Isbell, and A. L. Thomaz, \u201cPolicy shaping: Integrating human feedback with reinforcement learning,\u201d in Advances in Neural Information Processing Systems, pp. 2625\u20132633, 2013. [17] F. Fern\u00b4andez and M. Veloso, \u201cProbabilistic policy reuse in a reinforcement learning agent,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 720\u2013727, ACM, 2006. [18] G. Konidaris, S. Kuindersma, R. Grupen, and A. Barto, \u201cRobot learning from demonstration by constructing skill trees,\u201d The International Journal of Robotics Research, vol. 31, no. 3, pp. 360\u2013375, 2012. [19] L. Rozo, P. Jim\u00b4enez, and C. Torras, \u201cA robot learning from demonstration framework to perform force-based manipulation tasks,\u201d Intelligent service robotics, vol. 6, no. 1, pp. 33\u201351, 2013. [20] S.-A. Chen, V. Tangkaratt, H.-T. Lin, and M. Sugiyama, \u201cActive deep Q-learning with demonstration,\u201d Machine Learning, pp. 1\u201327, 2019. [21] W. B. Knox and P. Stone, \u201cCombining manual feedback with subsequent MDP reward signals for reinforcement learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 5\u2013 12, International Foundation for Autonomous Agents and Multiagent Systems, 2010. [22] M. E. Taylor and P. Stone, \u201cTransfer learning for reinforcement learning domains: A survey,\u201d Journal of Machine Learning Research, vol. 10, no. 7, pp. 1633\u20131685, 2009. [23] J. Randl\u00f8v and P. Alstr\u00f8m, \u201cLearning to drive a bicycle using reinforcement learning and shaping,\u201d in Proceedings of the International Conference on Machine Learning ICML, pp. 463\u2013471, 1998. [24] N. Vlassis, M. Ghavamzadeh, S. Mannor, and P. Poupart, \u201cBayesian reinforcement learning,\u201d Reinforcement Learning, pp. 359\u2013386, 2012. [25] T. Erez and W. D. Smart, \u201cWhat does shaping mean for computational reinforcement learning?,\u201d in Proceedings of the IEEE International Conference on Development and Learning ICDL, pp. 215\u2013219, IEEE, 2008. [26] A. L. Thomaz and C. Breazeal, \u201cAsymmetric interpretations of positive and negative human feedback for a social learning agent,\u201d in Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication RO-MAN, pp. 720\u2013725, IEEE, 2007. [27] H. Xu, R. Bector, and Z. Rabinovich, \u201cTeaching multiple learning agents by environmentdynamics tweaks,\u201d in AAMAS Adaptive and Learning Agents Workshop ALA 2020, p. 8, 2020. [28] C. Arzate Cruz and T. Igarashi, \u201cA survey on interactive reinforcement learning: Design principles and open challenges,\u201d in Proceedings of the 2020 ACM Designing Interactive Systems Conference, pp. 1195\u20131209, 2020. [29] J. Lin, Z. Ma, R. Gomez, K. Nakamura, B. He, and G. Li, \u201cA review on interactive reinforcement learning from human social feedback,\u201d IEEE Access, vol. 8, pp. 120757\u2013120765, 2020. 24 [30] F. L. Da Silva, G. Warnell, A. H. R. Costa, and P. Stone, \u201cAgents teaching agents: a survey on inter-agent transfer learning,\u201d Autonomous Agents and Multi-Agent Systems, vol. 34, no. 1, p. 9, 2020. [31] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, \u201cA comprehensive survey on transfer learning,\u201d Proceedings of the IEEE, vol. 109, no. 1, pp. 43\u201376, 2020. [32] F. Cruz, S. Magg, C. Weber, and S. Wermter, \u201cTraining agents with interactive reinforcement learning and contextual a\ufb00ordances,\u201d IEEE Transactions on Cognitive and Developmental Systems, vol. 8, no. 4, pp. 271\u2013284, 2016. [33] F. Cruz, G. I. Parisi, and S. Wermter, \u201cMultimodal feedback for a\ufb00ordance-driven interactive reinforcement learning,\u201d in Proceedings of the International Joint Conference on Neural Networks IJCNN, pp. 5515\u20135122, IEEE, 2018. [34] H. B. Suay and S. Chernova, \u201cE\ufb00ect of human guidance and state space size on interactive reinforcement learning,\u201d in Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication RO-MAN, pp. 1\u20136, IEEE, 2011. [35] C. Mill\u00b4an, B. Fernandes, and F. Cruz, \u201cHuman feedback in continuous actor-critic reinforcement learning,\u201d in Proceedings of the European Symposium on Arti\ufb01cial Neural Networks, Computational Intelligence and Machine Learning ESANN, pp. 661\u2013666, ESANN, 2019. [36] Y. Niv, \u201cReinforcement learning in the brain,\u201d Journal of Mathematical Psychology, vol. 53, no. 3, pp. 139\u2013154, 2009. [37] E. Sert, Y. Bar-Yam, and A. J. Morales, \u201cSegregation dynamics with reinforcement learning and agent based modeling,\u201d Scienti\ufb01c reports, vol. 10, no. 1, pp. 1\u201312, 2020. [38] S. Amershi, M. Cakmak, W. B. Knox, and T. Kulesza, \u201cPower to the people: The role of humans in interactive machine learning,\u201d AI Magazine, vol. 35, no. 4, pp. 105\u2013120, 2014. [39] F. Cruz, P. W\u00a8uppen, S. Magg, A. Fazrie, and S. Wermter, \u201cAgent-advising approaches in an interactive reinforcement learning scenario,\u201d in Proceedings of the Joint IEEE International Conference on Development and Learning and Epigenetic Robotics ICDL-EpiRob, pp. 209\u2013214, IEEE, 2017. [40] B. Argall, B. Browning, and M. Veloso, \u201cLearning by demonstration with critique from a human teacher,\u201d in Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction HRI, pp. 57\u201364, ACM, 2007. [41] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, \u201cOvercoming exploration in reinforcement learning with demonstrations,\u201d in Proceedings of the IEEE International Conference on Robotics and Automation ICRA, pp. 6292\u20136299, IEEE, 2018. [42] K. Shao, Y. Zhu, and D. Zhao, \u201cStarcraft micromanagement with reinforcement learning and curriculum transfer learning,\u201d IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 3, no. 1, pp. 73\u201384, 2018. [43] M. E. Taylor, P. Stone, and Y. Liu, \u201cValue functions for rl-based behavior transfer: A comparative study,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, vol. 5, pp. 880\u2013885, 2005. [44] C. L. Isbell, M. Kearns, D. Kormann, S. Singh, and P. Stone, \u201cCobot in LambdaMOO: A social statistics agent,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, pp. 36\u201341, 2000. [45] E. Kamar, S. Hacker, and E. Horvitz, \u201cCombining human and machine intelligence in largescale crowdsourcing,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 467\u2013 474, International Foundation for Autonomous Agents and Multiagent Systems, 2012. 25 [46] A. Y. Ng, D. Harada, and S. Russell, \u201cPolicy invariance under reward transformations: Theory and application to reward shaping,\u201d in Proceedings of the International Conference on Machine Learning ICML, vol. 99, pp. 278\u2013287, 1999. [47] A. L. Thomaz, C. Breazeal, et al., \u201cReinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, vol. 6, pp. 1000\u2013 1005, Boston, MA, 2006. [48] L. C. Cobo, K. Subramanian, C. L. Isbell Jr, A. D. Lanterman, and A. L. Thomaz, \u201cAbstraction from demonstration for e\ufb03cient reinforcement learning in high-dimensional domains,\u201d Arti\ufb01cial Intelligence, vol. 216, pp. 103\u2013128, 2014. [49] K. Subramanian, C. L. Isbell Jr, and A. L. Thomaz, \u201cExploration from demonstration for interactive reinforcement learning.,\u201d in Proceedings of the International Conference on Autonomous Agents and MultiAgent Systems AAMAS, pp. 447\u2013456, 2016. [50] F. Cruz, G. I. Parisi, J. Twiefel, and S. Wermter, \u201cMulti-modal integration of dynamic audiovisual patterns for an interactive reinforcement learning scenario,\u201d in Proceedings fo the IEEE/RSJ International Conference on Intelligent Robots and Systems IROS, pp. 759\u2013766, IEEE, 2016. [51] L. Torrey and M. E. Taylor, \u201cTeaching on a Budget: Agents Advising Agents in Reinforcement Learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, 2013. [52] M. E. Taylor, N. Carboni, A. Fachantidis, I. Vlahavas, and L. Torrey, \u201cReinforcement learning agents providing advice in complex video games,\u201d Connection Science, vol. 26, no. 1, pp. 45\u201363, 2014. [53] F. Cruz, S. Magg, Y. Nagai, and S. Wermter, \u201cImproving interactive reinforcement learning: What makes a good teacher?,\u201d Connection Science, vol. 30, no. 3, pp. 306\u2013325, 2018. [54] I. Partalas, D. Vrakas, and I. Vlahavas, \u201cReinforcement learning and automated planning: A survey,\u201d in Arti\ufb01cial Intelligence for Advanced Problem Solving Techniques, pp. 148\u2013165, IGI Global, 2008. [55] S.-T. Cheng, T.-Y. Chang, and C.-W. Hsu, \u201cA framework of an agent planning with reinforcement learning for e-pet,\u201d in Proceedings of the International Conference on Orange Technologies ICOT, pp. 310\u2013313, IEEE, 2013. [56] P. M. Pilarski and R. S. Sutton, \u201cBetween instruction and reward: human-prompted switching,\u201d in AAAI Fall Symposium Series: Robots Learning Interactively from Human Teachers, pp. 45\u201352, 2012. [57] C. Stahlhut, N. Navarro-Guerrero, C. Weber, S. Wermter, and V.-K.-S. WTM, \u201cInteraction is more bene\ufb01cial in complex reinforcement learning problems than in simple ones,\u201d in Proceedings of the Interdisziplin\u00a8arer Workshop Kognitive Systeme (KogSys), pp. 142\u2013150, 2015. [58] L. A. Celiberto Jr, C. H. Ribeiro, A. H. Costa, and R. A. Bianchi, Heuristic reinforcement learning applied to robocup simulation agents, pp. 220\u2013227. Springer, 2007. [59] S. Kalyanakrishnan, Y. Liu, and P. Stone, \u201cHalf \ufb01eld o\ufb00ense in RoboCup soccer: A multiagent reinforcement learning case study,\u201d in Robot Soccer World Cup, pp. 72\u201385, Springer, 2006. [60] M. Hausknecht, P. Mupparaju, S. Subramanian, S. Kalyanakrishnan, and P. Stone, \u201cHalf \ufb01eld o\ufb00ense: An environment for multiagent learning and ad hoc teamwork,\u201d in AAMAS Adaptive and Learning Agents Workshop ALA 2016, 2016. 26 [61] A. L. Thomaz, G. Ho\ufb00man, and C. Breazeal, \u201cReinforcement learning with human teachers: Understanding how people want to teach robots,\u201d in Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication RO-MAN, pp. 352\u2013357, IEEE, 2006. [62] K. Subramanian, C. Isbell, and A. Thomaz, \u201cLearning options through human interaction,\u201d in IJCAI Workshop on Agents Learning Interactively from Human Teachers (ALIHT), Citeseer, 2011. [63] A. Bignold, F. Cruz, R. Dazeley, P. Vamplew, and C. Foale, \u201cHuman engagement providing evaluative and informative advice for interactive reinforcement learning,\u201d arXiv preprint arXiv:2009.09575, 2020. [64] K. Shiarlis, J. ao Messias, and S. Whiteson, \u201cInverse reinforcement learning from failure,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 1060\u20131068, 2016. [65] F. Cruz, J. Twiefel, S. Magg, C. Weber, and S. Wermter, \u201cInteractive reinforcement learning through speech guidance in a domestic scenario,\u201d in Proceedings of the International Joint Conference on Neural Networks IJCNN, pp. 1341\u20131348, IEEE, 2015. [66] X. Liu, R. Deng, K.-K. R. Choo, and Y. Yang, \u201cPrivacy-preserving reinforcement learning design for patient-centric dynamic treatment regimes,\u201d IEEE Transactions on Emerging Topics in Computing, 2019. [67] F. L. Da Silva and A. H. R. Costa, \u201cA survey on transfer learning for multiagent reinforcement learning systems,\u201d Journal of Arti\ufb01cial Intelligence Research, vol. 64, pp. 645\u2013703, 2019. [68] M. E. Taylor, G. Kuhlmann, and P. Stone, \u201cAutonomous transfer for reinforcement learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 283\u2013290, International Foundation for Autonomous Agents and Multiagent Systems, 2008. [69] S. Narvekar, J. Sinapov, M. Leonetti, and P. Stone, \u201cSource task creation for curriculum learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 566\u2013574, 2016. [70] A. C. Tenorio-Gonzalez, E. F. Morales, and L. Villase\u02dcnor-Pineda, \u201cDynamic reward shaping: training a robot by voice,\u201d in Advances in Arti\ufb01cial Intelligence\u2013IBERAMIA 2010, pp. 483\u2013492, Springer, 2010. [71] F. Kaplan, P.-Y. Oudeyer, E. Kubinyi, and A. Mikl\u00b4osi, \u201cRobotic clicker training,\u201d Robotics and Autonomous Systems, vol. 38, no. 3, pp. 197\u2013 206, 2002. [72] W. B. Knox and P. Stone, \u201cInteractively shaping agents via human reinforcement: The TAMER framework,\u201d in Proceedings of the International Conference on Knowledge Capture, pp. 9\u201316, ACM, 2009. [73] M. Sharma, M. P. Holmes, J. C. Santamar\u00b4\u0131a, A. Irani, C. L. Isbell Jr, and A. Ram, \u201cTransfer learning in real-time strategy games using hybrid cbr/rl.,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, vol. 7, pp. 1041\u20131046, 2007. [74] A. Bignold, F. Cruz, R. Dazeley, P. Vamplew, and C. Foale, \u201cPersistent rule-based interactive reinforcement learning,\u201d Neural Computing and Applications, pp. 1\u201318, 2021. [75] I. Moreira, J. Rivas, F. Cruz, R. Dazeley, A. Ayala, and B. Fernandes, \u201cDeep reinforcement learning with interactive feedback in a human\u2013 robot environment,\u201d Applied Sciences, vol. 10, no. 16, p. 5574, 2020. [76] F. Cruz, G. I. Parisi, and S. Wermter, \u201cLearning contextual a\ufb00ordances with an associative neural architecture,\u201d in Proceedings of the European Symposium on Arti\ufb01cial Neural Network. Computational Intelligence and Machine Learning ESANN, pp. 665\u2013670, UCLouvain, 2016. 27 [77] M. Dorigo and L. Gambardella, \u201cAnt-Q: A reinforcement learning approach to the traveling salesman problem,\u201d in Proceedings of International Conference on Machine Learning ICML, pp. 252\u2013260, 2014. [78] B. Banerjee, \u201cGeneral game learning using knowledge transfer,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, pp. 672\u2013677, 2007. [79] P. Abbeel and A. Y. Ng, \u201cApprenticeship learning via inverse reinforcement learning,\u201d in Proceedings of the International Conference on Machine learning ICML, pp. 1\u20138, ACM, 2004. [80] B. F. Skinner, \u201cThe shaping of phylogenic behavior,\u201d Journal of the Experimental Analysis of Behavior, vol. 24, no. 1, pp. 117\u2013120, 1975. [81] N. Churamani, F. Cruz, S. Gri\ufb03ths, and P. Barros, \u201ciCub: learning emotion expressions using human reward,\u201d in Proceedings of the Workshop on Bio-inspired Social Robot Learning in Home Scenarios, IEEE/RSJ IROS, p. 2, 2016. [82] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Now\u00b4e, \u201cExpressing arbitrary reward functions as potential-based advice.,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, pp. 2652\u2013 2658, 2015. [83] P. Behboudian, Y. Satsangi, M. E. Taylor, A. Harutyunyan, and M. Bowling, \u201cUseful policy invariant shaping from arbitrary advice,\u201d in AAMAS Adaptive and Learning Agents Workshop ALA 2020, p. 9, 2020. [84] E. Wiewiora, G. Cottrell, and C. Elkan, \u201cPrincipled methods for advising reinforcement learning agents,\u201d in Proceedings of the International Conference on Machine learning ICML, pp. 792\u2013 799, 2003. [85] S. Devlin and D. Kudenko, \u201cDynamic potentialbased reward shaping,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 433\u2013 440, International Foundation for Autonomous Agents and Multiagent Systems, 2012. [86] S. Devlin and D. Kudenko, \u201cTheoretical considerations of potential-based reward shaping for multi-agent systems,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 225\u2013 232, International Foundation for Autonomous Agents and Multiagent Systems, 2011. [87] J. Grizou, M. Lopes, and P.-Y. Oudeyer, \u201cRobot learning simultaneously a task and how to interpret human instructions,\u201d in Proceedings of the Joint IEEE International Conference on Development and Learning and Epigenetic Robotics ICDL-EpiRob, pp. 1\u20138, IEEE, 2013. [88] N. Navidi, \u201cHuman AI interaction loop training: New approach for interactive reinforcement learning,\u201d arXiv preprint arXiv:2003.04203, 2020. [89] A. Holzinger, \u201cInteractive machine learning for health informatics: when do we need the humanin-the-loop?,\u201d Brain Informatics, vol. 3, no. 2, pp. 119\u2013131, 2016. [90] M. J. Knowles and S. Wermter, \u201cThe hybrid integration of perceptual symbol systems and interactive reinforcement learning,\u201d in Proceedings of the International Conference on Hybrid Intelligent Systems, pp. 404\u2013409, IEEE, 2008. [91] G. Tesauro, \u201cExtending Q-learning to general adaptive multi-agent systems,\u201d in Advances in neural information processing systems, pp. 871\u2013 878, 2004. [92] M. Kerzel, H. B. Mohammadi, M. A. Zamani, and S. Wermter, \u201cAccelerating deep continuous reinforcement learning through task simpli\ufb01cation,\u201d in Proceedings of the International Joint Conference on Neural Networks IJCNN, pp. 1\u20136, IEEE, 2018. 28 [93] M. Breyer, F. Furrer, T. Novkovic, R. Siegwart, and J. Nieto, \u201cComparing task simpli\ufb01cations to learn closed-loop object picking using deep reinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 1549\u20131556, 2019. [94] M. Sridharan, B. Meadows, and R. Gomez, \u201cWhat can I not do? towards an architecture for reasoning about and learning a\ufb00ordances,\u201d in Proceedings of the International Conference on Automated Planning and Scheduling, pp. 461\u2013 469, 2017. [95] K. Dixon, R. J. Malak, and P. K. Khosla, Incorporating prior knowledge and previously learned information into reinforcement learning agents. Carnegie Mellon University, Institute for Complex Engineered Systems, 2000. [96] C. Mill\u00b4an-Arias, B. Fernandes, F. Cruz, R. Dazeley, and S. Fernandes, \u201cA robust approach for continuous interactive actor-critic algorithms,\u201d IEEE Access, pp. 104242\u2013104260, 2021. [97] C. Mill\u00b4an-Arias, B. Fernandes, F. Cruz, R. Dazeley, and S. Fernandes, \u201cA robust approach for continuous interactive reinforcement learning,\u201d in Proceedings of the 8th International Conference on Human-Agent Interaction, pp. 278\u2013280, 2020. [98] R. A. Bianchi, L. A. Celiberto Jr, P. E. Santos, J. P. Matsuura, and R. L. de Mantaras, \u201cTransferring knowledge as heuristics in reinforcement learning: A case-based approach,\u201d Arti\ufb01cial Intelligence, vol. 226, pp. 102\u2013121, 2015. [99] M.-C. Yang, H. Samani, and K. Zhu, \u201cEmergency-response locomotion of hexapod robot with heuristic reinforcement learning using q-learning,\u201d in Proceedings of the International Conference on Interactive Collaborative Robotics, pp. 320\u2013329, Springer, 2019. [100] A. L. Thomaz, G. Ho\ufb00man, and C. Breazeal, \u201cReal-time interactive reinforcement learning for robots,\u201d in AAAI 2005 Workshop on Human Comprehensible Machine Learning, 2005. [101] G. Li, R. Gomez, K. Nakamura, and B. He, \u201cHuman-centered reinforcement learning: A survey,\u201d IEEE Transactions on Human-Machine Systems, vol. 49, no. 4, pp. 337\u2013349, 2019. [102] W. B. Knox and P. Stone, \u201cReinforcement learning from simultaneous human and MDP reward.,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 475\u2013482, 2012. [103] W. B. Knox and P. Stone, \u201cReinforcement learning from human reward: Discounting in episodic tasks,\u201d in Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication RO-MAN, pp. 878\u2013885, IEEE, 2012. [104] Y. Zhan, H. B. Ammar, and M. E. Taylor, \u201cTheoretically-Grounded Policy Advice from Multiple Teachers in Reinforcement Learning Settings with Applications to Negative Transfer,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, July 2016. [105] O. Amir, E. Kamar, A. Kolobov, and B. Grosz, \u201cInteractive teaching strategies for agent training,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, pp. 804\u2013811, 2016. [106] A. Bignold, F. Cruz, R. Dazeley, P. Vamplew, and C. Foale, \u201cAn evaluation methodology for interactive reinforcement learning with simulated users,\u201d Biomimetics, vol. 6, no. 1, p. 13, 2021. [107] A. Ayala, C. Henr\u00b4\u0131quez, and F. Cruz, \u201cReinforcement learning using continuous states and interactive feedback,\u201d in Proceedings of the International Conference on Applications of Intelligent Systems, pp. 1\u20135, 2019. [108] W. B. Knox, B. D. Glass, B. C. Love, W. T. Maddox, and P. Stone, \u201cHow humans teach agents,\u201d International Journal of Social Robotics, vol. 4, no. 4, pp. 409\u2013421, 2012. 29 [109] F. L. Da Silva, P. Hernandez-Leal, B. Kartal, and M. E. Taylor, \u201cUncertainty-aware action advising for deep reinforcement learning agents,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, pp. 5792\u20135799, 2020. [110] S. Schaal, \u201cLearning from demonstration,\u201d Advances in neural information processing systems, vol. 9, pp. 1040\u20131046, 1997. [111] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, \u201cA survey of robot learning from demonstration,\u201d Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009. [112] W. D. Smart and L. P. Kaelbling, \u201cE\ufb00ective reinforcement learning for mobile robots,\u201d in Proceedings of the IEEE International Conference on Robotics and Automation ICRA, vol. 4, pp. 3404\u20133410, IEEE, 2002. [113] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Now\u00b4e, \u201cReinforcement learning from demonstration through shaping,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, p. 26, 2015. [114] H. B. Suay, T. Brys, M. E. Taylor, and S. Chernova, \u201cLearning from demonstration for shaping through inverse reinforcement learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 429\u2013437, International Foundation for Autonomous Agents and Multiagent Systems, 2016. [115] M. E. Taylor, H. B. Suay, and S. Chernova, \u201cIntegrating reinforcement learning with human demonstrations of varying ability,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 617\u2013624, International Foundation for Autonomous Agents and Multiagent Systems, 2011. [116] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, \u201cCuriosity-driven exploration by selfsupervised prediction,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16\u201317, 2017. [117] H. Bou Ammar, M. E. Taylor, K. Tuyls, and G. Weiss, \u201cReinforcement learning transfer using a sparse coded inter-task mapping,\u201d in European Workshop on Multi-Agent Systems, pp. 1\u2013 16, Springer, 2011. [118] M. E. Taylor, P. Stone, and Y. Liu, \u201cTransfer learning via inter-task mappings for temporal di\ufb00erence learning,\u201d Journal of Machine Learning Research, vol. 8, no. 1, pp. 2125\u20132167, 2007. [119] M. E. Taylor, S. Whiteson, and P. Stone, \u201cTransfer via Inter-Task Mappings in Policy Search Reinforcement Learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 156\u2013163, 2007. [120] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E. Taylor, \u201cUnsupervised Cross-Domain Transfer in Policy Gradient Reinforcement Learning via Manifold Alignment,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, 2015. [121] P. Hernandez-Leal, Y. Zhan, M. E. Taylor, L. E. Sucar, and E. Munoz de Cote, \u201cE\ufb03ciently detecting switches against non-stationary opponents,\u201d Autonomous Agents and Multi-Agent Systems, pp. 1\u201323, November 2016. [122] V. Akila and G. Zayaraz, \u201cA brief survey on concept drift,\u201d in Intelligent Computing, Communication and Devices, pp. 293\u2013302, Springer, 2015. [123] M. E. Taylor, \u201cAssisting Transfer-Enabled Machine Learning Algorithms: Leveraging Human Knowledge for Curriculum Design,\u201d in The AAAI 2009 Spring Symposium on Agents that Learn from Human Teachers, March 2009. 30 [124] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum learning,\u201d in Proceedings of the International Conference on Machine learning ICML, (New York, NY, USA), pp. 41\u2013 48, ACM, 2009. [125] M. Eppe, S. Magg, and S. Wermter, \u201cCurriculum goal masking for continuous deep reinforcement learning,\u201d in Proceedings of the Joint IEEE International Conference on Development and Learning and Epigenetic Robotics ICDLEpiRob, pp. 183\u2013188, IEEE, 2019. [126] B. Peng, J. MacGlashan, R. Loftin, M. L. Littman, D. L. Roberts, and M. E. Taylor, \u201cCurriculum Design for Machine Learners in Sequential Decision Tasks (Extended Abstract),\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, May 2017. [127] S. Narvekar, J. Sinapov, and P. Stone, \u201cAutonomous task sequencing for customized curriculum design in reinforcement learning,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, August 2017. [128] F. L. Da Silva and A. H. R. Costa, \u201cObjectoriented curriculum generation for reinforcement learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 1026\u20131034, International Foundation for Autonomous Agents and Multiagent Systems, 2018. [129] Z. Chen and B. Liu, Lifelong Machine Learning. Synthesis Lectures on Arti\ufb01cial Intelligence and Machine Learning, Morgan & Claypool Publishers, 2016. [130] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, \u201cContinual lifelong learning with neural networks: A review,\u201d Neural Networks, 2019. [131] T. Brys, A. Harutyunyan, P. Vrancx, A. Now\u00b4e, and M. E. Taylor, \u201cMulti-objectivization and ensembles of shapings in reinforcement learning,\u201d Neurocomputing, vol. 263, pp. 48\u201359, 2017. [132] F. L. Da Silva, \u201cIntegrating agent advice and previous task solutions in multiagent reinforcement learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 2447\u20132448, International Foundation for Autonomous Agents and Multiagent Systems, 2019. [133] M. Gimelfarb, S. Sanner, and C.-G. Lee, \u201cReinforcement learning with multiple experts: A Bayesian model combination approach,\u201d in Advances in Neural Information Processing Systems, pp. 9528\u20139538, 2018. [134] T. Yamagata, R. Santos-Rodr\u00b4\u0131guez, R. McConville, and A. Elsts, \u201cOnline feature selection for activity recognition using reinforcement learning with multiple feedback,\u201d arXiv preprint arXiv:1908.06134, 2019. [135] C. R. Shelton, \u201cBalancing multiple sources of reward in reinforcement learning,\u201d in Advances in Neural Information Processing Systems, pp. 1082\u20131088, 2001. [136] L. Nunes and E. Oliveira, \u201cExchanging advice and learning to trust,\u201d Cooperative Information Agents VII, pp. 250\u2013265, 2003. [137] B. D. Argall, B. Browning, and M. Veloso, \u201cAutomatic weight learning for multiple data sources when learning from demonstration,\u201d in Proceedings of the IEEE International Conference on Robotics and Automation ICRA, pp. 226\u2013231, IEEE, 2009. [138] E. Parisotto, J. L. Ba, and R. Salakhutdinov, \u201cActor-mimic: Deep multitask and transfer reinforcement learning,\u201d arXiv preprint arXiv:1511.06342, 2015. [139] E. Talvitie and S. P. Singh, \u201cAn experts algorithm for transfer learning.,\u201d in Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence IJCAI, pp. 1065\u20131070, 2007. 31 [140] F. L. Da Silva, R. Glatt, and A. H. R. Costa, \u201cSimultaneously learning and advising in multiagent reinforcement learning,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 1100\u20131108, 2017. [141] A. Fachantidis, M. E. Taylor, and I. Vlahavas, \u201cLearning to teach reinforcement learning agents,\u201d Machine Learning and Knowledge Extraction, vol. 1, no. 1, pp. 21\u201342, 2019. [142] A. Holzinger, B. Malle, A. Saranti, and B. Pfeifer, \u201cTowards multi-modal causability with graph neural networks enabling information fusion for explainable ai,\u201d Information Fusion, vol. 71, pp. 28\u201337, 2021. [143] J. Karlsson, \u201cLearning to play games from multiple imperfect teachers,\u201d Master\u2019s thesis, Chalmers University of Technology, Gothenburg, Sweden, 2014. [144] T. Brys, A. Now\u00b4e, D. Kudenko, and M. E. Taylor, \u201cCombining multiple correlated reward and shaping signals by measuring con\ufb01dence.,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, pp. 1687\u20131693, 2014. [145] W. B. Knox, P. Stone, and C. Breazeal, \u201cTraining a robot via human feedback: A case study,\u201d in Proceedings of the International Conference on Social Robotics, pp. 460\u2013470, Springer, 2013. [146] A. K. Tanwani and A. Billard, \u201cTransfer in inverse reinforcement learning for multiple strategies,\u201d in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems IROS, pp. 3244\u20133250, IEEE, 2013. [147] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley, \u201cA survey of multi-objective sequential decision-making,\u201d Journal of Arti\ufb01cial Intelligence Research, vol. 48, pp. 67\u2013113, 2013. [148] K. Efthymiadis, S. Devlin, and D. Kudenko, \u201cOvercoming erroneous domain knowledge in plan-based reward shaping,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 1245\u20131246, International Foundation for Autonomous Agents and Multiagent Systems, 2013. [149] Y. Gao, H. Xu, J. Lin, F. Yu, S. Levine, and T. Darrell, \u201cReinforcement learning from imperfect demonstrations,\u201d arXiv preprint arXiv:1802.05313, 2018. [150] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang, B. Fang, and H. Liu, \u201cReinforcement learning from imperfect demonstrations under soft expert guidance,\u201d in Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, pp. 5109\u20135116, 2020. [151] F. Cruz, R. Dazeley, and P. Vamplew, \u201cMemorybased explainable reinforcement learning,\u201d in Proceedings of the Australasian Joint Conference on Arti\ufb01cial Intelligence, pp. 66\u201377, Springer, 2019. [152] R. Dazeley, P. Vamplew, C. Foale, C. Young, S. Aryal, and F. Cruz, \u201cLevels of explainable arti\ufb01cial intelligence for human-aligned conversational explanations,\u201d Arti\ufb01cial Intelligence, p. 103525, 2021. [153] A. Holzinger, A. Carrington, and H. M\u00a8uller, \u201cMeasuring the quality of explanations: the system causability scale (scs),\u201d KI-K\u00a8unstliche Intelligenz, pp. 1\u20136, 2020. [154] R. Dazeley, P. Vamplew, and F. Cruz, \u201cExplainable reinforcement learning for broad-xai: A conceptual framework and survey,\u201d arXiv preprint arXiv:2108.09003, 2021. [155] T. Kessler Faulkner, R. A. Gutierrez, E. S. Short, G. Ho\ufb00man, and A. L. Thomaz, \u201cActive attention-modi\ufb01ed policy shaping: socially interactive agents track,\u201d in Proceedings of the International Conference on Autonomous Agents and Multiagent Systems AAMAS, pp. 728\u2013736, International Foundation for Autonomous Agents and Multiagent Systems, 2019. 32 [156] D. J. Mankowitz, G. Dulac-Arnold, and T. Hester, \u201cChallenges of real-world reinforcement learning,\u201d in ICML Workshop on Real-Life Reinforcement Learning, p. 14, 2019. [157] S. Koenig and R. G. Simmons, \u201cComplexity analysis of real-time reinforcement learning,\u201d in Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence conference AAAI, pp. 99\u2013107, 1993. [158] T. A. Mann, S. Gowal, R. Jiang, H. Hu, B. Lakshminarayanan, and A. Gyorgy, \u201cLearning from delayed outcomes with intermediate observations,\u201d arXiv preprint arXiv:1807.09387, 2018. [159] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap, J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, \u201cDeep reinforcement learning in large discrete action spaces,\u201d arXiv preprint arXiv:1512.07679, 2015. [160] T. G. Karimpanal, S. Rana, S. Gupta, T. Tran, and S. Venkatesh, \u201cLearning transferable domain priors for safe exploration in reinforcement learning,\u201d in Proceedings of the International Joint Conference on Neural Networks IJCNN, pp. 1\u20138, 2019. [161] H. Chen, B. Yang, and J. Liu, \u201cPartially observable reinforcement learning for sustainable active surveillance,\u201d in Proceedings of the International Conference on Knowledge Science, Engineering and Management, pp. 425\u2013437, Springer, 2018. [162] F. Cruz, R. Dazeley, P. Vamplew, and M. Ithan, \u201cExplainable robotic systems: Understanding goal-driven actions in a reinforcement learning scenario,\u201d Neural Computing and Applications, pp. 1\u201317, 2021. [163] P. Vamplew, C. Foale, and R. Dazeley, \u201cA demonstration of issues with value-based multiobjective reinforcement learning under stochastic state transitions,\u201d arXiv preprint arXiv:2004.06277, 2020. 33 ",
    "title": "A Conceptual Framework for Externally-in\ufb02uenced Agents:",
    "paper_info": "Cite as: Bignold, A., Cruz, F., Taylor, M., Brys, T., Dazeley, R., Vamplew, P., Foale, C. (2021).\nA Conceptual Framework for Externally-in\ufb02uenced Agents:\nAn Assisted Reinforcement Learning\nReview. Journal of Ambient Intelligence and Humanized Computing.\nA Conceptual Framework for Externally-in\ufb02uenced Agents:\nAn Assisted Reinforcement Learning Review\nAdam Bignold1,\u2217\nFrancisco Cruz2,3,\u2217\nMatthew E. Taylor4\nTim Brys5\nRichard Dazeley2\nPeter Vamplew1\nCameron Foale1\n1 School of Engineering, IT and Physical Sciences, Federation University, Ballarat, Australia.\n2 School of Information Technology, Deakin University, Geelong, Australia.\n3 Escuela de Ingenier\u00b4\u0131a, Universidad Central de Chile, Santiago, Chile.\n4 Department of Computing Science and The Alberta Machine Intelligence Institute (Amii), University of\nAlberta, Edmonton, AB, Canada.\n5 Action Research Associates, Beirut, Lebanon.\n\u2217 Both authors contributed equally to this manuscript.\nCorresponding e-mails: {a.bignold, p.vamplew, c.foale}@federation.edu.au,\n{francisco.cruz, richard.dazeley}@deakin.edu.au, matthew.e.taylor@ualberta.ca,\ntbrys@actionresearchassociates.org\nAbstract\nA long-term goal of reinforcement learning agents is to\nbe able to perform tasks in complex real-world scenar-\nios. The use of external information is one way of scal-\ning agents to more complex problems. However, there\nis a general lack of collaboration or interoperability be-\ntween di\ufb00erent approaches using external information.\nIn this work, while reviewing externally-in\ufb02uenced\nmethods, we propose a conceptual framework and\ntaxonomy for assisted reinforcement learning, aimed\nat fostering collaboration by classifying and comparing\nvarious methods that use external information in the\nlearning process. The proposed taxonomy details the\nrelationship between the external information source\nand the learner agent, highlighting the process of in-\nformation decomposition, structure, retention, and\nhow it can be used to in\ufb02uence agent learning. As\nwell as reviewing state-of-the-art methods, we identify\ncurrent streams of reinforcement learning that use\nexternal information in order to improve the agent\u2019s\nperformance and its decision-making process. These\ninclude heuristic reinforcement learning, interactive\nreinforcement learning, learning from demonstration,\ntransfer learning, and learning from multiple sources,\namong others. These streams of reinforcement learn-\ning operate with the shared objective of sca\ufb00olding the\nlearner agent. Lastly, we discuss further possibilities\nfor future work in the \ufb01eld of assisted reinforcement\nlearning systems.\nKeywords:\nAssisted\nreinforcement\nlearning,\nExternally-in\ufb02uenced agents, Assistance taxonomy.\n1\nIntroduction\nReinforcement learning (RL) [1] is a learning approach\nin which an agent uses sequential decisions to interact\nwith its environment trying to \ufb01nd a (near-) opti-\nmal policy to perform an intended task. RL agents\nhave the ability to improve while operating, to learn\nwithout supervision, and to adapt to changing cir-\ncumstances [2]. By exploring, a standard agent learns\nsolely from the signals it receives from the environ-\nment. The RL approach has shown success in domains\nsuch as robotics [3, 4, 5, 6], game-playing [7, 8], inven-\ntory management [9], and cloud computing [10, 11, 12],\namong others.\nThis preprint has not undergone peer review or any post-submission improvements or corrections. The Version of\nRecord of this article is published in the Journal of Ambient Intelligence and Humanized Computing (JAIHC), and is\navailable online at https://doi.org/10.1007/s12652-021-03489-y.\narXiv:2007.01544v2  [cs.AI]  20 Sep 2021\n",
    "GPTsummary": "- (1): This paper aims to solve the problem of scaling reinforcement learning agents to more complex tasks and environments. The use of external information has been proposed as one solution, but there is a lack of collaboration and interoperability between different approaches using external information. This paper proposes a conceptual framework and taxonomy for assisted reinforcement learning to foster collaboration by classifying and comparing various methods that use external information in the learning process.\n\n- (2): Past methods for improving the performance of reinforcement learning agents using external information include heuristic, interactive, learning from demonstration, transfer learning, and learning from multiple sources. However, there is a lack of understanding of how these techniques are related and what characteristics they share. The approach proposed in this paper is well motivated because it provides a standardized taxonomy that will improve comparability and assist in identifying and addressing key questions for further research.\n\n- (3): The research methodology proposed in this paper is a conceptual framework and taxonomy for assisted reinforcement learning. The proposed taxonomy details the relationship between the external information source and the learner agent, highlighting the process of information decomposition, structure, retention, and how it can be used to influence agent learning.\n\n- (4): The methods proposed in this paper are not evaluated on any specific task or performance. Instead, the paper aims to provide a conceptual framework and taxonomy for assisted reinforcement learning.\n7. Methods:\n\n- (1): The article proposes a conceptual framework and taxonomy for assisted reinforcement learning (ARL) to foster collaboration and facilitate classification and comparison of various ARL methods that use external information in the learning process.\n\n- (2): The taxonomy proposed is structured by describing the relationship between the external information source and the learner agent, highlighting the process of information decomposition, structure, retention, and how it can be used to influence agent learning. The taxonomy also includes seven key features of ARL techniques that are divided into four processing components and three communication links.\n\n- (3): The authors conducted a comprehensive literature review of past ARL methods, including heuristic, interactive, learning from demonstration, transfer learning, and learning from multiple sources, and analyzed their characteristics and relationship to the proposed framework and taxonomy.\n\n- (4): Although the methods proposed in the article were not evaluated on any specific task or performance, the article provides a valuable conceptual framework and taxonomy for research on ARL methods that can use external information to supplement the information agents receive from the environment, improve performance, and decision-making.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The proposed conceptual framework and taxonomy for assisted reinforcement learning (ARL) in this article is significant for fostering collaboration and facilitating comparison and classification of various ARL methods that use external information in the learning process.\n\n- (2): Innovation Point: The proposed taxonomy provides a standardized approach that facilitates classification and comparison of ARL methods, allowing researchers to better understand the relationship between the external information source and the learner agent. \n\nPerformance: The article did not evaluate the proposed framework and taxonomy on any specific task or performance, but it does provide valuable insights for future research. \n\nWorkload: The article conducts a comprehensive literature review and analysis of past ARL methods, which is a relatively high workload, but it is necessary for establishing the proposed framework and taxonomy.\n\n\n",
    "GPTmethods": "- (1): The article proposes a conceptual framework and taxonomy for assisted reinforcement learning (ARL) to foster collaboration and facilitate classification and comparison of various ARL methods that use external information in the learning process.\n\n- (2): The taxonomy proposed is structured by describing the relationship between the external information source and the learner agent, highlighting the process of information decomposition, structure, retention, and how it can be used to influence agent learning. The taxonomy also includes seven key features of ARL techniques that are divided into four processing components and three communication links.\n\n- (3): The authors conducted a comprehensive literature review of past ARL methods, including heuristic, interactive, learning from demonstration, transfer learning, and learning from multiple sources, and analyzed their characteristics and relationship to the proposed framework and taxonomy.\n\n- (4): Although the methods proposed in the article were not evaluated on any specific task or performance, the article provides a valuable conceptual framework and taxonomy for research on ARL methods that can use external information to supplement the information agents receive from the environment, improve performance, and decision-making.\n\n\n\n\n\n8. Conclusion:\n\n- (1): The proposed conceptual framework and taxonomy for assisted reinforcement learning (ARL) in this article is significant for fostering collaboration and facilitating comparison and classification of various ARL methods that use external information in the learning process.\n\n- (2): Innovation Point: The proposed taxonomy provides a standardized approach that facilitates classification and comparison of ARL methods, allowing researchers to better understand the relationship between the external information source and the learner agent. \n\nPerformance: The article did not evaluate the proposed framework and taxonomy on any specific task or performance, but it does provide valuable insights for future research. \n\nWorkload: The article conducts a comprehensive literature review and analysis of past ARL methods, which is a relatively high workload, but it is necessary for establishing the proposed framework and taxonomy.\n\n\n",
    "GPTconclusion": "- (1): The proposed conceptual framework and taxonomy for assisted reinforcement learning (ARL) in this article is significant for fostering collaboration and facilitating comparison and classification of various ARL methods that use external information in the learning process.\n\n- (2): Innovation Point: The proposed taxonomy provides a standardized approach that facilitates classification and comparison of ARL methods, allowing researchers to better understand the relationship between the external information source and the learner agent. \n\nPerformance: The article did not evaluate the proposed framework and taxonomy on any specific task or performance, but it does provide valuable insights for future research. \n\nWorkload: The article conducts a comprehensive literature review and analysis of past ARL methods, which is a relatively high workload, but it is necessary for establishing the proposed framework and taxonomy.\n\n\n"
}