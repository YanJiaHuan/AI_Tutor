{
    "Abstract": "",
    "Introduction": "Introduction Currently the research area of reinforcement learning has two broad categories of problems, which are single-agent reinforcement learning and multi-agent reinforcement learning. For single-agent reinforcement learning problems, there is only one intelligent agent involved in the learning process. It learns through interacting with its surroundings in order to achieve the objective of optimal individual behaviour. For multi-agent reinforcement learning problems, there are multiple agents involved in the learning process. They can learn not only though interacting with their surroundings but also though interacting with each other, with the objective of not only optimal individual behaviour but also optimal team behaviour of cooperation or equilibrium behaviour of competition. These two parts of the learning objective are not separable since cooperative or competitive behaviour are actually an essential part of individual behaviour in multi-agent reinforcement learning. However, there are some real-world reinforcement learning scenarios which cannot be classi\ufb01ed into either one of the two categories and still lack understanding. In existing literature, there is a lack of understanding for pure cooperative learning and an ambiguity between cooperative learning and learning to cooperate. In a typical multi-agent problem, such as robotic soccer playing, all the agents perform learning activities together in a common environment. For each single agent, the other agents are part of its environment which becomes non-stationary due to the continually changing behaviour of those agents (Samsami and Alimadad, 2020). In this case, all agents from one team are learning together to cooperate with each other to achieve a common goal. It is obvious that learning to cooperate often involves cooperative learning (completely independent learning is also possible) which is basically the learning process with knowledge shared among agents, since agents would probably need information from others to cooperate with them. But from the other way around, cooperative learning does not involve learning to cooperate very often. Agents can be learning with different goals in their own separate environment which is not affected by others and arXiv:2202.05135v3  [cs.LG]  30 Apr 2022 ",
    "Background": "Background Reinforcement learning is a process of learning by trial and error. In this process, there is one or several intelligent agents interacting with their surroundings from which they could get feedbacks for the actions they take. In this way, the agents are able to gain knowledge on how to behave better and gradually improve their performance. There are some important components: an environment, a set of environment states, a set of actions, a policy that guides the agents on choosing actions, immediate rewards on the performed actions, and values of states (V (s)) or state-action pairs (Q(s, a)) which describe how good any state or state-action pair is in a long term into the future. Single-agent reinforcement learning is often modeled as a Markov Decision Process (MDP) which is introduced by Bellman (1957). All the states satisfy the memoryless state transition property P[St+1|St] = P[St+1|S1, S2, ..., St] where the next state is only relevant to the current state without being affected by any previous states or we can say that the current state grabs all necessary information from past states. An MDP is basically a tuple < S, A, P, R, \u03b3 > (1) 2 ",
    "Related Work": "Related Work Methods for single-agent reinforcement learning can be classi\ufb01ed into two broad categories, basic algorithms and distributed variants of the basic algorithms. For basic algorithms, the most well-known one would be Q-learning Introduced by Watkins (1989). Its core mechanism is a Q-table which contains the Q values for all state-action pairs in the current environment. Actions are selected based on the highest Q value under each state. The table will be updated with training. Presented by Hasselt (2010), double Q-learning maintains two Q tables, which are learned with different sets of experiences while being updated with information from each other. When selecting actions, both tables could be utilised. For the Q value of the next state, it uses the other Q table instead of the same Q table to avoid overestimation of the Q values. Replacing the Q tables with deep neural networks, called Q networks, we have deep reinforcement learning algorithms. Proposed by Mnih et al. (2015), DQN (Deep Q-Network) introduced the \ufb01rst deep neural network model into reinforcement learning which takes raw RGB images as input. The model applies the type of network architecture which takes a state s as input and outputs the Q values for all possible state-action pairs (s, ai). It replaces the Q table with this model, and also introduces a target Q-network which is a more stable version of the Q-network whose parameters are copied from the Q-network every C steps for the purpose of stabilising training. Introduced by Van Hasselt et al. (2016), double DQN combined the ideas of DQN and double Q-learning, which uses two deep Q-networks separately for action selection and action evaluation. Wang et al. (2016) proposed dueling DQN, a new Q-network architecture. Same as the original DQN, dueling DQN starts with convolutional layers. But instead of followed by a single sequence of fully connected layers, it has two fully connected layer sequences. One sequence is for estimating the state value function V (s) and the other is for advantage function A(s, a). Then the two estimated values are combined to get Q values for each state-action pair (s, a) under state s as the output. Other than the Q track methods, we also have actor-critic track. A2C, short for Advantage Actor-Critic, is a typical kind of actor-critic method for reinforcement learning. It has two neural networks for approximating policy function \u03c0\u03b8 and state-value function V (s). Another popular actor-critic method is PPO (Schulman et al., 2017), short for Proximal Policy Optimisation. For the distributed variants, there are A3C (asynchronous advantage actor-critic) (Mnih et al., 2016) which is an asynchronous version of distributed A2C, Gorila (Nair et al., 2015) which is distributed DQN, APPO (Luo et al., 2020) which is asynchronous PPO, and DDPPO (Wijmans et al., 2020) which is decentralised distributed PPO. These distributed reinforcement learning algorithms are simple parallelisation and cannot be applied to GARL. But each agent in GARL should be able to apply any one of the basic single-agent algorithms. Methods for multi-agent reinforcement learning can be classi\ufb01ed into three broad categories, cooperative algorithms, competitive algorithms and algorithms for a mix of cooperation and competition. 3 Rashid et al. (2018) introduced an approach for fully cooperative multi-agent tasks featuring monotonic value function factorisation. Lowe et al. (2017) presented a method that works in cooperative, competitive and mixed cooperative-competitive environments, which is an adaptation of classic actor-critic methods. Besides, the most well-known work would be DeepMind\u2019s series of work playing the game of Go (Silver et al., 2016, 2017, 2018; Schrittwieser et al., 2020). Go is a typical competitive environment where two players make moves against each other in order to win. We can see that the competition or cooperation here refers to the interactions between agents which is learned behaviour and the nature of the problem, in comparison to cooperative learning which is just having knowledge sharing added to the original learning problem and not learned behaviour. The multi-agent methods do not apply in GARL due to the inherent difference in problem de\ufb01nition. 4 Group-Agent Reinforcement Learning (GARL) This section formally proposes and formulates GARL, a new type of problem formulation which is different from single-agent and multi-agent reinforcement learning. In the classic single-agent setting, there is only one agent performing reinforcement learning tasks in its environment, who is learning simply through interactions with environment. In multi-agent setting, there are multiple learning agents working in a common environment and learning to cooperate or compete with each other. Each agent is part of the others\u2019 environment which becomes non-stationary because of the evolving behaviour policies of the agents. There is inherent dif\ufb01culty for efforts targeting at multi-agent reinforcement learning problems due to this non-stationarity and high-dimensionality of the joint action space (Wen et al., 2020). In our case of GARL, there are multiple agents doing reinforcement learning together in a \"study group\", which is inspired by a very common real-life behaviour in human intelligence. When we humans study, there are basically two knowledge sources, learning through trial and error in our environment (reinforcement learning) and learning cooperatively through retrieving available knowledge from other people. Hence we often study together in groups to bene\ufb01t the latter process. Note that this process happens through explicit communication between the agents. It does not have to happen in a single environment, but can rather work across multiple environments. In the case of GARL, multiple agents work separately in their independent environment to learn through trial and error while communicating with each other to gain available knowledge. Each of these environments is stationary because no one will interfere with others\u2019 environment. To clarify, these agents do not work in a common environment so that they do not learn cooperative or competitive behaviours as in multi-agent setting, and may have different tasks so that the problem cannot be solved by simple parallelisation in single-agent setting . We give a better picture of this scenario through Figure 1. INTROTO GroupRL Env 4 Env 3 Env 1 Env 2 Group Env Msg 1 Msg 2 Msg 3 Msg 4 Msg 5 Figure 1: These four agents are separately doing reinforcement learning in their own environment, while communicating with each other through sending messages within the group environment. The group environment is simply a communication network over which messages can be sent, no interaction is performed with it. We take autonomous driving as an example to give further explanation. The training of self-driving cars can well take place with reinforcement learning (Sallab et al., 2017). We describe it through three stages, where stage 2 is an application example of GARL. \u2022 Stage 1: Given a certain city environment, one single self-driving car is doing reinforcement learning to gain driving knowledge in one neighbourhood. Its environment, namely this neighbourhood, is stationary. Learning only happens in the form of trial and error in the single-agent setting. 4 \u2022 Stage 2: Still in the previous city environment, there are now multiple self-driving cars all doing reinforcement learning simultaneously, each in a different neighbourhood. Each of their environments, namely the neighbourhoods, is still stationary. The goal of every one of the agents is to learn to drive in its speci\ufb01c neighbourhood environment. We can see that the goals among the agents are slightly different from each other due to the difference between the neighbourhoods. However since these neighbourhoods belong to the same city environment, there is much similarity between them. Therefore, it will largely bene\ufb01t their learning if we create communication channels between the agents for them to exchange their knowledge acquired through environment exploration. It is very possible that one car is not able to explore its environment thoroughly and leave out many environment states, but some other peer car explores them well, so that sound knowledges can be acquired through communicating with that peer car. In this case, learning happens in a group-agent setting. \u2022 Stage 3: With the help of group-agent reinforcement learning method, the multiple selfdriving cars all learned to drive in its own environment well and fast. Now some of the cars drive out of their neighbourhoods to meet other peer cars. In one neighbourhood, there are several cars on the road. They need to learn to cooperate with each other to safely co-exist on the road, namely not causing any car crashes. This neighbourhood environment becomes non-stationary since each of these cars becomes a part of the others\u2019 environment and their behaviours are continually evolving. This turns to be a multi-agent reinforcement learning scenario. These three stages of an autonomous driving application clearly distinguish the three types of reinforcement learning scenarios. Note that GARL cannot be seen as a simpli\ufb01ed version of multiagent reinforcement learning with just the objective of cooperation or competition gotten rid of. In GARL, since each agent is in a separate environment, they can have different state sets and diversi\ufb01ed individual learning goals, thus there is inherently much more freedom for the agents compared to multi-agent reinforcement learning where the agents are very restricted by each other. We present this more clearly with a formal formulation as follows. Recall from section 2 that single-agent reinforcement learning can be modeled as an MDP and multi-agent reinforcement learning can be modeled as a stochastic game. Here we propose group MDP to model GARL. It can be stated as the following tuple < S1, \u00b7 \u00b7 \u00b7 , Sn, A1, \u00b7 \u00b7 \u00b7 , An, P1, \u00b7 \u00b7 \u00b7 , Pn, R1, \u00b7 \u00b7 \u00b7 , Rn, \u03b31, \u00b7 \u00b7 \u00b7 , \u03b3n, K1, \u00b7 \u00b7 \u00b7 , Kn, K\u22121, \u00b7 \u00b7 \u00b7 , K\u2212n > (3) where n is the number of agents, Si, Ai, Pi, Ri, \u03b3i, Ki, K\u2212i, i = 1, \u00b7 \u00b7 \u00b7 , n are the sets of environment states, the sets of actions, the state transition probability matrixes, the reward functions, the discount factors, the sets of knowledge from local environment interactions and the sets of received knowledge of every agent in the group. Note that K\u2212i = {K1,i, \u00b7 \u00b7 \u00b7 , Ki\u22121,i, Ki+1,i, \u00b7 \u00b7 \u00b7 , Kn,i} where Ki,i\u2032 \u2286 Ki is the knowledge of agent i shared to agent i\u2032. Each agent can send its knowledge to any other agents arbitrarily and store its received knowledge in local memory for training. From this formulation, we can see that different from multi-agent reinforcement learning where the state set and state transition probability matrix are shared among all agents, each agent in GARL works in its own separate environment so that it has its own set of environment states, and an agent\u2019s environment is independent of any other agent\u2019s environment so that it has its own state transition probability matrix. Every agent has its own set of actions, reward function, discount factor, set of local knowledge and set of received knowledge. The knowledge communication is the only fact that connects the agents in GARL. One interesting special case would be that every agent has the same state set, action set, state transition probability matrix, reward function and discount factor. Due to the difference in the starting points of their environments, agents would have different learning paths and are at different state at a given time. Thus they will have different sets of local and received knowledge. An example of this scenario is multiple reinforcement learning agents playing the same video game while sharing knowledge with each other. We later evaluate our method for GARL on this example scenario in section 6. Overall GARL describes a cooperative reinforcement learning problem involving multiple agents. The agents work in separate environments but communicate with each other sharing knowledge to help with each other\u2019s learning process. The knowledge can be in various forms, such as raw 5 experiences (state, action, reward tuple), policy parameters, values, gradients at each update iteration, etc. With this knowledge sharing, GARL aims to bene\ufb01t each single agent\u2019s learning quality and speed. It is abstracted from real-life scenarios (real human behaviour) and has many applications such as video game playing and autonomous driving. Besides, we claim that it has great potential in networking problems due to the independent environment of each network node and the natural communication network between them. Learning to make routing decisions is a promising direction in networking research area. For future efforts on GARL, we provide three directions for now: knowledge sharing, communication and adapted individual reinforcement learning algorithm. 5 Decentralised Distributed Asynchronous Learning (DDAL) This section proposes DDAL as the \ufb01rst framework designed for GARL. The idea is fourfold: \u2022 Decentralised control: The GARL system naturally comes in a decentralised manner where every agent is autonomous and can learn independently. Arti\ufb01cially having them managed by a central controller can be expensive and sometimes meaningless or unrealistic. Thus we work on decentralised control. \u2022 Asynchronous communication: To give as much freedom as possible to the agents, we design to let the communication happen in an asynchronous manner. Synchronous communication means that the agents should all agree to dedicated communication stages when they are all sending or receiving messages. This can be very dif\ufb01cult in real-world applications since organising these autonomous agents needs lots of efforts. Thus we have asynchronous communication where each agent can send knowledge to other agents or receive knowledge from them at any convenient time. \u2022 Independent learning at beginning stage: To explain this from intuition, we are probably not able to acquire very accurate knowledge at the beginning stage of our learning and sharing of beginners\u2019 mistakes would have negative effect on others\u2019 learning process, hence it is good practice to start group communication after everyone has reached a relatively stable learning status. \u2022 Weighted gradient average: We use gradients as the form of knowledge among agents and require that all gradients ever generated will be shared to every other agent (Ki,i\u2032 = Ki, i, i\u2032 = 1, \u00b7 \u00b7 \u00b7 , n). Each piece of gradients (for one model update) from any agent is accompanied with two extra pieces of information, the amount of training so far and its relevance to the agent that it\u2019s going to. For example, the number of training epochs performed by an agent can represent the amount of training that the agent has experienced so far, namely the amount of training so far for the piece of gradients just generated by this agent. We quantify these two pieces of information with Tj (training experience) and Rj (relevance) for the j \u2212th piece of gradients represented as gj in a chunk of received gradient pieces. When agent i is ready to perform a model update involving received gradients, it retrieves m pieces of gradients from Ki \u222a K\u2212i and calculates a weighted gradient average according to the following equation g = 1 2( m \ufffd j=1 Tj \ufffdm j=1 Tj gj + m \ufffd j=1 Rj \ufffdm j=1 Rj gj), (4) then perform the update with g. The average operation allows us to mitigate the in\ufb02uence introduced by poor experiences and introducing weights lowers the in\ufb02uence of immature or irrelevant knowledge. The algorithm at each agent is shown in Algorithm 1. After being trained for a number of epochs, the agent starts to send its gradients to other agents and perform model updates with received gradients every few epochs. We do not have global organising mechanism for the agent system thus each agent is basically on its own. In our implementation, this decentralised control and asynchronous communication is realised through multiprocessing queues. Every agent has its own queue to hold the knowledge received from other agents, and these queues are shared among all agents so that each agent is free to send its knowledge to any other agent\u2019s queue. And the agents are implemented with Salina (Denoyer et al., 2021) which is a lightweight library extending PyTorch modules for 6 developing sequential decision models. We claim that DDAL should not be restricted by agent type. The single-agent algorithms as discussed in section 3 should all be able to serve as our agent\u2019s brain. Here we discuss two popular types of agents, DQN agent and A2C agent. Algorithm 1 DDAL at the i \u2212 th agent Require: Initialise knowledge set Ki and K\u2212i 1: for each epoch do 2: Generate k experiences 3: Compute average loss 4: Compute gradients 5: if epoch < threshold then 6: Update model with the gradients 7: else 8: Append the gradients with weighting information T and R 9: Store the gradients in Ki 10: Send a copy of the gradients to every other agent j (stored in K\u2212j) (j = 1, \u00b7 \u00b7 \u00b7 , i \u2212 1, i + 1, \u00b7 \u00b7 \u00b7 , n) 11: if epoch%minibatch == 0 then 12: Get m pieces of gradients from Ki \u222a K\u2212i 13: Compute g of these gradients 14: Update model with g 15: end if 16: end if 17: end for 5.1 Decentralised Distributed Asynchronous Deep Q Network (DDADQN) We consider a combination Double DQN and Dueling DQN as discussed in section 3 to serve as the brain of our agents. Thus the gradients for this combined Q-network are calculated as \u2207\u03b8L = \u2207\u03b8(yt \u2212 Q(\u03c6t, at; \u03b8t))2 (5) where \u03c6t is a preprocessed version of state st, \u03b8t represents the network parameters and yt = \ufffdr for terminal \u03c6t+1 r + \u03b3Q(\u03c6t+1, argmaxa\u2032 Q(\u03c6t+1, a\u2032; \u03b8t); \u03b8\u2212 t ) for non \u2212 terminal \u03c6t+1 (6) where r is the immediate reward for taking action at under state st and \u03b8\u2212 t is the network parameters of the target Q-network. Further, we also have Q(st, at) = A(st, at) + V (st) (7) where A(st, at) and V (st) are the raw network output. With this DQN agent, we name the complete algorithm as DDADQN. 5.2 Decentralised Distributed Asynchronous Advantage Actor-Critic (DDA3C) Besides, we also consider a classic A2C agent. The A2C algorithm is brie\ufb02y discussed in section 3. Based on equation 7, we can have the gradients for policy network as \u2207\u03b8log\u03c0\u03b8(at|st)A(st, at) = \u2207\u03b8log\u03c0\u03b8(at|st)(Q(st, at) \u2212 V (st)) (8) where Q(st, at) = \ufffdr for terminal st+1 r + \u03b3V (st+1) for non \u2212 terminal st+1. (9) 7 (a) Single-agent (b) Group-agent (2 agents): agent 1 (c) Group-agent (2 agents): agent 2 Figure 2: DDA3C single-agent vs. group-agent (2 agents) (a) agent 1 (b) agent 2 (c) agent 3 (d) agent 4 Figure 3: DDA3C group-agent (4 agents) The gradients for value function network are derived following the same principle as in DQN. With this A2C agent, we name the complete algorithm as DDA3C. 6 Evaluations In this section, we evaluate DDA3C and DDADQN on the special example scenario where multiple agents play the same computer game while sharing knowledge with each other. Due to the consistency in learning environment and goal, every agent\u2019s knowledge is of equal relevance to other agents so that we ignore the Rj parameters in gradients. The Tj parameters are set proportional to the number of training epochs so far. The result is obtained from multiple runs as a typical one. 6.1 DDA3C We test DDA3C on the task of CartPole-v0 game in OpenAI Gym. In each epoch, we run one episode of CartPole-v0 with a limitation of maximum 100 steps. CartPole-v0 environment will give a reward of +1 for every timestep that the pole remains upright and end when terminal states reached. Hence a total reward of 100 means an optimal policy for a 100-step episode \u2013 every move scores. The epoch minibatch size is set to 100. In Figure 2, we do the training for totally 50k epochs (the x-axis is the epoch number) and start the knowledge sharing at 20k-th epoch for the two-agent group learning case. Figure 2a is the (a) agent 1 (b) agent 2 (c) agent 3 (d) agent 4 (e) agent 5 (f) agent 6 Figure 4: DDA3C group-agent (6 agents) 8 ",
    "Conclusion": "Conclusion We introduce the third type of reinforcement learning problem formulated as GARL in which multiple agents study cooperatively in a group, sharing knowledge with each other. GARL describes a very common and important type of real-life learning scenario \u2013 human learning behaviour, which has a huge application potential. We describe a three-stage training process of autonomous driving which can be a key application of GARL. Then we propose the \ufb01rst framework called DDAL for GARL, which should not be restricted by agent type. It is evaluated with two types of agent, DQN agent and A2C agent, on the CartPole-v0 game environment. The result shows that with just two agents DDA3C is able to learn stable optimal policy while single A2C agent can not. With more agents there is a better probability for some outliers to show up but the group learning system is robust since the normal agents that work well are still the majority. Though single DQN agent is already able to maintain a stable optimal policy, DDADQN manages to converge faster and more steadily. 9 ",
    "References": "References R. Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):679\u2013684, 1957. ISSN 00959057, 19435274. URL http://www.jstor.org/stable/24900506. L. Bu\u00b8soniu, R. Babu\u0161ka, and B. D. Schutter. Multi-agent reinforcement learning: An overview. Innovations in multi-agent systems and applications-1, pages 183\u2013221, 2010. L. Denoyer, A. de la Fuente, S. Duong, J.-B. Gaya, P.-A. Kamienny, and D. H. Thompson. Salina: Sequential learning of agents. https://gitHub.com/facebookresearch/salina, 2021. H. Hasselt. Double q-learning. Advances in neural information processing systems, 23:2613\u20132621, 2010. P. C. Heredia and S. Mou. Distributed multi-agent reinforcement learning by actor-critic method. IFAC-PapersOnLine, 52(20):363\u2013368, 2019. D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van Hasselt, and D. Silver. Distributed prioritized experience replay, 2018. R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. M. Luo, J. Yao, R. Liaw, E. Liang, and I. Stoica. Impact: Importance weighted asynchronous architectures with clipped target networks, 2020. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015. doi: 10.1038/nature14236. URL https://doi.org/10.1038/nature14236. V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016. A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D. Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, S. Legg, V. Mnih, K. Kavukcuoglu, and D. Silver. Massively parallel methods for deep reinforcement learning, 2015. T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and S. Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4295\u20134304. PMLR, 2018. A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani. Deep reinforcement learning framework for autonomous driving. Electronic Imaging, 2017(19):70\u201376, 2017. M. R. Samsami and H. Alimadad. Distributed deep reinforcement learning: An overview. CoRR, abs/2011.11012, 2020. URL https://arxiv.org/abs/2011.11012. J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, T. Lillicrap, and D. Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020. doi: 10.1038/s41586-020-03051-4. URL https://doi.org/10.1038/s41586-020-03051-4. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017. 10 D. Silver, L. Newnham, D. Barker, S. Weller, and J. McFall. Concurrent reinforcement learning from customer interactions. In International conference on machine learning, pages 924\u2013932. PMLR, 2013. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. doi: 10.1038/nature16961. URL https://doi.org/10.1038/nature16961. D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676): 354\u2013359, 2017. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018. H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on arti\ufb01cial intelligence, volume 30, 2016. Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pages 1995\u2013 2003. PMLR, 2016. C. J. C. H. Watkins. Learning from delayed rewards. 1989. C. Wen, X. Yao, Y. Wang, and X. Tan. Smix (\u03bb): Enhancing centralized value functions for cooperative multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pages 7301\u20137308, 2020. E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames, 2020. 11 ",
    "title": "Group-Agent Reinforcement Learning",
    "paper_info": "Group-Agent Reinforcement Learning\nKaiyue Wu\nDepartment of Computer Science\nThe University of Manchester\nManchester M13 9PL, UK\nkaiyue.wu@postgrad.manchester.ac.uk\nXiao-Jun Zeng\nDepartment of Computer Science\nThe University of Manchester\nManchester M13 9PL, UK\nx.zeng@manchester.ac.uk\nAbstract\nIt can largely bene\ufb01t the reinforcement learning process of each agent if multiple\nagents perform their separate reinforcement learning tasks cooperatively. Different\nfrom multi-agent reinforcement learning where multiple agents are in a common\nenvironment and should learn to cooperate or compete with each other, in this case\neach agent has its separate environment and only communicate with others to share\nknowledge without any cooperative or competitive behaviour as a learning outcome.\nIn fact, this learning scenario is not well understood yet and not well formulated.\nAs the \ufb01rst effort, we propose group-agent reinforcement learning as a formulation\nof this scenario and the third type of reinforcement learning problem with respect\nto single-agent and multi-agent reinforcement learning. We then propose the \ufb01rst\ndistributed reinforcement learning framework called DDAL (Decentralised Dis-\ntributed Asynchronous Learning) designed for group-agent reinforcement learning.\nWe show through experiments that DDAL achieved desirable performance with\nvery stable training and has good scalability.\n1\nIntroduction\nCurrently the research area of reinforcement learning has two broad categories of problems, which\nare single-agent reinforcement learning and multi-agent reinforcement learning. For single-agent\nreinforcement learning problems, there is only one intelligent agent involved in the learning process. It\nlearns through interacting with its surroundings in order to achieve the objective of optimal individual\nbehaviour. For multi-agent reinforcement learning problems, there are multiple agents involved in\nthe learning process. They can learn not only though interacting with their surroundings but also\nthough interacting with each other, with the objective of not only optimal individual behaviour but\nalso optimal team behaviour of cooperation or equilibrium behaviour of competition. These two parts\nof the learning objective are not separable since cooperative or competitive behaviour are actually an\nessential part of individual behaviour in multi-agent reinforcement learning.\nHowever, there are some real-world reinforcement learning scenarios which cannot be classi\ufb01ed\ninto either one of the two categories and still lack understanding. In existing literature, there is a\nlack of understanding for pure cooperative learning and an ambiguity between cooperative learning\nand learning to cooperate. In a typical multi-agent problem, such as robotic soccer playing, all\nthe agents perform learning activities together in a common environment. For each single agent,\nthe other agents are part of its environment which becomes non-stationary due to the continually\nchanging behaviour of those agents (Samsami and Alimadad, 2020). In this case, all agents from\none team are learning together to cooperate with each other to achieve a common goal. It is obvious\nthat learning to cooperate often involves cooperative learning (completely independent learning is\nalso possible) which is basically the learning process with knowledge shared among agents, since\nagents would probably need information from others to cooperate with them. But from the other\nway around, cooperative learning does not involve learning to cooperate very often. Agents can be\nlearning with different goals in their own separate environment which is not affected by others and\narXiv:2202.05135v3  [cs.LG]  30 Apr 2022\n",
    "GPTsummary": "\n                    - (1): The paper aims at addressing the reinforcement learning scenario where multiple agents perform their separate reinforcement learning tasks cooperatively.\n\n\n                    - (2): The paper proposes a new reinforcement learning problem called group-agent reinforcement learning, which is different from the traditional multi-agent reinforcement learning. The existing multi-agent reinforcement learning lacks understanding for pure cooperative learning, and there is an ambiguity between cooperative learning and learning to cooperate. The approach is well motivated as the cooperative learning scenario is common in real-world problems.\n\n\n                    - (3): The paper proposes a decentralized distributed asynchronous learning framework called DDAL designed for group-agent reinforcement learning.\n\n\n                    - (4): The proposed framework DDAL is evaluated on various cooperative learning tasks and achieves desirable performance with very stable training and good scalability. The performance supports their goals of achieving effective cooperative learning among multiple agents in separate environments.\n\n\n\n\n\n8. Conclusion:\n\n- (1): This work introduces a new reinforcement learning problem called group-agent reinforcement learning, which is motivated by real-world scenarios where multiple agents need to cooperate to achieve a common goal. The proposed framework, DDAL, provides an effective solution to this problem and achieves desirable performance with good scalability.\n                     \n- (2): Innovation point: The paper proposes a new reinforcement learning problem - group-agent reinforcement learning, which is an important and common scenario in real-world applications. It also proposes a novel framework DDAL designed specifically for this problem. Performance: The proposed framework achieves desirable performance with very stable training and good scalability in various cooperative learning tasks. Workload: The paper lacks detailed analysis of the computational complexity and memory usage of the proposed framework, and more experiments on larger-scale environments could be included to validate the scalability.\n\n\n",
    "GPTmethods": " not found",
    "GPTconclusion": "- (1): This work introduces a new reinforcement learning problem called group-agent reinforcement learning, which is motivated by real-world scenarios where multiple agents need to cooperate to achieve a common goal. The proposed framework, DDAL, provides an effective solution to this problem and achieves desirable performance with good scalability.\n                     \n- (2): Innovation point: The paper proposes a new reinforcement learning problem - group-agent reinforcement learning, which is an important and common scenario in real-world applications. It also proposes a novel framework DDAL designed specifically for this problem. Performance: The proposed framework achieves desirable performance with very stable training and good scalability in various cooperative learning tasks. Workload: The paper lacks detailed analysis of the computational complexity and memory usage of the proposed framework, and more experiments on larger-scale environments could be included to validate the scalability.\n\n\n"
}